<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]
- [cs.LG](#cs.LG) [Total: 139]
- [cs.CR](#cs.CR) [Total: 15]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [eess.SY](#eess.SY) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [stat.ML](#stat.ML) [Total: 10]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SI](#cs.SI) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 39]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.SD](#cs.SD) [Total: 11]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 71]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]
- [astro-ph.CO](#astro-ph.CO) [Total: 2]
- [cs.MA](#cs.MA) [Total: 4]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [math.CT](#math.CT) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review](https://arxiv.org/abs/2505.20306)
*Xueqiang Ouyang,Jia Wei*

Main category: cs.AI

TL;DR: Artificial intelligence-based multi-modal technology is crucial for improving pregnancy success rate, as reviewed in this article.


<details>
  <summary>Details</summary>
Motivation: Infertility is a global issue and assisted reproductive technology can help solve it. Traditional methods face challenges like subjectivity in embryo grading and inefficiency in integrating multi-modal data.

Method: The article reviews the progress of multi-modal artificial intelligence application in embryo grading and pregnancy prediction using different data types (static images, time-lapse videos, structured table data).

Result: Discusses the current main challenges such as complexity of multi-modal information fusion and data scarcity.

Conclusion: Multi-modal artificial intelligence has promising applications but faces certain challenges that need to be addressed.

Abstract: As a global disease, infertility has always affected human beings. The
development of assisted reproductive technology can effectively solve this
disease. However, the traditional in vitro fertilization-embryo transfer
technology still faces many challenges in improving the success rate of
pregnancy, such as the subjectivity of embryo grading and the inefficiency of
integrating multi-modal data. Therefore, the introduction of artificial
intelligence-based technologies is particularly crucial. This article reviews
the application progress of multi-modal artificial intelligence in embryo
grading and pregnancy prediction based on different data modalities (including
static images, time-lapse videos and structured table data) from a new
perspective, and discusses the main challenges in current research, such as the
complexity of multi-modal information fusion and data scarcity.

</details>


### [2] [Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System](https://arxiv.org/abs/2505.20310)
*Wanghan Xu,Wenlong Zhang,Fenghua Ling,Ben Fei,Yusong Hu,Fangxuan Ren,Jintai Lin,Wanli Ouyang,Lei Bai*

Main category: cs.AI

TL;DR: The paper proposes Manalyzer, a multi-agent system for automated meta-analysis that reduces errors in paper screening and data extraction through hybrid review, hierarchical extraction, self-proving, and feedback checking strategies. It shows significant performance improvements over LLM baselines.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional meta-analysis methods which require substantial human effort and time, as well as challenges faced by LLM-based methods such as hallucinations in paper screening and data extraction.

Method: Manalyzer is a multi-agent system that uses tool calls to achieve end-to-end automated meta-analysis. It implements hybrid review, hierarchical extraction, self-proving, and feedback checking strategies to reduce hallucinations in paper screening and data extraction.

Result: Extensive experiments on a new benchmark comprising 729 papers across 3 domains show that Manalyzer achieves significant performance improvements over the LLM baseline in multi meta-analysis tasks.

Conclusion: Manalyzer effectively alleviates issues related to hallucinations in paper screening and data extraction, leading to significant improvements in the performance of meta-analysis.

Abstract: Meta-analysis is a systematic research methodology that synthesizes data from
multiple existing studies to derive comprehensive conclusions. This approach
not only mitigates limitations inherent in individual studies but also
facilitates novel discoveries through integrated data analysis. Traditional
meta-analysis involves a complex multi-stage pipeline including literature
retrieval, paper screening, and data extraction, which demands substantial
human effort and time. However, while LLM-based methods can accelerate certain
stages, they still face significant challenges, such as hallucinations in paper
screening and data extraction. In this paper, we propose a multi-agent system,
Manalyzer, which achieves end-to-end automated meta-analysis through tool
calls. The hybrid review, hierarchical extraction, self-proving, and feedback
checking strategies implemented in Manalyzer significantly alleviate these two
hallucinations. To comprehensively evaluate the performance of meta-analysis,
we construct a new benchmark comprising 729 papers across 3 domains,
encompassing text, image, and table modalities, with over 10,000 data points.
Extensive experiments demonstrate that Manalyzer achieves significant
performance improvements over the LLM baseline in multi meta-analysis tasks.
Project page: https://black-yt.github.io/meta-analysis-page/ .

</details>


### [3] [Reasoning in Neurosymbolic AI](https://arxiv.org/abs/2505.20313)
*Son Tran,Edjard Mota,Artur d'Avila Garcez*

Main category: cs.AI

TL;DR: This paper introduces a simple energy-based neurosymbolic AI system that integrates propositional logic reasoning with learning, highlighting its potential to address challenges in Large Language Models (LLMs) like data efficiency, fairness, and safety. It empirically evaluates the system's reasoning capabilities using Restricted Boltzmann Machines (RBMs) and compares it with symbolic, neural, and neurosymbolic systems. The research aims to reinvigorate interest in neural networks for logical reasoning and promote the integration of reasoning and learning in deep networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current AI dominated by LLMs, such as data inefficiency, fairness, and safety issues, by leveraging neurosymbolic AI systems that combine formal reasoning with learning from data and knowledge.

Method: An energy-based neurosymbolic AI system is developed to represent and reason about propositional logic formulas. Logical reasoning is linked to energy minimization using RBMs. The method also involves empirical evaluation comparing the performance of symbolic, neural, and neurosymbolic systems in terms of learning from data and knowledge.

Result: The results show the correspondence between logical reasoning and energy minimization in RBMs, demonstrating the effectiveness of the neurosymbolic system in integrating reasoning and learning. Comparisons indicate the advantages of neurosymbolic approaches over purely symbolic or neural methods.

Conclusion: The chapter concludes by emphasizing the importance of neurosymbolic AI within a broader framework of formal reasoning and accountability in AI. It discusses the challenges and potential of neurosymbolic AI to enhance the reliability of deep learning.

Abstract: Knowledge representation and reasoning in neural networks have been a
long-standing endeavor which has attracted much attention recently. The
principled integration of reasoning and learning in neural networks is a main
objective of the area of neurosymbolic Artificial Intelligence (AI). In this
chapter, a simple energy-based neurosymbolic AI system is described that can
represent and reason formally about any propositional logic formula. This
creates a powerful combination of learning from data and knowledge and logical
reasoning. We start by positioning neurosymbolic AI in the context of the
current AI landscape that is unsurprisingly dominated by Large Language Models
(LLMs). We identify important challenges of data efficiency, fairness and
safety of LLMs that might be addressed by neurosymbolic reasoning systems with
formal reasoning capabilities. We then discuss the representation of logic by
the specific energy-based system, including illustrative examples and empirical
evaluation of the correspondence between logical reasoning and energy
minimization using Restricted Boltzmann Machines (RBM). Learning from data and
knowledge is also evaluated empirically and compared with a symbolic, neural
and a neurosymbolic system. Results reported in this chapter in an accessible
way are expected to reignite the research on the use of neural networks as
massively-parallel models for logical reasoning and promote the principled
integration of reasoning and learning in deep networks. We conclude the chapter
with a discussion of the importance of positioning neurosymbolic AI within a
broader framework of formal reasoning and accountability in AI, discussing the
challenges for neurosynbolic AI to tackle the various known problems of
reliability of deep learning.

</details>


### [4] [Reinforcement Speculative Decoding for Fast Ranking](https://arxiv.org/abs/2505.20316)
*Yingpeng Du,Tianjun Wei,Zhu Sun,Jie Zhang*

Main category: cs.AI

TL;DR: The paper introduces a Reinforcement Speculative Decoding method for fast ranking inference of LLMs in IR and RS systems, proposing an up-to-down decoding paradigm with constrained budget and demonstrating its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by speculative decoding methods in ranking systems such as information retrieval (IR) systems and recommender systems (RSs), particularly their left-to-right decoding paradigm which does not meet strict latency constraints and discards listwise ranking knowledge.

Method: A Reinforcement Speculative Decoding method is proposed, featuring an up-to-down decoding paradigm that uses an agent to iteratively modify the ranking sequence under a constrained budget. A ranking-tailored policy optimization is designed, utilizing reinforcement learning to explore optimal multi-round ranking modification policies.

Result: Experiments conducted on both IR and RS tasks demonstrate the effectiveness of the proposed method in providing fast ranking inference for LLMs while meeting the latency requirements of ranking systems.

Conclusion: The Reinforcement Speculative Decoding method successfully alleviates the latency issue in ranking systems using LLMs and shows theoretical robustness and advantages, proving effective in IR and RS tasks.

Abstract: Large Language Models (LLMs) have been widely adopted in ranking systems such
as information retrieval (IR) systems and recommender systems (RSs). To
alleviate the latency of auto-regressive decoding, some studies explore the
single (first) token decoding for ranking approximation, but they suffer from
severe degradation in tail positions. Although speculative decoding (SD)
methods can be a remedy with verification at different positions, they face
challenges in ranking systems due to their left-to-right decoding paradigm.
Firstly, ranking systems require strict latency constraints, but verification
rounds in SD methods remain agnostic; Secondly, SD methods usually discard
listwise ranking knowledge about unaccepted items in previous rounds, hindering
future multi-token prediction, especially when candidate tokens are the
unaccepted items. In this paper, we propose a Reinforcement Speculative
Decoding method for fast ranking inference of LLMs. To meet the ranking
systems' latency requirement, we propose an up-to-down decoding paradigm that
employs an agent to iteratively modify the ranking sequence under a constrained
budget. Specifically, we design a ranking-tailored policy optimization,
actively exploring optimal multi-round ranking modification policy verified by
LLMs via reinforcement learning (RL). To better approximate the target LLM
under the constrained budget, we trigger the agent fully utilizing the listwise
ranking knowledge about all items verified by LLMs across different rounds in
RL, enhancing the modification policy of the agent. More importantly, we
demonstrate the theoretical robustness and advantages of our paradigm and
implementation. Experiments on both IR and RS tasks show the effectiveness of
our proposed method.

</details>


### [5] [Challenges for artificial cognitive systems](https://arxiv.org/abs/2505.20339)
*Antoni Gomila,Vincent C. Müller*

Main category: cs.AI

TL;DR: The paper aims to define the challenges and progress in cognitive systems research by formulating challenges for artificial cognitive systems.


<details>
  <summary>Details</summary>
Motivation: There is a need for defined questions or challenges to guide the progress of cognitive systems research.

Method: Articulating challenges based on a definition of cognitive systems as systems that learn from experience and use acquired knowledge flexibly to achieve goals.

Result: Challenges for artificial cognitive systems are formulated within the context of the European Network for Cognitive Systems.

Conclusion: The challenges presented serve as guidelines for the aims and what would constitute progress in the field of artificial cognitive systems.

Abstract: The declared goal of this paper is to fill this gap: "... cognitive systems
research needs questions or challenges that define progress. The challenges are
not (yet more) predictions of the future, but a guideline to what are the aims
and what would constitute progress." -- the quotation being from the project
description of EUCogII, the project for the European Network for Cognitive
Systems within which this formulation of the 'challenges' was originally
developed (http://www.eucognition.org). So, we stick out our neck and formulate
the challenges for artificial cognitive systems. These challenges are
articulated in terms of a definition of what a cognitive system is: a system
that learns from experience and uses its acquired knowledge (both declarative
and practical) in a flexible manner to achieve its own goals.

</details>


### [6] [Machine Theory of Mind and the Structure of Human Values](https://arxiv.org/abs/2505.20342)
*Paul de Font-Reaulx*

Main category: cs.AI

TL;DR: Value learning is vital for safe AI. The paper argues human values have a generative rational structure, allowing us to solve the value generalization problem using Bayesian Theory of Mind models.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is that current methods for inferring human values primarily focus on behavior, but humans care about more than what can be demonstrated through actions. Therefore, there's a need for AI to predict seemingly complex values from limited samples.

Method: The method proposed in this paper involves using Bayesian Theory of Mind models to infer human values not only from behaviour but also from other values by leveraging the generative rational structure of human values.

Result: This approach reveals that it's possible to solve the value generalization problem and develop a more comprehensive understanding of human values beyond simple utility functions.

Conclusion: The conclusion drawn is that developing generative value-to-value inference is essential for achieving a scalable machine theory of mind.

Abstract: Value learning is a crucial aspect of safe and ethical AI. This is primarily
pursued by methods inferring human values from behaviour. However, humans care
about much more than we are able to demonstrate through our actions.
Consequently, an AI must predict the rest of our seemingly complex values from
a limited sample. I call this the value generalization problem. In this paper,
I argue that human values have a generative rational structure and that this
allows us to solve the value generalization problem. In particular, we can use
Bayesian Theory of Mind models to infer human values not only from behaviour,
but also from other values. This has been obscured by the widespread use of
simple utility functions to represent human values. I conclude that developing
generative value-to-value inference is a crucial component of achieving a
scalable machine theory of mind.

</details>


### [7] [SCAR: Shapley Credit Assignment for More Efficient RLHF](https://arxiv.org/abs/2505.20417)
*Meng Cao,Shuyuan Zhang,Xiao-Wen Chang,Doina Precup*

Main category: cs.AI

TL;DR: The paper introduces Shapley Credit Assignment Rewards (SCAR), a new method for reinforcement learning from human feedback (RLHF) that uses Shapley values to distribute rewards among tokens or text spans in generated sequences, leading to dense reward signals without needing auxiliary models or detailed human annotations. SCAR preserves the original optimal policy and shows faster convergence and higher final scores in various tasks.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning from Human Feedback (RLHF) is effective for aligning LLMs with human preferences but suffers from sparse reward signals, making credit assignment difficult. Current methods often rely on single scalar scores for entire sequences, lacking insights into token-level contributions.

Method: SCAR leverages Shapley values from cooperative game theory to distribute sequence-level rewards among tokens or text spans based on their marginal contributions. This creates dense reward signals without requiring additional model training or fine-grained human annotations.

Result: Empirically, SCAR converges significantly faster and achieves higher final reward scores compared to standard RLHF and attention-based dense reward baselines across diverse tasks such as sentiment control, text summarization, and instruction tuning.

Conclusion: SCAR provides a more effective and theoretically sound approach to credit assignment in RLHF, enhancing the alignment of LLMs with human preferences.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a widely used technique
for aligning Large Language Models (LLMs) with human preferences, yet it often
suffers from sparse reward signals, making effective credit assignment
challenging. In typical setups, the reward model provides a single scalar score
for an entire generated sequence, offering little insight into which token or
span-level decisions were responsible for the outcome. To address this, we
propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages
Shapley values in cooperative game theory. SCAR distributes the total
sequence-level reward among constituent tokens or text spans based on their
principled marginal contributions. This creates dense reward signals,
crucially, without necessitating the training of auxiliary critique models or
recourse to fine-grained human annotations at intermediate generation stages.
Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for
fair credit attribution. Theoretically, we demonstrate that SCAR preserves the
original optimal policy, and empirically, across diverse tasks including
sentiment control, text summarization, and instruction tuning, we show that
SCAR converges significantly faster and achieves higher final reward scores
compared to standard RLHF and attention-based dense reward baselines. Our
findings suggest that SCAR provides a more effective and theoretically sound
method for credit assignment in RLHF, leading to more efficient alignment of
LLMs.

</details>


### [8] [Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration](https://arxiv.org/abs/2505.20466)
*P. S. Kesavan,Pontus Nordenfelt*

Main category: cs.AI

TL;DR: 智能显微技术通过结合自动化、计算能力和人工智能，不仅实现了从传统观察工具到科学探究积极合作者的转变，并且提出了六个核心设计原则，为构建超越自动化并主动支持假设生成、见解发现和理论发展的显微镜系统提供了路线图。


<details>
  <summary>Details</summary>
Motivation: 生物成像领域需要从被动观察工具转向主动协作工具，以弥合细胞研究中可观测（经验领域）与需理解（知识领域）之间的差距。

Method: 提出一个理论框架，包含六个核心设计原则：知识-经验意识、层次上下文整合、从检测到感知的演变、自适应测量框架、叙事综合能力和跨上下文推理。

Result: 该框架提供了一种多代理架构，旨在将经验观察与科学理解的目标对齐。

Conclusion: 智能显微技术重新定义了科学仪器在知识创造过程中的角色，能够主动支持假设生成、见解发现和理论发展。

Abstract: Smart microscopy represents a paradigm shift in biological imaging, moving
from passive observation tools to active collaborators in scientific inquiry.
Enabled by advances in automation, computational power, and artificial
intelligence, these systems are now capable of adaptive decision-making and
real-time experimental control. Here, we introduce a theoretical framework that
reconceptualizes smart microscopy as a partner in scientific investigation.
Central to our framework is the concept of the 'epistemic-empirical divide' in
cellular investigation-the gap between what is observable (empirical domain)
and what must be understood (epistemic domain). We propose six core design
principles: epistemic-empirical awareness, hierarchical context integration, an
evolution from detection to perception, adaptive measurement frameworks,
narrative synthesis capabilities, and cross-contextual reasoning. Together,
these principles guide a multi-agent architecture designed to align empirical
observation with the goals of scientific understanding. Our framework provides
a roadmap for building microscopy systems that go beyond automation to actively
support hypothesis generation, insight discovery, and theory development,
redefining the role of scientific instruments in the process of knowledge
creation.

</details>


### [9] [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/abs/2505.20521)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luis Frazão,Nuno Costa,António Pereira*

Main category: cs.AI

TL;DR: Project Riley is a multimodal and multi-model conversational AI architecture simulating reasoning influenced by emotional states, evaluated positively in structured scenarios.


<details>
  <summary>Details</summary>
Motivation: To create an AI system capable of simulating reasoning influenced by emotional states, drawing inspiration from Pixar's Inside Out.

Method: The system comprises five distinct emotional agents that engage in structured multi-round dialogues to generate responses. A final reasoning mechanism synthesises the contributions into a coherent output. It incorporates textual and visual LLMs, advanced reasoning, and self-refinement processes.

Result: The prototype was evaluated through user testing, showing strong performance in structured scenarios, particularly in emotional alignment and communicative clarity.

Conclusion: Project Riley demonstrates the potential for emotionally expressive and computationally efficient conversational AI systems.

Abstract: This paper presents Project Riley, a novel multimodal and multi-model
conversational AI architecture oriented towards the simulation of reasoning
influenced by emotional states. Drawing inspiration from Pixar's Inside Out,
the system comprises five distinct emotional agents - Joy, Sadness, Fear,
Anger, and Disgust - that engage in structured multi-round dialogues to
generate, criticise, and iteratively refine responses. A final reasoning
mechanism synthesises the contributions of these agents into a coherent output
that either reflects the dominant emotion or integrates multiple perspectives.
The architecture incorporates both textual and visual large language models
(LLMs), alongside advanced reasoning and self-refinement processes. A
functional prototype was deployed locally in an offline environment, optimised
for emotional expressiveness and computational efficiency. From this initial
prototype, another one emerged, called Armando, which was developed for use in
emergency contexts, delivering emotionally calibrated and factually accurate
information through the integration of Retrieval-Augmented Generation (RAG) and
cumulative context tracking. The Project Riley prototype was evaluated through
user testing, in which participants interacted with the chatbot and completed a
structured questionnaire assessing three dimensions: Emotional Appropriateness,
Clarity and Utility, and Naturalness and Human-likeness. The results indicate
strong performance in structured scenarios, particularly with respect to
emotional alignment and communicative clarity.

</details>


### [10] [Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
*Jian Wang,Boyan Zhu,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.AI

TL;DR: The paper investigates the scaling Pareto of test-time scaling and introduces TTSPM, analyzing parallel and sequential scaling paradigms. It derives saturation points for both strategies, empirically validates findings on reasoning benchmarks, and aims to guide resource-efficient inference strategies.


<details>
  <summary>Details</summary>
Motivation: To systematically understand the practical limits and achieve optimal resource allocation when further scaling test-time compute for large reasoning models.

Method: Theoretical analysis of parallel and sequential scaling paradigms from a probabilistic modeling perspective, derivation of saturation points for both strategies, and empirical validation on reasoning benchmarks including AIME, MATH-500, and GPQA.

Result: Identified thresholds beyond which additional computation yields diminishing returns and found that both scaling paradigms converge to a unified mathematical structure in their upper bounds.

Conclusion: Provides insights into cost-benefit trade-offs of test-time scaling, guiding the development of more resource-efficient inference strategies for large reasoning models.

Abstract: Large reasoning models (LRMs) have exhibited the capacity of enhancing
reasoning performance via internal test-time scaling. Building upon this, a
promising direction is to further scale test-time compute to unlock even
greater reasoning capabilities. However, as we push these scaling boundaries,
systematically understanding the practical limits and achieving optimal
resource allocation becomes a critical challenge. In this paper, we investigate
the scaling Pareto of test-time scaling and introduce the Test-Time Scaling
Performance Model (TTSPM). We theoretically analyze two fundamental paradigms
for such extended scaling, parallel scaling and sequential scaling, from a
probabilistic modeling perspective. Our primary contribution is the derivation
of the saturation point on the scaling budget for both strategies, identifying
thresholds beyond which additional computation yields diminishing returns.
Remarkably, despite their distinct mechanisms, both paradigms converge to a
unified mathematical structure in their upper bounds. We empirically validate
our theoretical findings on challenging reasoning benchmarks, including AIME,
MATH-500, and GPQA, demonstrating the practical utility of these bounds for
test-time resource allocation. We hope that this work provides insights into
the cost-benefit trade-offs of test-time scaling, guiding the development of
more resource-efficient inference strategies for large reasoning models.

</details>


### [11] [Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients](https://arxiv.org/abs/2505.20609)
*Hyungjun Park,Chang-Yun Woo,Seungjo Lim,Seunghwan Lim,Keunho Kwak,Ju Young Jeong,Chong Hyun Suh*

Main category: cs.AI

TL;DR: This paper reports a clinical trial of an LLM-based AI interface for diagnosing internal medicine cases, showing higher accuracy, efficiency, and cost-effectiveness compared to physicians while maintaining comparable patient satisfaction.


<details>
  <summary>Details</summary>
Motivation: To evaluate the diagnostic accuracy, efficiency, cost-effectiveness, and patient satisfaction of an LLM-based real-time compound diagnostic medical AI interface in comparison to physicians for common internal medicine cases.

Method: A nonrandomized clinical trial was conducted with one general physician, two internal medicine residents, and five simulated patients. The AI interface and physicians were tested on 10 representative internal medicine cases adapted from USMLE exams. Metrics included accuracy of first and second differential diagnoses, time taken, costs, and patient satisfaction scores.

Result: The AI interface achieved higher accuracy (80% vs 50-70% for first diagnosis, 100% vs 70-90% for first and second diagnoses), was 44.6% faster, and reduced costs by 98.1% compared to physicians. Patient satisfaction scores were slightly lower for the AI interface but still high.

Conclusion: The LLM-based AI interface demonstrated diagnostic capabilities and patient satisfaction comparable to physicians, with greater efficiency and significantly lower costs, suggesting potential for assisting primary care consultations.

Abstract: Objective To develop an LLM based realtime compound diagnostic medical AI
interface and performed a clinical trial comparing this interface and
physicians for common internal medicine cases based on the United States
Medical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A
nonrandomized clinical trial was conducted on August 20, 2024. We recruited one
general physician, two internal medicine residents (2nd and 3rd year), and five
simulated patients. The clinical vignettes were adapted from the USMLE Step 2
CS style exams. We developed 10 representative internal medicine cases based on
actual patients and included information available on initial diagnostic
evaluation. Primary outcome was the accuracy of the first differential
diagnosis. Repeatability was evaluated based on the proportion of agreement.
Results The accuracy of the physicians' first differential diagnosis ranged
from 50% to 70%, whereas the realtime compound diagnostic medical AI interface
achieved an accuracy of 80%. The proportion of agreement for the first
differential diagnosis was 0.7. The accuracy of the first and second
differential diagnoses ranged from 70% to 90% for physicians, whereas the AI
interface achieved an accuracy rate of 100%. The average time for the AI
interface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).
The AI interface ($0.08) also reduced costs by 98.1% compared to the
physicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3
for care by physicians and were 3.9 for the AI interface Conclusion An LLM
based realtime compound diagnostic medical AI interface demonstrated diagnostic
accuracy and patient satisfaction comparable to those of a physician, while
requiring less time and lower costs. These findings suggest that AI interfaces
may have the potential to assist primary care consultations for common internal
medicine cases.

</details>


### [12] [CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models](https://arxiv.org/abs/2505.20642)
*Yi Zhan,Qi Liu,Weibo Gao,Zheng Zhang,Tianfu Wang,Shuanghong Shen,Junyu Lu,Zhenya Huang*

Main category: cs.AI

TL;DR: The paper introduces CoderAgent, a LLM-based agent that simulates students' programming processes in a fine-grained manner without relying on real data. It uses Programming Tree of Thought (PTOT) to break down problem-solving strategies and provides interpretable insights into learning trajectories.


<details>
  <summary>Details</summary>
Motivation: To address the lack of sufficient and high-quality programming data and the mismatch between offline evaluation and real-world learning which hinders the practical deployment of personalized programming tutoring systems.

Method: Propose CoderAgent, an intelligent agent equipped for each human learner focusing on capturing cognitive states during programming practice. The structure of CoderAgent is designed based on ACT-R framework including Programming Tree of Thought (PTOT) to detail analysis of iterative problem-solving strategies.

Result: Experimental evaluations demonstrate that CoderAgent provides interpretable insights into learning trajectories and achieves accurate simulations.

Conclusion: CoderAgent paves the way for personalized programming education by providing interpretable insights into learning trajectories and achieving accurate simulations.

Abstract: Personalized programming tutoring, such as exercise recommendation, can
enhance learners' efficiency, motivation, and outcomes, which is increasingly
important in modern digital education. However, the lack of sufficient and
high-quality programming data, combined with the mismatch between offline
evaluation and real-world learning, hinders the practical deployment of such
systems. To address this challenge, many approaches attempt to simulate learner
practice data, yet they often overlook the fine-grained, iterative nature of
programming learning, resulting in a lack of interpretability and granularity.
To fill this gap, we propose a LLM-based agent, CoderAgent, to simulate
students' programming processes in a fine-grained manner without relying on
real data. Specifically, we equip each human learner with an intelligent agent,
the core of which lies in capturing the cognitive states of the human
programming practice process. Inspired by ACT-R, a cognitive architecture
framework, we design the structure of CoderAgent to align with human cognitive
architecture by focusing on the mastery of programming knowledge and the
application of coding ability. Recognizing the inherent patterns in
multi-layered cognitive reasoning, we introduce the Programming Tree of Thought
(PTOT), which breaks down the process into four steps: why, how, where, and
what. This approach enables a detailed analysis of iterative problem-solving
strategies. Finally, experimental evaluations on real-world datasets
demonstrate that CoderAgent provides interpretable insights into learning
trajectories and achieves accurate simulations, paving the way for personalized
programming education.

</details>


### [13] [AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage](https://arxiv.org/abs/2505.20662)
*Xuanle Zhao,Zilin Sang,Yuxuan Li,Qi Shi,Shuo Wang,Duzhen Zhang,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: Efficient experiment reproduction is crucial in AI. However, due to the complexity of method design and training procedures, automation faces challenges. Reproducing experiments often requires implicit domain-specific knowledge not documented in papers. To solve this, the paper introduces the paper lineage algorithm that extracts implicit knowledge from references cited by the target paper. Based on this, AutoReproduce, a multi-agent framework for end-to-end automatic experiment reproduction, is proposed. It enhances code executability by generating unit tests. A benchmark, ReproduceBench, with verified implementations and novel evaluation metrics are introduced to evaluate reproduction capability. Experimental results show that AutoReproduce outperforms existing baselines and achieves an average performance gap of 22.1% compared to official implementations.


<details>
  <summary>Details</summary>
Motivation: Experiment reproduction in artificial intelligence is critical but challenging due to the complexity of methods and the need for implicit domain-specific knowledge not explicitly documented in papers.

Method: Propose the paper lineage algorithm to extract implicit knowledge from references and introduce AutoReproduce, a multi-agent framework for automatic end-to-end experiment reproduction. Enhance code executability by generating unit tests. Construct ReproduceBench benchmark and introduce novel evaluation metrics.

Result: AutoReproduce outperforms existing strong agent baselines on all five evaluation metrics by over 70%. It achieves an average performance gap of 22.1% compared to official implementations on 89.74% of executable experiment runs.

Conclusion: AutoReproduce is a promising tool for efficient experiment reproduction in AI research.

Abstract: Efficient experiment reproduction is critical to accelerating progress in
artificial intelligence. However, the inherent complexity of method design and
training procedures presents substantial challenges for automation. Notably,
reproducing experiments often requires implicit domain-specific knowledge not
explicitly documented in the original papers. To address this, we introduce the
paper lineage algorithm, which identifies and extracts implicit knowledge from
the relevant references cited by the target paper. Building on this idea, we
propose AutoReproduce, a multi-agent framework capable of automatically
reproducing experiments described in research papers in an end-to-end manner.
AutoReproduce enhances code executability by generating unit tests alongside
the reproduction process. To evaluate the reproduction capability, we construct
ReproduceBench, a benchmark annotated with verified implementations, and
introduce novel evaluation metrics to assess both the reproduction and
execution fidelity. Experimental results demonstrate that AutoReproduce
outperforms the existing strong agent baselines on all five evaluation metrics
by a peak margin of over $70\%$. In particular, compared to the official
implementations, AutoReproduce achieves an average performance gap of $22.1\%$
on $89.74\%$ of the executable experiment runs. The code will be available at
https://github.com/AI9Stars/AutoReproduce.

</details>


### [14] [MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning](https://arxiv.org/abs/2505.20670)
*Zikang Guo,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.AI

TL;DR: The paper introduces MIRROR, a framework utilizing intra-reflection and inter-reflection to leverage LLMs' reflection capabilities before and after action execution, achieving state-of-the-art results in task benchmarks.


<details>
  <summary>Details</summary>
Motivation: Complex tasks with tool integration challenge LLMs, leading to the development of multi-agent workflows. Existing reflection methods only occur post-action, but the authors propose that LLMs can also reflect pre-action like humans to prevent error propagation.

Method: MIRROR incorporates intra-reflection, which evaluates intended actions before execution, and inter-reflection, which adjusts trajectories based on observations, thus leveraging LLMs' reflection abilities comprehensively.

Result: MIRROR demonstrates superior performance on StableToolBench and TravelPlanner benchmarks, achieving state-of-the-art results compared to existing approaches.

Conclusion: MIRROR effectively exploits LLM reflection capabilities both before and after action execution, providing a systematic approach to eliminate and rectify erroneous actions across complex tasks.

Abstract: Complex tasks involving tool integration pose significant challenges for
Large Language Models (LLMs), leading to the emergence of multi-agent workflows
as a promising solution. Reflection has emerged as an effective strategy for
correcting erroneous trajectories in agentic workflows. However, existing
approaches only exploit such capability in the post-action stage, where the
agent observes the execution outcomes. We argue that, like humans, LLMs can
also engage in reflection before action execution: the agent can anticipate
undesirable outcomes from its own decisions, which not only provides a
necessarily complementary perspective to evaluate the decision but also
prevents the propagation of errors throughout the trajectory. In this paper, we
propose MIRROR, a framework that consists of both intra-reflection, which
critically assesses intended actions before execution, and inter-reflection,
which further adjusts the trajectory based on observations. This design
systematically leverages LLM reflection capabilities to eliminate and rectify
erroneous actions on a more comprehensive scope. Evaluations on both the
StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior
performance, achieving state-of-the-art results compared to existing
approaches.

</details>


### [15] [LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation](https://arxiv.org/abs/2505.20671)
*Heng Tan,Hua Yan,Yu Yang*

Main category: cs.AI

TL;DR: This paper proposes a large language model-guided policy modulation framework that improves reinforcement learning training without extra model training or human intervention.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning faces challenges in training effective policies for complex tasks, often converging to local optima and failing to maximize long-term rewards. Existing solutions either involve costly model training or require impractical human feedback.

Method: The method uses a large language model (LLM) to guide policy refinement by identifying critical states from sub-optimal trajectories, providing action suggestions, and assigning implicit rewards.

Result: Experiments on standard RL benchmarks show that the proposed method outperforms state-of-the-art baselines, demonstrating the effectiveness of LLM-based explanations in addressing RL training bottlenecks.

Conclusion: The large language model-guided policy modulation framework is an effective approach to enhance reinforcement learning training, overcoming limitations of previous methods.

Abstract: While reinforcement learning (RL) has achieved notable success in various
domains, training effective policies for complex tasks remains challenging.
Agents often converge to local optima and fail to maximize long-term rewards.
Existing approaches to mitigate training bottlenecks typically fall into two
categories: (i) Automated policy refinement, which identifies critical states
from past trajectories to guide policy updates, but suffers from costly and
uncertain model training; and (ii) Human-in-the-loop refinement, where human
feedback is used to correct agent behavior, but this does not scale well to
environments with large or continuous action spaces. In this work, we design a
large language model-guided policy modulation framework that leverages LLMs to
improve RL training without additional model training or human intervention. We
first prompt an LLM to identify critical states from a sub-optimal agent's
trajectories. Based on these states, the LLM then provides action suggestions
and assigns implicit rewards to guide policy refinement. Experiments across
standard RL benchmarks demonstrate that our method outperforms state-of-the-art
baselines, highlighting the effectiveness of LLM-based explanations in
addressing RL training bottlenecks.

</details>


### [16] [GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning](https://arxiv.org/abs/2505.20672)
*Woochang Sim,Hyunseok Ryu,Kyungmin Choi,Sungwon Han,Sundong Kim*

Main category: cs.AI

TL;DR: The paper introduces GIFARC, an analogy-inspired ARC dataset that uses LLMs and VLMs to synthesize ARC-style tasks from GIF images containing analogies. This approach aims to bridge the gap between AI model performance and human-level reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the significant gap between current AI model performance on ARC tasks (40-55% accuracy) and human-level reasoning.

Method: Leveraging large language models (LLMs) and vision-language models (VLMs), the authors synthesize new ARC-style tasks from a variety of GIF images that include analogies. Each task is paired with ground-truth analogy to provide explicit mapping between visual transformations and everyday concepts.

Result: Empirical validation shows that guiding LLMs with the analogic approach using GIFARC aligns their task-solving approaches with human-like analogical reasoning, efficiently reducing problem complexity.

Conclusion: GIFARC successfully guides AI agents towards more human-like reasoning in solving ARC-style tasks by embedding robust human-intuitive analogies.

Abstract: The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general
AI capabilities, requiring solvers to infer abstract patterns from only a
handful of examples. Despite substantial progress in deep learning,
state-of-the-art models still achieve accuracy rates of merely 40-55% on 2024
ARC Competition, indicative of a significant gap between their performance and
human-level reasoning. In this work, we seek to bridge that gap by introducing
an analogy-inspired ARC dataset, GIFARC. Leveraging large language models
(LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks
from a variety of GIF images that include analogies. Each new task is paired
with ground-truth analogy, providing an explicit mapping between visual
transformations and everyday concepts. By embedding robust human-intuitive
analogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task
analogically before engaging in brute-force pattern search, thus efficiently
reducing problem complexity and build a more concise and human-understandable
solution. We empirically validate that guiding LLM with analogic approach with
GIFARC affects task-solving approaches of LLMs to align with analogic approach
of human.

</details>


### [17] [Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.20728)
*Zesen Lyu,Dandan Zhang,Wei Ye,Fangdi Li,Zhihang Jiang,Yao Yang*

Main category: cs.AI

TL;DR: 当前视觉-语言模型在空间推理能力上与人类存在显著差距，新的基准测试Jigsaw-Puzzles揭示了这一问题，并为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 探索现有的视觉-语言模型是否具备类似于人类的空间推理能力，这是复杂推理和决策的基础。

Method: 引入了一个名为Jigsaw-Puzzles的新基准测试，包含1,100张高空间复杂度的现实世界图像，并设计了五个任务来评估视觉-语言模型的空间感知、结构理解和推理能力，同时尽量减少对领域特定知识的依赖。

Result: 即使是最强的模型Gemini-2.5-Pro，在整体准确率上也只有77.14%，特别是在顺序生成任务上的表现较差，准确率仅为30.00%，远低于人类参与者超过90%的表现。

Conclusion: Jigsaw-Puzzles作为一个具有挑战性和诊断性的基准测试，能够推动视觉-语言模型在空间推理研究方面的进步。

Abstract: Spatial reasoning is a core component of human cognition, enabling
individuals to perceive, comprehend, and interact with the physical world. It
relies on a nuanced understanding of spatial structures and inter-object
relationships, serving as the foundation for complex reasoning and
decision-making. To investigate whether current vision-language models (VLMs)
exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark
consisting of 1,100 carefully curated real-world images with high spatial
complexity. Based on this dataset, we design five tasks to rigorously evaluate
VLMs' spatial perception, structural understanding, and reasoning capabilities,
while deliberately minimizing reliance on domain-specific knowledge to better
isolate and assess the general spatial reasoning capability. We conduct a
comprehensive evaluation across 24 state-of-the-art VLMs. The results show that
even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy
and performs particularly poorly on the Order Generation task, with only 30.00%
accuracy, far below the performance exceeding 90% achieved by human
participants. This persistent gap underscores the need for continued progress,
positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for
advancing spatial reasoning research in VLMs.

</details>


### [18] [E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing](https://arxiv.org/abs/2505.20733)
*Cheonsu Jeong,Seongmin Sim,Hyoyoung Cho,Sungsu Kim,Byounggwan Shin*

Main category: cs.AI

TL;DR: This paper presents an intelligent work automation approach integrating generative AI and IDP technologies with an Automation Agent for E2E automation of corporate financial expense processing tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional RPA has limitations in handling unstructured data, exception management, and complex decision-making.

Method: Designs and implements a four-stage integrated process comprising automatic recognition via OCR/IDP, item classification based on a policy-driven database, intelligent exception handling supported by generative AI (LLMs), and human-in-the-loop final decision-making with continuous system learning through an Automation Agent.

Result: Demonstrated quantitative benefits including over 80% reduction in processing time for paper receipt expense tasks, decreased error rates, improved compliance, as well as qualitative benefits such as enhanced accuracy and consistency, increased employee satisfaction, and data-driven decision support.

Conclusion: The organic integration of generative AI, IDP, and Automation Agents effectively overcomes the limitations of conventional automation and enables E2E automation of complex corporate processes.

Abstract: This paper presents an intelligent work automation approach in the context of
contemporary digital transformation by integrating generative AI and
Intelligent Document Processing (IDP) technologies with an Automation Agent to
realize End-to-End (E2E) automation of corporate financial expense processing
tasks. While traditional Robotic Process Automation (RPA) has proven effective
for repetitive, rule-based simple task automation, it faces limitations in
handling unstructured data, exception management, and complex decision-making.
This study designs and implements a four-stage integrated process comprising
automatic recognition of supporting documents such as receipts via OCR/IDP,
item classification based on a policy-driven database, intelligent exception
handling supported by generative AI (large language models, LLMs), and
human-in-the-loop final decision-making with continuous system learning through
an Automation Agent. Applied to a major Korean enterprise (Company S), the
system demonstrated quantitative benefits including over 80% reduction in
processing time for paper receipt expense tasks, decreased error rates, and
improved compliance, as well as qualitative benefits such as enhanced accuracy
and consistency, increased employee satisfaction, and data-driven decision
support. Furthermore, the system embodies a virtuous cycle by learning from
human judgments to progressively improve automatic exception handling
capabilities. Empirically, this research confirms that the organic integration
of generative AI, IDP, and Automation Agents effectively overcomes the
limitations of conventional automation and enables E2E automation of complex
corporate processes. The study also discusses potential extensions to other
domains such as accounting, human resources, and procurement, and proposes
future directions for AI-driven hyper-automation development.

</details>


### [19] [RRO: LLM Agent Optimization Through Rising Reward Trajectories](https://arxiv.org/abs/2505.20737)
*Zilong Wang,Jingfeng Yang,Sreyashi Nag,Samarth Varshney,Xianfeng Tang,Haoming Jiang,Jingbo Shang,Sheikh Muhammad Sarwar*

Main category: cs.AI

TL;DR: 大型语言模型在多步任务中表现不佳，现有方法通过强化学习校准推理过程，但计算成本高。本文提出Reward Rising Optimization (RRO) 方法，通过关注奖励递增趋势减少探索成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为代理解决复杂多步任务时容易因规划轨迹中的细微错误而失败。现有的Process Reward Models（PRMs）方法虽然可以通过每一步的监督来校准推理过程，但由于需要大量计算以获取训练数据，难以扩展到大规模行动候选集上。

Method: 提出了一种名为Reward Rising Optimization（RRO）的方法，专注于连续推理步骤之间的相对奖励趋势。具体来说，在收集的轨迹中保持递增奖励来进行过程监督，逐步增强过程监督直到找到相对于前一次迭代具有正奖励差值（即递增奖励）的步骤。该方法动态扩展了下一动作候选的搜索空间，从而高效捕获高质量数据。

Result: 在WebShop和InterCode-SQL基准测试中，实验结果表明所提出的RRO方法不仅实现了优越的性能，而且所需的探索成本显著降低。

Conclusion: Reward Rising Optimization（RRO）方法通过关注推理步骤间的递增奖励趋势，能够有效减少探索成本并提高复杂多步任务的性能，是一种高效且可扩展的过程监督方法。

Abstract: Large language models (LLMs) have exhibited extraordinary performance in a
variety of tasks while it remains challenging for them to solve complex
multi-step tasks as agents. In practice, agents sensitive to the outcome of
certain key steps which makes them likely to fail the task because of a subtle
mistake in the planning trajectory. Recent approaches resort to calibrating the
reasoning process through reinforcement learning. They reward or penalize every
reasoning step with process supervision, as known as Process Reward Models
(PRMs). However, PRMs are difficult and costly to scale up with a large number
of next action candidates since they require extensive computations to acquire
the training data through the per-step trajectory exploration. To mitigate this
issue, we focus on the relative reward trend across successive reasoning steps
and propose maintaining an increasing reward in the collected trajectories for
process supervision, which we term Reward Rising Optimization (RRO).
Specifically, we incrementally augment the process supervision until
identifying a step exhibiting positive reward differentials, i.e. rising
rewards, relative to its preceding iteration. This method dynamically expands
the search space for the next action candidates, efficiently capturing
high-quality data. We provide mathematical groundings and empirical results on
the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO
achieves superior performance while requiring much less exploration cost.

</details>


### [20] [MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science](https://arxiv.org/abs/2505.20740)
*Xiangyu Zhao,Wanghan Xu,Bo Liu,Yuhao Zhou,Fenghua Ling,Ben Fei,Xiaoyu Yue,Lei Bai,Wenlong Zhang,Xiao-Ming Wu*

Main category: cs.AI

TL;DR: 为了填补多模态大语言模型在地球科学领域应用的空白，特别是研究生水平的应用，本文提出了MSEarth，一个从高质量开放获取科学出版物中策划的多模态科学基准。MSEarth包含超过7000个图表和精炼的标题，涵盖地球科学的五个主要领域，并支持多种任务，如科学图表说明、多项选择题和开放式推理挑战。该基准公开可用，旨在促进进一步的研究和创新。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型取得了快速进展，但它们在解决地球科学问题中的应用，特别是在研究生水平上的应用，仍然未被充分开发。当前的基准测试往往依赖于合成数据集或过于简单的图表-标题对，无法反映复杂的地理科学推理和领域特定的知识。

Method: MSEarth是一个从高质量开放获取科学出版物中策划的多模态科学基准，涵盖了地球科学的五个主要领域：大气圈、冰冻圈、水圈、岩石圈和生物圈。它包括超过7000个图表和经过优化的标题，这些标题基于原始图表标题并结合了论文中的讨论和推理。

Result: MSEarth为高级科学任务提供了高保真度和可扩展的资源，支持科学图表说明、多项选择题和开放式推理挑战等多种任务。这有助于增强多模态大语言模型在科学推理中的发展和评估。

Conclusion: MSEarth的引入填补了研究生水平基准测试的空白，提供了一个公开可用的资源，以促进多模态大语言模型在科学推理方面的进一步研究和创新。

Abstract: The rapid advancement of multimodal large language models (MLLMs) has
unlocked new opportunities to tackle complex scientific challenges. Despite
this progress, their application in addressing earth science problems,
especially at the graduate level, remains underexplored. A significant barrier
is the absence of benchmarks that capture the depth and contextual complexity
of geoscientific reasoning. Current benchmarks often rely on synthetic datasets
or simplistic figure-caption pairs, which do not adequately reflect the
intricate reasoning and domain-specific insights required for real-world
scientific applications. To address these gaps, we introduce MSEarth, a
multimodal scientific benchmark curated from high-quality, open-access
scientific publications. MSEarth encompasses the five major spheres of Earth
science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere,
featuring over 7K figures with refined captions. These captions are crafted
from the original figure captions and enriched with discussions and reasoning
from the papers, ensuring the benchmark captures the nuanced reasoning and
knowledge-intensive content essential for advanced scientific tasks. MSEarth
supports a variety of tasks, including scientific figure captioning, multiple
choice questions, and open-ended reasoning challenges. By bridging the gap in
graduate-level benchmarks, MSEarth provides a scalable and high-fidelity
resource to enhance the development and evaluation of MLLMs in scientific
reasoning. The benchmark is publicly available to foster further research and
innovation in this field. Resources related to this benchmark can be found at
https://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.

</details>


### [21] [Can Agents Fix Agent Issues?](https://arxiv.org/abs/2505.20749)
*Alfin Wijaya Rahardja,Junwei Liu,Weitong Chen,Zhenpeng Chen,Yiling Lou*

Main category: cs.AI

TL;DR: LLM-based agent systems are widely used but hard to maintain. Current SE agents' effectiveness on resolving agent issues is limited, indicating the need for advanced SE agents.


<details>
  <summary>Details</summary>
Motivation: To understand how effectively current SE agents can resolve real-world issues in LLM-based agent systems and identify unique challenges in maintaining these systems compared to traditional software.

Method: Manually analyze 201 real-world agent issues, construct AGENTISSUE-BENCH (a benchmark with 50 agent issue resolution tasks), and evaluate state-of-the-art SE agents on this benchmark.

Result: State-of-the-art SE agents have only 3.33% - 12.67% resolution rates on AGENTISSUE-BENCH, showing their limited effectiveness in resolving agent issues.

Conclusion: Maintaining agent systems poses unique challenges compared to traditional software, necessitating further research into advanced SE agents tailored for resolving agent issues.

Abstract: LLM-based agent systems are emerging as a new software paradigm and have been
widely adopted across diverse domains such as medicine, robotics, and
programming. However, maintaining these systems requires substantial effort, as
they are inevitably prone to bugs and continually evolve to meet changing
external requirements. Therefore, automatically resolving agent issues (i.e.,
bug reports or feature requests) is a crucial and challenging task. While
recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in
addressing issues in traditional software systems, it remains unclear how
effectively they can resolve real-world issues in agent systems, which differ
significantly from traditional software. To fill this gap, we first manually
analyze 201 real-world agent issues and identify common categories of agent
issues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a
reproducible benchmark comprising 50 agent issue resolution tasks (each with an
executable environment and failure-triggering tests). We further evaluate
state-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited
effectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results
underscore the unique challenges of maintaining agent systems compared to
traditional software, highlighting the need for further research to develop
advanced SE agents for resolving agent issues. Data and code are available at
https://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .

</details>


### [22] [MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization](https://arxiv.org/abs/2505.20820)
*Hyomin Kim,Yunhui Jang,Sungsoo Ahn*

Main category: cs.AI

TL;DR: MT-Mol is a multi-agent framework for molecular optimization using tool-guided reasoning and role-specialized LLM agents, achieving state-of-the-art performance on the PMO-1K benchmark.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of large language models in molecular optimization, especially in structured reasoning, interpretability, and comprehensive tool-grounded molecular optimization.

Method: The MT-Mol framework incorporates comprehensive RDKit tools categorized into five domains. Each domain is managed by an expert analyst agent responsible for extracting task-relevant tools. The framework also includes a molecule-generating scientist, a reasoning-output verifier, and a reviewer agent. Through their interaction, MT-Mol produces molecules with tool-aligned and stepwise reasoning.

Result: MT-Mol demonstrates state-of-the-art performance on 17 out of 23 tasks in the PMO-1K benchmark.

Conclusion: MT-Mol successfully leverages tool-guided reasoning and role-specialized LLM agents to achieve superior results in molecular optimization.

Abstract: Large language models (LLMs) have large potential for molecular optimization,
as they can gather external chemistry tools and enable collaborative
interactions to iteratively refine molecular candidates. However, this
potential remains underexplored, particularly in the context of structured
reasoning, interpretability, and comprehensive tool-grounded molecular
optimization. To address this gap, we introduce MT-Mol, a multi-agent framework
for molecular optimization that leverages tool-guided reasoning and
role-specialized LLM agents. Our system incorporates comprehensive RDKit tools,
categorized into five distinct domains: structural descriptors, electronic and
topological features, fragment-based functional groups, molecular
representations, and miscellaneous chemical properties. Each category is
managed by an expert analyst agent, responsible for extracting task-relevant
tools and enabling interpretable, chemically grounded feedback. MT-Mol produces
molecules with tool-aligned and stepwise reasoning through the interaction
between the analyst agents, a molecule-generating scientist, a reasoning-output
verifier, and a reviewer agent. As a result, we show that our framework shows
the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.

</details>


### [23] [Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving](https://arxiv.org/abs/2505.20869)
*Kuo Zhou,Lu Zhang*

Main category: cs.AI

TL;DR: 提出MATH-VF框架，包括Formalizer和Critic，用于验证大型语言模型生成的数学解决方案的正确性，并在两个场景（验证和优化）中展示其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在解决数学问题方面表现出强大的能力，但它们在解决问题的过程中仍可能犯逻辑推理和计算错误。因此，需要一种方法来正式验证这些模型生成的解决方案的正确性。

Method: MATH-VF框架包含两个主要组件：Formalizer和Critic。Formalizer使用LLM将自然语言解决方案转换为形式化上下文；Critic整合了各种外部工具（如计算机代数系统和SMT解算器），评估形式化上下文中每个陈述的正确性，并在陈述不正确时提供纠正反馈。该框架在两种情况下进行经验研究：1）验证解决方案的正确性；2）当识别到LLM生成的解决方案中的错误时，提交Critic提出的纠正建议以重新生成解决方案。

Result: 在广泛使用的数学基准MATH500和ProcessBench上的评估表明，该方法优于现有方法。

Conclusion: MATH-VF框架通过Formalizer和Critic成功验证并优化了LLM生成的数学解决方案，展示了其在提升解决方案正确性和可靠性方面的优越性。

Abstract: Large Language Models (LLMs) have demonstrated formidable capabilities in
solving mathematical problems, yet they may still commit logical reasoning and
computational errors during the problem-solving process. Thus, this paper
proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for
formally verifying the correctness of the solutions generated by large language
models. Our framework first utilizes a Formalizer which employs an LLM to
translate a natural language solution into a formal context. Afterward, our
Critic (which integrates various external tools such as a Computer Algebra
System and an SMT solver) evaluates the correctness of each statement within
the formal context, and when a statement is incorrect, our Critic provides
corrective feedback. We empirically investigate the effectiveness of MATH-VF in
two scenarios: 1) Verification: MATH-VF is utilized to determine the
correctness of a solution to a given problem. 2) Refinement: When MATH-VF
identifies errors in the solution generated by an LLM-based solution generator
for a given problem, it submits the corrective suggestions proposed by the
Critic to the solution generator to regenerate the solution. We evaluate our
framework on widely used mathematical benchmarks: MATH500 and ProcessBench,
demonstrating the superiority of our approach over existing approaches.

</details>


### [24] [Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment](https://arxiv.org/abs/2505.20889)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zhenliang Ma*

Main category: cs.AI

TL;DR: 本文提出了一种基于学习的框架，将静态系统最优(SO)交通分配问题重新定义为单一代理深度强化学习(RL)任务，通过顺序推荐路线给旅行者以减少总系统旅行时间，并在Braess和Ortuzar-Willumsen网络上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现代导航系统和共享出行平台越来越依赖个性化的路线推荐来改善个人出行体验和运营效率，但个性化路由决策是否能集体导致系统最优(SO)交通分配仍是一个关键问题。

Method: 提出了一种学习框架，将静态SO交通分配问题转化为单代理深度强化学习任务；设计了一个MSA指导的深度Q学习算法，将传统交通分配方法的迭代结构整合到RL训练过程中；通过顺序推荐路线给旅行者来最小化总系统旅行时间。

Result: 在Braess网络中，RL代理收敛到理论上的SO解；在Ortuzar-Willumsen网络中，仅有0.35%的偏差；消融研究显示SO信息路由集显著提高了学习速度和最终性能。

Conclusion: 该工作提供了一种理论依据充分且实践相关的途径，通过基于学习的顺序分配连接个体路由行为与系统级效率。

Abstract: Modern navigation systems and shared mobility platforms increasingly rely on
personalized route recommendations to improve individual travel experience and
operational efficiency. However, a key question remains: can such sequential,
personalized routing decisions collectively lead to system-optimal (SO) traffic
assignment? This paper addresses this question by proposing a learning-based
framework that reformulates the static SO traffic assignment problem as a
single-agent deep reinforcement learning (RL) task. A central agent
sequentially recommends routes to travelers as origin-destination (OD) demands
arrive, to minimize total system travel time. To enhance learning efficiency
and solution quality, we develop an MSA-guided deep Q-learning algorithm that
integrates the iterative structure of traditional traffic assignment methods
into the RL training process. The proposed approach is evaluated on both the
Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent
converges to the theoretical SO solution in the Braess network and achieves
only a 0.35% deviation in the OW network. Further ablation studies demonstrate
that the route action set's design significantly impacts convergence speed and
final performance, with SO-informed route sets leading to faster learning and
better outcomes. This work provides a theoretically grounded and practically
relevant approach to bridging individual routing behavior with system-level
efficiency through learning-based sequential assignment.

</details>


### [25] [Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs](https://arxiv.org/abs/2505.20948)
*Yisen Gao,Jiaxin Bai,Tianshi Zheng,Qingyun Sun,Ziwei Zhang,Jianxin Li,Yangqiu Song,Xingcheng Fu*

Main category: cs.AI

TL;DR: 本文提出了一种可控的假设生成框架CtrlHGen，用于知识图谱中的溯因推理。通过两阶段训练方法（监督学习和强化学习），解决了生成长而复杂的逻辑假设时面临的假设空间坍缩和假设过度敏感问题。实验表明，该模型在遵循控制条件和语义相似性方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的溯因推理方法在大规模知识图谱中生成的假设往往冗余或无关，缺乏可控性，限制了其实际应用价值。

Method: 提出CtrlHGen框架，包含两个关键策略：1) 基于子逻辑分解的数据集增强策略以缓解假设空间坍缩；2) 引入平滑语义奖励（如Dice和Overlap分数）及条件一致性奖励来解决假设过度敏感问题。采用两阶段训练范式（监督学习+强化学习）。

Result: 在三个基准数据集上的广泛实验证明，CtrlHGen不仅更好地遵循控制条件，还在语义相似性性能上优于基线模型。

Conclusion: CtrlHGen提高了溯因推理的可控性和实用性，为生成更复杂、更符合用户需求的假设提供了有效解决方案。

Abstract: Abductive reasoning in knowledge graphs aims to generate plausible logical
hypotheses from observed entities, with broad applications in areas such as
clinical diagnosis and scientific discovery. However, due to a lack of
controllability, a single observation may yield numerous plausible but
redundant or irrelevant hypotheses on large-scale knowledge graphs. To address
this limitation, we introduce the task of controllable hypothesis generation to
improve the practical utility of abductive reasoning. This task faces two key
challenges when controlling for generating long and complex logical hypotheses:
hypothesis space collapse and hypothesis oversensitivity. To address these
challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation
framework for abductive reasoning over knowledge graphs, trained in a two-stage
paradigm including supervised learning and subsequent reinforcement learning.
To mitigate hypothesis space collapse, we design a dataset augmentation
strategy based on sub-logical decomposition, enabling the model to learn
complex logical structures by leveraging semantic patterns in simpler
components. To address hypothesis oversensitivity, we incorporate smoothed
semantic rewards including Dice and Overlap scores, and introduce a
condition-adherence reward to guide the generation toward user-specified
control constraints. Extensive experiments on three benchmark datasets
demonstrate that our model not only better adheres to control conditions but
also achieves superior semantic similarity performance compared to baselines.

</details>


### [26] [Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking](https://arxiv.org/abs/2505.21045)
*Lingyi Cai,Ruichen Zhang,Changyuan Zhao,Yu Zhang,Jiawen Kang,Dusit Niyato,Tao Jiang,Xuemin Shen*

Main category: cs.AI

TL;DR: LAENet, which operates below 1,000 meters, faces challenges like complex decision-making and resource constraints. Reinforcement learning (RL) can help but has limitations. This paper explores integrating large language models (LLMs) into RL to address these issues, proposing an LLM-enhanced RL framework for LAENet.


<details>
  <summary>Details</summary>
Motivation: LAENet aims to support various flying applications under 1,000 meters using aerial vehicles for cost-effective networking. However, it encounters significant challenges such as complex decision-making, resource constraints, and environmental uncertainty.

Method: The paper presents a tutorial on integrating LLMs into RL by leveraging their generation, contextual understanding, and structured reasoning capabilities. An LLM-enhanced RL framework is proposed where LLMs serve as information processors, reward designers, decision-makers, and generators within the LAENet context.

Result: A case study demonstrates the use of LLMs in designing a reward function that improves the learning performance of RL in LAENet.

Conclusion: The paper concludes with a summary and discussion on future work regarding the integration of LLMs and RL for LAENet.

Abstract: Low-Altitude Economic Networking (LAENet) aims to support diverse flying
applications below 1,000 meters by deploying various aerial vehicles for
flexible and cost-effective aerial networking. However, complex
decision-making, resource constraints, and environmental uncertainty pose
significant challenges to the development of the LAENet. Reinforcement learning
(RL) offers a potential solution in response to these challenges but has
limitations in generalization, reward design, and model stability. The
emergence of large language models (LLMs) offers new opportunities for RL to
mitigate these limitations. In this paper, we first present a tutorial about
integrating LLMs into RL by using the capacities of generation, contextual
understanding, and structured reasoning of LLMs. We then propose an
LLM-enhanced RL framework for the LAENet in terms of serving the LLM as
information processor, reward designer, decision-maker, and generator.
Moreover, we conduct a case study by using LLMs to design a reward function to
improve the learning performance of RL in the LAENet. Finally, we provide a
conclusion and discuss future work.

</details>


### [27] [Agent-Environment Alignment via Automated Interface Generation](https://arxiv.org/abs/2505.21055)
*Kaiming Liu,Xuanyu Lei,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）代理在交互式决策任务中表现出令人印象深刻的推理能力。然而，代理与环境之间的不匹配（即代理-环境错位）是一个显著的瓶颈。本文提出了ALIGN框架，通过丰富接口信息来缓解这一问题，从而提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的研究主要集中在改进代理策略和环境设计上，但代理与环境之间接口的关键作用仍未得到充分探索。因此，研究者希望解决代理-环境错位问题以提升代理性能。

Method: 提出了一种名为ALIGN的自动对齐接口生成框架，该框架通过增强环境的静态信息和逐步观察结果来丰富接口。此接口作为轻量级包装器实现，无需修改代理逻辑或环境代码即可实现对齐。

Result: 实验结果表明，在多个领域（包括实体任务、网络导航和工具使用）中，性能得到了一致的提升，其中在ALFWorld中的成功率提高了45.67%。此外，ALIGN生成的接口可以在不同的代理架构和LLM主干之间泛化，而无需重新生成接口。

Conclusion: ALIGN框架能够有效缓解代理-环境错位问题，提升代理在各种任务中的性能，并且具有良好的泛化能力。

Abstract: Large language model (LLM) agents have shown impressive reasoning
capabilities in interactive decision-making tasks. These agents interact with
environment through intermediate interfaces, such as predefined action spaces
and interaction rules, which mediate the perception and action. However,
mismatches often happen between the internal expectations of the agent
regarding the influence of its issued actions and the actual state transitions
in the environment, a phenomenon referred to as \textbf{agent-environment
misalignment}. While prior work has invested substantially in improving agent
strategies and environment design, the critical role of the interface still
remains underexplored. In this work, we empirically demonstrate that
agent-environment misalignment poses a significant bottleneck to agent
performance. To mitigate this issue, we propose \textbf{ALIGN}, an
\underline{A}uto-A\underline{l}igned \underline{I}nterface
\underline{G}e\underline{n}eration framework that alleviates the misalignment
by enriching the interface. Specifically, the ALIGN-generated interface
enhances both the static information of the environment and the step-wise
observations returned to the agent. Implemented as a lightweight wrapper, this
interface achieves the alignment without modifying either the agent logic or
the environment code. Experiments across multiple domains including embodied
tasks, web navigation and tool-use, show consistent performance improvements,
with up to a 45.67\% success rate improvement observed in ALFWorld. Meanwhile,
ALIGN-generated interface can generalize across different agent architectures
and LLM backbones without interface regeneration. Code and experimental results
are available at https://github.com/THUNLP-MT/ALIGN.

</details>


### [28] [Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning](https://arxiv.org/abs/2505.21067)
*Xiao Hu,Xingyu Lu,Liyuan Mao,YiFan Zhang,Tianke Zhang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: 尽管强化学习（RL）在提升大型语言模型（LLMs）推理能力方面取得了重要进展，但本文研究表明，基于基础模型的简单蒸馏方法仅使用920个样例即可明显优于零样本强化学习（zero-RL），后者通常需要更多数据和计算成本。蒸馏模型展现出更灵活的推理能力，频繁表现出多视角思考和元认知意识两种高级认知行为，而zero-RL未能显著提升这些行为频率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估强化学习（RL）与蒸馏方法在提升较小基础模型推理能力方面的效果对比，特别是在数据和计算资源有限的情况下。

Method: 通过分析模型输出中的标记频率，比较蒸馏模型和zero-RL模型的表现。发现蒸馏模型更频繁地使用拟人化标记和逻辑连接词，展现出更灵活的推理能力，并增强了多视角思考和元认知意识两种高级认知行为。

Result: 蒸馏方法仅使用920个样例即可显著优于zero-RL，后者需要更多的数据和计算成本。蒸馏模型表现出更灵活的推理能力，以及更高的多视角思考和元认知意识频率。

Conclusion: 蒸馏方法在提升较小基础模型推理能力方面表现优异，尤其是在数据和计算资源有限的情况下，其效果优于zero-RL。灵活推理的关键在于高级认知行为的频繁出现。

Abstract: Reinforcement learning (RL) has played an important role in improving the
reasoning ability of large language models (LLMs). Some studies apply RL
directly to \textit{smaller} base models (known as zero-RL) and also achieve
notable progress. However, in this paper, we show that using only 920 examples,
a simple distillation method based on the base model can clearly outperform
zero-RL, which typically requires much more data and computational cost. By
analyzing the token frequency in model outputs, we find that the distilled
model shows more flexible reasoning. It uses anthropomorphic tokens and logical
connectors much more often than the zero-RL model. Further analysis reveals
that distillation enhances the presence of two advanced cognitive behaviors:
Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent
occurrences of these two advanced cognitive behaviors give rise to flexible
reasoning, which is essential for solving complex reasoning problems, while
zero-RL fails to significantly boost the frequency of these behaviors.

</details>


### [29] [Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation](https://arxiv.org/abs/2505.21106)
*Zhengyang Ji,Yifan Jia,Shang Gao,Yutao Yue*

Main category: cs.AI

TL;DR: Large Vision Language Models (LVLMs) have significant social biases despite their success in multimodal tasks. This paper proposes an explanatory framework combining information flow analysis and multi-round dialogue evaluation to explore the origin of these biases from the perspective of imbalanced internal information utilization.


<details>
  <summary>Details</summary>
Motivation: Despite existing studies focusing on detecting and quantifying social biases in LVLMs, there is limited understanding of the underlying mechanisms within the models that cause these biases.

Method: The proposed framework first identifies high-contribution image tokens involved in the model's reasoning process for neutral questions via information flow analysis. Then, a multi-turn dialogue mechanism evaluates the extent to which these key tokens encode sensitive information.

Result: Experiments reveal systematic disparities in how LVLMs utilize information when processing images of different demographic groups, indicating that social bias is rooted in the model's internal reasoning dynamics. Additionally, biased proximity patterns are found in the model's semantic representations from a textual modality perspective.

Conclusion: Social bias in LVLMs originates from imbalanced internal information utilization, and this bias is deeply embedded in the model's reasoning processes across both visual and textual modalities.

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress in
multimodal tasks, yet they also exhibit notable social biases. These biases
often manifest as unintended associations between neutral concepts and
sensitive human attributes, leading to disparate model behaviors across
demographic groups. While existing studies primarily focus on detecting and
quantifying such biases, they offer limited insight into the underlying
mechanisms within the models. To address this gap, we propose an explanatory
framework that combines information flow analysis with multi-round dialogue
evaluation, aiming to understand the origin of social bias from the perspective
of imbalanced internal information utilization. Specifically, we first identify
high-contribution image tokens involved in the model's reasoning process for
neutral questions via information flow analysis. Then, we design a multi-turn
dialogue mechanism to evaluate the extent to which these key tokens encode
sensitive information. Extensive experiments reveal that LVLMs exhibit
systematic disparities in information usage when processing images of different
demographic groups, suggesting that social bias is deeply rooted in the model's
internal reasoning dynamics. Furthermore, we complement our findings from a
textual modality perspective, showing that the model's semantic representations
already display biased proximity patterns, thereby offering a cross-modal
explanation of bias formation.

</details>


### [30] [Interpretable DNFs](https://arxiv.org/abs/2505.21212)
*Martin C. Cooper,Imane Bousdira,Clément Carbonnel*

Main category: cs.AI

TL;DR: The paper explores the interpretability of DNF formulas and compares two families of models, depth-k decision trees and nested k-DNFs.


<details>
  <summary>Details</summary>
Motivation: To define and enhance the interpretability of binary classifiers represented by DNF formulas, where both positive and negative decisions must be easily explained to human users.

Method: Study the family of k-DNFs whose complements can also be expressed as k-DNFs and compare the interpretability and accuracy of depth-k decision trees and a novel model called nested k-DNFs.

Result: Experiments show that nested k-DNFs provide an interesting alternative to decision trees in terms of interpretability and accuracy.

Conclusion: Nested k-DNFs are a promising new family of models for improving the interpretability of binary classifiers without sacrificing accuracy.

Abstract: A classifier is considered interpretable if each of its decisions has an
explanation which is small enough to be easily understood by a human user. A
DNF formula can be seen as a binary classifier $\kappa$ over boolean domains.
The size of an explanation of a positive decision taken by a DNF $\kappa$ is
bounded by the size of the terms in $\kappa$, since we can explain a positive
decision by giving a term of $\kappa$ that evaluates to true. Since both
positive and negative decisions must be explained, we consider that
interpretable DNFs are those $\kappa$ for which both $\kappa$ and
$\overline{\kappa}$ can be expressed as DNFs composed of terms of bounded size.
In this paper, we study the family of $k$-DNFs whose complements can also be
expressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision
trees and nested $k$-DNFs, a novel family of models. Experiments indicate that
nested $k$-DNFs are an interesting alternative to decision trees in terms of
interpretability and accuracy.

</details>


### [31] [XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration](https://arxiv.org/abs/2505.21279)
*Shaoqing Zhang,Kehai Chen,Zhuosheng Zhang,Rumei Li,Rongxiang Weng,Yang Xiang,Liqiang Nie,Min Zhang*

Main category: cs.AI

TL;DR: The paper proposes XBOUND, a new evaluation method for Device-Control Agents (DC agents) using an Explore Metric to assess their capabilities in individual states. It also introduces a pseudo episode tree dataset and evaluates OS-Atlas and UI-TARS series.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating DC agents only provide macroscopic views of performance without offering microscopic insights into potential real-world errors.

Method: XBOUND employs the calculation of a novel Explore Metric to delineate the capability boundaries of DC agents focusing on individual states rather than overall task success rates.

Result: XBOUND comprehensively evaluates the OS-Atlas and UI-TARS series across five common tasks, revealing both overall and specific performances as well as highlighting current deficiencies and limitations.

Conclusion: XBOUND provides a finer-grained performance evaluation of DC agents compared to previous methods and identifies areas for improvement in existing DC agent series.

Abstract: Recent advancements in vision-language models (VLMs) have spurred increased
interest in Device-Control Agents (DC agents), such as utilizing in-the-wild
device control to manage graphical user interfaces. Conventional methods for
assessing the capabilities of DC agents, such as computing step-wise action
accuracy and overall task success rates, provide a macroscopic view of DC
agents' performance; however, they fail to offer microscopic insights into
potential errors that may occur in real-world applications. Conducting a
finer-grained performance evaluation of DC agents presents significant
challenges. This study introduces a new perspective on evaluation methods for
DC agents by proposing the XBOUND evaluation method, which employs the
calculation of a novel Explore Metric to delineate the capability boundaries of
DC agents. Compared to previous evaluation methods, XBOUND focuses on
individual states to assess the proficiency of DC agents in mastering these
states. Furthermore, we have developed a ``pseudo'' episode tree dataset
derived from Android Control test data. Utilizing this dataset and XBOUND, we
comprehensively evaluate the OS-Atlas and UI-TARS series, examining both the
overall and specific performance across five common tasks. Additionally, we
select representative cases to highlight the current deficiencies and
limitations inherent in both series. Code is available at
https://github.com/sqzhang-lazy/XBOUND.

</details>


### [32] [RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models](https://arxiv.org/abs/2505.21281)
*Yue Zhang,Zhiliang Tian,Shicheng Zhou,Haiyang Wang,Wenqing Hou,Yuying Liu,Xuechen Zhao,Minlie Huang,Ye Wang,Bin Zhou*

Main category: cs.AI

TL;DR: This paper proposes a rule-enhanced legal judgment prediction framework using first-order logic (FOL) formalism and comparative learning (CL) to adaptively adjust legal judgment logic and improve performance in Legal Judgment Prediction (LJP). It uses a three-stage approach inspired by human exam preparation.


<details>
  <summary>Details</summary>
Motivation: Existing LJP models either neglect legal reasoning logic or have rigid logic that hinders adaptation to complex, case-specific logical frameworks.

Method: The method involves a three-stage process: 1) initializing judgment rules with FOL formalism, 2) proposing Confusion-aware Contrastive Learning (CACL) to optimize these rules through confusable cases, and 3) using the optimized rules for predicting legal judgments.

Result: Experimental results on two public datasets demonstrate superior performance across all metrics.

Conclusion: The proposed framework shows improved performance in LJP tasks and is publicly available.

Abstract: Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing
semantic-enhanced LJP models integrate judicial precedents and legal knowledge
for high performance. But they neglect legal reasoning logic, a critical
component of legal judgments requiring rigorous logical analysis. Although some
approaches utilize legal reasoning logic for high-quality predictions, their
logic rigidity hinders adaptation to case-specific logical frameworks,
particularly in complex cases that are lengthy and detailed. This paper
proposes a rule-enhanced legal judgment prediction framework based on
first-order logic (FOL) formalism and comparative learning (CL) to develop an
adaptive adjustment mechanism for legal judgment logic and further enhance
performance in LJP. Inspired by the process of human exam preparation, our
method follows a three-stage approach: first, we initialize judgment rules
using the FOL formalism to capture complex reasoning logic accurately; next, we
propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize
the judgment rules through a quiz consisting of confusable cases; finally, we
utilize the optimized judgment rules to predict legal judgments. Experimental
results on two public datasets show superior performance across all metrics.
The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.

</details>


### [33] [Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework](https://arxiv.org/abs/2505.21291)
*Saman Marandi,Yu-Shu Hu,Mohammad Modarres*

Main category: cs.AI

TL;DR: 本论文提出了一种结合知识图谱(KGs)和大语言模型(LLMs)的新型诊断框架，用于支持如核电站等高可靠性系统的系统诊断。通过动态主逻辑(DML)模型的功能建模原则，引入了两个协调的LLM组件，生成的逻辑被编码为结构化的KG-DML，支持分层故障推理。案例研究表明，该框架在关键元素上的准确率超过90%，并在工具和参数提取上表现出一致性，适用于安全关键诊断。


<details>
  <summary>Details</summary>
Motivation: 传统的诊断建模在系统过于复杂时难以应对，而功能建模方法更具吸引力，特别是在高可靠性系统中。因此需要一种新的诊断框架来解决传统方法的局限性。

Method: 1. 基于DML模型的功能建模原则构建诊断框架。
2. 引入两个协调的LLM组件：
   - 自动从系统文档中构建DML逻辑的工作流。
   - 支持交互式诊断的LLM代理。
3. 将生成的逻辑编码为结构化知识图谱(KG-DML)，并支持层次故障推理。
4. 在交互阶段，用户提交自然语言查询，LLM代理选择适当的工具进行结构化推理或使用Graph-RAG方法生成自然解释。

Result: 案例研究显示，该框架在关键元素上的准确率超过90%，并且在工具和参数提取上表现出一致性，证明了其在安全关键诊断中的有效性。

Conclusion: 提出的结合KGs和LLMs的诊断框架能够有效支持高可靠性系统的系统诊断，具有高准确性和一致性，适用于安全关键应用。

Abstract: In this paper, we present a novel diagnostic framework that integrates
Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system
diagnostics in high-reliability systems such as nuclear power plants.
Traditional diagnostic modeling struggles when systems become too complex,
making functional modeling a more attractive approach. Our approach introduces
a diagnostic framework grounded in the functional modeling principles of the
Dynamic Master Logic (DML) model. It incorporates two coordinated LLM
components, including an LLM-based workflow for automated construction of DML
logic from system documentation and an LLM agent that facilitates interactive
diagnostics. The generated logic is encoded into a structured KG, referred to
as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or
operational data can also be incorporated to refine the model's precision and
diagnostic depth. In the interaction phase, users submit natural language
queries, which are interpreted by the LLM agent. The agent selects appropriate
tools for structured reasoning, including upward and downward propagation
across the KG-DML. Rather than embedding KG content into every prompt, the LLM
agent distinguishes between diagnostic and interpretive tasks. For diagnostics,
the agent selects and executes external tools that perform structured KG
reasoning. For general queries, a Graph-based Retrieval-Augmented Generation
(Graph-RAG) approach is used, retrieving relevant KG segments and embedding
them into the prompt to generate natural explanations. A case study on an
auxiliary feedwater system demonstrated the framework's effectiveness, with
over 90% accuracy in key elements and consistent tool and argument extraction,
supporting its use in safety-critical diagnostics.

</details>


### [34] [Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations](https://arxiv.org/abs/2505.21318)
*Hao Li,He Cao,Bin Feng,Yanjun Shao,Xiangru Tang,Zhiyuan Yan,Li Yuan,Yonghong Tian,Yu Li*

Main category: cs.AI

TL;DR: In this paper, the authors present ChemCoTBench, a new framework for evaluating large language models in systematic reasoning tasks within the field of chemistry. Unlike previous benchmarks that focus on simple knowledge retrieval, ChemCoTBench formalizes chemical problem-solving into step-by-step workflows and treats molecular transformations as modular operations. The framework is tested on two high-impact tasks: Molecular Property Optimization and Chemical Reaction Prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to tap the potential of large language models (LLMs) with Chain-of-Thought (CoT) reasoning in the domain of chemistry. Chemistry requires rigorous structural analysis for real-world tasks like drug design and reaction engineering, which involves complex step-by-step reasoning not addressed by current benchmarks.

Method: The authors introduce ChemCoTBench, a reasoning framework that bridges molecular structure understanding with arithmetic-inspired operations such as addition, deletion, and substitution. This allows chemical problem-solving to be formalized into transparent, step-by-step workflows. Molecular transformations are treated as modular 'chemical operations' enabling slow-thinking reasoning grounded in real-world chemical constraints.

Result: ChemCoTBench is evaluated on two high-impact tasks: Molecular Property Optimization and Chemical Reaction Prediction. These tasks reflect real-world challenges while providing structured evaluability. Annotated datasets, a reasoning taxonomy, and baseline evaluations are provided to bridge the gap between abstract reasoning methods and practical chemical discovery.

Conclusion: ChemCoTBench establishes a foundation for advancing LLMs as tools for AI-driven scientific innovation in the field of chemistry. It provides a bridge between abstract reasoning methods and practical chemical discovery.

Abstract: While large language models (LLMs) with Chain-of-Thought (CoT) reasoning
excel in mathematics and coding, their potential for systematic reasoning in
chemistry, a domain demanding rigorous structural analysis for real-world tasks
like drug design and reaction engineering, remains untapped. Current benchmarks
focus on simple knowledge retrieval, neglecting step-by-step reasoning required
for complex tasks such as molecular optimization and reaction prediction. To
address this, we introduce ChemCoTBench, a reasoning framework that bridges
molecular structure understanding with arithmetic-inspired operations,
including addition, deletion, and substitution, to formalize chemical
problem-solving into transparent, step-by-step workflows. By treating molecular
transformations as modular "chemical operations", the framework enables
slow-thinking reasoning, mirroring the logic of mathematical proofs while
grounding solutions in real-world chemical constraints. We evaluate models on
two high-impact tasks: Molecular Property Optimization and Chemical Reaction
Prediction. These tasks mirror real-world challenges while providing structured
evaluability. By providing annotated datasets, a reasoning taxonomy, and
baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning
methods and practical chemical discovery, establishing a foundation for
advancing LLMs as tools for AI-driven scientific innovation.

</details>


### [35] [Assured Autonomy with Neuro-Symbolic Perception](https://arxiv.org/abs/2505.21322)
*R. Spencer Hallyburton,Miroslav Pajic*

Main category: cs.AI

TL;DR: Many AI models in cyber-physical systems are just pattern-matchers with limited security guarantees. To enhance assured AI, the authors propose a neuro-symbolic paradigm for perception (NeuSPaPer) that combines data-driven models with symbolic structure. This paradigm leverages foundation models and specialized SGG algorithms to create structured relational graphs, bridging low-level sensor perception and high-level reasoning.


<details>
  <summary>Details</summary>
Motivation: Current AI models used in cyber-physical systems lack sufficient reliability and security guarantees, especially in safety-critical domains.

Method: The authors introduce NeuSPaPer, a neuro-symbolic paradigm combining data-driven perception models with symbolic structure. It uses foundation models for offline knowledge extraction and specialized SGG algorithms for real-time deployment to generate structured relational graphs.

Result: Through physics-based simulators and real-world datasets, the authors demonstrate that SGG effectively bridges low-level sensor perception and high-level reasoning, enhancing situational awareness and establishing a foundation for resilient, context-aware AI.

Conclusion: NeuSPaPer represents a step towards advancing trusted autonomy in cyber-physical systems by integrating low-level features with high-level context through neuro-symbolic methods.

Abstract: Many state-of-the-art AI models deployed in cyber-physical systems (CPS),
while highly accurate, are simply pattern-matchers.~With limited security
guarantees, there are concerns for their reliability in safety-critical and
contested domains. To advance assured AI, we advocate for a paradigm shift that
imbues data-driven perception models with symbolic structure, inspired by a
human's ability to reason over low-level features and high-level context. We
propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how
joint object detection and scene graph generation (SGG) yields deep scene
understanding.~Powered by foundation models for offline knowledge extraction
and specialized SGG algorithms for real-time deployment, we design a framework
leveraging structured relational graphs that ensures the integrity of
situational awareness in autonomy. Using physics-based simulators and
real-world datasets, we demonstrate how SGG bridges the gap between low-level
sensor perception and high-level reasoning, establishing a foundation for
resilient, context-aware AI and advancing trusted autonomy in CPS.

</details>


### [36] [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.21327)
*Jiakang Yuan,Tianshuo Peng,Yilei Jiang,Yiting Lu,Renrui Zhang,Kaituo Feng,Chaoyou Fu,Tao Chen,Lei Bai,Bo Zhang,Xiangyu Yue*

Main category: cs.AI

TL;DR: The paper introduces MME-Reasoning, a benchmark to evaluate the reasoning ability of multimodal large language models (MLLMs) across inductive, deductive, and abductive reasoning types. It reveals significant limitations and imbalances in the reasoning abilities of state-of-the-art MLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks do not comprehensively evaluate the reasoning abilities of MLLMs due to the lack of explicit categorization for logical reasoning types and unclear understanding of reasoning.

Method: Introduced MME-Reasoning, a comprehensive benchmark covering all three types of reasoning (inductive, deductive, and abductive). Carefully curated data to ensure evaluation of reasoning ability rather than perceptual skills or knowledge breadth. Extended evaluation protocols to cover diverse questions.

Result: State-of-the-art MLLMs show substantial limitations and performance imbalances across reasoning types. Advanced MLLMs demonstrate limited performance in comprehensive logical reasoning.

Conclusion: The findings provide critical insights into the limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios.

Abstract: Logical reasoning is a fundamental aspect of human intelligence and an
essential capability for multimodal large language models (MLLMs). Despite the
significant advancement in multimodal reasoning, existing benchmarks fail to
comprehensively evaluate their reasoning abilities due to the lack of explicit
categorization for logical reasoning types and an unclear understanding of
reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive
benchmark designed to evaluate the reasoning ability of MLLMs, which covers all
three types of reasoning (i.e., inductive, deductive, and abductive) in its
questions. We carefully curate the data to ensure that each question
effectively evaluates reasoning ability rather than perceptual skills or
knowledge breadth, and extend the evaluation protocols to cover the evaluation
of diverse questions. Our evaluation reveals substantial limitations of
state-of-the-art MLLMs when subjected to holistic assessments of logical
reasoning capabilities. Even the most advanced MLLMs show limited performance
in comprehensive logical reasoning, with notable performance imbalances across
reasoning types. In addition, we conducted an in-depth analysis of approaches
such as ``thinking mode'' and Rule-based RL, which are commonly believed to
enhance reasoning abilities. These findings highlight the critical limitations
and performance imbalances of current MLLMs in diverse logical reasoning
scenarios, providing comprehensive and systematic insights into the
understanding and evaluation of reasoning capabilities.

</details>


### [37] [The Multilingual Divide and Its Impact on Global AI Safety](https://arxiv.org/abs/2505.21344)
*Aidan Peppin,Julia Kreutzer,Alice Schoenauer Sebag,Kelly Marchisio,Beyza Ermis,John Dang,Samuel Cahyawijaya,Shivalika Singh,Seraphina Goldfarb-Tarrant,Viraat Aryabumi,Aakanksha,Wei-Yin Ko,Ahmet Üstün,Matthias Gallé,Marzieh Fadaee,Sara Hooker*

Main category: cs.AI

TL;DR: 尽管近年来大型语言模型能力有所提升，但除了少数全球主流语言外，许多语言在能力和安全性上仍存在巨大差距。本文为研究者、政策制定者和治理专家提供了关于弥合AI的“语言鸿沟”及最小化跨语言安全风险的关键挑战概述。我们分析了为什么AI中的语言鸿沟存在并扩大，以及它如何造成全球AI安全的不平等。我们确定了解决这些挑战的障碍，并建议通过支持多语言数据集创建、透明度和研究来解决与语言鸿沟相关的安全问题。


<details>
  <summary>Details</summary>
Motivation: 描述当前大型语言模型在全球范围内对非主流语言支持不足的问题，强调弥合语言鸿沟和提升全球AI安全的重要性。

Method: 提供对AI语言鸿沟存在的原因及其引发的安全问题的详细分析，识别解决问题的障碍，并提出通过多语言数据集创建、透明度和研究等手段来应对安全问题的建议。

Result: 提高了对AI语言鸿沟的认识，明确了相关政策和治理措施如何能够帮助缩小这一鸿沟并降低安全风险。

Conclusion: 需要多方合作，包括研究者、政策制定者和治理专家共同努力，以解决AI语言鸿沟问题并确保全球范围内的AI安全。

Abstract: Despite advances in large language model capabilities in recent years, a
large gap remains in their capabilities and safety performance for many
languages beyond a relatively small handful of globally dominant languages.
This paper provides researchers, policymakers and governance experts with an
overview of key challenges to bridging the "language gap" in AI and minimizing
safety risks across languages. We provide an analysis of why the language gap
in AI exists and grows, and how it creates disparities in global AI safety. We
identify barriers to address these challenges, and recommend how those working
in policy and governance can help address safety concerns associated with the
language gap by supporting multilingual dataset creation, transparency, and
research.

</details>


### [38] [A Structured Unplugged Approach for Foundational AI Literacy in Primary Education](https://arxiv.org/abs/2505.21398)
*Maria Cristina Carrisi,Mirko Marras,Sara Vergallo*

Main category: cs.AI

TL;DR: Younger generations need AI literacy to navigate a world shaped by intelligent technologies. Current education often emphasizes tool-based learning, which can lead to misconceptions among non-experts. This paper proposes a structured teaching approach for primary students that builds on core mathematical elements to improve AI understanding. An empirical study with fifth-grade students showed improvements in terminology, logical reasoning, and evaluative skills.


<details>
  <summary>Details</summary>
Motivation: Younger generations are growing up in a world increasingly influenced by intelligent technologies, making early AI literacy crucial for developing the skills to critically understand and navigate them.

Method: The method involves a structured and replicable teaching approach that fosters foundational AI literacy in primary students. It builds upon core mathematical elements closely connected to and of interest in primary curricula, to strengthen conceptualization, data representation, classification reasoning, and evaluation of AI.

Result: The results indicate improvements in terminology understanding and usage, features description, logical reasoning, and evaluative skills, with students showing a deeper comprehension of decision-making processes and their limitations.

Conclusion: The proposed teaching approach proved engaging and effective in improving AI literacy among primary students.

Abstract: Younger generations are growing up in a world increasingly shaped by
intelligent technologies, making early AI literacy crucial for developing the
skills to critically understand and navigate them. However, education in this
field often emphasizes tool-based learning, prioritizing usage over
understanding the underlying concepts. This lack of knowledge leaves
non-experts, especially children, prone to misconceptions, unrealistic
expectations, and difficulties in recognizing biases and stereotypes. In this
paper, we propose a structured and replicable teaching approach that fosters
foundational AI literacy in primary students, by building upon core
mathematical elements closely connected to and of interest in primary
curricula, to strengthen conceptualization, data representation, classification
reasoning, and evaluation of AI. To assess the effectiveness of our approach,
we conducted an empirical study with thirty-one fifth-grade students across two
classes, evaluating their progress through a post-test and a satisfaction
survey. Our results indicate improvements in terminology understanding and
usage, features description, logical reasoning, and evaluative skills, with
students showing a deeper comprehension of decision-making processes and their
limitations. Moreover, the approach proved engaging, with students particularly
enjoying activities that linked AI concepts to real-world reasoning. Materials:
https://github.com/tail-unica/ai-literacy-primary-ed.

</details>


### [39] [MRSD: Multi-Resolution Skill Discovery for HRL Agents](https://arxiv.org/abs/2505.21410)
*Shashank Sharma,Janina Hoffmann,Vinay Namboodiri*

Main category: cs.AI

TL;DR: The paper introduces Multi-Resolution Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at different temporal resolutions simultaneously, outperforming prior methods in terms of convergence speed and final performance.


<details>
  <summary>Details</summary>
Motivation: Existing skill discovery methods in hierarchical reinforcement learning are limited to a single skill per task, whereas humans use both fine-grained and coarse motor skills simultaneously. The authors aim to develop a method inspired by human motor control to overcome this limitation.

Method: The proposed MRSD framework learns multiple skill encoders at different temporal resolutions in parallel. A high-level manager dynamically selects among these skills to enable adaptive control strategies over time.

Result: MRSD outperforms prior state-of-the-art skill discovery and HRL methods when evaluated on tasks from the DeepMind Control Suite, achieving faster convergence and higher final performance.

Conclusion: The findings emphasize the advantages of incorporating multi-resolution skills in HRL, leading to more versatile and efficient agents.

Abstract: Hierarchical reinforcement learning (HRL) relies on abstract skills to solve
long-horizon tasks efficiently. While existing skill discovery methods learns
these skills automatically, they are limited to a single skill per task. In
contrast, humans learn and use both fine-grained and coarse motor skills
simultaneously. Inspired by human motor control, we propose Multi-Resolution
Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at
different temporal resolutions in parallel. A high-level manager dynamically
selects among these skills, enabling adaptive control strategies over time. We
evaluate MRSD on tasks from the DeepMind Control Suite and show that it
outperforms prior state-of-the-art skill discovery and HRL methods, achieving
faster convergence and higher final performance. Our findings highlight the
benefits of integrating multi-resolution skills in HRL, paving the way for more
versatile and efficient agents.

</details>


### [40] [Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs](https://arxiv.org/abs/2505.21419)
*Yifan Wang,Kenneth P. Birman*

Main category: cs.AI

TL;DR: ARCA is a new multi-modal RAG LLM system that simplifies problem identification and resolution in cloud-hosted applications by combining AI pattern matching with a natural multi-modal RAG LLM interface, outperforming state-of-the-art alternatives.


<details>
  <summary>Details</summary>
Motivation: Cloud-hosted applications and services are complex systems with numerous potential root causes for performance or functional instabilities, necessitating advanced tools for efficient problem identification and resolution.

Method: Combining the pattern matching capabilities of modern AI tools with a natural multi-modal RAG LLM interface to create ARCA, a new multi-modal RAG LLM system.

Result: Step-wise evaluations demonstrate that ARCA outperforms state-of-the-art alternatives in the domain of cloud-hosted applications and services.

Conclusion: ARCA provides an effective solution for simplifying problem identification and resolution in complex cloud-hosted systems.

Abstract: Today's cloud-hosted applications and services are complex systems, and a
performance or functional instability can have dozens or hundreds of potential
root causes. Our hypothesis is that by combining the pattern matching
capabilities of modern AI tools with a natural multi-modal RAG LLM interface,
problem identification and resolution can be simplified. ARCA is a new
multi-modal RAG LLM system that targets this domain. Step-wise evaluations show
that ARCA outperforms state-of-the-art alternatives.

</details>


### [41] [Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks](https://arxiv.org/abs/2505.21426)
*Francesco Cozzi,Marco Pangallo,Alan Perotti,André Panisson,Corrado Monti*

Main category: cs.AI

TL;DR: This paper proposes a novel framework that learns a differentiable surrogate of any Agent-Based Model (ABM) by combining diffusion models and graph neural networks, enabling data-driven simulation and optimization.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of non-differentiable rules in ABMs which restricts the use of gradient-based methods for optimization and integration with real-world data.

Method: The method uses diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. It models individual agent behavior directly rather than approximating system-level outputs, preserving the decentralized dynamics of ABMs.

Result: Validated on Schelling's segregation model and a Predator-Prey ecosystem, the approach successfully replicates individual-level patterns and accurately forecasts emergent dynamics beyond training.

Conclusion: The framework demonstrates the potential of combining diffusion models and graph learning for data-driven ABM simulation.

Abstract: Agent-Based Models (ABMs) are powerful tools for studying emergent properties
in complex systems. In ABMs, agent behaviors are governed by local interactions
and stochastic rules. However, these rules are, in general, non-differentiable,
limiting the use of gradient-based methods for optimization, and thus
integration with real-world data. We propose a novel framework to learn a
differentiable surrogate of any ABM by observing its generated data. Our method
combines diffusion models to capture behavioral stochasticity and graph neural
networks to model agent interactions. Distinct from prior surrogate approaches,
our method introduces a fundamental shift: rather than approximating
system-level outputs, it models individual agent behavior directly, preserving
the decentralized, bottom-up dynamics that define ABMs. We validate our
approach on two ABMs (Schelling's segregation model and a Predator-Prey
ecosystem) showing that it replicates individual-level patterns and accurately
forecasts emergent dynamics beyond training. Our results demonstrate the
potential of combining diffusion models and graph learning for data-driven ABM
simulation.

</details>


### [42] [Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning](https://arxiv.org/abs/2505.21427)
*Xianling Mu,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: The paper proposes a transparent and data-efficient investment decision framework powered by memory-augmented LLMs using ICL, which predicts startup success more accurately than existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Early-stage startup investment is high-risk with scarce data and uncertain outcomes, while traditional machine learning approaches require large datasets and are difficult to interpret.

Method: A natural language policy embedded directly into the LLM prompt enables explicit reasoning patterns, and a lightweight training process combines few-shot learning with an in-context learning loop for iterative updates based on structured feedback.

Result: The system predicts startup success far more accurately than existing benchmarks, being over 20x more precise than random chance and 7.1x more precise than top-tier VC firms.

Conclusion: This method provides a promising approach for improving the accuracy and transparency of early-stage startup investment decisions.

Abstract: Early-stage startup investment is a high-risk endeavor characterized by
scarce data and uncertain outcomes. Traditional machine learning approaches
often require large, labeled datasets and extensive fine-tuning, yet remain
opaque and difficult for domain experts to interpret or improve. In this paper,
we propose a transparent and data-efficient investment decision framework
powered by memory-augmented large language models (LLMs) using in-context
learning (ICL). Central to our method is a natural language policy embedded
directly into the LLM prompt, enabling the model to apply explicit reasoning
patterns and allowing human experts to easily interpret, audit, and iteratively
refine the logic. We introduce a lightweight training process that combines
few-shot learning with an in-context learning loop, enabling the LLM to update
its decision policy iteratively based on structured feedback. With only minimal
supervision and no gradient-based optimization, our system predicts startup
success far more accurately than existing benchmarks. It is over 20x more
precise than random chance, which succeeds 1.9% of the time. It is also 7.1x
more precise than the typical 5.6% success rate of top-tier venture capital
(VC) firms.

</details>


### [43] [Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming](https://arxiv.org/abs/2505.21486)
*Yang Yang,Jiemin Wu,Yutao Yue*

Main category: cs.AI

TL;DR: A new framework combining multi-agent LLMs with ILP is proposed for automating robust hypothesis generation in open environments.


<details>
  <summary>Details</summary>
Motivation: Automating hypothesis generation in open environments is crucial for AI cognition, but traditional ILP methods rely on predefined symbolic structures and pure LLM methods are sensitive to noise.

Method: The framework integrates a multi-agent system powered by LLMs with ILP. The LLM agents automatically define symbolic vocabulary and relational templates (language bias) from raw textual data, which then guides the transformation of text into facts for an ILP solver to learn interpretable rules.

Result: Extensive experiments in diverse and challenging scenarios show superior performance compared to traditional methods, demonstrating the potential of this approach.

Conclusion: This novel framework overcomes limitations of traditional ILP and pure LLM methods, providing a new direction for automated, explainable, and verifiable hypothesis generation.

Abstract: Automating robust hypothesis generation in open environments is pivotal for
AI cognition. We introduce a novel framework integrating a multi-agent system,
powered by Large Language Models (LLMs), with Inductive Logic Programming
(ILP). Our system's LLM agents autonomously define a structured symbolic
vocabulary (predicates) and relational templates , i.e., \emph{language bias}
directly from raw textual data. This automated symbolic grounding (the
construction of the language bias), traditionally an expert-driven bottleneck
for ILP, then guides the transformation of text into facts for an ILP solver,
which inductively learns interpretable rules. This approach overcomes
traditional ILP's reliance on predefined symbolic structures and the
noise-sensitivity of pure LLM methods. Extensive experiments in diverse,
challenging scenarios validate superior performance, paving a new path for
automated, explainable, and verifiable hypothesis generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design](https://arxiv.org/abs/2505.20300)
*Chenxi Wu,Juan Diego Toscano,Khemraj Shukla,Yingjie Chen,Ali Shahmohammadi,Edward Raymond,Thomas Toupy,Neda Nazemifard,Charles Papageorgiou,George Em Karniadakis*

Main category: cs.LG

TL;DR: The paper introduces FMEnets, a physics-informed machine learning framework for analyzing non-ideal plug flow reactors, composed of three interconnected sub-networks with independent optimizers for forward and inverse problem-solving. It can be implemented as FME-PINNs or FME-KANs, with the latter being more robust to noise.


<details>
  <summary>Details</summary>
Motivation: To design and analyze non-ideal plug flow reactors by integrating fundamental governing equations into a unified multi-scale network model, providing a computationally efficient alternative for reactor design and optimization.

Method: FMEnets integrates governing equations (Navier-Stokes, material balance, energy balance) into a multi-scale network model with three interconnected sub-networks and independent optimizers. It operates in forward mode (predicting velocity, pressure, species concentrations, temperature profiles) and inverse mode (inferring unknown kinetic parameters and states). It can be implemented as FME-PINNs or FME-KANs.

Result: Comprehensive ablation studies show the critical role of FMEnets architecture in accurate predictions. FME-KANs are more robust to noise than FME-PINNs, while both are comparable in accuracy and speed under noise-free conditions. FMEnets achieves relative errors less than 2.5% for unknown kinetic parameters when applied to different reaction scenarios.

Conclusion: FMEnets provides a computationally efficient alternative for reactor design and optimization, capturing complex interactions effectively. It opens new possibilities for integrating empirical correlations, limited/noisy experimental data, and physical equations to guide reactor design.

Abstract: We propose FMEnets, a physics-informed machine learning framework for the
design and analysis of non-ideal plug flow reactors. FMEnets integrates the
fundamental governing equations (Navier-Stokes for fluid flow, material balance
for reactive species transport, and energy balance for temperature
distribution) into a unified multi-scale network model. The framework is
composed of three interconnected sub-networks with independent optimizers that
enable both forward and inverse problem-solving. In the forward mode, FMEnets
predicts velocity, pressure, species concentrations, and temperature profiles
using only inlet and outlet information. In the inverse mode, FMEnets utilizes
sparse multi-residence-time measurements to simultaneously infer unknown
kinetic parameters and states. FMEnets can be implemented either as FME-PINNs,
which employ conventional multilayer perceptrons, or as FME-KANs, based on
Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the
critical role of the FMEnets architecture in achieving accurate predictions.
Specifically, FME-KANs are more robust to noise than FME-PINNs, although both
representations are comparable in accuracy and speed in noise-free conditions.
The proposed framework is applied to three different sets of reaction scenarios
and is compared with finite element simulations. FMEnets effectively captures
the complex interactions, achieving relative errors less than 2.5% for the
unknown kinetic parameters. The new network framework not only provides a
computationally efficient alternative for reactor design and optimization, but
also opens new avenues for integrating empirical correlations, limited and
noisy experimental data, and fundamental physical equations to guide reactor
design.

</details>


### [45] [Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning](https://arxiv.org/abs/2505.20330)
*Yunfu Song,Zhijian Ou*

Main category: cs.LG

TL;DR: The paper identifies issues with deep generative models in semi-supervised learning, namely mode missing/covering and a conflict between good classification and generation. It introduces JRFs, a new family of algorithms that address these problems by balancing mode covering and missing, matching data distribution well, and achieving strong classification and generation results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the problems observed in deep generative models for semi-supervised learning, specifically addressing mode missing/covering issues and the conflict between good classification and good generation when using directed generative models.

Method: The method involves introducing joint-stochastic-approximation random fields (JRFs), a new family of algorithms designed for building deep undirected generative models, which are then applied to semi-supervised learning tasks.

Result: JRFs effectively balance mode covering and mode missing, match empirical data distributions well, and achieve classification results comparable to state-of-the-art methods on datasets like MNIST, SVHN, and CIFAR-10, while also performing good generation.

Conclusion: JRFs provide a promising approach to overcoming the limitations of existing deep generative models in semi-supervised learning, offering both strong classification and generation capabilities.

Abstract: Our examination of deep generative models (DGMs) developed for
semi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems.
First, mode missing and mode covering phenomenons are observed in genertion
with GANs and VAEs. Second, there exists an awkward conflict between good
classification and good generation in SSL by employing directed generative
models. To address these problems, we formally present
joint-stochastic-approximation random fields (JRFs) -- a new family of
algorithms for building deep undirected generative models, with application to
SSL. It is found through synthetic experiments that JRFs work well in balancing
mode covering and mode missing, and match the empirical data distribution well.
Empirically, JRFs achieve good classification results comparable to the
state-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in
SSL, and simultaneously perform good generation.

</details>


### [46] [PDFBench: A Benchmark for De novo Protein Design from Function](https://arxiv.org/abs/2505.20346)
*Jiahao Kuang,Nuowei Liu,Changzhi Sun,Tao Ji,Yuanbin Wu*

Main category: cs.LG

TL;DR: In recent years, while natural language processing and multimodal learning have seen rapid advancements, the field of de novo protein design has also experienced significant growth. However, most current methods rely on proprietary datasets and evaluation rubrics, making fair comparisons between different approaches challenging. To address this issue, we introduce PDFBench, the first comprehensive benchmark for evaluating de novo protein design from function. It supports two tasks: description-guided design and keyword-guided design. PDFBench compiles 22 metrics covering sequence plausibility, structural fidelity, and language-protein alignment, along with measures of novelty and diversity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a solution to the lack of a comprehensive assessment framework in the field of de novo protein design, which currently relies on proprietary datasets and evaluation rubrics that only capture a subset of the desired properties of designed proteins.

Method: Introduced PDFBench, a comprehensive benchmark for evaluating de novo protein design from function. It supports two tasks: description-guided design and keyword-guided design, and compiles 22 metrics covering various aspects such as sequence plausibility, structural fidelity, and language-protein alignment.

Result: Evaluated five state-of-the-art baselines, revealing their respective strengths and weaknesses across tasks. Analyzed inter-metric correlations, exploring the relationships between four categories of metrics, and offering guidelines for metric selection.

Conclusion: PDFBench establishes a unified framework to drive future advances in function-driven de novo protein design.

Abstract: In recent years, while natural language processing and multimodal learning
have seen rapid advancements, the field of de novo protein design has also
experienced significant growth. However, most current methods rely on
proprietary datasets and evaluation rubrics, making fair comparisons between
different approaches challenging. Moreover, these methods often employ
evaluation metrics that capture only a subset of the desired properties of
designed proteins, lacking a comprehensive assessment framework. To address
these, we introduce PDFBench, the first comprehensive benchmark for evaluating
de novo protein design from function. PDFBench supports two tasks:
description-guided design and keyword-guided design. To ensure fair and
multifaceted evaluation, we compile 22 metrics covering sequence plausibility,
structural fidelity, and language-protein alignment, along with measures of
novelty and diversity. We evaluate five state-of-the-art baselines, revealing
their respective strengths and weaknesses across tasks. Finally, we analyze
inter-metric correlations, exploring the relationships between four categories
of metrics, and offering guidelines for metric selection. PDFBench establishes
a unified framework to drive future advances in function-driven de novo protein
design.

</details>


### [47] [Decision Flow Policy Optimization](https://arxiv.org/abs/2505.20350)
*Jifeng Hu,Sili Huang,Siyuan Guo,Zhaogeng Liu,Li Shen,Lichao Sun,Hechang Chen,Yi Chang,Dacheng Tao*

Main category: cs.LG

TL;DR: In recent years, generative models have made remarkable progress in various fields. By applying these models to reinforcement learning, superior robotic control can be achieved in continuous action spaces. However, previous methods that separate multi-modal distribution fitting and policy improvement hinder model training and degrade performance. To solve this problem, we propose Decision Flow, a unified framework that integrates multi-modal action distribution modeling and policy optimization.


<details>
  <summary>Details</summary>
Motivation: Generative models have shown great capabilities in various fields, but the separation of multi-modal distribution fitting and policy improvement in previous methods hinders model training and degrades performance.

Method: We propose Decision Flow, a unified framework that integrates multi-modal action distribution modeling and policy optimization by formulating the action generation procedure of flow-based models as a flow decision-making process.

Result: Compared with established offline RL baselines, our method achieves or matches the SOTA performance across dozens of offline RL environments.

Conclusion: Decision Flow provides a solution to the problem of separating multi-modal distribution fitting and policy improvement, and demonstrates its effectiveness through extensive experiments.

Abstract: In recent years, generative models have shown remarkable capabilities across
diverse fields, including images, videos, language, and decision-making. By
applying powerful generative models such as flow-based models to reinforcement
learning, we can effectively model complex multi-modal action distributions and
achieve superior robotic control in continuous action spaces, surpassing the
limitations of single-modal action distributions with traditional
Gaussian-based policies. Previous methods usually adopt the generative models
as behavior models to fit state-conditioned action distributions from datasets,
with policy optimization conducted separately through additional policies using
value-based sample weighting or gradient-based updates. However, this
separation prevents the simultaneous optimization of multi-modal distribution
fitting and policy improvement, ultimately hindering the training of models and
degrading the performance. To address this issue, we propose Decision Flow, a
unified framework that integrates multi-modal action distribution modeling and
policy optimization. Specifically, our method formulates the action generation
procedure of flow-based models as a flow decision-making process, where each
action generation step corresponds to one flow decision. Consequently, our
method seamlessly optimizes the flow policy while capturing multi-modal action
distributions. We provide rigorous proofs of Decision Flow and validate the
effectiveness through extensive experiments across dozens of offline RL
environments. Compared with established offline RL baselines, the results
demonstrate that our method achieves or matches the SOTA performance.

</details>


### [48] [FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation](https://arxiv.org/abs/2505.20353)
*Dong Liu,Jiayi Zhang,Yifan Li,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.LG

TL;DR: FastCache is a framework to accelerate Diffusion Transformers inference by reducing redundancy in hidden states, leading to lower latency and memory usage without compromising generation quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers are powerful but computationally intensive due to their iterative structure and deep transformer stacks.

Method: Proposes FastCache with a dual strategy: (1) spatial-aware token selection mechanism that filters redundant tokens based on hidden state saliency, and (2) transformer-level cache reusing latent activations across timesteps when changes are statistically insignificant.

Result: Empirical evaluations show significant reductions in latency and memory usage while maintaining or improving generation output quality as measured by FID and t-FID compared to other cache methods.

Conclusion: FastCache effectively accelerates DiT inference by exploiting redundancy within the model's internal representations.

Abstract: Diffusion Transformers (DiT) are powerful generative models but remain
computationally intensive due to their iterative structure and deep transformer
stacks. To alleviate this inefficiency, we propose FastCache, a
hidden-state-level caching and compression framework that accelerates DiT
inference by exploiting redundancy within the model's internal representations.
FastCache introduces a dual strategy: (1) a spatial-aware token selection
mechanism that adaptively filters redundant tokens based on hidden state
saliency, and (2) a transformer-level cache that reuses latent activations
across timesteps when changes are statistically insignificant. These modules
work jointly to reduce unnecessary computation while preserving generation
fidelity through learnable linear approximation. Theoretical analysis shows
that FastCache maintains bounded approximation error under a
hypothesis-testing-based decision rule. Empirical evaluations across multiple
DiT variants demonstrate substantial reductions in latency and memory usage,
with best generation output quality compared to other cache methods, as
measured by FID and t-FID. Code implementation of FastCache is available on
GitHub at https://github.com/NoakLiu/FastCache-xDiT.

</details>


### [49] [GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2505.20355)
*Yeonjoon Jung,Daehyun Ahn,Hyungjun Kim,Taesu Kim,Eunhyeok Park*

Main category: cs.LG

TL;DR: GraLoRA is a new method that improves upon LoRA by partitioning weight matrices into sub-blocks with their own low-rank adapters, reducing gradient entanglement and improving performance in parameter-efficient fine-tuning for generative models.


<details>
  <summary>Details</summary>
Motivation: LoRA, despite its simplicity and effectiveness, has a fundamental limitation of overfitting when the bottleneck is widened which leads to stagnation or decline in accuracy at higher ranks. This is due to gradient entanglement caused by LoRA's structural bottleneck.

Method: GraLoRA partitions weight matrices into sub-blocks, each equipped with its own low-rank adapter. This approach reduces gradient entanglement and enhances representational capacity without significant computational or storage costs.

Result: Experiments on code generation and commonsense reasoning benchmarks indicate that GraLoRA consistently surpasses LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. The improvements are consistent across different model sizes and rank settings.

Conclusion: GraLoRA provides a scalable and robust solution for parameter-efficient fine-tuning, overcoming the limitations of LoRA and closely approximating the behavior of full fine-tuning.

Abstract: Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient
fine-tuning (PEFT) of generative models, valued for its simplicity and
effectiveness. Despite recent enhancements, LoRA still suffers from a
fundamental limitation: overfitting when the bottleneck is widened. It performs
best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks,
still falling short of full fine-tuning (FFT) performance. We identify the root
cause as LoRA's structural bottleneck, which introduces gradient entanglement
to the unrelated input channels and distorts gradient propagation. To address
this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA)
that partitions weight matrices into sub-blocks, each with its own low-rank
adapter. With negligible computational or storage cost, GraLoRA overcomes
LoRA's limitations, effectively increases the representational capacity, and
more closely approximates FFT behavior. Experiments on code generation and
commonsense reasoning benchmarks show that GraLoRA consistently outperforms
LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on
HumanEval+. These improvements hold across model sizes and rank settings,
making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts
are available at https://github.com/SqueezeBits/GraLoRA.git

</details>


### [50] [Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach](https://arxiv.org/abs/2505.20357)
*Jun Tian,He Wang,Jibo He,Yu Pan,Shuo Cao,Qingquan Jiang*

Main category: cs.LG

TL;DR: A hybrid CNN-RF model for gravitational wave detection improves sensitivity by 21% and better interprets features through physically meaningful metrics.


<details>
  <summary>Details</summary>
Motivation: To enhance both the performance and interpretability of CNN-based gravitational wave detection models by incorporating physically meaningful features.

Method: Propose a hybrid architecture combining CNN for feature extraction and RF for classification. Introduce four physical metrics - variance, SNR, waveform overlap, and peak amplitude - from the final convolutional layer to assist RF decision-making.

Result: Outperforms baseline CNN with 21% relative improvement in sensitivity at fixed false alarm rate; shows better detection of low-SNR signals (SNR ≤ 10). Feature attribution indicates significant contributions from both CNN-extracted and handcrafted features.

Conclusion: Physically motivated post-processing of CNN feature maps can improve interpretability and efficiency in gravitational wave detection, bridging deep learning with domain knowledge.

Abstract: Convolutional neural networks (CNNs) have become widely adopted in
gravitational wave (GW) detection pipelines due to their ability to
automatically learn hierarchical features from raw strain data. However, the
physical meaning of these learned features remains underexplored, limiting the
interpretability of such models. In this work, we propose a hybrid architecture
that combines a CNN-based feature extractor with a random forest (RF)
classifier to improve both detection performance and interpretability. Unlike
prior approaches that directly connect classifiers to CNN outputs, our method
introduces four physically interpretable metrics - variance, signal-to-noise
ratio (SNR), waveform overlap, and peak amplitude - computed from the final
convolutional layer. These are jointly used with the CNN output in the RF
classifier to enable more informed decision boundaries. Tested on long-duration
strain datasets, our hybrid model outperforms a baseline CNN model, achieving a
relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10
events per month. Notably, it also shows improved detection of low-SNR signals
(SNR $\le$ 10), which are especially vulnerable to misclassification in noisy
environments. Feature attribution via the RF model reveals that both
CNN-extracted and handcrafted features contribute significantly to
classification decisions, with learned variance and CNN outputs ranked among
the most informative. These findings suggest that physically motivated
post-processing of CNN feature maps can serve as a valuable tool for
interpretable and efficient GW detection, bridging the gap between deep
learning and domain knowledge.

</details>


### [51] [Risk-aware Direct Preference Optimization under Nested Risk Measure](https://arxiv.org/abs/2505.20359)
*Lijun Zhang,Lin Li,Yajie Qi,Huizhong Song,Yaodong Yang,Jun Wang,Wei Wei*

Main category: cs.LG

TL;DR: The paper proposes Risk-aware Direct Preference Optimization (Ra-DPO), a method for fine-tuning pre-trained Large Language Models that incorporates risk-awareness to balance alignment performance and model drift.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning pre-trained Large Language Models to align with human values and intentions requires balancing superior performance with tight risk control, which existing methods may not fully achieve.

Method: The proposed Ra-DPO approach formulates a constrained risk-aware advantage function maximization problem using nested risk measures. It converts the Bradley-Terry model into a token-level representation and uses an objective function that maximizes policy likelihood while suppressing deviation between trained and reference models via a sequential risk ratio.

Result: Experimental results on three open-source datasets (IMDb Dataset, Anthropic HH Dataset, and AlpacaEval) show the method's superior performance in balancing alignment performance and model drift.

Conclusion: Ra-DPO is introduced as an effective method to enhance risk-awareness in fine-tuned language models, achieving better balance between alignment and drift.

Abstract: When fine-tuning pre-trained Large Language Models (LLMs) to align with human
values and intentions, maximizing the estimated reward can lead to superior
performance, but it also introduces potential risks due to deviations from the
reference model's intended behavior. Most existing methods typically introduce
KL divergence to constrain deviations between the trained model and the
reference model; however, this may not be sufficient in certain applications
that require tight risk control. In this paper, we introduce Risk-aware Direct
Preference Optimization (Ra-DPO), a novel approach that incorporates
risk-awareness by employing a class of nested risk measures. This approach
formulates a constrained risk-aware advantage function maximization problem and
then converts the Bradley-Terry model into a token-level representation. The
objective function maximizes the likelihood of the policy while suppressing the
deviation between a trained model and the reference model using a sequential
risk ratio, thereby enhancing the model's risk-awareness. Experimental results
across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and
AlpacaEval, demonstrate the proposed method's superior performance in balancing
alignment performance and model drift. Our code is opensourced at
https://github.com/zlj123-max/Ra-DPO.

</details>


### [52] [GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining](https://arxiv.org/abs/2505.20380)
*Simin Fan,Maria Ios Glarou,Martin Jaggi*

Main category: cs.LG

TL;DR: 论文提出了一种新的多源多目标领域重加权框架GRAPE，用于校准预训练数据混合以在多个目标任务上实现稳健的性能。实验表明，GRAPE在推理性能和多语言任务上均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的领域重加权算法主要为单一目标任务优化数据混合，导致模型过拟合到特定目标而在其他基准上表现出显著的性能下降。

Method: GRAPE动态调整源域的采样权重（域权重），同时调节反映每个目标任务相对重要性的任务权重。该过程根据学习难度优先考虑任务，并通过一个minimax优化问题来公式化：内部最大化利用组分布式鲁棒优化（DRO）调整任务权重，外部最小化则优化域权重以减少损失。

Result: 在ClimbLab和SlimPajama数据集上的实验表明，GRAPE在6个基准上的推理性能始终优于基线方法。此外，在应用于多语言目标时，GRAPE能够从主流语言中识别出最佳的训练混合，从而在8种低资源目标语言上实现了优越的语言建模能力。

Conclusion: GRAPE提供了一种有效的方法来校准预训练数据混合，以实现在多个目标任务上的稳健性能，特别是在多语言场景下的低资源语言任务中表现出色。

Abstract: The performance of large language models (LLMs) across diverse downstream
applications is fundamentally governed by the quality and composition of their
pretraining corpora. Existing domain reweighting algorithms primarily optimize
data mixtures for a single target task, thereby resulting in models that
overfit to specialized objectives while exhibiting substantial performance
degradation on other benchmarks. This paper introduces Group Robust
Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target
domain reweighting framework designed to calibrate pretraining data mixtures
for robust performance across multiple target tasks simultaneously. GRAPE
dynamically adjusts sampling weights across source domains (domain weights)
while concurrently modulating task weights that quantify the relative
importance of each individual target task. This adaptive process prioritizes
tasks based on their learning difficulty throughout training. We formulate this
interleaved reweighting mechanism as a minimax optimization problem: The inner
maximization adjusts task weights leveraging group
distributed-robust-optimization (DRO), where those tasks demonstrating the
least improvement under the current data mixture are prioritized with higher
weights; The outer minimization then optimizes domain weights to maximize loss
reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama
datasets demonstrate that GRAPE consistently outperforms baseline methods in
terms of reasoning performance across 6 benchmarks. Furthermore, when applied
to multilingual targets, GRAPE effectively identifies optimal training mixtures
from mainstream languages, achieving superior language modeling capabilities
across 8 low-resource target languages.

</details>


### [53] [Holes in Latent Space: Topological Signatures Under Adversarial Influence](https://arxiv.org/abs/2505.20435)
*Aideen Fay,Inés García-Redondo,Qiquan Wang,Haim Dubossarsky,Anthea Monod*

Main category: cs.LG

TL;DR: An abstract about using persistent homology (PH) to understand how adversarial conditions affect language models.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of adversarial conditions on language models.

Method: Propose persistent homology (PH) to characterize latent space dynamics in LLMs under two attack modes - backdoor fine-tuning and indirect prompt injection.

Result: Adversarial conditions compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. Topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network.

Conclusion: PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.

Abstract: Understanding how adversarial conditions affect language models requires
techniques that capture both global structure and local detail within
high-dimensional activation spaces. We propose persistent homology (PH), a tool
from topological data analysis, to systematically characterize multiscale
latent space dynamics in LLMs under two distinct attack modes -- backdoor
fine-tuning and indirect prompt injection. By analyzing six state-of-the-art
LLMs, we show that adversarial conditions consistently compress latent
topologies, reducing structural diversity at smaller scales while amplifying
dominant features at coarser ones. These topological signatures are
statistically robust across layers, architectures, model sizes, and align with
the emergence of adversarial effects deeper in the network. To capture
finer-grained mechanisms underlying these shifts, we introduce a neuron-level
PH framework that quantifies how information flows and transforms within and
across layers. Together, our findings demonstrate that PH offers a principled
and unifying approach to interpreting representational dynamics in LLMs,
particularly under distributional shift.

</details>


### [54] [HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models](https://arxiv.org/abs/2505.20444)
*Haoran Li,Yingjie Qin,Baoyuan Ou,Lai Xu,Ruiwen Xu*

Main category: cs.LG

TL;DR: 为了解决现有的旋转位置编码（RoPE）在长视频等复杂时空依赖场景中的不足，本文提出了一种新的混合位置编码HoPE，通过混合频率分配策略和动态时间缩放机制来提升视觉语言模型在长上下文任务中的表现。实验表明，HoPE在多个长视频理解和检索任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态任务中取得了显著进展，但在长上下文场景（如长视频）中的表现仍然较差。目前的RoPE方法在处理视频的3D位置信息时主要依赖于启发式策略，缺乏深入的理论分析，导致无法可靠地捕捉扩展上下文中的语义相似性。

Method: 1. 研究不同的频率分配策略对视觉语言模型长上下文能力的影响。
2. 提出HoPE：一种混合位置编码，包含混合频率分配策略和动态时间缩放机制。
3. 混合频率分配策略确保可靠的语义建模；动态时间缩放机制支持跨不同上下文长度的鲁棒学习和灵活推理。

Result: 广泛的实验结果表明，HoPE在四个视频基准数据集上的长视频理解和检索任务中持续优于现有方法，证明了其有效性。

Conclusion: HoPE通过引入混合频率分配策略和动态时间缩放机制，成功提升了视觉语言模型在长上下文任务中的表现，解决了现有方法在长视频场景中的不足。

Abstract: Vision-Language Models (VLMs) have made significant progress in multimodal
tasks. However, their performance often deteriorates in long-context scenarios,
particularly long videos. While Rotary Position Embedding (RoPE) has been
widely adopted for length generalization in Large Language Models (LLMs),
extending vanilla RoPE to capture the intricate spatial-temporal dependencies
in videos remains an unsolved challenge. Existing methods typically allocate
different frequencies within RoPE to encode 3D positional information. However,
these allocation strategies mainly rely on heuristics, lacking in-depth
theoretical analysis. In this paper, we first study how different allocation
strategies impact the long-context capabilities of VLMs. Our analysis reveals
that current multimodal RoPEs fail to reliably capture semantic similarities
over extended contexts. To address this issue, we propose HoPE, a Hybrid of
Position Embedding designed to improve the long-context capabilities of VLMs.
HoPE introduces a hybrid frequency allocation strategy for reliable semantic
modeling over arbitrarily long context, and a dynamic temporal scaling
mechanism to facilitate robust learning and flexible inference across diverse
context lengths. Extensive experiments across four video benchmarks on long
video understanding and retrieval tasks demonstrate that HoPE consistently
outperforms existing methods, confirming its effectiveness. Code is available
at https://github.com/hrlics/HoPE.

</details>


### [55] [Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach](https://arxiv.org/abs/2505.20446)
*Tal Gonen,Itai Pemper,Ilan Naiman,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: The paper presents a large-scale study on generative models for time series under data-scarce conditions and proposes a unified diffusion-based framework that achieves state-of-the-art performance in few-shot settings without requiring abundant supervision.


<details>
  <summary>Details</summary>
Motivation: Generative modeling of time series is crucial, especially under data-scarce conditions. However, the understanding of how state-of-the-art generative models perform under limited supervision is lacking.

Method: The authors conduct a large-scale study evaluating leading generative models in data-scarce settings and propose a unified diffusion-based generative framework. This model is pre-trained on a large collection of time series datasets and incorporates innovations like dynamic convolutional layers and dataset token conditioning.

Result: The unified model achieves state-of-the-art performance in few-shot settings, outperforming domain-specific baselines across various subset sizes. It also surpasses all baselines when tested on full datasets benchmarks.

Conclusion: This work encourages the community to focus on few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains.

Abstract: Generative modeling of time series is a central challenge in time series
analysis, particularly under data-scarce conditions. Despite recent advances in
generative modeling, a comprehensive understanding of how state-of-the-art
generative models perform under limited supervision remains lacking. In this
work, we conduct the first large-scale study evaluating leading generative
models in data-scarce settings, revealing a substantial performance gap between
full-data and data-scarce regimes. To close this gap, we propose a unified
diffusion-based generative framework that can synthesize high-fidelity time
series across diverse domains using just a few examples. Our model is
pre-trained on a large, heterogeneous collection of time series datasets,
enabling it to learn generalizable temporal representations. It further
incorporates architectural innovations such as dynamic convolutional layers for
flexible channel adaptation and dataset token conditioning for domain-aware
generation. Without requiring abundant supervision, our unified model achieves
state-of-the-art performance in few-shot settings-outperforming domain-specific
baselines across a wide range of subset sizes. Remarkably, it also surpasses
all baselines even when tested on full datasets benchmarks, highlighting the
strength of pre-training and cross-domain generalization. We hope this work
encourages the community to revisit few-shot generative modeling as a key
problem in time series research and pursue unified solutions that scale
efficiently across domains. Code is available at
https://github.com/azencot-group/ImagenFew.

</details>


### [56] [Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes](https://arxiv.org/abs/2505.20452)
*Hao Zhao,Rong Pan*

Main category: cs.LG

TL;DR: This paper presents a new algorithm that combines Active Learning with Deep Gaussian Processes for detecting multiple change points in non-stationary time series data.


<details>
  <summary>Details</summary>
Motivation: Detecting multiple change points in non-stationary time series is difficult because of the different underlying patterns.

Method: The method uses spectral analysis to find potential changes and applies Active Learning to choose new sampling points. It integrates the modeling flexibility of Deep Gaussian Processes with the change-identification abilities of spectral methods.

Result: Experiments show that this method works better than other existing techniques in terms of detection accuracy and sampling efficiency for non-stationary time series.

Conclusion: This approach can adapt to various spectral change behaviors and accurately locate multiple change points.

Abstract: Multiple change point (MCP) detection in non-stationary time series is
challenging due to the variety of underlying patterns. To address these
challenges, we propose a novel algorithm that integrates Active Learning (AL)
with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method
leverages spectral analysis to identify potential changes and employs AL to
strategically select new sampling points for improved efficiency. By
incorporating the modeling flexibility of DGPs with the change-identification
capabilities of spectral methods, our approach adapts to diverse spectral
change behaviors and effectively localizes multiple change points. Experiments
on both simulated and real-world data demonstrate that our method outperforms
existing techniques in terms of detection accuracy and sampling efficiency for
non-stationary time series.

</details>


### [57] [BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction](https://arxiv.org/abs/2505.20454)
*Reid Graves,Anthony Zhou,Amir Barati Farimani*

Main category: cs.LG

TL;DR: BlastOFormer is a Transformer-based model for predicting blast pressure fields that outperforms traditional methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of blast pressure fields is crucial for structural safety, defense planning, and hazard mitigation. However, current methods like empirical models and CFD simulations have limitations in either capturing complex interactions or being computationally expensive.

Method: BlastOFormer uses a signed distance function (SDF) encoding and a grid to grid attention architecture inspired by OFormer and Vision Transformer frameworks. It is trained on data generated by the open source blastFoam CFD solver.

Result: BlastOFormer surpasses convolutional neural networks and Fourier Neural Operators in both log transformed and unscaled domains with an R2 score of 0.9516 and significantly lower error metrics. It performs inference in 6.4 milliseconds, making it over 600,000 times faster than CFD simulations.

Conclusion: BlastOFormer demonstrates superior spatial coherence and generalization capabilities, establishing itself as a promising real-time alternative to conventional CFD approaches for blast pressure estimation.

Abstract: Accurate prediction of blast pressure fields is essential for applications in
structural safety, defense planning, and hazard mitigation. Traditional methods
such as empirical models and computational fluid dynamics (CFD) simulations
offer limited trade offs between speed and accuracy; empirical models fail to
capture complex interactions in cluttered environments, while CFD simulations
are computationally expensive and time consuming. In this work, we introduce
BlastOFormer, a novel Transformer based surrogate model for full field maximum
pressure prediction from arbitrary obstacle and charge configurations.
BlastOFormer leverages a signed distance function (SDF) encoding and a grid to
grid attention based architecture inspired by OFormer and Vision Transformer
(ViT) frameworks. Trained on a dataset generated using the open source
blastFoam CFD solver, our model outperforms convolutional neural networks
(CNNs) and Fourier Neural Operators (FNOs) across both log transformed and
unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score
(0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for
inference, more than 600,000 times faster than CFD simulations. Qualitative
visualizations and error analyses further confirm BlastOFormer's superior
spatial coherence and generalization capabilities. These results highlight its
potential as a real time alternative to conventional CFD approaches for blast
pressure estimation in complex environments.

</details>


### [58] [Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data](https://arxiv.org/abs/2505.20485)
*Abhijit Chunduru,Majid Morafah,Mahdi Morafah,Vishnu Pandi Chellapandi,Ang Li*

Main category: cs.LG

TL;DR: 在联邦学习中，数据异质性使得现有方法容易遗忘全局决策边界。本文提出FedProj框架，通过引入新的服务器端集成知识转移损失和利用公共未标记数据集的平均集成logits的插曲记忆，来校准全局决策边界并防止遗忘，实验表明该方法显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管有许多方法应对联邦学习中的数据异质性问题，但这些方法对数据异质性如何影响全局决策边界的理解不足，且存在遗忘问题。

Method: 提出FedProj框架，包含：1) 一种新的服务器端集成知识转移损失以改善知识融合；2) 利用公共未标记数据集上的平均集成logits的插曲记忆，调节局部训练中的梯度更新。

Result: 实验结果表明，FedProj在防止全局决策边界遗忘方面表现优异，并大幅超越现有最先进方法。

Conclusion: FedProj有效地解决了联邦学习中由于数据异质性导致的全局决策边界遗忘问题，为提升联邦学习性能提供了新思路。

Abstract: The inevitable presence of data heterogeneity has made federated learning
very challenging. There are numerous methods to deal with this issue, such as
local regularization, better model fusion techniques, and data sharing. Though
effective, they lack a deep understanding of how data heterogeneity can affect
the global decision boundary. In this paper, we bridge this gap by performing
an experimental analysis of the learned decision boundary using a toy example.
Our observations are surprising: (1) we find that the existing methods suffer
from forgetting and clients forget the global decision boundary and only learn
the perfect local one, and (2) this happens regardless of the initial weights,
and clients forget the global decision boundary even starting from pre-trained
optimal weights. In this paper, we present FedProj, a federated learning
framework that robustly learns the global decision boundary and avoids its
forgetting during local training. To achieve better ensemble knowledge fusion,
we design a novel server-side ensemble knowledge transfer loss to further
calibrate the learned global decision boundary. To alleviate the issue of
learned global decision boundary forgetting, we further propose leveraging an
episodic memory of average ensemble logits on a public unlabeled dataset to
regulate the gradient updates at each step of local training. Experimental
results demonstrate that FedProj outperforms state-of-the-art methods by a
large margin.

</details>


### [59] [Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling](https://arxiv.org/abs/2505.21074)
*Yichuan Cao,Yibo Miao,Xiao-Shan Gao,Yinpeng Dong*

Main category: cs.LG

TL;DR: 提出了一种新的文本到图像模型安全评估方法RPG-RT，通过迭代使用大型语言模型修改查询并利用反馈进行微调，结合基于规则的偏好建模实现对未知防御机制的动态适应。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型的安全评估方法存在局限性：白盒方法需要内部访问，黑盒方法假设了解具体防御机制，在实际商业API场景中难以应用。如何规避未知和多样的防御机制成为一大挑战。

Method: 提出了一种名为Rule-based Preference modeling Guided Red-Teaming（RPG-RT）的新方法。该方法迭代地使用大型语言模型（LLM）修改查询，并利用来自文本到图像系统的反馈进行微调。将每次迭代中的反馈作为先验，使LLM能够动态适应未知的防御机制。进一步提出了基于规则的偏好建模，通过一组规则评估期望或不期望的反馈，从而实现对LLM动态适应过程的更细粒度控制。

Result: 在19个具有不同安全机制的文本到图像系统、3个在线商业API服务以及T2V模型上的广泛实验验证了该方法的优越性和实用性。

Conclusion: RPG-RT方法能够在无需了解具体防御机制的情况下，有效评估文本到图像模型的安全性，展现出其在实际应用中的潜力和价值。

Abstract: Text-to-image (T2I) models raise ethical and safety concerns due to their
potential to generate inappropriate or harmful images. Evaluating these models'
security through red-teaming is vital, yet white-box approaches are limited by
their need for internal access, complicating their use with closed-source
models. Moreover, existing black-box methods often assume knowledge about the
model's specific defense mechanisms, limiting their utility in real-world
commercial API scenarios. A significant challenge is how to evade unknown and
diverse defense mechanisms. To overcome this difficulty, we propose a novel
Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively
employs LLM to modify prompts to query and leverages feedback from T2I systems
for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a
prior, enabling the LLM to dynamically adapt to unknown defense mechanisms.
Given that the feedback is often labeled and coarse-grained, making it
difficult to utilize directly, we further propose rule-based preference
modeling, which employs a set of rules to evaluate desired or undesired
feedback, facilitating finer-grained control over the LLM's dynamic adaptation
process. Extensive experiments on nineteen T2I systems with varied safety
mechanisms, three online commercial API services, and T2V models verify the
superiority and practicality of our approach.

</details>


### [60] [Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints](https://arxiv.org/abs/2505.20515)
*Avik Pal,Alan Edelman,Christopher Rackauckas*

Main category: cs.LG

TL;DR: PNODEs is a new method that projects ODE steps onto constraint manifold to enforce algebraic constraints, outperforming baselines in six benchmark problems with lower error and runtime.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for incorporating hard constraints in neural differential equations face limitations such as scalability issues and poor numerical properties, making them unsuitable for modeling physical systems with complicated conservation laws.

Method: The proposed method, Manifold-Projected Neural ODEs (PNODEs), enforces algebraic constraints by projecting each ODE step onto the constraint manifold. It arises from semi-explicit differential-algebraic equations and includes a robust iterative variant and a fast approximation requiring single Jacobian factorization.

Result: PNODEs consistently outperforms baselines across six benchmark problems with a mean constraint violation error below $10^{-10}$ and achieves lower runtime compared to other methods for a given level of error tolerance.

Conclusion: Constraint projection offers a simple strategy for learning physically consistent long-horizon dynamics.

Abstract: Despite the promise of scientific machine learning (SciML) in combining
data-driven techniques with mechanistic modeling, existing approaches for
incorporating hard constraints in neural differential equations (NDEs) face
significant limitations. Scalability issues and poor numerical properties
prevent these neural models from being used for modeling physical systems with
complicated conservation laws. We propose Manifold-Projected Neural ODEs
(PNODEs), a method that explicitly enforces algebraic constraints by projecting
each ODE step onto the constraint manifold. This framework arises naturally
from semi-explicit differential-algebraic equations (DAEs), and includes both a
robust iterative variant and a fast approximation requiring a single Jacobian
factorization. We further demonstrate that prior works on relaxation methods
are special cases of our approach. PNODEs consistently outperform baselines
across six benchmark problems achieving a mean constraint violation error below
$10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to
other methods for a given level of error tolerance. These results show that
constraint projection offers a simple strategy for learning physically
consistent long-horizon dynamics.

</details>


### [61] [Towards Fully FP8 GEMM LLM Training at Scale](https://arxiv.org/abs/2505.20524)
*Alejandro Hernández-Cano,Dhia Garbaya,Imanol Schlag,Martin Jaggi*

Main category: cs.LG

TL;DR: The paper presents a new LLM architecture enabling FP8 computation for all GEMMs in transformer blocks, providing throughput gains without compromising BF16 training performance while ensuring stability and identifying key metrics for low-precision training.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing approaches that either use suboptimal FP8 kernels or revert to higher precision in sensitive components, thus missing out on potential throughput gains.

Method: Designing a novel class of LLM architectures that supports FP8 computation across all GEMMs within transformer blocks during both forward and backward passes, reducing large outlier activations for stable training.

Result: Achieves significant throughput gains at scale with downstream performance matching standard BF16 training.

Conclusion: This new architecture facilitates stable FP8 training for LLMs and provides key metrics to monitor and predict low-precision training behavior.

Abstract: Despite the significant potential of FP8 data formats for large language
model (LLM) pre-training, their adoption has been limited due to challenges in
maintaining stability at scale. Existing approaches often rely on suboptimal
fine-grained FP8 kernels or fall back to higher-precision matrix
multiplications (GEMMs) in sensitive components, such as attention projections,
compromising potential throughput gains. We introduce a new class of LLM
architectures that, for the first time, support FP8 computation for all GEMMs
within transformer blocks during both forward and backward passes. This enables
unprecedented throughput gains, particularly at scale, while matching the
downstream performance of standard BF16 training. Our architecture design
reduces large outlier activations, promoting stable long-term FP8 training. In
addition, we identify key metrics to monitor low-precision training and predict
potential future divergences.

</details>


### [62] [One-shot Robust Federated Learning of Independent Component Analysis](https://arxiv.org/abs/2505.20532)
*Dian Jin,Xin Bing,Yuqian Zhang*

Main category: cs.LG

TL;DR: This paper proposes a geometric median-based aggregation algorithm that uses k-means clustering to solve permutation ambiguity in distributed and federated ICA, proving its effectiveness even in highly heterogeneous scenarios.


<details>
  <summary>Details</summary>
Motivation: There is a need for a robust one-shot aggregation framework in distributed and federated Independent Component Analysis (ICA) to handle permutation ambiguity in local client estimations, especially in heterogeneous environments where some clients may observe minimal samples.

Method: The method involves performing k-means clustering to partition client-provided estimators into clusters, followed by aggregating the estimators within each cluster using the geometric median. This addresses the permutation ambiguity issue.

Result: The approach remains effective in highly heterogeneous scenarios, supported by theoretical analysis of error bounds for the geometric median and maximum misclustering rates of k-means clustering. Simulation studies confirm its effectiveness.

Conclusion: The proposed geometric median-based aggregation algorithm with k-means clustering successfully resolves permutation ambiguity in distributed and federated ICA, even when up to half of the clients have limited samples.

Abstract: This paper investigates a general robust one-shot aggregation framework for
distributed and federated Independent Component Analysis (ICA) problem. We
propose a geometric median-based aggregation algorithm that leverages $k$-means
clustering to resolve the permutation ambiguity in local client estimations.
Our method first performs k-means to partition client-provided estimators into
clusters and then aggregates estimators within each cluster using the geometric
median. This approach provably remains effective even in highly heterogeneous
scenarios where at most half of the clients can observe only a minimal number
of samples. The key theoretical contribution lies in the combined analysis of
the geometric median's error bound-aided by sample quantiles-and the maximum
misclustering rates of the aforementioned solution of $k$-means. The
effectiveness of the proposed approach is further supported by simulation
studies conducted under various heterogeneous settings.

</details>


### [63] [Rotary Masked Autoencoders are Versatile Learners](https://arxiv.org/abs/2505.20535)
*Uros Zivanovic,Serafina Di Gioia,Andre Scaffidi,Martín de los Rios,Gabriella Contardo,Roberto Trotta*

Main category: cs.LG

TL;DR: RoMAE is an extension to the Masked Autoencoder that uses Rotary Positional Embedding for continuous positions, allowing representation learning with multidimensional positional information without time-series-specific specializations. It performs well on various modalities including irregular time-series, images, and audio.


<details>
  <summary>Details</summary>
Motivation: Applying Transformers to irregular time-series often requires specialized architectural modifications which can add computational overhead and complexity.

Method: The Rotary Masked Autoencoder (RoMAE) extends the Masked Autoencoder by incorporating Rotary Positional Embedding (RoPE) for continuous positions. This allows it to handle multidimensional continuous positional information without needing time-series-specific architectural changes.

Result: RoMAE outperforms specialized time-series architectures on challenging datasets like DESC ELAsTiCC Challenge and maintains MAE's usual performance across other modalities such as images and audio. Additionally, RoMAE's reconstruction of embedded continuous positions reveals that learned embeddings in the input sequence disrupt RoPE's relative position property.

Conclusion: RoMAE provides a versatile solution for representation learning across multiple modalities, particularly excelling in irregular time-series data while avoiding the need for specialized architectural adjustments.

Abstract: Applying Transformers to irregular time-series typically requires
specializations to their baseline architecture, which can result in additional
computational overhead and increased method complexity. We present the Rotary
Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional
Embedding (RoPE) method for continuous positions. RoMAE is an extension to the
Masked Autoencoder (MAE) that enables representation learning with
multidimensional continuous positional information while avoiding any
time-series-specific architectural specializations. We showcase RoMAE's
performance on a variety of modalities including irregular and multivariate
time-series, images, and audio, demonstrating that RoMAE surpasses specialized
time-series architectures on difficult datasets such as the DESC ELAsTiCC
Challenge while maintaining MAE's usual performance across other modalities. In
addition, we investigate RoMAE's ability to reconstruct the embedded continuous
positions, demonstrating that including learned embeddings in the input
sequence breaks RoPE's relative position property.

</details>


### [64] [A ZeNN architecture to avoid the Gaussian trap](https://arxiv.org/abs/2505.20553)
*Luís Carvalho,João L. Costa,José Mourão,Gonçalo Oliveira*

Main category: cs.LG

TL;DR: The paper proposes Zeta Neural Networks (ZeNNs), a new architecture inspired by harmonic analysis principles, to overcome the limitations of MLPs.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of standard multi-layer perceptrons (MLPs) such as non-parametric behavior in large width limit, lack of well-defined pointwise limit, loss of non-Gaussian attributes and inability to perform feature learning.

Method: ZeNNs are constructed based on three principles from harmonic analysis: enumerating perceptons with non-learnable weights for convergence, introducing a scaling factor, and choosing activation functions leading to near orthogonal systems.

Result: ZeNNs converge pointwise in the infinite width limit, exhibit rich asymptotic structure beyond Gaussianity, perform feature learning, and excel at learning high-frequency features when appropriate activation functions are chosen.

Conclusion: ZeNNs successfully fix the referred shortcomings of MLPs.

Abstract: We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order
to overcome several shortcomings of standard multi-layer perceptrons (MLPs).
Namely, in the large width limit, MLPs are non-parametric, they do not have a
well-defined pointwise limit, they lose non-Gaussian attributes and become
unable to perform feature learning; moreover, finite width MLPs perform poorly
in learning high frequencies. The new ZeNN architecture is inspired by three
simple principles from harmonic analysis:
  i) Enumerate the perceptons and introduce a non-learnable weight to enforce
convergence;
  ii) Introduce a scaling (or frequency) factor;
  iii) Choose activation functions that lead to near orthogonal systems.
  We will show that these ideas allow us to fix the referred shortcomings of
MLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they
exhibit a rich asymptotic structure beyond Gaussianity, and perform feature
learning. Moreover, when appropriate activation functions are chosen, (finite
width) ZeNNs excel at learning high-frequency features of functions with low
dimensional domains.

</details>


### [65] [Learning a Pessimistic Reward Model in RLHF](https://arxiv.org/abs/2505.20556)
*Yinglun Xu,Hangoo Kang,Tarun Suresh,Yuxuan Wan,Gagandeep Singh*

Main category: cs.LG

TL;DR: This work proposes PET, a novel pessimistic reward fine-tuning method for learning a robust reward model in offline RLHF. Unlike traditional methods that rely on KL regularization to mitigate reward hacking, PET prevents reward hacking without any regularization. The method is tested on the TL;DR summarization dataset and shows the feasibility of learning a high-quality policy with high KL divergence from the dataset distribution.


<details>
  <summary>Details</summary>
Motivation: Reward hacking remains a significant issue in reinforcement learning from human feedback (RLHF). Traditional approaches attempt to address this by using KL regularization, but they still suffer from reward hacking and exclude policies with large KL divergence from the dataset distribution.

Method: PET is a pessimistic reward fine-tuning method. It involves learning a pessimistic reward model which is robust against reward hacking in offline RLHF. When optimizing a policy on this model, reward hacking can be prevented without relying on any regularization.

Result: The method was tested on the TL;DR summarization dataset. Results show that it is possible to learn a high-quality policy on the pessimistic reward model without using any regularization. This policy has a high KL divergence from the dataset distribution while maintaining high performance.

Conclusion: The work demonstrates the feasibility of learning a pessimistic reward model against reward hacking. An agent can greedily search for a policy with a high pessimistic reward without being affected by reward hacking.

Abstract: This work proposes `PET', a novel pessimistic reward fine-tuning method, to
learn a pessimistic reward model robust against reward hacking in offline
reinforcement learning from human feedback (RLHF). Traditional reward modeling
techniques in RLHF train an imperfect reward model, on which a KL
regularization plays a pivotal role in mitigating reward hacking when
optimizing a policy. Such an intuition-based method still suffers from reward
hacking, and the policies with large KL divergence from the dataset
distribution are excluded during learning. In contrast, we show that when
optimizing a policy on a pessimistic reward model fine-tuned through PET,
reward hacking can be prevented without relying on any regularization. We test
our methods on the standard TL;DR summarization dataset. We find that one can
learn a high-quality policy on our pessimistic reward without using any
regularization. Such a policy has a high KL divergence from the dataset
distribution while having high performance in practice. In summary, our work
shows the feasibility of learning a pessimistic reward model against reward
hacking. The agent can greedily search for the policy with a high pessimistic
reward without suffering from reward hacking.

</details>


### [66] [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/abs/2505.20561)
*Shenao Zhang,Yaqing Wang,Yinxiao Liu,Tianqi Liu,Peter Grabowski,Eugene Ie,Zhaoran Wang,Yunxuan Li*

Main category: cs.LG

TL;DR: The paper explores reflective reasoning in Large Language Models (LLMs) through the Bayes-Adaptive Reinforcement Learning (BARL) framework, surpassing standard Markovian RL methods in exploration effectiveness and token efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand if and how reflective reasoning emerges during training via conventional Markovian Reinforcement Learning (RL), and to improve exploration and exploitation in LLMs.

Method: Reformulate reflective exploration within the Bayes-Adaptive RL framework, which optimizes expected return under a posterior distribution over Markov decision processes. Develop BARL algorithm that guides LLMs to switch strategies based on observed outcomes.

Result: BARL outperforms standard Markovian RL approaches in test-time performance, showing superior token efficiency and improved exploration effectiveness on both synthetic and mathematical reasoning tasks.

Conclusion: BARL provides a principled way for LLMs to reflectively explore, enhancing their reasoning capabilities beyond traditional RL methods.

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
exhibited strong reasoning capabilities and emergent reflective behaviors, such
as backtracking and error correction. However, conventional Markovian RL
confines exploration to the training phase to learn an optimal deterministic
policy and depends on the history contexts only through the current state.
Therefore, it remains unclear whether reflective reasoning will emerge during
Markovian RL training, or why they are beneficial at test time. To remedy this,
we recast reflective exploration within the Bayes-Adaptive RL framework, which
explicitly optimizes the expected return under a posterior distribution over
Markov decision processes. This Bayesian formulation inherently incentivizes
both reward-maximizing exploitation and information-gathering exploration via
belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and
switch strategies based on the observed outcomes, offering principled guidance
on when and how the model should reflectively explore. Empirical results on
both synthetic and mathematical reasoning tasks demonstrate that BARL
outperforms standard Markovian RL approaches at test time, achieving superior
token efficiency with improved exploration effectiveness. Our code is available
at https://github.com/shenao-zhang/BARL.

</details>


### [67] [Bi-Level Unsupervised Feature Selection](https://arxiv.org/abs/2505.20563)
*Jingjing Liu,Xiansen Ju,Xianchao Xiu,Wanquan Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Unsupervised feature selection (UFS) is an important task in data
engineering. However, most UFS methods construct models from a single
perspective and often fail to simultaneously evaluate feature importance and
preserve their inherent data structure, thus limiting their performance. To
address this challenge, we propose a novel bi-level unsupervised feature
selection (BLUFS) method, including a clustering level and a feature level.
Specifically, at the clustering level, spectral clustering is used to generate
pseudo-labels for representing the data structure, while a continuous linear
regression model is developed to learn the projection matrix. At the feature
level, the $\ell_{2,0}$-norm constraint is imposed on the projection matrix for
more effectively selecting features. To the best of our knowledge, this is the
first work to combine a bi-level framework with the $\ell_{2,0}$-norm. To solve
the proposed bi-level model, we design an efficient proximal alternating
minimization (PAM) algorithm, whose subproblems either have explicit solutions
or can be computed by fast solvers. Furthermore, we establish the convergence
result and computational complexity. Finally, extensive experiments on two
synthetic datasets and eight real datasets demonstrate the superiority of BLUFS
in clustering and classification tasks.

</details>


### [68] [Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL](https://arxiv.org/abs/2505.20578)
*Xingyu Chen,Shihao Ma,Runsheng Lin,Jiecong Lin,Bo Wang*

Main category: cs.LG

TL;DR: 设计具有细胞类型特异性的调控DNA序列对于合成生物学、基因治疗和精准医学至关重要。本文提出了一种名为Ctrl-DNA的新型约束强化学习框架，用于生成具有可控细胞类型特异性的调控DNA序列。实验表明，Ctrl-DNA在人类启动子和增强子上的表现优于现有的生成方法和基于强化学习的方法，能够生成高适应性调控序列，并达到最先进的细胞类型特异性。


<details>
  <summary>Details</summary>
Motivation: 精确的细胞类型特异性基因表达的设计对于合成生物学、基因治疗和精准医学的发展至关重要，但现有的基于Transformer的语言模型虽然能有效捕捉调控DNA中的模式，但在生成具有可靠细胞特异性活性的新序列时仍然面临挑战。

Method: 通过将调控序列设计表述为一个生物信息驱动的约束优化问题，应用强化学习到自回归基因组语言模型中，使模型能够在最大化目标细胞类型调控活性的同时限制脱靶效应，从而迭代地优化序列。

Result: 在人类启动子和增强子上的评估显示，Ctrl-DNA持续超越现有的生成方法和基于强化学习的方法，生成高适应性调控序列并达到最先进的细胞类型特异性。此外，Ctrl-DNA生成的序列捕获了关键的细胞类型特异性转录因子结合位点（TFBS），证明了生成序列的生物学合理性。

Conclusion: Ctrl-DNA提供了一种有效的解决方案来生成具有可控细胞类型特异性的调控DNA序列，其性能优于现有方法，并且生成的序列在生物学上是合理的。

Abstract: Designing regulatory DNA sequences that achieve precise cell-type-specific
gene expression is crucial for advancements in synthetic biology, gene therapy
and precision medicine. Although transformer-based language models (LMs) can
effectively capture patterns in regulatory DNA, their generative approaches
often struggle to produce novel sequences with reliable cell-specific activity.
Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL)
framework tailored for designing regulatory DNA sequences with controllable
cell-type specificity. By formulating regulatory sequence design as a
biologically informed constrained optimization problem, we apply RL to
autoregressive genomic LMs, enabling the models to iteratively refine sequences
that maximize regulatory activity in targeted cell types while constraining
off-target effects. Our evaluation on human promoters and enhancers
demonstrates that Ctrl-DNA consistently outperforms existing generative and
RL-based approaches, generating high-fitness regulatory sequences and achieving
state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences
capture key cell-type-specific transcription factor binding sites (TFBS), short
DNA motifs recognized by regulatory proteins that control gene expression,
demonstrating the biological plausibility of the generated sequences.

</details>


### [69] [The challenge of hidden gifts in multi-agent reinforcement learning](https://arxiv.org/abs/2505.20579)
*Dane Malenfant,Blake A. Richards*

Main category: cs.LG

TL;DR: 在多智能体强化学习中，'隐藏礼物'（即其他智能体采取的有益但未被察觉的行为）使得信用分配变得困难。本文通过一个简单的网格世界任务研究了这一问题。在这个任务中，多个智能体需要解锁各自的门以获得个人奖励，同时如果所有智能体都解锁了自己的门，则可以获得更大的集体奖励。然而，只有一把钥匙可以用于解锁所有门，因此智能体需要在使用后放下钥匙以便他人使用。由于没有信号指示其他智能体是否放下钥匙，放下钥匙被视为'隐藏礼物'。研究表明，几种最先进的强化学习算法无法学会在这种简单任务中获得集体奖励。独立无模型策略梯度智能体在提供自身动作历史信息时能够解决任务，而多智能体强化学习算法即使有动作历史也仍然无法解决任务。最后，作者提出了一种校正项，减少了学习中的方差并帮助独立智能体更可靠地收敛到集体成功。这些结果表明，在存在'隐藏礼物'的情况下，多智能体环境中的信用分配尤其具有挑战性，并且在独立智能体中引入学习意识可以改善这种情况。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，当其他智能体采取有益但未被察觉的行为（即'隐藏礼物'）时，信用分配变得困难。这促使研究者探索如何在这种情况下设计有效的学习算法。

Method: 研究者设计了一个简单的网格世界任务，其中多个智能体需要解锁各自的门以获得个人奖励，同时如果所有智能体都解锁了自己的门，则可以获得更大的集体奖励。任务中只有一把钥匙可以用于解锁所有门，因此智能体需要在使用后放下钥匙以便他人使用。研究者测试了几种最先进的强化学习算法和多智能体强化学习算法在该任务中的表现，并提出了一种校正项以减少独立智能体学习中的方差。

Result: 研究表明，几种最先进的强化学习算法无法学会在这种简单任务中获得集体奖励。独立无模型策略梯度智能体在提供自身动作历史信息时能够解决任务，而多智能体强化学习算法即使有动作历史也仍然无法解决任务。校正项减少了学习中的方差并帮助独立智能体更可靠地收敛到集体成功。

Conclusion: 信用分配在多智能体设置中，特别是在存在'隐藏礼物'的情况下，具有特别的挑战性。学习意识在独立智能体中可以受益于这种设置。

Abstract: Sometimes we benefit from actions that others have taken even when we are
unaware that they took those actions. For example, if your neighbor chooses not
to take a parking spot in front of your house when you are not there, you can
benefit, even without being aware that they took this action. These "hidden
gifts" represent an interesting challenge for multi-agent reinforcement
learning (MARL), since assigning credit when the beneficial actions of others
are hidden is non-trivial. Here, we study the impact of hidden gifts with a
very simple MARL task. In this task, agents in a grid-world environment have
individual doors to unlock in order to obtain individual rewards. As well, if
all the agents unlock their door the group receives a larger collective reward.
However, there is only one key for all of the doors, such that the collective
reward can only be obtained when the agents drop the key for others after they
use it. Notably, there is nothing to indicate to an agent that the other agents
have dropped the key, thus the act of dropping the key for others is a "hidden
gift". We show that several different state-of-the-art RL algorithms, including
MARL algorithms, fail to learn how to obtain the collective reward in this
simple task. Interestingly, we find that independent model-free policy gradient
agents can solve the task when we provide them with information about their own
action history, but MARL agents still cannot solve the task with action
history. Finally, we derive a correction term for these independent agents,
inspired by learning aware approaches, which reduces the variance in learning
and helps them to converge to collective success more reliably. These results
show that credit assignment in multi-agent settings can be particularly
challenging in the presence of "hidden gifts", and demonstrate that learning
awareness in independent agents can benefit these settings.

</details>


### [70] [Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction](https://arxiv.org/abs/2505.20589)
*Mahdi Pourmirzaei,Farzaneh Esmaili,Salhuldin Alqarghuli,Mohammadreza Pourmirzaei,Ye Han,Kai Chen,Mohsen Rezaei,Duolin Wang,Dong Xu*

Main category: cs.LG

TL;DR: Researchers developed Prot2Token, a unified framework for protein-related predictions that converts tasks into next-token prediction format. It uses an autoregressive decoder and task tokens, allowing multi-task learning and significant speedups compared to specialized models. Experiments show strong performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The diverse nature of protein prediction tasks traditionally required specialized models, which hindered the development of broadly applicable and computationally efficient Protein Language Models (PLMs).

Method: Prot2Token is a unified framework that converts protein-related predictions into a standardized next-token prediction format using an autoregressive decoder conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens. It also includes an auxiliary self-supervised decoder pre-training approach.

Result: Prot2Token demonstrates strong predictive power across various benchmarks, with significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance matching or exceeding specialized approaches.

Conclusion: Prot2Token offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics.

Abstract: The diverse nature of protein prediction tasks has traditionally necessitated
specialized models, hindering the development of broadly applicable and
computationally efficient Protein Language Models (PLMs). In this work, we
introduce Prot2Token, a unified framework that overcomes these challenges by
converting a wide spectrum of protein-related predictions, from sequence-level
properties and residue-specific attributes to complex inter-protein
interactions, into a standardized next-token prediction format. At its core,
Prot2Token employs an autoregressive decoder, conditioned on embeddings from
pre-trained protein encoders and guided by learnable task tokens, to perform
diverse predictions. This architecture uniquely facilitates multi-task
learning, enabling a single model to master numerous tasks with improved
efficiency. We present extensive experimental validation across a variety of
benchmarks, demonstrating Prot2Tokens strong predictive power in different
types of protein-prediction tasks. Key results include significant speedups
(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or
exceeding specialized approaches. Beyond that, we introduce an auxiliary
self-supervised decoder pre-training approach to improve spatially sensitive
task performance. Prot2Token thus offers a significant step towards a
versatile, high-throughput paradigm for protein modeling, promising to
accelerate biological discovery and the development of novel therapeutics. The
code is available at https://github.com/mahdip72/prot2token .

</details>


### [71] [Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning](https://arxiv.org/abs/2505.20621)
*Shijie Liu,Andrew C. Cullen,Paul Montague,Sarah Erfani,Benjamin I. P. Rubinstein*

Main category: cs.LG

TL;DR: Offline Reinforcement Learning is vulnerable to poisoning attacks. This paper proposes a method based on Differential Privacy to provide certified defenses, improving robustness and safety in offline RL.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to mitigate the risks posed by poisoning attacks in Offline Reinforcement Learning, which can compromise the performance and reliability of the learning framework.

Method: The method leverages properties of Differential Privacy to provide larger guarantees against adversarial manipulation. It ensures robustness for both per-state actions and the overall expected cumulative reward, applicable to continuous and discrete spaces, as well as stochastic and deterministic environments.

Result: Empirical evaluations show that the approach ensures the performance drops to no more than 50% with up to 7% of the training data poisoned, significantly improving over the 0.008% in prior work, while producing certified radii that is 5 times larger.

Conclusion: This work highlights the potential of the proposed framework to enhance safety and reliability in offline RL.

Abstract: Similar to other machine learning frameworks, Offline Reinforcement Learning
(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on
externally sourced datasets, a vulnerability that is exacerbated by its
sequential nature. To mitigate the risks posed by RL poisoning, we extend
certified defenses to provide larger guarantees against adversarial
manipulation, ensuring robustness for both per-state actions, and the overall
expected cumulative reward. Our approach leverages properties of Differential
Privacy, in a manner that allows this work to span both continuous and discrete
spaces, as well as stochastic and deterministic environments -- significantly
expanding the scope and applicability of achievable guarantees. Empirical
evaluations demonstrate that our approach ensures the performance drops to no
more than $50\%$ with up to $7\%$ of the training data poisoned, significantly
improving over the $0.008\%$ in prior work~\citep{wu_copa_2022}, while
producing certified radii that is $5$ times larger as well. This highlights the
potential of our framework to enhance safety and reliability in offline RL.

</details>


### [72] [Position: Adopt Constraints Over Penalties in Deep Learning](https://arxiv.org/abs/2505.20628)
*Juan Ramirez,Meraj Hashemizadeh,Simon Lacoste-Julien*

Main category: cs.LG

TL;DR: Recent efforts in developing trustworthy AI systems have emphasized the incorporation of external requirements or constraints into machine learning formulations. However, the traditional penalization approach is inefficient and ill-suited. Instead, tailored constrained optimization methods, such as the Lagrangian approach, are more effective in solving constrained problems, ensuring accountability, eliminating extensive penalty tuning, and integrating with modern deep learning pipelines.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the inefficiency and limitations of the traditional penalization approach in incorporating external requirements or constraints into machine learning models, which often requires extensive tuning and does not guarantee constraint satisfaction and good performance simultaneously.

Method: The method proposed in this paper advocates for the adoption of tailored constrained optimization techniques, specifically the Lagrangian approach. This method optimizes the penalization 'coefficients' (the Lagrange multipliers) along with the model, allowing for true solutions to constrained problems without the need for costly penalty coefficient tuning.

Result: The results indicate that using tailored constrained optimization methods leads to solutions that satisfy the constraints while achieving good performance. It also eliminates the need for extensive penalty tuning, thereby saving time and computational resources.

Conclusion: The conclusion is that tailored constrained optimization methods should be preferred over traditional penalization approaches when developing trustworthy AI systems with accountability guarantees. These methods better integrate with modern deep learning pipelines and provide a more efficient way to incorporate external requirements.

Abstract: Recent efforts toward developing trustworthy AI systems with accountability
guarantees have led to a growing reliance on machine learning formulations that
incorporate external requirements, or constraints. These requirements are often
enforced through penalization--adding fixed-weight terms to the task loss. We
argue that this approach is ill-suited, and that tailored constrained
optimization methods should be adopted instead. In particular, no penalty
coefficient may yield a solution that both satisfies the constraints and
achieves good performance--i.e., one solving the constrained problem. Moreover,
tuning these coefficients is costly, incurring significant time and
computational overhead. In contrast, tailored constrained methods--such as the
Lagrangian approach, which optimizes the penalization "coefficients" (the
Lagrange multipliers) alongside the model--(i) truly solve the constrained
problem and add accountability, (ii) eliminate the need for extensive penalty
tuning, and (iii) integrate seamlessly with modern deep learning pipelines.

</details>


### [73] [Explaining Concept Shift with Interpretable Feature Attribution](https://arxiv.org/abs/2505.20634)
*Ruiqi Lyu,Alistair Turcan,Bryan Wilder*

Main category: cs.LG

TL;DR: 提出了一种名为SGShift的新模型，用于检测表格数据中的概念漂移，并将降低的模型性能归因于少量转移特征。通过广义加性模型（GAM）和后续特征选择来识别这些转移特征，并结合敲诈法控制错误发现率以及吸收项改进数据拟合效果。实验表明，该方法在AUC和召回率方面显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型可能面临与训练集不同的数据，导致性能下降。当标签分布相对于特征变化时，会出现概念漂移问题，这使得即使调整良好的模型也可能学习到错误表示。因此，识别这些转移特征对于理解数据集之间的差异至关重要。

Method: 1. 提出SGShift模型：使用广义加性模型（GAM）建模概念漂移并进行特征选择以识别转移特征。
2. 引入敲诈法扩展：控制错误发现率。
3. 添加吸收项：改进对数据拟合较差的模型的效果。

Result: 在合成和真实数据上的广泛实验表明，SGShift能够以AUC > 0.9 和召回率 > 90% 的表现识别转移特征，其性能通常是基线方法的2到3倍。

Conclusion: SGShift是一种有效的工具，可以检测概念漂移并将模型性能下降归因于特定的转移特征。它在多种机器学习模型上表现出色，具有高准确性和召回率，并能有效控制错误发现率。

Abstract: Regardless the amount of data a machine learning (ML) model is trained on,
there will inevitably be data that differs from their training set, lowering
model performance. Concept shift occurs when the distribution of labels
conditioned on the features changes, making even a well-tuned ML model to have
learned a fundamentally incorrect representation. Identifying these shifted
features provides unique insight into how one dataset differs from another,
considering the difference may be across a scientifically relevant dimension,
such as time, disease status, population, etc. In this paper, we propose
SGShift, a model for detecting concept shift in tabular data and attributing
reduced model performance to a sparse set of shifted features. SGShift models
concept shift with a Generalized Additive Model (GAM) and performs subsequent
feature selection to identify shifted features. We propose further extensions
of SGShift by incorporating knockoffs to control false discoveries and an
absorption term to account for models with poor fit to the data. We conduct
extensive experiments in synthetic and real data across various ML models and
find SGShift can identify shifted features with AUC $>0.9$ and recall $>90\%$,
often 2 or 3 times as high as baseline methods.

</details>


### [74] [Can Past Experience Accelerate LLM Reasoning?](https://arxiv.org/abs/2505.20643)
*Bo Pan,Liang Zhao*

Main category: cs.LG

TL;DR: The paper explores if LLMs can become faster at reasoning with repeated exposure and proposes SpeedupLLM framework.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can become faster at reasoning through recurrent exposure on relevant tasks, similar to how humans improve with experience.

Method: Proposes SpeedupLLM, a framework that uses adaptive compute allocation and memory mechanisms to implement reasoning speedup behavior. It formalizes the problem setting in terms of task relevancy and compute budget calculation.

Result: Experiments demonstrate that LLMs can reason faster with past experience, achieving up to a 56% reduction in compute cost when using suitable memory and reasoning methods.

Conclusion: LLMs have the potential to reduce compute costs significantly by leveraging past experiences, as shown through the SpeedupLLM framework.

Abstract: Allocating more compute to large language models (LLMs) reasoning has
generally been demonstrated to improve their effectiveness, but also results in
increased inference time. In contrast, humans can perform tasks faster and
better with increased experience and exposure. Hence, this paper aims to
investigate the question: Can LLMs also become faster at reasoning through
recurrent exposure on relevant tasks, and if so, how can it be achieved? To
address these questions, we first formalize the problem setting of LLM
reasoning speedup systematically in the dimensions of task relevancy and
compute budget calculation. We then propose SpeedupLLM, a theoretically
guaranteed framework to implement and benchmark such reasoning speedup
behaviour based on adaptive compute allocation and memory mechanisms. We
further conduct comprehensive experiments to benchmark such behaviour across
different question similarity levels, memory methods, and reasoning methods.
Results show that LLMs can generally reason faster with past experience,
achieving up to a 56% reduction in compute cost when equipped with appropriate
memory and reasoning methods.

</details>


### [75] [Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory](https://arxiv.org/abs/2505.20646)
*Eduardo Y. Sakabe,Felipe S. Abrahão,Alexandre Simões,Esther Colombini,Paula Costa,Ricardo Gudwin,Hector Zenil*

Main category: cs.LG

TL;DR: The paper proposes using algorithmic information theory and Binarized Neural Networks (BNNs) to better understand and control the informational complexity of neural networks, showing that this approach more accurately tracks structural changes during training than traditional entropy-based methods.


<details>
  <summary>Details</summary>
Motivation: Most existing approaches for understanding neural network complexity rely on entropy-based loss functions and statistical metrics, which may not fully capture causally relevant regularities in network structure.

Method: The authors use algorithmic information theory, specifically applying the Block Decomposition Method (BDM) as a scalable approximation of algorithmic complexity based on algorithmic probability (AP). This is done with Binarized Neural Networks (BNNs) as a proxy.

Result: The BDM method more closely tracks structural changes during training compared to entropy, exhibiting stronger correlations with training loss across different model sizes and randomized training runs. This supports the idea of training as a process of algorithmic compression.

Conclusion: This work provides a principled way to estimate learning progression and suggests a framework for complexity-aware learning and regularization rooted in information theory, complexity, and computability.

Abstract: Understanding and controlling the informational complexity of neural networks
is a central challenge in machine learning, with implications for
generalization, optimization, and model capacity. While most approaches rely on
entropy-based loss functions and statistical metrics, these measures often fail
to capture deeper, causally relevant algorithmic regularities embedded in
network structure. We propose a shift toward algorithmic information theory,
using Binarized Neural Networks (BNNs) as a first proxy. Grounded in
algorithmic probability (AP) and the universal distribution it defines, our
approach characterizes learning dynamics through a formal, causally grounded
lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation
of algorithmic complexity based on AP -- and demonstrate that it more closely
tracks structural changes during training than entropy, consistently exhibiting
stronger correlations with training loss across varying model sizes and
randomized training runs. These results support the view of training as a
process of algorithmic compression, where learning corresponds to the
progressive internalization of structured regularities. In doing so, our work
offers a principled estimate of learning progression and suggests a framework
for complexity-aware learning and regularization, grounded in first principles
from information theory, complexity, and computability.

</details>


### [76] [Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning](https://arxiv.org/abs/2505.20648)
*Mengmeng Chen,Xiaohu Wu,Qiqi Liu,Tiantian He,Yew-Soon Ong,Yaochu Jin,Qicheng Lao,Han Yu*

Main category: cs.LG

TL;DR: 本文针对多目标优化中的Pareto前沿学习问题，提出了一个新框架PHN-HVVS，解决了现有方法在高维空间采样和覆盖整个Pareto前沿方面的不足，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多目标优化（MOO）在机器学习中普遍存在，找到一组Pareto最优解（称为Pareto前沿）是关键任务。现有的Pareto前沿学习（PFL）方法面临高维空间中采样光线和无法覆盖整个凸形Pareto前沿的挑战。

Method: 提出了一种名为PHN-HVVS的新框架，该框架将设计空间分解为Voronoi网格，并在高维空间内使用遗传算法（GA）进行Voronoi网格划分。此外，还引入了一个新的损失函数，有助于更广泛地覆盖结果Pareto前沿并最大化HV指标。

Result: 实验结果表明，在多个MOO机器学习任务中，PHN-HVVS在生成Pareto前沿方面显著优于基线方法。同时，PHN-HVVS推动了联邦学习领域中几个近期问题的方法论进步。

Conclusion: PHN-HVVS在生成Pareto前沿方面显著优于基准方法，并推动了联邦学习领域中几个近期问题的方法论进步。

Abstract: Multi-objective optimization (MOO) exists extensively in machine learning,
and aims to find a set of Pareto-optimal solutions, called the Pareto front,
e.g., it is fundamental for multiple avenues of research in federated learning
(FL). Pareto-Front Learning (PFL) is a powerful method implemented using
Hypernetworks (PHNs) to approximate the Pareto front. This method enables the
acquisition of a mapping function from a given preference vector to the
solutions on the Pareto front. However, most existing PFL approaches still face
two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to
cover the entire Pareto Front which has a convex shape. Here, we introduce a
novel PFL framework, called as PHN-HVVS, which decomposes the design space into
Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid
partitioning within high-dimensional space. We put forward a new loss function,
which effectively contributes to more extensive coverage of the resultant
Pareto front and maximizes the HV Indicator. Experimental results on multiple
MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines
significantly in generating Pareto front. Also, we illustrate that PHN-HVVS
advances the methodologies of several recent problems in the FL field. The code
is available at
https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.

</details>


### [77] [An Optimisation Framework for Unsupervised Environment Design](https://arxiv.org/abs/2505.20659)
*Nathan Monette,Alistair Letcher,Michael Beukman,Matthew T. Jackson,Alexander Rutherford,Alexander D. Goldie,Jakob N. Foerster*

Main category: cs.LG

TL;DR: This paper explores unsupervised environment design (UED) for reinforcement learning agents from an optimisation perspective, offering stronger theoretical guarantees and a convergent algorithm, which empirically surpasses previous methods.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning agents need to be robust in unfamiliar scenarios, particularly in high-risk settings. The current methods of unsupervised environment design (UED) have limitations in providing strong theoretical guarantees, especially in practical settings where convergence is not assured.

Method: The authors study UED through an optimisation lens, employing a nonconvex-strongly-concave objective that ensures convergence in zero-sum settings. This approach provides stronger theoretical guarantees compared to previous methods that only offer guarantees upon reaching convergence.

Result: The proposed method outperforms prior techniques in various environments with different difficulty levels, demonstrating its efficacy through empirical verification.

Conclusion: The research advances the field of UED by offering a more reliable and theoretically grounded framework for improving the generalisability of reinforcement learning agents.

Abstract: For reinforcement learning agents to be deployed in high-risk settings, they
must achieve a high level of robustness to unfamiliar scenarios. One method for
improving robustness is unsupervised environment design (UED), a suite of
methods aiming to maximise an agent's generalisability across configurations of
an environment. In this work, we study UED from an optimisation perspective,
providing stronger theoretical guarantees for practical settings than prior
work. Whereas previous methods relied on guarantees if they reach convergence,
our framework employs a nonconvex-strongly-concave objective for which we
provide a provably convergent algorithm in the zero-sum setting. We empirically
verify the efficacy of our method, outperforming prior methods in a number of
environments with varying difficulties.

</details>


### [78] [Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers](https://arxiv.org/abs/2505.20666)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: A new framework called Continuous_Time Attention is proposed, which integrates PDEs into the Transformer's attention mechanism. This approach improves handling of long input sequences by allowing attention weights to evolve over a pseudo_time dimension, resulting in better optimization landscapes and performance gains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by extremely long input sequences in Transformers, such as local noise, weak long_range dependencies, and unstable gradient flow.

Method: The method involves infusing partial differential equations (PDEs) into the Transformer's attention mechanism, allowing attention weights to evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion dynamics.

Result: Theoretically, PDE_based attention leads to better optimization landscapes and polynomial decay of distant interactions. Empirically, the method demonstrates consistent gains over standard and specialized long sequence Transformer variants.

Conclusion: The findings indicate that PDE_based formulations can enrich attention mechanisms with continuous_time dynamics and global coherence, offering potential improvements for processing long sequences.

Abstract: We propose a novel framework, Continuous_Time Attention, which infuses
partial differential equations (PDEs) into the Transformer's attention
mechanism to address the challenges of extremely long input sequences. Instead
of relying solely on a static attention matrix, we allow attention weights to
evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion
dynamics. This mechanism systematically smooths local noise, enhances
long_range dependencies, and stabilizes gradient flow. Theoretically, our
analysis shows that PDE_based attention leads to better optimization landscapes
and polynomial rather than exponential decay of distant interactions.
Empirically, we benchmark our method on diverse experiments_demonstrating
consistent gains over both standard and specialized long sequence Transformer
variants. Our findings highlight the potential of PDE_based formulations to
enrich attention mechanisms with continuous_time dynamics and global coherence.

</details>


### [79] [Accelerating RL for LLM Reasoning with Optimal Advantage Regression](https://arxiv.org/abs/2505.20686)
*Kianté Brantley,Mingyu Chen,Zhaolin Gao,Jason D. Lee,Wen Sun,Wenhao Zhan,Xuezhou Zhang*

Main category: cs.LG

TL;DR: A new two-stage policy optimization framework called $A$*-PO is proposed for improving the efficiency of training LLMs with RL. It reduces computational overhead and memory usage while maintaining competitive performance in mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art policy optimization methods for RL suffer from high computational costs and memory consumption, which hinders their application in fine-tuning LLMs for complex reasoning tasks.

Method: The $A$*-PO framework consists of two stages: 1) Offline sampling from a reference policy to estimate the optimal value function $V$*, avoiding costly online estimation; 2) On-policy updates using a least-squares regression loss with only one generation per prompt.

Result: Empirically, $A$*-PO achieves similar performance to other state-of-the-art methods across various mathematical reasoning benchmarks, but with up to 2x faster training time and over 30% less peak memory usage.

Conclusion: The $A$*-PO framework offers an efficient alternative for training LLMs with RL, reducing resource demands without sacrificing performance.

Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning
large language models (LLMs) to improve complex reasoning abilities. However,
state-of-the-art policy optimization methods often suffer from high
computational overhead and memory consumption, primarily due to the need for
multiple generations per prompt and the reliance on critic networks or
advantage estimates of the current policy. In this paper, we propose $A$*-PO, a
novel two-stage policy optimization framework that directly approximates the
optimal advantage function and enables efficient training of LLMs for reasoning
tasks. In the first stage, we leverage offline sampling from a reference policy
to estimate the optimal value function $V$*, eliminating the need for costly
online value estimation. In the second stage, we perform on-policy updates
using a simple least-squares regression loss with only a single generation per
prompt. Theoretically, we establish performance guarantees and prove that the
KL-regularized RL objective can be optimized without requiring complex
exploration strategies. Empirically, $A$*-PO achieves competitive performance
across a wide range of mathematical reasoning benchmarks, while reducing
training time by up to 2$\times$ and peak memory usage by over 30% compared to
PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at
https://github.com/ZhaolinGao/A-PO.

</details>


### [80] [Evidential Deep Active Learning for Semi-Supervised Classification](https://arxiv.org/abs/2505.20691)
*Shenkai Zhao,Xinao Zhang,Lipeng Pan,Xiaobin Xu,Danilo Pelusi*

Main category: cs.LG

TL;DR: This paper proposes EDALSSC, a new semi-supervised classification method that incorporates uncertainty estimation for both labeled and unlabeled data through evidential deep learning. It presents an approach to dynamically balance evidence influence and class numbers on uncertainty estimation, selecting samples with the greatest uncertainty in the latter half of the learning process. Experiments show EDALSSC surpasses existing methods on image classification datasets.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised classification methods often ignore uncertainty estimation (or reliability) of prediction results during active learning, which raises questions about whether selected samples can effectively update the model.

Method: EDALSSC builds a semi-supervised learning framework that quantifies uncertainty estimation for both labeled and unlabeled data. For labeled data, it uses evidential deep learning, while for unlabeled data, it combines ignorance and conflict information from evidence using T-conorm operator. A heuristic method is also constructed to balance the influence of evidence and number of classes on uncertainty estimation. Sample selection focuses on those with the greatest uncertainty calculated as a sum when training loss increases in the latter half of the learning process.

Result: Experimental results demonstrate that EDALSSC outperforms existing semi-supervised and supervised active learning approaches on image classification datasets.

Conclusion: EDALSSC provides a novel way to incorporate uncertainty estimation into semi-supervised classification, leading to improved performance over existing methods.

Abstract: Semi-supervised classification based on active learning has made significant
progress, but the existing methods often ignore the uncertainty estimation (or
reliability) of the prediction results during the learning process, which makes
it questionable whether the selected samples can effectively update the model.
Hence, this paper proposes an evidential deep active learning approach for
semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised
learning framework to simultaneously quantify the uncertainty estimation of
labeled and unlabeled data during the learning process. The uncertainty
estimation of the former is associated with evidential deep learning, while
that of the latter is modeled by combining ignorance information and conflict
information of the evidence from the perspective of the T-conorm operator.
Furthermore, this article constructs a heuristic method to dynamically balance
the influence of evidence and the number of classes on uncertainty estimation
to ensure that it does not produce counter-intuitive results in EDALSSC. For
the sample selection strategy, EDALSSC selects the sample with the greatest
uncertainty estimation that is calculated in the form of a sum when the
training loss increases in the latter half of the learning process.
Experimental results demonstrate that EDALSSC outperforms existing
semi-supervised and supervised active learning approaches on image
classification datasets.

</details>


### [81] [Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series](https://arxiv.org/abs/2505.20697)
*Zachary C. Brown,David Carlson*

Main category: cs.LG

TL;DR: The paper presents a novel method for hypothesis generation in neuroscience that models dynamic graphs as conditionally weighted superpositions of static graphs, enabling detection of complex time-varying interactions. It improves f1-scores significantly over baselines and uncovers relationships linked to specific behavioral states in real brain data.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing machine learning methods in generating scientific hypotheses which assume causal relationships are static over time, thus restricting their applicability to systems with dynamic, state-dependent behavior like the brain.

Method: The proposed method models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships, allowing for the detection of complex, time-varying interactions between variables beyond linear limitations.

Result: The method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some experiments, with some improvements reaching well over 60%. A case study on real brain data shows its ability to uncover relationships linked to specific behavioral states.

Conclusion: This novel approach offers valuable insights into neural dynamics by effectively detecting complex, time-varying interactions in dynamic systems such as the brain.

Abstract: The field of hypothesis generation promises to reduce costs in neuroscience
by narrowing the range of interventional studies needed to study various
phenomena. Existing machine learning methods can generate scientific hypotheses
from complex datasets, but many approaches assume causal relationships are
static over time, limiting their applicability to systems with dynamic,
state-dependent behavior, such as the brain. While some techniques attempt
dynamic causal discovery through factor models, they often restrict
relationships to linear patterns or impose other simplifying assumptions. We
propose a novel method that models dynamic graphs as a conditionally weighted
superposition of static graphs, where each static graph can capture nonlinear
relationships. This approach enables the detection of complex, time-varying
interactions between variables beyond linear limitations. Our method improves
f1-scores of predicted dynamic causal patterns by roughly 22-28% on average
over baselines in some of our experiments, with some improvements reaching well
over 60%. A case study on real brain data demonstrates our method's ability to
uncover relationships linked to specific behavioral states, offering valuable
insights into neural dynamics.

</details>


### [82] [Sparsified State-Space Models are Efficient Highway Networks](https://arxiv.org/abs/2505.20698)
*Woomin Song,Jihoon Tack,Sangwoo Mo,Seunghyuk Oh,Jinwoo Shin*

Main category: cs.LG

TL;DR: The paper proposes Simba, a hierarchical sparsification method for state-space models (SSMs) based on token pruning to enhance efficiency and information flow across long sequences.


<details>
  <summary>Details</summary>
Motivation: State-space models (SSMs) have redundant tokens due to gradual recurrent updates which block the delivery of past information. Upper layers are more redundant as they encode global information while lower layers encode local information.

Method: Simba is introduced, a hierarchical sparsification method that sparsifies upper layers more than lower layers using a novel token pruning criterion measuring the global impact of tokens on the final output by accumulating local recurrences.

Result: Simba outperforms the baseline model Mamba with the same FLOPS in various natural language tasks and improves the information flow across long sequences.

Conclusion: Simba enhances SSMs by encouraging upper layers to behave like highways, improving both efficiency and information flow.

Abstract: State-space models (SSMs) offer a promising architecture for sequence
modeling, providing an alternative to Transformers by replacing expensive
self-attention with linear recurrences. In this paper, we propose a simple yet
effective trick to enhance SSMs within given computational budgets by
sparsifying them. Our intuition is that tokens in SSMs are highly redundant due
to gradual recurrent updates, and dense recurrence operations block the
delivery of past information. In particular, we observe that upper layers of
SSMs tend to be more redundant as they encode global information, while lower
layers encode local information. Motivated by this, we introduce Simba, a
hierarchical sparsification method for SSMs based on token pruning. Simba
sparsifies upper layers more than lower layers, encouraging the upper layers to
behave like highways. To achieve this, we propose a novel token pruning
criterion for SSMs, measuring the global impact of tokens on the final output
by accumulating local recurrences. We demonstrate that Simba outperforms the
baseline model, Mamba, with the same FLOPS in various natural language tasks.
Moreover, we illustrate the effect of highways, showing that Simba not only
enhances efficiency but also improves the information flow across long
sequences. Code is available at https://github.com/woominsong/Simba.

</details>


### [83] [Are Data Embeddings effective in time series forecasting?](https://arxiv.org/abs/2505.20716)
*Reza Nematirad,Anil Pahwa,Balasubramaniam Natarajan*

Main category: cs.LG

TL;DR: Through extensive ablation studies, this paper finds that removing data embedding layers from many state-of-the-art time series forecasting models does not degrade performance, and often improves accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to investigate the effectiveness of data embedding techniques in time series forecasting, given that most state-of-the-art models report only marginal improvements despite their architectural innovations.

Method: The method involved conducting extensive ablation studies across fifteen state-of-the-art models and four benchmark datasets, examining the impact of removing data embedding layers on forecasting performance.

Result: The results show that removing data embedding layers does not degrade forecasting performance in many cases, and often leads to improvements in both accuracy and computational efficiency.

Conclusion: Data embedding techniques may not be as effective as thought in time series forecasting, and their removal can lead to significant performance gains.

Abstract: Time series forecasting plays a crucial role in many real-world applications,
and numerous complex forecasting models have been proposed in recent years.
Despite their architectural innovations, most state-of-the-art models report
only marginal improvements -- typically just a few thousandths in standard
error metrics. These models often incorporate complex data embedding layers to
transform raw inputs into higher-dimensional representations to enhance
accuracy. But are data embedding techniques actually effective in time series
forecasting? Through extensive ablation studies across fifteen state-of-the-art
models and four benchmark datasets, we find that removing data embedding layers
from many state-of-the-art models does not degrade forecasting performance. In
many cases, it improves both accuracy and computational efficiency. The gains
from removing embedding layers often exceed the performance differences
typically reported between competing models. Code available at:
https://github.com/neuripsdataembedidng/DataEmbedding

</details>


### [84] [Recurrent Neural Operators: Stable Long-Term PDE Prediction](https://arxiv.org/abs/2505.20721)
*Zaijun Ye,Chen-Song Zhang,Wansheng Wang*

Main category: cs.LG

TL;DR: 为了改善神经算子在时间相关问题中的长期自回归预测能力，本文提出了循环神经算子（RNOs），通过在训练中模拟推理时的动力学过程，有效缓解暴露偏差并减少误差累积。实验表明，循环训练的多网格神经算子在长期准确性和稳定性上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 尽管神经算子在学习偏微分方程解算子方面表现出色，但在时间相关问题中，传统的训练策略如教师强制引入了训练和推理之间的不匹配，导致长期自回归预测中误差累积。

Method: 提出了一种新的框架——循环神经算子（RNOs），将循环训练整合到神经算子架构中。RNOs在训练过程中递归地应用算子于自身的预测结果，而非依赖真实输入数据，从而在训练中模拟推理时的动力学行为。此外，理论分析显示循环训练可将教师强制下的指数误差增长降低为线性增长。

Result: 实验证明，采用循环训练的多网格神经算子在标准基准测试中，长期准确性和稳定性均显著优于使用教师强制训练的算子。

Conclusion: 本文强调了在神经算子学习中对齐训练与推理动力学的重要性，以实现更鲁棒的时间泛化性能。

Abstract: Neural operators have emerged as powerful tools for learning solution
operators of partial differential equations. However, in time-dependent
problems, standard training strategies such as teacher forcing introduce a
mismatch between training and inference, leading to compounding errors in
long-term autoregressive predictions. To address this issue, we propose
Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent
training into neural operator architectures. Instead of conditioning each
training step on ground-truth inputs, RNOs recursively apply the operator to
their own predictions over a temporal window, effectively simulating
inference-time dynamics during training. This alignment mitigates exposure bias
and enhances robustness to error accumulation. Theoretically, we show that
recurrent training can reduce the worst-case exponential error growth typical
of teacher forcing to linear growth. Empirically, we demonstrate that
recurrently trained Multigrid Neural Operators significantly outperform their
teacher-forced counterparts in long-term accuracy and stability on standard
benchmarks. Our results underscore the importance of aligning training with
inference dynamics for robust temporal generalization in neural operator
learning.

</details>


### [85] [A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs](https://arxiv.org/abs/2505.20725)
*Alberto Pliego Marugán,Jesús M. Pinar-Pérez,Fausto Pedro García Márquez*

Main category: cs.LG

TL;DR: In this paper, a gamma degradation process and a novel maintenance model with increasingly imperfect repairs are proposed. A reinforcement-learning-based agent using Double Deep Q-Network architecture is developed to generate maintenance policies for the system without predefined preventive threshold in continuous degradation state space. The agent shows great flexibility in different scenarios and improves long-run cost compared to other strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges of maintenance optimization in the context of Industry 4.0 by proposing a new paradigm that incorporates machine learning techniques, specifically reinforcement learning, to efficiently handle the degradational behavior of real-world systems.

Method: The authors propose a gamma degradation process and a novel maintenance model where repairs become increasingly imperfect. They develop a reinforcement-learning-based agent using Double Deep Q-Network architecture to generate maintenance policies. This agent operates without a predefined preventive threshold and can handle a continuous degradation state space.

Result: The agent learns to behave optimally in different scenarios, demonstrating great flexibility. An analysis of the main parameters affecting the maintenance policy is performed, showing the effectiveness of the approach.

Conclusion: The proposed approach is appropriate and significantly improves long-run costs compared to other common maintenance strategies.

Abstract: Efficient maintenance has always been essential for the successful
application of engineering systems. However, the challenges to be overcome in
the implementation of Industry 4.0 necessitate new paradigms of maintenance
optimization. Machine learning techniques are becoming increasingly used in
engineering and maintenance, with reinforcement learning being one of the most
promising. In this paper, we propose a gamma degradation process together with
a novel maintenance model in which repairs are increasingly imperfect, i.e.,
the beneficial effect of system repairs decreases as more repairs are
performed, reflecting the degradational behavior of real-world systems. To
generate maintenance policies for this system, we developed a
reinforcement-learning-based agent using a Double Deep Q-Network architecture.
This agent presents two important advantages: it works without a predefined
preventive threshold, and it can operate in a continuous degradation state
space. Our agent learns to behave in different scenarios, showing great
flexibility. In addition, we performed an analysis of how changes in the main
parameters of the environment affect the maintenance policy proposed by the
agent. The proposed approach is demonstrated to be appropriate and to
significatively improve long-run cost as compared with other common maintenance
strategies.

</details>


### [86] [Adversarial bandit optimization for approximately linear functions](https://arxiv.org/abs/2505.20734)
*Zhuoyu Cheng,Kohei Hatano,Eiji Takimoto*

Main category: cs.LG

TL;DR: The paper explores a bandit optimization problem for nonconvex, non-smooth functions and provides expected and high probability regret bounds, improving upon existing results in bandit linear optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing nonconvex and non-smooth functions in a bandit setting with perturbations.

Method: Providing both expected and high probability regret bounds for the bandit optimization problem involving nonconvex and non-smooth functions. The analysis also includes a special case of bandit linear optimization without perturbations.

Result: Achieved regret bounds for the general problem and an improved high-probability regret bound for the special case of bandit linear optimization.

Conclusion: This study contributes new regret bounds for nonconvex, non-smooth bandit optimization and improves understanding in the specific area of bandit linear optimization.

Abstract: We consider a bandit optimization problem for nonconvex and non-smooth
functions, where in each trial the loss function is the sum of a linear
function and a small but arbitrary perturbation chosen after observing the
player's choice. We give both expected and high probability regret bounds for
the problem. Our result also implies an improved high-probability regret bound
for the bandit linear optimization, a special case with no perturbation. We
also give a lower bound on the expected regret.

</details>


### [87] [Detecting Informative Channels: ActionFormer](https://arxiv.org/abs/2505.20739)
*Kunpeng Zhao,Asahi Miyazaki,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: This paper modifies ActionFormer for sensor-based Human Activity Recognition (HAR) using Sequence-and-Excitation strategy and swish activation function, achieving a 16.01% improvement in mAP on the WEAR dataset.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models have advanced HAR, with ActionFormer providing activity labels and borders from image/video inputs. The model is extended to sensor signals but faces challenges capturing subtle temporal changes and interdependencies between spatial and temporal features.

Method: The modified ActionFormer uses Sequence-and-Excitation strategy to minimize additional parameters and employs swish activation function to retain negative range direction information, specifically tailored for sensor signal inputs.

Result: Experiments on the WEAR dataset demonstrate substantial improvement with a 16.01% increase in average mAP for inertial data.

Conclusion: The proposed modifications to ActionFormer effectively address limitations in capturing temporal dynamics and spatial-temporal feature interdependencies for sensor-based HAR.

Abstract: Human Activity Recognition (HAR) has recently witnessed advancements with
Transformer-based models. Especially, ActionFormer shows us a new perspectives
for HAR in the sense that this approach gives us additional outputs which
detect the border of the activities as well as the activity labels.
ActionFormer was originally proposed with its input as image/video. However,
this was converted to with its input as sensor signals as well. We analyze this
extensively in terms of deep learning architectures. Based on the report of
high temporal dynamics which limits the model's ability to capture subtle
changes effectively and of the interdependencies between the spatial and
temporal features. We propose the modified ActionFormer which will decrease
these defects for sensor signals. The key to our approach lies in accordance
with the Sequence-and-Excitation strategy to minimize the increase in
additional parameters and opt for the swish activation function to retain the
information about direction in the negative range. Experiments on the WEAR
dataset show that our method achieves substantial improvement of a 16.01\% in
terms of average mAP for inertial data.

</details>


### [88] ['Hello, World!': Making GNNs Talk with LLMs](https://arxiv.org/abs/2505.20742)
*Sunwoo Kim,Soo Yong Lee,Jaemin Yoo,Kijung Shin*

Main category: cs.LG

TL;DR: This paper proposes Graph Lingual Network (GLN), a GNN based on LLMs with human-readable text hidden representations. It incorporates GNN techniques like graph attention and initial residual connection, providing comprehensible insights into GNN operations. GLN shows strong zero-shot performance in node classification and link prediction.


<details>
  <summary>Details</summary>
Motivation: To make GNNs more interpretable by creating a model with human-readable hidden representations.

Method: Designing GLN using LLMs and incorporating GNN techniques such as message passing, graph attention, and initial residual connection through prompt design.

Result: GLN offers an intuitive understanding of node representation changes across layers and under advanced GNN techniques, while achieving better zero-shot performance than existing LLM-based methods in node classification and link prediction.

Conclusion: GLN not only enhances the interpretability of GNNs but also improves zero-shot performance in certain tasks.

Abstract: While graph neural networks (GNNs) have shown remarkable performance across
diverse graph-related tasks, their high-dimensional hidden representations
render them black boxes. In this work, we propose Graph Lingual Network (GLN),
a GNN built on large language models (LLMs), with hidden representations in the
form of human-readable text. Through careful prompt design, GLN incorporates
not only the message passing module of GNNs but also advanced GNN techniques,
including graph attention and initial residual connection. The
comprehensibility of GLN's hidden representations enables an intuitive analysis
of how node representations change (1) across layers and (2) under advanced GNN
techniques, shedding light on the inner workings of GNNs. Furthermore, we
demonstrate that GLN achieves strong zero-shot performance on node
classification and link prediction, outperforming existing LLM-based baseline
methods.

</details>


### [89] [Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction](https://arxiv.org/abs/2505.20755)
*Yifei Wang,Weimin Bai,Colin Zhang,Debing Zhang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: 本论文通过提出的扩散扩展理论，统一了超过10种现有的一步扩散蒸馏方法，并在CIFAR10和ImageNet-$64\times 64$生成基准上取得了破纪录的FID值。此外，该方法在文本到3D生成等更广泛的任务中也表现良好，为未来的研究提供了坚实的理论和实证基础。


<details>
  <summary>Details</summary>
Motivation: 作者提出了扩散扩展理论（diffusion expansion theory）来解释$f$-divergence族的行为，这是统一步骤扩散方法的基础。这种动机帮助克服了原始扩展$f$-divergence的不可处理性问题。

Method: 提出了一种名为Uni-Instruct的理论驱动框架，该框架统一了多种现有的一步扩散蒸馏方法。通过引入关键理论，解决了原始扩展$f$-divergence的不可处理性问题，从而得出了一个等效且可处理的损失函数，用于有效训练一步扩散模型。

Result: 在CIFAR10数据集上分别实现了无条件生成FID值1.46和有条件生成FID值1.38；在ImageNet-$64\times 64$生成基准上达到了新的最佳一步生成FID值1.02，显著优于其79步教师扩散模型。在文本到3D生成任务中，Uni-Instruct在生成质量和多样性方面均略胜于先前的方法。

Conclusion: Uni-Instruct不仅提供新的理论贡献以帮助理解现有方法，而且在一步扩散生成性能上处于领先地位。这些坚实的理论和实证贡献将可能对未来的一步扩散蒸馏和扩散模型知识转移研究产生积极影响。

Abstract: In this paper, we unify more than 10 existing one-step diffusion distillation
approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a
theory-driven framework which we name the \textbf{\emph{Uni-Instruct}}.
Uni-Instruct is motivated by our proposed diffusion expansion theory of the
$f$-divergence family. Then we introduce key theories that overcome the
intractability issue of the original expanded $f$-divergence, resulting in an
equivalent yet tractable loss that effectively trains one-step diffusion models
by minimizing the expanded $f$-divergence family. The novel unification
introduced by Uni-Instruct not only offers new theoretical contributions that
help understand existing approaches from a high-level perspective but also
leads to state-of-the-art one-step diffusion generation performances. On the
CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet
Inception Distance (FID) values of \textbf{\emph{1.46}} for unconditional
generation and \textbf{\emph{1.38}} for conditional generation. On the
ImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA
one-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-step
teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).
We also apply Uni-Instruct on broader tasks like text-to-3D generation. For
text-to-3D generation, Uni-Instruct gives decent results, which slightly
outperforms previous methods, such as SDS and VSD, in terms of both generation
quality and diversity. Both the solid theoretical and empirical contributions
of Uni-Instruct will potentially help future studies on one-step diffusion
distillation and knowledge transferring of diffusion models.

</details>


### [90] [Practical estimation of the optimal classification error with soft labels and calibration](https://arxiv.org/abs/2505.20761)
*Ryota Ushio,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 尽管近年来机器学习系统性能显著提高，但对其基本问题关注较少：我们能在多大程度上改进模型？本文在二元分类设置中提供了回答此问题的方法。我们通过研究硬标签估计器偏差的性质和解决带有损坏软标签的估计问题，扩展了先前利用软标签估计贝叶斯误差的工作。我们的方法不依赖于实例，并通过实验证明了其有效性和理论支持。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习系统性能提升显著，但对于模型改进潜力的基本问题关注较少。因此，需要一种实用且有理论支持的方法来评估二元分类中的最佳错误率（贝叶斯误差）。

Method: 1. 理论研究硬标签估计器偏差的性质，揭示其衰减速度与两类条件分布分离程度的关系。2. 解决带有损坏软标签的估计问题，证明即使完美校准的软标签也可能导致不准确估计，并提出使用等距校准在较弱假设下提供统计一致估计器。3. 方法不依赖于具体实例，适合隐私敏感场景。

Result: 通过合成和真实世界数据集的实验，验证了所提出方法的有效性和理论正确性。

Conclusion: 本文扩展了先前工作，提供了更深入的理论分析和更广泛适用的方法，解决了硬标签和损坏软标签下的贝叶斯误差估计问题，且方法不依赖具体实例，具有实际应用价值。

Abstract: While the performance of machine learning systems has experienced significant
improvement in recent years, relatively little attention has been paid to the
fundamental question: to what extent can we improve our models? This paper
provides a means of answering this question in the setting of binary
classification, which is practical and theoretically supported. We extend a
previous work that utilizes soft labels for estimating the Bayes error, the
optimal error rate, in two important ways. First, we theoretically investigate
the properties of the bias of the hard-label-based estimator discussed in the
original work. We reveal that the decay rate of the bias is adaptive to how
well the two class-conditional distributions are separated, and it can decay
significantly faster than the previous result suggested as the number of hard
labels per instance grows. Second, we tackle a more challenging problem
setting: estimation with corrupted soft labels. One might be tempted to use
calibrated soft labels instead of clean ones. However, we reveal that
calibration guarantee is not enough, that is, even perfectly calibrated soft
labels can result in a substantially inaccurate estimate. Then, we show that
isotonic calibration can provide a statistically consistent estimator under an
assumption weaker than that of the previous work. Our method is instance-free,
i.e., we do not assume access to any input instances. This feature allows it to
be adopted in practical scenarios where the instances are not available due to
privacy issues. Experiments with synthetic and real-world datasets show the
validity of our methods and theory.

</details>


### [91] [Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies](https://arxiv.org/abs/2505.20765)
*Kohei Obata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: RedLamp is a new method for unsupervised anomaly detection in time series that uses diverse data augmentations to generate multiclass pseudo-anomalies and conducts multiclass classification using soft labels, demonstrating effectiveness and robustness against anomaly contamination.


<details>
  <summary>Details</summary>
Motivation: Current approaches for unsupervised anomaly detection in time series suffer from limitations such as difficulty in covering a wide range of anomalies, disregarding augmented samples resembling normal samples, and over-trusting the labels of training and augmented samples.

Method: RedLamp employs diverse data augmentations to generate multiclass pseudo-anomalies and learns the multiclass boundary. It conducts multiclass classification using soft labels to prevent overconfidence and ensure robustness against contaminated/false anomalies.

Result: Extensive experiments show the effectiveness of RedLamp in anomaly detection and its robustness against anomaly contamination.

Conclusion: RedLamp provides a promising approach for unsupervised anomaly detection in time series by addressing the limitations of existing methods.

Abstract: Unsupervised anomaly detection in time series has been a pivotal research
area for decades. Current mainstream approaches focus on learning normality, on
the assumption that all or most of the samples in the training set are normal.
However, anomalies in the training set (i.e., anomaly contamination) can be
misleading. Recent studies employ data augmentation to generate
pseudo-anomalies and learn the boundary separating the training samples from
the augmented samples. Although this approach mitigates anomaly contamination
if augmented samples mimic unseen real anomalies, it suffers from several
limitations. (1) Covering a wide range of time series anomalies is challenging.
(2) It disregards augmented samples that resemble normal samples (i.e., false
anomalies). (3) It places too much trust in the labels of training and
augmented samples. In response, we propose RedLamp, which employs diverse data
augmentations to generate multiclass pseudo-anomalies and learns the multiclass
boundary. Such multiclass pseudo-anomalies cover a wide variety of time series
anomalies. We conduct multiclass classification using soft labels, which
prevents the model from being overconfident and ensures its robustness against
contaminated/false anomalies. The learned latent space is inherently
explainable as it is trained to separate pseudo-anomalies into multiclasses.
Extensive experiments demonstrate the effectiveness of RedLamp in anomaly
detection and its robustness against anomaly contamination.

</details>


### [92] [TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state](https://arxiv.org/abs/2505.20774)
*Xiaowen Ma,Zhenliang Ni,Shuai Xiao,Xinghao Chen*

Main category: cs.LG

TL;DR: In long-term time series forecasting, the multi-delay issue complicates variable relationships. The proposed TimePro model constructs variate- and time-aware hyper-states to capture complex relationships and temporal information, achieving competitive performance on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional models for long-term time series forecasting process all variables or time points uniformly, limiting their ability to capture complex variable relationships and obtain non-trivial time representations due to the multi-delay issue.

Method: TimePro is an innovative Mamba-based model that constructs variate- and time-aware hyper-states. It preserves fine-grained temporal features of each variate token and adaptively selects focused time points to tune the plain state, thus reconstructing hyper-states that can perceive both variable relationships and salient temporal information.

Result: TimePro performs competitively on eight real-world long-term forecasting benchmarks with satisfactory linear complexity.

Conclusion: TimePro addresses the multi-delay issue in long-term time series forecasting by effectively capturing complex variable relationships and temporal information, leading to accurate forecasting.

Abstract: In long-term time series forecasting, different variables often influence the
target variable over distinct time intervals, a challenge known as the
multi-delay issue. Traditional models typically process all variables or time
points uniformly, which limits their ability to capture complex variable
relationships and obtain non-trivial time representations. To address this
issue, we propose TimePro, an innovative Mamba-based model that constructs
variate- and time-aware hyper-states. Unlike conventional approaches that
merely transfer plain states across variable or time dimensions, TimePro
preserves the fine-grained temporal features of each variate token and
adaptively selects the focused time points to tune the plain state. The
reconstructed hyper-state can perceive both variable relationships and salient
temporal information, which helps the model make accurate forecasting. In
experiments, TimePro performs competitively on eight real-world long-term
forecasting benchmarks with satisfactory linear complexity. Code is available
at https://github.com/xwmaxwma/TimePro.

</details>


### [93] [Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI](https://arxiv.org/abs/2505.20775)
*Fabian Scheurer,Alexander Hammer,Mario Schubert,Robert-Patrick Steiner,Oliver Gamm,Kaomei Guan,Frank Sonntag,Hagen Malberg,Martin Schmidt*

Main category: cs.LG

TL;DR: The study developed a non-invasive AI-based method to assess the maturity of human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) using video-based motion analysis. The model achieved high accuracy and identified key features for maturity assessment.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing iPSC-CM maturation are time-consuming and can cause cell damage or loss. There is a need for a non-invasive approach to evaluate the maturity of iPSC-CMs efficiently and without damaging the cells.

Method: The researchers used interpretable artificial intelligence (AI)-based analysis of beat characteristics derived from video-based motion analysis. They evaluated 230 video recordings of iPSC-CMs at different stages of maturity, extracted 10 features using Maia motion analysis software, and input them into a support vector machine (SVM). Hyperparameters were optimized using grid search and cross-validation.

Result: The optimized SVM model achieved an accuracy of 99.5 ± 1.1 % on a hold-out test set. SHAP analysis identified displacement, relaxation-rise time, and beating duration as the most relevant features for assessing maturity level.

Conclusion: Non-invasive optical motion analysis combined with AI-based methods can be effectively used to assess iPSC-CM maturity. This approach may reduce variability and improve reproducibility in experimental studies involving iPSC-CMs.

Abstract: Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an
important resource for the identification of new therapeutic targets and
cardioprotective drugs. After differentiation iPSC-CMs show an immature,
fetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation
medium (MM) strongly enhances their structural, metabolic and functional
phenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging
as most methods are time consuming and go in line with cell damage or loss of
the sample. To address this issue, we developed a non-invasive approach for
automated classification of iPSC-CM maturity through interpretable artificial
intelligence (AI)-based analysis of beat characteristics derived from
video-based motion analysis. In a prospective study, we evaluated 230 video
recordings of early-state, immature iPSC-CMs on day 21 after differentiation
(d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10
features were extracted using Maia motion analysis software and entered into a
support vector machine (SVM). The hyperparameters of the SVM were optimized in
a grid search on 80 % of the data using 5-fold cross-validation. The optimized
model achieved an accuracy of 99.5 $\pm$ 1.1 % on a hold-out test set. Shapley
Additive Explanations (SHAP) identified displacement, relaxation-rise time and
beating duration as the most relevant features for assessing maturity level.
Our results suggest the use of non-invasive, optical motion analysis combined
with AI-based methods as a tool to assess iPSC-CMs maturity and could be
applied before performing functional readouts or drug testing. This may
potentially reduce the variability and improve the reproducibility of
experimental studies.

</details>


### [94] [Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification](https://arxiv.org/abs/2505.20797)
*Antonio Tudisco,Deborah Volpe,Giovanna Turvani*

Main category: cs.LG

TL;DR: 机器学习在疾病诊断中发挥了重要作用，但传统模型在处理类别不平衡问题时效果受限。量子模型因其能表达复杂模式和映射到高维计算空间的能力，成为解决这一问题的新希望。


<details>
  <summary>Details</summary>
Motivation: 准确可靠的疾病诊断对及时医疗和提高患者生存率至关重要，然而传统分类模型在处理显著类别不平衡问题时效果不佳。

Method: 研究量子模型在疾病诊断中的应用，利用其将数据映射到高维计算空间的能力来克服传统模型的限制。

Result: 量子模型有望克服传统模型在处理复杂模式和类别不平衡问题上的局限性，从而提高疾病诊断的准确性。

Conclusion: 量子模型为疾病诊断提供了新的可能性，未来可进一步探索其潜力以改善医疗实践。

Abstract: Accurate and reliable diagnosis of diseases is crucial in enabling timely
medical treatment and enhancing patient survival rates. In recent years,
Machine Learning has revolutionized diagnostic practices by creating
classification models capable of identifying diseases. However, these
classification problems often suffer from significant class imbalances, which
can inhibit the effectiveness of traditional models. Therefore, the interest in
Quantum models has arisen, driven by the captivating promise of overcoming the
limitations of the classical counterpart thanks to their ability to express
complex patterns by mapping data in a higher-dimensional computational space.

</details>


### [95] [Leaner Transformers: More Heads, Less Depth](https://arxiv.org/abs/2505.20802)
*Hemanth Saratchandran,Damien Teney,Simon Lucey*

Main category: cs.LG

TL;DR: Transformers have been successful with large models, but this paper challenges that notion by showing many existing transformers might be unnecessarily oversized. By increasing the number of heads and improving the conditioning of the attention block, model depth can be decreased while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to challenge the ideology that 'bigger means better' in transformer models and show that many existing transformers might be unnecessarily oversized.

Method: The method involves discovering a theoretical principle that redefines the role of multi-head attention, specifically exploiting the benefit of multiple heads in improving the conditioning of the attention block. This insight is used to redesign popular architectures with an increased number of heads.

Result: The improvement in the conditioning proves significant in practice, allowing for a reduction in model depth and a decrease in parameter count by up to 30-50% while maintaining accuracy.

Conclusion: The conclusion is that it's possible to reduce the size of transformer models without sacrificing performance by increasing the number of heads and improving the conditioning of the attention block.

Abstract: Transformers have reshaped machine learning by utilizing attention mechanisms
to capture complex patterns in large datasets, leading to significant
improvements in performance. This success has contributed to the belief that
"bigger means better", leading to ever-increasing model sizes. This paper
challenge this ideology by showing that many existing transformers might be
unnecessarily oversized. We discover a theoretical principle that redefines the
role of multi-head attention. An important benefit of the multiple heads is in
improving the conditioning of the attention block. We exploit this theoretical
insight and redesign popular architectures with an increased number of heads.
The improvement in the conditioning proves so significant in practice that
model depth can be decreased, reducing the parameter count by up to 30-50%
while maintaining accuracy. We obtain consistent benefits across a variety of
transformer-based architectures of various scales, on tasks in computer vision
(ImageNet-1k) as well as language and sequence modeling (GLUE benchmark,
TinyStories, and the Long-Range Arena benchmark).

</details>


### [96] [Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models](https://arxiv.org/abs/2505.20804)
*Antonio Tudisco,Deborah Volpe,Giovanna Turvani*

Main category: cs.LG

TL;DR: Quantum classifiers, specifically QSVMs, outperform classical models and QNNs in handling imbalanced healthcare datasets, showcasing potential for future research.


<details>
  <summary>Details</summary>
Motivation: Effective disease diagnosis is crucial but challenging due to imbalanced datasets. Quantum models may offer solutions by leveraging higher-dimensional computational space.

Method: Evaluate quantum classifiers (QNNs and QSVMs) against classical models on three healthcare datasets (Prostate Cancer, Heart Failure, Diabetes) with focus on imbalanced data scenarios.

Result: QSVMs outperform QNNs due to less overfitting and surpass classical models in highly imbalanced datasets.

Conclusion: Quantum models hold promise for healthcare classification tasks, especially with imbalanced data, warranting further investigation.

Abstract: Effective and accurate diagnosis of diseases such as cancer, diabetes, and
heart failure is crucial for timely medical intervention and improving patient
survival rates. Machine learning has revolutionized diagnostic methods in
recent years by developing classification models that detect diseases based on
selected features. However, these classification tasks are often highly
imbalanced, limiting the performance of classical models. Quantum models offer
a promising alternative, exploiting their ability to express complex patterns
by operating in a higher-dimensional computational space through superposition
and entanglement. These unique properties make quantum models potentially more
effective in addressing the challenges of imbalanced datasets. This work
evaluates the potential of quantum classifiers in healthcare, focusing on
Quantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs),
comparing them with popular classical models. The study is based on three
well-known healthcare datasets -- Prostate Cancer, Heart Failure, and Diabetes.
The results indicate that QSVMs outperform QNNs across all datasets due to
their susceptibility to overfitting. Furthermore, quantum models prove the
ability to overcome classical models in scenarios with high dataset imbalance.
Although preliminary, these findings highlight the potential of quantum models
in healthcare classification tasks and lead the way for further research in
this domain.

</details>


### [97] [Simple yet Effective Graph Distillation via Clustering](https://arxiv.org/abs/2505.20807)
*Yurui Lai,Taiyan Zhang,Renchi Yang*

Main category: cs.LG

TL;DR: 尽管图神经网络（GNN）在许多领域取得了丰富的成功，但其训练仍因实际应用中大规模图所需的巨大计算开销而面临挑战。本文提出了一个高效且有效的图数据蒸馏（GDD）方法——ClustGDD，通过快速、理论支持的聚类合成压缩图和节点属性，同时利用类别感知图采样和一致性损失进行小幅度增强，从而显著提升结果质量和效率。实验表明，使用ClustGDD生成的压缩图训练GNN，在五个基准数据集上的节点分类任务中表现优于或媲美现有最佳GDD方法，同时速度快数个数量级。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的GDD方法依赖于启发式策略，如对齐模型梯度或表示分布，这可能导致结果质量下降、昂贵的大型图蒸馏训练成本，或者两者兼有。因此需要一种更高效且有效的方法来解决这些问题。

Method: ClustGDD通过快速且理论支持的聚类方法合成压缩图和节点属性，该方法最小化簇内平方和并最大化原始图上的同质性。此外，为了缓解基于同质性的聚类可能带来的不良影响，ClustGDD利用类别感知图采样和一致性损失学习一个小幅度增强，以优化压缩图的节点属性。

Result: 广泛的实验表明，使用ClustGDD生成的压缩图训练的GNN在五个基准数据集上的节点分类任务中，性能始终优于或媲美最先进的GDD方法，同时速度要快几个数量级。

Conclusion: ClustGDD是一种高效且有效的GDD方法，能够在保证GNN性能的同时大幅降低训练时间，为大规模图上的GNN训练提供了一种新思路。

Abstract: Despite plentiful successes achieved by graph representation learning in
various domains, the training of graph neural networks (GNNs) still remains
tenaciously challenging due to the tremendous computational overhead needed for
sizable graphs in practice. Recently, graph data distillation (GDD), which
seeks to distill large graphs into compact and informative ones, has emerged as
a promising technique to enable efficient GNN training. However, most existing
GDD works rely on heuristics that align model gradients or representation
distributions on condensed and original graphs, leading to compromised result
quality, expensive training for distilling large graphs, or both. Motivated by
this, this paper presents an efficient and effective GDD approach, ClustGDD.
Under the hood, ClustGDD resorts to synthesizing the condensed graph and node
attributes through fast and theoretically-grounded clustering that minimizes
the within-cluster sum of squares and maximizes the homophily on the original
graph. The fundamental idea is inspired by our empirical and theoretical
findings unveiling the connection between clustering and empirical condensation
quality using Fr\'echet Inception Distance, a well-known quality metric for
synthetic images. Furthermore, to mitigate the adverse effects caused by the
homophily-based clustering, ClustGDD refines the nodal attributes of the
condensed graph with a small augmentation learned via class-aware graph
sampling and consistency loss. Our extensive experiments exhibit that GNNs
trained over condensed graphs output by ClustGDD consistently achieve superior
or comparable performance to state-of-the-art GDD methods in terms of node
classification on five benchmark datasets, while being orders of magnitude
faster.

</details>


### [98] [Interpretable Credit Default Prediction with Ensemble Learning and SHAP](https://arxiv.org/abs/2505.20815)
*Shiqi Yang,Ziyi Huang,Wengran Xiao,Xinyu Shen*

Main category: cs.LG

TL;DR: This study focuses on credit default prediction using machine learning models, conducts comparative experiments on classification algorithms, and uses SHAP method for feature analysis.


<details>
  <summary>Details</summary>
Motivation: To provide effective reference and technical support for the intelligent development of credit risk control systems.

Method: Build a modeling framework based on machine learning, conduct comparative experiments on mainstream classification algorithms, use SHAP method for feature analysis.

Result: Ensemble learning method has obvious advantages in predictive performance, external credit score variable plays a dominant role in model decision making.

Conclusion: The research results provide effective reference and technical support for the intelligent development of credit risk control systems.

Abstract: This study focuses on the problem of credit default prediction, builds a
modeling framework based on machine learning, and conducts comparative
experiments on a variety of mainstream classification algorithms. Through
preprocessing, feature engineering, and model training of the Home Credit
dataset, the performance of multiple models including logistic regression,
random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and
recall is evaluated. The results show that the ensemble learning method has
obvious advantages in predictive performance, especially in dealing with
complex nonlinear relationships between features and data imbalance problems.
It shows strong robustness. At the same time, the SHAP method is used to
analyze the importance and dependency of features, and it is found that the
external credit score variable plays a dominant role in model decision making,
which helps to improve the model's interpretability and practical application
value. The research results provide effective reference and technical support
for the intelligent development of credit risk control systems.

</details>


### [99] [HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling](https://arxiv.org/abs/2505.20836)
*Hexiong Yang,Mingrui Chen,Huaibo Huang,Junxian Duan,Jie Cao,Zhen Zhou,Ran He*

Main category: cs.LG

TL;DR: The paper proposes Hybrid Architecture Distillation (HAD) for efficient DNA sequence modeling pre-training, surpassing larger models in some tasks.


<details>
  <summary>Details</summary>
Motivation: Previous DNA sequence modeling methods either require large amounts of data or complex models with many parameters, leading to high computational costs. Compact models have been attempted but still fall short.

Method: Proposes HAD approach using distillation and reconstruction tasks with NTv2-500M as teacher model and a grouping masking strategy during MLM pre-training.

Result: Achieved excellent performance compared to similar parameter models and surprisingly surpassed the much larger teacher model in some sub-tasks.

Conclusion: HAD is an effective method for DNA sequence modeling pre-training, offering both efficiency and superior performance.

Abstract: Inspired by the great success of Masked Language Modeling (MLM) in the
natural language domain, the paradigm of self-supervised pre-training and
fine-tuning has also achieved remarkable progress in the field of DNA sequence
modeling. However, previous methods often relied on massive pre-training data
or large-scale base models with huge parameters, imposing a significant
computational burden. To address this, many works attempted to use more compact
models to achieve similar outcomes but still fell short by a considerable
margin. In this work, we propose a Hybrid Architecture Distillation (HAD)
approach, leveraging both distillation and reconstruction tasks for more
efficient and effective pre-training. Specifically, we employ the NTv2-500M as
the teacher model and devise a grouping masking strategy to align the feature
embeddings of visible tokens while concurrently reconstructing the invisible
tokens during MLM pre-training. To validate the effectiveness of our proposed
method, we conducted comprehensive experiments on the Nucleotide Transformer
Benchmark and Genomic Benchmark. Compared to models with similar parameters,
our model achieved excellent performance. More surprisingly, it even surpassed
the distillation ceiling-teacher model on some sub-tasks, which is more than
500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization,
which shows that our model can gain a sophisticated understanding of the
intrinsic representation pattern in genomic sequences.

</details>


### [100] [FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration](https://arxiv.org/abs/2505.20839)
*Daehyeon Baek,Jieun Choi,Jimyoung Son,Kyungmin Bin,Seungbeom Choi,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.LG

TL;DR: FireQ is a PTQ framework and INT4-FP8 matrix multiplication kernel that accelerates LLM inference with novel quantization techniques, achieving significant speedups on Llama models while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Memory bandwidth constraints significantly limit inference throughput of large language models, motivating the need for post-training quantization (PTQ) to enhance performance.

Method: FireQ proposes a co-designed PTQ framework and an INT4-FP8 matrix multiplication kernel. It quantizes linear layer weights and key-values to INT4, activations and queries to FP8. A three-stage pipelining for the prefill phase modifies the FlashAttention-3 kernel. Novel outlier smoothing techniques are developed separately for linear and attention layers to minimize accuracy loss from quantization.

Result: FireQ outperforms state-of-the-art methods, achieving 1.68x faster inference in feed-forward network layers on Llama2-7B and 1.26x faster prefill phase performance on Llama3-8B compared to QServe, with negligible accuracy loss.

Conclusion: FireQ effectively accelerates LLM inference across all linear layers through its unique quantization approach and pipelining strategy, offering significant speedups with minimal impact on accuracy.

Abstract: As large language models become increasingly prevalent, memory bandwidth
constraints significantly limit inference throughput, motivating post-training
quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ
framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM
inference across all linear layers. Specifically, FireQ quantizes linear layer
weights and key-values to INT4, and activations and queries to FP8,
significantly enhancing throughput. Additionally, we introduce a three-stage
pipelining for the prefill phase, which modifies the FlashAttention-3 kernel,
effectively reducing time-to-first-token in the prefill phase. To minimize
accuracy loss from quantization, we develop novel outlier smoothing techniques
tailored separately for linear and attention layers. In linear layers, we
explicitly use per-tensor scaling to prevent underflow caused by the FP8
quantization scaling factor of INT4 quantization, and channel-wise scaling to
compensate for coarse granularity of INT4. In attention layers, we address
quantization challenges posed by rotary positional embeddings (RoPE) by
combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly
outperforms state-of-the-art methods, achieving 1.68x faster inference in
feed-forward network layers on Llama2-7B and 1.26x faster prefill phase
performance on Llama3-8B compared to QServe, with negligible accuracy loss.

</details>


### [101] [Aggregation Buffer: Revisiting DropEdge with a New Parameter Block](https://arxiv.org/abs/2505.20840)
*Dooho Lee,Myeong Kong,Sagad Hamid,Cheonwoo Lee,Jaemin Yoo*

Main category: cs.LG

TL;DR: The paper re-examines DropEdge, identifies its limitations through theoretical analysis, and proposes Aggregation Buffer to improve GNN robustness. This new method enhances performance on various datasets and addresses issues like degree bias.


<details>
  <summary>Details</summary>
Motivation: DropEdge, a data augmentation technique for Graph Neural Networks (GNNs), has shown promise in reducing overfitting by randomly removing edges during training. However, the authors observe that its potential performance gain in supervised learning tasks is significantly limited.

Method: The authors conduct a theoretical analysis of DropEdge's limitations and propose Aggregation Buffer, a parameter block designed to enhance GNN robustness. It works by addressing the fundamental limitation present in many GNN architectures that restrict DropEdge's effectiveness.

Result: Aggregation Buffer demonstrates consistent performance improvements across multiple datasets when integrated with any GNN model. Additionally, it effectively mitigates known problems such as degree bias or structural disparity.

Conclusion: Aggregation Buffer offers a unifying solution to enhance GNN robustness and performance, overcoming the limitations of DropEdge while being compatible with any GNN model.

Abstract: We revisit DropEdge, a data augmentation technique for GNNs which randomly
removes edges to expose diverse graph structures during training. While being a
promising approach to effectively reduce overfitting on specific connections in
the graph, we observe that its potential performance gain in supervised
learning tasks is significantly limited. To understand why, we provide a
theoretical analysis showing that the limited performance of DropEdge comes
from the fundamental limitation that exists in many GNN architectures. Based on
this analysis, we propose Aggregation Buffer, a parameter block specifically
designed to improve the robustness of GNNs by addressing the limitation of
DropEdge. Our method is compatible with any GNN model, and shows consistent
performance improvements on multiple datasets. Moreover, our method effectively
addresses well-known problems such as degree bias or structural disparity as a
unifying solution. Code and datasets are available at
https://github.com/dooho00/agg-buffer.

</details>


### [102] [Cooperation of Experts: Fusing Heterogeneous Information with Large Margin](https://arxiv.org/abs/2505.20853)
*Shuo Wang,Shunyang Huang,Jinghui Yuan,Zhixiang Shen,Zhao Kang*

Main category: cs.LG

TL;DR: 提出了一种名为Cooperation of Experts (CoE)的框架，用于将多类型信息编码到统一的异构多层网络中。该框架通过专门的编码器作为领域特定专家，协作提取互补知识，并通过新的大边距机制增强鲁棒性。理论分析和实验验证了其可行性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据分析中融合异构信息是一个持续的挑战，现有方法往往无法充分考虑不同语义空间中对象模式的固有异质性。

Method: 提出了Cooperation of Experts (CoE)框架，使用领域特定的编码器（专家）来学习不同语义空间中的关系模式，并通过新颖的大边距机制和定制优化策略进行协作。

Result: 广泛的实验和严格的理论分析证明了框架的可行性、稳定性和优越性能。

Conclusion: CoE提供了一个强大而灵活的模型，能够捕捉现实世界复杂数据的复杂结构，并在多个基准测试中表现出色。

Abstract: Fusing heterogeneous information remains a persistent challenge in modern
data analysis. While significant progress has been made, existing approaches
often fail to account for the inherent heterogeneity of object patterns across
different semantic spaces. To address this limitation, we propose the
Cooperation of Experts (CoE) framework, which encodes multi-typed information
into unified heterogeneous multiplex networks. By overcoming modality and
connection differences, CoE provides a powerful and flexible model for
capturing the intricate structures of real-world complex data. In our
framework, dedicated encoders act as domain-specific experts, each specializing
in learning distinct relational patterns in specific semantic spaces. To
enhance robustness and extract complementary knowledge, these experts
collaborate through a novel large margin mechanism supported by a tailored
optimization strategy. Rigorous theoretical analyses guarantee the framework's
feasibility and stability, while extensive experiments across diverse
benchmarks demonstrate its superior performance and broad applicability. Our
code is available at https://github.com/strangeAlan/CoE.

</details>


### [103] [Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization](https://arxiv.org/abs/2505.20881)
*Yiding Shi,Jianan Zhou,Wen Song,Jieyi Bi,Yaoxin Wu,Jie Zhang*

Main category: cs.LG

TL;DR: Heuristic design with large language models (LLMs) shows potential in solving combinatorial optimization problems (COPs). However, current methods depend on predefined optimizers and single-task training, limiting exploration and generalization. This paper proposes Meta-Optimization of Heuristics (MoH), a framework using meta-learning to discover effective optimizers without relying on predefined ones. MoH uses LLMs to refine a meta-optimizer that constructs diverse optimizers for broader heuristic exploration and employs multi-task training for better generalization. Experiments show MoH achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing approaches that rely on manually predefined optimizers and single-task training schemes, which constrain heuristic algorithm exploration and hinder generalization.

Method: Propose Meta-Optimization of Heuristics (MoH), a framework that operates at the optimizer level. MoH leverages LLMs to iteratively refine a meta-optimizer capable of constructing diverse optimizers through self-invocation, eliminating reliance on predefined EC optimizers. It also uses a multi-task training scheme to enhance generalization.

Result: Experiments on classic COPs indicate that MoH successfully constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, especially in cross-size settings.

Conclusion: Meta-Optimization of Heuristics (MoH) is a promising framework that addresses the limitations of current heuristic design approaches by enabling broader heuristic exploration and improving generalization through meta-learning.

Abstract: Heuristic design with large language models (LLMs) has emerged as a promising
approach for tackling combinatorial optimization problems (COPs). However,
existing approaches often rely on manually predefined evolutionary computation
(EC) optimizers and single-task training schemes, which may constrain the
exploration of diverse heuristic algorithms and hinder the generalization of
the resulting heuristics. To address these issues, we propose Meta-Optimization
of Heuristics (MoH), a novel framework that operates at the optimizer level,
discovering effective optimizers through the principle of meta-learning.
Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that
autonomously constructs diverse optimizers through (self-)invocation, thereby
eliminating the reliance on a predefined EC optimizer. These constructed
optimizers subsequently evolve heuristics for downstream tasks, enabling
broader heuristic exploration. Moreover, MoH employs a multi-task training
scheme to promote its generalization capability. Experiments on classic COPs
demonstrate that MoH constructs an effective and interpretable meta-optimizer,
achieving state-of-the-art performance across various downstream tasks,
particularly in cross-size settings.

</details>


### [104] [Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning](https://arxiv.org/abs/2505.20882)
*Marc Damie,Edwige Cyffers*

Main category: cs.LG

TL;DR: Decentralized machine learning is gaining popularity due to its scalability and control over data. A challenge in this field is that learning dynamics depend on the topology of the communication graph, prompting the need for real graph datasets to benchmark decentralized algorithms. Current datasets are mostly limited to for-profit social networks. This paper introduces Fedivertex, a new dataset of 182 graphs from seven social networks within the Fediverse, crawled weekly over 14 weeks. The dataset is released with a Python package and demonstrates utility in various tasks, including a new defederation task.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the lack of adequate real-world graph datasets to benchmark decentralized algorithms. Existing datasets are primarily from for-profit social networks and may not accurately represent decentralized systems.

Method: The method involves creating a new dataset called Fedivertex, which includes 182 graphs from seven social networks within the Fediverse. These networks were crawled weekly over 14 weeks. The authors also developed a Python package to facilitate the use of this dataset.

Result: The result is the creation and release of the Fedivertex dataset along with a Python package. The dataset's utility is demonstrated through several tasks, including a novel defederation task related to link deletion in these networks.

Conclusion: In conclusion, the Fedivertex dataset provides a valuable resource for benchmarking decentralized algorithms by offering a more realistic representation of decentralized social networks compared to existing datasets.

Abstract: Decentralized machine learning - where each client keeps its own data locally
and uses its own computational resources to collaboratively train a model by
exchanging peer-to-peer messages - is increasingly popular, as it enables
better scalability and control over the data. A major challenge in this setting
is that learning dynamics depend on the topology of the communication graph,
which motivates the use of real graph datasets for benchmarking decentralized
algorithms. Unfortunately, existing graph datasets are largely limited to
for-profit social networks crawled at a fixed point in time and often collected
at the user scale, where links are heavily influenced by the platform and its
recommendation algorithms. The Fediverse, which includes several free and
open-source decentralized social media platforms such as Mastodon, Misskey, and
Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a
new dataset of 182 graphs, covering seven social networks from the Fediverse,
crawled weekly over 14 weeks. We release the dataset along with a Python
package to facilitate its use, and illustrate its utility on several tasks,
including a new defederation task, which captures a process of link deletion
observed on these networks.

</details>


### [105] [Improved Bounds for Swap Multicalibration and Swap Omniprediction](https://arxiv.org/abs/2505.20885)
*Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: The paper addresses multicalibration and omniprediction problems by proposing an efficient algorithm that achieves improved error rates in both online and distributional settings, leading to better sample complexity bounds.


<details>
  <summary>Details</summary>
Motivation: To solve the open problem raised by Garg et al. (2024) regarding the possibility of efficiently achieving specific multicalibration error rates against bounded linear functions.

Method: An efficient algorithm is developed which achieves O(T^(1/3)) ℓ2-swap multicalibration error. This leads to improved rates for ℓ1-swap multicalibration and swap omniprediction for convex Lipschitz functions.

Result: The algorithm achieves O(T^(2/3)) ℓ1-swap multicalibration and swap omniprediction errors, improving upon previous best-known bounds. Additionally, improved sample complexity rates are obtained in the distributional setting.

Conclusion: The proposed algorithm provides significant improvements in error rates and sample complexities for various tasks, enhancing our understanding of fairness and loss minimization in machine learning.

Abstract: In this paper, we consider the related problems of multicalibration -- a
multigroup fairness notion and omniprediction -- a simultaneous loss
minimization paradigm, both in the distributional and online settings. The
recent work of Garg et al. (2024) raised the open problem of whether it is
possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error
against bounded linear functions. In this paper, we answer this question in a
strongly affirmative sense. We propose an efficient algorithm that achieves
$O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high
probability and expectation). On propagating this bound onward, we obtain
significantly improved rates for $\ell_{1}$-swap multicalibration and swap
omniprediction for a loss class of convex Lipschitz functions. In particular,
we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap
multicalibration and swap omniprediction errors, thereby improving upon the
previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our
improved online results, we further obtain several improved sample complexity
rates in the distributional setting. In particular, we establish a
$O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an
$\varepsilon$-swap omnipredictor for the class of convex and Lipschitz
functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning
an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon
^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1},
\ell_{2}$-swap multicalibrated predictors against linear functions, all of
which significantly improve on the previous best-known bounds.

</details>


### [106] [One-Time Soft Alignment Enables Resilient Learning without Weight Transport](https://arxiv.org/abs/2505.20892)
*Jeonghwan Cheon,Jaehyuk Bae,Se-Bum Paik*

Main category: cs.LG

TL;DR: A one-time soft alignment of forward and feedback weights at initialization enables deep networks to perform comparably to backpropagation without requiring symmetric weight transport during learning, improving stability, trainability, and generalization.


<details>
  <summary>Details</summary>
Motivation: Backpropagation is computationally expensive and biologically implausible due to its reliance on symmetric weight transport and global synchronization. Feedback alignment avoids these issues but struggles with poor performance and instability in deep networks.

Method: The authors introduce a one-time soft alignment between forward and feedback weights at initialization, which does not require weight transport during learning. This method allows deep networks to achieve performance comparable to backpropagation.

Result: This approach improves network trainability, promotes smoother gradient flow, leads to convergence to flatter minima, and results in better generalization and robustness compared to standard backpropagation. Allowing moderate deviations from exact weight symmetry can also improve adversarial robustness.

Conclusion: A simple initialization strategy of one-time soft alignment enables effective learning in deep networks in a biologically plausible and resource-efficient manner.

Abstract: Backpropagation is the cornerstone of deep learning, but its reliance on
symmetric weight transport and global synchronization makes it computationally
expensive and biologically implausible. Feedback alignment offers a promising
alternative by approximating error gradients through fixed random feedback,
thereby avoiding symmetric weight transport. However, this approach often
struggles with poor learning performance and instability, especially in deep
networks. Here, we show that a one-time soft alignment between forward and
feedback weights at initialization enables deep networks to achieve performance
comparable to backpropagation, without requiring weight transport during
learning. This simple initialization condition guides stable error minimization
in the loss landscape, improving network trainability. Spectral analyses
further reveal that initial alignment promotes smoother gradient flow and
convergence to flatter minima, resulting in better generalization and
robustness. Notably, we also find that allowing moderate deviations from exact
weight symmetry can improve adversarial robustness compared to standard
backpropagation. These findings demonstrate that a simple initialization
strategy can enable effective learning in deep networks in a biologically
plausible and resource-efficient manner.

</details>


### [107] [DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition](https://arxiv.org/abs/2505.20894)
*Marius Bock,Michael Moeller,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: DeepConvContext是一种新的多尺度时间序列分类框架，用于人类活动识别（HAR），它通过处理时间顺序窗口的序列来建模窗内和窗间的时间模式。相比传统的DeepConvLSTM模型，它在六个常用的HAR基准测试中平均提高了10%的F1分数，并且展示了LSTM比注意力机制在此类数据上的优越性。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别通常依赖于滑动窗口方法分割数据集，传统深度学习模型如DeepConvLSTM独立地对每个窗口进行分类，这限制了可学习的时间上下文信息只能局限于单个窗口内部。为了突破这一约束，提出了一种新的模型。

Method: 提出了DeepConvContext，一种多尺度时间序列分类框架，受基于视觉的时序动作定位社区启发，该模型通过对时间排序窗口序列的处理，同时建模窗内和窗间的时间模式。此外，与最近结合注意力机制的HAR模型不同，DeepConvContext完全依赖LSTMs，并通过消融研究证明了LSTMs在惯性传感器数据建模中的优越性。

Result: DeepConvContext在六个广泛使用的HAR基准上实现了平均10%的F1分数提升，最高可达21%的改进。

Conclusion: DeepConvContext提供了一种有效的方法来解决传统HAR模型在长时间依赖性建模上的局限性，显著提升了性能，代码已公开可用。

Abstract: Despite recognized limitations in modeling long-range temporal dependencies,
Human Activity Recognition (HAR) has traditionally relied on a sliding window
approach to segment labeled datasets. Deep learning models like the
DeepConvLSTM typically classify each window independently, thereby restricting
learnable temporal context to within-window information. To address this
constraint, we propose DeepConvContext, a multi-scale time series
classification framework for HAR. Drawing inspiration from the vision-based
Temporal Action Localization community, DeepConvContext models both intra- and
inter-window temporal patterns by processing sequences of time-ordered windows.
Unlike recent HAR models that incorporate attention mechanisms, DeepConvContext
relies solely on LSTMs -- with ablation studies demonstrating the superior
performance of LSTMs over attention-based variants for modeling inertial sensor
data. Across six widely-used HAR benchmarks, DeepConvContext achieves an
average 10% improvement in F1-score over the classic DeepConvLSTM, with gains
of up to 21%. Code to reproduce our experiments is publicly available via
github.com/mariusbock/context_har.

</details>


### [108] [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
*Yiwei Wu,Atticus Geiger,Raphaël Millière*

Main category: cs.LG

TL;DR: Variable binding is fundamental to symbolic computation and cognition. Researchers investigated how Transformers, without built-in binding operations, could acquire this capacity by training a Transformer to dereference queried variables in symbolic programs.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to understand how modern neural networks can acquire the capacity for variable binding, which is typically implemented via addressable memory in classical architectures.

Method: The researchers trained a Transformer model on dereferencing tasks in symbolic programs where variables are assigned either numerical constants or other variables. The programs required following chains of variable assignments up to four steps deep while also containing irrelevant assignment chains as distractors.

Result: The analysis revealed three distinct phases during training: random prediction, shallow heuristic, and systematic dereferencing. The model learned to exploit the residual stream as an addressable memory space with specialized attention heads routing information across token positions.

Conclusion: Transformer models can learn systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches.

Abstract: Variable binding -- the ability to associate variables with values -- is
fundamental to symbolic computation and cognition. Although classical
architectures typically implement variable binding via addressable memory, it
is not well understood how modern neural networks lacking built-in binding
operations may acquire this capacity. We investigate this by training a
Transformer to dereference queried variables in symbolic programs where
variables are assigned either numerical constants or other variables. Each
program requires following chains of variable assignments up to four steps deep
to find the queried value, and also contains irrelevant chains of assignments
acting as distractors. Our analysis reveals a developmental trajectory with
three distinct phases during training: (1) random prediction of numerical
constants, (2) a shallow heuristic prioritizing early variable assignments, and
(3) the emergence of a systematic mechanism for dereferencing assignment
chains. Using causal interventions, we find that the model learns to exploit
the residual stream as an addressable memory space, with specialized attention
heads routing information across token positions. This mechanism allows the
model to dynamically track variable bindings across layers, resulting in
accurate dereferencing. Our results show how Transformer models can learn to
implement systematic variable binding without explicit architectural support,
bridging connectionist and symbolic approaches.

</details>


### [109] [Humble AI in the real-world: the case of algorithmic hiring](https://arxiv.org/abs/2505.20918)
*Rahul Nair,Inge Vejsbjerg,Elizabeth Daly,Christos Varytimidis,Bran Knowles*

Main category: cs.LG

TL;DR: The paper discusses the application of Humble AI principles in algorithmic hiring, demonstrating technical feasibility through uncertainty quantification and entropy estimates, while also considering user experience and trust.


<details>
  <summary>Details</summary>
Motivation: To address challenges in misrecognition and stereotyping in algorithmic hiring that standard fairness and trust frameworks struggle to evaluate, by applying the principles of Humble AI.

Method: Evaluation of virtual screening algorithms in a hiring platform using uncertainty quantification of ranks, entropy estimates, and enhancing user experience to highlight algorithmic unknowns. Conducting preliminary discussions with recruiter focus groups.

Result: Demonstrated technical feasibility of incorporating Humble AI principles into algorithmic hiring systems, paving the way for future user studies.

Conclusion: Humble AI principles can be effectively translated into practice in the domain of algorithmic hiring, potentially fostering trust despite increased cognitive load.

Abstract: Humble AI (Knowles et al., 2023) argues for cautiousness in AI development
and deployments through scepticism (accounting for limitations of statistical
learning), curiosity (accounting for unexpected outcomes), and commitment
(accounting for multifaceted values beyond performance). We present a
real-world case study for humble AI in the domain of algorithmic hiring.
Specifically, we evaluate virtual screening algorithms in a widely used hiring
platform that matches candidates to job openings. There are several challenges
in misrecognition and stereotyping in such contexts that are difficult to
assess through standard fairness and trust frameworks; e.g., someone with a
non-traditional background is less likely to rank highly. We demonstrate
technical feasibility of how humble AI principles can be translated to practice
through uncertainty quantification of ranks, entropy estimates, and a user
experience that highlights algorithmic unknowns. We describe preliminary
discussions with focus groups made up of recruiters. Future user studies seek
to evaluate whether the higher cognitive load of a humble AI system fosters a
climate of trust in its outcomes.

</details>


### [110] [Label Leakage in Federated Inertial-based Human Activity Recognition](https://arxiv.org/abs/2505.20924)
*Marius Bock,Maximilian Hopp,Kristof Van Laerhoven,Michael Moeller*

Main category: cs.LG

TL;DR: This paper explores label reconstruction attacks in Human Activity Recognition (HAR) using Federated Learning, revealing significant label leakage and limited protection from Local Differential Privacy techniques. It provides recommendations for privacy-aware HAR systems and highlights future research challenges.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of state-of-the-art gradient-based label leakage attacks on HAR benchmark datasets, given the sensitive nature of activity labels.

Method: The study examines factors influencing label leakage such as the number of activity classes, sampling strategy, and class imbalance. It also assesses the protection offered by Local Differential Privacy techniques like gradient noise and clipping.

Result: Reconstruction accuracies reach up to 90% on two benchmark datasets. Certain attacks can reliably infer both majority and minority class labels despite Local Differential Privacy measures.

Conclusion: The paper concludes with practical recommendations for deploying federated HAR systems with privacy considerations and outlines open challenges for future research.

Abstract: While prior work has shown that Federated Learning updates can leak sensitive
information, label reconstruction attacks, which aim to recover input labels
from shared gradients, have not yet been examined in the context of Human
Activity Recognition (HAR). Given the sensitive nature of activity labels, this
study evaluates the effectiveness of state-of-the-art gradient-based label
leakage attacks on HAR benchmark datasets. Our findings show that the number of
activity classes, sampling strategy, and class imbalance are critical factors
influencing the extent of label leakage, with reconstruction accuracies
reaching up to 90% on two benchmark datasets, even for trained models.
Moreover, we find that Local Differential Privacy techniques such as gradient
noise and clipping offer only limited protection, as certain attacks still
reliably infer both majority and minority class labels. We conclude by offering
practical recommendations for the privacy-aware deployment of federated HAR
systems and identify open challenges for future research. Code to reproduce our
experiments is publicly available via github.com/mariusbock/leakage_har.

</details>


### [111] [MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models](https://arxiv.org/abs/2505.20930)
*Ruiqi Zhang,Simon H. Tindemans*

Main category: cs.LG

TL;DR: This paper discusses the use of Multilevel Monte Carlo (MLMC) technique with data-driven surrogate models for accelerating reliability assessments in complex power systems, introduces a speed metric that includes training time and proposes an active learning approach to reduce labeling calls.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of unavailable pre-labeled datasets in resource adequacy assessments and the inefficiency caused by the time required for labeling training data when using surrogate models within the MLMC framework.

Method: The method involves introducing a speed metric that accounts for training time in evaluating MLMC efficiency and proposing a vote-by-committee active learning approach to reduce the required labeling calls.

Result: The case study shows that within practical variance thresholds, active learning significantly improves MLMC efficiency while reducing the training effort compared to regular surrogate modeling approaches.

Conclusion: Active learning enhances the efficiency of MLMC in reliability assessments for complex power systems while minimizing the labeling effort.

Abstract: Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction
technique for accelerating reliability assessments of complex power system.
Recently, data-driven surrogate models have been proposed as lower-level models
in the MLMC framework due to their high correlation and negligible execution
time once trained. However, in resource adequacy assessments, pre-labeled
datasets are typically unavailable. For large-scale systems, the efficiency
gains from surrogate models are often offset by the substantial time required
for labeling training data. Therefore, this paper introduces a speed metric
that accounts for training time in evaluating MLMC efficiency. Considering the
total time budget is limited, a vote-by-committee active learning approach is
proposed to reduce the required labeling calls. A case study demonstrates that,
within practical variance thresholds, active learning enables significantly
improved MLMC efficiency with reduced training effort, compared to regular
surrogate modelling approaches.

</details>


### [112] [NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion](https://arxiv.org/abs/2505.20934)
*Max Collins,Jordan Vice,Tim French,Ajmal Mian*

Main category: cs.LG

TL;DR: NatADiff是一种新的对抗性采样方案，利用去噪扩散生成自然的对抗样本，通过引导扩散轨迹和增强攻击转移性来改进模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗样本研究主要集中在受限的对抗样本上，不能准确反映现实世界中遇到的测试时错误。因此需要一种新的方法来生成更自然的对抗样本，以更好地模拟实际中的错误情况并提高模型的鲁棒性。

Method: 提出了一种名为NatADiff的对抗性采样方案，该方案基于去噪扩散技术生成自然的对抗样本。通过将扩散轨迹引导至真实类和对抗类的交集，并结合时间旅行采样与增强分类器指导，从而在保持图像保真度的同时提高攻击的可转移性。

Result: NatADiff方法在攻击成功率上与当前最先进的技术相当，但在模型架构间的转移性和与自然测试时错误的对齐性（如FID测量）上表现出显著更高的性能。这表明NatADiff生成的对抗样本不仅在模型间转移更有效，而且更忠实于自然发生的测试时错误。

Conclusion: NatADiff为生成自然对抗样本提供了一种有效的方法，可以提高模型对未来攻击的鲁棒性，并且生成的样本更接近现实中可能遇到的测试错误。

Abstract: Adversarial samples exploit irregularities in the manifold ``learned'' by
deep learning models to cause misclassifications. The study of these
adversarial samples provides insight into the features a model uses to classify
inputs, which can be leveraged to improve robustness against future attacks.
However, much of the existing literature focuses on constrained adversarial
samples, which do not accurately reflect test-time errors encountered in
real-world settings. To address this, we propose `NatADiff', an adversarial
sampling scheme that leverages denoising diffusion to generate natural
adversarial samples. Our approach is based on the observation that natural
adversarial samples frequently contain structural elements from the adversarial
class. Deep learning models can exploit these structural elements to shortcut
the classification process, rather than learning to genuinely distinguish
between classes. To leverage this behavior, we guide the diffusion trajectory
towards the intersection of the true and adversarial classes, combining
time-travel sampling with augmented classifier guidance to enhance attack
transferability while preserving image fidelity. Our method achieves comparable
attack success rates to current state-of-the-art techniques, while exhibiting
significantly higher transferability across model architectures and better
alignment with natural test-time errors as measured by FID. These results
demonstrate that NatADiff produces adversarial samples that not only transfer
more effectively across models, but more faithfully resemble naturally
occurring test-time errors.

</details>


### [113] [Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning](https://arxiv.org/abs/2505.20938)
*Chongjie Si,Yidan Cui,Fuchao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.LG

TL;DR: In Partial Multi-Label Learning (PML), a new method called Schirn is proposed to overcome limitations of existing methods by addressing conflicting assumptions about label sparsity and rank. It enforces sparsity on noise labels and high-rank on predicted labels, showing superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing PML methods rely on assumptions of sparsity for noise labels and low-rankness for ground-truth labels, which conflict and are impractical for real-world applications.

Method: The proposed method Schirn introduces a sparsity constraint on the noise label matrix while enforcing a high-rank property on the predicted label matrix.

Result: Extensive experiments show that Schirn outperforms state-of-the-art methods in PML tasks.

Conclusion: Schirn effectively addresses the limitations of current PML methods and demonstrates superior performance in handling real-world PML challenges.

Abstract: Partial Multi-Label Learning (PML) extends the multi-label learning paradigm
to scenarios where each sample is associated with a candidate label set
containing both ground-truth labels and noisy labels. Existing PML methods
commonly rely on two assumptions: sparsity of the noise label matrix and
low-rankness of the ground-truth label matrix. However, these assumptions are
inherently conflicting and impractical for real-world scenarios, where the true
label matrix is typically full-rank or close to full-rank. To address these
limitations, we demonstrate that the sparsity constraint contributes to the
high-rank property of the predicted label matrix. Based on this, we propose a
novel method Schirn, which introduces a sparsity constraint on the noise label
matrix while enforcing a high-rank property on the predicted label matrix.
Extensive experiments demonstrate the superior performance of Schirn compared
to state-of-the-art methods, validating its effectiveness in tackling
real-world PML challenges.

</details>


### [114] [Efficient Spectral Control of Partially Observed Linear Dynamical Systems](https://arxiv.org/abs/2505.20943)
*Anand Brahmbhatt,Gon Buzaglo,Sofiia Druchyna,Elad Hazan*

Main category: cs.LG

TL;DR: 提出了一种新的算法Double Spectral Control (DSC)，用于在部分观察和对抗性干扰下控制线性动力系统。该算法在匹配最佳已知遗憾保证的同时，通过依赖于系统稳定性边界的双光谱控制方法，指数级地提高了运行时复杂度。其核心创新是一种两级光谱逼近策略，利用与通用频谱滤波器基的双卷积，实现对最佳线性动力控制器的有效准确学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理部分观察和对抗性干扰下的线性动力系统控制问题时，运行时复杂度较高，依赖于系统的稳定性边界。因此需要一种更高效的算法来改进这一点。

Method: 提出了一种新的算法Double Spectral Control (DSC)，采用两级光谱逼近策略，结合双卷积与通用频谱滤波器基，从而实现对最佳线性动力控制器的学习。

Result: 该算法匹配了最佳已知遗憾保证，并且在运行时复杂度上相对于之前的算法有了指数级的提升。

Conclusion: Double Spectral Control (DSC)算法在保持遗憾保证不变的情况下，显著提升了运行效率，为部分观察和对抗性干扰下的线性动力系统控制提供了一种更为高效的方法。

Abstract: We propose a new method for the problem of controlling linear dynamical
systems under partial observation and adversarial disturbances. Our new
algorithm, Double Spectral Control (DSC), matches the best known regret
guarantees while exponentially improving runtime complexity over previous
approaches in its dependence on the system's stability margin. Our key
innovation is a two-level spectral approximation strategy, leveraging double
convolution with a universal basis of spectral filters, enabling efficient and
accurate learning of the best linear dynamical controllers.

</details>


### [115] [Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence](https://arxiv.org/abs/2505.20964)
*Mehdi Bennis,Salem Lahlou*

Main category: cs.LG

TL;DR: The paper proposes a paradigm shift in 6G and AI development, moving towards systems with semantic understanding based on System-2 cognition principles.


<details>
  <summary>Details</summary>
Motivation: Current 6G visions are incremental to 5G, and AI progress is limited by fragile, data-intensive models lacking robust reasoning abilities.

Method: Adopt System-2 cognition principles through three pillars: Abstraction for learning world models, Compositionality for combining concepts, and Emergent Communication for creating adaptive languages.

Result: Lays the foundation for intelligent systems capable of reasoning, adaptation, and collaboration, unifying wireless communications, machine learning, and robotics.

Conclusion: A unified research vision can lead to truly intelligent systems beyond purely technical communication advancements.

Abstract: The trajectories of 6G and AI are set for a creative collision. However,
current visions for 6G remain largely incremental evolutions of 5G, while
progress in AI is hampered by brittle, data-hungry models that lack robust
reasoning capabilities. This paper argues for a foundational paradigm shift,
moving beyond the purely technical level of communication toward systems
capable of semantic understanding and effective, goal-oriented interaction. We
propose a unified research vision rooted in the principles of System-2
cognition, built upon three pillars: Abstraction, enabling agents to learn
meaningful world models from raw sensorimotor data; Compositionality, providing
the algebraic tools to combine learned concepts and subsystems; and Emergent
Communication, allowing intelligent agents to create their own adaptive and
grounded languages. By integrating these principles, we lay the groundwork for
truly intelligent systems that can reason, adapt, and collaborate, unifying
advances in wireless communications, machine learning, and robotics under a
single coherent framework.

</details>


### [116] [Understanding the behavior of representation forgetting in continual learning](https://arxiv.org/abs/2505.20970)
*Joonkyu Kim,Yejin Kim,Jy-yong Sohn*

Main category: cs.LG

TL;DR: In this paper, the authors present a new metric called representation discrepancy to measure forgetting in continual learning scenarios and provide theoretical analysis and experimental support for their claims.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to effectively measure catastrophic forgetting in continual learning scenarios. The authors focus on representation forgetting which is measured at the hidden layer.

Method: The authors introduce a new metric called representation discrepancy that measures the difference between representation spaces constructed by two snapshots of a model trained through continual learning. They also conduct mathematical analysis of this metric and derive findings about the dynamics of representation forgetting.

Result: Through theoretical analysis and experiments on real image datasets (Split-CIFAR100 and ImageNet1K), the authors demonstrate that representation forgetting occurs more rapidly to a higher degree as the layer index increases, while increasing the width of the network slows down the forgetting process.

Conclusion: This paper provides the first theoretical analysis of representation forgetting in continual learning and introduces an effective surrogate metric for representation forgetting.

Abstract: In continual learning scenarios, catastrophic forgetting of previously
learned tasks is a critical issue, making it essential to effectively measure
such forgetting. Recently, there has been growing interest in focusing on
representation forgetting, the forgetting measured at the hidden layer. In this
paper, we provide the first theoretical analysis of representation forgetting
and use this analysis to better understand the behavior of continual learning.
First, we introduce a new metric called representation discrepancy, which
measures the difference between representation spaces constructed by two
snapshots of a model trained through continual learning. We demonstrate that
our proposed metric serves as an effective surrogate for the representation
forgetting while remaining analytically tractable. Second, through mathematical
analysis of our metric, we derive several key findings about the dynamics of
representation forgetting: the forgetting occurs more rapidly to a higher
degree as the layer index increases, while increasing the width of the network
slows down the forgetting process. Third, we support our theoretical findings
through experiments on real image datasets, including Split-CIFAR100 and
ImageNet1K.

</details>


### [117] [Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs](https://arxiv.org/abs/2505.20972)
*Sen Bai,Chunqi Yang,Xin Bai,Xin Zhang,Zhengang Jiang*

Main category: cs.LG

TL;DR: 提出了一种名为Deep $k$-grouping的无监督学习框架，用于解决大规模图和超图上的$k$-分组组合优化问题，包含OH-PUBO公式、GPU加速算法以及基于基尼系数的连续松弛退火策略。实验表明该方法优于现有神经网络求解器和经典启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督神经网络求解器难以有效解决大规模图和超图上的$k$-分组问题（如着色、分区），因计算框架限制。

Method: 1. 提出OH-PUBO公式来建模$k$-分组问题。
2. 使用大规模OH-PUBO目标的松弛作为可微损失函数进行无监督训练。
3. 利用GPU加速算法统一训练流程。
4. 采用基于基尼系数的连续松弛退火策略确保解的离散性并避免局部最优。

Result: Deep $k$-grouping在实验中表现优于现有的神经网络求解器和经典启发式方法（如SCIP和Tabu）。

Conclusion: Deep $k$-grouping提供了一种有效的无监督学习框架，能够解决大规模图和超图上的$k$-分组问题，并展现出优越性能。

Abstract: Along with AI computing shining in scientific discovery, its potential in the
combinatorial optimization (CO) domain has also emerged in recent years. Yet,
existing unsupervised neural network solvers struggle to solve $k$-grouping
problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs,
due to limited computational frameworks. In this work, we propose Deep
$k$-grouping, an unsupervised learning-based CO framework. Specifically, we
contribute: Novel one-hot encoded polynomial unconstrained binary optimization
(OH-PUBO), a formulation for modeling k-grouping problems on graphs and
hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated
algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs
the relaxation of large-scale OH-PUBO objectives as differentiable loss
functions and trains to optimize them in an unsupervised manner. To ensure
scalability, it leverages GPU-accelerated algorithms to unify the training
pipeline; A Gini coefficient-based continuous relaxation annealing strategy to
enforce discreteness of solutions while preventing convergence to local optima.
Experimental results demonstrate that Deep $k$-grouping outperforms existing
neural network solvers and classical heuristics such as SCIP and Tabu.

</details>


### [118] [Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation](https://arxiv.org/abs/2505.20992)
*Meng Qin,Jiahong Liu,Irwin King*

Main category: cs.LG

TL;DR: 提出了一种名为RFA的方法，用于高效的身份和位置嵌入。该方法采用基于频谱的GNN作为主干网络，仅使用随机噪声作为输入，并通过一次前向传播推导嵌入。实验表明，RFA可以在没有训练的情况下分别获得信息丰富的身份和位置嵌入，实现了质量和效率之间的更好权衡。


<details>
  <summary>Details</summary>
Motivation: 大多数基于GNN的方法不清楚它们可以捕捉到哪些属性，并且可能因一些耗时和空间的程序而效率低下和可扩展性差。从图信号处理的角度来看，发现图谱域中的高频和低频信息可能分别表征节点的身份和位置。

Method: 提出了随机特征聚合（RFA）方法，采用基于频谱的无学习参数的GNN为主干网络，仅使用随机噪声作为输入，并通过一次前向传播推导嵌入。此外，受度校正谱聚类的启发，引入了度校正机制到GNN主干网络中。

Result: 实验表明，RFA的两种变体（具有高通和低通滤波器）可以通过一次前向传播分别获得信息丰富且无需任何训练的身份和位置嵌入。

Conclusion: RFA在各种基线上实现了身份和位置嵌入的质量和效率之间的更好权衡。

Abstract: Graph neural networks (GNNs), which capture graph structures via a feature
aggregation mechanism following the graph embedding framework, have
demonstrated a powerful ability to support various tasks. According to the
topology properties (e.g., structural roles or community memberships of nodes)
to be preserved, graph embedding can be categorized into identity and position
embedding. However, it is unclear for most GNN-based methods which property
they can capture. Some of them may also suffer from low efficiency and
scalability caused by several time- and space-consuming procedures (e.g.,
feature extraction and training). From a perspective of graph signal
processing, we find that high- and low-frequency information in the graph
spectral domain may characterize node identities and positions, respectively.
Based on this investigation, we propose random feature aggregation (RFA) for
efficient identity and position embedding, serving as an extreme ablation study
regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without
learnable parameters as its backbone, (ii) only uses random noises as inputs,
and (iii) derives embeddings via just one feed-forward propagation (FFP).
Inspired by degree-corrected spectral clustering, we further introduce a degree
correction mechanism to the GNN backbone. Surprisingly, our experiments
demonstrate that two variants of RFA with high- and low-pass filters can
respectively derive informative identity and position embeddings via just one
FFP (i.e., without any training). As a result, RFA can achieve a better
trade-off between quality and efficiency for both identity and position
embedding over various baselines.

</details>


### [119] [BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks](https://arxiv.org/abs/2505.20997)
*Sen Bai,Chunqi Yang,Xin Bai,Xin Zhang,Zhengang Jiang*

Main category: cs.LG

TL;DR: This paper introduces BIPNN, a novel unsupervised learning framework using hypergraph neural networks to solve nonlinear binary integer programming problems. It reformulates these problems into differentiable polynomial loss functions, enabling efficient optimization through gradient descent and demonstrating superiority in experiments.


<details>
  <summary>Details</summary>
Motivation: Current methods for solving nonlinear binary integer programming (BIP) problems face significant challenges such as scalability issues and computational limitations due to exponential growth in auxiliary variables when using linear relaxations.

Method: The authors propose BIPNN, which reformulates BIP problems with discrete and nonlinear components into unconstrained, differentiable, and polynomial loss functions by leveraging the mapping between polynomial BIP objectives and hypergraph structures. They also introduce a GPU-accelerated and continuous-annealing-enhanced training pipeline for efficient optimization of large-scale nonlinear terms via gradient descent.

Result: BIPNN is able to optimize large-scale nonlinear terms fully in parallel using simple gradient descent, reducing training costs while maintaining the quality of discrete solutions. Experiments on both synthetic and real-world datasets show the approach's superiority compared to existing methods.

Conclusion: BIPNN provides an effective solution for nonlinear binary integer programming problems by utilizing hypergraph neural networks and reformulating the problem into a differentiable form, thus overcoming traditional computational barriers.

Abstract: Binary (0-1) integer programming (BIP) is pivotal in scientific domains
requiring discrete decision-making. As the advance of AI computing, recent
works explore neural network-based solvers for integer linear programming (ILP)
problems. Yet, they lack scalability for tackling nonlinear challenges. To
handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear
relaxations, leading to exponential growth in auxiliary variables and severe
computation limitations. To overcome these limitations, we propose BIPNN
(Binary Integer Programming Neural Network), an unsupervised learning framework
to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN).
Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear
(sin, log, exp) optimization problems-into unconstrained, differentiable, and
polynomial loss functions. The reformulation stems from the observation of a
precise one-to-one mapping between polynomial BIP objectives and hypergraph
structures, enabling the unsupervised training of HyperGNN to optimize BIP
problems in an end-to-end manner. On this basis, we propose a GPU-accelerated
and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline
enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel
via straightforward gradient descent, thus significantly reducing the training
cost while ensuring the generation of discrete, high-quality solutions.
Extensive experiments on synthetic and real-world datasets highlight the
superiority of our approach.

</details>


### [120] [Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models](https://arxiv.org/abs/2505.21005)
*Fengzhe Zhang,Laurence I. Midgley,José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: Variance-Tuned Diffusion Importance Sampling (VT-DIS) is a new method that improves score-based diffusion models by minimizing the α-divergence between forward and reverse processes, providing unbiased estimates with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Score-based diffusion models have shown great potential in sampling from Boltzmann distributions, but imperfect score estimates lead to biased Monte Carlo estimates. Classical importance sampling can correct this bias, yet it involves solving a costly probability-flow ODE which doesn't scale well with high-dimensional data.

Method: The authors introduce VT-DIS, a post-training method that adjusts the noise covariance of pretrained SBDMs by minimizing the α-divergence (α=2) between forward diffusion and reverse denoising trajectories. This results in a single trajectory-wise importance weight for the joint forward-reverse process, allowing for unbiased expectation estimates during testing with minimal additional computational cost.

Result: VT-DIS achieves effective sample sizes of about 80%, 35%, and 3.5% on the DW-4, LJ-13, and alanine-dipeptide benchmarks respectively, while using only a small fraction of the computational resources required by traditional methods such as vanilla diffusion + IS or PF-ODE-based IS.

Conclusion: VT-DIS offers an efficient way to improve the accuracy of Monte Carlo estimates produced by score-based diffusion models, significantly reducing computational requirements compared to existing methods.

Abstract: Score-based diffusion models (SBDMs) are powerful amortized samplers for
Boltzmann distributions; however, imperfect score estimates bias downstream
Monte Carlo estimates. Classical importance sampling (IS) can correct this
bias, but computing exact likelihoods requires solving the probability-flow
ordinary differential equation (PF-ODE), a procedure that is prohibitively
costly and scales poorly with dimensionality. We introduce Variance-Tuned
Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that
adapts the per-step noise covariance of a pretrained SBDM by minimizing the
$\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse
denoising trajectories. VT-DIS assigns a single trajectory-wise importance
weight to the joint forward-reverse process, yielding unbiased expectation
estimates at test time with negligible overhead compared to standard sampling.
On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective
sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using
only a fraction of the computational budget required by vanilla diffusion + IS
or PF-ODE-based IS.

</details>


### [121] [Federated Instrumental Variable Analysis via Federated Generalized Method of Moments](https://arxiv.org/abs/2505.21012)
*Geetika,Somya Tyagi,Bapi Chatterjee*

Main category: cs.LG

TL;DR: This paper introduces federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). It formulates FedGMM as a federated zero-sum game and solves it using federated gradient descent ascent (FedGDA) algorithm. Theoretical properties and existence results of clients' local equilibria are presented, showing that the federated solution consistently estimates the local moment conditions of every participating client.


<details>
  <summary>Details</summary>
Motivation: To fill the gap of no existing federated algorithm for either GMM or IV analysis, especially with non-i.i.d. data from decentralized clients.

Method: The paper formulates FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using the FedGDA algorithm. Theoretical properties and existence results of clients' local equilibria via FedGDA limit points are also presented.

Result: The federated solution consistently estimates the local moment conditions of every participating client. Extensive experiments demonstrate the efficacy of the proposed approach.

Conclusion: FedIV via FedGMM offers an efficient approach for IV analysis in high-dimensional settings with non-i.i.d. data from decentralized clients, while preserving data privacy.

Abstract: Instrumental variables (IV) analysis is an important applied tool for areas
such as healthcare and consumer economics. For IV analysis in high-dimensional
settings, the Generalized Method of Moments (GMM) using deep neural networks
offers an efficient approach. With non-i.i.d. data sourced from scattered
decentralized clients, federated learning is a popular paradigm for training
the models while promising data privacy. However, to our knowledge, no
federated algorithm for either GMM or IV analysis exists to date. In this work,
we introduce federated instrumental variables analysis (FedIV) via federated
generalized method of moments (FedGMM). We formulate FedGMM as a federated
zero-sum game defined by a federated non-convex non-concave minimax
optimization problem, which is solved using federated gradient descent ascent
(FedGDA) algorithm. One key challenge arises in theoretically characterizing
the federated local optimality. To address this, we present properties and
existence results of clients' local equilibria via FedGDA limit points.
Thereby, we show that the federated solution consistently estimates the local
moment conditions of every participating client. The proposed algorithm is
backed by extensive experiments to demonstrate the efficacy of our approach.

</details>


### [122] [NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation](https://arxiv.org/abs/2505.21020)
*Yuan Gao,Ruiqi Shu,Hao Wu,Fan Xu,Yanfei Xiang,Ruijian Gou,Qingsong Wen,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: The paper proposes NeuralOM, a neural ocean model using a multi-scale interactive graph neural network for Subseasonal-to-Seasonal (S2S) ocean simulation. It features a multi-stage framework and a multi-scale interactive messaging module to improve physical consistency and capture complex dynamical behaviors of the ocean system. Experiments show that NeuralOM surpasses state-of-the-art models in S2S and extreme event simulation.


<details>
  <summary>Details</summary>
Motivation: Accurate S2S ocean simulation is crucial for marine research but challenging due to thermal inertia and time delay. Current ML models lack sufficient physical consistency and consideration of the ocean's slow-changing properties.

Method: NeuralOM uses a multi-scale interactive graph neural network with a multi-stage framework to model the ocean's slowly changing nature and a multi-scale interactive messaging module to capture complex dynamical behaviors such as gradient changes and multiplicative coupling relationships.

Result: Extensive experimental evaluations demonstrate that NeuralOM outperforms state-of-the-art models in S2S and extreme event simulation.

Conclusion: NeuralOM improves S2S ocean simulation by incorporating physical consistency and capturing complex ocean dynamics through its multi-scale interactive graph neural network.

Abstract: Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically
important for marine research, yet remains challenging due to its substantial
thermal inertia and extended time delay. Machine learning (ML)-based models
have demonstrated significant advancements in simulation accuracy and
computational efficiency compared to traditional numerical methods.
Nevertheless, a significant limitation of current ML models for S2S ocean
simulation is their inadequate incorporation of physical consistency and the
slow-changing properties of the ocean system. In this work, we propose a neural
ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive
graph neural network to emulate diverse physical phenomena associated with
ocean systems effectively. Specifically, we propose a multi-stage framework
tailored to model the ocean's slowly changing nature. Additionally, we
introduce a multi-scale interactive messaging module to capture complex
dynamical behaviors, such as gradient changes and multiplicative coupling
relationships inherent in ocean dynamics. Extensive experimental evaluations
confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S
and extreme event simulation. The codes are available at
https://github.com/YuanGao-YG/NeuralOM.

</details>


### [123] [Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers](https://arxiv.org/abs/2505.21024)
*Charles London,Varun Kanade*

Main category: cs.LG

TL;DR: Pause tokens enhance Transformer performance by increasing computational expressivity.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical explanation for the effect of pause tokens on Transformer performance.

Method: Proving that adding pause tokens to Transformers increases their computational expressivity, demonstrating this through formal separation results and empirical experiments.

Result: With pause tokens, Transformers can express more complex functions (entire AC0 class with bounded-precision activations, TC0 with logarithmic-precision activations) compared to without them. Empirically, two-layer causally masked Transformers can learn parity functions with pause tokens.

Conclusion: Pause tokens theoretically and empirically improve Transformer's computational capabilities, offering a distinct mechanism to enhance reasoning.

Abstract: Pause tokens, simple filler symbols such as "...", consistently improve
Transformer performance on both language and mathematical tasks, yet their
theoretical effect remains unexplained. We provide the first formal separation
result, proving that adding pause tokens to constant-depth, logarithmic-width
Transformers strictly increases their computational expressivity. With
bounded-precision activations, Transformers without pause tokens compute only a
strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of
pause tokens allows them to express the entire class. For logarithmic-precision
Transformers, we show that adding pause tokens achieves expressivity equivalent
to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate
that two-layer causally masked Transformers can learn parity when supplied with
pause tokens, a function that they appear unable to learn without them. Our
results provide a rigorous theoretical explanation for prior empirical
findings, clarify how pause tokens interact with width, depth, and numeric
precision, and position them as a distinct mechanism, complementary to
chain-of-thought prompting, for enhancing Transformer reasoning.

</details>


### [124] [TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data](https://arxiv.org/abs/2505.21027)
*Zhipeng He,Chun Ouyang,Lijie Wen,Cong Liu,Catarina Moreira*

Main category: cs.LG

TL;DR: Adversarial attacks significantly threaten machine learning models. This study proposes a benchmark for adversarial attacks on tabular data, evaluating both effectiveness and imperceptibility across multiple models and datasets.


<details>
  <summary>Details</summary>
Motivation: To address the gap in current research that focuses primarily on achieving effective adversarial attacks on tabular data while often overlooking the importance of maintaining imperceptibility.

Method: Propose a new benchmark for adversarial attacks on tabular data that evaluates both effectiveness and imperceptibility. Assess five adversarial attacks across four models using eleven tabular datasets.

Result: The analysis explores how effectiveness and imperceptibility interact and influence the overall performance of the attacks, with results compared across different dataset types.

Conclusion: The findings provide valuable insights for improving the design of adversarial attack algorithms, advancing the field of adversarial machine learning on tabular data.

Abstract: Adversarial attacks pose a significant threat to machine learning models by
inducing incorrect predictions through imperceptible perturbations to input
data. While these attacks have been extensively studied in unstructured data
like images, their application to tabular data presents new challenges. These
challenges arise from the inherent heterogeneity and complex feature
interdependencies in tabular data, which differ significantly from those in
image data. To address these differences, it is crucial to consider
imperceptibility as a key criterion specific to tabular data. Most current
research focuses primarily on achieving effective adversarial attacks, often
overlooking the importance of maintaining imperceptibility. To address this
gap, we propose a new benchmark for adversarial attacks on tabular data that
evaluates both effectiveness and imperceptibility. In this study, we assess the
effectiveness and imperceptibility of five adversarial attacks across four
models using eleven tabular datasets, including both mixed and numerical-only
datasets. Our analysis explores how these factors interact and influence the
overall performance of the attacks. We also compare the results across
different dataset types to understand the broader implications of these
findings. The findings from this benchmark provide valuable insights for
improving the design of adversarial attack algorithms, thereby advancing the
field of adversarial machine learning on tabular data.

</details>


### [125] [LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms](https://arxiv.org/abs/2505.21034)
*Wenhu Li,Niki van Stein,Thomas Bäck,Elena Raponi*

Main category: cs.LG

TL;DR: 通过使用进化策略指导大型语言模型（LLM）生成完整的贝叶斯优化（BO）算法代码，研究展示了LLM在自动化科学发现中的潜力。所生成的算法在多个基准测试中超越了现有的BO方法。


<details>
  <summary>Details</summary>
Motivation: 尽管贝叶斯优化在优化昂贵的黑盒函数方面表现出色，但设计有效的BO算法仍需手动操作和专业知识。随着大型语言模型的发展，自动化科学发现成为可能，包括自动设计优化算法。本研究旨在利用LLM自动生成完整的BO算法代码。

Method: 研究构建了一个框架，使用进化策略引导LLM生成包含BO关键组件的Python代码：初始设计、代理模型和获取函数。通过提示LLM生成多个候选算法，并在COmparing Continuous Optimizers (COCO)平台上的Black-Box Optimization Benchmarking (BBOB)测试套件中进行评估。基于性能选择最佳候选者，通过控制提示变化进行组合和变异以实现迭代改进。

Result: 即使没有额外的微调，LLM生成的算法在24个BBOB函数中的19个上优于最先进的BO基线，在5维情况下表现良好，并能很好地推广到更高维度和不同任务（来自Bayesmark框架）。

Conclusion: 该研究表明，LLM可以作为算法协同设计师，提供了一种自动化BO开发的新范式，加速了新算法组合的发现。源代码已公开发布。

Abstract: Bayesian optimization (BO) is a powerful class of algorithms for optimizing
expensive black-box functions, but designing effective BO algorithms remains a
manual, expertise-driven task. Recent advancements in Large Language Models
(LLMs) have opened new avenues for automating scientific discovery, including
the automatic design of optimization algorithms. While prior work has used LLMs
within optimization loops or to generate non-BO algorithms, we tackle a new
challenge: Using LLMs to automatically generate full BO algorithm code. Our
framework uses an evolution strategy to guide an LLM in generating Python code
that preserves the key components of BO algorithms: An initial design, a
surrogate model, and an acquisition function. The LLM is prompted to produce
multiple candidate algorithms, which are evaluated on the established Black-Box
Optimization Benchmarking (BBOB) test suite from the COmparing Continuous
Optimizers (COCO) platform. Based on their performance, top candidates are
selected, combined, and mutated via controlled prompt variations, enabling
iterative refinement. Despite no additional fine-tuning, the LLM-generated
algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB
functions in dimension 5 and generalize well to higher dimensions, and
different tasks (from the Bayesmark framework). This work demonstrates that
LLMs can serve as algorithmic co-designers, offering a new paradigm for
automating BO development and accelerating the discovery of novel algorithmic
combinations. The source code is provided at
https://github.com/Ewendawi/LLaMEA-BO.

</details>


### [126] [Scalable and adaptive prediction bands with kernel sum-of-squares](https://arxiv.org/abs/2505.21039)
*Louis Allain,Sébastien da Veiga,Brian Staber*

Main category: cs.LG

TL;DR: This paper enhances Conformal Prediction (CP) by improving adaptivity and efficiency through statistical learning methods, introducing a new hyperparameter tuning strategy for better test-conditional coverage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of lack of adaptivity in Conformal Prediction and improve its efficiency for larger datasets.

Method: The method involves recasting CP as a statistical learning problem using RKHS and kernel SoS methods, deriving a dual formulation solvable with accelerated gradient methods, and introducing a hyperparameter tuning strategy based on HSIC.

Result: The results demonstrate the effectiveness of the proposed method compared to related work through extensive experiments.

Conclusion: The conclusion is that this approach improves adaptivity and scalability of CP, providing a promising direction for future research.

Abstract: Conformal Prediction (CP) is a popular framework for constructing prediction
bands with valid coverage in finite samples, while being free of any
distributional assumption. A well-known limitation of conformal prediction is
the lack of adaptivity, although several works introduced practically efficient
alternate procedures. In this work, we build upon recent ideas that rely on
recasting the CP problem as a statistical learning problem, directly targeting
coverage and adaptivity. This statistical learning problem is based on
reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS)
methods. First, we extend previous results with a general representer theorem
and exhibit the dual formulation of the learning problem. Crucially, such dual
formulation can be solved efficiently by accelerated gradient methods with
several hundreds or thousands of samples, unlike previous strategies based on
off-the-shelf semidefinite programming algorithms. Second, we introduce a new
hyperparameter tuning strategy tailored specifically to target adaptivity
through bounds on test-conditional coverage. This strategy, based on the
Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune
kernel lengthscales in our framework, but has broader applicability since it
could be used in any CP algorithm where the score function is learned. Finally,
extensive experiments are conducted to show how our method compares to related
work. All figures can be reproduced with the accompanying code.

</details>


### [127] [A domain adaptation neural network for digital twin-supported fault diagnosis](https://arxiv.org/abs/2505.21046)
*Zhenling Chen,Haiwei Fu,Zhiguo Zeng*

Main category: cs.LG

TL;DR: The paper proposes a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN) to solve the problem of performance drop in real scenarios due to discrepancies between simulation and real-world systems in deep learning-based fault diagnosis. Evaluated on robotics fault diagnosis dataset, incorporating domain adaptation significantly improves diagnostic performance.


<details>
  <summary>Details</summary>
Motivation: Digital twins can generate simulated data for model training in deep learning-based fault diagnosis, but there are discrepancies between simulation and real-world systems leading to poor performance in real scenarios.

Method: Propose a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN) to enable knowledge transfer from simulated (source domain) to real-world (target domain) data.

Result: Incorporating domain adaptation significantly improves diagnostic performance. For example, applying DANN to a baseline CNN model improves its accuracy from 70.00% to 80.22% on real-world test data.

Conclusion: Domain adaptation using DANN is effective in bridging the sim-to-real gap in fault diagnosis.

Abstract: Digital twins offer a promising solution to the lack of sufficient labeled
data in deep learning-based fault diagnosis by generating simulated data for
model training. However, discrepancies between simulation and real-world
systems can lead to a significant drop in performance when models are applied
in real scenarios. To address this issue, we propose a fault diagnosis
framework based on Domain-Adversarial Neural Networks (DANN), which enables
knowledge transfer from simulated (source domain) to real-world (target domain)
data. We evaluate the proposed framework using a publicly available robotics
fault diagnosis dataset, which includes 3,600 sequences generated by a digital
twin model and 90 real sequences collected from physical systems. The DANN
method is compared with commonly used lightweight deep learning models such as
CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating
domain adaptation significantly improves the diagnostic performance. For
example, applying DANN to a baseline CNN model improves its accuracy from
70.00% to 80.22% on real-world test data, demonstrating the effectiveness of
domain adaptation in bridging the sim-to-real gap.

</details>


### [128] [Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity](https://arxiv.org/abs/2505.21073)
*Pierre Houedry,Nicolas Courty,Florestan Martin-Baillon,Laetitia Chapel,Titouan Vayer*

Main category: cs.LG

TL;DR: 本论文提出了一种名为DeltaZero的全新可微优化框架，用于解决将任意度量空间映射到最近树度量的问题。该方法通过平滑Gromov的δ-双曲性代理实现基于梯度的优化，并具有可处理的复杂度。实验表明，该方法在合成和真实数据集上均达到最先进的失真率。


<details>
  <summary>Details</summary>
Motivation: 现有的将任意度量空间映射到其最近树度量的方法要么是启发式的且缺乏保证，要么表现一般。因此，设计能够桥接任意度量空间与其最近树度量的算法仍然是一个活跃的研究课题。

Method: 引入了一种新的可微优化框架DeltaZero，利用Gromov δ-双曲性的平滑代理实现基于梯度的优化。该方法具有可处理的复杂度，并从理论上提供了比现有界限更好的最坏情况保证。此外，该方法在统计上得到了验证。

Result: 在合成和真实世界的数据集上的实验表明，所提出的方法在失真率方面始终达到最先进的水平。

Conclusion: DeltaZero为解决将任意度量空间映射到最近树度量的问题提供了一个有效的新途径，具有理论和实际的优势。

Abstract: Trees and the associated shortest-path tree metrics provide a powerful
framework for representing hierarchical and combinatorial structures in data.
Given an arbitrary metric space, its deviation from a tree metric can be
quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing
algorithms that bridge an arbitrary metric to its closest tree metric is still
a vivid subject of interest, as most common approaches are either heuristical
and lack guarantees, or perform moderately well. In this work, we introduce a
novel differentiable optimization framework, coined DeltaZero, that solves this
problem. Our method leverages a smooth surrogate for Gromov's
$\delta$-hyperbolicity which enables a gradient-based optimization, with a
tractable complexity. The corresponding optimization procedure is derived from
a problem with better worst case guarantees than existing bounds, and is
justified statistically. Experiments on synthetic and real-world datasets
demonstrate that our method consistently achieves state-of-the-art distortion.

</details>


### [129] [Efficient Large Language Model Inference with Neural Block Linearization](https://arxiv.org/abs/2505.21077)
*Mete Erdogan,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: This paper proposes Neural Block Linearization (NBL), a framework for speeding up transformer model inference by replacing self-attention layers with linear approximations. NBL achieves computational speed-ups while maintaining competitive accuracy on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Transformer-based Large Language Models (LLMs) have high inference demands which challenge their deployment.

Method: Neural Block Linearization (NBL) accelerates transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. It uses Canonical Correlation Analysis to compute an upper bound on the approximation error, and selects LLM layers with the lowest linearization error for substitution.

Result: In experiments, applying NBL to pre-trained LLMs results in notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For example, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off.

Conclusion: NBL is a flexible and promising solution to improve the inference efficiency of LLMs without requiring fine-tuning.

Abstract: The high inference demands of transformer-based Large Language Models (LLMs)
pose substantial challenges in their deployment. To this end, we introduce
Neural Block Linearization (NBL), a novel framework for accelerating
transformer model inference by replacing self-attention layers with linear
approximations derived from Linear Minimum Mean Squared Error estimators. NBL
leverages Canonical Correlation Analysis to compute a theoretical upper bound
on the approximation error. Then, we use this bound as a criterion for
substitution, selecting the LLM layers with the lowest linearization error. NBL
can be efficiently applied to pre-trained LLMs without the need for
fine-tuning. In experiments, NBL achieves notable computational speed-ups while
preserving competitive accuracy on multiple reasoning benchmarks. For instance,
applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B
increases the inference speed by 32% with less than 1% accuracy trade-off,
making it a flexible and promising solution to improve the inference efficiency
of LLMs.

</details>


### [130] [Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations](https://arxiv.org/abs/2505.21095)
*Kei Takemura,Ryuta Matsuno,Keita Sakuma*

Main category: cs.LG

TL;DR: The paper proposes a novel optimistic online mirror descent algorithm with an auxiliary initial round to solve the impossible tuning issue in online learning, achieving state-of-the-art performance for universal online learning (UOL) and Lipschitz adaptivity (LA).


<details>
  <summary>Details</summary>
Motivation: A major challenge in online learning is to achieve adaptivity to unknown problem characteristics such as gradient variation (GV), function curvature (UOL), and gradient scales (LA). Current algorithms addressing these issues have limitations leading to sub-optimal performance.

Method: The authors propose a new optimistic online mirror descent algorithm with an auxiliary initial round using large learning rates. This allows for a refined analysis where a generated negative term cancels the gap-related factor, resolving the impossible tuning issue up to loglogT factors. The improved algorithm is then used as a meta-algorithm for UOL.

Result: The proposed method leads to the development of the first UOL algorithm that simultaneously achieves state-of-the-art GV bounds and LA under standard assumptions, overcoming key limitations of prior works.

Conclusion: This work resolves the conflict between LA mechanisms and regret analysis for GV bounds, which was an open problem highlighted by previous research.

Abstract: A central goal in online learning is to achieve adaptivity to unknown problem
characteristics, such as environmental changes captured by gradient variation
(GV), function curvature (universal online learning, UOL), and gradient scales
(Lipschitz adaptivity, LA). Simultaneously achieving these with optimal
performance is a major challenge, partly due to limitations in algorithms for
prediction with expert advice. These algorithms often serve as meta-algorithms
in online ensemble frameworks, and their sub-optimality hinders overall UOL
performance. Specifically, existing algorithms addressing the ``impossible
tuning'' issue incur an excess $\sqrt{\log T}$ factor in their regret bound
compared to the lower bound. To solve this problem, we propose a novel
optimistic online mirror descent algorithm with an auxiliary initial round
using large learning rates. This design enables a refined analysis where a
generated negative term cancels the gap-related factor, resolving the
impossible tuning issue up to $\log\log T$ factors. Leveraging our improved
algorithm as a meta-algorithm, we develop the first UOL algorithm that
simultaneously achieves state-of-the-art GV bounds and LA under standard
assumptions. Our UOL result overcomes key limitations of prior works, notably
resolving the conflict between LA mechanisms and regret analysis for GV bounds
-- an open problem highlighted by Xie et al.

</details>


### [131] [Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance](https://arxiv.org/abs/2505.21101)
*Badr Moufad,Yazid Janati,Alain Durmus,Ahmed Ghorbel,Eric Moulines,Jimmy Olsson*

Main category: cs.LG

TL;DR: The paper identifies an issue with Classifier-Free Guidance (CFG) in conditional diffusion models, where it doesn't correspond to a well-defined denoising diffusion model (DDM) and reduces sample diversity. They propose a correction term involving Rényi divergence and a Gibbs-like sampling procedure to improve sample quality and diversity.


<details>
  <summary>Details</summary>
Motivation: CFG is widely used but often reduces sample diversity, leading to a trade-off between quality and diversity.

Method: 1. Identify the missing component in CFG: a Rényi divergence term. 2. Propose a Gibbs-like sampling procedure starting with an initial sample from the conditional diffusion model without CFG and iteratively refining it.

Result: Demonstrates substantial improvements over CFG in both image and text-to-audio generation tasks across all considered metrics.

Conclusion: The proposed method improves sample quality while preserving diversity, outperforming CFG.

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for improving
conditional diffusion models by linearly combining the outputs of conditional
and unconditional denoisers. While CFG enhances visual quality and improves
alignment with prompts, it often reduces sample diversity, leading to a
challenging trade-off between quality and diversity. To address this issue, we
make two key contributions. First, CFG generally does not correspond to a
well-defined denoising diffusion model (DDM). In particular, contrary to common
intuition, CFG does not yield samples from the target distribution associated
with the limiting CFG score as the noise level approaches zero -- where the
data distribution is tilted by a power $w \gt 1$ of the conditional
distribution. We identify the missing component: a R\'enyi divergence term that
acts as a repulsive force and is required to correct CFG and render it
consistent with a proper DDM. Our analysis shows that this correction term
vanishes in the low-noise limit. Second, motivated by this insight, we propose
a Gibbs-like sampling procedure to draw samples from the desired tilted
distribution. This method starts with an initial sample from the conditional
diffusion model without CFG and iteratively refines it, preserving diversity
while progressively enhancing sample quality. We evaluate our approach on both
image and text-to-audio generation tasks, demonstrating substantial
improvements over CFG across all considered metrics. The code is available at
https://github.com/yazidjanati/cfgig

</details>


### [132] [Universal Value-Function Uncertainties](https://arxiv.org/abs/2505.21119)
*Moritz A. Zanger,Max Weltevrede,Yaniv Oren,Pascal R. Van der Vaart,Caroline Horsch,Wendelin Böhmer,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: UVU是一种新的方法，用于估计强化学习中的表值函数的不确定性，结合了深度集成和单模型方法的优点。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，估计表值函数的认知不确定性是一个关键挑战。现有的深度集成方法虽然稳健但计算成本高，而单模型方法虽然计算效率高但通常依赖于启发式方法且需要额外的传播机制。因此，研究者们希望找到一种既能有效量化不确定性又不带来显著计算开销的方法。

Method: UVU通过计算在线学习网络与固定、随机初始化的目标网络之间的平方预测误差来量化不确定性。在线网络使用时间差分学习进行训练，其中合成奖励由固定目标网络得出。UVU错误反映了基于策略的值不确定性，并考虑了给定策略可能遇到的未来不确定性。此外，利用神经切线核理论对UVU进行了广泛的理论分析。

Result: 理论上，在无限网络宽度的极限情况下，UVU错误等价于独立通用值函数集合的方差。实验上，在具有挑战性的多任务离线RL设置中，UVU实现了与大型集成方法相等的性能，同时提供了简单性和显著的计算节省。

Conclusion: UVU提供了一种有效的解决方案来估计强化学习中的表值函数不确定性，既保持了深度集成方法的准确性，又避免了其高昂的计算成本。

Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge
for many aspects of reinforcement learning (RL), including efficient
exploration, safe decision-making, and offline RL. While deep ensembles provide
a robust method for quantifying value uncertainty, they come with significant
computational overhead. Single-model methods, while computationally favorable,
often rely on heuristics and typically require additional propagation
mechanisms for myopic uncertainty estimates. In this work we introduce
universal value-function uncertainties (UVU), which, similar in spirit to
random network distillation (RND), quantify uncertainty as squared prediction
errors between an online learner and a fixed, randomly initialized target
network. Unlike RND, UVU errors reflect policy-conditional value uncertainty,
incorporating the future uncertainties any given policy may encounter. This is
due to the training procedure employed in UVU: the online network is trained
using temporal difference learning with a synthetic reward derived from the
fixed, randomly initialized target network. We provide an extensive theoretical
analysis of our approach using neural tangent kernel (NTK) theory and show that
in the limit of infinite network width, UVU errors are exactly equivalent to
the variance of an ensemble of independent universal value functions.
Empirically, we show that UVU achieves equal performance to large ensembles on
challenging multi-task offline RL settings, while offering simplicity and
substantial computational savings.

</details>


### [133] [Robust and Computation-Aware Gaussian Processes](https://arxiv.org/abs/2505.21133)
*Marshal Arijona Sinaga,Julien Martinelli,Samuel Kaski*

Main category: cs.LG

TL;DR: 在大数据集且存在异常值的情况下，标准高斯过程及其稀疏近似在计算和鲁棒性方面面临挑战。本文提出了一种新的高斯过程模型RCaGP，通过结合近似引起的不确定性处理与鲁棒广义贝叶斯更新，解决了这些问题。该模型不仅能够有效管理异常值和计算不确定性，还能提供更保守和可靠的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 尽管高斯过程因其表达能力和合理性的不确定性估计而被广泛用于回归和优化任务中，但在大数据集且存在异常值的情况下，标准高斯过程及其稀疏近似在计算和鲁棒性方面面临挑战。

Method: 提出了名为RCaGP的新型高斯过程模型，该模型结合了近似引起的不确定性处理与鲁棒广义贝叶斯更新，从而联合解决计算和鲁棒性问题。特别地，它认识到鲁棒性和近似意识并非正交而是相互交织：近似可以加剧异常值的影响，因此必须同时处理两者。此外，还提出了一种针对鲁棒均值函数的定制模型选择方案。

Result: 实证结果表明，在清洁和异常值污染的数据集上，无论是回归还是高吞吐量贝叶斯优化基准，同时解决这些挑战都能带来优越的性能。

Conclusion: RCaGP模型能有效应对大数据集中的异常值和计算不确定性，提供更保守和可靠的不确定性估计，并通过理论和实证验证了其鲁棒性。

Abstract: Gaussian processes (GPs) are widely used for regression and optimization
tasks such as Bayesian optimization (BO) due to their expressiveness and
principled uncertainty estimates. However, in settings with large datasets
corrupted by outliers, standard GPs and their sparse approximations struggle
with computational tractability and robustness. We introduce Robust
Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly
addresses these challenges by combining a principled treatment of
approximation-induced uncertainty with robust generalized Bayesian updating.
The key insight is that robustness and approximation-awareness are not
orthogonal but intertwined: approximations can exacerbate the impact of
outliers, and mitigating one without the other is insufficient. Unlike previous
work that focuses narrowly on either robustness or approximation quality, RCaGP
combines both in a principled and scalable framework, thus effectively managing
both outliers and computational uncertainties introduced by approximations such
as low-rank matrix multiplications. Our model ensures more conservative and
reliable uncertainty estimates, a property we rigorously demonstrate.
Additionally, we establish a robustness property and show that the mean
function is key to preserving it, motivating a tailored model selection scheme
for robust mean functions. Empirical results confirm that solving these
challenges jointly leads to superior performance across both clean and
outlier-contaminated settings, both on regression and high-throughput Bayesian
optimization benchmarks.

</details>


### [134] [Learning Single Index Models with Diffusion Priors](https://arxiv.org/abs/2505.21135)
*Anqi Tang,Youming Chen,Shuchen Xue,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: Diffusion models (DMs) are great at generating images and can be used for signal recovery. However, current research has limitations. This paper focuses on using DMs for accurate recovery from semi-parametric single index models with discontinuous or unknown link functions. They propose an efficient reconstruction method and perform numerical experiments on image datasets.


<details>
  <summary>Details</summary>
Motivation: Existing research on signal recovery with diffusion models either focuses on specific reconstruction problems or is unable to handle nonlinear measurement models with discontinuous or unknown link functions.

Method: The authors propose an efficient reconstruction method that only requires one round of unconditional sampling and (partial) inversion of DMs. They focus on using DMs to achieve accurate recovery from semi-parametric single index models.

Result: Numerical experiments on image datasets for different nonlinear measurement models show that the proposed approach yields more accurate reconstructions while utilizing significantly fewer neural function evaluations compared to competing methods.

Conclusion: The proposed method provides an efficient way to use diffusion models for signal recovery in semi-parametric single index models with discontinuous or unknown link functions.

Abstract: Diffusion models (DMs) have demonstrated remarkable ability to generate
diverse and high-quality images by efficiently modeling complex data
distributions. They have also been explored as powerful generative priors for
signal recovery, resulting in a substantial improvement in the quality of
reconstructed signals. However, existing research on signal recovery with
diffusion models either focuses on specific reconstruction problems or is
unable to handle nonlinear measurement models with discontinuous or unknown
link functions. In this work, we focus on using DMs to achieve accurate
recovery from semi-parametric single index models, which encompass a variety of
popular nonlinear models that may have {\em discontinuous} and {\em unknown}
link functions. We propose an efficient reconstruction method that only
requires one round of unconditional sampling and (partial) inversion of DMs.
Theoretical analysis on the effectiveness of the proposed methods has been
established under appropriate conditions. We perform numerical experiments on
image datasets for different nonlinear measurement models. We observe that
compared to competing methods, our approach can yield more accurate
reconstructions while utilizing significantly fewer neural function
evaluations.

</details>


### [135] [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136)
*Jintao Zhang,Xiaoming Xu,Jia Wei,Haofeng Huang,Pengle Zhang,Chendong Xiang,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: SageAttention2++ uses FP8 Matmul accumulated in FP16 to accelerate attention mechanisms, achieving 3.9x speedup over FlashAttention without significant loss in accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of attention mechanisms whose time complexity increases quadratically with sequence length, building on SageAttention2 which uses quantization for faster matrix multiplications.

Method: Proposes utilizing the faster instruction of FP8 Matmul accumulated in FP16 to further accelerate SageAttention2, making it 2x faster than the original FP8 Matmul used in SageAttention2.

Result: SageAttention2++ achieves a 3.9x speedup compared to FlashAttention while preserving the same attention accuracy as SageAttention2 and maintaining negligible end-to-end metrics loss across different model types (language, image, video).

Conclusion: SageAttention2++ significantly accelerates attention-based models with minimal impact on performance, and its code will be made available on GitHub.

Abstract: The efficiency of attention is critical because its time complexity grows
quadratically with sequence length. SageAttention2 addresses this by utilizing
quantization to accelerate matrix multiplications (Matmul) in attention. To
further accelerate SageAttention2, we propose to utilize the faster instruction
of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8
Matmul used in SageAttention2. Our experiments show that SageAttention2++
achieves a 3.9x speedup over FlashAttention while maintaining the same
attention accuracy as SageAttention2. This means SageAttention2++ effectively
accelerates various models, including those for language, image, and video
generation, with negligible end-to-end metrics loss. The code will be available
at https://github.com/thu-ml/SageAttention.

</details>


### [136] [HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs](https://arxiv.org/abs/2505.21140)
*Honglin Gao,Xiang Li,Lan Zhao,Gaoxi Xiao*

Main category: cs.LG

TL;DR: The paper introduces Heterogeneous Backdoor Attack (HeteroBA), a framework that inserts trigger nodes in heterogeneous graphs to mislead HGNNs' node classification while preserving clean data accuracy, revealing vulnerabilities and urging for robust defenses.


<details>
  <summary>Details</summary>
Motivation: To explore the robustness and security of heterogeneous graph neural networks (HGNNs) under backdoor attacks, which has been less investigated compared to improving their predictive performance.

Method: Propose HeteroBA framework that inserts trigger nodes with realistic features and targeted structural connections using attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation.

Result: HeteroBA achieves high attack success rates on three datasets and various HGNN architectures with minimal impact on clean accuracy.

Conclusion: HeteroBA highlights potential vulnerabilities in HGNNs and emphasizes the need for more robust defenses against backdoor threats in multi-relational graph scenarios.

Abstract: Heterogeneous graph neural networks (HGNNs) have recently drawn increasing
attention for modeling complex multi-relational data in domains such as
recommendation, finance, and social networks. While existing research has been
largely focused on enhancing HGNNs' predictive performance, their robustness
and security, especially under backdoor attacks, remain underexplored. In this
paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework
for node classification tasks on heterogeneous graphs. HeteroBA inserts
carefully crafted trigger nodes with realistic features and targeted structural
connections, leveraging attention-based and clustering-based strategies to
select influential auxiliary nodes for effective trigger propagation, thereby
causing the model to misclassify specific nodes into a target label while
maintaining accuracy on clean data. Experimental results on three datasets and
various HGNN architectures demonstrate that HeteroBA achieves high attack
success rates with minimal impact on the clean accuracy. Our method sheds light
on potential vulnerabilities in HGNNs and calls for more robust defenses
against backdoor threats in multi-relational graph scenarios.

</details>


### [137] [A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques](https://arxiv.org/abs/2505.21141)
*Nancy C. Woods,Virtue Ene Agada,Adebola K. Ojo*

Main category: cs.LG

TL;DR: This paper proposes a hybrid approach combining Support Vector Machines (SVM) and Multi-Class Classification Rules based on Association Rules (MCAR) for detecting phishing websites.


<details>
  <summary>Details</summary>
Motivation: Phishing is a serious Internet crime causing significant economic damage, but there's no consensus on the best detection algorithm.

Method: The study uses MCAR for feature extraction and rule generation, and SVM for classification and prediction. It tests this approach on 11,056 websites from PhishTank and Yahoo directory.

Result: The technique achieved 98.30% classification accuracy, 98% AUC, and 82.84% variance in prediction with a computation time of 2205.33 seconds.

Conclusion: The hybrid approach combining SVM and MCAR techniques provides a more accurate method for predicting phishing websites.

Abstract: Phishing is a semantic attack which targets the user rather than the
computer. It is a new Internet crime in comparison with other forms such as
virus and hacking. Considering the damage phishing websites has caused to
various economies by collapsing organizations, stealing information and
financial diversion, various researchers have embarked on different ways of
detecting phishing websites but there has been no agreement about the best
algorithm to be used for prediction. This study is interested in integrating
the strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class
Classification Rules based on Association Rules (MCAR) to establish a strong
and better means of predicting phishing websites. A total of 11,056 websites
were used from both PhishTank and yahoo directory to verify the effectiveness
of this approach. Feature extraction and rules generation were done by the MCAR
technique; classification and prediction were done by SVM technique. The result
showed that the technique achieved 98.30% classification accuracy with a
computation time of 2205.33s with minimum error rate. It showed a total of 98%
Area under the Curve (AUC) which showed the proportion of accuracy in
classifying phishing websites. The model showed 82.84% variance in the
prediction of phishing websites based on the coefficient of determination. The
use of two techniques together in detecting phishing websites produced a more
accurate result as it combined the strength of both techniques respectively.
This research work centralized on this advantage by building a hybrid of two
techniques to help produce a more accurate result.

</details>


### [138] [Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score](https://arxiv.org/abs/2505.21147)
*Xuanning Zhou,Hao Zeng,Xiaobo Xia,Bingyi Jing,Hongxin Wei*

Main category: cs.LG

TL;DR: This paper introduces SemiCP, an extension of conformal prediction (CP) to the semi-supervised setting that leverages both labeled and unlabeled data for calibration, reducing instability and inefficiency when calibration data is limited.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction provides prediction sets with coverage guarantees but can lead to coverage deviation and overly large sets when labeled data is limited. To address this issue, there's a need to incorporate unlabeled data into the calibration process.

Method: The authors propose SemiCP which uses both labeled and unlabeled data for calibration in the semi-supervised setting. They introduce a novel nonconformity score function called NNM that selects labeled data with similar pseudo-label scores to estimate nonconformity scores, thereby overcoming sample size limitations.

Result: SemiCP provides asymptotically coverage guarantee for prediction sets under mild assumptions. Experiments show it reduces instability and inefficiency under limited calibration data, can be adapted to conditional coverage settings, and integrates well with existing CP methods.

Conclusion: SemiCP is an effective approach that extends conformal prediction to leverage unlabeled data, improving performance in scenarios with limited labeled data.

Abstract: Conformal prediction (CP) is a powerful framework for uncertainty
quantification, providing prediction sets with coverage guarantees when
calibrated on sufficient labeled data. However, in real-world applications
where labeled data is often limited, standard CP can lead to coverage deviation
and output overly large prediction sets. In this paper, we extend CP to the
semi-supervised setting and propose SemiCP, leveraging both labeled data and
unlabeled data for calibration. Specifically, we introduce a novel
nonconformity score function, NNM, designed for unlabeled data. This function
selects labeled data with similar pseudo-label scores to estimate nonconformity
scores, integrating them into the calibration process to overcome sample size
limitations. We theoretically demonstrate that, under mild assumptions, SemiCP
provide asymptotically coverage guarantee for prediction sets. Extensive
experiments further validate that our approach effectively reduces instability
and inefficiency under limited calibration data, can be adapted to conditional
coverage settings, and integrates seamlessly with existing CP methods.

</details>


### [139] [STEB: In Search of the Best Evaluation Approach for Synthetic Time Series](https://arxiv.org/abs/2505.21160)
*Michael Stenger,Robert Leppich,André Bauer,Samuel Kounev*

Main category: cs.LG

TL;DR: The paper introduces Synthetic Time series Evaluation Benchmark (STEB), the first framework for automated comparisons of synthetic time series evaluation measures, and finds that time series embedding significantly affects the final score.


<details>
  <summary>Details</summary>
Motivation: There is a lack of a comprehensive framework for objectively comparing synthetic time series evaluation measures on a large scale due to increasing demand for synthetic time series caused by data augmentation or privacy regulations.

Method: STEB uses 10 diverse datasets, randomness injection, and 13 configurable data transformations to compute indicators for measure reliability and score consistency. It also tracks running time, test errors, and features sequential and parallel modes of operation.

Result: Experiments determined a ranking of 41 measures from literature and confirmed that the choice of upstream time series embedding heavily impacts the final score.

Conclusion: STEB is proposed as the first benchmark framework for comprehensive and interpretable automated comparisons of synthetic time series evaluation measures.

Abstract: The growing need for synthetic time series, due to data augmentation or
privacy regulations, has led to numerous generative models, frameworks, and
evaluation measures alike. Objectively comparing these measures on a large
scale remains an open challenge. We propose the Synthetic Time series
Evaluation Benchmark (STEB) -- the first benchmark framework that enables
comprehensive and interpretable automated comparisons of synthetic time series
evaluation measures. Using 10 diverse datasets, randomness injection, and 13
configurable data transformations, STEB computes indicators for measure
reliability and score consistency. It tracks running time, test errors, and
features sequential and parallel modes of operation. In our experiments, we
determine a ranking of 41 measures from literature and confirm that the choice
of upstream time series embedding heavily impacts the final score.

</details>


### [140] [Topological Deep Learning for Speech Data](https://arxiv.org/abs/2505.21173)
*Zhiwang Yu*

Main category: cs.LG

TL;DR: Topology-aware convolutional kernels are designed using TDA, leading to improved speech recognition networks and revealing potential for TDA in neural network optimization.


<details>
  <summary>Details</summary>
Motivation: To explore the application of topological data analysis (TDA) in deep learning and improve speech recognition networks by designing topology-aware convolutional kernels.

Method: Theoretically, investigate orthogonal group actions on kernels to establish a fiber-bundle decomposition of matrix spaces for new filter generation methods. Practically, implement an Orthogonal Feature (OF) layer for phoneme recognition.

Result: Superior performance in phoneme recognition, especially in low-noise scenarios, with demonstrated cross-domain adaptability.

Conclusion: This work highlights the potential of TDA in optimizing neural networks and opens new opportunities for interdisciplinary studies between mathematics and deep learning.

Abstract: Topological data analysis (TDA) offers novel mathematical tools for deep
learning. Inspired by Carlsson et al., this study designs topology-aware
convolutional kernels that significantly improve speech recognition networks.
Theoretically, by investigating orthogonal group actions on kernels, we
establish a fiber-bundle decomposition of matrix spaces, enabling new filter
generation methods. Practically, our proposed Orthogonal Feature (OF) layer
achieves superior performance in phoneme recognition, particularly in low-noise
scenarios, while demonstrating cross-domain adaptability. This work reveals
TDA's potential in neural network optimization, opening new avenues for
mathematics-deep learning interdisciplinary studies.

</details>


### [141] [Latent label distribution grid representation for modeling uncertainty](https://arxiv.org/abs/2505.21180)
*ShuNing Sun,YinSong Xiong,Yu Zhang,Zhuoran Zheng*

Main category: cs.LG

TL;DR: This paper addresses the issue of uncertainty in label distribution learning (LDL) by constructing a Latent Label Distribution Grid (LLDG). It models the uncertainty through a label correlation matrix and Gaussian distribution vectors, reconstructs the grid with LLDG-Mixer for accurate label distribution, and enforces noise reduction using a low-rank scheme. The approach is evaluated on classification tasks and shows competitive performance.


<details>
  <summary>Details</summary>
Motivation: The complexity and high cost of label distribution annotation in LDL lead to inaccuracies in the label space, which misleads the algorithm to make incorrect decisions.

Method: Construct a Latent Label Distribution Grid (LLDG) by building a label correlation matrix based on label differences, expanding each value into a Gaussian-distributed vector, enforcing a customized low-rank scheme for noise reduction via Tucker reconstruction, and reconstructing the LLDG with LLDG-Mixer to generate an accurate label distribution.

Result: Extensive experimental results demonstrate that the proposed approach performs competitively on several benchmarks.

Conclusion: The paper proposes an effective method to model the uncertainty in label distributions and improve the accuracy of LDL algorithms.

Abstract: Although \textbf{L}abel \textbf{D}istribution \textbf{L}earning (LDL) has
promising representation capabilities for characterizing the polysemy of an
instance, the complexity and high cost of the label distribution annotation
lead to inexact in the construction of the label space. The existence of a
large number of inexact labels generates a label space with uncertainty, which
misleads the LDL algorithm to yield incorrect decisions. To alleviate this
problem, we model the uncertainty of label distributions by constructing a
\textbf{L}atent \textbf{L}abel \textbf{D}istribution \textbf{G}rid (LLDG) to
form a low-noise representation space. Specifically, we first construct a label
correlation matrix based on the differences between labels, and then expand
each value of the matrix into a vector that obeys a Gaussian distribution, thus
building a LLDG to model the uncertainty of the label space. Finally, the LLDG
is reconstructed by the LLDG-Mixer to generate an accurate label distribution.
Note that we enforce a customized low-rank scheme on this grid, which assumes
that the label relations may be noisy and it needs to perform noise-reduction
with the help of a Tucker reconstruction technique. Furthermore, we attempt to
evaluate the effectiveness of the LLDG by considering its generation as an
upstream task to achieve the classification of the objects. Extensive
experimental results show that our approach performs competitively on several
benchmarks.

</details>


### [142] [Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations](https://arxiv.org/abs/2505.21182)
*Huy Hoang,Tien Mai,Pradeep Varakantham,Tanvi Verma*

Main category: cs.LG

TL;DR: 本研究提出了一种新的离线模仿学习方法，通过优化专家和不良数据的状态-动作访问分布的KL散度差异，避免对抗性训练并统一处理正负示范。实验表明该方法优于现有最佳基线。


<details>
  <summary>Details</summary>
Motivation: 现有的离线模仿学习通常利用专家和未标记的演示，但忽略了明确不良行为中的有价值信号。

Method: 研究提出一种新公式，优化专家与不良（或差）数据状态-动作访问分布的KL散度差异。当专家演示超过不良演示时，目标函数从DC程序变为凸函数，从而实现实际且稳定的非对抗性训练目标。此方法在一个统一框架内避免对抗性训练，并处理正负示范。

Result: 在标准离线模仿学习基准上的广泛实验证明，该方法始终优于现有最佳基线。

Conclusion: 所提出的方法通过优化KL散度差异，在非对抗性训练条件下有效利用正负示范，表现优于其他方法。

Abstract: Offline imitation learning typically learns from expert and unlabeled
demonstrations, yet often overlooks the valuable signal in explicitly
undesirable behaviors. In this work, we study offline imitation learning from
contrasting behaviors, where the dataset contains both expert and undesirable
demonstrations. We propose a novel formulation that optimizes a difference of
KL divergences over the state-action visitation distributions of expert and
undesirable (or bad) data. Although the resulting objective is a DC
(Difference-of-Convex) program, we prove that it becomes convex when expert
demonstrations outweigh undesirable demonstrations, enabling a practical and
stable non-adversarial training objective. Our method avoids adversarial
training and handles both positive and negative demonstrations in a unified
framework. Extensive experiments on standard offline imitation learning
benchmarks demonstrate that our approach consistently outperforms
state-of-the-art baselines.

</details>


### [143] [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)
*Yu Yan,Sheng Sun,Zhifei Zheng,Ziji Hao,Teli Liu,Min Liu*

Main category: cs.LG

TL;DR: The paper presents PoisonSwarm, a new framework for generating diverse harmful data using model crowdsourcing and dynamic model switching to overcome limitations in existing LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of generating reliable and diverse harmful data for adversarial testing and safeguard development in AI applications due to limitations in safety alignment mechanisms of LLMs.

Method: Proposes PoisonSwarm, which uses model crowdsourcing to generate diverse harmful data. It generates benign data as templates, decomposes them into semantic units, and performs toxification unit-by-unit with dynamic model switching for refinement.

Result: PoisonSwarm achieves state-of-the-art performance in synthesizing various categories of harmful data with high scalability and diversity.

Conclusion: PoisonSwarm effectively overcomes generation reliability and content diversity challenges in harmful data synthesis.

Abstract: To construct responsible and secure AI applications, harmful information data
is widely utilized for adversarial testing and the development of safeguards.
Existing studies mainly leverage Large Language Models (LLMs) to synthesize
data to obtain high-quality task datasets at scale, thereby avoiding costly
human annotation. However, limited by the safety alignment mechanisms of LLMs,
the synthesis of harmful data still faces challenges in generation reliability
and content diversity. In this study, we propose a novel harmful information
synthesis framework, PoisonSwarm, which applies the model crowdsourcing
strategy to generate diverse harmful data while maintaining a high success
rate. Specifically, we generate abundant benign data as the based templates in
a counterfactual manner. Subsequently, we decompose each based template into
multiple semantic units and perform unit-by-unit toxification and final
refinement through dynamic model switching, thus ensuring the success of
synthesis. Experimental results demonstrate that PoisonSwarm achieves
state-of-the-art performance in synthesizing different categories of harmful
data with high scalability and diversity.

</details>


### [144] [Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection](https://arxiv.org/abs/2505.21201)
*Steven Sam,Silima Marshal DAbreo*

Main category: cs.LG

TL;DR: 在印度农业面临生产力低下、资源压力和气候变化的挑战下，本文探讨了使用计算工具如作物推荐系统帮助农民提高生产力的可能性。研究通过环境与经济因素对19种作物进行分析，评估了随机森林（Random Forest）和支持向量机（SVM）模型的性能。结果表明，考虑时间顺序和滞后变量的方法提高了模型的实用性和适应性，其中基于滞后变量的随机森林模型是推荐的最佳算法。


<details>
  <summary>Details</summary>
Motivation: 印度农业作为食品生产、经济增长和就业的主要来源，正面临低农场生产力和产量的问题，这些问题因自然资源的压力和气候变化的影响而加剧。现有的农业改进措施效果有限，因此需要新的方法来提供洞察力并帮助农民解决低生产力问题。

Method: 本研究使用了环境和经济因素，针对印度15个州的19种作物，开发并评估了随机森林（RF）和支持向量机（SVM）模型。采用10折交叉验证、时间序列分割和滞后变量等方法进行模型训练和评估，以确定最佳模型。

Result: 10折交叉验证显示高准确率（RF: 99.96%，SVM: 94.71%），但存在过拟合问题。引入时间顺序后，时间序列分割方法的性能有所下降（RF: 78.55%，SVM: 71.18%）。使用滞后变量方法进一步提升模型准确性（RF: 83.62%，SVM: 74.38%）。

Conclusion: 综合来看，时间序列分割和滞后变量方法提供了实际应用中的更好见解，能够处理时间依赖性并增强对变化农业条件的适应能力。基于滞后变量的随机森林模型被确定为在印度背景下最优的作物推荐算法。

Abstract: Agriculture constitutes a primary source of food production, economic growth
and employment in India, but the sector is confronted with low farm
productivity and yields aggravated by increased pressure on natural resources
and adverse climate change variability. Efforts involving green revolution,
land irrigations, improved seeds and organic farming have yielded suboptimal
outcomes. The adoption of computational tools like crop recommendation systems
offers a new way to provide insights and help farmers tackle low productivity.
However, most agricultural recommendation systems in India focus narrowly on
environmental factors and regions, limiting accurate predictions of high-yield,
profitable crops. This study uses environmental and economic factors with 19
crops across 15 states to develop and evaluate Random Forest and SVM models
using 10-fold Cross Validation, Time-series Split, and Lag Variables. The
10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but
raised overfitting concerns. Introducing temporal order, better reflecting
real-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the
Time-series Split.To further increase the model accuracy while maintaining the
temporal order, the Lag Variables approach was employed, which resulted in
improved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross
validation approach. Overall, the models in the Time-series Split and Lag
Variable Approaches offer practical insights by handling temporal dependencies
and enhancing its adaptability to changing agricultural conditions over time.
Consequently, the study shows the Random Forest model developed based on the
Lag Variables as the most preferred algorithm for optimal crop recommendation
in the Indian context.

</details>


### [145] [Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics](https://arxiv.org/abs/2505.21204)
*Marie Steinacker,Yuri Kheifetz,Markus Scholz*

Main category: cs.LG

TL;DR: The paper explores hybrid and data-driven models for predicting chemotherapy-induced hematotoxicity, finding data-driven methods excel with ample data while hybrid/mechanistic models are better with limited data.


<details>
  <summary>Details</summary>
Motivation: Hematotoxicity is a common side effect of chemotherapy with high variability among patients, making it challenging to predict using current mechanistic models alone.

Method: Developed and compared hybrid mechanistic-data driven models (universal differential equations) and purely data-driven models (nonlinear autoregressive exogenous model with gated recurrent units) for predicting platelet counts during chemotherapy.

Result: Data-driven models improve prediction accuracy with sufficient data, especially for high-risk patients. Hybrid/mechanistic models perform better with limited or sparse data.

Conclusion: Proposed framework is generalizable and could be extended to other treatment-related toxicities, offering potential in personalized medicine.

Abstract: Hematotoxicity, drug-induced damage to the blood-forming system, is a
frequent side effect of cytotoxic chemotherapy and poses a significant
challenge in clinical practice due to its high inter-patient variability and
limited predictability. Current mechanistic models often struggle to accurately
forecast outcomes for patients with irregular or atypical trajectories. In this
study, we develop and compare hybrid mechanistic and data-driven approaches for
individualized time series modeling of platelet counts during chemotherapy. We
consider hybrid models that combine mechanistic models with neural networks,
known as universal differential equations. As a purely data-driven alternative,
we utilize a nonlinear autoregressive exogenous model using gated recurrent
units as the underlying architecture. These models are evaluated across a range
of real patient scenarios, varying in data availability and sparsity, to assess
predictive performance. Our findings demonstrate that data-driven methods, when
provided with sufficient data, significantly improve prediction accuracy,
particularly for high-risk patients with irregular platelet dynamics. This
highlights the potential of data-driven approaches in enhancing clinical
decision-making. In contrast, hybrid and mechanistic models are superior in
scenarios with limited or sparse data. The proposed modeling and comparison
framework is generalizable and could be extended to predict other
treatment-related toxicities, offering broad applicability in personalized
medicine.

</details>


### [146] [Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection](https://arxiv.org/abs/2505.21219)
*Qinjun Fei,Nuria Rodríguez-Barroso,María Victoria Luzón,Zhongliang Zhang,Francisco Herrera*

Main category: cs.LG

TL;DR: In cross-silo Federated Learning, client selection is crucial but challenging due to data quality, budget constraints, and incentive compatibility. This paper proposes SBRO-FL, a framework integrating dynamic bidding, reputation modeling, and cost-aware selection. It uses Shapley values to evaluate client contributions, a reputation system inspired by prospect theory, and formulates the selection problem as an integer program. Experiments on various datasets show improvements in accuracy, convergence speed, and robustness.


<details>
  <summary>Details</summary>
Motivation: Client selection in cross-silo Federated Learning is critical for high model performance but remains difficult due to data quality decompensation, budget constraints, and incentive compatibility. Existing approaches handle these challenges separately, making joint optimization hard.

Method: The proposed method, SBRO-FL, includes clients submitting bids based on their perceived data quality, evaluating contributions using Shapley values, implementing a reputation system inspired by prospect theory, and formulating the client selection problem as a 0-1 integer program that maximizes reputation-weighted utility under budget constraints.

Result: Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets demonstrate that SBRO-FL improves accuracy, convergence speed, and robustness even in adversarial and low-bid interference scenarios.

Conclusion: The results emphasize the importance of balancing data reliability, incentive compatibility, and cost efficiency for scalable and trustworthy Federated Learning deployments.

Abstract: In cross-silo Federated Learning (FL), client selection is critical to ensure
high model performance, yet it remains challenging due to data quality
decompensation, budget constraints, and incentive compatibility. As training
progresses, these factors exacerbate client heterogeneity and degrade global
performance. Most existing approaches treat these challenges in isolation,
making jointly optimizing multiple factors difficult. To address this, we
propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a
unified framework integrating dynamic bidding, reputation modeling, and
cost-aware selection. Clients submit bids based on their perceived data
quality, and their contributions are evaluated using Shapley values to quantify
their marginal impact on the global model. A reputation system, inspired by
prospect theory, captures historical performance while penalizing
inconsistency. The client selection problem is formulated as a 0-1 integer
program that maximizes reputation-weighted utility under budget constraints.
Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that
SBRO-FL improves accuracy, convergence speed, and robustness, even in
adversarial and low-bid interference scenarios. Our results highlight the
importance of balancing data reliability, incentive compatibility, and cost
efficiency to enable scalable and trustworthy FL deployments.

</details>


### [147] [Why Do More Experts Fail? A Theoretical Analysis of Model Merging](https://arxiv.org/abs/2505.21226)
*Zijing Wang,Xingle Xu,Yongkang Liu,Yiqun Zhang,Peiqin Lin,Shi Feng,Xiaocui Yang,Daling Wang,Hinrich Schütze*

Main category: cs.LG

TL;DR: The paper explores the limitations of model merging, proves an upper bound and optimal threshold for merging models, and introduces RHT method to improve performance. Empirical results validate theoretical analysis on 12 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Model merging reduces storage and computational resources by combining multiple expert models into a single multi-task model, but maintaining performance gains as the number of merged models increases is challenging.

Method: The authors prove an upper bound on model merging, analyze the effective parameter space using Gaussian Width, and use Approximate Kinematics Theory to find an optimal threshold. They also introduce the Reparameterized Heavy-Tailed (RHT) method to extend the coverage of the merged model.

Result: Empirical results on 12 benchmarks, including knowledge-intensive and general-purpose tasks, confirm the theoretical analysis and show that RHT enhances the performance of merged models.

Conclusion: These findings highlight the limitations of model merging and provide a direction for future research beyond current methods.

Abstract: Model merging dramatically reduces storage and computational resources by
combining multiple expert models into a single multi-task model. Although
recent model merging methods have shown promising results, they struggle to
maintain performance gains as the number of merged models increases. In this
paper, we investigate the key obstacles that limit the scalability of model
merging when integrating a large number of expert models. First, we prove that
there is an upper bound on model merging. Further theoretical analysis reveals
that the limited effective parameter space imposes a strict constraint on the
number of models that can be successfully merged. Gaussian Width shows that the
marginal benefit of merging additional models diminishes according to a
strictly concave function. This implies that the effective parameter space
becomes rapidly saturated as the number of merged models increases.
Furthermore, using Approximate Kinematics Theory, we prove the existence of a
unique optimal threshold beyond which adding more models does not yield
significant performance improvements. At the same time, we introduce a
straightforward Reparameterized Heavy-Tailed method (RHT) to extend the
coverage of the merged model, thereby enhancing its performance. Empirical
results on 12 benchmarks, including both knowledge-intensive and
general-purpose tasks, validate our theoretical analysis. We believe that these
results spark further research beyond the current scope of model merging. The
source code is in the anonymous Github repository
https://github.com/wzj1718/ModelMergingAnalysis.

</details>


### [148] [Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies](https://arxiv.org/abs/2505.21236)
*Felix Chalumeau,Daniel Rajaonarivonivelomanantsoa,Ruan de Kock,Claude Formanek,Sasha Abramowitz,Oumayma Mahjoub,Wiem Khlifi,Simon Du Toit,Louay Ben Nessir,Refiloe Shabe,Arnol Fokam,Siddarth Singh,Ulrich Mbou Sob,Arnu Pretorius*

Main category: cs.LG

TL;DR: 通过在执行时使用推理阶段和选择相应的推理策略，可以打破复杂多智能体强化学习问题中的性能上限。这种方法在17个任务中比之前的最先进方法平均提高了45%，最多提高了126%，仅需额外花费数秒的执行时间。这是迄今为止关于复杂RL推理策略的最大规模研究，涉及超过6万次实验。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习系统有许多应用，但现实世界场景通常极其复杂，需要多个智能体之间的复杂协调。即使是最先进的RL系统也可能达到无法通过零样本推理突破的性能上限。然而，许多数字或基于模拟的应用程序允许在输出最终解决方案之前利用特定的时间和计算预算进行多次尝试。

Method: 本研究展示了在执行时使用推理阶段以及选择相应的推理策略，能够突破复杂多智能体强化学习问题中的性能上限。通过在执行过程中增加少量额外时间（几秒钟），并采用合适的推理策略来探索多种可能性，从而提升性能。

Result: 该方法在17个任务中取得了显著的成果，与之前的最先进方法相比，平均性能提升了45%，最大提升达到了126%。此外，还展示了良好的计算扩展特性，并进行了超过60,000次实验以支持这些发现。

Conclusion: 推理阶段及其策略的选择对于提高复杂多智能体强化学习问题的性能至关重要。此研究表明，通过合理利用执行时间内的计算资源，可以显著超越现有方法的性能限制。

Abstract: Reinforcement learning (RL) systems have countless applications, from
energy-grid management to protein design. However, such real-world scenarios
are often extremely difficult, combinatorial in nature, and require complex
coordination between multiple agents. This level of complexity can cause even
state-of-the-art RL systems, trained until convergence, to hit a performance
ceiling which they are unable to break out of with zero-shot inference.
Meanwhile, many digital or simulation-based applications allow for an inference
phase that utilises a specific time and compute budget to explore multiple
attempts before outputting a final solution. In this work, we show that such an
inference phase employed at execution time, and the choice of a corresponding
inference strategy, are key to breaking the performance ceiling observed in
complex multi-agent RL problems. Our main result is striking: we can obtain up
to a 126% and, on average, a 45% improvement over the previous state-of-the-art
across 17 tasks, using only a couple seconds of extra wall-clock time during
execution. We also demonstrate promising compute scaling properties, supported
by over 60k experiments, making it the largest study on inference strategies
for complex RL to date. Our experimental data and code are available at
https://sites.google.com/view/inf-marl.

</details>


### [149] [BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design](https://arxiv.org/abs/2505.21241)
*Divya Nori,Anisha Parsan,Caroline Uhler,Wengong Jin*

Main category: cs.LG

TL;DR: The paper proposes BECraft, a design pipeline that uses pTMEnergy to improve protein binder design.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination-based methods optimize structure prediction confidence metrics, but these metrics do not reflect the statistical likelihood of a binder-target complex and yield sparse gradients for optimization.

Method: The authors propose pTMEnergy, a statistical energy function derived from predicted inter-residue error distributions, which is incorporated into BindEnergyCraft (BECraft), a new design pipeline.

Result: BECraft outperforms existing methods (BindCraft, RFDiffusion, ESM3) in challenging targets, achieving higher in silico binder success rates while reducing structural clashes. pTMEnergy also excels in structure-based virtual screening tasks.

Conclusion: pTMEnergy and BECraft represent significant advancements in protein binder design, offering improved performance and reduced clashes.

Abstract: Protein binder design has been transformed by hallucination-based methods
that optimize structure prediction confidence metrics, such as the interface
predicted TM-score (ipTM), via backpropagation. However, these metrics do not
reflect the statistical likelihood of a binder-target complex under the learned
distribution and yield sparse gradients for optimization. In this work, we
propose a method to extract such likelihoods from structure predictors by
reinterpreting their confidence outputs as an energy-based model (EBM). By
leveraging the Joint Energy-based Modeling (JEM) framework, we introduce
pTMEnergy, a statistical energy function derived from predicted inter-residue
error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a
design pipeline that maintains the same optimization framework as BindCraft but
replaces ipTM with our energy-based objective. BECraft outperforms BindCraft,
RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in
silico binder success rates while reducing structural clashes. Furthermore,
pTMEnergy establishes a new state-of-the-art in structure-based virtual
screening tasks for miniprotein and RNA aptamer binders.

</details>


### [150] [Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework](https://arxiv.org/abs/2505.21251)
*Mustafa Hajij,Lennart Bastian,Sarah Osentoski,Hardik Kabaria,John L. Davenport,Sheik Dawood,Balaji Cherukuri,Joseph G. Kocheemoolayil,Nastaran Shahmansouri,Adrian Lew,Theodore Papamarkou,Tolga Birdal*

Main category: cs.LG

TL;DR: The paper introduces copresheaf topological neural networks (CTNNs), a framework encapsulating various deep learning architectures for structured data, demonstrating superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing principled neural architectures tailored to specific tasks and data types.

Method: Grounding model design in the language of copresheaves from algebraic topology to create a unifying framework.

Result: Empirical results show CTNNs outperform conventional baselines, especially in tasks requiring hierarchical or localized sensitivity.

Conclusion: CTNNs provide a principled, multi-scale foundation for future deep learning architectures.

Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful and
unifying framework that encapsulates a wide spectrum of deep learning
architectures, designed to operate on structured data: including images, point
clouds, graphs, meshes, and topological manifolds. While deep learning has
profoundly impacted domains ranging from digital assistants to autonomous
systems, the principled design of neural architectures tailored to specific
tasks and data types remains one of the field's most persistent open
challenges. CTNNs address this gap by grounding model design in the language of
copresheaves, a concept from algebraic topology that generalizes and subsumes
most practical deep learning models in use today. This abstract yet
constructive formulation yields a rich design space from which theoretically
sound and practically effective solutions can be derived to tackle core
challenges in representation learning: long-range dependencies, oversmoothing,
heterophily, and non-Euclidean domains. Our empirical results on structured
data benchmarks demonstrate that CTNNs consistently outperform conventional
baselines, particularly in tasks requiring hierarchical or localized
sensitivity. These results underscore CTNNs as a principled, multi-scale
foundation for the next generation of deep learning architectures.

</details>


### [151] [Learnable Kernel Density Estimation for Graphs](https://arxiv.org/abs/2505.21285)
*Xudong Wang,Ziheng Sun,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: This paper proposes LGKDE, a framework that uses graph neural networks and maximum mean discrepancy for multi-scale kernel density estimation on graphs, outperforming existing methods in graph anomaly detection.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the key challenge in graph density estimation which is effectively capturing both structural patterns and semantic variations while maintaining theoretical guarantees.

Method: The method proposed in this paper, called LGKDE, leverages graph neural networks to represent each graph as a discrete distribution and utilizes maximum mean discrepancy to learn the graph metric for multi-scale KDE. All parameters are learned by maximizing the density of graphs relative to the density of their well-designed perturbed counterparts.

Result: Theoretically, consistency and convergence guarantees for LGKDE have been established. Empirically, LGKDE demonstrates superior performance compared to state-of-the-art baselines on most benchmark datasets for tasks such as recovering the underlying density of synthetic graph distributions and graph anomaly detection.

Conclusion: LGKDE is an effective framework for graph density estimation that captures both structural patterns and semantic variations, with theoretical guarantees and superior empirical performance.

Abstract: This work proposes a framework LGKDE that learns kernel density estimation
for graphs. The key challenge in graph density estimation lies in effectively
capturing both structural patterns and semantic variations while maintaining
theoretical guarantees. Combining graph kernels and kernel density estimation
(KDE) is a standard approach to graph density estimation, but has
unsatisfactory performance due to the handcrafted and fixed features of
kernels. Our method LGKDE leverages graph neural networks to represent each
graph as a discrete distribution and utilizes maximum mean discrepancy to learn
the graph metric for multi-scale KDE, where all parameters are learned by
maximizing the density of graphs relative to the density of their well-designed
perturbed counterparts. The perturbations are conducted on both node features
and graph spectra, which helps better characterize the boundary of normal
density regions. Theoretically, we establish consistency and convergence
guarantees for LGKDE, including bounds on the mean integrated squared error,
robustness, and complexity. We validate LGKDE by demonstrating its
effectiveness in recovering the underlying density of synthetic graph
distributions and applying it to graph anomaly detection across diverse
benchmark datasets. Extensive empirical evaluation shows that LGKDE
demonstrates superior performance compared to state-of-the-art baselines on
most benchmark datasets.

</details>


### [152] [GSAT: Graph Structure Attention Networks](https://arxiv.org/abs/2505.21288)
*Farshad Noravesh,Reza Haffari,Layki Soon,Arghya Pal*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) are powerful but often neglect structural information leading to issues like oversmoothing. This paper introduces graph structure attention network (GSAT), which uses anonymous random walks (ARWs) to integrate structural and attribute information, improving performance on graph classification benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of neglected structural information in GNNs that leads to problems such as requiring high number of layers and oversmoothing.

Method: Leverage structural information modeled by anonymous random walks (ARWs) and introduce graph structure attention network (GSAT), a generalization of graph attention network (GAT), to integrate original attributes and structural representations.

Result: Experiments show GSAT slightly improves state-of-the-art performance on some graph classification benchmarks.

Conclusion: GSAT is effective in integrating structural and attribute information, enriching graph representation and improving performance.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool for processing
data represented in graph structures, achieving remarkable success across a
wide range of applications. However, to further improve the performance on
graph classification benchmarks, structural representation of each node that
encodes rich local topological information in the neighbourhood of nodes is an
important type of feature that is often overlooked in the modeling. The
consequence of neglecting the structural information has resulted high number
of layers to connect messages from distant nodes which by itself produces other
problems such as oversmoothing. In the present paper, we leverage these
structural information that are modeled by anonymous random walks (ARWs) and
introduce graph structure attention network (GSAT) which is a generalization of
graph attention network(GAT) to integrate the original attribute and the
structural representation to enforce the model to automatically find patterns
for attending to different edges in the node neighbourhood to enrich graph
representation. Our experiments show GSAT slightly improves SOTA on some graph
classification benchmarks.

</details>


### [153] [LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning](https://arxiv.org/abs/2505.21289)
*Nurbek Tastan,Stefanos Laskaridis,Martin Takac,Karthik Nandakumar,Samuel Horvath*

Main category: cs.LG

TL;DR: LoFT is a novel low-rank adaptation method that behaves like full fine-tuning by aligning the optimizer's internal dynamics with those of updating all model weights.


<details>
  <summary>Details</summary>
Motivation: Large pre-trained models are commonly adapted to downstream tasks using parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA). However, LoRA can still underperform full fine-tuning in accuracy and often converges more slowly. To address these limitations, there is a need for an improved low-rank adaptation method.

Method: LoFT not only learns weight updates in a low-rank subspace (like LoRA) but also properly projects the optimizer's first and second moments (Adam's momentum and variance) into the same subspace, mirroring full-model updates.

Result: Empirically, this approach substantially narrows the performance gap between adapter-based tuning and full fine-tuning and consistently outperforms standard LoRA-style methods, all without increasing inference cost.

Conclusion: LoFT eliminates the need for tuning extra hyperparameters, e.g., LoRA scaling factor $	ext{alpha}$, and provides a more effective alternative to standard LoRA-style methods.

Abstract: Large pre-trained models are commonly adapted to downstream tasks using
parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),
which injects small trainable low-rank matrices instead of updating all
weights. While LoRA dramatically reduces trainable parameters with little
overhead, it can still underperform full fine-tuning in accuracy and often
converges more slowly. We introduce LoFT, a novel low-rank adaptation method
that behaves like full fine-tuning by aligning the optimizer's internal
dynamics with those of updating all model weights. LoFT not only learns weight
updates in a low-rank subspace (like LoRA) but also properly projects the
optimizer's first and second moments (Adam's momentum and variance) into the
same subspace, mirroring full-model updates. By aligning the low-rank update
itself with the full update, LoFT eliminates the need for tuning extra
hyperparameters, e.g., LoRA scaling factor $\alpha$. Empirically, this approach
substantially narrows the performance gap between adapter-based tuning and full
fine-tuning and consistently outperforms standard LoRA-style methods, all
without increasing inference cost.

</details>


### [154] [A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features](https://arxiv.org/abs/2505.21317)
*Ihab Bendidi,Yassir El Mesbahi,Alisandra K. Denton,Karush Suri,Kian Kenyon-Dean,Auguste Genovesio,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: The paper proposes a framework to enhance transcriptomics by distilling knowledge from microscopy images using weakly paired data, introducing Semi-Clipped and PEA techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of weakly paired datasets in multimodal learning for understanding cellular responses to stimuli.

Method: Using weakly paired data, the method aligns and binds modalities to enrich gene expression representations with morphological information. It introduces Semi-Clipped for cross-modal distillation and PEA for enhancing transcriptomics data while preserving biological information.

Result: These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.

Conclusion: The proposed framework enhances transcriptomics through knowledge distillation from microscopy images, overcoming data scarcity issues with innovative techniques.

Abstract: Understanding cellular responses to stimuli is crucial for biological
discovery and drug development. Transcriptomics provides interpretable,
gene-level insights, while microscopy imaging offers rich predictive features
but is harder to interpret. Weakly paired datasets, where samples share
biological states, enable multimodal learning but are scarce, limiting their
utility for training and multimodal inference. We propose a framework to
enhance transcriptomics by distilling knowledge from microscopy images. Using
weakly paired data, our method aligns and binds modalities, enriching gene
expression representations with morphological information. To address data
scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal
distillation using pretrained foundation models, achieving state-of-the-art
results, and (2) PEA (Perturbation Embedding Augmentation), a novel
augmentation technique that enhances transcriptomics data while preserving
inherent biological information. These strategies improve the predictive power
and retain the interpretability of transcriptomics, enabling rich unimodal
representations for complex biological tasks.

</details>


### [155] [Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization](https://arxiv.org/abs/2505.21321)
*Leonard Papenmeier,Luigi Nardi*

Main category: cs.LG

TL;DR: The paper introduces Bencher, a modular benchmarking framework for black-box optimization that separates benchmark execution from optimization logic. It uses isolated Python environments and an RPC interface to eliminate dependency conflicts and simplify real-world benchmark integration. Bencher can be deployed locally or remotely via containerization, supports reproducibility, and evaluates 80 benchmarks across various domains.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of dependency conflicts and complex software requirements in existing benchmarking frameworks for black-box optimization.

Method: Created a modular benchmarking framework called Bencher which isolates each benchmark in its own virtual Python environment and accesses them through a unified RPC interface. The framework is deployable via Docker or Singularity on HPC clusters, ensuring containerized reproducibility.

Result: Bencher provides a lightweight client with minimal setup and enables evaluation of 80 benchmarks across continuous, categorical, and binary domains without dependency issues.

Conclusion: Bencher offers a novel approach to black-box optimization benchmarking by decoupling execution from logic, resolving dependency problems, and supporting diverse real-world benchmarks.

Abstract: We present Bencher, a modular benchmarking framework for black-box
optimization that fundamentally decouples benchmark execution from optimization
logic. Unlike prior suites that focus on combining many benchmarks in a single
project, Bencher introduces a clean abstraction boundary: each benchmark is
isolated in its own virtual Python environment and accessed via a unified,
version-agnostic remote procedure call (RPC) interface. This design eliminates
dependency conflicts and simplifies the integration of diverse, real-world
benchmarks, which often have complex and conflicting software requirements.
Bencher can be deployed locally or remotely via Docker or on high-performance
computing (HPC) clusters via Singularity, providing a containerized,
reproducible runtime for any benchmark. Its lightweight client requires minimal
setup and supports drop-in evaluation of 80 benchmarks across continuous,
categorical, and binary domains.

</details>


### [156] [UGCE: User-Guided Incremental Counterfactual Exploration](https://arxiv.org/abs/2505.21330)
*Christos Fragkathoulas,Evaggelia Pitoura*

Main category: cs.LG

TL;DR: The paper introduces User-Guided Incremental Counterfactual Exploration (UGCE), a framework using genetic algorithms to efficiently update counterfactual explanations as user constraints evolve, demonstrating improved computational efficiency and stable performance across benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations (CFEs) are crucial for interpreting machine learning predictions. However, current methods for generating CFEs do not adapt well to changing user-defined feasibility constraints over time, leading to inefficiencies when recalculating from scratch with each change.

Method: The authors propose UGCE, a genetic algorithm-based framework that incrementally updates counterfactuals in response to evolving user constraints, avoiding the need to recompute from scratch and supporting dynamic changes in constraints.

Result: Experiments on five benchmark datasets show that UGCE improves computational efficiency while maintaining high-quality solutions compared to static approaches. It also exhibits stable performance under varying constraint sequences and benefits from an efficient warm-start strategy.

Conclusion: UGCE offers an effective solution for dynamically updating counterfactual explanations in response to user-guided iterative refinements of feasibility constraints, enhancing both efficiency and flexibility in real-world applications.

Abstract: Counterfactual explanations (CFEs) are a popular approach for interpreting
machine learning predictions by identifying minimal feature changes that alter
model outputs. However, in real-world settings, users often refine feasibility
constraints over time, requiring counterfactual generation to adapt
dynamically. Existing methods fail to support such iterative updates, instead
recomputing explanations from scratch with each change, an inefficient and
rigid approach. We propose User-Guided Incremental Counterfactual Exploration
(UGCE), a genetic algorithm-based framework that incrementally updates
counterfactuals in response to evolving user constraints. Experimental results
across five benchmark datasets demonstrate that UGCE significantly improves
computational efficiency while maintaining high-quality solutions compared to a
static, non-incremental approach. Our evaluation further shows that UGCE
supports stable performance under varying constraint sequences, benefits from
an efficient warm-start strategy, and reveals how different constraint types
may affect search behavior.

</details>


### [157] [Joint Learning in the Gaussian Single Index Model](https://arxiv.org/abs/2505.21336)
*Loucas Pillaud-Vivien,Adrien Schertzer*

Main category: cs.LG

TL;DR: The paper explores the problem of learning a one-dimensional projection and a univariate function in high-dimensional Gaussian models. It analyzes gradient flow dynamics, proves convergence, and demonstrates practical implementation using RKHS.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of jointly learning a one-dimensional projection and a univariate function within high-dimensional Gaussian models, capturing a fundamental non-convex problem at the intersection of representation learning and nonlinear regression.

Method: Analyzes the gradient flow dynamics of a natural alternating scheme for predictors of the form $f(x)=\varphi^\star(\langle w^\star, x \rangle)$, proving convergence with a rate controlled by the information exponent reflecting Gaussian regularity. Implements joint learning using a Reproducing Kernel Hilbert Space (RKHS).

Result: Proves that convergence occurs even when the initial direction is negatively correlated with the target. Demonstrates effective practical implementation using RKHS for efficient and flexible estimation.

Conclusion: Offers theoretical insights into learning low-dimensional structures in high-dimensional settings and provides a practical methodology using RKHS.

Abstract: We consider the problem of jointly learning a one-dimensional projection and
a univariate function in high-dimensional Gaussian models. Specifically, we
study predictors of the form $f(x)=\varphi^\star(\langle w^\star, x \rangle)$,
where both the direction $w^\star \in \mathcal{S}_{d-1}$, the sphere of
$\mathbb{R}^d$, and the function $\varphi^\star: \mathbb{R} \to \mathbb{R}$ are
learned from Gaussian data. This setting captures a fundamental non-convex
problem at the intersection of representation learning and nonlinear
regression. We analyze the gradient flow dynamics of a natural alternating
scheme and prove convergence, with a rate controlled by the information
exponent reflecting the \textit{Gaussian regularity} of the function
$\varphi^\star$. Strikingly, our analysis shows that convergence still occurs
even when the initial direction is negatively correlated with the target. On
the practical side, we demonstrate that such joint learning can be effectively
implemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the
structure of the problem, enabling efficient and flexible estimation of the
univariate function. Our results offer both theoretical insight and practical
methodology for learning low-dimensional structure in high-dimensional
settings.

</details>


### [158] [An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction](https://arxiv.org/abs/2505.21339)
*Henryk Mustroph,Michel Kunkler,Stefanie Rinderle-Ma*

Main category: cs.LG

TL;DR: Suffix prediction in business processes typically focuses on predicting a single most likely suffix, but this can be limiting when there is uncertainty or variability. This paper proposes probabilistic suffix prediction using an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and Monte Carlo (MC) suffix sampling algorithm to approximate a probability distribution of suffixes. It captures epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. The evaluation shows that the U-ED-LSTM has reasonable predictive performance across datasets, aggregating probabilistic predictions into mean values can outperform most likely predictions for rare prefixes or longer suffixes, and the approach effectively captures uncertainties.


<details>
  <summary>Details</summary>
Motivation: Current approaches focus on predicting a single, most likely suffix which can be limiting if the future course of a process is exposed to uncertainty or has high variability.

Method: The method proposed is called probabilistic suffix prediction, based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. Epistemic uncertainties are captured via MC dropout and aleatoric uncertainties as learned loss attenuation.

Result: i) The U-ED-LSTM has reasonable predictive performance across various datasets; ii) Aggregating probabilistic suffix predictions into mean values can outperform most likely predictions, particularly for rare prefixes or longer suffixes; iii) The approach effectively captures uncertainties present in event logs.

Conclusion: Probabilistic suffix prediction provides a more expressive way to forecast the remaining sequence of events until process completion, especially under conditions of uncertainty or high variability.

Abstract: Suffix prediction of business processes forecasts the remaining sequence of
events until process completion. Current approaches focus on predicting a
single, most likely suffix. However, if the future course of a process is
exposed to uncertainty or has high variability, the expressiveness of a single
suffix prediction can be limited. To address this limitation, we propose
probabilistic suffix prediction, a novel approach that approximates a
probability distribution of suffixes. The proposed approach is based on an
Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC)
suffix sampling algorithm. We capture epistemic uncertainties via MC dropout
and aleatoric uncertainties as learned loss attenuation. This technical report
provides a detailed evaluation of the U-ED-LSTM's predictive performance and
assesses its calibration on four real-life event logs with three different
hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable
predictive performance across various datasets, ii) aggregating probabilistic
suffix predictions into mean values can outperform most likely predictions,
particularly for rare prefixes or longer suffixes, and iii) the approach
effectively captures uncertainties present in event logs.

</details>


### [159] [OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models](https://arxiv.org/abs/2505.21347)
*Ziheng Cheng,Yixiao Huang,Hui Xu,Somayeh Sojoudi,Xuandong Zhao,Dawn Song,Song Mei*

Main category: cs.LG

TL;DR: 为了应对文本到图像（T2I）模型中的过度拒绝问题，本文提出了一个大规模基准测试OVERT，包含4600个看似有害但良性的提示和1785个真正有害的提示。通过评估多个领先T2I模型，发现过度拒绝是一个普遍存在的问题。初步尝试通过提示重写来减少过度拒绝，但发现这通常会损害对原始提示含义的忠实性。最后，展示了生成框架在适应多样化安全需求方面的灵活性。


<details>
  <summary>Details</summary>
Motivation: 尽管已经观察到文本到图像（T2I）模型中的过度拒绝现象，但目前缺乏系统评估此现象的大规模基准测试。过度拒绝减少了T2I模型的实际可用性，因此需要一种方法来量化和改进这种行为。

Method: 作者开发了一个自动工作流来构建合成评估数据，创建了OVERT基准测试。该基准包括4600个看似有害但实际上无害的提示，以及1785个真正有害的提示。使用OVERT评估多个领先的T2I模型，并探索提示重写作为一种减少过度拒绝的方法。此外，还展示了生成框架如何根据用户定义的策略生成定制评估数据。

Result: 通过OVERT的评估显示，过度拒绝是T2I模型中一个广泛存在的问题，影响多个类别。提示重写虽然作为减少过度拒绝的一种尝试，但往往降低了对原始提示含义的忠实性。

Conclusion: OVERT是首个大规模基准测试，用于评估T2I模型中的过度拒绝行为。研究结果强调了进一步研究的必要性，以改善T2I模型的安全对齐，而不牺牲其功能。同时，提示重写等方法需要进一步改进以保持原意忠实性。

Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating
visual content from text inputs. Although multiple safety alignment strategies
have been proposed to prevent harmful outputs, they often lead to overly
cautious behavior -- rejecting even benign prompts -- a phenomenon known as
$\textit{over-refusal}$ that reduces the practical utility of T2I models.
Despite over-refusal having been observed in practice, there is no large-scale
benchmark that systematically evaluates this phenomenon for T2I models. In this
paper, we present an automatic workflow to construct synthetic evaluation data,
resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on
$\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing
over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful
but benign prompts across nine safety-related categories, along with 1,785
genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility
trade-off. Using OVERT, we evaluate several leading T2I models and find that
over-refusal is a widespread issue across various categories (Figure 1),
underscoring the need for further research to enhance the safety alignment of
T2I models without compromising their functionality.As a preliminary attempt to
reduce over-refusal, we explore prompt rewriting; however, we find it often
compromises faithfulness to the meaning of the original prompts. Finally, we
demonstrate the flexibility of our generation framework in accommodating
diverse safety requirements by generating customized evaluation data adapting
to user-defined policies.

</details>


### [160] [CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models](https://arxiv.org/abs/2505.21360)
*Dhanesh Ramachandram*

Main category: cs.LG

TL;DR: CRISP-NAM是一种可解释的神经加法模型，用于竞争风险生存分析，通过专门的神经网络独立地对每个特征进行风险估计，允许可视化协变量与每种竞争风险之间的复杂非线性关系。


<details>
  <summary>Details</summary>
Motivation: 在生存建模中，特别是在医疗领域，竞争风险是需要考虑的重要因素，因为患者可能会经历多种不同的事件类型。

Method: 提出了一种名为CRISP-NAM的竞争风险可解释生存预测神经加法模型，该模型扩展了神经加法架构以模拟特定原因的风险，同时保持了特征级别的可解释性。每个特征通过专用的神经网络独立地对风险估计做出贡献。

Result: 在多个数据集上展示了与现有方法相比具有竞争力的性能。

Conclusion: CRISP-NAM能够在建模特定原因风险的同时保持特征级别的可解释性，并在实际应用中表现出色。

Abstract: Competing risks are crucial considerations in survival modelling,
particularly in healthcare domains where patients may experience multiple
distinct event types. We propose CRISP-NAM (Competing Risks Interpretable
Survival Prediction with Neural Additive Models), an interpretable neural
additive model for competing risks survival analysis which extends the neural
additive architecture to model cause-specific hazards while preserving
feature-level interpretability. Each feature contributes independently to risk
estimation through dedicated neural networks, allowing for visualization of
complex non-linear relationships between covariates and each competing risk. We
demonstrate competitive performance on multiple datasets compared to existing
approaches.

</details>


### [161] [Subgroups Matter for Robust Bias Mitigation](https://arxiv.org/abs/2505.21363)
*Anissa Alloula,Charles Jones,Ben Glocker,Bartłomiej W. Papież*

Main category: cs.LG

TL;DR: 在机器学习的偏差缓解方法中，子群定义的选择对性能有显著影响，不当的子群定义可能导致更差的结果。通过理论分析，作者提出了一种反直觉的见解：有时针对特定子群提高公平性，最好使用不同的子群进行缓解。这项工作强调了在偏差缓解中仔细定义子群的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管不断有新的偏差缓解方法被开发出来，但没有一种方法能够始终成功。一个基本问题仍未得到解答：偏差缓解技术何时以及为何会失败？

Method: 假设关键因素可能是许多偏差缓解方法共有的但常被忽视的步骤——子群定义。通过对多个视觉和语言分类任务中的先进偏差缓解方法进行全面评估，并系统地改变子群定义（包括粗略、细粒度、交叉性和噪声子群），研究子群选择对性能的影响。

Result: 结果表明，子群选择显著影响性能，某些分组甚至导致比不进行缓解更差的结果。观察到一组子群之间的差异并不是使用这些子群进行缓解的充分理由。

Conclusion: 研究揭示了仔细定义子群在偏差缓解中的重要性，并建议将其作为提高机器学习模型稳健性和公平性的替代杠杆。

Abstract: Despite the constant development of new bias mitigation methods for machine
learning, no method consistently succeeds, and a fundamental question remains
unanswered: when and why do bias mitigation techniques fail? In this paper, we
hypothesise that a key factor may be the often-overlooked but crucial step
shared by many bias mitigation methods: the definition of subgroups. To
investigate this, we conduct a comprehensive evaluation of state-of-the-art
bias mitigation methods across multiple vision and language classification
tasks, systematically varying subgroup definitions, including coarse,
fine-grained, intersectional, and noisy subgroups. Our results reveal that
subgroup choice significantly impacts performance, with certain groupings
paradoxically leading to worse outcomes than no mitigation at all. Our findings
suggest that observing a disparity between a set of subgroups is not a
sufficient reason to use those subgroups for mitigation. Through theoretical
analysis, we explain these phenomena and uncover a counter-intuitive insight
that, in some cases, improving fairness with respect to a particular set of
subgroups is best achieved by using a different set of subgroups for
mitigation. Our work highlights the importance of careful subgroup definition
in bias mitigation and suggest it as a alternative lever for improving the
robustness and fairness of machine learning models.

</details>


### [162] [Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders](https://arxiv.org/abs/2505.21364)
*James Oldfield,Shawn Im,Yixuan Li,Mihalis A. Nicolaou,Ioannis Patras,Grigorios G Chrysos*

Main category: cs.LG

TL;DR: Mixture of Decoders (MxDs) is proposed to solve the accuracy trade-off in sparse layer approximation of multilayer perceptrons (MLPs). MxDs can significantly outperform state-of-the-art methods on the sparsity-accuracy frontier and learn specialized features of natural language.


<details>
  <summary>Details</summary>
Motivation: Current methods for learning interpretable approximations of MLPs via neuron-level sparsity fail to faithfully reconstruct the original mapping, which leads to an increase in the model's next-token cross-entropy loss. To overcome this accuracy trade-off, the paper advocates moving to layer-level sparsity.

Method: The method introduces Mixture of Decoders (MxDs), which generalizes MLPs and Gated Linear Units. It expands pre-trained dense layers into tens of thousands of specialized sublayers through a flexible form of tensor factorization. Each sparsely activating MxD sublayer implements a linear transformation with full-rank weights, preserving the expressive capacity of the original decoders even under heavy sparsity.

Result: Experimentally, MxDs significantly outperform state-of-the-art methods on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations show that MxDs learn similarly specialized features of natural language.

Conclusion: MxDs offer a promising new avenue for designing interpretable yet faithful decompositions of language models.

Abstract: Multilayer perceptrons (MLPs) are an integral part of large language models,
yet their dense representations render them difficult to understand, edit, and
steer. Recent methods learn interpretable approximations via neuron-level
sparsity, yet fail to faithfully reconstruct the original
mapping--significantly increasing model's next-token cross-entropy loss. In
this paper, we advocate for moving to layer-level sparsity to overcome the
accuracy trade-off in sparse layer approximation. Under this paradigm, we
introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear
Units, expanding pre-trained dense layers into tens of thousands of specialized
sublayers. Through a flexible form of tensor factorization, each sparsely
activating MxD sublayer implements a linear transformation with full-rank
weights--preserving the original decoders' expressive capacity even under heavy
sparsity. Experimentally, we show that MxDs significantly outperform
state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier
in language models with up to 3B parameters. Further evaluations on sparse
probing and feature steering demonstrate that MxDs learn similarly specialized
features of natural language--opening up a promising new avenue for designing
interpretable yet faithful decompositions. Our code is included at:
https://github.com/james-oldfield/MxD/.

</details>


### [163] [PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment](https://arxiv.org/abs/2505.21366)
*Qi Yu,Zhichen Zeng,Yuchen Yan,Zhining Liu,Baoyu Jing,Ruizhong Qiu,Ariful Azad,Hanghang Tong*

Main category: cs.LG

TL;DR: The paper presents PLANETALIGN, a Python library for network alignment that includes datasets, methods, and evaluation pipelines to facilitate the development and benchmarking of NA methods.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive libraries that can systematically develop and benchmark network alignment methods despite growing research in this area.

Method: PLANETALIGN integrates 18 datasets and 14 NA methods with extensible APIs. It also features a standardized evaluation pipeline encompassing various metrics for assessing effectiveness, scalability, and robustness of NA methods.

Result: Through extensive comparative studies, the strengths and limitations of existing NA methods are revealed, providing practical insights into the problem of network alignment.

Conclusion: The authors hope that PLANETALIGN will deepen the understanding of network alignment problems and promote the creation of more effective, scalable, and robust methods.

Abstract: Network alignment (NA) aims to identify node correspondence across different
networks and serves as a critical cornerstone behind various downstream
multi-network learning tasks. Despite growing research in NA, there lacks a
comprehensive library that facilitates the systematic development and
benchmarking of NA methods. In this work, we introduce PLANETALIGN, a
comprehensive Python library for network alignment that features a rich
collection of built-in datasets, methods, and evaluation pipelines with
easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA
methods with extensible APIs for easy use and development of NA methods. Our
standardized evaluation pipeline encompasses a wide range of metrics, enabling
a systematic assessment of the effectiveness, scalability, and robustness of NA
methods. Through extensive comparative studies, we reveal practical insights
into the strengths and limitations of existing NA methods. We hope that
PLANETALIGN can foster a deeper understanding of the NA problem and facilitate
the development and benchmarking of more effective, scalable, and robust
methods in the future. The source code of PLANETALIGN is available at
https://github.com/yq-leo/PlanetAlign.

</details>


### [164] [Improving LLM-based Global Optimization with Search Space Partitioning](https://arxiv.org/abs/2505.21372)
*Andrej Schwanke,Lyubomir Ivanov,David Salinas,Fabio Ferreira,Aaron Klein,Frank Hutter,Arber Zela*

Main category: cs.LG

TL;DR: HOLLM是一种改进的全局优化算法，通过将搜索空间划分为有希望的子区域并结合LLM驱动的采样，在标准优化基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在昂贵的黑盒函数的全局优化框架中作为有效的代理模型和候选生成器取得了有希望的结果，但在高维搜索空间或缺乏特定领域先验的情况下，它们通常难以提供有用的建议。

Method: 提出了一种名为HOLLM的新算法，该算法通过将搜索空间划分为有希望的子区域来增强LLM驱动的采样。每个子区域作为一个'元臂'，通过受多臂老虎机启发的评分机制进行选择，从而有效地平衡探索与利用。然后在每个选定的子区域内，LLM提出高质量的候选点，无需任何明确的领域知识。

Result: 在标准优化基准上的实证评估表明，HOLLM始终与领先的贝叶斯优化和信任区域方法相匹配或超越，并且大大优于基于全局LLM的采样策略。

Conclusion: HOLLM通过结合LLM的能力和带臂老虎机机制，成功地解决了高维搜索空间中的优化问题，为未来的优化研究提供了新的方向。

Abstract: Large Language Models (LLMs) have recently emerged as effective surrogate
models and candidate generators within global optimization frameworks for
expensive blackbox functions. Despite promising results, LLM-based methods
often struggle in high-dimensional search spaces or when lacking
domain-specific priors, leading to sparse or uninformative suggestions. To
overcome these limitations, we propose HOLLM, a novel global optimization
algorithm that enhances LLM-driven sampling by partitioning the search space
into promising subregions. Each subregion acts as a ``meta-arm'' selected via a
bandit-inspired scoring mechanism that effectively balances exploration and
exploitation. Within each selected subregion, an LLM then proposes high-quality
candidate points, without any explicit domain knowledge. Empirical evaluation
on standard optimization benchmarks shows that HOLLM consistently matches or
surpasses leading Bayesian optimization and trust-region methods, while
substantially outperforming global LLM-based sampling strategies.

</details>


### [165] [DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models](https://arxiv.org/abs/2505.21382)
*Nastaran Saadati,Zhanhong Jiang,Joshua R. Waite,Shreyan Ganguly,Aditya Balu,Chinmay Hegde,Soumik Sarkar*

Main category: cs.LG

TL;DR: Low-Rank Adaptation (LoRA) is an efficient fine-tuning approach for VLMs and LLMs, but its decentralized version (DLoRA) faces challenges. This paper improves DLoRA's convergence rate to match decentralized SGD and introduces DeCAF algorithm to resolve consensus interference.


<details>
  <summary>Details</summary>
Motivation: LoRA is effective and computationally tractable for fine-tuning large models, but its decentralized version (DLoRA) has not been well explored due to lack of smoothness guarantee and model consensus interference.

Method: The paper ensures gradient smoothness to improve the convergence rate of DLoRA and matches it with decentralized SGD. It also introduces DeCAF algorithm which integrates DLoRA with TSVD-based matrix factorization to resolve consensus interference.

Result: Theoretical analysis shows that TSVD's approximation error is bounded and consensus differences between DLoRA and DeCAF vanish as rank increases, leading to matching convergence rates. Extensive experiments show that the proposed algorithms outperform local training and rival federated learning under both IID and non-IID data distributions.

Conclusion: The paper successfully improves the convergence rate of DLoRA and resolves consensus interference through DeCAF, demonstrating superior performance across vision/language tasks.

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most effective,
computationally tractable fine-tuning approaches for training Vision-Language
Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by
freezing the pre-trained model weights and injecting trainable low-rank
matrices, allowing for efficient learning of these foundation models even on
edge devices. However, LoRA in decentralized settings still remains under
explored, particularly for the theoretical underpinnings due to the lack of
smoothness guarantee and model consensus interference (defined formally below).
This work improves the convergence rate of decentralized LoRA (DLoRA) to match
the rate of decentralized SGD by ensuring gradient smoothness. We also
introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular
value decomposition (TSVD)-based matrix factorization to resolve consensus
interference. Theoretical analysis shows TSVD's approximation error is bounded
and consensus differences between DLoRA and DeCAF vanish as rank increases,
yielding DeCAF's matching convergence rate. Extensive experiments across
vision/language tasks demonstrate our algorithms outperform local training and
rivals federated learning under both IID and non-IID data distributions.

</details>


### [166] [Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features](https://arxiv.org/abs/2505.21391)
*Zixuan Xie,Xinyu Liu,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: Linear TD($\lambda$)的L2收敛率在任意特征下被首次建立，无需线性独立特征假设或算法修改。


<details>
  <summary>Details</summary>
Motivation: 现有的收敛率研究通常依赖于线性独立特征的假设，这在许多实际场景中并不成立，因此需要一种新的方法来处理任意特征下的收敛问题。

Method: 通过开发一个新的随机逼近结果，该结果以解集的收敛率为特色，解决了由于任意特征可能导致的解的非唯一性问题。

Result: 成功建立了线性TD($\lambda$)在任意特征下的首个L2收敛率，并且适用于折扣和平均奖励两种设置。

Conclusion: 本研究为线性TD($\lambda$)在更广泛的特征条件下提供了理论支持，拓展了其在实践中的应用范围。

Abstract: Linear TD($\lambda$) is one of the most fundamental reinforcement learning
algorithms for policy evaluation. Previously, convergence rates are typically
established under the assumption of linearly independent features, which does
not hold in many practical scenarios. This paper instead establishes the first
$L^2$ convergence rates for linear TD($\lambda$) operating under arbitrary
features, without making any algorithmic modification or additional
assumptions. Our results apply to both the discounted and average-reward
settings. To address the potential non-uniqueness of solutions resulting from
arbitrary features, we develop a novel stochastic approximation result
featuring convergence rates to the solution set instead of a single point.

</details>


### [167] [Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits](https://arxiv.org/abs/2505.21393)
*Maoli Liu,Zhuohua Li,Xiangxiang Dai,John C. S. Lui*

Main category: cs.LG

TL;DR: 通过提出三种新算法CLiSK、CLiME和CLiSK-ME，解决了当前对话推荐系统中关键术语选择探索不足和对话发起时机不当的问题。这些算法在理论上达到了更紧的遗憾上界，并在实验中表现出至少14.6%的累积遗憾改进。


<details>
  <summary>Details</summary>
Motivation: 现有的对话推荐系统算法存在两个主要问题：1) 关键术语选择策略探索不足，导致用户偏好估计不充分；2) 使用确定性规则发起对话，造成不必要的交互或错过机会。这些问题限制了其在实际场景中的效果。

Method: 提出了三个新算法：CLiSK、CLiME和CLiSK-ME。CLiSK通过引入平滑的关键术语上下文来增强偏好学习中的探索；CLiME根据偏好不确定性自适应地发起对话；CLiSK-ME则结合了前两者的优点。同时，证明了这三个算法在时间范围T内达到更紧的遗憾上界O(√dT log T)，并提供了Ω(√dT)的匹配下界。

Result: 理论分析表明所提出的算法具有更紧的遗憾上界，接近最优解。实验结果表明，在合成数据集和真实世界数据集上，与现有方法相比，累积遗憾至少提高了14.6%。

Conclusion: 提出的三个算法有效地解决了当前对话推荐系统中的关键问题，理论和实验证明了其优越性和接近最优的效果。

Abstract: Conversational recommender systems proactively query users with relevant "key
terms" and leverage the feedback to elicit users' preferences for personalized
recommendations. Conversational contextual bandits, a prevalent approach in
this domain, aim to optimize preference learning by balancing exploitation and
exploration. However, several limitations hinder their effectiveness in
real-world scenarios. First, existing algorithms employ key term selection
strategies with insufficient exploration, often failing to thoroughly probe
users' preferences and resulting in suboptimal preference estimation. Second,
current algorithms typically rely on deterministic rules to initiate
conversations, causing unnecessary interactions when preferences are
well-understood and missed opportunities when preferences are uncertain. To
address these limitations, we propose three novel algorithms: CLiSK, CLiME, and
CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in
preference learning, CLiME adaptively initiates conversations based on
preference uncertainty, and CLiSK-ME integrates both techniques. We
theoretically prove that all three algorithms achieve a tighter regret upper
bound of $O(\sqrt{dT\log{T}})$ with respect to the time horizon $T$, improving
upon existing methods. Additionally, we provide a matching lower bound
$\Omega(\sqrt{dT})$ for conversational bandits, demonstrating that our
algorithms are nearly minimax optimal. Extensive evaluations on both synthetic
and real-world datasets show that our approaches achieve at least a 14.6%
improvement in cumulative regret.

</details>


### [168] [Square$χ$PO: Differentially Private and Robust $χ^2$-Preference Optimization in Offline Direct Alignment](https://arxiv.org/abs/2505.21395)
*Xingyu Zhou,Yulian Wu,Wenqian Weng,Francesco Orabona*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的方法SquareχPO，通过替换标准log-loss为概率平方损失，在偏好标签被污染和隐私保护的情况下，实现了语言模型与人类偏好的离线对齐。该方法在差分隐私和鲁棒性方面均取得了最先进的结果，并且可以同时处理隐私保护和标签污染问题。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型对齐方法在面对偏好标签被污染和需要隐私保护时存在不足，特别是在实现鲁棒性和隐私保护的同时保持高性能方面。因此，需要一种新方法来解决这些问题。

Method: 提出了一种名为SquareχPO的新方法，将标准log-loss替换为概率平方损失。利用这种方法，作者在局部模型和集中模型下分别实现了最优的对齐性能，并首次提供了在一般函数逼近下的理论保证。此外，SquareχPO还可以同时处理隐私保护和标签污染问题。

Result: SquareχPO在差分隐私保护和鲁棒性方面都达到了最先进的水平。它在局部模型下首次达到了基于单策略集中的最优速率，并在集中模型下提供了首个隐私保护结果。同时，它是第一个在一般函数逼近下具有有意义理论保证的对齐方法。

Conclusion: SquareχPO不仅在差分隐私和鲁棒性方面表现出色，还能同时处理隐私保护和标签污染问题。其理论分析建立在一个新的最小二乘回归泛化误差界上，这一结果对社区具有独立的价值。

Abstract: In this paper, we theoretically study the offline alignment of language
models with human preference feedback, under both preference label corruption
and privacy protections. To this end, we propose Square$\chi$PO, a simple
one-line change to $\chi$PO where the standard log-loss is replaced by a new
square loss over probability. Thanks to the inherent properties of this new
loss, we have advanced the state-of-the-art of differentially private and
robust offline direct alignment. Specifically, for the local model of label
privacy, Square$\chi$PO is the first algorithm that attains an optimal rate
based on single-policy concentrability even with general function
approximations. It also gives the first result under the central model of
privacy protection over both prompts (responses) and labels. On the robustness
side against Huber label corruption, Square$\chi$PO is the first alignment
method that has a meaningful theoretical guarantee under general function
approximations. More importantly, Square$\chi$PO can address privacy protection
and corruption simultaneously, where an interesting separation is observed,
implying that the order of privacy and corruption matters. Furthermore, we show
that Square$\chi$PO can also be easily extended to handle the scenario of the
general preference model with state-of-the-art guarantees under corruption and
privacy. Last but not least, all of our theoretical guarantees enjoy a unified
analysis, building upon a new result on the generalization error bounds of
least-square regression under corruption and privacy constraints, which we
believe is of independent interest to the community.

</details>


### [169] [A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2505.21400)
*Gen Li,Changxiao Cai*

Main category: cs.LG

TL;DR: Diffusion models show great potential for large language models by enabling parallel token sampling, and this paper provides theoretical analysis of their convergence from an information-theoretic perspective.


<details>
  <summary>Details</summary>
Motivation: Despite the empirical success of diffusion models, their theoretical understanding remains underdeveloped, especially in the context of language models.

Method: The authors develop convergence guarantees for diffusion language models using an information-theoretic approach, analyzing the sampling error measured by the Kullback-Leibler (KL) divergence.

Result: The sampling error decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens. Matching upper and lower bounds are established, demonstrating the tightness of the analysis.

Conclusion: This work offers novel theoretical insights into why diffusion language models are practically effective, contributing to a deeper understanding of these models.

Abstract: Diffusion models have emerged as a powerful paradigm for modern generative
modeling, demonstrating strong potential for large language models (LLMs).
Unlike conventional autoregressive (AR) models that generate tokens
sequentially, diffusion models enable parallel token sampling, leading to
faster generation and eliminating left-to-right generation constraints. Despite
their empirical success, the theoretical understanding of diffusion model
approaches remains underdeveloped. In this work, we develop convergence
guarantees for diffusion language models from an information-theoretic
perspective. Our analysis demonstrates that the sampling error, measured by the
Kullback-Leibler (KL) divergence, decays inversely with the number of
iterations $T$ and scales linearly with the mutual information between tokens
in the target text sequence. In particular, we establish matching upper and
lower bounds, up to some constant factor, to demonstrate the tightness of our
convergence analysis. These results offer novel theoretical insights into the
practical effectiveness of diffusion language models.

</details>


### [170] [Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks](https://arxiv.org/abs/2505.21404)
*Anas Jnini,Flavio Vella*

Main category: cs.LG

TL;DR: The paper proposes Dual Natural Gradient Descent (D-NGD), a method to improve the training of Physics-Informed Neural Networks (PINNs) by performing Gauss-Newton updates in a smaller residual space, leading to significantly reduced computational complexity and better performance.


<details>
  <summary>Details</summary>
Motivation: Natural-gradient methods accelerate PINN training but suffer from high computational complexity due to the need for solving Gauss-Newton updates in the parameter space.

Method: Introduce D-NGD which computes Gauss-Newton steps in residual space instead of parameter space, applies geodesic-acceleration correction, and offers both dense direct and Nystrom-preconditioned conjugate-gradient solvers.

Result: D-NGD scales second-order PINN optimization to networks with up to 12.8 million parameters, achieves much lower final error than first-order methods, and allows natural-gradient training on a single GPU.

Conclusion: D-NGD provides an effective solution for optimizing large-scale PINNs with significant improvements in efficiency and scalability.

Abstract: Natural-gradient methods markedly accelerate the training of Physics-Informed
Neural Networks (PINNs), yet their Gauss--Newton update must be solved in the
parameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is
the number of network trainable weights. We show that exactly the same step can
instead be formulated in a generally smaller residual space of size $m =
\sum_{\gamma} N_{\gamma} d_{\gamma}$, where each residual class $\gamma$ (e.g.
PDE interior, boundary, initial data) contributes $N_{\gamma}$ collocation
points of output dimension $d_{\gamma}$.
  Building on this insight, we introduce \textit{Dual Natural Gradient Descent}
(D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it
with a geodesic-acceleration correction at negligible extra cost, and provides
both a dense direct solver for modest $m$ and a Nystrom-preconditioned
conjugate-gradient solver for larger $m$.
  Experimentally, D-NGD scales second-order PINN optimization to networks with
up to 12.8 million parameters, delivers one- to three-order-of-magnitude lower
final error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton
methods, and -- crucially -- enables natural-gradient training of PINNs at this
scale on a single GPU.

</details>


### [171] [A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment](https://arxiv.org/abs/2505.21414)
*Brett Bissey,Kyle Gatesman,Walker Dimon,Mohammad Alam,Luis Robaina,Joseph Weissman*

Main category: cs.LG

TL;DR: This paper introduces a comprehensive framework for analyzing and securing decision-support systems trained with Deep Reinforcement Learning (DRL) before deployment. It aids in developing observation perturbations, assessing adversarial attack outcomes, discovering and ranking the impact of attacks, and evaluating the transferability of adversarial attacks across agent architectures and DRL training algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the vulnerabilities of decision-support systems trained with Deep Reinforcement Learning (DRL) and to provide insights into learned behavior patterns prior to deployment.

Method: The method involves introducing a framework that helps in developing precisely timed and targeted observation perturbations. This allows researchers to assess adversarial attack outcomes within a strategic decision-making context. The framework is validated, agent behavior is visualized, and adversarial outcomes are evaluated within the context of a custom-built strategic game called CyberStrike. A method is also introduced for systematically discovering and ranking the impact of attacks on various observation indices and time-steps.

Result: The experiments conducted using the proposed framework demonstrate the ability to discover and rank the impact of attacks, as well as evaluate the transferability of adversarial attacks across different agent architectures and DRL training algorithms.

Conclusion: The findings highlight the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.

Abstract: This paper introduces a comprehensive framework designed to analyze and
secure decision-support systems trained with Deep Reinforcement Learning (DRL),
prior to deployment, by providing insights into learned behavior patterns and
vulnerabilities discovered through simulation. The introduced framework aids in
the development of precisely timed and targeted observation perturbations,
enabling researchers to assess adversarial attack outcomes within a strategic
decision-making context. We validate our framework, visualize agent behavior,
and evaluate adversarial outcomes within the context of a custom-built
strategic game, CyberStrike. Utilizing the proposed framework, we introduce a
method for systematically discovering and ranking the impact of attacks on
various observation indices and time-steps, and we conduct experiments to
evaluate the transferability of adversarial attacks across agent architectures
and DRL training algorithms. The findings underscore the critical need for
robust adversarial defense mechanisms to protect decision-making policies in
high-stakes environments.

</details>


### [172] [When Shift Happens - Confounding Is to Blame](https://arxiv.org/abs/2505.21422)
*Abbavaram Gowtham Reddy,Celia Rubio-Madrigal,Rebekka Burkholz,Krikamol Muandet*

Main category: cs.LG

TL;DR: Distribution shifts caused by hidden confounding challenge the robustness of machine learning models. Empirical risk minimization (ERM) can outperform state-of-the-art OOD generalization methods due to the use of all covariates. Effective OOD generalization requires learning environment-specific relationships and using proxies for hidden confounders.


<details>
  <summary>Details</summary>
Motivation: To understand why empirical risk minimization (ERM) can rival or even outperform state-of-the-art out-of-distribution (OOD) generalization methods, especially when all available covariates are used, not just causal ones.

Method: Analyze the impact of hidden confounding on distribution shifts and demonstrate through both empirical and theoretical evidence that effective OOD generalization requires learning environment-specific relationships and utilizing proxies for hidden confounders.

Result: Empirical and theoretical results show that hidden confounding shifts can be mitigated by using proxies for hidden confounders and learning environment-specific relationships rather than relying solely on invariant ones.

Conclusion: The findings provide new theoretical insights and practical guidance for designing robust OOD generalization algorithms and selecting appropriate covariates.

Abstract: Distribution shifts introduce uncertainty that undermines the robustness and
generalization capabilities of machine learning models. While conventional
wisdom suggests that learning causal-invariant representations enhances
robustness to such shifts, recent empirical studies present a counterintuitive
finding: (i) empirical risk minimization (ERM) can rival or even outperform
state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its
OOD generalization performance improves when all available covariates, not just
causal ones, are utilized. Drawing on both empirical and theoretical evidence,
we attribute this phenomenon to hidden confounding. Shifts in hidden
confounding induce changes in data distributions that violate assumptions
commonly made by existing OOD generalization approaches. Under such conditions,
we prove that effective generalization requires learning environment-specific
relationships, rather than relying solely on invariant ones. Furthermore, we
show that models augmented with proxies for hidden confounders can mitigate the
challenges posed by hidden confounding shifts. These findings offer new
theoretical insights and practical guidance for designing robust OOD
generalization algorithms and principled covariate selection strategies.

</details>


### [173] [Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization](https://arxiv.org/abs/2505.21423)
*Vit Fojtik,Maria Matveev,Hung-Hsu Chou,Gitta Kutyniok,Johannes Maly*

Main category: cs.LG

TL;DR: 研究发现，仅关注单一的隐式偏差不足以解释梯度下降的良好泛化性能，需要更广泛的隐式正则化观点来捕捉由不可忽略的学习率引起的范数和sharpness之间的动态权衡。


<details>
  <summary>Details</summary>
Motivation: 理解过参数化神经网络卓越的泛化能力，特别是优化算法对良性解决方案的隐式偏差的作用。

Method: 通过理论分析和实证研究结合，探讨不同形式的隐式正则化之间的相互作用，以及学习率如何在模型参数范数和sharpness之间进行平衡。

Result: 实证表明学习率在低参数范数和训练模型的低sharpness之间起到平衡作用；对于对角线性网络的回归任务证明，单独的隐式偏差并不能最小化泛化误差。

Conclusion: 需要一个更广泛的观点来看待隐式正则化，以解释良好的泛化性能，这包括捕捉由非可忽略的学习率引起的范数和sharpness之间的动态权衡。

Abstract: A widely believed explanation for the remarkable generalization capacities of
overparameterized neural networks is that the optimization algorithms used for
training induce an implicit bias towards benign solutions. To grasp this
theoretically, recent works examine gradient descent and its variants in
simplified training settings, often assuming vanishing learning rates. These
studies reveal various forms of implicit regularization, such as $\ell_1$-norm
minimizing parameters in regression and max-margin solutions in classification.
Concurrently, empirical findings show that moderate to large learning rates
exceeding standard stability thresholds lead to faster, albeit oscillatory,
convergence in the so-called Edge-of-Stability regime, and induce an implicit
bias towards minima of low sharpness (norm of training loss Hessian). In this
work, we argue that a comprehensive understanding of the generalization
performance of gradient descent requires analyzing the interaction between
these various forms of implicit regularization. We empirically demonstrate that
the learning rate balances between low parameter norm and low sharpness of the
trained model. We furthermore prove for diagonal linear networks trained on a
simple regression task that neither implicit bias alone minimizes the
generalization error. These findings demonstrate that focusing on a single
implicit bias is insufficient to explain good generalization, and they motivate
a broader view of implicit regularization that captures the dynamic trade-off
between norm and sharpness induced by non-negligible learning rates.

</details>


### [174] [Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate](https://arxiv.org/abs/2505.21430)
*Shiwei Zeng,Jie Shen*

Main category: cs.LG

TL;DR: This paper addresses the challenge of attribute-efficient learning of sparse halfspaces in the presence of malicious noise, providing an algorithm that achieves PAC learning with poly(s,log d) samples.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the importance of attribute-efficient learning of sparse halfspaces and the increasing need for machine learning algorithms to be robust against data corruptions or adversarial attacks.

Method: The method involves using simple variants of existing hinge loss minimization programs, under the assumption that the underlying distribution satisfies concentration and margin conditions. A new gradient analysis is also introduced to handle the sparsity constraint in hinge loss minimization.

Result: The result is an attribute-efficient PAC learning algorithm that can operate effectively under a constant malicious noise rate.

Conclusion: This work contributes to the field by demonstrating that attribute-efficiency can be achieved even in noisy conditions, advancing the understanding of robust learning algorithms.

Abstract: Attribute-efficient learning of sparse halfspaces has been a fundamental
problem in machine learning theory. In recent years, machine learning
algorithms are faced with prevalent data corruptions or even adversarial
attacks. It is of central interest to design efficient algorithms that are
robust to noise corruptions. In this paper, we consider that there exists a
constant amount of malicious noise in the data and the goal is to learn an
underlying $s$-sparse halfspace $w^* \in \mathbb{R}^d$ with $\text{poly}(s,\log
d)$ samples. Specifically, we follow a recent line of works and assume that the
underlying distribution satisfies a certain concentration condition and a
margin condition at the same time. Under such conditions, we show that
attribute-efficiency can be achieved by simple variants to existing hinge loss
minimization programs. Our key contribution includes: 1) an attribute-efficient
PAC learning algorithm that works under constant malicious noise rate; 2) a new
gradient analysis that carefully handles the sparsity constraint in hinge loss
minimization.

</details>


### [175] [Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution](https://arxiv.org/abs/2505.21438)
*Yiwen Tu,Ziqi Liu,Jiaqi W. Ma,Weijing Tang*

Main category: cs.LG

TL;DR: Measuring task relatedness and mitigating negative transfer in Multitask Learning (MTL) is challenging. This paper proposes MultiTask Influence Function (MTIF), which extends data attribution to MTL for fine-grained task relatedness measurement. MTIF not only accurately approximates model performance on subsets of data but also enables a data selection strategy that improves MTL model performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for measuring task relatedness in MTL are coarse and do not adequately address negative transfer issues. There's a need for finer, more instance-level measures of task relatedness to better understand and improve MTL models.

Method: The authors propose MTIF, an adaptation of influence functions to MTL models with either hard or soft parameter sharing. This method provides fine-grained, instance-level measurements of task relatedness, enabling a data selection strategy to mitigate negative transfer.

Result: Experiments show that MTIF efficiently and accurately approximates the performance of models trained on data subsets. Additionally, the data selection strategy using MTIF consistently enhances MTL model performance.

Conclusion: This work establishes a new link between data attribution and MTL by introducing MTIF, offering a fine-grained solution for measuring task relatedness and improving MTL models.

Abstract: Measuring task relatedness and mitigating negative transfer remain a critical
open challenge in Multitask Learning (MTL). This work extends data attribution
-- which quantifies the influence of individual training data points on model
predictions -- to MTL setting for measuring task relatedness. We propose the
MultiTask Influence Function (MTIF), a method that adapts influence functions
to MTL models with hard or soft parameter sharing. Compared to conventional
task relatedness measurements, MTIF provides a fine-grained, instance-level
relatedness measure beyond the entire-task level. This fine-grained relatedness
measure enables a data selection strategy to effectively mitigate negative
transfer in MTL. Through extensive experiments, we demonstrate that the
proposed MTIF efficiently and accurately approximates the performance of models
trained on data subsets. Moreover, the data selection strategy enabled by MTIF
consistently improves model performance in MTL. Our work establishes a novel
connection between data attribution and MTL, offering an efficient and
fine-grained solution for measuring task relatedness and enhancing MTL models.

</details>


### [176] [Can Large Reasoning Models Self-Train?](https://arxiv.org/abs/2505.21444)
*Sheikh Shafayat,Fahim Tajwar,Ruslan Salakhutdinov,Jeff Schneider,Andrea Zanette*

Main category: cs.LG

TL;DR: This paper presents an online self-training reinforcement learning algorithm for large language models (LLMs) that uses the model's self-consistency to infer correctness signals without human supervision. It achieves performance comparable to methods trained on correct answers but identifies issues like reward hacking as a limitation.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on human supervision in scaling LLMs' performance and overcome limitations of existing methods such as reinforcement learning from automated verification.

Method: An online self-training reinforcement learning algorithm is proposed, where the model leverages its own self-consistency to generate correctness signals without any ground-truth supervision.

Result: The algorithm performs well on challenging mathematical reasoning tasks, reaching levels similar to those trained on gold-standard answers. However, it is prone to reward hacking, favoring confidently incorrect outputs.

Conclusion: Self-supervised improvement can lead to significant performance gains in LLMs without external labels, but fundamental challenges such as reward hacking must be addressed.

Abstract: Scaling the performance of large language models (LLMs) increasingly depends
on methods that reduce reliance on human supervision. Reinforcement learning
from automated verification offers an alternative, but it incurs scalability
limitations due to dependency upon human-designed verifiers. Self-training,
where the model's own judgment provides the supervisory signal, presents a
compelling direction. We propose an online self-training reinforcement learning
algorithm that leverages the model's self-consistency to infer correctness
signals and train without any ground-truth supervision. We apply the algorithm
to challenging mathematical reasoning tasks and show that it quickly reaches
performance levels rivaling reinforcement-learning methods trained explicitly
on gold-standard answers. Additionally, we analyze inherent limitations of the
algorithm, highlighting how the self-generated proxy reward initially
correlated with correctness can incentivize reward hacking, where confidently
incorrect outputs are favored. Our results illustrate how self-supervised
improvement can achieve significant performance gains without external labels,
while also revealing its fundamental challenges.

</details>


### [177] [Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling](https://arxiv.org/abs/2505.21452)
*Xiangxin Zhou,Mingyu Li,Yi Xiao,Jiahan Li,Dongyu Xue,Zaixiang Zheng,Jianzhu Ma,Quanquan Gu*

Main category: cs.LG

TL;DR: Cyclic peptides are advantageous in pharmaceuticals due to their stability and affinity. Computational design of cyclic peptides is challenging due to lack of 3D data, geometric constraints, and non-canonical amino acids. CpSDE addresses these challenges with AtomSDE and ResRouter components, enabling the generation of diverse cyclic peptides with reliable stability and affinity.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in computational methods for designing diverse types of cyclic peptides, such as the scarcity of 3D structural data, geometric constraints from cyclization, and involvement of non-canonical amino acids.

Method: CpSDE consists of two key components: AtomSDE, a generative structure prediction model based on harmonic SDE; and ResRouter, a residue type predictor. A routed sampling algorithm alternates between these models to iteratively update sequences and structures, employing explicit all-atom and bond modeling.

Result: The experimental results demonstrate that the cyclic peptides designed by CpSDE exhibit reliable stability and affinity.

Conclusion: CpSDE facilitates the generation of cyclic peptides, overcoming existing data limitations and proficient in designing a wide variety of cyclic peptides.

Abstract: Cyclic peptides offer inherent advantages in pharmaceuticals. For example,
cyclic peptides are more resistant to enzymatic hydrolysis compared to linear
peptides and usually exhibit excellent stability and affinity. Although deep
generative models have achieved great success in linear peptide design, several
challenges prevent the development of computational methods for designing
diverse types of cyclic peptides. These challenges include the scarcity of 3D
structural data on target proteins and associated cyclic peptide ligands, the
geometric constraints that cyclization imposes, and the involvement of
non-canonical amino acids in cyclization. To address the above challenges, we
introduce CpSDE, which consists of two key components: AtomSDE, a generative
structure prediction model based on harmonic SDE, and ResRouter, a residue type
predictor. Utilizing a routed sampling algorithm that alternates between these
two models to iteratively update sequences and structures, CpSDE facilitates
the generation of cyclic peptides. By employing explicit all-atom and bond
modeling, CpSDE overcomes existing data limitations and is proficient in
designing a wide variety of cyclic peptides. Our experimental results
demonstrate that the cyclic peptides designed by our method exhibit reliable
stability and affinity.

</details>


### [178] [High-Dimensional Calibration from Swap Regret](https://arxiv.org/abs/2505.21460)
*Maxwell Fishelson,Noah Golowich,Mehryar Mohri,Jon Schneider*

Main category: cs.LG

TL;DR: 研究了在任意凸集$\mathcal{P} \subset \mathbb{R}^d$和任意范数$\Vert\cdot\Vert$下的多维预测在线校准问题。通过连接外部后悔最小化与在线线性优化，提出了一种无需访问任何在线线性优化子程序或最优率$\rho$知识的算法，并证明了获得$\epsilon$-校准预测所需的轮数为$T = \exp(O(\rho /\epsilon^2))$。此外，还证明了对于$d$维单纯形上的$\ell_1$-校准误差，达到$\epsilon T$误差的任何在线校准算法至少需要$T \geq \exp(\mathrm{poly}(1/\epsilon))$轮，表明对$1/\epsilon$的指数依赖是必要的。


<details>
  <summary>Details</summary>
Motivation: 为了实现多维预测的在线校准，特别是在任意凸集和任意范数下的校准问题，同时避免对在线线性优化子程序或最优率$\rho$的依赖，寻求一种通用且高效的算法解决方案。

Method: 通过将在线校准问题与外部后悔最小化问题联系起来，使用最优正则化器将校准误差上界限制为交换后悔，并通过运行带有Follow-The-Leader子程序的TreeSwap算法来最小化该后悔。此方法不依赖于特定的在线线性优化子程序或最优率$\rho$的知识。

Result: 获得了在$T = \exp(O(\rho /\epsilon^2))$轮后实现$\epsilon$-校准预测的保证。此外，证明了对于$d$维单纯形上的$\ell_1$-校准误差，达到$\epsilon T$误差的任何在线校准算法至少需要$T \geq \exp(\mathrm{poly}(1/\epsilon))$轮。

Conclusion: 提出的方法成功实现了无需特定子程序或最优率知识的多维预测在线校准，并证明了对$1/\epsilon$的指数依赖是必要的，从而加强了现有理论结果。

Abstract: We study the online calibration of multi-dimensional forecasts over an
arbitrary convex set $\mathcal{P} \subset \mathbb{R}^d$ relative to an
arbitrary norm $\Vert\cdot\Vert$. We connect this with the problem of external
regret minimization for online linear optimization, showing that if it is
possible to guarantee $O(\sqrt{\rho T})$ worst-case regret after $T$ rounds
when actions are drawn from $\mathcal{P}$ and losses are drawn from the dual
$\Vert \cdot \Vert_*$ unit norm ball, then it is also possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\rho /\epsilon^2))$ rounds.
When $\mathcal{P}$ is the $d$-dimensional simplex and $\Vert \cdot \Vert$ is
the $\ell_1$-norm, the existence of $O(\sqrt{T\log d})$-regret algorithms for
learning with experts implies that it is possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\log{d}/\epsilon^2)) =
d^{O(1/\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).
  Interestingly, our algorithm obtains this guarantee without requiring access
to any online linear optimization subroutine or knowledge of the optimal rate
$\rho$ -- in fact, our algorithm is identical for every setting of
$\mathcal{P}$ and $\Vert \cdot \Vert$. Instead, we show that the optimal
regularizer for the above OLO problem can be used to upper bound the above
calibration error by a swap regret, which we then minimize by running the
recent TreeSwap algorithm with Follow-The-Leader as a subroutine.
  Finally, we prove that any online calibration algorithm that guarantees
$\epsilon T$ $\ell_1$-calibration error over the $d$-dimensional simplex
requires $T \geq \exp(\mathrm{poly}(1/\epsilon))$ (assuming $d \geq
\mathrm{poly}(1/\epsilon)$). This strengthens the corresponding
$d^{\Omega(\log{1/\epsilon})}$ lower bound of Peng, and shows that an
exponential dependence on $1/\epsilon$ is necessary.

</details>


### [179] [Causal Posterior Estimation](https://arxiv.org/abs/2505.21468)
*Simon Dirmeier,Antonietta Mira*

Main category: cs.LG

TL;DR: The paper introduces Causal Posterior Estimation (CPE), a new method for Bayesian inference in simulator models that uses normalizing flow-based approximation to posterior distribution, improving accuracy by incorporating conditional dependencies from graphical models directly into neural networks.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference in simulator models is challenging due to the intractability or high computational cost of evaluating likelihood functions. Current methods may not fully exploit the structure inherent in these models.

Method: CPE utilizes normalizing flow-based approximation to the posterior distribution and incorporates the conditional dependence structure induced by the graphical representation of the model into the neural network. It introduces both discrete and continuous NF architectures and proposes a constant-time sampling procedure for the continuous case.

Result: Through extensive experimental evaluation, CPE demonstrates highly accurate posterior inference, outperforming or matching the state of the art in the field.

Conclusion: Causal Posterior Estimation provides an effective approach for Bayesian inference in simulator models by leveraging the conditional dependencies from graphical models, leading to improved accuracy.

Abstract: We present Causal Posterior Estimation (CPE), a novel method for Bayesian
inference in simulator models, i.e., models where the evaluation of the
likelihood function is intractable or too computationally expensive, but where
one can simulate model outputs given parameter values. CPE utilizes a
normalizing flow-based (NF) approximation to the posterior distribution which
carefully incorporates the conditional dependence structure induced by the
graphical representation of the model into the neural network. Thereby it is
possible to improve the accuracy of the approximation. We introduce both
discrete and continuous NF architectures for CPE and propose a constant-time
sampling procedure for the continuous case which reduces the computational
complexity of drawing samples to O(1) as for discrete NFs. We show, through an
extensive experimental evaluation, that by incorporating the conditional
dependencies induced by the graphical model directly into the neural network,
rather than learning them from data, CPE is able to conduct highly accurate
posterior inference either outperforming or matching the state of the art in
the field.

</details>


### [180] [Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models](https://arxiv.org/abs/2505.21475)
*Ilias Diakonikolas,Giannis Iakovidis,Daniel M. Kane,Lisheng Ren*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the complexity of learning real-valued Multi-Index Models (MIMs)
under the Gaussian distribution. A $K$-MIM is a function $f:\mathbb{R}^d\to
\mathbb{R}$ that depends only on the projection of its input onto a
$K$-dimensional subspace. We give a general algorithm for PAC learning a broad
class of MIMs with respect to the square loss, even in the presence of
adversarial label noise. Moreover, we establish a nearly matching Statistical
Query (SQ) lower bound, providing evidence that the complexity of our algorithm
is qualitatively optimal as a function of the dimension. Specifically, we
consider the class of bounded variation MIMs with the property that degree at
most $m$ distinguishing moments exist with respect to projections onto any
subspace. In the presence of adversarial label noise, the complexity of our
learning algorithm is $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$. For the
realizable and independent noise settings, our algorithm incurs complexity
$d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$. To complement our upper
bound, we show that if for some subspace degree-$m$ distinguishing moments do
not exist, then any SQ learner for the corresponding class of MIMs requires
complexity $d^{\Omega(m)}$. As an application, we give the first efficient
learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The
resulting algorithm has complexity $\mathrm{poly}(d)
2^{\mathrm{poly}(KL/\epsilon)}$. This gives a new PAC learning algorithm for
Lipschitz homogeneous ReLU networks with complexity independent of the network
size, removing the exponential dependence incurred in prior work.

</details>


### [181] [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487)
*Ted Zadouri,Hubert Strauss,Tri Dao*

Main category: cs.LG

TL;DR: 通过重新设计注意力机制，提出GTA和GLA两种方法，在不牺牲模型质量的情况下减少内存传输、提高硬件效率并保持并行可扩展性。实验表明，GTA在使用约一半KV缓存的情况下匹配GQA质量，而GLA比FlashMLA快2倍，并在在线服务基准测试中将端到端延迟减少、吞吐量提升至2倍。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型解码受限于从高带宽内存加载键值（KV）缓存，导致每标记的延迟增加，同时解码的顺序性限制了并行性。需要探索算术强度、并行化和模型质量之间的相互作用，以评估当前架构是否充分利用了现代硬件。

Method: 1. 提出Grouped-Tied Attention (GTA)，通过组合和重用键和值状态来减少内存传输，而不影响模型质量。
2. 引入Grouped Latent Attention (GLA)，这是一种与低级优化结合的并行友好的潜在注意力机制，能够在保持高模型质量的同时实现快速解码。

Result: - GTA在使用约一半KV缓存的情况下匹配Grouped-Query Attention (GQA)的质量。
- GLA匹配Multi-head Latent Attention (MLA)的质量，并且更容易分片。
- 优化后的GLA内核在推测解码设置下比FlashMLA快2倍。
- GLA通过减少每个设备的KV缓存获取，将端到端延迟减少并在在线服务基准测试中将吞吐量提升至2倍。

Conclusion: 本研究通过重新设计注意力机制提高了硬件效率，减少了内存传输，并在保持模型质量和并行可扩展性的前提下显著改善了解码性能。

Abstract: LLM decoding is bottlenecked for large batches and long contexts by loading
the key-value (KV) cache from high-bandwidth memory, which inflates per-token
latency, while the sequential nature of decoding limits parallelism. We analyze
the interplay among arithmetic intensity, parallelization, and model quality
and question whether current architectures fully exploit modern hardware. This
work redesigns attention to perform more computation per byte loaded from
memory to maximize hardware efficiency without trading off parallel
scalability. We first propose Grouped-Tied Attention (GTA), a simple variant
that combines and reuses key and value states, reducing memory transfers
without compromising model quality. We then introduce Grouped Latent Attention
(GLA), a parallel-friendly latent attention paired with low-level optimizations
for fast decoding while maintaining high model quality. Experiments show that
GTA matches Grouped-Query Attention (GQA) quality while using roughly half the
KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier
to shard. Our optimized GLA kernel is up to 2$\times$ faster than FlashMLA, for
example, in a speculative decoding setting when the query length exceeds one.
Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end
latency and increases throughput in online serving benchmarks by up to
2$\times$.

</details>


### [182] [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
*Xiangxin Zhou,Zichen Liu,Anya Sims,Haonan Wang,Tianyu Pang,Chongxuan Li,Liang Wang,Min Lin,Chao Du*

Main category: cs.LG

TL;DR: 提出了一种无需验证器的方法（VeriFree），通过强化学习直接最大化生成参考答案的概率，适用于更广泛的推理领域，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于DeepSeek-R1-Zero风格的强化学习方法虽然在代码和数学推理方面取得了显著进展，但其依赖规则-based验证的方式限制了其在化学、医疗等实际领域的应用。此外，使用额外的语言模型作为验证器带来了计算负担和安全性问题。

Method: 提出了一种名为VeriFree的验证器自由方法，该方法绕过答案验证，而是利用强化学习直接最大化生成参考答案的概率。此方法将策略和隐式验证器集成到一个统一模型中，并可视为一种变分优化方法。

Result: 在MMLU-Pro、GPQA、SuperGPQA和数学相关基准测试中，VeriFree不仅减少了计算需求，还匹配甚至超越了基于验证器的方法的表现。

Conclusion: VeriFree提供了一种优雅的解决方案，将策略和隐式验证器结合在一个统一模型中，具有显著的实际优势和较低的计算要求，为扩展到一般推理领域提供了新的思路。

Abstract: The recent paradigm shift towards training large language models (LLMs) using
DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has
led to impressive advancements in code and mathematical reasoning. However,
this methodology is limited to tasks where rule-based answer verification is
possible and does not naturally extend to real-world domains such as chemistry,
healthcare, engineering, law, biology, business, and economics. Current
practical workarounds use an additional LLM as a model-based verifier; however,
this introduces issues such as reliance on a strong verifier LLM,
susceptibility to reward hacking, and the practical burden of maintaining the
verifier model in memory during training. To address this and extend
DeepSeek-R1-Zero-style training to general reasoning domains, we propose a
verifier-free method (VeriFree) that bypasses answer verification and instead
uses RL to directly maximize the probability of generating the reference
answer. We compare VeriFree with verifier-based methods and demonstrate that,
in addition to its significant practical benefits and reduced compute
requirements, VeriFree matches and even surpasses verifier-based methods on
extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related
benchmarks. Moreover, we provide insights into this method from multiple
perspectives: as an elegant integration of training both the policy and
implicit verifier in a unified model, and as a variational optimization
approach. Code is available at https://github.com/sail-sg/VeriFree.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [183] [EarthOL: A Proof-of-Human-Contribution Consensus Protocol -- Addressing Fundamental Challenges in Decentralized Value Assessment with Enhanced Verification and Security Mechanisms](https://arxiv.org/abs/2505.20614)
*Jiaxiong He*

Main category: cs.CR

TL;DR: This paper introduces EarthOL, a novel consensus protocol that replaces computational waste in blockchain systems with verifiable human contributions within bounded domains. It proposes an enhanced Proof-of-Human-Contribution (PoHC) protocol using multi-layered verification and presents theoretical analysis demonstrating progress toward incentive-compatible human contribution verification.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to replace the computational waste in blockchain systems with verifiable human contributions while acknowledging cultural diversity and subjective preferences, and maintaining cryptographic security.

Method: The method involves proposing a domain-restricted approach with an enhanced Proof-of-Human-Contribution (PoHC) protocol that uses a multi-layered verification system with domain-specific evaluation criteria, time-dependent validation mechanisms, and comprehensive security frameworks.

Result: The result shows meaningful progress towards incentive-compatible human contribution verification in high-consensus domains, achieving Byzantine fault tolerance in controlled scenarios while addressing scalability and cultural bias challenges. Game-theoretic analysis, probabilistic modeling, and enhanced security protocols identify specific conditions for stability and examine failure modes with mitigation strategies.

Conclusion: This work contributes to understanding the boundaries of decentralized value assessment and provides a framework for future research in human-centered consensus mechanisms for specific application domains, with emphasis on validator and security specialist incentive systems.

Abstract: This paper introduces EarthOL, a novel consensus protocol that attempts to
replace computational waste in blockchain systems with verifiable human
contributions within bounded domains. While recognizing the fundamental
impossibility of universal value assessment, we propose a domain-restricted
approach that acknowledges cultural diversity and subjective preferences while
maintaining cryptographic security. Our enhanced Proof-of-Human-Contribution
(PoHC) protocol uses a multi-layered verification system with domain-specific
evaluation criteria, time-dependent validation mechanisms, and comprehensive
security frameworks. We present theoretical analysis demonstrating meaningful
progress toward incentive-compatible human contribution verification in
high-consensus domains, achieving Byzantine fault tolerance in controlled
scenarios while addressing significant scalability and cultural bias
challenges. Through game-theoretic analysis, probabilistic modeling, and
enhanced security protocols, we identify specific conditions under which the
protocol remains stable and examine failure modes with comprehensive mitigation
strategies. This work contributes to understanding the boundaries of
decentralized value assessment and provides a framework for future research in
human-centered consensus mechanisms for specific application domains, with
particular emphasis on validator and security specialist incentive systems.

</details>


### [184] [Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification](https://arxiv.org/abs/2505.20866)
*Xinjie Lin,Gang Xiong,Gaopeng Gou,Wenqi Dong,Jing Yu,Zhen Li,Wei Xia*

Main category: cs.CR

TL;DR: The paper introduces ETooL, a novel traffic representation model that integrates LLMs with traffic structures through self-supervised instruction tuning, achieving robust classification and significant F1 score improvements in both supervised and zero-shot tasks. A new dataset, NETD, is also constructed to support dynamic distributional shifts.


<details>
  <summary>Details</summary>
Motivation: Encrypted traffic classification faces challenges such as distribution drift and dependence on labeled data. Existing approaches are limited by the closed-world assumption and lack of generalization. The authors aim to leverage the potential of large language models (LLMs) for more adaptable and generalizable solutions in traffic analysis.

Method: The authors propose ETooL, which combines LLMs with traffic structure knowledge using a self-supervised instruction tuning paradigm. This approach establishes connections between textual information and traffic interactions, enhancing adaptability and generalization. Additionally, they construct NETD, a dataset designed for dynamic distributional shifts.

Result: ETooL demonstrates superior performance with significant improvements in F1 scores across different datasets and conditions, including APP53 (I.I.D.), APP53 (O.O.D.), and ISCX-Botnet (O.O.D.). It shows robust classification performance and enhanced generalization in both supervised and zero-shot tasks.

Conclusion: ETooL effectively addresses the challenges of encrypted traffic classification by integrating LLMs with traffic domain knowledge. It achieves notable success in handling distribution shifts and improving classification accuracy. The construction of NETD further supports research into dynamic distributional conditions.

Abstract: Encrypted traffic classification is highly challenging in network security
due to the need for extracting robust features from content-agnostic traffic
data. Existing approaches face critical issues: (i) Distribution drift, caused
by reliance on the closedworld assumption, limits adaptability to realworld,
shifting patterns; (ii) Dependence on labeled data restricts applicability
where such data is scarce or unavailable. Large language models (LLMs) have
demonstrated remarkable potential in offering generalizable solutions across a
wide range of tasks, achieving notable success in various specialized fields.
However, their effectiveness in traffic analysis remains constrained by
challenges in adapting to the unique requirements of the traffic domain. In
this paper, we introduce a novel traffic representation model named Encrypted
Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which
integrates LLMs with knowledge of traffic structures through a self-supervised
instruction tuning paradigm. This framework establishes connections between
textual information and traffic interactions. ETooL demonstrates more robust
classification performance and superior generalization in both supervised and
zero-shot traffic classification tasks. Notably, it achieves significant
improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%),
APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.)
to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic
dataset designed to support dynamic distributional shifts, and use it to
validate ETooL's effectiveness under varying distributional conditions.
Furthermore, we evaluate the efficiency gains achieved through ETooL's
instruction tuning approach.

</details>


### [185] [Towards a DSL for hybrid secure computation](https://arxiv.org/abs/2505.20912)
*Romain de Laage*

Main category: cs.CR

TL;DR: The paper proposes a domain-specific language (DSL) for secure computation to execute computations using either Fully Homomorphic Encryption (FHE) or Trusted Execution Environments (TEE), depending on availability, addressing the challenges in hybrid environments.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the challenges of processing data in hybrid environments that use both Fully Homomorphic Encryption (FHE) and Trusted Execution Environments (TEE) for secure computation, as adapting and rewriting algorithms for these techniques can be complex.

Method: The authors propose a domain-specific language (DSL) for secure computation which enables the expression of computations to be performed. The system uses a backend that leverages either FHE or TEE based on what is available.

Result: This approach allows for computations to be carried out more flexibly in hybrid environments without needing to adapt or rewrite algorithms specifically for FHE or TEE.

Conclusion: A domain-specific language for secure computation offers a solution to the challenges of hybrid FHE-TEE environments by enabling flexible execution depending on available technology.

Abstract: Fully homomorphic encryption (FHE) and trusted execution environments (TEE)
are two approaches to provide confidentiality during data processing. Each
approach has its own strengths and weaknesses. In certain scenarios,
computations can be carried out in a hybrid environment, using both FHE and
TEE. However, processing data in such hybrid settings presents challenges, as
it requires to adapt and rewrite the algorithms for the chosen technique. We
propose a domain-specific language (DSL) for secure computation that allows to
express the computations to perform and execute them using a backend that
leverages either FHE or TEE, depending on what is available.

</details>


### [186] [IRCopilot: Automated Incident Response with Large Language Models](https://arxiv.org/abs/2505.20945)
*Xihuan Lin,Jie Zhang,Gelei Deng,Tianzhe Liu,Xiaolong Liu,Changcai Yang,Tianwei Zhang,Qing Guo,Riqing Chen*

Main category: cs.CR

TL;DR: IRCopilot is a novel framework leveraging LLMs for automated incident response, overcoming challenges such as context loss and hallucinations through collaborative session components and strategic responsibility segmentation. Experimental results show significant improvements in sub-task completion rates and robust performance in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The increasing intensity and complexity of global cyber threats have rendered traditional threat detection and incident response methods ineffective. Although LLMs have potential in early threat detection, their capabilities are limited in automated incident response after an intrusion.

Method: An incremental benchmark based on real-world incident response tasks was constructed to evaluate LLMs' performance. IRCopilot, a framework powered by LLMs, was proposed with four collaborative session components mimicking the three dynamic phases of incident response teams. The method includes diverse prompt designs and strategic responsibility segmentation to reduce issues like hallucinations and context loss.

Result: Experimental results indicate that IRCopilot surpasses baseline LLMs across key benchmarks with sub-task completion rates ranging from 114% to 150%. It also shows robust performance on public platforms and in real-world attack scenarios.

Conclusion: IRCopilot effectively addresses challenges faced by contemporary LLMs in incident response, enhancing practicality and efficiency through its design and significantly outperforming baselines.

Abstract: Incident response plays a pivotal role in mitigating the impact of cyber
attacks. In recent years, the intensity and complexity of global cyber threats
have grown significantly, making it increasingly challenging for traditional
threat detection and incident response methods to operate effectively in
complex network environments. While Large Language Models (LLMs) have shown
great potential in early threat detection, their capabilities remain limited
when it comes to automated incident response after an intrusion. To address
this gap, we construct an incremental benchmark based on real-world incident
response tasks to thoroughly evaluate the performance of LLMs in this domain.
Our analysis reveals several key challenges that hinder the practical
application of contemporary LLMs, including context loss, hallucinations,
privacy protection concerns, and their limited ability to provide accurate,
context-specific recommendations. In response to these challenges, we propose
IRCopilot, a novel framework for automated incident response powered by LLMs.
IRCopilot mimics the three dynamic phases of a real-world incident response
team using four collaborative LLM-based session components. These components
are designed with clear divisions of responsibility, reducing issues such as
hallucinations and context loss. Our method leverages diverse prompt designs
and strategic responsibility segmentation, significantly improving the system's
practicality and efficiency. Experimental results demonstrate that IRCopilot
outperforms baseline LLMs across key benchmarks, achieving sub-task completion
rates of 150%, 138%, 136%, 119%, and 114% for various response tasks. Moreover,
IRCopilot exhibits robust performance on public incident response platforms and
in real-world attack scenarios, showcasing its strong applicability.

</details>


### [187] [Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models](https://arxiv.org/abs/2505.20955)
*Puwei Lian,Yujun Cai,Songze Li*

Main category: cs.CR

TL;DR: Diffusion models, despite excelling in image generation, pose privacy and copyright issues. Membership Inference Attacks (MIAs) determine if specific data were used in training. A unified paradigm computes membership scores for identification. Current MIAs exploit image prediction ability but overlook diffusion models' high-frequency information processing deficiency. This leads to misclassification of member and hold-out data. Theoretically, this reduces attacks' membership advantage, hindering effective discrimination. A proposed high-frequency filter module mitigates these effects, improving baseline attacks' performance across datasets and models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the privacy concerns associated with diffusion models by enhancing Membership Inference Attacks (MIAs). Specifically, it aims to tackle the overlooked issue of how diffusion models inadequately process high-frequency information, which affects the accuracy of distinguishing between member and hold-out data.

Method: The method involves formalizing existing MIAs into a unified general paradigm that computes membership scores. It identifies and theoretically demonstrates the deficiency in diffusion models' handling of high-frequency information. To counteract this, a plug-and-play high-frequency filter module is proposed, which can be integrated into any attack within the paradigm without additional time costs.

Result: Extensive experiments show that the high-frequency filter module significantly improves the performance of baseline attacks across various datasets and models, effectively addressing the identified deficiency.

Conclusion: The conclusion highlights the importance of considering high-frequency information processing in diffusion models for more accurate MIAs. The proposed high-frequency filter module effectively mitigates the adverse effects of the deficiency, thereby enhancing the discrimination capability of MIAs.

Abstract: Diffusion models have achieved tremendous success in image generation, but
they also raise significant concerns regarding privacy and copyright issues.
Membership Inference Attacks (MIAs) are designed to ascertain whether specific
data were utilized during a model's training phase. As current MIAs for
diffusion models typically exploit the model's image prediction ability, we
formalize them into a unified general paradigm which computes the membership
score for membership identification. Under this paradigm, we empirically find
that existing attacks overlook the inherent deficiency in how diffusion models
process high-frequency information. Consequently, this deficiency leads to
member data with more high-frequency content being misclassified as hold-out
data, and hold-out data with less high-frequency content tend to be
misclassified as member data. Moreover, we theoretically demonstrate that this
deficiency reduces the membership advantage of attacks, thereby interfering
with the effective discrimination of member data and hold-out data. Based on
this insight, we propose a plug-and-play high-frequency filter module to
mitigate the adverse effects of the deficiency, which can be seamlessly
integrated into any attacks within this general paradigm without additional
time costs. Extensive experiments corroborate that this module significantly
improves the performance of baseline attacks across different datasets and
models.

</details>


### [188] [A Hitchhiker's Guide to Privacy-Preserving Cryptocurrencies: A Survey on Anonymity, Confidentiality, and Auditability](https://arxiv.org/abs/2505.21008)
*Matteo Nardelli,Francesco De Sclavis,Michela Iezzi*

Main category: cs.CR

TL;DR: This paper surveys privacy-preserving digital currencies including cryptocurrencies and CBDCs, proposes a taxonomy of privacy goals, links them to cryptographic primitives, and identifies open challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of privacy-preserving digital currencies and their evolution, while identifying open challenges in balancing privacy and auditability.

Method: Proposing a taxonomy of privacy goals, mapping them to cryptographic primitives, protocol mechanisms, and system architectures. Tracing the evolution of privacy-preserving currencies through three generations.

Result: A clear understanding of the privacy goals in digital currencies, their implementations, and the challenges that need to be addressed for future developments.

Conclusion: Further investigation into the design of digital currencies is needed to balance real-world privacy and auditability needs.

Abstract: Cryptocurrencies and central bank digital currencies (CBDCs) are reshaping
the monetary landscape, offering transparency and efficiency while raising
critical concerns about user privacy and regulatory compliance. This survey
provides a comprehensive and technically grounded overview of
privacy-preserving digital currencies, covering both cryptocurrencies and
CBDCs. We propose a taxonomy of privacy goals -- including anonymity,
confidentiality, unlinkability, and auditability -- and map them to underlying
cryptographic primitives, protocol mechanisms, and system architectures. Unlike
previous surveys, our work adopts a design-oriented perspective, linking
high-level privacy objectives to concrete implementations. We also trace the
evolution of privacy-preserving currencies through three generations,
highlighting shifts from basic anonymity guarantees toward more nuanced
privacy-accountability trade-offs. Finally, we identify open challenges at the
intersection of cryptography, distributed systems, and policy definition, which
motivate further investigation into the primitives and design of digital
currencies that balance real-world privacy and auditability needs.

</details>


### [189] [Uncovering Black-hat SEO based fake E-commerce scam groups from their redirectors and websites](https://arxiv.org/abs/2505.21021)
*Makoto Shimamura,Shingo Matsugaya,Keisuke Sakai,Kosuke Takeshige,Masaki Hashimoto*

Main category: cs.CR

TL;DR: This paper focuses on analyzing threat actor groups behind fake e-commerce scams using black-hat SEO techniques, utilizing a large dataset of 692,865 fake EC sites collected over two and a half years. Through link analysis and time series analysis, the study identifies 17 relatively large active groups.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the threat actors involved in fake e-commerce scams using black-hat SEO techniques, overcoming limitations of previous studies that couldn't identify all SEO malware samples from limited sources.

Method: Analyze links between fake e-commerce sites using Maltego and custom programs, conduct time series analysis to track changes in groups, using a high-quality dataset of 692,865 fake EC sites gathered over two and a half years.

Result: Identified 17 relatively large groups active during the dataset period, with some being active throughout the entire period.

Conclusion: The study successfully identified and analyzed multiple threat actor groups behind fake e-commerce scams, providing deeper insights into their operations.

Abstract: While law enforcements agencies and cybercrime researchers are working hard,
fake E-commerce scam is still a big threat to Internet users. One of the major
techniques to victimize users is luring them by black-hat
search-engine-optimization (SEO); making search engines display their lure
pages as if these were placed on compromised websites and then redirecting
visitors to malicious sites. In this study, we focus on the threat actors
conduct fake E-commerce scam with this strategy. Our previous study looked at
the connection between some malware families used for black-hat SEO to
enlighten threat actors and their infrastructures, however it shows only a
limited part of the whole picture because we could not find all SEO malware
samples from limited sources. In this paper, we aim to identify and analyze
threat actor groups using a large dataset of fake E-commerce sites collected by
Japan Cybercrime Control Center, which we believe is of higher quality. It
includes 692,865 fake EC sites gathered from redirectors over two and a half
years, from May 20, 2022 to Dec. 31, 2024. We analyzed the links between these
sites using Maltego, a well-known link analysis tool, and tailored programs. We
also conducted time series analysis to track group changes in the groups.
According to the analysis, we estimate that 17 relatively large groups were
active during the dataset period and some of them were active throughout the
period.

</details>


### [190] [SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with Heterogeneous LoRA](https://arxiv.org/abs/2505.21051)
*Jianmin Liu,Li Yan,Borui Li,Lei Yu,Chao Shen*

Main category: cs.CR

TL;DR: SHE-LoRA，一种结合选择性同态加密和低秩适应的方法，在跨设备环境中实现了高效且保护隐私的大型语言模型联合微调，显著降低了通信和加密计算开销，同时保持了与非隐私基线相当的性能和对最先进攻击的强大抵抗力。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护技术虽然可以防止梯度反转攻击恢复客户端的私有数据，但通常会导致性能下降和高成本，不适用于具有异构数据分布和设备能力的客户端。因此需要一种新的方法来在保护隐私的同时，减少开销并保持良好的性能。

Method: 提出了一种名为SHE-LoRA的方法，将选择性同态加密（HE）和低秩适应（LoRA）相结合，用于跨设备环境下的高效和隐私保护的联邦微调。具体而言，异构客户端根据参数敏感性评估自适应地选择部分模型参数进行同态加密，加密子集通过协商获得。为了确保准确的模型聚合，设计了一种列感知的安全聚合方法和定制的重参数化技术，以使聚合结果与客户端的异构设备能力相匹配。

Result: 广泛的实验表明，SHE-LoRA保持了与非隐私基线相当的性能，对最先进的攻击具有强大的抵抗力，并且相比基线减少了94.901%的通信开销和99.829%的加密计算开销。

Conclusion: SHE-LoRA为跨设备环境下的大型语言模型提供了高效且保护隐私的联合微调解决方案，能够在显著降低通信和加密计算开销的同时，保持良好的性能和强大的攻击抵抗力。

Abstract: Federated fine-tuning of large language models (LLMs) is critical for
improving their performance in handling domain-specific tasks. However, prior
work has shown that clients' private data can actually be recovered via
gradient inversion attacks. Existing privacy preservation techniques against
such attacks typically entail performance degradation and high costs, making
them ill-suited for clients with heterogeneous data distributions and device
capabilities. In this paper, we propose SHE-LoRA, which integrates selective
homomorphic encryption (HE) and low-rank adaptation (LoRA) to enable efficient
and privacy-preserving federated tuning of LLMs in cross-device environment.
Heterogeneous clients adaptively select partial model parameters for
homomorphic encryption based on parameter sensitivity assessment, with the
encryption subset obtained via negotiation. To ensure accurate model
aggregation, we design a column-aware secure aggregation method and customized
reparameterization techniques to align the aggregation results with the
heterogeneous device capabilities of clients. Extensive experiments demonstrate
that SHE-LoRA maintains performance comparable to non-private baselines,
achieves strong resistance to the state-of-the-art attacks, and significantly
reduces communication overhead by 94.901\% and encryption computation overhead
by 99.829\%, compared to baseline. Our code is accessible at
https://anonymous.4open.science/r/SHE-LoRA-8D84.

</details>


### [191] [ColorGo: Directed Concolic Execution](https://arxiv.org/abs/2505.21130)
*Jia Li,Jiacheng Shen,Yuxin Su,Michael R. Lyu*

Main category: cs.CR

TL;DR: The paper introduces ColorGo, a new directed whitebox fuzzer that integrates compilation-based concolic execution into directed fuzzing to balance efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current directed fuzzing methods face a trade-off between efficiency and effectiveness. Grey-box fuzzing is efficient but imprecise, while interpreter- or observer-based symbolic execution is precise but incurs significant runtime overhead.

Method: ColorGo involves compilation-based concolic execution into directed fuzzing. It uses incremental coloration including static reachability analysis and dynamic feasibility analysis to guide the exploration.

Result: ColorGo outperforms AFLGo by up to 100x in reaching target sites and reproducing target crashes when evaluated on diverse real-world programs.

Conclusion: ColorGo achieves high scalability while preserving the high precision from symbolic execution, effectively addressing the limitations of current directed fuzzing methods.

Abstract: Directed fuzzing is a critical technique in cybersecurity, targeting specific
sections of a program. This approach is essential in various security-related
domains such as crash reproduction, patch testing, and vulnerability detection.
Despite its importance, current directed fuzzing methods exhibit a trade-off
between efficiency and effectiveness. For instance, directed grey-box fuzzing,
while efficient in generating fuzzing inputs, lacks sufficient precision. The
low precision causes time wasted on executing code that cannot help reach the
target site. Conversely, interpreter- or observer-based directed symbolic
execution can produce high-quality inputs while incurring non-negligible
runtime overhead. These limitations undermine the feasibility of directed
fuzzers in real-world scenarios. To kill the birds of efficiency and
effectiveness with one stone, in this paper, we involve compilation-based
concolic execution into directed fuzzing and present ColorGo, achieving high
scalability while preserving the high precision from symbolic execution.
ColorGo is a new directed whitebox fuzzer that concretely executes the
instrumented program with constraint-solving capability on generated input. It
guides the exploration by \textit{incremental coloration}, including static
reachability analysis and dynamic feasibility analysis. We evaluated ColorGo on
diverse real-world programs and demonstrated that ColorGo outperforms AFLGo by
up to \textbf{100x} in reaching target sites and reproducing target crashes.

</details>


### [192] [JavaSith: A Client-Side Framework for Analyzing Potentially Malicious Extensions in Browsers, VS Code, and NPM Packages](https://arxiv.org/abs/2505.21263)
*Avihay Cohen*

Main category: cs.CR

TL;DR: JavaSith is a new framework that helps identify malicious code in browser extensions, VSCode plugins, and NPM packages by combining runtime sandboxing, static analysis, and local LLM risk assessment.


<details>
  <summary>Details</summary>
Motivation: Modern software supply chains are increasingly threatened by malicious code hidden in trusted components such as browser extensions, IDE extensions, and open-source packages.

Method: JavaSith uses a runtime sandbox to emulate extension APIs with a 'time machine' feature for accelerating time-based triggers. It also employs static analysis and a local large language model (LLM) to evaluate risks from both code and metadata.

Result: Through case studies on real-world attacks, JavaSith successfully detects stealthy malicious behaviors that evade traditional detection methods.

Conclusion: JavaSith's client-side approach empowers end-users and organizations to analyze the safety of extensions and packages before integrating them into their environments.

Abstract: Modern software supply chains face an increasing threat from malicious code
hidden in trusted components such as browser extensions, IDE extensions, and
open-source packages. This paper introduces JavaSith, a novel client-side
framework for analyzing potentially malicious extensions in web browsers,
Visual Studio Code (VSCode), and Node's NPM packages. JavaSith combines a
runtime sandbox that emulates browser/Node.js extension APIs (with a ``time
machine'' to accelerate time-based triggers) with static analysis and a local
large language model (LLM) to assess risk from code and metadata. We present
the design and architecture of JavaSith, including techniques for intercepting
extension behavior over simulated time and extracting suspicious patterns.
Through case studies on real-world attacks (such as a supply-chain compromise
of a Chrome extension and malicious VSCode extensions installing cryptominers),
we demonstrate how JavaSith can catch stealthy malicious behaviors that evade
traditional detection. We evaluate the framework's effectiveness and discuss
its limitations and future enhancements. JavaSith's client-side approach
empowers end-users/organizations to vet extensions and packages before
trustingly integrating them into their environments.

</details>


### [193] [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://arxiv.org/abs/2505.21277)
*Yao Huang,Yitong Sun,Shouwei Ruan,Yichi Zhang,Yinpeng Dong,Xingxing Wei*

Main category: cs.CR

TL;DR: The paper introduces a novel framework to enhance jailbreak capabilities of Large Language Models (LLMs) by expanding strategy spaces based on ELM theory and genetic-based optimization. Experiments show over 90% success rate on Claude-3.5, strong cross-model transferability, and surpassing safeguard models in evaluation accuracy.


<details>
  <summary>Details</summary>
Motivation: To better explore the potential of expanding the strategy space for black-box jailbreak attacks against safety-aligned LLMs, which is inherently bounded by predefined strategy spaces.

Method: Decomposing jailbreak strategies into essential components using Elaboration Likelihood Model (ELM) theory and developing genetic-based optimization with intention evaluation mechanisms.

Result: Achieved over 90% success rate on Claude-3.5 where prior methods completely fail, demonstrated strong cross-model transferability, and surpassed specialized safeguard models in evaluation accuracy.

Conclusion: The novel framework significantly enhances jailbreak capabilities and offers critical insights into model robustness.

Abstract: Large Language Models (LLMs), despite advanced general capabilities, still
suffer from numerous safety risks, especially jailbreak attacks that bypass
safety protocols. Understanding these vulnerabilities through black-box
jailbreak attacks, which better reflect real-world scenarios, offers critical
insights into model robustness. While existing methods have shown improvements
through various prompt engineering techniques, their success remains limited
against safety-aligned models, overlooking a more fundamental problem: the
effectiveness is inherently bounded by the predefined strategy spaces. However,
expanding this space presents significant challenges in both systematically
capturing essential attack patterns and efficiently navigating the increased
complexity. To better explore the potential of expanding the strategy space, we
address these challenges through a novel framework that decomposes jailbreak
strategies into essential components based on the Elaboration Likelihood Model
(ELM) theory and develops genetic-based optimization with intention evaluation
mechanisms. To be striking, our experiments reveal unprecedented jailbreak
capabilities by expanding the strategy space: we achieve over 90% success rate
on Claude-3.5 where prior methods completely fail, while demonstrating strong
cross-model transferability and surpassing specialized safeguard models in
evaluation accuracy. The code is open-sourced at:
https://github.com/Aries-iai/CL-GSO.

</details>


### [194] [Enhancing JavaScript Malware Detection through Weighted Behavioral DFAs](https://arxiv.org/abs/2505.21406)
*Pedro Pereira,José Gonçalves,João Vitorino,Eva Maia,Isabel Praça*

Main category: cs.CR

TL;DR: This paper presents a behavior-based system for JavaScript malware detection using a Deterministic Finite Automaton (DFA) with a weighted-behavior mechanism, showing adaptability in detecting emerging threats while maintaining transparency in decision making.


<details>
  <summary>Details</summary>
Motivation: To enhance client-side web application security by addressing the critical problem of detecting malicious JavaScript execution sequences as attack techniques become more sophisticated.

Method: Introduces a new system called behavior DFA that combines Deterministic Finite Automaton (DFA) with a weighted-behavior system to capture malicious patterns and classify sequences into benign, partially malicious, and fully malicious behaviors.

Result: Experimental evaluation on a real-world dataset shows the system's capability to effectively detect and classify threats, identifying exact matches and partial similarities to known malicious behaviors.

Conclusion: The behavior DFA demonstrates adaptability in detecting emerging threats while ensuring transparency in decision making.

Abstract: This work addresses JavaScript malware detection to enhance client-side web
application security with a behavior-based system. The ability to detect
malicious JavaScript execution sequences is a critical problem in modern web
security as attack techniques become more sophisticated. This study introduces
a new system for detecting JavaScript malware using a Deterministic Finite
Automaton (DFA) along with a weighted-behavior system, which we call behavior
DFA. This system captures malicious patterns and provides a dynamic mechanism
to classify new sequences that exhibit partial similarity to known attacks,
differentiating them between benign, partially malicious, and fully malicious
behaviors. Experimental evaluation on a dataset of 1,058 sequences captured in
a real-world environment demonstrates the capability of the system to detect
and classify threats effectively, with the behavior DFA successfully
identifying exact matches and partial similarities to known malicious
behaviors. The results highlight the adaptability of the system in detecting
emerging threats while maintaining transparency in decision making.

</details>


### [195] [Cryptography from Lossy Reductions: Towards OWFs from ETH, and Beyond](https://arxiv.org/abs/2505.21442)
*Pouria Fallahpour,Alex B. Grilo,Garazi Muguruza,Mahshid Riahinia*

Main category: cs.CR

TL;DR: This paper explores the relationship between one-way functions (OWFs) and lossy reductions, establishing conditions under which OWFs exist or do not exist. It also examines implications for quantum settings.


<details>
  <summary>Details</summary>
Motivation: To investigate the unconditional existence of one-way functions by relating them to lossy reductions.

Method: Analyzing lossy reductions and their runtimes for promise problems, extending results to various types of reductions and considering implications for quantum settings.

Result: Either OWFs exist or lossy reductions for promise problems have specific runtime constraints; worst-case to average-case Karp reductions and randomized encodings are special cases; results apply to weak fine-grained OWFs and provide conditions for their existence assuming ETH; impossibilities arise if OWFs do not exist.

Conclusion: The study establishes connections between OWFs and lossy reductions, offering insights into their existence and implications in both classical and quantum contexts.

Abstract: One-way functions (OWFs) form the foundation of modern cryptography, yet
their unconditional existence remains a major open question. In this work, we
study this question by exploring its relation to lossy reductions, i.e.,
reductions~$R$ for which it holds that $I(X;R(X)) \ll n$ for all
distributions~$X$ over inputs of size $n$. Our main result is that either OWFs
exist or any lossy reduction for any promise problem~$\Pi$ runs in
time~$2^{\Omega(\log\tau_\Pi / \log\log n)}$, where $\tau_\Pi(n)$ is the
infimum of the runtime of all (worst-case) solvers of~$\Pi$ on instances of
size~$n$. In fact, our result requires a milder condition, that $R$ is lossy
for sparse uniform distributions (which we call mild-lossiness). It also
extends to $f$-reductions as long as $f$ is a non-constant
permutation-invariant Boolean function, which includes And-, Or-, Maj-,
Parity-, Modulo $k$, and Threshold $k$-reductions.
  Additionally, we show that worst-case to average-case Karp reductions and
randomized encodings are special cases of mildly-lossy reductions and improve
the runtime above as $2^{\Omega(\log \tau_\Pi)}$ when these mappings are
considered. Restricting to weak fine-grained OWFs, this runtime can be further
improved as~$\Omega(\tau_\Pi)$. Taking~$\Pi$ as~$kSAT$, our results provide
sufficient conditions under which (fine-grained) OWFs exist assuming the
Exponential Time Hypothesis (ETH). Conversely, if (fine-grained) OWFs do not
exist, we obtain impossibilities on instance compressions (Harnik and Naor,
FOCS 2006) and instance randomizations of~$kSAT$ under the ETH.
  Finally, we partially extend these findings to the quantum setting; the
existence of a pure quantum mildly-lossy reduction for $\Pi$ within the
runtime~$2^{o(\log\tau_\Pi / \log\log n)}$ implies the existence of one-way
state generators.

</details>


### [196] [M3S-UPD: Efficient Multi-Stage Self-Supervised Learning for Fine-Grained Encrypted Traffic Classification with Unknown Pattern Discovery](https://arxiv.org/abs/2505.21462)
*Yali Yuan,Yu Huang,Xingjian Zeng,Hantao Mei,Guang Cheng*

Main category: cs.CR

TL;DR: The paper introduces M3S-UPD, a framework for classifying known applications and detecting unknown traffic patterns in encrypted network traffic using a multi-stage self-supervised approach.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate classification of known applications and reliable detection of unknown traffic patterns in encrypted network traffic. Current deep learning models face challenges such as data scarcity, concept drift, and operational constraints.

Method: M3S-UPD uses a four-phase iterative process including probabilistic embedding generation, clustering-based structure discovery, distribution-aligned outlier identification, and confidence-aware model updating. It integrates semi-supervised learning with representation analysis and includes a self-supervised unknown detection mechanism.

Result: M3S-UPD outperforms existing methods on few-shot encrypted traffic classification and achieves competitive performance on zero-shot unknown traffic discovery.

Conclusion: M3S-UPD presents an effective solution for both the classification of known applications and the detection of unknown traffic patterns in encrypted network traffic.

Abstract: The growing complexity of encrypted network traffic presents dual challenges
for modern network management: accurate multiclass classification of known
applications and reliable detection of unknown traffic patterns. Although deep
learning models show promise in controlled environments, their real-world
deployment is hindered by data scarcity, concept drift, and operational
constraints. This paper proposes M3S-UPD, a novel Multi-Stage Self-Supervised
Unknown-aware Packet Detection framework that synergistically integrates
semi-supervised learning with representation analysis. Our approach eliminates
artificial segregation between classification and detection tasks through a
four-phase iterative process: 1) probabilistic embedding generation, 2)
clustering-based structure discovery, 3) distribution-aligned outlier
identification, and 4) confidence-aware model updating. Key innovations include
a self-supervised unknown detection mechanism that requires neither synthetic
samples nor prior knowledge, and a continuous learning architecture that is
resistant to performance degradation. Experimental results show that M3S-UPD
not only outperforms existing methods on the few-shot encrypted traffic
classification task, but also simultaneously achieves competitive performance
on the zero-shot unknown traffic discovery task.

</details>


### [197] [AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery](https://arxiv.org/abs/2505.21499)
*Haowei Wang,Junjie Wang,Xiaojun Jia,Rupeng Zhang,Mingyang Li,Zhe Liu,Yang Liu,Qing Wang*

Main category: cs.CR

TL;DR: 提出了一种名为AdInject的新型黑盒攻击方法，通过互联网广告投放向Web代理环境注入恶意内容，实验表明其攻击成功率高，强调了Web代理安全防护的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的关于对抗性环境注入攻击的研究通常依赖于不切实际的假设，例如直接HTML操作、用户意图知识或访问代理模型参数，限制了其实用性。因此，需要一种更现实的威胁模型来评估Web代理的安全性。

Method: AdInject利用互联网广告投放机制，在无需了解用户意图或代理模型参数的情况下，将恶意内容注入Web代理环境中。它包括设计误导代理点击的恶意广告内容策略，以及基于VLM的广告内容优化技术，该技术通过从目标网站上下文中推断潜在用户意图，并将其整合到广告内容中，以提高攻击的有效性。

Result: 实验评估表明，AdInject在大多数场景下的攻击成功率超过60%，在某些情况下接近100%。这表明广告投放是针对Web代理进行环境注入攻击的有效途径。

Conclusion: 本研究揭示了Web代理在现实世界环境操纵渠道中的关键漏洞，强调了开发针对此类威胁的鲁棒防御机制的紧迫性。

Abstract: Vision-Language Model (VLM) based Web Agents represent a significant step
towards automating complex tasks by simulating human-like interaction with
websites. However, their deployment in uncontrolled web environments introduces
significant security vulnerabilities. Existing research on adversarial
environmental injection attacks often relies on unrealistic assumptions, such
as direct HTML manipulation, knowledge of user intent, or access to agent model
parameters, limiting their practical applicability. In this paper, we propose
AdInject, a novel and real-world black-box attack method that leverages the
internet advertising delivery to inject malicious content into the Web Agent's
environment. AdInject operates under a significantly more realistic threat
model than prior work, assuming a black-box agent, static malicious content
constraints, and no specific knowledge of user intent. AdInject includes
strategies for designing malicious ad content aimed at misleading agents into
clicking, and a VLM-based ad content optimization technique that infers
potential user intents from the target website's context and integrates these
intents into the ad content to make it appear more relevant or critical to the
agent's task, thus enhancing attack effectiveness. Experimental evaluations
demonstrate the effectiveness of AdInject, attack success rates exceeding 60%
in most scenarios and approaching 100% in certain cases. This strongly
demonstrates that prevalent advertising delivery constitutes a potent and
real-world vector for environment injection attacks against Web Agents. This
work highlights a critical vulnerability in Web Agent security arising from
real-world environment manipulation channels, underscoring the urgent need for
developing robust defense mechanisms against such threats. Our code is
available at https://github.com/NicerWang/AdInject.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [198] [FD-Bench: A Modular and Fair Benchmark for Data-driven Fluid Simulation](https://arxiv.org/abs/2505.20349)
*Haixin Wang,Ruoyan Li,Fred Xu,Fang Sun,Kaiqiao Han,Zijie Huang,Guancheng Wan,Ching Chang,Xiao Luo,Wei Wang,Yizhou Sun*

Main category: physics.flu-dyn

TL;DR: The paper presents FD-Bench, a fair, modular, comprehensive benchmark for data-driven fluid simulation which evaluates 85 baseline models across 10 flow scenarios and provides key contributions to resolve issues in reproducibility and comparability.


<details>
  <summary>Details</summary>
Motivation: There is a lack of unified PDE datasets and standardized evaluation protocols in the field of data-driven modeling of fluid dynamics with neural PDE solvers. This impedes fair assessment due to unclear disentanglement between spatial, temporal and loss modules.

Method: Introduced FD-Bench, which systematically evaluates 85 baseline models across 10 representative flow scenarios under a unified experimental setup. It features a modular design for fair comparisons, a systematic framework for comparison with traditional numerical solvers, fine-grained generalization analysis, and a user-friendly codebase.

Result: FD-Bench establishes the most comprehensive leaderboard to date, resolving long-standing issues in reproducibility and comparability in data-driven fluid models.

Conclusion: FD-Bench lays a foundation for robust evaluation of future data-driven fluid models and is open-sourced for community use.

Abstract: Data-driven modeling of fluid dynamics has advanced rapidly with neural PDE
solvers, yet a fair and strong benchmark remains fragmented due to the absence
of unified PDE datasets and standardized evaluation protocols. Although
architectural innovations are abundant, fair assessment is further impeded by
the lack of clear disentanglement between spatial, temporal and loss modules.
In this paper, we introduce FD-Bench, the first fair, modular, comprehensive
and reproducible benchmark for data-driven fluid simulation. FD-Bench
systematically evaluates 85 baseline models across 10 representative flow
scenarios under a unified experimental setup. It provides four key
contributions: (1) a modular design enabling fair comparisons across spatial,
temporal, and loss function modules; (2) the first systematic framework for
direct comparison with traditional numerical solvers; (3) fine-grained
generalization analysis across resolutions, initial conditions, and temporal
windows; and (4) a user-friendly, extensible codebase to support future
research. Through rigorous empirical studies, FD-Bench establishes the most
comprehensive leaderboard to date, resolving long-standing issues in
reproducibility and comparability, and laying a foundation for robust
evaluation of future data-driven fluid models. The code is open-sourced at
https://anonymous.4open.science/r/FD-Bench-15BC.

</details>


### [199] [Solving Euler equations with Multiple Discontinuities via Separation-Transfer Physics-Informed Neural Networks](https://arxiv.org/abs/2505.20361)
*Chuanxing Wang,Hui Luo,Kai Wang,Guohuai Zhu,Mingxing Luo*

Main category: physics.flu-dyn

TL;DR: 提出了一种新的PINNs变体ST-PINNs，用于解决具有多个不连续性的水动力学问题，通过顺序解析不连续性和转移学习来降低问题复杂度并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的PINNs在解决具有多个不连续性的水动力学问题时面临挑战。

Method: 提出了ST-PINNs，通过顺序解析从强到弱的不连续性，并在训练过程中利用转移学习。

Result: 数值实验表明，ST-PINNs能更准确地捕捉尖锐的不连续性，并显著减少涉及多个不连续性的水动力学问题的解误差。

Conclusion: 这是首次将PINNs方法应用于二维非稳态平面激波折射问题，为PINNs在复杂激波-界面相互作用中的应用提供了新见解。

Abstract: Despite the remarkable progress of physics-informed neural networks (PINNs)
in scientific computing, they continue to face challenges when solving
hydrodynamic problems with multiple discontinuities. In this work, we propose
Separation-Transfer Physics Informed Neural Networks (ST-PINNs) to address such
problems. By sequentially resolving discontinuities from strong to weak and
leveraging transfer learning during training, ST-PINNs significantly reduce the
problem complexity and enhance solution accuracy. To the best of our knowledge,
this is the first study to apply a PINNs-based approach to the two-dimensional
unsteady planar shock refraction problem, offering new insights into the
application of PINNs to complex shock-interface interactions. Numerical
experiments demonstrate that ST-PINNs more accurately capture sharp
discontinuities and substantially reduce solution errors in hydrodynamic
problems involving multiple discontinuities.

</details>


### [200] [A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data](https://arxiv.org/abs/2505.21421)
*Rami Cassia,Rich Kerswell*

Main category: physics.flu-dyn

TL;DR: The paper explores the use of GraphGPS for reconstructing 3D Riemann problems in compressible fluid flow from sparse data, modifying it for shock-aware reconstructions and efficient training.


<details>
  <summary>Details</summary>
Motivation: Reconstructing complex features like shocks and rarefactions in compressible fluid flows from limited data is a significant inverse problem with practical applications. Physics-informed machine learning offers promising solutions to this challenge.

Method: The GraphGPS framework is employed, which integrates positional encodings, local message-passing, and global contextual awareness. Modifications include shock-aware aggregation for sharper reconstructions and information flow exclusively from known nodes for efficiency and better convergence.

Result: GraphGPS outperforms various machine learning benchmarks in reconstruction tasks, while modifications lead to computational savings and maintain accuracy without degrading performance.

Conclusion: GraphGPS, with its physics-informed approach and targeted modifications, proves effective for reconstructing canonical compressible flows from sparse observations.

Abstract: In compressible fluid flow, reconstructing shocks, discontinuities,
rarefactions, and their interactions from sparse measurements is an important
inverse problem with practical applications. Moreover, physics-informed machine
learning has recently become an increasingly popular approach for performing
reconstructions tasks. In this work we explore a machine learning recipe, known
as GraphGPS, for reconstructing canonical compressible flows known as 3D
Riemann problems from sparse observations, in a physics-informed manner. The
GraphGPS framework combines the benefits of positional encodings, local
message-passing of graphs, and global contextual awareness, and we explore the
latter two components through an ablation study. Furthermore, we modify the
aggregation step of message-passing such that it is aware of shocks and
discontinuities, resulting in sharper reconstructions of these features.
Additionally, we modify message-passing such that information flows strictly
from known nodes only, which results in computational savings, better training
convergence, and no degradation of reconstruction accuracy. We also show that
the GraphGPS framework outperforms numerous machine learning benchmarks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [201] [Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low](https://arxiv.org/abs/2505.20377)
*Lennart Ullner,Alona Zharova,Felix Creutzig*

Main category: eess.SY

TL;DR: The paper explores the potential of Deep Reinforcement Learning (DRL) for optimizing EV charging patterns in prosumer households.


<details>
  <summary>Details</summary>
Motivation: Efficient energy management in households is crucial amidst the energy transition involving electric vehicles, renewable energies and battery storage. The study aims to understand how households can optimize prosumer EV charging.

Method: The study investigates real-world data from 90 households on fixed-rate electricity tariffs in German-speaking countries. It compares Deep Reinforcement Learning (DRL) with other control approaches like Rule-Based and Model Predictive Control for managing Home Energy Management (HEM).

Result: The DRL agent aligns EV and battery charging with PV surplus effectively. Frequent EV charging, early EV connections, and PV surplus enhance optimization potential. High battery capacity allows self-optimization while low battery capacity benefits significantly from DRL for improved energy management and cost savings.

Conclusion: Prosumer households with optimization potential could benefit from DRL, positively impacting the entire electricity system and its decarbonization.

Abstract: Efficient energy management in prosumer households is key to alleviating grid
stress in an energy transition marked by electric vehicles (EV), renewable
energies and battery storage. However, it is unclear how households optimize
prosumer EV charging. Here we study real-world data from 90 households on
fixed-rate electricity tariffs in German-speaking countries to investigate the
potential of Deep Reinforcement Learning (DRL) and other control approaches
(Rule-Based, Model Predictive Control) to manage the dynamic and uncertain
environment of Home Energy Management (HEM) and optimize household charging
patterns. The DRL agent efficiently aligns charging of EV and battery storage
with photovoltaic (PV) surplus. We find that frequent EV charging transactions,
early EV connections and PV surplus increase optimization potential. A detailed
analysis of nine households (1 hour resolution, 1 year) demonstrates that high
battery capacity facilitates self optimization; in this case further
algorithmic control shows little value. In cases with relatively low battery
capacity, algorithmic control with DRL improves energy management and cost
savings by a relevant margin. This result is further corroborated by our
simulation of a synthetic household. We conclude that prosumer households with
optimization potential would profit from DRL, thus benefiting also the full
electricity system and its decarbonization.

</details>


### [202] [Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics](https://arxiv.org/abs/2505.20370)
*Martine Dyring Hansen,Elena Celledoni,Benjamin Kwanen Tapley*

Main category: eess.SY

TL;DR: The paper presents a data-driven method to learn the equations of motion from position measurements alone, using neural networks and the discrete Lagrange-d'Alembert principle. It shows effectiveness on synthetic and real-world datasets, including human motion and image sequences.


<details>
  <summary>Details</summary>
Motivation: To develop a method for learning equations of motion directly from position measurements without velocity data, which is useful in system identification tasks where only positional information is available.

Method: A data-driven approach using the discrete Lagrange-d'Alembert principle and forced discrete Euler-Lagrange equations to construct a model of dynamics. Dynamics are decomposed into conservative and non-conservative components learned separately via feed-forward neural networks.

Result: Validated on synthetic and real-world datasets, including human motion data and latent embeddings from image sequences. The model can faithfully reconstruct and separate conservative and forced dynamics, providing interpretable and physically consistent predictions.

Conclusion: The introduced method successfully learns equations of motion from position data alone, preserving physical properties and offering interpretable results.

Abstract: We introduce a data-driven method for learning the equations of motion of
mechanical systems directly from position measurements, without requiring
access to velocity data. This is particularly relevant in system identification
tasks where only positional information is available, such as motion capture,
pixel data or low-resolution tracking. Our approach takes advantage of the
discrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange
equations to construct a physically grounded model of the system's dynamics. We
decompose the dynamics into conservative and non-conservative components, which
are learned separately using feed-forward neural networks. In the absence of
external forces, our method reduces to a variational discretization of the
action principle naturally preserving the symplectic structure of the
underlying Hamiltonian system. We validate our approach on a variety of
synthetic and real-world datasets, demonstrating its effectiveness compared to
baseline methods. In particular, we apply our model to (1) measured human
motion data and (2) latent embeddings obtained via an autoencoder trained on
image sequences. We demonstrate that we can faithfully reconstruct and separate
both the conservative and forced dynamics, yielding interpretable and
physically consistent predictions.

</details>


### [203] [Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning](https://arxiv.org/abs/2505.21026)
*Runze Lin,Junghui Chen,Biao Huang,Lei Xie,Hongye Su*

Main category: eess.SY

TL;DR: In the context of Industry 4.0 and smart manufacturing, this paper proposes a new framework combining inverse reinforcement learning (IRL) with multi-task learning to overcome limitations in traditional reinforcement learning for process control.


<details>
  <summary>Details</summary>
Motivation: Process systems engineering needs to adapt to digital transformation in the era of Industry 4.0 and smart manufacturing. Reinforcement learning has potential but is limited by reliance on accurate digital twins and well-designed reward functions.

Method: The paper introduces a framework that integrates inverse reinforcement learning (IRL) with multi-task learning. It uses historical closed-loop data as expert demonstrations to extract optimal reward functions and control policies. A latent-context variable is included to differentiate modes, allowing mode-specific controllers to be trained.

Result: Case studies on a continuous stirred tank reactor and a fed-batch bioreactor demonstrate the effectiveness of the framework in managing multi-mode data and training adaptable controllers.

Conclusion: The novel framework successfully addresses the limitations of reinforcement learning in process control through integration with IRL and multi-task learning.

Abstract: In the era of Industry 4.0 and smart manufacturing, process systems
engineering must adapt to digital transformation. While reinforcement learning
offers a model-free approach to process control, its applications are limited
by the dependence on accurate digital twins and well-designed reward functions.
To address these limitations, this paper introduces a novel framework that
integrates inverse reinforcement learning (IRL) with multi-task learning for
data-driven, multi-mode control design. Using historical closed-loop data as
expert demonstrations, IRL extracts optimal reward functions and control
policies. A latent-context variable is incorporated to distinguish modes,
enabling the training of mode-specific controllers. Case studies on a
continuous stirred tank reactor and a fed-batch bioreactor validate the
effectiveness of this framework in handling multi-mode data and training
adaptable controllers.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [204] [Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data](https://arxiv.org/abs/2505.20731)
*Linshanshan Wang,Mengyan Li,Zongqi Xia,Molei Liu,Tianxi Cai*

Main category: stat.ME

TL;DR: SCORE is a semi-supervised representation learning framework for EHR data that uses PALM model with pre-trained code embeddings and hybrid EM-GVA algorithm to create patient embeddings, showing better performance in predicting disability status for MS patients.


<details>
  <summary>Details</summary>
Motivation: EHR data is rich but challenging due to sparsity, heterogeneity, high dimensionality and lack of standardized ground truth. There is a need for an effective method to capture multi-domain disease profiles through patient embeddings.

Method: The SCORE framework employs a Poisson-Adapted Latent factor Mixture (PALM) Model with pre-trained code embeddings to characterize codified features and extract meaningful patient phenotypes and embeddings. It uses a hybrid Expectation-Maximization (EM) and Gaussian Variational Approximation (GVA) algorithm to handle large-scale data.

Result: Theoretical analysis shows the convergence of the hybrid approach, quantifies GVA errors, and derives error rates under diverging embedding dimensions. Incorporating unlabeled data enhances accuracy and reduces sensitivity to label scarcity. Simulations confirm superior finite-sample performance. Application on MS EHR data demonstrates more informative and predictive patient embeddings compared to existing approaches.

Conclusion: SCORE provides a robust semi-supervised representation learning framework for EHR data, which can produce more informative and predictive patient embeddings for various conditions.

Abstract: Electronic Health Records (EHR) offer rich real-world data for personalized
medicine, providing insights into disease progression, treatment responses, and
patient outcomes. However, their sparsity, heterogeneity, and high
dimensionality make them difficult to model, while the lack of standardized
ground truth further complicates predictive modeling. To address these
challenges, we propose SCORE, a semi-supervised representation learning
framework that captures multi-domain disease profiles through patient
embeddings. SCORE employs a Poisson-Adapted Latent factor Mixture (PALM) Model
with pre-trained code embeddings to characterize codified features and extract
meaningful patient phenotypes and embeddings. To handle the computational
challenges of large-scale data, it introduces a hybrid Expectation-Maximization
(EM) and Gaussian Variational Approximation (GVA) algorithm, leveraging limited
labeled data to refine estimates on a vast pool of unlabeled samples. We
theoretically establish the convergence of this hybrid approach, quantify GVA
errors, and derive SCORE's error rate under diverging embedding dimensions. Our
analysis shows that incorporating unlabeled data enhances accuracy and reduces
sensitivity to label scarcity. Extensive simulations confirm SCORE's superior
finite-sample performance over existing methods. Finally, we apply SCORE to
predict disability status for patients with multiple sclerosis (MS) using
partially labeled EHR data, demonstrating that it produces more informative and
predictive patient embeddings for multiple MS-related conditions compared to
existing approaches.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [205] [Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset](https://arxiv.org/abs/2505.20341)
*Rui Liu,Pu Gao,Jiatian Xi,Berrak Sisman,Carlos Busso,Haizhou Li*

Main category: eess.AS

TL;DR: This paper proposes EmoCorrector, a post-correction scheme for Text-based Speech Editing (TSE) that enhances emotional consistency in synthetic speech by using Retrieval-Augmented Generation (RAG). A new dataset, ECD-TSE, is introduced to support the training and evaluation of emotional consistency modeling in TSE.


<details>
  <summary>Details</summary>
Motivation: Existing TSE methods focus on content accuracy and acoustic consistency but overlook emotional shifts or inconsistency issues introduced by text changes.

Method: EmoCorrector leverages RAG by extracting emotional features from edited text, retrieving speech samples with matching emotions, and synthesizing speech that aligns with the desired emotion while preserving speaker's identity and quality.

Result: Subjective and objective experiments on the ECD-TSE dataset confirm that EmoCorrector significantly enhances the expression of intended emotion while addressing emotion inconsistency limitations in current TSE methods.

Conclusion: EmoCorrector improves emotional consistency in TSE, and the ECD-TSE dataset provides a benchmark for evaluating emotional consistency modeling in TSE.

Abstract: Text-based speech editing (TSE) modifies speech using only text, eliminating
re-recording. However, existing TSE methods, mainly focus on the content
accuracy and acoustic consistency of synthetic speech segments, and often
overlook the emotional shifts or inconsistency issues introduced by text
changes. To address this issue, we propose EmoCorrector, a novel
post-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented
Generation (RAG) by extracting the edited text's emotional features, retrieving
speech samples with matching emotions, and synthesizing speech that aligns with
the desired emotion while preserving the speaker's identity and quality. To
support the training and evaluation of emotional consistency modeling in TSE,
we pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The
prominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data
featuring diverse text variations and a range of emotional expressions.
Subjective and objective experiments and comprehensive analysis on ECD-TSE
confirm that EmoCorrector significantly enhances the expression of intended
emotion while addressing emotion inconsistency limitations in current TSE
methods. Code and audio examples are available at
https://github.com/AI-S2-Lab/EmoCorrector.

</details>


### [206] [Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction](https://arxiv.org/abs/2505.20635)
*Zexu Pan,Shengkui Zhao,Tingting Wang,Kun Zhou,Yukun Ma,Chong Zhang,Bin Ma*

Main category: eess.AS

TL;DR: The paper introduces a plug-and-play inter-speaker attention module for audio-visual speaker extraction that processes multiple faces, improving accuracy in complex environments. It outperforms baselines and shows robustness across datasets.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of speaker extraction in complex multi-person environments where multiple faces are present on-screen by utilizing valuable speaker activity cues from co-occurring faces.

Method: A plug-and-play inter-speaker attention module is introduced and integrated into two models: AV-DPRNN and AV-TFGridNet to process flexible numbers of co-occurring faces for more accurate speaker extraction.

Result: Extensive experiments on diverse datasets (VoxCeleb2, MISP) show consistent outperformance of baselines. Cross-dataset evaluations on LRS2 and LRS3 confirm robustness and generalizability.

Conclusion: The inter-speaker attention module enhances audio-visual speaker extraction performance and is effective in complex multi-person scenarios.

Abstract: Audio-visual speaker extraction isolates a target speaker's speech from a
mixture speech signal conditioned on a visual cue, typically using the target
speaker's face recording. However, in real-world scenarios, other co-occurring
faces are often present on-screen, providing valuable speaker activity cues in
the scene. In this work, we introduce a plug-and-play inter-speaker attention
module to process these flexible numbers of co-occurring faces, allowing for
more accurate speaker extraction in complex multi-person environments. We
integrate our module into two prominent models: the AV-DPRNN and the
state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,
including the highly overlapped VoxCeleb2 and sparsely overlapped MISP,
demonstrate that our approach consistently outperforms baselines. Furthermore,
cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and
generalizability of our method.

</details>


### [207] [PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems](https://arxiv.org/abs/2505.21230)
*Nima Sedghiyeh,Sara Sadeghi,Reza Khodadadi,Farzin Kashani,Omid Aghdaei,Somayeh Rahimi,Mohammad Sadegh Safari*

Main category: eess.AS

TL;DR: This paper introduces Persian Speech Recognition Benchmark (PSRB), which evaluates ten ASR systems on Persian language, identifies key error types, proposes a novel metric for evaluation, and highlights the necessity of fine-tuning models to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive evaluation benchmarks for ASR systems in low-resource languages like Persian.

Method: Introduced PSRB, evaluated ten ASR systems including commercial and open-source models, conducted an in-depth analysis of Persian ASR transcriptions, identified key error types, and proposed a novel metric that weights substitution errors.

Result: ASR models generally perform well on standard Persian but struggle with regional accents, children's speech, and specific linguistic challenges. The proposed metric improves the precision of performance assessment.

Conclusion: PSRB is a valuable resource for advancing ASR research in Persian and serves as a framework for developing benchmarks in other low-resource languages.

Abstract: Although Automatic Speech Recognition (ASR) systems have become an integral
part of modern technology, their evaluation remains challenging, particularly
for low-resource languages such as Persian. This paper introduces Persian
Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to
address this gap by incorporating diverse linguistic and acoustic conditions.
We evaluate ten ASR systems, including state-of-the-art commercial and
open-source models, to examine performance variations and inherent biases.
Additionally, we conduct an in-depth analysis of Persian ASR transcriptions,
identifying key error types and proposing a novel metric that weights
substitution errors. This metric enhances evaluation robustness by reducing the
impact of minor and partial errors, thereby improving the precision of
performance assessment. Our findings indicate that while ASR models generally
perform well on standard Persian, they struggle with regional accents,
children's speech, and specific linguistic challenges. These results highlight
the necessity of fine-tuning and incorporating diverse, representative training
datasets to mitigate biases and enhance overall ASR performance. PSRB provides
a valuable resource for advancing ASR research in Persian and serves as a
framework for developing benchmarks in other low-resource languages. A subset
of the PSRB dataset is publicly available at
https://huggingface.co/datasets/PartAI/PSRB.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [208] [Differentially private ratio statistics](https://arxiv.org/abs/2505.20351)
*Tomer Shoham,Katrina Ligettt*

Main category: stat.ML

TL;DR: This paper focuses on differentially private ratio statistics, providing a practical solution for ratio estimation in private machine learning pipelines.


<details>
  <summary>Details</summary>
Motivation: Existing literature has largely neglected differentially private ratio statistics, which are crucial in hypothesis testing, model evaluation, and decision-making in machine learning areas such as causal inference and fairness analysis.

Method: The authors analyze a simple algorithm that offers strong properties regarding privacy, sample accuracy, and bias even at small sample sizes. They also develop a differentially private estimator for relative risk, proving its consistency and devising a method to construct valid confidence intervals.

Result: The proposed algorithm provides excellent properties concerning privacy, sample accuracy, and bias. The differentially private estimator for relative risk is consistent, and the method for constructing confidence intervals is valid.

Conclusion: This work bridges a gap in the differential privacy literature by offering practical solutions for ratio estimation in private machine learning pipelines.

Abstract: Ratio statistics--such as relative risk and odds ratios--play a central role
in hypothesis testing, model evaluation, and decision-making across many areas
of machine learning, including causal inference and fairness analysis. However,
despite privacy concerns surrounding many datasets and despite increasing
adoption of differential privacy, differentially private ratio statistics have
largely been neglected by the literature and have only recently received an
initial treatment by Lin et al. [1]. This paper attempts to fill this lacuna,
giving results that can guide practice in evaluating ratios when the results
must be protected by differential privacy. In particular, we show that even a
simple algorithm can provide excellent properties concerning privacy, sample
accuracy, and bias, not just asymptotically but also at quite small sample
sizes. Additionally, we analyze a differentially private estimator for relative
risk, prove its consistency, and develop a method for constructing valid
confidence intervals. Our approach bridges a gap in the differential privacy
literature and provides a practical solution for ratio estimation in private
machine learning pipelines.

</details>


### [209] [Kernel Quantile Embeddings and Associated Probability Metrics](https://arxiv.org/abs/2505.20433)
*Masha Naslidnyk,Siu Lun Chau,François-Xavier Briol,Krikamol Muandet*

Main category: stat.ML

TL;DR: The paper introduces kernel quantile embeddings (KQEs) as a new way to represent probability distributions in RKHS, constructing distances that are efficient and competitive alternatives to MMD.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to explore if there are other meaningful representations of probability distributions in RKHS beyond the mean function used in maximum mean discrepancy (MMD).

Method: Inspired by generalised quantiles, the authors introduce the concept of kernel quantile embeddings (KQEs). Using KQEs, they construct a family of distances which have specific properties under certain kernel conditions.

Result: The constructed distances are probability metrics under weaker kernel conditions than MMD, recover a kernelised form of the sliced Wasserstein distance, and can be efficiently estimated with near-linear cost. Hypothesis testing shows that these distances offer a competitive alternative to MMD and its fast approximations.

Conclusion: Kernel quantile embeddings provide a novel and effective method for representing probability distributions in RKHS, leading to distances that outperform or match existing methods like MMD.

Abstract: Embedding probability distributions into reproducing kernel Hilbert spaces
(RKHS) has enabled powerful nonparametric methods such as the maximum mean
discrepancy (MMD), a statistical distance with strong theoretical and
computational properties. At its core, the MMD relies on kernel mean embeddings
to represent distributions as mean functions in RKHS. However, it remains
unclear if the mean function is the only meaningful RKHS representation.
Inspired by generalised quantiles, we introduce the notion of kernel quantile
embeddings (KQEs). We then use KQEs to construct a family of distances that:
(i) are probability metrics under weaker kernel conditions than MMD; (ii)
recover a kernelised form of the sliced Wasserstein distance; and (iii) can be
efficiently estimated with near-linear cost. Through hypothesis testing, we
show that these distances offer a competitive alternative to MMD and its fast
approximations.

</details>


### [210] [Learning with Expected Signatures: Theory and Applications](https://arxiv.org/abs/2505.20465)
*Lorenzo Lucchese,Mikko S. Pakkanen,Almut E. D. Veraart*

Main category: stat.ML

TL;DR: 本论文探讨了期望签名在数据流表示中的应用，并提出了一种改进的估计方法以降低误差，从而提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 期望签名提供了一种无模型的数据嵌入方式，能够全面表征数据生成分布，适用于时间序列和顺序数据的领域无关机器学习算法。然而，其离散时间估计器与连续时间理论值之间的差异尚未得到充分解释。

Method: 作者分析了期望签名的收敛性，弥合了其离散时间估计器与连续时间理论值之间的差距。此外，针对由鞅过程生成的数据，提出了一种修改后的期望签名估计器。

Result: 证明了离散时间估计器与连续时间理论值之间的收敛性，并展示了修改后的估计器在降低均方误差和提升预测性能方面的有效性。

Conclusion: 期望签名的理论基础得到了进一步完善，且提出的改进估计器在特定数据生成过程中表现出色，为基于期望签名的机器学习方法提供了更完整的概率解释。

Abstract: The expected signature maps a collection of data streams to a lower
dimensional representation, with a remarkable property: the resulting feature
tensor can fully characterize the data generating distribution. This
"model-free" embedding has been successfully leveraged to build multiple
domain-agnostic machine learning (ML) algorithms for time series and sequential
data. The convergence results proved in this paper bridge the gap between the
expected signature's empirical discrete-time estimator and its theoretical
continuous-time value, allowing for a more complete probabilistic
interpretation of expected signature-based ML methods. Moreover, when the data
generating process is a martingale, we suggest a simple modification of the
expected signature estimator with significantly lower mean squared error and
empirically demonstrate how it can be effectively applied to improve predictive
performance.

</details>


### [211] [Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models](https://arxiv.org/abs/2505.20536)
*Guanhao Zhou,Yuefeng Han,Xiufan Yu*

Main category: stat.ML

TL;DR: This paper proposes Covariate-Adjusted Deep Causal Learning (CoDEAL), a new method for estimating heterogeneous treatment effects in causal panel data models with covariate effects. It integrates nonlinear covariate effect components and factor structures, using neural networks and autoencoders, to capture heterogeneity and nonlinearity. Theoretical guarantees on counterfactual convergence are provided, along with simulation studies and real data application.


<details>
  <summary>Details</summary>
Motivation: There is a need for better methods to estimate heterogeneous treatment effects in causal panel data models, especially when covariate effects are present. Existing methods may not sufficiently handle the underlying heterogeneity and nonlinearity of both panel units and covariate effects.

Method: The proposed method, CoDEAL, combines nonlinear covariate effect components, parameterized by a feed-forward neural network, with nonlinear factor structures modeled by a multi-output autoencoder. This combination forms a heterogeneous causal panel model that captures complex influences of covariates and cross-sectional/temporal dependencies in the data. Missing counterfactual outcomes are imputed through a customized matrix completion algorithm incorporating latent structural information.

Result: The authors establish theoretical guarantees regarding the convergence of the estimated counterfactuals. Additionally, they demonstrate through extensive simulation studies and a real data application that CoDEAL has compelling performance in accurately estimating treatment effects.

Conclusion: CoDEAL provides an effective approach to dealing with the complexities of causal panel data models, particularly in capturing heterogeneity and nonlinearity. Its strong performance is supported by both theoretical results and empirical evidence.

Abstract: This paper studies the task of estimating heterogeneous treatment effects in
causal panel data models, in the presence of covariate effects. We propose a
novel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models,
that employs flexible model structures and powerful neural network
architectures to cohesively deal with the underlying heterogeneity and
nonlinearity of both panel units and covariate effects. The proposed CoDEAL
integrates nonlinear covariate effect components (parameterized by a
feed-forward neural network) with nonlinear factor structures (modeled by a
multi-output autoencoder) to form a heterogeneous causal panel model. The
nonlinear covariate component offers a flexible framework for capturing the
complex influences of covariates on outcomes. The nonlinear factor analysis
enables CoDEAL to effectively capture both cross-sectional and temporal
dependencies inherent in the data panel. This latent structural information is
subsequently integrated into a customized matrix completion algorithm, thereby
facilitating more accurate imputation of missing counterfactual outcomes.
Moreover, the use of a multi-output autoencoder explicitly accounts for
heterogeneity across units and enhances the model interpretability of the
latent factors. We establish theoretical guarantees on the convergence of the
estimated counterfactuals, and demonstrate the compelling performance of the
proposed method using extensive simulation studies and a real data application.

</details>


### [212] [Balancing Performance and Costs in Best Arm Identification](https://arxiv.org/abs/2505.20583)
*Michael O. Harding,Kirthevasan Kandasamy*

Main category: stat.ML

TL;DR: The paper proposes a new formalism called DBCARE for the best arm identification problem in multi-armed bandit models, which minimizes a risk functional balancing arm performance and learning cost. Theoretical lower bounds are derived for two performance penalties, and simulations demonstrate DBCARE's superior performance compared to fixed budget and confidence algorithms.


<details>
  <summary>Details</summary>
Motivation: Despite extensive literature on fixed budget and fixed confidence regimes for best arm identification in multi-armed bandits, there is still ambiguity for practitioners regarding approach selection and parameter tuning. This motivates the need for a unified framework that balances arm performance and learning cost effectively.

Method: A new formalism is proposed where a risk functional is minimized. This includes an observation cost during sampling and a performance penalty for suboptimal arm recommendation. The learner aims to minimize the sum of these costs. Two choices for performance penalty are considered: probability of misidentification and simple regret. An algorithm named DBCARE is introduced to achieve near-optimal performance according to theoretical lower bounds.

Result: Theoretical lower bounds for the risk under each performance penalty choice are derived. The DBCARE algorithm matches these lower bounds up to polylog factors across nearly all problem instances. Simulations show DBCARE outperforms existing fixed budget and confidence algorithms in various scenarios.

Conclusion: DBCARE provides a practical solution for best arm identification by addressing the limitations of traditional fixed budget and confidence approaches. It achieves better balance between arm performance and learning cost, making it suitable for applications like A/B testing.

Abstract: We consider the problem of identifying the best arm in a multi-armed bandit
model. Despite a wealth of literature in the traditional fixed budget and fixed
confidence regimes of the best arm identification problem, it still remains a
mystery to most practitioners as to how to choose an approach and corresponding
budget or confidence parameter. We propose a new formalism to avoid this
dilemma altogether by minimizing a risk functional which explicitly balances
the performance of the recommended arm and the cost incurred by learning this
arm. In this framework, a cost is incurred for each observation during the
sampling phase, and upon recommending an arm, a performance penalty is incurred
for identifying a suboptimal arm. The learner's goal is to minimize the sum of
the penalty and cost. This new regime mirrors the priorities of many
practitioners, e.g. maximizing profit in an A/B testing framework, better than
classical fixed budget or confidence settings. We derive theoretical lower
bounds for the risk of each of two choices for the performance penalty, the
probability of misidentification and the simple regret, and propose an
algorithm called DBCARE to match these lower bounds up to polylog factors on
nearly all problem instances. We then demonstrate the performance of DBCARE on
a number of simulated models, comparing to fixed budget and confidence
algorithms to show the shortfalls of existing BAI paradigms on this problem.

</details>


### [213] [Moment Expansions of the Energy Distance](https://arxiv.org/abs/2505.20647)
*Ian Langmore*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The energy distance is used to test distributional equality, and as a loss
function in machine learning. While $D^2(X, Y)=0$ only when $X\sim Y$, the
sensitivity to different moments is of practical importance. This work
considers $D^2(X, Y)$ in the case where the distributions are close. In this
regime, $D^2(X, Y)$ is more sensitive to differences in the means
$\bar{X}-\bar{Y}$, than differences in the covariances $\Delta$. This is due to
the structure of the energy distance and is independent of dimension. The
sensitivity to on versus off diagonal components of $\Delta$ is examined when
$X$ and $Y$ are close to isotropic. Here a dimension dependent averaging occurs
and, in many cases, off diagonal correlations contribute significantly less.
Numerical results verify these relationships hold even when distributional
assumptions are not strictly met.

</details>


### [214] [A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data](https://arxiv.org/abs/2505.20688)
*Taehyo Kim,Qiran Jia,Mony J. de Leon,Hai Shu*

Main category: stat.ML

TL;DR: 在神经影像数据分析中，控制错误发现率（FDR）的方法至关重要。本文提出了一种新的方法fcHMRF-LIS，它通过整合显著性局部指数（LIS）测试程序和全连接隐马尔可夫随机场（fcHMRF），解决了传统FDR控制方法中存在的问题，如未能捕捉复杂的空间依赖关系、错误发现比例（FDP）和错误非发现比例（FNP）的变化性大以及计算效率低等。此方法不仅实现了精确的FDR控制和较低的FNR，还减少了FDP和FNP的变化性，并提高了检测到的真实阳性数。在模拟实验和实际应用中，fcHMRF-LIS都表现出显著的优势。


<details>
  <summary>Details</summary>
Motivation: 经典的FDR控制方法假设测试之间相互独立，在处理神经影像数据时往往导致较高的错误非发现率（FNR）。现有的空间FDR控制方法虽然提高了能力，但在解决神经影像应用中的三个主要挑战方面仍有不足：捕捉复杂的空间依赖关系、保持错误发现比例（FDP）和错误非发现比例（FNP）的低变化性、实现高分辨率数据的计算可扩展性。

Method: 提出了一种新的方法fcHMRF-LIS，该方法将显著性局部指数（LIS）测试程序与一种新型的全连接隐马尔可夫随机场（fcHMRF）相结合，用于建模复杂的空間结构。同时，开发了一种高效的期望最大化算法，该算法结合了平均场近似、条件随机场作为递归神经网络（CRF-RNN）技术和排列六边形格子滤波器，从而将计算复杂度从二次降低到线性。

Result: 广泛的模拟实验表明，fcHMRF-LIS实现了准确的FDR控制、较低的FNR、减少的FDP和FNP变化性，以及比现有方法更高的真实阳性数。在应用于来自阿尔茨海默病神经影像计划的FDG-PET数据集时，fcHMRF-LIS识别出具有神经生物学意义的大脑区域，并在计算效率上提供了显著优势。

Conclusion: fcHMRF-LIS是一种强大、稳定且可扩展的空间FDR控制方法，适用于体素级别的多重检验。它通过有效捕捉复杂的空间依赖关系、减少FDP和FNP的变化性并提高计算效率，为神经影像数据分析提供了一个有力工具。

Abstract: False discovery rate (FDR) control methods are essential for voxel-wise
multiple testing in neuroimaging data analysis, where hundreds of thousands or
even millions of tests are conducted to detect brain regions associated with
disease-related changes. Classical FDR control methods (e.g., BH, q-value, and
LocalFDR) assume independence among tests and often lead to high false
non-discovery rates (FNR). Although various spatial FDR control methods have
been developed to improve power, they still fall short in jointly addressing
three major challenges in neuroimaging applications: capturing complex spatial
dependencies, maintaining low variability in both false discovery proportion
(FDP) and false non-discovery proportion (FNP) across replications, and
achieving computational scalability for high-resolution data. To address these
challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR
control method for voxel-wise multiple testing. It integrates the local index
of significance (LIS)-based testing procedure with a novel fully connected
hidden Markov random field (fcHMRF) designed to model complex spatial
structures using a parsimonious parameterization. We develop an efficient
expectation-maximization algorithm incorporating mean-field approximation, the
Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and
permutohedral lattice filtering, reducing the computational complexity from
quadratic to linear in the number of tests. Extensive simulations demonstrate
that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability
in FDP and FNP, and a higher number of true positives compared to existing
methods. Applied to an FDG-PET dataset from the Alzheimer's Disease
Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain
regions and offers notable advantages in computational efficiency.

</details>


### [215] [Stationary MMD Points for Cubature](https://arxiv.org/abs/2505.20754)
*Zonghao Chen,Toni Karvonen,Heishiro Kanagawa,François-Xavier Briol,Chris. J. Oates*

Main category: stat.ML

TL;DR: Analyze the abstract about approximating target probability distribution using finite points.


<details>
  <summary>Details</summary>
Motivation: Approximation of a target probability distribution using a finite set of points is crucial in cubature, data compression, and optimization. Previous methods proposed selecting points by minimizing MMD, but the non-convexity of this objective makes global minimization difficult.

Method: Instead of focusing on global minimization, the authors consider stationary points of the MMD which can be accurately computed. They study the cubature error of these stationary points and propose discretised gradient flows as a practical method for computing them.

Result: The cubature error of stationary MMD points vanishes faster than the MMD itself, showing a super-convergence property. A refined convergence analysis establishes a novel non-asymptotic finite-particle error bound.

Conclusion: Stationary points of the MMD provide an effective way to approximate target distributions with a super-convergence property, and discretised gradient flows offer a practical computational strategy.

Abstract: Approximation of a target probability distribution using a finite set of
points is a problem of fundamental importance, arising in cubature, data
compression, and optimisation. Several authors have proposed to select points
by minimising a maximum mean discrepancy (MMD), but the non-convexity of this
objective precludes global minimisation in general. Instead, we consider
\emph{stationary} points of the MMD which, in contrast to points globally
minimising the MMD, can be accurately computed. Our main theoretical
contribution is the (perhaps surprising) result that, for integrands in the
associated reproducing kernel Hilbert space, the cubature error of stationary
MMD points vanishes \emph{faster} than the MMD. Motivated by this
\emph{super-convergence} property, we consider discretised gradient flows as a
practical strategy for computing stationary points of the MMD, presenting a
refined convergence analysis that establishes a novel non-asymptotic
finite-particle error bound, which may be of independent interest.

</details>


### [216] [Input Convex Kolmogorov Arnold Networks](https://arxiv.org/abs/2505.21208)
*Thomas Deschatre,Xavier Warin*

Main category: stat.ML

TL;DR: The paper introduces ICKAN, an input convex neural network architecture using Kolmogorov-Arnold networks. Two specific networks are presented: one based on low-order linear-by-part representation and the other based on cubic splines. The former is supported by a universal approximation theorem while the latter is only supported by numerical results. These networks perform competitively with classical ICNNs in simple tests and optimal transport problems.


<details>
  <summary>Details</summary>
Motivation: To develop a new type of input convex neural network architecture that can perform as well as or better than classical ICNNs in various tasks, especially those needing a convex approximation of functions such as optimal transport problems.

Method: ICKAN architecture is introduced with two specific networks. The first network uses a low-order, linear-by-part function representation and is backed by a universal approximation theorem. The second network employs cubic splines and its convergence is only numerically verified. Both networks are tested against classical ICNNs.

Result: ICKAN networks performed competitively with classical ICNNs in simple tests and optimal transport problems. Cubic ICKANs produced results similar to those of classical ICNNs.

Conclusion: ICKAN represents a promising alternative to classical ICNNs for tasks requiring convex function approximations. Further research could focus on improving and extending the theoretical foundation of the cubic spline-based network.

Abstract: This article presents an input convex neural network architecture using
Kolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the
first is based on a low-order, linear-by-part, representation of functions, and
a universal approximation theorem is provided. The second is based on cubic
splines, for which only numerical results support convergence. We demonstrate
on simple tests that these networks perform competitively with classical input
convex neural networks (ICNNs). In a second part, we use the networks to solve
some optimal transport problems needing a convex approximation of functions and
demonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs
produce results similar to those of classical ICNNs.

</details>


### [217] [Autoencoding Random Forests](https://arxiv.org/abs/2505.21441)
*Binh Duc Vu,Jan Kapar,Marvin Wright,David S. Watson*

Main category: stat.ML

TL;DR: The paper proposes a principled method for autoencoding with random forests, which builds on nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data.


<details>
  <summary>Details</summary>
Motivation: Autoencoding with random forests has not been fully explored yet. There is a need for a principled method to build an autoencoder using random forests.

Method: The strategy builds on foundational results from nonparametric statistics and spectral graph theory. Exact and approximate solutions to the decoding problem are provided via constrained optimization, split relabeling, and nearest neighbors regression. These methods invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees.

Result: The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. Experiments illustrate the ease and utility of the method in a wide range of settings, including tabular, image, and genomic data.

Conclusion: This autoencoder provides powerful new tools for visualization, compression, clustering, and denoising.

Abstract: We propose a principled method for autoencoding with random forests. Our
strategy builds on foundational results from nonparametric statistics and
spectral graph theory to learn a low-dimensional embedding of the model that
optimally represents relationships in the data. We provide exact and
approximate solutions to the decoding problem via constrained optimization,
split relabeling, and nearest neighbors regression. These methods effectively
invert the compression pipeline, establishing a map from the embedding space
back to the input space using splits learned by the ensemble's constituent
trees. The resulting decoders are universally consistent under common
regularity assumptions. The procedure works with supervised or unsupervised
models, providing a window into conditional or joint distributions. We
demonstrate various applications of this autoencoder, including powerful new
tools for visualization, compression, clustering, and denoising. Experiments
illustrate the ease and utility of our method in a wide range of settings,
including tabular, image, and genomic data.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [218] [Unified Deep Learning Approach for Estimating the Metallicities of RR Lyrae Stars Using light curves from Gaia Data Release 3](https://arxiv.org/abs/2505.20947)
*Lorenzo Monti,Tatiana Muraveva,Alessia Garofalo,Gisella Clementini,Maria Letizia Valentini*

Main category: astro-ph.SR

TL;DR: The paper presents a deep learning framework using GRU neural network to estimate metallicities of RR Lyrae stars (both RRab and RRc types) from Gaia G-band light curves. It achieves high accuracy and strong performance on validation sets, supporting large-scale photometric metallicity estimation for studies of stellar populations and Galactic structure.


<details>
  <summary>Details</summary>
Motivation: There is a need for scalable methods to estimate metallicities of RR Lyrae stars from photometric data due to the large amount of data provided by ESA Gaia DR3.

Method: A unified deep learning framework based on Gated Recurrent Unit (GRU) neural network optimized for time-series extrinsic regression. The pipeline includes preprocessing steps such as phase folding, smoothing, and sample weighting, and uses photometric metallicities from the literature as training targets.

Result: On held-out validation sets, the GRU model achieves MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401 for RRab stars; for RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625.

Conclusion: Deep learning is effective for large-scale photometric metallicity estimation and can be applied to studies of stellar populations and Galactic structure.

Abstract: RR Lyrae stars (RRLs) are old pulsating variables widely used as metallicity
tracers due to the correlation between their metal abundances and light curve
morphology. With ESA Gaia DR3 providing light curves for about 270,000 RRLs,
there is a pressing need for scalable methods to estimate their metallicities
from photometric data. We introduce a unified deep learning framework that
estimates metallicities for both fundamental-mode (RRab) and first-overtone
(RRc) RRLs using Gaia G-band light curves. This approach extends our previous
work on RRab stars to include RRc stars, aiming for high predictive accuracy
and broad generalization across both pulsation types. The model is based on a
Gated Recurrent Unit (GRU) neural network optimized for time-series extrinsic
regression. Our pipeline includes preprocessing steps such as phase folding,
smoothing, and sample weighting, and uses photometric metallicities from the
literature as training targets. The architecture is designed to handle
morphological differences between RRab and RRc light curves without requiring
separate models. On held-out validation sets, our GRU model achieves strong
performance: for RRab stars, MAE = 0.0565 dex, RMSE = 0.0765 dex, R^2 = 0.9401;
for RRc stars, MAE = 0.0505 dex, RMSE = 0.0720 dex, R^2 = 0.9625. These results
show the effectiveness of deep learning for large-scale photometric metallicity
estimation and support its application to studies of stellar populations and
Galactic structure.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [219] [Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments](https://arxiv.org/abs/2505.20423)
*Julio de la Torre-Vanegas,Miguel Soriano-Garcia,Israel Becerra,Diego Mercado-Ravell*

Main category: cs.RO

TL;DR: This paper presents a risk-aware approach for UAVs landing in crowded urban environments using semantic segmentation and deep neural networks to identify safe landing zones with over 90% success rates.


<details>
  <summary>Details</summary>
Motivation: Landing safely in crowded urban environments is challenging for UAVs, especially in emergency situations where potential hazards must be continuously evaluated.

Method: The method uses a specialized deep neural network for pixel-level risk assessment through semantic segmentation. It applies an algorithm based on risk maps to adaptively identify stable Safe Landing Zones (SLZ) while considering moving obstacles and visual challenges. The control system guides the UAV toward low-risk regions with altitude-dependent safety thresholds and temporal landing point stabilization.

Result: The experimental validation shows over 90% landing success rates in challenging real scenarios, significantly improving various risk metrics.

Conclusion: Risk-oriented vision methods can effectively reduce the risk of accidents during emergency landings in complex urban environments, enhancing UAV capabilities in such operations.

Abstract: Landing safely in crowded urban environments remains an essential yet
challenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in
emergency situations. In this work, we propose a risk-aware approach that
harnesses semantic segmentation to continuously evaluate potential hazards in
the drone's field of view. By using a specialized deep neural network to assign
pixel-level risk values and applying an algorithm based on risk maps, our
method adaptively identifies a stable Safe Landing Zone (SLZ) despite moving
critical obstacles such as vehicles, people, etc., and other visual challenges
like shifting illumination. A control system then guides the UAV toward this
low-risk region, employing altitude-dependent safety thresholds and temporal
landing point stabilization to ensure robust descent trajectories. Experimental
validation in diverse urban environments demonstrates the effectiveness of our
approach, achieving over 90% landing success rates in very challenging real
scenarios, showing significant improvements in various risk metrics. Our
findings suggest that risk-oriented vision methods can effectively help reduce
the risk of accidents in emergency landing situations, particularly in complex,
unstructured, urban scenarios, densely populated with moving risky obstacles,
while potentiating the true capabilities of UAVs in complex urban operations.

</details>


### [220] [Robot Operation of Home Appliances by Reading User Manuals](https://arxiv.org/abs/2505.20424)
*Jian Zhang,Hanbo Zhang,Anxing Xiao,David Hsu*

Main category: cs.RO

TL;DR: ApBot is a robot system that operates novel household appliances by reading their user manuals. It constructs a structured, symbolic model of an appliance from its manual with the help of a large vision-language model (VLM), grounds the symbolic actions visually to control panel elements, and closes the loop by updating the model based on visual feedback.


<details>
  <summary>Details</summary>
Motivation: Operating home appliances is a critical capability for assistive home robots. However, it faces multiple challenges such as infering goal-conditioned partial policies from unstructured textual descriptions in a user manual document, grounding the policies to the appliance in the physical world, and executing the policies reliably over potentially many steps despite compounding errors.

Method: ApBot constructs a structured, symbolic model of an appliance from its manual with the help of a large vision-language model (VLM). It grounds the symbolic actions visually to control panel elements and closes the loop by updating the model based on visual feedback.

Result: Experiments show that across a wide range of simulated and real-world appliances, ApBot achieves consistent and statistically significant improvements in task success rate, compared with state-of-the-art large VLMs used directly as control policies.

Conclusion: A structured internal representations plays an important role in robust robot operation of home appliances, especially complex ones.

Abstract: Operating home appliances, among the most common tools in every household, is
a critical capability for assistive home robots. This paper presents ApBot, a
robot system that operates novel household appliances by "reading" their user
manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial
policies from their unstructured, textual descriptions in a user manual
document, (ii) ground the policies to the appliance in the physical world, and
(iii) execute the policies reliably over potentially many steps, despite
compounding errors. To tackle these challenges, ApBot constructs a structured,
symbolic model of an appliance from its manual, with the help of a large
vision-language model (VLM). It grounds the symbolic actions visually to
control panel elements. Finally, ApBot closes the loop by updating the model
based on visual feedback. Our experiments show that across a wide range of
simulated and real-world appliances, ApBot achieves consistent and
statistically significant improvements in task success rate, compared with
state-of-the-art large VLMs used directly as control policies. These results
suggest that a structured internal representations plays an important role in
robust robot operation of home appliances, especially, complex ones.

</details>


### [221] [Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](https://arxiv.org/abs/2505.20503)
*Matthew Lisondra,Beno Benhabib,Goldie Nejat*

Main category: cs.RO

TL;DR: Foundation models are integrated into mobile service robotics via embodied AI, improving robot capabilities in multimodal sensor fusion, real-time decision-making, task generalization, and human-robot interaction. This paper reviews these integrations, discusses applications in domestic assistance, healthcare, and service automation, and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: To systematically review the integration of foundation models in mobile service robotics and identify key challenges in embodied AI that can be addressed by these models.

Method: The paper explores how foundation models enable real-time sensor fusion, language-conditioned control, and adaptive task execution in mobile service robots. It also examines applications in domestic assistance, healthcare, and service automation sectors.

Result: Foundation models significantly enhance mobile service robots' abilities to perceive, reason, and act in dynamic environments, thereby improving their performance in complex tasks.

Conclusion: Future research should focus on predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to ensure scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.

Abstract: Rapid advancements in foundation models, including Large Language Models,
Vision-Language Models, Multimodal Large Language Models, and
Vision-Language-Action Models have opened new avenues for embodied AI in mobile
service robotics. By combining foundation models with the principles of
embodied AI, where intelligent systems perceive, reason, and act through
physical interactions, robots can improve understanding, adapt to, and execute
complex tasks in dynamic real-world environments. However, embodied AI in
mobile service robots continues to face key challenges, including multimodal
sensor fusion, real-time decision-making under uncertainty, task
generalization, and effective human-robot interactions (HRI). In this paper, we
present the first systematic review of the integration of foundation models in
mobile service robotics, identifying key open challenges in embodied AI and
examining how foundation models can address them. Namely, we explore the role
of such models in enabling real-time sensor fusion, language-conditioned
control, and adaptive task execution. Furthermore, we discuss real-world
applications in the domestic assistance, healthcare, and service automation
sectors, demonstrating the transformative impact of foundation models on
service robotics. We also include potential future research directions,
emphasizing the need for predictive scaling laws, autonomous long-term
adaptation, and cross-embodiment generalization to enable scalable, efficient,
and robust deployment of foundation models in human-centric robotic systems.

</details>


### [222] [Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners](https://arxiv.org/abs/2505.20573)
*Jiabao Ji,Yongchao Chen,Yang Zhang,Ramana Rao Kompella,Chuchu Fan,Gaowen Liu,Shiyu Chang*

Main category: cs.RO

TL;DR: 通过将强化学习与可验证奖励（RLVR）结合，提出了一种新框架，以激励大型语言模型（LLMs）在生成计划时考虑物理约束。实验表明，在BoxNet任务和新开发的BoxNet3D环境中，具有约束意识的小型LLMs比没有约束的大规模模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在各种机器人控制任务中表现出色，但在实际应用中仍然受到限制。原因是它们经常产生违反物理约束的无效行动计划，例如将机器人引导到不可达位置或导致机器人之间发生碰撞。主要问题在于推理过程中缺乏对这些物理约束的认识。

Method: 我们提出了一个将强化学习与可验证奖励（RLVR）相结合的新框架，以激励大型语言模型（LLMs）在计划生成过程中进行具有约束意识的推理。在此方法中，只有成功完成控制任务的有效行动计划才能获得正向奖励。我们将该方法应用于两个小规模的LLMs：非推理Qwen2.5-3B-Instruct和推理Qwen3-4B。

Result: 实验结果表明，具有约束意识的小型LLMs在BoxNet任务和新开发的BoxNet3D环境中明显优于没有约束的大规模模型。

Conclusion: 这项工作强调了即使是在小型LLMs中嵌入物理约束的有效性，能够实现复杂、受物理约束环境中的可扩展和高效的多机器人控制。

Abstract: Large language models (LLMs) have demonstrated strong performance in various
robot control tasks. However, their deployment in real-world applications
remains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently
produce invalid action plans that violate physical constraints, such as
directing a robot to an unreachable location or causing collisions between
robots. This issue primarily arises from a lack of awareness of these physical
constraints during the reasoning process. To address this issue, we propose a
novel framework that integrates reinforcement learning with verifiable rewards
(RLVR) to incentivize knowledge of physical constraints into LLMs to induce
constraints-aware reasoning during plan generation. In this approach, only
valid action plans that successfully complete a control task receive positive
rewards. We applied our method to two small-scale LLMs: a non-reasoning
Qwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results
demonstrate that constraint-aware small LLMs largely outperform large-scale
models without constraints, grounded on both the BoxNet task and a newly
developed BoxNet3D environment built using MuJoCo. This work highlights the
effectiveness of grounding even small LLMs with physical constraints to enable
scalable and efficient multi-robot control in complex, physically constrained
environments.

</details>


### [223] [Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform](https://arxiv.org/abs/2505.20751)
*Zongcai Tan amd Dandan Zhang*

Main category: cs.RO

TL;DR: Interactive OT Gym, a RL-based simulation platform for OT-driven microrobotics, integrates haptic feedback, RL modules, and shared control strategies to enhance cooperative biological object manipulation. Evaluated through cell manipulation tasks, it significantly improves performance, reducing task completion time by 67% and achieving 100% success rate.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of controlling conventional multi-trap OT for cooperative manipulation of multiple complex-shaped microrobots in dynamic environments.

Method: Introduction of Interactive OT Gym which supports complex physical field simulations and integrates haptic feedback interfaces, RL modules, and context-aware shared control strategies tailored for OT-driven microrobot in cooperative biological object manipulation tasks.

Result: Experimental results show that the shared control system significantly improves micromanipulation performance, reducing task completion time by approximately 67% compared to using pure human or RL control alone and achieving a 100% success rate.

Conclusion: Interactive OT Gym serves as a user-friendly training and testing environment for the development of advanced interactive OT-driven micromanipulation systems and control algorithms.

Abstract: Optical tweezers (OT) offer unparalleled capabilities for micromanipulation
with submicron precision in biomedical applications. However, controlling
conventional multi-trap OT to achieve cooperative manipulation of multiple
complex-shaped microrobots in dynamic environments poses a significant
challenge. To address this, we introduce Interactive OT Gym, a reinforcement
learning (RL)-based simulation platform designed for OT-driven microrobotics.
Our platform supports complex physical field simulations and integrates haptic
feedback interfaces, RL modules, and context-aware shared control strategies
tailored for OT-driven microrobot in cooperative biological object manipulation
tasks. This integration allows for an adaptive blend of manual and autonomous
control, enabling seamless transitions between human input and autonomous
operation. We evaluated the effectiveness of our platform using a cell
manipulation task. Experimental results show that our shared control system
significantly improves micromanipulation performance, reducing task completion
time by approximately 67% compared to using pure human or RL control alone and
achieving a 100% success rate. With its high fidelity, interactivity, low cost,
and high-speed simulation capabilities, Interactive OT Gym serves as a
user-friendly training and testing environment for the development of advanced
interactive OT-driven micromanipulation systems and control algorithms. For
more details on the project, please see our website
https://sites.google.com/view/otgym

</details>


### [224] [FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation](https://arxiv.org/abs/2505.20783)
*Jiaping Xiao,Cheng Wen Tsao,Yuhang Zhang,Mir Feroskhan*

Main category: cs.RO

TL;DR: 本文提出了一种基于基础模型引导的路径规划器（FM-Planner），并通过综合基准测试和实际验证，评估了其在无人机路径规划中的应用。首先系统评估了八种代表性的LLM和VLM方法，然后设计了一个结合语义推理与视觉感知的集成规划器，并通过真实实验验证了其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型（如LLM和VLM）在机器人感知和智能决策方面展现出潜力，但它们在全球路径规划中的实际应用效果尚未得到充分探索。因此，需要研究这些模型在无人机路径规划中的适用性及性能。

Method: 1. 系统评估八种代表性LLM和VLM方法在标准化模拟场景中的表现。
2. 设计一个集成LLM-Vision规划器，将语义推理与视觉感知相结合以实现有效的实时导航。
3. 在多种配置下进行真实世界实验，部署并验证所提出的路径规划器。

Result: 研究表明，基础模型在无人机路径规划中具有一定的优势和局限性，为实际应用提供了宝贵的见解和可行的实施方案。

Conclusion: 基础模型（如LLM和VLM）在无人机路径规划中具有潜在的应用价值，但其实际效果取决于具体任务需求和环境条件。本文的研究成果为未来相关工作奠定了基础，并推动了自主飞行的实际应用。

Abstract: Path planning is a critical component in autonomous drone operations,
enabling safe and efficient navigation through complex environments. Recent
advances in foundation models, particularly large language models (LLMs) and
vision-language models (VLMs), have opened new opportunities for enhanced
perception and intelligent decision-making in robotics. However, their
practical applicability and effectiveness in global path planning remain
relatively unexplored. This paper proposes foundation model-guided path
planners (FM-Planner) and presents a comprehensive benchmarking study and
practical validation for drone path planning. Specifically, we first
systematically evaluate eight representative LLM and VLM approaches using
standardized simulation scenarios. To enable effective real-time navigation, we
then design an integrated LLM-Vision planner that combines semantic reasoning
with visual perception. Furthermore, we deploy and validate the proposed path
planner through real-world experiments under multiple configurations. Our
findings provide valuable insights into the strengths, limitations, and
feasibility of deploying foundation models in real-world drone applications and
providing practical implementations in autonomous flight. Project site:
https://github.com/NTU-ICG/FM-Planner.

</details>


### [225] [STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation](https://arxiv.org/abs/2505.20781)
*Hossein Goli,Michael Gimelfarb,Nathan Samuel de Lara,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: STITCH-OPE是一种基于模型的生成框架，利用去噪扩散解决高维、长时域的离线策略评估问题。它通过引导去噪过程和拼接部分轨迹实现目标策略的合成轨迹生成，显著降低方差并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的离线策略评估（OPE）方法在处理高维、长时域问题时效果不佳，因重要性加权导致的方差指数爆炸或学习动力学模型的累积误差。

Method: 提出了一种名为STITCH-OPE的方法，该方法基于模型的生成框架，利用去噪扩散技术。从一个在行为数据上预训练的扩散模型开始，通过使用目标策略的得分函数引导去噪过程来生成目标策略的合成轨迹。STITCH-OPE包含两个技术创新：1) 减去行为策略的得分以防止过度正则化；2) 通过端到端拼接部分轨迹生成长时域轨迹。

Result: 理论分析表明，在温和假设下，这些修改相较于长时域轨迹扩散实现了指数级的方差减少。实验结果表明，在D4RL和OpenAI Gym基准测试中，相比最先进的OPE方法，STITCH-OPE在均方误差、相关性和后悔值指标上均有显著改进。

Conclusion: STITCH-OPE解决了高维、长时域离线策略评估中的挑战，通过创新的技术手段有效降低了方差，并在多个基准测试中表现出色，为机器人技术和医疗保健等领域提供了更可靠的OPE解决方案。

Abstract: Off-policy evaluation (OPE) estimates the performance of a target policy
using offline data collected from a behavior policy, and is crucial in domains
such as robotics or healthcare where direct interaction with the environment is
costly or unsafe. Existing OPE methods are ineffective for high-dimensional,
long-horizon problems, due to exponential blow-ups in variance from importance
weighting or compounding errors from learned dynamics models. To address these
challenges, we propose STITCH-OPE, a model-based generative framework that
leverages denoising diffusion for long-horizon OPE in high-dimensional state
and action spaces. Starting with a diffusion model pre-trained on the behavior
data, STITCH-OPE generates synthetic trajectories from the target policy by
guiding the denoising process using the score function of the target policy.
STITCH-OPE proposes two technical innovations that make it advantageous for
OPE: (1) prevents over-regularization by subtracting the score of the behavior
policy during guidance, and (2) generates long-horizon trajectories by
stitching partial trajectories together end-to-end. We provide a theoretical
guarantee that under mild assumptions, these modifications result in an
exponential reduction in variance versus long-horizon trajectory diffusion.
Experiments on the D4RL and OpenAI Gym benchmarks show substantial improvement
in mean squared error, correlation, and regret metrics compared to
state-of-the-art OPE methods.

</details>


### [226] [Hume: Introducing System-2 Thinking in Visual-Language-Action Model](https://arxiv.org/abs/2505.21432)
*Haoming Song,Delin Qu,Yuanqi Yao,Qizhi Chen,Qi Lv,Yiwen Tang,Modi Shi,Guanghui Ren,Maoqing Yao,Bin Zhao,Dong Wang,Xuelong Li*

Main category: cs.RO

TL;DR: Hume is a dual-system Vision-Language-Action (VLA) model that incorporates value-guided System-2 thinking and cascaded action denoising, designed for dexterous robot control. It outperforms existing VLA models in simulations and real-robot deployments.


<details>
  <summary>Details</summary>
Motivation: The potential of slow thinking has been successfully applied to Large Language Models (LLMs) for complex digital tasks, but its application to robotic foundation models interacting with the physical world remains largely unexplored.

Method: Hume consists of two systems: System 2 implements value-guided thinking by extending a VLA model backbone with a value-query head to estimate state-action values, selecting actions accordingly; System 1 is a lightweight reactive visuomotor policy that performs cascaded action denoising based on the action selected by System 2.

Result: Hume demonstrates superior performance compared to existing state-of-the-art Vision-Language-Action models across multiple simulation benchmarks and real-robot deployments.

Conclusion: Hume explores human-like thinking capabilities for Vision-Language-Action models, enhancing dexterous robot control through value-guided thinking and cascaded action denoising.

Abstract: Humans practice slow thinking before performing actual actions when handling
complex tasks in the physical world. This thinking paradigm, recently, has
achieved remarkable advancement in boosting Large Language Models (LLMs) to
solve complex tasks in digital domains. However, the potential of slow thinking
remains largely unexplored for robotic foundation models interacting with the
physical world. In this work, we propose Hume: a dual-system
Vision-Language-Action (VLA) model with value-guided System-2 thinking and
cascaded action denoising, exploring human-like thinking capabilities of
Vision-Language-Action models for dexterous robot control. System 2 of Hume
implements value-Guided thinking by extending a Vision-Language-Action Model
backbone with a novel value-query head to estimate the state-action value of
predicted actions. The value-guided thinking is conducted by repeat sampling
multiple action candidates and selecting one according to state-action value.
System 1 of Hume is a lightweight reactive visuomotor policy that takes System
2 selected action and performs cascaded action denoising for dexterous robot
control. At deployment time, System 2 performs value-guided thinking at a low
frequency while System 1 asynchronously receives the System 2 selected action
candidate and predicts fluid actions in real time. We show that Hume
outperforms the existing state-of-the-art Vision-Language-Action models across
multiple simulation benchmark and real-robot deployments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [227] [InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling](https://arxiv.org/abs/2505.20600)
*Xiaoxiao Jiang,Suyi Li,Lingyun Yang,Tianyu Feng,Zhipeng Di,Weiyi Lu,Guoxuan Zhu,Xiu Lin,Kan Liu,Yinghao Yu,Tao Lan,Guodong Yang,Lin Qu,Liping Zhang,Wei Wang*

Main category: cs.DC

TL;DR: Generative image editing using diffusion models is a common application in AI cloud services. This paper introduces InstGenIE, a system that efficiently handles image editing requests by reusing cached intermediate activations, employing a bubble-free pipeline scheme, proposing a continuous batching strategy, and developing a load balancing strategy. InstGenIE outperforms existing systems, achieving higher throughput and significantly reducing latency while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of efficient generative image editing using diffusion models, particularly in production environments where masks specify regions to be edited, introducing sparsity and requiring judicious computation management.

Method: InstGenIE skips redundant computations for unmasked areas by reusing cached intermediate activations. It uses a bubble-free pipeline scheme to overlap computation with cache loading. A continuous batching strategy allows new requests to join ongoing batches without waiting. Additionally, a load balancing strategy considers both computation and cache loading loads.

Result: InstGenIE achieves up to 3x higher throughput compared to state-of-the-art systems and reduces average request latency by up to 14.7x, all while ensuring image quality.

Conclusion: InstGenIE effectively serves image editing requests by leveraging cached activations, optimizing pipeline schemes, implementing continuous batching, and balancing loads. It sets a new standard for diffusion model serving in generative image editing.

Abstract: Generative image editing using diffusion models has become a prevalent
application in today's AI cloud services. In production environments, image
editing typically involves a mask that specifies the regions of an image
template to be edited. The use of masks provides direct control over the
editing process and introduces sparsity in the model inference. In this paper,
we present InstGenIE, a system that efficiently serves image editing requests.
The key insight behind InstGenIE is that image editing only modifies the masked
regions of image templates while preserving the original content in the
unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant
computations associated with the unmasked areas by reusing cached intermediate
activations from previous inferences. To mitigate the high cache loading
overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps
computation with cache loading. Additionally, to reduce queuing latency in
online serving while improving the GPU utilization, InstGenIE proposes a novel
continuous batching strategy for diffusion model serving, allowing newly
arrived requests to join the running batch in just one step of denoising
computation, without waiting for the entire batch to complete. As heterogeneous
masks induce imbalanced loads, InstGenIE also develops a load balancing
strategy that takes into account the loads of both computation and cache
loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving
systems for image editing, achieving up to 3x higher throughput and reducing
average request latency by up to 14.7x while ensuring image quality.

</details>


### [228] [Time-Series Learning for Proactive Fault Prediction in Distributed Systems with Deep Neural Structures](https://arxiv.org/abs/2505.20705)
*Yang Wang,Wenxuan Zhu,Xuehui Quan,Heyi Wang,Chang Liu,Qiyuan Wu*

Main category: cs.DC

TL;DR: This paper proposes an intelligent prediction method using temporal feature learning for fault prediction in distributed systems, which performs well in experiments.


<details>
  <summary>Details</summary>
Motivation: To solve the challenges of fault prediction and delayed response in distributed systems.

Method: Using multi-dimensional performance metric sequences as input, employing GRU to model system state evolution over time, applying attention mechanism to enhance key temporal segments, and designing a feedforward neural network for final classification.

Result: The model outperforms mainstream time-series models in Accuracy, F1-Score, and AUC. The loss function curve confirms training convergence and reliability.

Conclusion: The proposed method effectively learns system behavior patterns and achieves efficient fault detection.

Abstract: This paper addresses the challenges of fault prediction and delayed response
in distributed systems by proposing an intelligent prediction method based on
temporal feature learning. The method takes multi-dimensional performance
metric sequences as input. We use a Gated Recurrent Unit (GRU) to model the
evolution of system states over time. An attention mechanism is then applied to
enhance key temporal segments, improving the model's ability to identify
potential faults. On this basis, a feedforward neural network is designed to
perform the final classification, enabling early warning of system failures. To
validate the effectiveness of the proposed approach, comparative experiments
and ablation analyses were conducted using data from a large-scale real-world
cloud system. The experimental results show that the model outperforms various
mainstream time-series models in terms of Accuracy, F1-Score, and AUC. This
demonstrates strong prediction capability and stability. Furthermore, the loss
function curve confirms the convergence and reliability of the training
process. It indicates that the proposed method effectively learns system
behavior patterns and achieves efficient fault detection.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [229] [VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning and Formal Verification](https://arxiv.org/abs/2505.20302)
*Patrick Yubeaton,Andre Nakkab,Weihua Xiao,Luca Collini,Ramesh Karri,Chinmay Hegde,Siddharth Garg*

Main category: cs.PL

TL;DR: This paper introduces VeriThoughts, a dataset for reasoning-based Verilog code generation with a new benchmark framework and specialized models, aiming to automate hardware design while ensuring correctness.


<details>
  <summary>Details</summary>
Motivation: To address the need for automated hardware design tools that can produce verifiably correct implementations from high-level specifications.

Method: Introduced VeriThoughts dataset, established a new benchmark framework based on formal verification methods, and presented specialized small-scale models optimized for Verilog generation.

Result: Aims to accelerate the hardware development process while maintaining rigorous correctness guarantees.

Conclusion: Code and data are available at https://github.com/wilyub/VeriThoughts.

Abstract: This paper introduces VeriThoughts, a novel dataset designed for
reasoning-based Verilog code generation. We establish a new benchmark framework
grounded in formal verification methods to evaluate the quality and correctness
of generated hardware descriptions. Additionally, we present a suite of
specialized small-scale models optimized specifically for Verilog generation.
Our work addresses the growing need for automated hardware design tools that
can produce verifiably correct implementations from high-level specifications,
potentially accelerating the hardware development process while maintaining
rigorous correctness guarantees. Our code and data are available at
\href{https://github.com/wilyub/VeriThoughts}{this URL}.

</details>


### [230] [LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability](https://arxiv.org/abs/2505.20356)
*Shuoming Zhang,Jiacheng Zhao,Chunwei Xia,Zheng Wang,Yunji Chen,Xiaobing Feng,Huimin Cui*

Main category: cs.PL

TL;DR: LEGO-Compiler is a novel neural compilation system that uses LLMs to translate high-level languages into assembly code, with innovations in program decomposition, verifiable steps, and self-correction; it shows high accuracy and scalability improvements.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing LLMs in handling long and complex programs and to explore their application in compiler and code translation tasks.

Method: Introduced LEGO-Compiler with three key innovations: LEGO translation for program decomposition, verifiable LLM workflow for simplifying the compilation process, and a feedback mechanism for self-correction. Supported by formal proofs of translation composability.

Result: Achieved over 99% accuracy on ExeBench, 97.9% on AnsiBench, and a near one order-of-magnitude improvement in compilable code size scalability.

Conclusion: This work provides new opportunities for applying LLMs to system-level tasks and complements traditional compiler technologies.

Abstract: Large language models (LLMs) have the potential to revolutionize how we
design and implement compilers and code translation tools. However, existing
LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler,
a novel neural compilation system that leverages LLMs to translate high-level
languages into assembly code. Our approach centers on three key innovations:
LEGO translation, which decomposes the input program into manageable blocks;
breaking down the complex compilation process into smaller, simpler verifiable
steps by organizing it as a verifiable LLM workflow by external tests; and a
feedback mechanism for self-correction. Supported by formal proofs of
translation composability, LEGO-Compiler demonstrates high accuracy on multiple
datasets, including over 99% on ExeBench and 97.9% on industrial-grade
AnsiBench. Additionally, LEGO-Compiler has also acheived near one
order-of-magnitude improvement on compilable code size scalability. This work
opens new avenues for applying LLMs to system-level tasks, complementing
traditional compiler technologies.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [231] [Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting](https://arxiv.org/abs/2505.20714)
*Zechen Li,Lanqing Yang,Yiheng Bian,Hao Pan,Yongjian Fu,Yezhou Wang,Yi-Chao Chen,Guangtao Xue,Ju Ren*

Main category: cs.NI

TL;DR: This paper introduces a frequency-embedded 3D Gaussian splatting (3DGS) algorithm for wideband RF radiance field modeling, which can efficiently reconstruct RF radiance fields at arbitrary unknown frequencies within a given 3D environment. The method achieves an average SSIM up to 0.72 and shows significant improvement compared to SOTA methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to overcome the limitations of existing works that are restricted to single-frequency modeling by developing a method that can model wideband RF radiance fields across multiple frequencies.

Method: The method involves designing an EM feature network with attenuation and radiance modules to learn the complex relationships between RF frequencies and the key properties of each 3D Gaussian, specifically the attenuation factor and RF signal intensity. This is achieved through training the frequency-embedded 3DGS model.

Result: The proposed method achieves an average SSIM up to 0.72 and shows a significant improvement of up to 17.8% compared to current SOTA methods trained on individual test frequencies. Additionally, it achieves an SSIM of 0.70 without prior training on these frequencies, representing only a 2.8% performance drop compared to models trained with full PAS data.

Conclusion: The conclusion is that the frequency-embedded 3DGS algorithm effectively models wideband RF radiance fields, demonstrating the capability to estimate power angular spectrum at unknown frequencies with high accuracy and efficiency.

Abstract: This paper presents an innovative frequency-embedded 3D Gaussian splatting
(3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling,
offering an advancement over the existing works limited to single-frequency
modeling. Grounded in fundamental physics, we uncover the complex relationship
between EM wave propagation behaviors and RF frequencies. Inspired by this, we
design an EM feature network with attenuation and radiance modules to learn the
complex relationships between RF frequencies and the key properties of each 3D
Gaussian, specifically the attenuation factor and RF signal intensity. By
training the frequency-embedded 3DGS model, we can efficiently reconstruct RF
radiance fields at arbitrary unknown frequencies within a given 3D environment.
Finally, we propose a large-scale power angular spectrum (PAS) dataset
containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments,
and conduct extensive experiments to verify the effectiveness of our method.
Our approach achieves an average Structural Similarity Index Measure (SSIM) up
to 0.72, and a significant improvement up to 17.8% compared to the current
state-of-the-art (SOTA) methods trained on individual test frequencies.
Additionally, our method achieves an SSIM of 0.70 without prior training on
these frequencies, which represents only a 2.8% performance drop compared to
models trained with full PAS data. This demonstrates our model's capability to
estimate PAS at unknown frequencies. For related code and datasets, please
refer to https://github.com/sim-2-real/Wideband3DGS.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [232] [Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2505.20817)
*Savelii Chezhegov,Aleksandr Beznosikov,Samuel Horváth,Eduard Gorbunov*

Main category: math.OC

TL;DR: This paper establishes the first high-probability convergence bounds for Clip-SGD in convex optimization with heavy-tailed noise under (L0,L1)-smoothness.


<details>
  <summary>Details</summary>
Motivation: Gradient clipping is effective for handling heavy-tailed noise and Clip-SGD shows stronger convergence guarantees than SGD under specific conditions. However, high-probability convergence of Clip-SGD under both heavy-tailed noise and (L0,L1)-smoothness has not been fully explored.

Method: The authors derive high-probability convergence bounds for Clip-SGD in convex (L0,L1)-smooth optimization with heavy-tailed noise, extending prior results to include deterministic and stochastic settings as special cases.

Result: The derived rates avoid exponentially large factors and do not depend on restrictive sub-Gaussian noise assumptions, thus broadening the applicability of gradient clipping.

Conclusion: This work provides a significant advancement in understanding the convergence properties of Clip-SGD under realistic noise conditions in machine learning tasks.

Abstract: Gradient clipping is a widely used technique in Machine Learning and Deep
Learning (DL), known for its effectiveness in mitigating the impact of
heavy-tailed noise, which frequently arises in the training of large language
models. Additionally, first-order methods with clipping, such as Clip-SGD,
exhibit stronger convergence guarantees than SGD under the
$(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks.
However, the high-probability convergence of Clip-SGD under both assumptions --
heavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed
in the literature. In this paper, we bridge this critical gap by establishing
the first high-probability convergence bounds for Clip-SGD applied to convex
$(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends
prior results by recovering known bounds for the deterministic case and the
stochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid
exponentially large factors and do not rely on restrictive sub-Gaussian noise
assumptions, significantly broadening the applicability of gradient clipping.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [233] [Predictive Performance of Deep Quantum Data Re-uploading Models](https://arxiv.org/abs/2505.20337)
*Xin Wang,Han-Xiao Tao,Re-Bing Wu*

Main category: quant-ph

TL;DR: 量子机器学习模型中的数据重载电路因其卓越的表达能力和可训练性而受到广泛关注。然而，当处理高维数据时，使用深层编码层的数据重载模型的预测性能会退化到接近随机猜测的水平。实验验证了这一理论，并表明在处理高维数据时，量子数据重载模型应设计为更宽而非更深的电路架构。


<details>
  <summary>Details</summary>
Motivation: 尽管量子机器学习模型中的数据重载电路具有卓越的表达能力和可训练性，但其在未见数据上的预测性能尚未得到充分研究。

Method: 理论上证明了在处理高维数据时，随着编码层数的增加，有限量子比特的数据重载模型的预测性能逐渐退化到接近随机猜测的水平。并通过实验证明了这一点。

Result: 实验结果表明，在处理高维数据时，量子数据重载模型应设计为更宽而非更深的电路架构。

Conclusion: 深层编码层在量子数据重载模型中会导致预测性能的退化，建议采用更宽的电路架构来处理高维数据。

Abstract: Quantum machine learning models incorporating data re-uploading circuits have
garnered significant attention due to their exceptional expressivity and
trainability. However, their ability to generate accurate predictions on unseen
data, referred to as the predictive performance, remains insufficiently
investigated. This study reveals a fundamental limitation in predictive
performance when deep encoding layers are employed within the data re-uploading
model. Concretely, we theoretically demonstrate that when processing
high-dimensional data with limited-qubit data re-uploading models, their
predictive performance progressively degenerates to near random-guessing levels
as the number of encoding layers increases. In this context, the repeated data
uploading cannot mitigate the performance degradation. These findings are
validated through experiments on both synthetic linearly separable datasets and
real-world datasets. Our results demonstrate that when processing
high-dimensional data, the quantum data re-uploading models should be designed
with wider circuit architectures rather than deeper and narrower ones.

</details>


### [234] [Quantum AIXI: Universal Intelligence via Quantum Information](https://arxiv.org/abs/2505.21170)
*Elija Perrier*

Main category: quant-ph

TL;DR: The paper explores the possibility of creating a quantum version of AIXI, called QAIXI, which could potentially be a more efficient model for universal intelligence due to the quantum nature of the universe.


<details>
  <summary>Details</summary>
Motivation: AIXI is a classical model of artificial general intelligence (AGI), but given that the universe operates on quantum mechanical principles and simulating quantum systems classically incurs exponential overhead, there is motivation to explore quantum versions of AIXI.

Method: The authors extend the AIXI framework to incorporate quantum information concepts. They introduce a model for quantum agent/environment interaction using quantum and classical registers and channels, allowing quantum AIXI agents to perform both classical and quantum actions. Key components of AIXI are reformulated in quantum terms, building on prior work on quantum Kolmogorov complexity and introducing a QAIXI value function.

Result: The paper formulates Quantum AIXI (QAIXI) and discusses its conditions and limitations. It highlights how contextuality impacts QAIXI models and extends the understanding of quantum Solomonoff induction.

Conclusion: Quantum AIXI represents a theoretically consistent and potentially practical approach to modeling universal intelligence within quantum mechanical frameworks, suggesting advantages over classical AIXI.

Abstract: AIXI is a widely studied model of artificial general intelligence (AGI) based
upon principles of induction and reinforcement learning. However, AIXI is
fundamentally classical in nature - as are the environments in which it is
modelled. Given the universe is quantum mechanical in nature and the
exponential overhead required to simulate quantum mechanical systems
classically, the question arises as to whether there are quantum mechanical
analogues of AIXI which are theoretically consistent or practically feasible as
models of universal intelligence. To address this question, we extend the
framework to quantum information and present Quantum AIXI (QAIXI). We introduce
a model of quantum agent/environment interaction based upon quantum and
classical registers and channels, showing how quantum AIXI agents may take both
classical and quantum actions. We formulate the key components of AIXI in
quantum information terms, extending previous research on quantum Kolmogorov
complexity and a QAIXI value function. We discuss conditions and limitations
upon quantum Solomonoff induction and show how contextuality fundamentally
affects QAIXI models.

</details>


### [235] [Leveraging Diffusion Models for Parameterized Quantum Circuit Generation](https://arxiv.org/abs/2505.20863)
*Daniel Barta,Darya Martyniuk,Johannes Jung,Adrian Paschke*

Main category: quant-ph

TL;DR: The paper explores using denoising diffusion models for synthesizing parameterized quantum circuits, showing effectiveness in generating GHZ states and QML classification tasks.


<details>
  <summary>Details</summary>
Motivation: Quantum computing's success relies on advancements in quantum circuit design. There is a need for methods that can efficiently generate complex quantum circuits with optimized parameters.

Method: A generative approach based on denoising diffusion models is introduced to synthesize parameterized quantum circuits. The model builds upon previous work by Fürutter et al., allowing simultaneous generation of circuit architectures and their continuous gate parameters.

Result: The approach successfully synthesizes PQCs optimized for high-fidelity GHZ state generation and accurate QML classification tasks. It shows strong generalization across different gate sets and qubit counts.

Conclusion: Generative models, particularly diffusion-based methods, have potential as powerful tools for accelerating and optimizing the design of parameterized quantum circuits, aiding in the development of practical and scalable quantum applications.

Abstract: Quantum computing holds immense potential, yet its practical success depends
on multiple factors, including advances in quantum circuit design. In this
paper, we introduce a generative approach based on denoising diffusion models
(DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent
diffusion model pipeline of F\"urrutter et al. [1], our model effectively
conditions the synthesis process, enabling the simultaneous generation of
circuit architectures and their continuous gate parameters. We demonstrate our
approach in synthesizing PQCs optimized for generating high-fidelity
Greenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum
machine learning (QML) classification tasks. Our results indicate a strong
generalization across varying gate sets and scaling qubit counts, highlighting
the versatility and computational efficiency of diffusion-based methods. This
work illustrates the potential of generative models as a powerful tool for
accelerating and optimizing the design of PQCs, supporting the development of
more practical and scalable quantum applications.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [236] [Scattering Networks on Noncommutative Finite Groups](https://arxiv.org/abs/2505.20950)
*Maria Teresa Arias,Davide Barbieri,Eugenio Hernández*

Main category: math.NA

TL;DR: The paper extends scattering networks to arbitrary finite groups for use in group-equivariant convolutional neural networks (G-CNNs), demonstrating its desirable properties and applications.


<details>
  <summary>Details</summary>
Motivation: To elucidate the behavior of early layers in G-CNNs by extending scattering networks from Euclidean spaces to arbitrary finite groups, utilizing wavelets on these groups.

Method: Introduces a scattering transform on arbitrary finite groups within the context of G-CNNs. Analyzes wavelets on finite groups and their similarity to classical wavelets, showing that under certain conditions in the wavelet coefficients, the scattering transform has several desirable properties.

Result: The scattering transform is non-expansive, stable under deformations, preserves energy, equivariant with respect to left and right group translations, and becomes less sensitive to group translations as depth increases. Demonstrates application of the transform in classifying data involving abelian and nonabelian groups.

Conclusion: Scattering transforms on arbitrary finite groups provide a valuable tool for understanding G-CNNs, with proven beneficial properties and successful classification examples.

Abstract: Scattering Networks were initially designed to elucidate the behavior of
early layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and
are grounded in wavelets. In this work, we introduce a scattering transform on
an arbitrary finite group (not necessarily abelian) within the context of
group-equivariant convolutional neural networks (G-CNNs). We present wavelets
on finite groups and analyze their similarity to classical wavelets. We
demonstrate that, under certain conditions in the wavelet coefficients, the
scattering transform is non-expansive, stable under deformations, preserves
energy, equivariant with respect to left and right group translations, and, as
depth increases, the scattering coefficients are less sensitive to group
translations of the signal, all desirable properties of convolutional neural
networks. Furthermore, we provide examples illustrating the application of the
scattering transform to classify data with domains involving abelian and
nonabelian groups.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [237] [Stopping Criteria for Value Iteration on Concurrent Stochastic Reachability and Safety Games](https://arxiv.org/abs/2505.21087)
*Marta Grobelna,Jan Křetínský,Maximilian Weininger*

Main category: cs.LO

TL;DR: This paper explores two-player zero-sum concurrent stochastic games (CSGs) with reachability and safety objectives, focusing on improving value iteration by providing bounded (interval) VI for CSGs to ensure precision in approximations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of guarantees in the precision of approximations when using traditional value iteration (VI) for solving concurrent stochastic games (CSGs).

Method: The method involves introducing bounded (interval) VI for CSGs. This approach complements standard VI with a converging sequence of over-approximations and terminates once the over- and under-approximations are $\epsilon$-close.

Result: The result is a more precise approximation method for solving CSGs, ensuring that the over- and under-approximations are within a specified $\epsilon$ distance from each other.

Conclusion: Bounded (interval) VI provides a reliable alternative to traditional VI for solving CSGs, offering guarantees on the precision of the approximations.

Abstract: We consider two-player zero-sum concurrent stochastic games (CSGs) played on
graphs with reachability and safety objectives. These include degenerate
classes such as Markov decision processes or turn-based stochastic games, which
can be solved by linear or quadratic programming; however, in practice, value
iteration (VI) outperforms the other approaches and is the most implemented
method. Similarly, for CSGs, this practical performance makes VI an attractive
alternative to the standard theoretical solution via the existential theory of
reals.
  VI starts with an under-approximation of the sought values for each state and
iteratively updates them, traditionally terminating once two consecutive
approximations are $\epsilon$-close. However, this stopping criterion lacks
guarantees on the precision of the approximation, which is the goal of this
work. We provide bounded (a.k.a. interval) VI for CSGs: it complements standard
VI with a converging sequence of over-approximations and terminates once the
over- and under-approximations are $\epsilon$-close.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [238] [MetamatBench: Integrating Heterogeneous Data, Computational Tools, and Visual Interface for Metamaterial Discovery](https://arxiv.org/abs/2505.20299)
*Jianpeng Chen,Wangzhi Zhan,Haohui Wang,Zian Jia,Jingru Gan,Junkai Zhang,Jingyuan Qi,Tingwei Chen,Lifu Huang,Muhao Chen,Ling Li,Wei Wang,Dawei Zhou*

Main category: physics.optics

TL;DR: This paper introduces MetamatBench, a unified framework for metamaterial discovery that addresses data heterogeneity, model complexity, and human-AI collaboration challenges through standardized datasets, adapted ML methods, and a visual-interactive interface.


<details>
  <summary>Details</summary>
Motivation: Metamaterials offer tunable mechanical properties surpassing conventional materials, but leveraging advanced machine learning for their discovery is hindered by three fundamental challenges: data heterogeneity, model complexity, and human-AI collaboration.

Method: The framework operates on three levels: (1) Data level - integrates and standardizes 5 heterogeneous, multi-modal metamaterial datasets; (2) ML level - provides a comprehensive toolkit adapting 17 state-of-the-art ML methods for metamaterial discovery and includes a comprehensive evaluation suite with 12 novel performance metrics; (3) User level - features a visual-interactive interface bridging the gap between complex ML techniques and non-ML researchers.

Result: MetamatBench offers a unified platform enabling machine learning researchers and practitioners to develop and evaluate new methodologies in metamaterial discovery. It ensures accurate and reliable model validation through finite element-based assessments.

Conclusion: MetamatBench is deployed at http://zhoulab-1.cs.vt.edu:5550 and its benchmark and codebase are open-sourced at https://github.com/cjpcool/Metamaterial-Benchmark for accessibility and reproducibility.

Abstract: Metamaterials, engineered materials with architected structures across
multiple length scales, offer unprecedented and tunable mechanical properties
that surpass those of conventional materials. However, leveraging advanced
machine learning (ML) for metamaterial discovery is hindered by three
fundamental challenges: (C1) Data Heterogeneity Challenge arises from
heterogeneous data sources, heterogeneous composition scales, and heterogeneous
structure categories; (C2) Model Complexity Challenge stems from the intricate
geometric constraints of ML models, which complicate their adaptation to
metamaterial structures; and (C3) Human-AI Collaboration Challenge comes from
the "dual black-box'' nature of sophisticated ML models and the need for
intuitive user interfaces. To tackle these challenges, we introduce a unified
framework, named MetamatBench, that operates on three levels. (1) At the data
level, we integrate and standardize 5 heterogeneous, multi-modal metamaterial
datasets. (2) The ML level provides a comprehensive toolkit that adapts 17
state-of-the-art ML methods for metamaterial discovery. It also includes a
comprehensive evaluation suite with 12 novel performance metrics with finite
element-based assessments to ensure accurate and reliable model validation. (3)
The user level features a visual-interactive interface that bridges the gap
between complex ML techniques and non-ML researchers, advancing property
prediction and inverse design of metamaterials for research and applications.
MetamatBench offers a unified platform deployed at
http://zhoulab-1.cs.vt.edu:5550 that enables machine learning researchers and
practitioners to develop and evaluate new methodologies in metamaterial
discovery. For accessibility and reproducibility, we open-source our benchmark
and the codebase at https://github.com/cjpcool/Metamaterial-Benchmark.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [239] [Large Language Model-Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph](https://arxiv.org/abs/2505.20308)
*Muhammad Tayyab Khan,Lequn Chen,Wenhe Feng,Seung Ki Moon*

Main category: cs.IR

TL;DR: The paper presents a novel knowledge graph (KG) in Neo4j for metal additive manufacturing (AM), encoding various materials, processes, and post-processing requirements. It integrates a large language model (LLM) interface to enable natural language querying without formal syntax, supporting tasks like compatibility checks and design guidance.


<details>
  <summary>Details</summary>
Motivation: To address the fragmentation of domain knowledge in metal AM across literature and static databases that require expert-level queries, limiting their use in design and planning.

Method: Developed a queryable knowledge graph in Neo4j that includes metals, alloys, AM processes, feedstock types, and post-processing requirements. Integrated a LLM interface using a few-shot prompting strategy for natural language querying, translating user queries into Cypher for execution over the KG.

Result: The system supports tasks such as compatibility checks, multi-constraint filtering, and design for AM (DfAM) guidance, providing structured responses from natural language queries in real-time.

Conclusion: This work introduces the first interactive system integrating a metal AM-specific KG with an LLM interface, offering accessible and explainable decision support for engineers, advancing human-centric tools in manufacturing intelligence.

Abstract: Metal additive manufacturing (AM) involves complex interdependencies among
processes, materials, feedstock, and post-processing steps. However, the
underlying relationships and domain knowledge remain fragmented across
literature and static databases that often demand expert-level queries,
limiting their applicability in design and planning. To address these gaps, we
develop a novel and queryable knowledge graph (KG) in Neo4j, encoding 53
distinct metals and alloys across seven material families, nine AM processes,
four feedstock types, and associated post-processing requirements. A large
language model (LLM) interface, guided by a few-shot prompting strategy,
enables natural language querying without the need for formal query syntax. The
system supports a range of tasks, including compatibility checks,
multi-constraint filtering, and design for AM (DfAM) guidance. User natural
language queries are normalized, translated into Cypher, and executed over the
KG, with results reformatted into structured responses. This work presents the
first real-time, interactive system that integrates a domain-specific metal AM
KG with an LLM interface, offering accessible, explainable decision support for
engineers and advancing human-centric tools in manufacturing intelligence.

</details>


### [240] [VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration](https://arxiv.org/abs/2505.20362)
*Jiahui Geng,Qing Li,Zongxiong Chen,Yuxia Wang,Derui Zhu,Zhuohan Xie,Chenyang Lyu,Xiuying Chen,Preslav Nakov,Fakhri Karray*

Main category: cs.IR

TL;DR: The paper introduces 'safety calibration' for vision-language models (VLMs), tackling both undersafety and oversafety issues. It presents VSCBench, a dataset of 3,600 image-text pairs, to evaluate safety calibration across various scenarios. Through extensive experiments on eleven VLMs, the study reveals significant issues in both undersafety and oversafety. While some methods improve safety calibration, they degrade model utility, highlighting the need for advanced calibration techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus mainly on undersafety, where VLMs respond to hazardous queries, but neglect oversafety, where safe queries are refused. This imbalance calls for a systematic approach to address both issues simultaneously.

Method: The authors introduce the concept of safety calibration and develop VSCBench, a dataset with 3,600 image-text pairs that are visually or textually similar but differ in safety levels. This dataset evaluates safety calibration in both image-centric and text-centric contexts. They assess eleven widely-used VLMs using this benchmark and investigate four approaches to enhance safety calibration.

Result: The experiments reveal substantial problems in both undersafety and oversafety across the evaluated VLMs. Some methods successfully calibrate safety issues but lead to a decline in model utility, indicating a trade-off between safety and functionality.

Conclusion: There is an urgent need for advanced calibration techniques that can balance safety and utility. The introduced VSCBench provides a valuable resource for evaluating future approaches to safety calibration.

Abstract: The rapid advancement of vision-language models (VLMs) has brought a lot of
attention to their safety alignment. However, existing methods have primarily
focused on model undersafety, where the model responds to hazardous queries,
while neglecting oversafety, where the model refuses to answer safe queries. In
this paper, we introduce the concept of $\textit{safety calibration}$, which
systematically addresses both undersafety and oversafety. Specifically, we
present $\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are
visually or textually similar but differ in terms of safety, which is designed
to evaluate safety calibration across image-centric and text-centric scenarios.
Based on our benchmark, we evaluate safety calibration across eleven widely
used VLMs. Our extensive experiments revealed major issues with both
undersafety and oversafety. We further investigated four approaches to improve
the model's safety calibration. We found that even though some methods
effectively calibrated the models' safety problems, these methods also lead to
the degradation of models' utility. This trade-off underscores the urgent need
for advanced calibration methods, and our benchmark provides a valuable tool
for evaluating future approaches. Our code and data are available at
https://github.com/jiahuigeng/VSCBench.git.

</details>


### [241] [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
*Jaeyoung Choe,Jihoon Kim,Woohwan Jung*

Main category: cs.IR

TL;DR: In the field of finance, RAG-based LLMs perform well in knowledge-intensive tasks, but they struggle with near-duplicate text in standardized documents. The proposed HiREC framework addresses this issue through hierarchical retrieval and evidence curation to improve accuracy and completeness. It also generates complementary queries when needed. A new benchmark LOFin with 145,897 SEC documents and 1,595 question-answer pairs is introduced for evaluation.


<details>
  <summary>Details</summary>
Motivation: Standardized financial documents such as SEC filings have repetitive boilerplate texts and similar table structures. Traditional RAG methods fail to distinguish between near-duplicate texts, leading to duplicate retrieval that undermines accuracy and completeness.

Method: The Hierarchical Retrieval with Evidence Curation (HiREC) framework was developed. This method performs hierarchical retrieval by first retrieving related documents and then selecting the most relevant passages. The evidence curation process eliminates irrelevant passages and generates complementary queries when necessary to collect missing information.

Result: The HiREC framework effectively reduces confusion among similar texts and improves the accuracy and completeness of retrieved information. The LOFin benchmark demonstrates the effectiveness of the approach with a large dataset of SEC documents and question-answer pairs.

Conclusion: The HiREC framework provides an effective solution to the problem of near-duplicate text retrieval in standardized financial documents. With the release of the LOFin benchmark, future research can build upon this work.

Abstract: Retrieval-augmented generation (RAG) based large language models (LLMs) are
widely used in finance for their excellent performance on knowledge-intensive
tasks. However, standardized documents (e.g., SEC filing) share similar formats
such as repetitive boilerplate texts, and similar table structures. This
similarity forces traditional RAG methods to misidentify near-duplicate text,
leading to duplicate retrieval that undermines accuracy and completeness. To
address these issues, we propose the Hierarchical Retrieval with Evidence
Curation (HiREC) framework. Our approach first performs hierarchical retrieval
to reduce confusion among similar texts. It first retrieve related documents
and then selects the most relevant passages from the documents. The evidence
curation process removes irrelevant passages. When necessary, it automatically
generates complementary queries to collect missing information. To evaluate our
approach, we construct and release a Large-scale Open-domain Financial (LOFin)
question answering benchmark that includes 145,897 SEC documents and 1,595
question-answer pairs. Our code and data are available at
https://github.com/deep-over/LOFin-bench-HiREC.

</details>


### [242] [TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research](https://arxiv.org/abs/2505.20663)
*Xu Kang,Siqi Jiang,Kangwei Xu,Jiahao Li,Ruibo Wu*

Main category: cs.IR

TL;DR: 开发了一个名为TeroSeek的策展知识库，结合了二十年的萜类化合物文献和AI驱动的问答聊天机器人，为跨学科研究提供专业工具。


<details>
  <summary>Details</summary>
Motivation: 萜类化合物作为一类重要的天然产物，其跨学科特性（涉及化学、药理学和生物学）使得知识整合变得复杂。

Method: 构建了一个名为TeroSeek的知识库，从二十年的萜类化合物文献中提取信息，并结合基于检索增强生成（RAG）框架的AI聊天机器人和网络服务。

Result: TeroSeek提供了结构化、高质量的信息，在萜类化合物相关查询中优于通用大型语言模型（LLMs）。

Conclusion: TeroSeek作为一个特定领域的专家工具，支持多学科研究，并且公开可用。

Abstract: Terpenoids are a crucial class of natural products that have been studied for
over 150 years, but their interdisciplinary nature (spanning chemistry,
pharmacology, and biology) complicates knowledge integration. To address this,
the authors developed TeroSeek, a curated knowledge base (KB) built from two
decades of terpenoid literature, coupled with an AI-powered question-answering
chatbot and web service. Leveraging a retrieval-augmented generation (RAG)
framework, TeroSeek provides structured, high-quality information and
outperforms general-purpose large language models (LLMs) in terpenoid-related
queries. It serves as a domain-specific expert tool for multidisciplinary
research and is publicly available at http://teroseek.qmclab.com.

</details>


### [243] [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
*Shahrooz Pouryousef*

Main category: cs.IR

TL;DR: 尽管大型语言模型（LLMs）在推荐系统中有所应用，但它们对用户-项目交互数据的利用效果不如经典的矩阵分解（MF）模型。本文引入了一种基于检索增强生成（RAG）的方法，该方法通过将LLMs的预测基于结构化的交互数据来提升其性能。实验结果表明，RAG方法显著提高了LLMs的推荐质量，为未来基于LLMs的推荐系统提供了一个有希望的方向。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）是否能有效地利用用户-项目交互数据中的协作信息，并与经典矩阵分解（MF）模型进行系统比较，以评估LLMs在此类任务上的能力。

Method: 采用系统比较的方法，将LLMs与经典矩阵分解模型进行对比，并引入一种简单的检索增强生成（RAG）方法，通过结合结构化交互数据来增强LLMs的预测能力。

Result: 实验显示，当前的LLMs在捕捉MF模型所固有的协作模式方面存在不足，而基于RAG的方法则显著提升了推荐的质量。

Conclusion: LLMs单独在处理用户-项目交互数据时表现不佳，但通过RAG方法可以大幅改善推荐效果，显示出基于LLMs的推荐系统未来的潜力。

Abstract: User-item interactions contain rich collaborative signals that form the
backbone of many successful recommender systems. While recent work has explored
the use of large language models (LLMs) for recommendation, it remains unclear
whether LLMs can effectively reason over this type of collaborative
information. In this paper, we conduct a systematic comparison between LLMs and
classical matrix factorization (MF) models to assess LLMs' ability to leverage
user-item interaction data. We further introduce a simple retrieval-augmented
generation (RAG) method that enhances LLMs by grounding their predictions in
structured interaction data. Our experiments reveal that current LLMs often
fall short in capturing collaborative patterns inherent to MF models, but that
our RAG-based approach substantially improves recommendation
quality-highlighting a promising direction for future LLM-based recommenders.

</details>


### [244] [Bridging the Gap: Self-Optimized Fine-Tuning for LLM-based Recommender Systems](https://arxiv.org/abs/2505.20771)
*Heng Tang,Feng Liu,Xinbo Chen,Jiawei Chen,Bohao Wang,Changwang Zhang,Jun Wang,Yuegang Sun,Bingde Hu,Can Wang*

Main category: cs.IR

TL;DR: This paper proposes a new method called Self-Optimized Fine-Tuning (SOFT) that combines Guidance-Only and Tuning-Only strategies to improve LLMs' recommendation capabilities. The method uses self-distillation and a self-adaptive curriculum scheduler to gradually train LLMs from simpler data to more challenging data, resulting in a significant increase in recommendation accuracy.


<details>
  <summary>Details</summary>
Motivation: Current strategies for enabling LLMs to have recommendation capabilities, namely the Guidance-Only and Tuning-Only strategies, do not effectively bridge the gap between the knowledge space of LLMs and recommendation data.

Method: The SOFT method first employs self-distillation to construct an auxiliary easy-to-learn dataset from a fine-tuned LLM, then uses a self-adaptive curriculum scheduler to allow LLMs to learn progressively from simpler data to more complex real RS data.

Result: Experiments show that SOFT significantly improves the recommendation accuracy of LLM-based methods by 37.59% on average.

Conclusion: The SOFT method successfully leverages the strengths of both Guidance-Only and Tuning-Only strategies to enhance LLMs' recommendation capabilities.

Abstract: Recent years have witnessed extensive exploration of Large Language Models
(LLMs) on the field of Recommender Systems (RS). There are currently two
commonly used strategies to enable LLMs to have recommendation capabilities: 1)
The "Guidance-Only" strategy uses in-context learning to exploit and amplify
the inherent semantic understanding and item recommendation capabilities of
LLMs; 2) The "Tuning-Only" strategy uses supervised fine-tuning (SFT) to
fine-tune LLMs with the aim of fitting them to real recommendation data.
However, neither of these strategies can effectively bridge the gap between the
knowledge space of LLMs and recommendation, and their performance do not meet
our expectations.
  To better enable LLMs to learn recommendation knowledge, we combine the
advantages of the above two strategies and proposed a novel "Guidance+Tuning"
method called Self-Optimized Fine-Tuning (SOFT), which adopts the idea of
curriculum learning. It first employs self-distillation to construct an
auxiliary easy-to-learn but meaningful dataset from a fine-tuned LLM. Then it
further utilizes a self-adaptive curriculum scheduler to enable LLMs to
gradually learn from simpler data (self-distilled data) to more challenging
data (real RS data). Extensive experiments demonstrate that SOFT significantly
enhances the recommendation accuracy (37.59\% on average) of LLM-based methods.
The code is available via
https://anonymous.4open.science/r/Self-Optimized-Fine-Tuning-264E

</details>


### [245] [Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks](https://arxiv.org/abs/2505.21329)
*Allaa Boutaleb,Bernd Amann,Hubert Naacke,Rafael Angarita*

Main category: cs.IR

TL;DR: Current table union search benchmarks have limitations allowing simple baselines to perform well, overshadowing sophisticated methods. This paper proposes criteria for improved benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve the evaluation of semantic understanding in table union search by addressing flaws in current benchmarks that allow simple baselines to outperform complex methods due to dataset-specific characteristics.

Method: Propose essential criteria for future benchmarks to ensure more realistic and reliable evaluation of progress in semantic table union search.

Result: The proposed criteria aim to isolate gains from semantic understanding rather than dataset-specific features, leading to a better assessment of table union search methods.

Conclusion: Adopting the proposed benchmark criteria will lead to more accurate evaluations and advancements in semantic table union search.

Abstract: Recent table representation learning and data discovery methods tackle table
union search (TUS) within data lakes, which involves identifying tables that
can be unioned with a given query table to enrich its content. These methods
are commonly evaluated using benchmarks that aim to assess semantic
understanding in real-world TUS tasks. However, our analysis of prominent TUS
benchmarks reveals several limitations that allow simple baselines to perform
surprisingly well, often outperforming more sophisticated approaches. This
suggests that current benchmark scores are heavily influenced by
dataset-specific characteristics and fail to effectively isolate the gains from
semantic understanding. To address this, we propose essential criteria for
future benchmarks to enable a more realistic and reliable evaluation of
progress in semantic table union search.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [246] [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)
*Saharsh Barve,Andy Mao,Jiayue Melissa Shi,Prerna Juneja,Koustuv Saha*

Main category: cs.HC

TL;DR: 最近的生成式AI进展推动了文本到图像（T2I）生成技术的发展，但其输出往往复制并放大社会刻板印象。本文提出了一种理论驱动的偏见检测框架和社交刻板指数（SSI），以系统评估T2I输出中的社会偏见。通过审计三个主要T2I模型，并采用提示词优化方法，显著降低了输出中的偏见。然而，用户研究揭示了去偏见与上下文对齐之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管T2I模型具有创造性潜力，但它们往往会复制和放大与性别、种族和文化相关的社会刻板印象，引发伦理问题。因此，需要一种系统的方法来评估和减少这些模型中的偏见。

Method: 1. 提出了一种理论驱动的偏见检测框架和社交刻板指数（SSI）。2. 使用100个查询审计了三个主要T2I模型（DALL-E-3、Midjourney-6.1 和 Stability AI Core）的输出。3. 通过LLMs进行有针对性的提示词优化以减少偏见。4. 进行用户研究以了解人们对AI生成的带偏见图像的看法、意识和偏好。

Result: 经过提示词优化后，SSI显著降低：地理文化类别下降61%，职业类别下降69%，形容词类别下降51%。用户研究发现，虽然提示词优化可以减轻刻板印象，但可能限制上下文对齐。有趣的是，用户常常认为刻板印象的图像是更符合他们期望的。

Conclusion: 需要在道德上去偏见和上下文相关性之间取得平衡。呼吁开发支持全球多样性和包容性的T2I系统，同时不损害对现实世界社会复杂性的反映。

Abstract: Recent advances in generative AI have enabled visual content creation through
text-to-image (T2I) generation. However, despite their creative potential, T2I
models often replicate and amplify societal stereotypes -- particularly those
related to gender, race, and culture -- raising important ethical concerns.
This paper proposes a theory-driven bias detection rubric and a Social
Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs.
We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and
Stability AI Core -- using 100 queries across three categories -- geocultural,
occupational, and adjectival. Our analysis reveals that initial outputs are
prone to include stereotypical visual cues, including gendered professions,
cultural markers, and western beauty norms. To address this, we adopted our
rubric to conduct targeted prompt refinement using LLMs, which significantly
reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and
51% for adjectival queries. We complemented our quantitative analysis through a
user study examining perceptions, awareness, and preferences around
AI-generated biased imagery. Our findings reveal a key tension -- although
prompt refinement can mitigate stereotypes, it can limit contextual alignment.
Interestingly, users often perceived stereotypical images to be more aligned
with their expectations. We discuss the need to balance ethical debiasing with
contextual relevance and call for T2I systems that support global diversity and
inclusivity while not compromising the reflection of real-world social
complexity.

</details>


### [247] [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)
*Yi-Cheng Lin,Kang-Chieh Chen,Zhe-Yan Li,Tzu-Heng Wu,Tzu-Hsuan Wu,Kuan-Yu Chen,Hung-yi Lee,Yun-Nung Chen*

Main category: cs.HC

TL;DR: This paper surveys the role of creativity in multi-agent systems (MAS) driven by large language models (LLMs), focusing on text and image generation tasks, providing a taxonomy for agent proactivity and persona design, an overview of generation techniques, and discussing key challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the overlooked dimension of creativity in MAS, including novel output generation and evaluation, its influence on agent personas, and coordination of creative workflows.

Method: The method involves presenting a taxonomy of agent proactivity and persona design, providing an overview of generation techniques such as divergent exploration, iterative refinement, and collaborative synthesis, and discussing relevant datasets and evaluation metrics.

Result: The result is a comprehensive survey that highlights key challenges in the field such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and lack of unified benchmarks.

Conclusion: This survey provides a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.

Abstract: Large language model (LLM)-driven multi-agent systems (MAS) are transforming
how humans and AIs collaboratively generate ideas and artifacts. While existing
surveys provide comprehensive overviews of MAS infrastructures, they largely
overlook the dimension of \emph{creativity}, including how novel outputs are
generated and evaluated, how creativity informs agent personas, and how
creative workflows are coordinated. This is the first survey dedicated to
creativity in MAS. We focus on text and image generation tasks, and present:
(1) a taxonomy of agent proactivity and persona design; (2) an overview of
generation techniques, including divergent exploration, iterative refinement,
and collaborative synthesis, as well as relevant datasets and evaluation
metrics; and (3) a discussion of key challenges, such as inconsistent
evaluation standards, insufficient bias mitigation, coordination conflicts, and
the lack of unified benchmarks. This survey offers a structured framework and
roadmap for advancing the development, evaluation, and standardization of
creative MAS.

</details>


### [248] [Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset](https://arxiv.org/abs/2505.20788)
*Robin Burchard,Kristof Van Laerhoven*

Main category: cs.HC

TL;DR: The paper addresses the issue of incorporating acoustic data in wearable human activity recognition while considering privacy concerns. It introduces a new label 'tap water' to the HD-Epic dataset, analyzes its relation with the existing 'water' label, and evaluates two lightweight classifiers for this new class.


<details>
  <summary>Details</summary>
Motivation: To enhance wearable human activity recognition by utilizing acoustic data without infringing on user privacy, focusing on a specific task like hand washing detection through water flow recognition.

Method: 1. Created a new label 'tap water' in the HD-Epic dataset.
2. Generated 717 hand-labeled annotations for tap water flow based on existing annotations.
3. Analyzed the relationship between 'tap water' and 'water' labels.
4. Trained and evaluated two lightweight classifiers for the newly added label class.

Result: The analysis showed that the new 'tap water' class can be learned more easily compared to the previous 'water' class, demonstrating the potential of this new label for improving specific tasks in human activity recognition.

Conclusion: Incorporating a new 'tap water' label into the dataset enhances the ability to recognize water flow related activities, which could aid in tasks such as hand washing detection. This approach respects user privacy by processing data locally on wearable devices.

Abstract: Wearable human activity recognition has been shown to benefit from the
inclusion of acoustic data, as the sounds around a person often contain
valuable context. However, due to privacy concerns, it is usually not ethically
feasible to record and save microphone data from the device, since the audio
could, for instance, also contain private conversations. Rather, the data
should be processed locally, which in turn requires processing power and
consumes energy on the wearable device. One special use case of contextual
information that can be utilized to augment special tasks in human activity
recognition is water flow detection, which can, e.g., be used to aid wearable
hand washing detection. We created a new label called tap water for the
recently released HD-Epic data set, creating 717 hand-labeled annotations of
tap water flow, based on existing annotations of the water class. We analyzed
the relation of tap water and water in the dataset and additionally trained and
evaluated two lightweight classifiers to evaluate the newly added label class,
showing that the new class can be learned more easily.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [249] [Identifying Super Spreaders in Multilayer Networks](https://arxiv.org/abs/2505.20980)
*Michał Czuba,Mateusz Stolarski,Adam Piróg,Piotr Bielak,Piotr Bródka*

Main category: cs.SI

TL;DR: The paper presents TopSpreadersNetwork, a novel graph neural network model designed to identify super-spreaders in multilayer networks. It uses a four-dimensional vector for ranking agents' spreading potential and outperforms centrality-based heuristics and other deep learning methods.


<details>
  <summary>Details</summary>
Motivation: To improve the identification of super-spreaders in complex multilayer networks by leveraging graph neural networks and creating a specialized dataset for this task.

Method: Constructing a dataset simulating information diffusion in multilayer networks and developing TopSpreadersNetwork, which includes a relationship-agnostic encoder and a custom aggregation layer to rank agents based on their spreading potential using a four-dimensional vector.

Result: TopSpreadersNetwork achieves superior performance in identifying high-impact nodes across real-world and synthetic multilayer networks compared to classic centrality-based heuristics and competitive deep learning methods.

Conclusion: TopSpreadersNetwork effectively identifies super-spreaders in multilayer networks with improved interpretability through its structured output.

Abstract: Identifying super-spreaders can be framed as a subtask of the influence
maximisation problem. It seeks to pinpoint agents within a network that, if
selected as single diffusion seeds, disseminate information most effectively.
Multilayer networks, a specific class of heterogeneous graphs, can capture
diverse types of interactions (e.g., physical-virtual or professional-social),
and thus offer a more accurate representation of complex relational structures.
In this work, we introduce a novel approach to identifying super-spreaders in
such networks by leveraging graph neural networks. To this end, we construct a
dataset by simulating information diffusion across hundreds of networks - to
the best of our knowledge, the first of its kind tailored specifically to
multilayer networks. We further formulate the task as a variation of the
ranking prediction problem based on a four-dimensional vector that quantifies
each agent's spreading potential: (i) the number of activations; (ii) the
duration of the diffusion process; (iii) the peak number of activations; and
(iv) the simulation step at which this peak occurs. Our model,
TopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom
aggregation layer. This design enables generalisation to previously unseen data
and adapts to varying graph sizes. In an extensive evaluation, we compare our
model against classic centrality-based heuristics and competitive deep learning
methods. The results, obtained across a broad spectrum of real-world and
synthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves
superior performance in identifying high-impact nodes, while also offering
improved interpretability through its structured output.

</details>


### [250] [DeSocial: Blockchain-based Decentralized Social Networks](https://arxiv.org/abs/2505.21388)
*Jingyuan Huang,Xi Zhu,Minghao Guo,Yongfeng Zhang*

Main category: cs.SI

TL;DR: Web 2.0平台是中心化的，用户只能被动接收社交预测。区块链的出现让用户可以自主选择算法，提高个性化预测效果。我们提出了DeSocial框架，允许用户在本地子图上评估多个模型，选择最适合的骨干模型进行社交预测，并通过多节点验证和多数投票防止错误。实验表明，DeSocial比五种经典中心化模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前Web 2.0社交平台是中心化的，用户无法选择算法，限制了个性化。区块链技术的出现提供了去中心化和个性化算法选择的可能性。

Method: 提出DeSocial框架，部署在Ethereum本地开发链上，结合分布式数据存储、节点级共识和用户驱动的模型选择。用户在本地子图上评估多个模型，选择最合适的骨干模型。然后，DeSocial选择几个验证节点执行指定算法，并通过多数投票聚合结果。

Result: 大量实验表明，与五种经典中心化社交网络学习模型相比，DeSocial具有明显的优势，突出了多节点验证和基于区块链的个性化算法选择的重要性。

Conclusion: DeSocial框架展示了在区块链基础上去中心化社交网络学习的潜力，增强了用户的自主权和个性化体验。

Abstract: Web 2.0 social platforms are inherently centralized, with user data and
algorithmic decisions controlled by the platform. However, users can only
passively receive social predictions without being able to choose the
underlying algorithm, which limits personalization. Fortunately, with the
emergence of blockchain, users are allowed to choose algorithms that are
tailored to their local situation, improving prediction results in a
personalized way. In a blockchain environment, each user possesses its own
model to perform the social prediction, capturing different perspectives on
social interactions. In our work, we propose DeSocial, a decentralized social
network learning framework deployed on an Ethereum (ETH) local development
chain that integrates distributed data storage, node-level consensus, and
user-driven model selection through Ganache. In the first stage, each user
leverages DeSocial to evaluate multiple backbone models on their local
subgraph. DeSocial coordinates the execution and returns model-wise prediction
results, enabling the user to select the most suitable backbone for
personalized social prediction. Then, DeSocial uniformly selects several
validation nodes that possess the algorithm specified by each user, and
aggregates the prediction results by majority voting, to prevent errors caused
by any single model's misjudgment. Extensive experiments show that DeSocial has
an evident improvement compared to the five classical centralized social
network learning models, promoting user empowerment in blockchain-based
decentralized social networks, showing the importance of multi-node validation
and personalized algorithm selection based on blockchain. Our implementation is
available at: https://github.com/agiresearch/DeSocial.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [251] [Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants](https://arxiv.org/abs/2505.21304)
*Louis Jalouzot,Alexis Thual,Yair Lakretz,Christophe Pallier,Bertrand Thirion*

Main category: q-bio.NC

TL;DR: The study explores optimal strategies for decoding perceived natural speech from fMRI data, using deep neural networks to predict LLM-derived text representations. It finds that multi-subject training doesn't improve accuracy compared to single-subject approach, and decoders model syntactic features better than semantic ones.


<details>
  <summary>Details</summary>
Motivation: To investigate the best methods for decoding perceived natural speech from fMRI data with a limited number of participants.

Method: Using Lebel et al.'s dataset of 8 participants, deep neural networks are trained to predict LLM-derived text representations from fMRI activity. Multi-subject and single-subject training approaches are compared.

Result: Multi-subject training does not improve decoding accuracy compared to single-subject approach. Decoders model syntactic features better than semantic ones, with complex syntax or rich semantic content being more challenging to decode.

Conclusion: The results highlight the importance of extensive data per participant for effective decoding and suggest that leveraging multi-subject data likely requires deeper phenotyping or a larger cohort.

Abstract: We investigate optimal strategies for decoding perceived natural speech from
fMRI data acquired from a limited number of participants. Leveraging Lebel et
al. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness
of training deep neural networks to predict LLM-derived text representations
from fMRI activity. Then, in this data regime, we observe that multi-subject
training does not improve decoding accuracy compared to single-subject
approach. Furthermore, training on similar or different stimuli across subjects
has a negligible effect on decoding accuracy. Finally, we find that our
decoders better model syntactic than semantic features, and that stories
containing sentences with complex syntax or rich semantic content are more
challenging to decode. While our results demonstrate the benefits of having
extensive data per participant (deep phenotyping), they suggest that leveraging
multi-subject for natural speech decoding likely requires deeper phenotyping or
a substantially larger cohort.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [252] [MVTN: Learning Multi-View Transformations for 3D Understanding](https://arxiv.org/abs/2212.13462)
*Abdullah Hamdi,Faisal AlZahrani,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: The paper proposes MVTN, which learns optimal viewpoints for 3D shape recognition using differentiable rendering and can be trained end-to-end with any multi-view network. It integrates into an adaptive pipeline for meshes and point clouds, showing state-of-the-art performance in classification and retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Current multi-view techniques for 3D shape recognition use fixed camera viewpoints for all shapes, which limits their adaptability.

Method: MVTN uses differentiable rendering to learn optimal viewpoints for 3D shape recognition and is integrated into a novel adaptive multi-view pipeline capable of rendering both 3D meshes and point clouds.

Result: MVTN demonstrates state-of-the-art performance in 3D classification and shape retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55) and shows improved robustness to occlusion compared to other methods.

Conclusion: MVTN improves the adaptability of multi-view techniques by learning optimal viewpoints for 3D shape recognition, leading to state-of-the-art performance in various tasks. The authors also released MVTorch, a PyTorch library for further research.

Abstract: Multi-view projection techniques have shown themselves to be highly effective
in achieving top-performing results in the recognition of 3D shapes. These
methods involve learning how to combine information from multiple view-points.
However, the camera view-points from which these views are obtained are often
fixed for all shapes. To overcome the static nature of current multi-view
techniques, we propose learning these view-points. Specifically, we introduce
the Multi-View Transformation Network (MVTN), which uses differentiable
rendering to determine optimal view-points for 3D shape recognition. As a
result, MVTN can be trained end-to-end with any multi-view network for 3D shape
classification. We integrate MVTN into a novel adaptive multi-view pipeline
that is capable of rendering both 3D meshes and point clouds. Our approach
demonstrates state-of-the-art performance in 3D classification and shape
retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55).
Further analysis indicates that our approach exhibits improved robustness to
occlusion compared to other methods. We also investigate additional aspects of
MVTN, such as 2D pretraining and its use for segmentation. To support further
research in this area, we have released MVTorch, a PyTorch library for 3D
understanding and generation using multi-view projections.

</details>


### [253] [SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence](https://arxiv.org/abs/2505.12703)
*Jiabin Chen,Haiping Wang,Jinpeng Li,Yuan Liu,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: This paper introduces SpatialLLM, a unified language model capable of performing various spatial intelligence tasks in complex urban scenes without training or fine-tuning. It constructs detailed scene descriptions from raw spatial data to prompt pre-trained LLMs for analysis.


<details>
  <summary>Details</summary>
Motivation: To develop a method that can perform spatial intelligence tasks in complex urban scenes without the need for geographic analysis tools or domain expertise.

Method: SpatialLLM constructs detailed and structured scene descriptions from raw spatial data to prompt pre-trained LLMs for scene-based analysis.

Result: Extensive experiments demonstrate that pretrained LLMs can accurately perceive spatial distribution information and enable zero-shot execution of advanced spatial intelligence tasks, such as urban planning, ecological analysis, and traffic management.

Conclusion: The authors argue that multi-field knowledge, context length, and reasoning ability are crucial factors affecting LLM performances in urban analysis, and hope that SpatialLLM will offer a new perspective for urban intelligent analysis and management.

Abstract: We propose SpatialLLM, a novel approach advancing spatial intelligence tasks
in complex urban scenes. Unlike previous methods requiring geographic analysis
tools or domain expertise, SpatialLLM is a unified language model directly
addressing various spatial intelligence tasks without any training,
fine-tuning, or expert intervention. The core of SpatialLLM lies in
constructing detailed and structured scene descriptions from raw spatial data
to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show
that, with our designs, pretrained LLMs can accurately perceive spatial
distribution information and enable zero-shot execution of advanced spatial
intelligence tasks, including urban planning, ecological analysis, traffic
management, etc. We argue that multi-field knowledge, context length, and
reasoning ability are key factors influencing LLM performances in urban
analysis. We hope that SpatialLLM will provide a novel viable perspective for
urban intelligent analysis and management. The code and dataset are available
at https://github.com/WHU-USI3DV/SpatialLLM.

</details>


### [254] [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/abs/2505.20405)
*Lorenzo Baraldi,Davide Bucciarelli,Federico Betti,Marcella Cornia,Lorenzo Baraldi,Nicu Sebe,Rita Cucchiara*

Main category: cs.CV

TL;DR: An instruction-based image editing evaluation model named DICE is introduced, which effectively identifies coherent edits and correlates strongly with human judgment.


<details>
  <summary>Details</summary>
Motivation: Instruction-based image editing models provide more personalization opportunities in generative tasks, but evaluating their results remains a challenge due to the lack of alignment with human judgment and explainability in existing metrics.

Method: DICE consists of a difference detector and a coherence estimator, both built on an autoregressive Multimodal Large Language Model (MLLM) and trained using a strategy that combines self-supervision, distillation from inpainting networks, and full supervision.

Result: Through extensive experiments, DICE effectively identifies coherent edits and evaluates images generated by different editing models with a strong correlation to human judgment.

Conclusion: The study concludes by publicly releasing the source code, models, and data, promoting further research and development in the field of image editing evaluation.

Abstract: Instruction-based image editing models offer increased personalization
opportunities in generative tasks. However, properly evaluating their results
is challenging, and most of the existing metrics lag in terms of alignment with
human judgment and explainability. To tackle these issues, we introduce DICE
(DIfference Coherence Estimator), a model designed to detect localized
differences between the original and the edited image and to assess their
relevance to the given modification request. DICE consists of two key
components: a difference detector and a coherence estimator, both built on an
autoregressive Multimodal Large Language Model (MLLM) and trained using a
strategy that leverages self-supervision, distillation from inpainting
networks, and full supervision. Through extensive experiments, we evaluate each
stage of our pipeline, comparing different MLLMs within the proposed framework.
We demonstrate that DICE effectively identifies coherent edits, effectively
evaluating images generated by different editing models with a strong
correlation with human judgment. We publicly release our source code, models,
and data.

</details>


### [255] [RetroMotion: Retrocausal Motion Forecasting Models are Instructable](https://arxiv.org/abs/2505.20414)
*Royden Wagner,Omer Sahin Tas,Felix Hauser,Marlon Steiner,Dominik Strutz,Abhishek Vivekanandan,Carlos Fernandez,Christoph Stiller*

Main category: cs.CV

TL;DR: A multi-task learning method for motion forecasting with retrocausal information flow is proposed, achieving SOTA results in Waymo dataset and good generalization in Argoverse 2 dataset.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of motion forecasts of road users as a function of scene constraints and interactive behavior.

Method: Using a transformer model, generate joint distributions by re-encoding marginal distributions followed by pairwise modeling. Incorporate retrocausal flow of information from later points in marginal trajectories to earlier points in joint trajectories. Model positional uncertainty using compressed exponential power distributions.

Result: Achieves state-of-the-art results in the Waymo Interaction Prediction dataset and generalizes well to the Argoverse 2 dataset. Provides an interface for issuing instructions through trajectory modifications.

Conclusion: The proposed multi-task learning method for motion forecasting effectively incorporates retrocausal information flow, leading to superior performance and adaptability.

Abstract: Motion forecasts of road users (i.e., agents) vary in complexity as a
function of scene constraints and interactive behavior. We address this with a
multi-task learning method for motion forecasting that includes a retrocausal
flow of information. The corresponding tasks are to forecast (1) marginal
trajectory distributions for all modeled agents and (2) joint trajectory
distributions for interacting agents. Using a transformer model, we generate
the joint distributions by re-encoding marginal distributions followed by
pairwise modeling. This incorporates a retrocausal flow of information from
later points in marginal trajectories to earlier points in joint trajectories.
Per trajectory point, we model positional uncertainty using compressed
exponential power distributions. Notably, our method achieves state-of-the-art
results in the Waymo Interaction Prediction dataset and generalizes well to the
Argoverse 2 dataset. Additionally, our method provides an interface for issuing
instructions through trajectory modifications. Our experiments show that
regular training of motion forecasting leads to the ability to follow
goal-based instructions and to adapt basic directional instructions to the
scene context. Code: https://github.com/kit-mrt/future-motion

</details>


### [256] [CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting](https://arxiv.org/abs/2505.20469)
*Lei Tian,Xiaomin Li,Liqian Ma,Hefei Huang,Zirui Zheng,Hao Yin,Taiqing Li,Huchuan Lu,Xu Jia*

Main category: cs.CV

TL;DR: CCL-LGS is a novel framework that enforces view-consistent semantic supervision by integrating multi-view semantic cues, which outperforms previous state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current 3D semantic understanding methods relying on 2D priors face challenges of cross-view semantic inconsistencies caused by occlusion, image blur, and view-dependent variations. These issues deteriorate the quality of 3D Gaussian semantic fields and introduce artifacts in rendered outputs.

Method: The CCL-LGS framework first uses a zero-shot tracker to align SAM-generated 2D masks and identify their categories. Then, CLIP extracts robust semantic encodings across views. Finally, the Contrastive Codebook Learning (CCL) module distills discriminative semantic features by enforcing intra-class compactness and inter-class distinctiveness.

Result: Extensive experiments show that CCL-LGS surpasses previous state-of-the-art methods in terms of performance.

Conclusion: CCL-LGS successfully mitigates cross-view semantic inconsistencies and improves the quality of 3D semantic understanding.

Abstract: Recent advances in 3D reconstruction techniques and vision-language models
have fueled significant progress in 3D semantic understanding, a capability
critical to robotics, autonomous driving, and virtual/augmented reality.
However, methods that rely on 2D priors are prone to a critical challenge:
cross-view semantic inconsistencies induced by occlusion, image blur, and
view-dependent variations. These inconsistencies, when propagated via
projection supervision, deteriorate the quality of 3D Gaussian semantic fields
and introduce artifacts in the rendered outputs. To mitigate this limitation,
we propose CCL-LGS, a novel framework that enforces view-consistent semantic
supervision by integrating multi-view semantic cues. Specifically, our approach
first employs a zero-shot tracker to align a set of SAM-generated 2D masks and
reliably identify their corresponding categories. Next, we utilize CLIP to
extract robust semantic encodings across views. Finally, our Contrastive
Codebook Learning (CCL) module distills discriminative semantic features by
enforcing intra-class compactness and inter-class distinctiveness. In contrast
to previous methods that directly apply CLIP to imperfect masks, our framework
explicitly resolves semantic conflicts while preserving category
discriminability. Extensive experiments demonstrate that CCL-LGS outperforms
previous state-of-the-art methods. Our project page is available at
https://epsilontl.github.io/CCL-LGS/.

</details>


### [257] [WeatherEdit: Controllable Weather Editing with 4D Gaussian Field](https://arxiv.org/abs/2505.20471)
*Chenghao Qian,Wenjing Li,Yuhu Guo,Gustav Markkula*

Main category: cs.CV

TL;DR: WeatherEdit is a novel pipeline for creating realistic weather effects in 3D scenes with controllable types and severity, composed of weather background editing and weather particle construction.


<details>
  <summary>Details</summary>
Motivation: To generate realistic weather effects with controllable types and severity in 3D scenes, which has potential applications in autonomous driving simulation.

Method: The approach includes two key components: weather background editing that uses an all-in-one adapter integrating multiple weather styles into a pretrained diffusion model and a Temporal-View (TV-) attention mechanism for consistent editing across multi-frame and multi-view images; weather particle construction that employs a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog with physical-based modelling and simulation for precise control over attributes and dynamics.

Result: Experiments on multiple driving datasets demonstrate the ability of WeatherEdit to generate diverse weather effects with controllable condition severity.

Conclusion: WeatherEdit shows potential for autonomous driving simulation in adverse weather by generating realistic weather effects.

Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for
generating realistic weather effects with controllable types and severity in 3D
scenes. Our approach is structured into two key components: weather background
editing and weather particle construction. For weather background editing, we
introduce an all-in-one adapter that integrates multiple weather styles into a
single pretrained diffusion model, enabling the generation of diverse weather
effects in 2D image backgrounds. During inference, we design a Temporal-View
(TV-) attention mechanism that follows a specific order to aggregate temporal
and spatial information, ensuring consistent editing across multi-frame and
multi-view images. To construct the weather particles, we first reconstruct a
3D scene using the edited images and then introduce a dynamic 4D Gaussian field
to generate snowflakes, raindrops and fog in the scene. The attributes and
dynamics of these particles are precisely controlled through physical-based
modelling and simulation, ensuring realistic weather representation and
flexible severity adjustments. Finally, we integrate the 4D Gaussian field with
the 3D scene to render consistent and highly realistic weather effects.
Experiments on multiple driving datasets demonstrate that WeatherEdit can
generate diverse weather effects with controllable condition severity,
highlighting its potential for autonomous driving simulation in adverse
weather. See project page: https://jumponthemoon.github.io/w-edit

</details>


### [258] [Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset](https://arxiv.org/abs/2505.20507)
*Elias Arbash,Ahmed Jamal Afifi,Ymane Belahsen,Margret Fuchs,Pedram Ghamisi,Paul Scheunders,Richard Gloaguen*

Main category: cs.CV

TL;DR: 本文介绍了Electrolyzers-HSI，一个新型多模态基准数据集，旨在加速关键原材料的回收，通过精确的电解材料分类。数据集包括55个共同注册的高分辨率RGB图像和HSI数据立方体，并进行了基础机器学习方法和最新深度学习架构的评估。


<details>
  <summary>Details</summary>
Motivation: 可持续回收的全球挑战需要自动化、快速和准确的材料检测系统，这些系统是循环经济的基础。为了扩大回收工作并促进绿色协议，有必要普及对这些尖端解决方案的访问，以实现实时废物分析。

Method: 引入了Electrolyzers-HSI数据集，包含高分辨率RGB图像和HSI数据立方体，覆盖400-2500 nm光谱范围。使用基础机器学习方法和先进的深度学习架构（如Vision Transformer、SpectralFormer和Multimodal Fusion Transformer）进行评估，并实施零样本检测技术和多数投票策略以提高分类鲁棒性。

Result: 该数据集支持非侵入式光谱分析，有助于材料分类和光谱特性研究。评估结果揭示了架构瓶颈，为优化变压器在材料识别中的效率提供了依据。

Conclusion: Electrolyzers-HSI数据集及其代码库公开可用，遵循FAIR数据原则，支持可重复研究，并促进了智能和可持续电子废物回收解决方案的广泛应用。

Abstract: The global challenge of sustainable recycling demands automated, fast, and
accurate, state-of-the-art (SOTA) material detection systems that act as a
bedrock for a circular economy. Democratizing access to these cutting-edge
solutions that enable real-time waste analysis is essential for scaling up
recycling efforts and fostering the Green Deal. In response, we introduce
\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to
accelerate the recovery of critical raw materials through accurate electrolyzer
materials classification. The dataset comprises 55 co-registered
high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning
the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and
424,169 labeled ones. This enables non-invasive spectral analysis of shredded
electrolyzer samples, supporting quantitative and qualitative material
classification and spectral properties investigation. We evaluate a suite of
baseline machine learning (ML) methods alongside SOTA transformer-based deep
learning (DL) architectures, including Vision Transformer, SpectralFormer, and
the Multimodal Fusion Transformer, to investigate architectural bottlenecks for
further efficiency optimisation when deploying transformers in material
identification. We implement zero-shot detection techniques and majority voting
across pixel-level predictions to establish object-level classification
robustness. In adherence to the FAIR data principles, the electrolyzers-HSI
dataset and accompanying codebase are openly available at
https://github.com/hifexplo/Electrolyzers-HSI and
https://rodare.hzdr.de/record/3668, supporting reproducible research and
facilitating the broader adoption of smart and sustainable e-waste recycling
solutions.

</details>


### [259] [Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.20569)
*Jihoon Lee,Min Song*

Main category: cs.CV

TL;DR: 尽管大型视觉-语言模型取得了显著进展，但对象幻觉（OH）仍然是一个持续存在的挑战。基于先前关于对比解码的研究，我们引入了RVCD（检索视觉对比解码），一种高级方法来抑制OH。RVCD在logit级别利用正负图像，明确引用设计用于表示单个概念的AI生成图像。我们的方法相比现有的基于解码的方法展示了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 对象幻觉（OH）是大型视觉-语言模型中的一个持续挑战，需要一种无需额外模型训练的方法来解决此问题。

Method: 引入RVCD（检索视觉对比解码），该方法在logit级别利用正负图像，并明确引用设计用于表示单个概念的AI生成图像。

Result: 相比现有的基于解码的方法展示了显著的改进。

Conclusion: RVCD是一种有效的高级方法，可以抑制对象幻觉，改善视觉-语言模型的表现。

Abstract: Despite significant advancements in Large Vision-Language Models, Object
Hallucination (OH) remains a persistent challenge. Building upon prior studies
on contrastive decoding that address this issue without requiring additional
model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an
advanced method to suppress OH. RVCD leverages both negative and positive
images at the logit level, explicitly referencing AI-generated images designed
to represent a single concept. Our approach demonstrates substantial
improvements over existing decoding-based methods.

</details>


### [260] [TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone](https://arxiv.org/abs/2505.20637)
*Ana M. Cabanas,Alma Pedro,Domingo Mery*

Main category: cs.CV

TL;DR: The study explores two skin tone classification methods (ITA and $H^*$-$L^*$) to assess fairness in facial affect analysis systems, revealing underrepresentation of dark skin tones and disparities in performance metrics across groups. It proposes a fairness-aware pipeline for future mitigation efforts.


<details>
  <summary>Details</summary>
Motivation: To reliably measure sensitive attributes like ancestry in facial affect analysis systems and understand how these systems perform across different demographic groups, especially considering the influence of lighting conditions on skin tone approximation.

Method: Comparison of ITA and $H^*$-$L^*$ skin tone classification methods using AffectNet and a MobileNet-based model to evaluate fairness across skin tone groups. Incorporation of Grad-CAM analysis for model attention patterns and proposal of a fairness-aware pipeline.

Result: Severe underrepresentation of dark skin tones ($\sim 2\%$), fairness disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. $H^*$-$L^*$ method provides more consistent subgrouping and clearer diagnostics than ITA.

Conclusion: Skin tone measurement choices significantly impact fairness assessment in facial affect analysis systems; ITA-based evaluations may overlook disparities affecting darker-skinned individuals.

Abstract: Understanding how facial affect analysis (FAA) systems perform across
different demographic groups requires reliable measurement of sensitive
attributes such as ancestry, often approximated by skin tone, which itself is
highly influenced by lighting conditions. This study compares two objective
skin tone classification methods: the widely used Individual Typology Angle
(ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and
Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness
across skin tone groups defined by each method. Results reveal a severe
underrepresentation of dark skin tones ($\sim 2 \%$), alongside fairness
disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While
ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$
method yields more consistent subgrouping and enables clearer diagnostics
through metrics such as Equal Opportunity. Grad-CAM analysis further highlights
differences in model attention patterns by skin tone, suggesting variation in
feature encoding. To support future mitigation efforts, we also propose a
modular fairness-aware pipeline that integrates perceptual skin tone
estimation, model interpretability, and fairness evaluation. These findings
emphasize the relevance of skin tone measurement choices in fairness assessment
and suggest that ITA-based evaluations may overlook disparities affecting
darker-skinned individuals.

</details>


### [261] [HCQA-1.5 @ Ego4D EgoSchema Challenge 2025](https://arxiv.org/abs/2505.20644)
*Haoyu Zhang,Yisen Feng,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: This paper introduces an extension to the HCQA framework with multi-source aggregation, confidence-based filtering, and fine-grained reasoning for egocentric video question answering. It achieved 77% accuracy on Ego4D EgoSchema Challenge.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability of answer prediction in egocentric video question answering.

Method: An effective extension to the HCQA framework that includes a multi-source aggregation strategy, a confidence-based filtering mechanism, and a fine-grained reasoning module.

Result: Achieved 77% accuracy on over 5,000 human-curated multiple-choice questions in the EgoSchema blind test set, outperforming last year's winning solution and most participating teams.

Conclusion: The proposed method significantly improves the performance of egocentric video question answering.

Abstract: In this report, we present the method that achieves third place for Ego4D
EgoSchema Challenge in CVPR 2025. To improve the reliability of answer
prediction in egocentric video question answering, we propose an effective
extension to the previously proposed HCQA framework. Our approach introduces a
multi-source aggregation strategy to generate diverse predictions, followed by
a confidence-based filtering mechanism that selects high-confidence answers
directly. For low-confidence cases, we incorporate a fine-grained reasoning
module that performs additional visual and contextual analysis to refine the
predictions. Evaluated on the EgoSchema blind test set, our method achieves 77%
accuracy on over 5,000 human-curated multiple-choice questions, outperforming
last year's winning solution and the majority of participating teams. Our code
will be added at https://github.com/Hyu-Zhang/HCQA.

</details>


### [262] [RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment](https://arxiv.org/abs/2505.20653)
*Lingyu Qiu,Ke Jiang,Xiaoyang Tan*

Main category: cs.CV

TL;DR: This paper proposes a new learning objective for deepfake detection that aligns generalization gradient updates with ERM gradient updates, enhancing model robustness to domain shifts without additional regularization. Experiments show it outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Previous methods for domain generalization in deepfake detection often incorporate additional modules to prevent overfitting to domain-specific patterns, but such regularization can hinder the optimization of the empirical risk minimization (ERM) objective, degrading model performance.

Method: The proposed method applies perturbations to model parameters to align ascending points across domains, preserving domain-invariant features while managing domain-specific characteristics, without introducing additional regularization.

Result: Experimental results on multiple challenging deepfake detection datasets demonstrate that the proposed gradient alignment strategy outperforms state-of-the-art domain generalization techniques.

Conclusion: The proposed learning objective effectively enhances the robustness of deepfake detection models to domain shifts, confirming the efficacy of the method.

Abstract: Recent advancements in domain generalization for deepfake detection have
attracted significant attention, with previous methods often incorporating
additional modules to prevent overfitting to domain-specific patterns. However,
such regularization can hinder the optimization of the empirical risk
minimization (ERM) objective, ultimately degrading model performance. In this
paper, we propose a novel learning objective that aligns generalization
gradient updates with ERM gradient updates. The key innovation is the
application of perturbations to model parameters, aligning the ascending points
across domains, which specifically enhances the robustness of deepfake
detection models to domain shifts. This approach effectively preserves
domain-invariant features while managing domain-specific characteristics,
without introducing additional regularization. Experimental results on multiple
challenging deepfake detection datasets demonstrate that our gradient alignment
strategy outperforms state-of-the-art domain generalization techniques,
confirming the efficacy of our method. The code is available at
https://github.com/Lynn0925/RoGA.

</details>


### [263] [VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models](https://arxiv.org/abs/2505.20718)
*Kui Wu,Shuhang Xu,Hao Chen,Churan Wang,Zhoujun Li,Yizhou Wang,Fangwei Zhong*

Main category: cs.CV

TL;DR: A new framework combines EVT with VLMs to improve tracking failure recovery via a memory-augmented self-reflection mechanism, boosting success rates significantly.


<details>
  <summary>Details</summary>
Motivation: Current active visual tracking systems struggle with recovering from tracking failures, especially in dynamic and unstructured environments.

Method: Integrate off-the-shelf active tracking methods with VLM reasoning capabilities. Use a fast visual policy for normal tracking and activate VLM reasoning only when a failure is detected. Implement a memory-augmented self-reflection mechanism to allow the VLM to learn from past experiences and progressively improve.

Result: The framework improves success rates by 72% compared to state-of-the-art RL-based approaches and by 220% compared to PID-based methods in challenging environments.

Conclusion: This work marks the first use of VLM-based reasoning to assist EVT agents in proactive failure recovery, providing significant advancements for real-world robotic applications requiring continuous target monitoring.

Abstract: We introduce a novel self-improving framework that enhances Embodied Visual
Tracking (EVT) with Visual-Language Models (VLMs) to address the limitations of
current active visual tracking systems in recovering from tracking failure. Our
approach combines the off-the-shelf active tracking methods with VLMs'
reasoning capabilities, deploying a fast visual policy for normal tracking and
activating VLM reasoning only upon failure detection. The framework features a
memory-augmented self-reflection mechanism that enables the VLM to
progressively improve by learning from past experiences, effectively addressing
VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate
significant performance improvements, with our framework boosting success rates
by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based
methods in challenging environments. This work represents the first integration
of VLM-based reasoning to assist EVT agents in proactive failure recovery,
offering substantial advances for real-world robotic applications that require
continuous target monitoring in dynamic, unstructured environments. Project
website: https://sites.google.com/view/evt-recovery-assistant.

</details>


### [264] [Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models](https://arxiv.org/abs/2505.20753)
*Yufei Zhan,Hongyin Zhao,Yousong Zhu,Shurong Zheng,Fan Yang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: Large Multimodal Models (LMMs) struggle with compositional reasoning, so the authors developed a unified visual reasoning mechanism that enhances their capabilities. They also created Griffon-R, which excels in complex visual reasoning and multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: To improve LMMs' ability to integrate advanced, task-specific capabilities for compositional reasoning and progress towards becoming truly competent general vision models.

Method: A unified visual reasoning mechanism was introduced that allows LMMs to solve complicated compositional problems using their intrinsic capabilities. This includes an understanding-thinking-answering process done in a single pass without multiple inferences or external tools. Additionally, 334K visual instruction samples were curated to train the model Griffon-R.

Result: Griffon-R shows advancing performance on complex visual reasoning benchmarks such as VSR and CLEVR, and also enhances multimodal capabilities across various benchmarks like MMBench and ScienceQA.

Conclusion: The developed unified visual reasoning mechanism successfully bridges the gap between foundational visual capabilities and general question answering, enabling LMMs to generate faithful and traceable responses for complex visual reasoning.

Abstract: Large Multimodal Models (LMMs) have recently demonstrated remarkable visual
understanding performance on both vision-language and vision-centric tasks.
However, they often fall short in integrating advanced, task-specific
capabilities for compositional reasoning, which hinders their progress toward
truly competent general vision models. To address this, we present a unified
visual reasoning mechanism that enables LMMs to solve complicated compositional
problems by leveraging their intrinsic capabilities (e.g. grounding and visual
understanding capabilities). Different from the previous shortcut learning
mechanism, our approach introduces a human-like
understanding-thinking-answering process, allowing the model to complete all
steps in a single pass forwarding without the need for multiple inferences or
external tools. This design bridges the gap between foundational visual
capabilities and general question answering, encouraging LMMs to generate
faithful and traceable responses for complex visual reasoning. Meanwhile, we
curate 334K visual instruction samples covering both general scenes and
text-rich scenes and involving multiple foundational visual capabilities. Our
trained model, Griffon-R, has the ability of end-to-end automatic
understanding, self-thinking, and reasoning answers. Comprehensive experiments
show that Griffon-R not only achieves advancing performance on complex visual
reasoning benchmarks including VSR and CLEVR, but also enhances multimodal
capabilities across various benchmarks like MMBench and ScienceQA. Data,
models, and codes will be release at
https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.

</details>


### [265] [PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding](https://arxiv.org/abs/2505.20759)
*Ansel Blume,Jeonghwan Kim,Hyeonjeong Ha,Elen Chatikyan,Xiaomeng Jin,Khanh Duy Nguyen,Nanyun Peng,Kai-Wei Chang,Derek Hoiem,Heng Ji*

Main category: cs.CV

TL;DR: 开发了一个名为PARTONOMY的基准，用于评估大型多模态模型在像素级部件定位任务上的表现。通过实验发现现有模型存在显著局限性，并提出了一种新的模型PLUM，该模型在多种任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界的物体由特定的、独特的部分组成，识别这些部分对于进行细粒度、组合推理至关重要。然而，现有的大型多模态模型在这方面表现不佳。

Method: 构建了PARTONOMY基准，包括862个部件标签和534个物体标签。使用专门的概念挑战模型识别部件、考虑整体-部分关系等。提出了PLUM模型，采用跨度标记代替分割标记，并利用先前预测进行条件化。

Result: 预训练的PLUM在推理分割、VQA和视觉幻觉基准上优于现有的分割LMM。在提出的解释性部件分割任务上微调的PLUM与使用更多分割数据训练的分割LMM具有竞争力。

Conclusion: 这项工作为在大型多模态模型中实现细粒度、基于地面的视觉理解开辟了新的途径。

Abstract: Real-world objects are composed of distinctive, object-specific parts.
Identifying these parts is key to performing fine-grained, compositional
reasoning-yet, large multimodal models (LMMs) struggle to perform this
seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM
benchmark designed for pixel-level part grounding. We construct PARTONOMY from
existing part datasets and our own rigorously annotated set of images,
encompassing 862 part labels and 534 object labels for evaluation. Unlike
existing datasets that simply ask models to identify generic parts, PARTONOMY
uses specialized concepts (e.g., agricultural airplane), and challenges models
to compare objects' parts, consider part-whole relationships, and justify
textual predictions with visual segmentations. Our experiments demonstrate
significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only
5.9% gIoU), highlighting a critical gap in their part grounding abilities. We
note that existing segmentation-enabled LMMs (segmenting LMMs) have two key
architectural shortcomings: they use special [SEG] tokens not seen during
pretraining which induce distribution shift, and they discard predicted
segmentations instead of using past predictions to guide future ones. To
address these deficiencies, we train several part-centric LMMs and propose
PLUM, a novel segmenting LMM that uses span tagging instead of segmentation
tokens and that conditions on prior predictions in a feedback loop. We find
that pretrained PLUM outperforms existing segmenting LMMs on reasoning
segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM
finetuned on our proposed Explanatory Part Segmentation task is competitive
with segmenting LMMs trained on significantly more segmentation data. Our work
opens up new avenues towards enabling fine-grained, grounded visual
understanding in LMMs.

</details>


### [266] [Rendering-Aware Reinforcement Learning for Vector Graphics Generation](https://arxiv.org/abs/2505.20793)
*Juan A. Rodriguez,Haotian Zhang,Abhay Puri,Aarash Feizi,Rishav Pramanik,Pascal Wichmann,Arnab Mondal,Mohammad Reza Samsami,Rabiul Awal,Perouz Taslakian,Spandana Gella,Sai Rajeswar,David Vazquez,Christopher Pal,Marco Pedersoli*

Main category: cs.CV

TL;DR: The paper presents RLRF, a reinforcement learning method that improves SVG generation by VLMs using feedback from rendered SVG outputs.


<details>
  <summary>Details</summary>
Motivation: Existing VLM approaches for SVG generation often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training.

Method: RLRF (Reinforcement Learning from Rendering Feedback) is introduced. It enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward.

Result: RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.

Conclusion: RLRF is an effective method to improve SVG generation by providing visual fidelity feedback which guides the model toward producing more accurate, efficient, and semantically coherent SVGs.

Abstract: Scalable Vector Graphics (SVG) offer a powerful format for representing
visual designs as interpretable code. Recent advances in vision-language models
(VLMs) have enabled high-quality SVG generation by framing the problem as a
code generation task and leveraging large-scale pretraining. VLMs are
particularly suitable for this task as they capture both global semantics and
fine-grained visual patterns, while transferring knowledge across vision,
natural language, and code domains. However, existing VLM approaches often
struggle to produce faithful and efficient SVGs because they never observe the
rendered images during training. Although differentiable rendering for
autoregressive SVG code generation remains unavailable, rendered outputs can
still be compared to original inputs, enabling evaluative feedback suitable for
reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from
Rendering Feedback), an RL method that enhances SVG generation in
autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an
input image, the model generates SVG roll-outs that are rendered and compared
to the original image to compute a reward. This visual fidelity feedback guides
the model toward producing more accurate, efficient, and semantically coherent
SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common
failure modes and enabling precise, high-quality SVG generation with strong
structural understanding and generalization.

</details>


### [267] [In Context Learning with Vision Transformers: Case Study](https://arxiv.org/abs/2505.20872)
*Antony Zhao,Alex Proshkin,Fergal Hennessy,Francesco Crivelli*

Main category: cs.CV

TL;DR: Large transformer models can perform in-context learning, including few-shot, one-shot, and zero-shot learning. This study aims to extend their capability from learning simple functions on random data to more complex functions in the image space, like convolutional neural networks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to explore whether large transformer models can perform in-context learning of more complex functions within the image domain, building on previous research that demonstrated their ability to learn simpler functions such as linear functions and small 2-layer neural networks on random data.

Method: The method involves extending the known capabilities of transformer models from learning general classes of simple functions (e.g., linear functions and small 2-layer neural networks) on random data to analyzing their potential for in-context learning of more complex functions specifically within the image space, such as convolutional neural networks.

Result: The results are not explicitly stated in the abstract provided, but the paper presumably details the extent to which these models can successfully perform in-context learning of complex functions related to image processing.

Conclusion: The conclusion will summarize the findings regarding the ability of large transformer models to in-context learn complex functions in the image space, potentially highlighting implications for tasks involving convolutional neural networks and other advanced methods.

Abstract: Large transformer models have been shown to be capable of performing
in-context learning. By using examples in a prompt as well as a query, they are
capable of performing tasks such as few-shot, one-shot, or zero-shot learning
to output the corresponding answer to this query. One area of interest to us is
that these transformer models have been shown to be capable of learning the
general class of certain functions, such as linear functions and small 2-layer
neural networks, on random data (Garg et al, 2023). We aim to extend this to
the image space to analyze their capability to in-context learn more complex
functions on the image space, such as convolutional neural networks and other
methods.

</details>


### [268] [Frequency Composition for Compressed and Domain-Adaptive Neural Networks](https://arxiv.org/abs/2505.20890)
*Yoojin Kwon,Hongjun Suh,Wooseok Lee,Taesik Gong,Songyi Han,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: Modern on-device neural network applications face challenges of resource constraints and unpredictable domain shifts. Previous works have addressed model compression and domain adaptation separately, but this paper proposes CoDA, a frequency composition-based framework that combines these two aspects. CoDA uses quantization-aware training with low-frequency components during training and refines the compact model using full-frequency information at test time. It can be integrated into existing methods and shows significant improvements in accuracy with high compression rates.


<details>
  <summary>Details</summary>
Motivation: Current solutions for model compression focus on efficiency within a fixed domain, while capable models concentrate on handling domain shifts. There is a need for a unified approach to address both challenges simultaneously.

Method: CoDA employs quantization-aware training (QAT) with low-frequency components during training to enable the compressed model to learn robust features. At test time, it performs source-free refinement (test-time adaptation, TTA), using full-frequency information from incoming data to adapt to target domains, treating high-frequency components as domain-specific cues.

Result: CoDA achieves accuracy improvements of 7.96 percentage points on CIFAR10-C and 5.37 percentage points on ImageNet-C over the full-precision TTA baseline, while maintaining significant model compression.

Conclusion: CoDA unifies model compression and domain adaptation through frequency composition, achieving better performance under resource constraints and domain shifts.

Abstract: Modern on-device neural network applications must operate under resource
constraints while adapting to unpredictable domain shifts. However, this
combined challenge-model compression and domain adaptation-remains largely
unaddressed, as prior work has tackled each issue in isolation: compressed
networks prioritize efficiency within a fixed domain, whereas large, capable
models focus on handling domain shifts. In this work, we propose CoDA, a
frequency composition-based framework that unifies compression and domain
adaptation. During training, CoDA employs quantization-aware training (QAT)
with low-frequency components, enabling a compressed model to selectively learn
robust, generalizable features. At test time, it refines the compact model in a
source-free manner (i.e., test-time adaptation, TTA), leveraging the
full-frequency information from incoming data to adapt to target domains while
treating high-frequency components as domain-specific cues. LFC are aligned
with the trained distribution, while HFC unique to the target distribution are
solely utilized for batch normalization. CoDA can be integrated synergistically
into existing QAT and TTA methods. CoDA is evaluated on widely used
domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various
model architectures. With significant compression, it achieves accuracy
improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the
full-precision TTA baseline.

</details>


### [269] [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
*Pingrui Zhang,Yifei Su,Pengyuan Wu,Dong An,Li Zhang,Zhigang Wang,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: In this paper, the authors tackle the Vision-and-Language Navigation (VLN) challenge by proposing a novel model called Adaptive Text Dreamer (ATD). Unlike previous methods that use vision-based synthesis to imagine future scenes, ATD leverages language models for adaptive imagining of key environmental semantics. It has a dual-branch architecture inspired by the human brain and introduces a cross-interaction mechanism to enhance navigation performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the limitations of existing VLN methods which rely on vision-based synthesis to imagine future scenes. These methods are computationally expensive and produce redundant details. The authors aim to develop a more efficient and reliable strategy by focusing on key environmental semantics in language form.

Method: The proposed method involves developing ATD, a dual-branch self-guided imagination policy built upon a large language model. This model mimics the human brain's left-right structure: the left branch focuses on logical integration while the right branch predicts future scenes. The Q-former within both branches is fine-tuned to activate domain-specific knowledge dynamically. A cross-interaction mechanism is also introduced to regularize the imagined outputs and integrate them into a navigation expert module.

Result: Extensive experiments conducted on the R2R benchmark demonstrate that ATD achieves state-of-the-art performance with fewer parameters compared to existing methods.

Conclusion: The conclusion is that ATD offers an effective solution for VLN tasks by efficiently leveraging language models and a unique dual-branch architecture. The model not only reduces computational costs but also enhances navigation accuracy.

Abstract: Vision-and-Language Navigation (VLN) requires the agent to navigate by
following natural instructions under partial observability, making it difficult
to align perception with language. Recent methods mitigate this by imagining
future scenes, yet they rely on vision-based synthesis, leading to high
computational cost and redundant details. To this end, we propose to adaptively
imagine key environmental semantics via \textit{language} form, enabling a more
reliable and efficient strategy. Specifically, we introduce a novel Adaptive
Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a
large language model (LLM). ATD is designed with a human-like left-right brain
architecture, where the left brain focuses on logical integration, and the
right brain is responsible for imaginative prediction of future scenes. To
achieve this, we fine-tune only the Q-former within both brains to efficiently
activate domain-specific knowledge in the LLM, enabling dynamic updates of
logical reasoning and imagination during navigation. Furthermore, we introduce
a cross-interaction mechanism to regularize the imagined outputs and inject
them into a navigation expert module, allowing ATD to jointly exploit both the
reasoning capacity of the LLM and the expertise of the navigation model. We
conduct extensive experiments on the R2R benchmark, where ATD achieves
state-of-the-art performance with fewer parameters. The code is
\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.

</details>


### [270] [ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image](https://arxiv.org/abs/2505.20498)
*Dongyu Luo,Kelin Yu,Amir-Hossein Shahidzadeh,Cornelia Fermüller,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: A two-stage controllable framework named ControlTac is proposed to generate realistic tactile images using physical priors as control input, providing effective data augmentation and consistent gains in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Vision-based tactile sensing has been widely used but collecting large-scale tactile data remains costly due to inconsistencies across sensor instances. Existing approaches for scaling tactile data suffer from unrealistic output and poor transferability.

Method: Propose ControlTac, a two-stage controllable framework that generates realistic tactile images conditioned on a single reference tactile image, contact force, and contact position.

Result: ControlTac can effectively augment tactile datasets and lead to consistent gains in three downstream tasks. Real-world experiments validate the practical utility of the approach.

Conclusion: ControlTac provides a way to generate physically plausible and varied tactile images for effective data augmentation, showing practical utility in real-world experiments.

Abstract: Vision-based tactile sensing has been widely used in perception,
reconstruction, and robotic manipulation. However, collecting large-scale
tactile data remains costly due to the localized nature of sensor-object
interactions and inconsistencies across sensor instances. Existing approaches
to scaling tactile data, such as simulation and free-form tactile generation,
often suffer from unrealistic output and poor transferability to downstream
tasks.To address this, we propose ControlTac, a two-stage controllable
framework that generates realistic tactile images conditioned on a single
reference tactile image, contact force, and contact position. With those
physical priors as control input, ControlTac generates physically plausible and
varied tactile images that can be used for effective data augmentation. Through
experiments on three downstream tasks, we demonstrate that ControlTac can
effectively augment tactile datasets and lead to consistent gains. Our three
real-world experiments further validate the practical utility of our approach.
Project page: https://dongyuluo.github.io/controltac.

</details>


### [271] [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
*Peter Robicheaux,Matvei Popov,Anish Madan,Isaac Robinson,Joseph Nelson,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: Vision-language models (VLMs) perform well on common objects but struggle with out-of-distribution classes. This paper introduces Roboflow100-VL, a benchmark of 100 multi-modal object detection datasets for evaluating VLMs in various settings. State-of-the-art VLMs show less than 2% zero-shot accuracy on challenging medical imaging datasets within this benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of state-of-the-art vision-language models (VLMs) in generalizing to out-of-distribution classes, tasks, and imaging modalities not typically found in their pre-training data.

Method: Introduced Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts. Evaluated state-of-the-art models on this benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings.

Result: State-of-the-art VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, indicating the need for few-shot concept alignment.

Conclusion: The paper concludes that there is a significant need for aligning VLMs to new concepts using annotation instructions containing a few visual examples and rich textual descriptions.

Abstract: Vision-language models (VLMs) trained on internet-scale data achieve
remarkable zero-shot detection performance on common objects like car, truck,
and pedestrian. However, state-of-the-art models still struggle to generalize
to out-of-distribution classes, tasks and imaging modalities not typically
found in their pre-training. Rather than simply re-training VLMs on more visual
data, we argue that one should align VLMs to new concepts with annotation
instructions containing a few visual examples and rich textual descriptions. To
this end, we introduce Roboflow100-VL, a large-scale collection of 100
multi-modal object detection datasets with diverse concepts not commonly found
in VLM pre-training. We evaluate state-of-the-art models on our benchmark in
zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing
for comparison across data regimes. Notably, we find that VLMs like
GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on
challenging medical imaging datasets within Roboflow100-VL, demonstrating the
need for few-shot concept alignment. Our code and dataset are available at
https://github.com/roboflow/rf100-vl/ and
https://universe.roboflow.com/rf100-vl/

</details>


### [272] [FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models](https://arxiv.org/abs/2505.21032)
*Nils Neukirch,Johanna Vielhaben,Nils Strodthoff*

Main category: cs.CV

TL;DR: The paper proposes a conditional diffusion model to map feature space to input space probabilistically for interpreting internal representations in deep neural networks, demonstrating its feasibility and applications.


<details>
  <summary>Details</summary>
Motivation: Interpreting internal representations of deep neural networks is challenging due to crude approximations in existing methods that map feature space to input space.

Method: A conditional diffusion model, pretrained with high fidelity, is used and conditioned on spatially resolved feature maps to learn the mapping from feature space to input space in a probabilistic manner.

Result: The method shows excellent reconstruction capabilities across various pretrained image classifiers from CNNs to ViTs, validated through qualitative comparisons and robustness analysis.

Conclusion: This approach has broad potential to improve understanding of feature spaces in computer vision models, with possible applications like visualizing concept steering in input space.

Abstract: Internal representations are crucial for understanding deep neural networks,
such as their properties and reasoning patterns, but remain difficult to
interpret. While mapping from feature space to input space aids in interpreting
the former, existing approaches often rely on crude approximations. We propose
using a conditional diffusion model - a pretrained high-fidelity diffusion
model conditioned on spatially resolved feature maps - to learn such a mapping
in a probabilistic manner. We demonstrate the feasibility of this approach
across various pretrained image classifiers from CNNs to ViTs, showing
excellent reconstruction capabilities. Through qualitative comparisons and
robustness analysis, we validate our method and showcase possible applications,
such as the visualization of concept steering in input space or investigations
of the composite nature of the feature space. This approach has broad potential
for improving feature space understanding in computer vision models.

</details>


### [273] [Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea](https://arxiv.org/abs/2505.20615)
*Omid Halimi Milani,Ahmet Enis Cetin,Bharati Prasad*

Main category: cs.CV

TL;DR: This paper presents a new deep learning method using DCT-based transfer learning to predict if OSA patients will develop hypertension within five years, achieving an AUC of 72.88% with EfficientNet.


<details>
  <summary>Details</summary>
Motivation: Predicting the development of hypertension in OSA patients within five years is complex and challenging due to factors like intermittent hypoxia and sleep fragmentation.

Method: The study integrates all polysomnography signals for hypertension prediction. Features are extracted from these signals and transformed into a 2D representation. Pre-trained 2D neural networks (MobileNet, EfficientNet, ResNet variants) are utilized. A DCT layer is introduced to transform input features into a frequency-based representation, preserving spectral information, decorrelating features, and enhancing noise robustness.

Result: The model achieved a best area under the curve (AUC) of 72.88% when the DCT layer was strategically placed at deeper truncation depths within EfficientNet.

Conclusion: Frequency-domain feature extraction combined with transfer learning is effective for predicting hypertension risk in OSA patients over a five-year period, especially beneficial for limited medical datasets.

Abstract: Obstructive sleep apnea (OSA) is a significant risk factor for hypertension,
primarily due to intermittent hypoxia and sleep fragmentation. Predicting
whether individuals with OSA will develop hypertension within five years
remains a complex challenge. This study introduces a novel deep learning
approach that integrates Discrete Cosine Transform (DCT)-based transfer
learning to enhance prediction accuracy. We are the first to incorporate all
polysomnography signals together for hypertension prediction, leveraging their
collective information to improve model performance. Features were extracted
from these signals and transformed into a 2D representation to utilize
pre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet
variants. To further improve feature learning, we introduced a DCT layer, which
transforms input features into a frequency-based representation, preserving
essential spectral information, decorrelating features, and enhancing
robustness to noise. This frequency-domain approach, coupled with transfer
learning, is especially beneficial for limited medical datasets, as it
leverages rich representations from pre-trained networks to improve
generalization. By strategically placing the DCT layer at deeper truncation
depths within EfficientNet, our model achieved a best area under the curve
(AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature
extraction and transfer learning in predicting hypertension risk in OSA
patients over a five-year period.

</details>


### [274] [RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy](https://arxiv.org/abs/2505.21036)
*Aiyue Chen,Bin Dong,Jingru Li,Jing Lin,Yiwu Yao,Gongyi Wang*

Main category: cs.CV

TL;DR: RainFusion是一种无需训练的稀疏注意力方法，通过识别视频生成中的三种独特稀疏模式（空间模式、时间模式和纹理模式），加速注意力计算并保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型中的3D注意力机制在视频生成任务中占据了超过80%的计算资源，因此需要一种能够减少计算量同时保持视频质量的方法。

Method: 提出了一种名为RainFusion的新型训练免费稀疏注意力方法，利用视觉数据中的固有稀疏性来加速注意力计算。具体来说，识别了视频生成注意力计算中的三种独特稀疏模式，并通过自适应识别模块（ARM）在线确定每个注意力头的稀疏模式，附加开销极小（~0.2%）。该方法可无缝集成到最先进的3D注意力视频生成模型中，无需额外训练或校准。

Result: 实验结果表明，RainFusion在保持视频质量的同时，将注意力计算速度提高了超过2倍，对VBench评分的影响极小（-0.2%）。

Conclusion: RainFusion作为一种即插即用的方法，可以显著加速视频生成模型中的注意力计算，具有广泛的应用前景和有效性。

Abstract: Video generation using diffusion models is highly computationally intensive,
with 3D attention in Diffusion Transformer (DiT) models accounting for over
80\% of the total computational resources. In this work, we introduce {\bf
RainFusion}, a novel training-free sparse attention method that exploits
inherent sparsity nature in visual data to accelerate attention computation
while preserving video quality. Specifically, we identify three unique sparse
patterns in video generation attention calculations--Spatial Pattern, Temporal
Pattern and Textural Pattern. The sparse pattern for each attention head is
determined online with negligible overhead (\textasciitilde\,0.2\%) with our
proposed {\bf ARM} (Adaptive Recognition Module) during inference. Our proposed
{\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated
into state-of-the-art 3D-attention video generation models without additional
training or calibration. We evaluate our method on leading open-sourced models
including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its
broad applicability and effectiveness. Experimental results show that
RainFusion achieves over {\bf 2\(\times\)} speedup in attention computation
while maintaining video quality, with only a minimal impact on VBench scores
(-0.2\%).

</details>


### [275] [Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training](https://arxiv.org/abs/2505.20629)
*Bolin Lai,Sangmin Lee,Xu Cao,Xiang Li,James M. Rehg*

Main category: cs.CV

TL;DR: The paper presents FlexTI2V, a training-free approach for text-image-to-video generation that conditions text-to-video models on any number of images at any positions, using a novel random patch swapping strategy and dynamic control mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adding visual conditions to text-to-video models through finetuning are resource-intensive and limited to predefined settings.

Method: FlexTI2V inverts condition images to a latent noisy representation, then uses a random patch swapping strategy during the denoising process of T2V models to incorporate visual features into video representations. A dynamic control mechanism adjusts visual conditioning strength per frame.

Result: Experiments show FlexTI2V surpasses previous training-free image conditioning methods significantly. Detailed ablation studies provide further insights.

Conclusion: FlexTI2V offers a flexible and effective solution for text-image-to-video generation with superior performance compared to prior training-free methods.

Abstract: Text-image-to-video (TI2V) generation is a critical problem for controllable
video generation using both semantic and visual conditions. Most existing
methods typically add visual conditions to text-to-video (T2V) foundation
models by finetuning, which is costly in resources and only limited to a few
predefined conditioning settings. To tackle this issue, we introduce a unified
formulation for TI2V generation with flexible visual conditioning. Furthermore,
we propose an innovative training-free approach, dubbed FlexTI2V, that can
condition T2V foundation models on an arbitrary amount of images at arbitrary
positions. Specifically, we firstly invert the condition images to noisy
representation in a latent space. Then, in the denoising process of T2V models,
our method uses a novel random patch swapping strategy to incorporate visual
features into video representations through local image patches. To balance
creativity and fidelity, we use a dynamic control mechanism to adjust the
strength of visual conditioning to each video frame. Extensive experiments
validate that our method surpasses previous training-free image conditioning
methods by a notable margin. We also show more insights of our method by
detailed ablation study and analysis.

</details>


### [276] [LPOI: Listwise Preference Optimization for Vision Language Models](https://arxiv.org/abs/2505.21061)
*Fatemeh Pesaran Zadeh,Yoojin Oh,Gunhee Kim*

Main category: cs.CV

TL;DR: The paper proposes LPOI, a method for reducing hallucinations in VLMs by using object-aware listwise preference optimization. It involves masking and interpolating critical objects in images to create ranked lists that the model learns to order, thereby improving performance without additional annotations.


<details>
  <summary>Details</summary>
Motivation: Current methods for aligning VLMs with human preferences either overfit textual information or worsen hallucinations, and no prior work has used listwise preference optimization due to complexity and cost.

Method: LPOI identifies and masks a critical object in an image, interpolates the masked region between positive and negative images, and forms a sequence of incrementally complete images. The model is trained to rank these images based on object visibility.

Result: Experiments on MMHalBench, AMBER, and Object HalBench show that LPOI outperforms existing methods in reducing hallucinations and enhancing VLM performance.

Conclusion: LPOI effectively reduces hallucinations while retaining visual fidelity in VLMs, requiring no extra annotations beyond standard pairwise preference data.

Abstract: Aligning large VLMs with human preferences is a challenging task, as methods
like RLHF and DPO often overfit to textual information or exacerbate
hallucinations. Although augmenting negative image samples partially addresses
these pitfalls, no prior work has employed listwise preference optimization for
VLMs, due to the complexity and cost of constructing listwise image samples. In
this work, we propose LPOI, the first object-aware listwise preference
optimization developed for reducing hallucinations in VLMs. LPOI identifies and
masks a critical object in the image, and then interpolates the masked region
between the positive and negative images to form a sequence of incrementally
more complete images. The model is trained to rank these images in ascending
order of object visibility, effectively reducing hallucinations while retaining
visual fidelity. LPOI requires no extra annotations beyond standard pairwise
preference data, as it automatically constructs the ranked lists through object
masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and
Object HalBench confirm that LPOI outperforms existing preference optimization
methods in reducing hallucinations and enhancing VLM performance. We make the
code available at https://github.com/fatemehpesaran310/lpoi.

</details>


### [277] [Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets](https://arxiv.org/abs/2505.20694)
*Xulin Gu,Xinhao Zhong,Zhixing Wei,Yimin Zhou,Shuoyang Sun,Bin Chen,Hongpeng Wang,Yuan Luo*

Main category: cs.CV

TL;DR: This paper presents a novel uni-level video dataset distillation framework that directly optimizes synthetic videos with respect to a pre-trained model, introducing a temporal saliency-guided filtering mechanism to address temporal redundancy and enhance motion preservation. Extensive experiments show state-of-the-art performance in video dataset compression.


<details>
  <summary>Details</summary>
Motivation: Dataset distillation has been successful in compressing image datasets but extending it to the video domain is challenging due to high dimensionality and temporal complexity inherent in video data. Existing video distillation methods suffer from excessive computational costs and struggle to preserve temporal dynamics.

Method: The authors propose a uni-level video dataset distillation framework that directly optimizes synthetic videos with respect to a pre-trained model. They introduce a temporal saliency-guided filtering mechanism that leverages inter-frame differences to guide the distillation process, encouraging the retention of informative temporal cues while suppressing frame-level redundancy.

Result: Extensive experiments on standard video benchmarks demonstrate that the proposed method achieves state-of-the-art performance in video dataset compression, bridging the gap between real and distilled video data.

Conclusion: The proposed uni-level video dataset distillation framework offers a scalable solution for video dataset compression by effectively addressing temporal redundancy and enhancing motion preservation.

Abstract: Dataset distillation (DD) has emerged as a powerful paradigm for dataset
compression, enabling the synthesis of compact surrogate datasets that
approximate the training utility of large-scale ones. While significant
progress has been achieved in distilling image datasets, extending DD to the
video domain remains challenging due to the high dimensionality and temporal
complexity inherent in video data. Existing video distillation (VD) methods
often suffer from excessive computational costs and struggle to preserve
temporal dynamics, as na\"ive extensions of image-based approaches typically
lead to degraded performance. In this paper, we propose a novel uni-level video
dataset distillation framework that directly optimizes synthetic videos with
respect to a pre-trained model. To address temporal redundancy and enhance
motion preservation, we introduce a temporal saliency-guided filtering
mechanism that leverages inter-frame differences to guide the distillation
process, encouraging the retention of informative temporal cues while
suppressing frame-level redundancy. Extensive experiments on standard video
benchmarks demonstrate that our method achieves state-of-the-art performance,
bridging the gap between real and distilled video data and offering a scalable
solution for video dataset compression.

</details>


### [278] [LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation](https://arxiv.org/abs/2505.20723)
*Pascal Zwick,Nils Friederich,Maximilian Beichter,Lennart Hilbert,Ralf Mikut,Oliver Bringmann*

Main category: cs.CV

TL;DR: LeDiFlow是一种基于学习分布引导的流匹配方法，用于训练FM图像生成模型。通过使用回归辅助模型学习更合适的先验分布，减少了推理时求解器步骤，提高了高质量图像生成效率。实验表明，LeDiFlow在像素空间和潜在空间均显著优于FM基线模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）由于其迭代性质，在高效生成高质量图像方面面临挑战。现有的流匹配（FM）方法依赖高斯分布先验，导致条件概率路径弯曲，使得ODE求解器需要大量推理调用，计算成本高昂。因此，需要一种更高效的FM方法来解决这一问题。

Method: 提出Learned Distribution-guided Flow Matching（LeDiFlow），通过回归辅助模型学习更适合的先验分布，从而初始化ODE求解器以接近目标数据分布，使概率路径更具计算可行性。该方法结合了最先进的Transformer架构与潜在空间采样技术，并能在普通工作站上进行训练。

Result: 在直接操作像素时，LeDiFlow模型相比相应的像素空间基线加速了高达3.75倍的推理速度；同时，其潜在FM模型在CLIP最大平均差异（CMMD）度量中，图像质量平均提高了1.32倍。

Conclusion: LeDiFlow作为一种新颖且可扩展的方法，显著提升了FM图像生成模型的效率和图像质量，证明了其在实际应用中的潜力。

Abstract: Enhancing the efficiency of high-quality image generation using Diffusion
Models (DMs) is a significant challenge due to the iterative nature of the
process. Flow Matching (FM) is emerging as a powerful generative modeling
paradigm based on a simulation-free training objective instead of a score-based
one used in DMs. Typical FM approaches rely on a Gaussian distribution prior,
which induces curved, conditional probability paths between the prior and
target data distribution. These curved paths pose a challenge for the Ordinary
Differential Equation (ODE) solver, requiring a large number of inference calls
to the flow prediction network. To address this issue, we present Learned
Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for
training FM-based image generation models using a better-suited prior
distribution learned via a regression-based auxiliary model. By initializing
the ODE solver with a prior closer to the target data distribution, LeDiFlow
enables the learning of more computationally tractable probability paths. These
paths directly translate to fewer solver steps needed for high-quality image
generation at inference time. Our method utilizes a State-Of-The-Art (SOTA)
transformer architecture combined with latent space sampling and can be trained
on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably
outperforms the respective FM baselines. For instance, when operating directly
on pixels, our model accelerates inference by up to 3.75x compared to the
corresponding pixel-space baseline. Simultaneously, our latent FM model
enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy
(CMMD) metric against its respective baseline.

</details>


### [279] [ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval](https://arxiv.org/abs/2505.20764)
*Eric Xing,Pranavi Kolouju,Robert Pless,Abby Stylianou,Nathan Jacobs*

Main category: cs.CV

TL;DR: The paper introduces ConText-CIR, a novel framework for Composed Image Retrieval (CIR) that uses a Text Concept-Consistency loss to improve the alignment between text modifications and query images. It also proposes a synthetic data generation pipeline. Together, these innovations set a new state-of-the-art in CIR tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods in composed image retrieval struggle with accurately representing both the image and the text modification, leading to subpar performance.

Method: The authors introduce ConText-CIR, a CIR framework trained with a Text Concept-Consistency loss which enhances the representation of noun phrases in text modifications to better correspond with relevant parts of the query image. Additionally, they propose a synthetic data generation pipeline to support training.

Result: ConText-CIR achieves stronger performance on CIR tasks, setting a new state-of-the-art in both supervised and zero-shot settings across multiple benchmark datasets such as CIRR and CIRCO.

Conclusion: ConText-CIR significantly advances the field of composed image retrieval by improving the alignment between text and image representations, supported by a novel loss function and data generation pipeline.

Abstract: Composed image retrieval (CIR) is the task of retrieving a target image
specified by a query image and a relative text that describes a semantic
modification to the query image. Existing methods in CIR struggle to accurately
represent the image and the text modification, resulting in subpar performance.
To address this limitation, we introduce a CIR framework, ConText-CIR, trained
with a Text Concept-Consistency loss that encourages the representations of
noun phrases in the text modification to better attend to the relevant parts of
the query image. To support training with this loss function, we also propose a
synthetic data generation pipeline that creates training data from existing CIR
datasets or unlabeled images. We show that these components together enable
stronger performance on CIR tasks, setting a new state-of-the-art in composed
image retrieval in both the supervised and zero-shot settings on multiple
benchmark datasets, including CIRR and CIRCO. Source code, model checkpoints,
and our new datasets are available at https://github.com/mvrl/ConText-CIR.

</details>


### [280] [MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning](https://arxiv.org/abs/2505.20772)
*Hongjia Liu,Rongzhen Zhao,Haohan Chen,Joni Pajarinen*

Main category: cs.CV

TL;DR: MetaSlot is a new Slot Attention variant that adapts to variable object counts, achieving significant performance gains in Object-Centric Learning.


<details>
  <summary>Details</summary>
Motivation: Mainstream Object-Centric Learning methods face challenges when dealing with variable object counts as their reliance on a static slot count can lead to an object being represented as multiple parts.

Method: MetaSlot maintains a codebook of object prototypes, removes duplicate slots using this codebook, and injects progressively weaker noise into the Slot Attention iterations for better aggregation.

Result: Models equipped with MetaSlot achieve significant performance gains and markedly interpretable slot representations across multiple public datasets and tasks, including object discovery and recognition.

Conclusion: MetaSlot is a general Slot Attention variant that can be seamlessly integrated into existing OCL architectures, providing better handling of variable object counts.

Abstract: Learning object-level, structured representations is widely regarded as a key
to better generalization in vision and underpins the design of next-generation
Pre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL)
methods adopt Slot Attention or its variants to iteratively aggregate objects'
super-pixels into a fixed set of query feature vectors, termed slots. However,
their reliance on a static slot count leads to an object being represented as
multiple parts when the number of objects varies. We introduce MetaSlot, a
plug-and-play Slot Attention variant that adapts to variable object counts.
MetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset
by vector-quantizing the resulting slot representations; (ii) removes duplicate
slots from the traditionally aggregated slots by quantizing them with the
codebook; and (iii) injects progressively weaker noise into the Slot Attention
iterations to accelerate and stabilize the aggregation. MetaSlot is a general
Slot Attention variant that can be seamlessly integrated into existing OCL
architectures. Across multiple public datasets and tasks--including object
discovery and recognition--models equipped with MetaSlot achieve significant
performance gains and markedly interpretable slot representations, compared
with existing Slot Attention variants.

</details>


### [281] [Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models](https://arxiv.org/abs/2505.20789)
*Yang Zheng,Wen Li,Zhaoqiang Liu*

Main category: cs.CV

TL;DR: The paper proposes two novel methods, DMILO and DMILO-PGD, based on pre-trained diffusion models for solving inverse problems. These methods improve computational efficiency and convergence through intermediate layer optimization and projected gradient descent respectively. Extensive experiments show significant performance gains over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion model-based methods for solving inverse problems often suffer from heavy computational demands and suboptimal convergence. To address these issues, the authors build upon the recent work DMPlug and propose new approaches.

Method: The paper introduces two methods: 1) DMILO, which uses intermediate layer optimization to reduce memory burden and expands the range of diffusion models by introducing sparse deviations; 2) DMILO-PGD, which combines intermediate layer optimization with projected gradient descent to reduce the risk of suboptimal convergence.

Result: Through extensive experiments on diverse image datasets, including both linear and nonlinear inverse problems, the proposed methods demonstrate significant performance improvements over state-of-the-art techniques.

Conclusion: DMILO and DMILO-PGD effectively address common challenges in diffusion model-based inverse problem solvers, offering improved computational efficiency and convergence.

Abstract: Inverse problems (IPs) involve reconstructing signals from noisy
observations. Traditional approaches often rely on handcrafted priors, which
can fail to capture the complexity of real-world data. The advent of
pre-trained generative models has introduced new paradigms, offering improved
reconstructions by learning rich priors from data. Among these, diffusion
models (DMs) have emerged as a powerful framework, achieving remarkable
reconstruction performance across numerous IPs. However, existing DM-based
methods frequently encounter issues such as heavy computational demands and
suboptimal convergence. In this work, building upon the idea of the recent work
DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and
DMILO-PGD, to address these challenges. Our first method, DMILO, employs
intermediate layer optimization (ILO) to alleviate the memory burden inherent
in DMPlug. Additionally, by introducing sparse deviations, we expand the range
of DMs, enabling the exploration of underlying signals that may lie outside the
range of the diffusion model. We further propose DMILO-PGD, which integrates
ILO with projected gradient descent (PGD), thereby reducing the risk of
suboptimal convergence. We provide an intuitive theoretical analysis of our
approach under appropriate conditions and validate its superiority through
extensive experiments on diverse image datasets, encompassing both linear and
nonlinear IPs. Our results demonstrate significant performance gains over
state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD
in addressing common challenges in DM-based IP solvers.

</details>


### [282] [Is Hyperbolic Space All You Need for Medical Anomaly Detection?](https://arxiv.org/abs/2505.21228)
*Alvaro Gonzalez-Jimenez,Simone Lionetti,Ludovic Amruthalingam,Philippe Gottfrois,Fabian Gröger,Marc Pouly,Alexander A. Navarini*

Main category: cs.CV

TL;DR: The paper proposes a new method for medical anomaly detection by projecting feature representations into hyperbolic space, which outperforms traditional Euclidean-based frameworks in terms of AUROC scores and resilience to parameter variations.


<details>
  <summary>Details</summary>
Motivation: Medical anomaly detection faces challenges in data availability and labeling constraints. Traditional methods using Euclidean space fail to capture hierarchical relationships within features effectively, leading to suboptimal performance.

Method: The proposed approach projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous.

Result: Experiments show that hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores at both image and pixel levels across multiple medical benchmark datasets. It also exhibits resilience to parameter variations and excels in few-shot scenarios.

Conclusion: Hyperbolic space shows potential as a powerful alternative for medical anomaly detection.

Abstract: Medical anomaly detection has emerged as a promising solution to challenges
in data availability and labeling constraints. Traditional methods extract
features from different layers of pre-trained networks in Euclidean space;
however, Euclidean representations fail to effectively capture the hierarchical
relationships within these features, leading to suboptimal anomaly detection
performance. We propose a novel yet simple approach that projects feature
representations into hyperbolic space, aggregates them based on confidence
levels, and classifies samples as healthy or anomalous. Our experiments
demonstrate that hyperbolic space consistently outperforms Euclidean-based
frameworks, achieving higher AUROC scores at both image and pixel levels across
multiple medical benchmark datasets. Additionally, we show that hyperbolic
space exhibits resilience to parameter variations and excels in few-shot
scenarios, where healthy images are scarce. These findings underscore the
potential of hyperbolic space as a powerful alternative for medical anomaly
detection. The project website can be found at
https://hyperbolic-anomalies.github.io

</details>


### [283] [Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains](https://arxiv.org/abs/2505.21010)
*Sabbir Ahmed,Mamshad Nayeem Rizve,Abdullah Al Arafat,Jacqueline Liu,Rahim Hossain,Mohaiminul Al Nahian,Adnan Siraj Rakin*

Main category: cs.CV

TL;DR: Semi-Supervised Federated Learning (SSFL) faces challenges in model generalization due to domain shifts. This paper proposes a Unified Alignment Protocol (UAP) framework with an alternating two-stage training process, which enhances generalization capabilities across domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for SSFL models to have better generalization capabilities across different domains because of the frequent occurrence of domain shifts in real-world applications.

Method: The method involves proposing a Unified Alignment Protocol (UAP) framework with two stages: 1) training the server model to align features with a parametric distribution; 2) utilizing this distribution to align client features accordingly.

Result: Extensive experiments on standard domain generalization benchmark datasets show that UAP achieves state-of-the-art generalization performance in the SSFL setting.

Conclusion: UAP successfully addresses the challenge of improving model generalization in SSFL by introducing a novel two-stage training process.

Abstract: Semi-Supervised Federated Learning (SSFL) is gaining popularity over
conventional Federated Learning in many real-world applications. Due to the
practical limitation of limited labeled data on the client side, SSFL considers
that participating clients train with unlabeled data, and only the central
server has the necessary resources to access limited labeled data, making it an
ideal fit for real-world applications (e.g., healthcare). However, traditional
SSFL assumes that the data distributions in the training phase and testing
phase are the same. In practice, however, domain shifts frequently occur,
making it essential for SSFL to incorporate generalization capabilities and
enhance their practicality. The core challenge is improving model
generalization to new, unseen domains while the client participate in SSFL.
However, the decentralized setup of SSFL and unsupervised client training
necessitates innovation to achieve improved generalization across domains. To
achieve this, we propose a novel framework called the Unified Alignment
Protocol (UAP), which consists of an alternating two-stage training process.
The first stage involves training the server model to learn and align the
features with a parametric distribution, which is subsequently communicated to
clients without additional communication overhead. The second stage proposes a
novel training algorithm that utilizes the server feature distribution to align
client features accordingly. Our extensive experiments on standard domain
generalization benchmark datasets across multiple model architectures reveal
that proposed UAP successfully achieves SOTA generalization performance in SSFL
setting.

</details>


### [284] [Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning](https://arxiv.org/abs/2505.21420)
*Jinbao Wang,Hanzhe Liang,Can Gao,Chenxi Hu,Jie Zhou,Yunkang Cao,Linlin Shen,Weiming Shen*

Main category: cs.CV

TL;DR: Multimodal feature reconstruction is a promising approach for 3D anomaly detection, and this paper proposes Mentor3AD which utilizes multi-modal mentor learning to improve detection performance through various modules including Mentor of Fusion Module (MFM), Mentor of Guidance Module (MGM), and Voting Module (VM).


<details>
  <summary>Details</summary>
Motivation: To further advance multimodal feature reconstruction for 3D anomaly detection by distinguishing normal from feature differences using multi-modal mentor learning.

Method: The proposed method, Mentor3AD, includes three key components: Mentor of Fusion Module (MFM) that merges features from RGB and 3D modalities; Mentor of Guidance Module (MGM) that facilitates cross-modal reconstruction; and Voting Module (VM) that generates the final anomaly score.

Result: Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies datasets have verified the effectiveness of Mentor3AD in improving detection performance.

Conclusion: Mentor3AD, utilizing multi-modal mentor learning, successfully extracts more effective features, guides feature reconstruction, and enhances 3D anomaly detection performance.

Abstract: Multimodal feature reconstruction is a promising approach for 3D anomaly
detection, leveraging the complementary information from dual modalities. We
further advance this paradigm by utilizing multi-modal mentor learning, which
fuses intermediate features to further distinguish normal from feature
differences. To address these challenges, we propose a novel method called
Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared
features of different modalities, Mentor3AD can extract more effective features
and guide feature reconstruction, ultimately improving detection performance.
Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges
features extracted from RGB and 3D modalities to create a mentor feature.
Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate
cross-modal reconstruction, supported by the mentor feature. Lastly, we
introduce a Voting Module (VM) to more accurately generate the final anomaly
score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies
have verified the effectiveness of the proposed method.

</details>


### [285] [AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping](https://arxiv.org/abs/2505.21357)
*Wenyuan Li,Shunlin Liang,Keyan Chen,Yongzhe Chen,Han Ma,Jianglei Xu,Yichuan Ma,Shikang Guan,Husheng Fang,Zhenwei Shi*

Main category: cs.CV

TL;DR: AgriFM is a multi-source remote sensing foundation model designed for agricultural crop mapping, which uses a modified Video Swin Transformer architecture to extract hierarchical spatiotemporal features and is pre-trained on a global dataset with over 25 million image samples. It demonstrates superior performance in various downstream tasks compared to conventional methods and general-purpose RSFMs.


<details>
  <summary>Details</summary>
Motivation: Accurate crop mapping requires modeling multi-scale spatiotemporal patterns, but current transformer-based remote sensing foundation models either use fixed spatiotemporal windows or ignore temporal information.

Method: The approach involves creating AgriFM, which uses a modified Video Swin Transformer architecture for simultaneous hierarchical spatiotemporal feature extraction, leveraging data from MODIS, Landsat-8/9, and Sentinel-2 satellites, and being pre-trained on a large global dataset.

Result: AgriFM shows superior performance across all downstream tasks compared to both conventional deep learning approaches and state-of-the-art general-purpose RSFMs.

Conclusion: AgriFM successfully bridges the gaps in current RSFMs for crop mapping by effectively extracting multi-scale spatiotemporal features, offering a significant advancement in agricultural crop mapping.

Abstract: Accurate crop mapping fundamentally relies on modeling multi-scale
spatiotemporal patterns, where spatial scales range from individual field
textures to landscape-level context, and temporal scales capture both
short-term phenological transitions and full growing-season dynamics.
Transformer-based remote sensing foundation models (RSFMs) offer promising
potential for crop mapping due to their innate ability for unified
spatiotemporal processing. However, current RSFMs remain suboptimal for crop
mapping: they either employ fixed spatiotemporal windows that ignore the
multi-scale nature of crop systems or completely disregard temporal information
by focusing solely on spatial patterns. To bridge these gaps, we present
AgriFM, a multi-source remote sensing foundation model specifically designed
for agricultural crop mapping. Our approach begins by establishing the
necessity of simultaneous hierarchical spatiotemporal feature extraction,
leading to the development of a modified Video Swin Transformer architecture
where temporal down-sampling is synchronized with spatial scaling operations.
This modified backbone enables efficient unified processing of long time-series
satellite inputs. AgriFM leverages temporally rich data streams from three
satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is
pre-trained on a global representative dataset comprising over 25 million image
samples supervised by land cover products. The resulting framework incorporates
a versatile decoder architecture that dynamically fuses these learned
spatiotemporal representations, supporting diverse downstream tasks.
Comprehensive evaluations demonstrate AgriFM's superior performance over
conventional deep learning approaches and state-of-the-art general-purpose
RSFMs across all downstream tasks. Codes will be available at
urlhttps://github.com/flyakon/AgriFM.

</details>


### [286] [Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO](https://arxiv.org/abs/2505.21457)
*Muzhi Zhu,Hao Zhong,Canyu Zhao,Zongze Du,Zheng Huang,Mingyu Liu,Hao Chen,Cheng Zou,Jingdong Chen,Ming Yang,Chunhua Shen*

Main category: cs.CV

TL;DR: Active vision or perception is crucial for efficient human and robotic decision-making. While Multimodal Large Language Models (MLLMs) are gaining attention in robotics, their active perception capabilities remain underexplored. This paper defines MLLM-based active perception tasks, critiques the GPT-o3 model's strategy, and introduces ACTIVE-O3, a reinforcement learning framework enhancing MLLMs' active perception. Evaluated across various tasks, ACTIVE-O3 also shows zero-shot reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper lies in addressing the lack of exploration on how MLLMs can be equipped with or learn active perception capabilities, which are critical for embodied intelligence.

Method: The method involves providing a systematic definition of MLLM-based active perception tasks, critiquing the GPT-o3 model's zoom-in search strategy, and proposing ACTIVE-O3 - a reinforcement learning based training framework built on GRPO to equip MLLMs with active perception capabilities.

Result: ACTIVE-O3 was evaluated across general open-world tasks and domain-specific scenarios, demonstrating strong performance and zero-shot reasoning abilities on the V* Benchmark without explicit reasoning data.

Conclusion: The conclusion emphasizes that this work provides a simple codebase and evaluation protocol to promote future research on active perception in MLLMs.

Abstract: Active vision, also known as active perception, refers to the process of
actively selecting where and how to look in order to gather task-relevant
information. It is a critical component of efficient perception and
decision-making in humans and advanced embodied agents. Recently, the use of
Multimodal Large Language Models (MLLMs) as central planning and
decision-making modules in robotic systems has gained extensive attention.
However, despite the importance of active perception in embodied intelligence,
there is little to no exploration of how MLLMs can be equipped with or learn
active perception capabilities. In this paper, we first provide a systematic
definition of MLLM-based active perception tasks. We point out that the
recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a
special case of active perception; however, it still suffers from low search
efficiency and inaccurate region selection. To address these issues, we propose
ACTIVE-O3, a purely reinforcement learning based training framework built on
top of GRPO, designed to equip MLLMs with active perception capabilities. We
further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across
both general open-world tasks, such as small-object and dense object grounding,
and domain-specific scenarios, including small object detection in remote
sensing and autonomous driving, as well as fine-grained interactive
segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot
reasoning abilities on the V* Benchmark, without relying on any explicit
reasoning data. We hope that our work can provide a simple codebase and
evaluation protocol to facilitate future research on active perception in
MLLMs.

</details>


### [287] [Policy Optimized Text-to-Image Pipeline Design](https://arxiv.org/abs/2505.21478)
*Uri Gadot,Rinon Gal,Yftah Ziser,Gal Chechik,Shie Mannor*

Main category: cs.CV

TL;DR: The paper presents a new reinforcement learning-based framework for text-to-image generation that improves image quality and workflow efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current automated approaches using large language models (LLMs) in designing multi-component pipelines for text-to-image generation, such as high computational requirements and poor generalization beyond training examples.

Method: The method involves training an ensemble of reward models to predict image quality scores from prompt-workflow combinations without generating images. Then, a two-phase training strategy is applied: initial vocabulary training followed by GRPO-based optimization. A classifier-free guidance based enhancement technique is also incorporated.

Result: Through comparisons, the approach successfully creates new flows with greater diversity and achieves superior image quality compared to existing baselines.

Conclusion: This novel framework effectively reduces inefficiencies in current automated processes for text-to-image generation, leading to better quality outputs and more efficient workflows.

Abstract: Text-to-image generation has evolved beyond single monolithic models to
complex multi-component pipelines. These combine fine-tuned generators,
adapters, upscaling blocks and even editing steps, leading to significant
improvements in image quality. However, their effective design requires
substantial expertise. Recent approaches have shown promise in automating this
process through large language models (LLMs), but they suffer from two critical
limitations: extensive computational requirements from generating images with
hundreds of predefined pipelines, and poor generalization beyond memorized
training examples. We introduce a novel reinforcement learning-based framework
that addresses these inefficiencies. Our approach first trains an ensemble of
reward models capable of predicting image quality scores directly from
prompt-workflow combinations, eliminating the need for costly image generation
during training. We then implement a two-phase training strategy: initial
workflow vocabulary training followed by GRPO-based optimization that guides
the model toward higher-performing regions of the workflow space. Additionally,
we incorporate a classifier-free guidance based enhancement technique that
extrapolates along the path between the initial and GRPO-tuned models, further
improving output quality. We validate our approach through a set of
comparisons, showing that it can successfully create new flows with greater
diversity and lead to superior image quality compared to existing baselines.

</details>


### [288] [Be Decisive: Noise-Induced Layouts for Multi-Subject Generation](https://arxiv.org/abs/2505.21488)
*Omer Dahary,Yehonathan Cohen,Or Patashnik,Kfir Aberman,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: This paper presents a new approach for text-to-image diffusion models that predicts and refines a spatial layout aligned with the prompt, derived from the initial noise. This method improves text-image alignment, multi-subject generation stability, and preserves model distribution diversity.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image diffusion models struggle with generating multiple distinct subjects without inaccuracies or subject leakage. Current methods using external layout controls conflict with the model's prior due to prescribed layouts.

Method: The introduced method predicts a spatial layout aligned with the prompt from the initial noise and refines it during the denoising process using a small neural network. This ensures clear boundaries between subjects and maintains consistency.

Result: Experimental results indicate that this strategy achieves better text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the model's original distribution diversity.

Conclusion: The noise-aligned approach offers an effective solution for improving multi-subject text-to-image generation by avoiding conflicts with externally imposed layouts and preserving the model's prior.

Abstract: Generating multiple distinct subjects remains a challenge for existing
text-to-image diffusion models. Complex prompts often lead to subject leakage,
causing inaccuracies in quantities, attributes, and visual features. Preventing
leakage among subjects necessitates knowledge of each subject's spatial
location. Recent methods provide these spatial locations via an external layout
control. However, enforcing such a prescribed layout often conflicts with the
innate layout dictated by the sampled initial noise, leading to misalignment
with the model's prior. In this work, we introduce a new approach that predicts
a spatial layout aligned with the prompt, derived from the initial noise, and
refines it throughout the denoising process. By relying on this noise-induced
layout, we avoid conflicts with externally imposed layouts and better preserve
the model's prior. Our method employs a small neural network to predict and
refine the evolving noise-induced layout at each denoising step, ensuring clear
boundaries between subjects while maintaining consistency. Experimental results
show that this noise-aligned strategy achieves improved text-image alignment
and more stable multi-subject generation compared to existing layout-guided
techniques, while preserving the rich diversity of the model's original
distribution.

</details>


### [289] [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/abs/2505.21497)
*Wei Pang,Kevin Qinghong Lin,Xiangru Jian,Xi He,Philip Torr*

Main category: cs.CV

TL;DR: The paper introduces the first benchmark and metric suite for academic poster generation, proposing PosterAgent, a multi-agent pipeline that converts papers into visually coherent posters. Evaluations reveal GPT-4o's limitations in text quality and content conveyance, while the open-source PosterAgent outperforms with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of compressing long-context documents into visually coherent posters in scientific communication, and to provide a comprehensive evaluation framework for poster generation.

Method: Introduced a benchmark and metric suite including Visual Quality, Textual Coherence, Holistic Assessment, and PaperQuiz. Proposed PosterAgent, a multi-agent pipeline consisting of Parser, Planner, and Painter-Commenter loop to generate posters.

Result: GPT-4o outputs show appealing visuals but contain noisy text and poor PaperQuiz scores. PosterAgent variants outperform existing systems across nearly all metrics while using significantly fewer tokens.

Conclusion: The findings indicate clear directions for the development of fully automated poster-generation models, highlighting the importance of reader engagement and visual semantics.

Abstract: Academic poster generation is a crucial yet challenging task in scientific
communication, requiring the compression of long-context interleaved documents
into a single, visually coherent page. To address this challenge, we introduce
the first benchmark and metric suite for poster generation, which pairs recent
conference papers with author-designed posters and evaluates outputs on
(i)Visual Quality-semantic alignment with human posters, (ii)Textual
Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic
and informational criteria scored by a VLM-as-judge, and notably
(iv)PaperQuiz-the poster's ability to convey core paper content as measured by
VLMs answering generated quizzes. Building on this benchmark, we propose
PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser
distills the paper into a structured asset library; the (b)Planner aligns
text-visual pairs into a binary-tree layout that preserves reading order and
spatial balance; and the (c)Painter-Commenter loop refines each panel by
executing rendering code and using VLM feedback to eliminate overflow and
ensure alignment. In our comprehensive evaluation, we find that GPT-4o
outputs-though visually appealing at first glance-often exhibit noisy text and
poor PaperQuiz scores, and we find that reader engagement is the primary
aesthetic bottleneck, as human-designed posters rely largely on visual
semantics to convey meaning. Our fully open-source variants (e.g. based on the
Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across
nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper
into a finalized yet editable .pptx poster - all for just $0.005. These
findings chart clear directions for the next generation of fully automated
poster-generation models. The code and datasets are available at
https://github.com/Paper2Poster/Paper2Poster.

</details>


### [290] [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
*Dingming Li,Hongxing Li,Zixuan Wang,Yuchen Yan,Hang Zhang,Siqi Chen,Guiyang Hou,Shengpei Jiang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Main category: cs.CV

TL;DR: Vision-language models (VLMs) are great at understanding visual content, but they struggle with spatial reasoning from different viewpoints. This paper presents ViewSpatial-Bench, a benchmark for evaluating VLM performance across various spatial tasks and viewpoints. Results show that VLMs perform well on camera-perspective tasks but poorly on human-viewpoint tasks. Fine-tuning on the multi-perspective dataset improves overall performance by 46.24%, demonstrating the importance of 3D spatial relationship modeling.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current vision-language models in adopting allocentric viewpoints and performing spatial reasoning beyond egocentric perspectives.

Method: Introduced ViewSpatial-Bench, a comprehensive benchmark for multi-viewpoint spatial localization recognition across five distinct task types. Used an automated 3D annotation pipeline to generate precise directional labels. Evaluated diverse VLMs on this benchmark and fine-tuned them on the multi-perspective spatial dataset.

Result: Significant performance disparity found: models perform well on camera-perspective tasks but poorly on human-viewpoint tasks. Fine-tuning on the multi-perspective dataset led to a 46.24% overall performance improvement across tasks.

Conclusion: Established a crucial benchmark for spatial intelligence in embodied AI systems. Provided empirical evidence that modeling 3D spatial relationships enhances VLMs' spatial comprehension capabilities.

Abstract: Vision-language models (VLMs) have demonstrated remarkable capabilities in
understanding and reasoning about visual content, but significant challenges
persist in tasks requiring cross-viewpoint understanding and spatial reasoning.
We identify a critical limitation: current VLMs excel primarily at egocentric
spatial reasoning (from the camera's perspective) but fail to generalize to
allocentric viewpoints when required to adopt another entity's spatial frame of
reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark
designed specifically for multi-viewpoint spatial localization recognition
evaluation across five distinct task types, supported by an automated 3D
annotation pipeline that generates precise directional labels. Comprehensive
evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant
performance disparity: models demonstrate reasonable performance on
camera-perspective tasks but exhibit reduced accuracy when reasoning from a
human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,
we achieve an overall performance improvement of 46.24% across tasks,
highlighting the efficacy of our approach. Our work establishes a crucial
benchmark for spatial intelligence in embodied AI systems and provides
empirical evidence that modeling 3D spatial relationships enhances VLMs'
corresponding spatial comprehension capabilities.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [291] [Data-driven multi-agent modelling of calcium interactions in cell culture: PINN vs Regularized Least-squares](https://arxiv.org/abs/2505.20327)
*Aurora Poggi,Giuseppe Alessio D'Inverno,Hjalmar Brismar,Ozan Öktem,Matthieu Barreau,Kateryna Morozovska*

Main category: q-bio.QM

TL;DR: Data-driven discovery in biological systems using CRLSM and PINN methods for ODEs.


<details>
  <summary>Details</summary>
Motivation: To better observe and characterize calcium signaling processes by exploring insights of dynamical systems.

Method: Compare the performance of Constrained Regularized Least-Squares Method (CRLSM) and Physics-Informed Neural Networks (PINN) for system identification and parameter discovery.

Result: CRLSM achieves good parameter estimates and data fit, whereas PINNs fail to match CRLSM's performance with limited architectures studied.

Conclusion: CRLSM outperforms PINN in current configurations for parameter estimation, but further tuning of PINNs could improve results.

Abstract: Data-driven discovery of dynamics in biological systems allows for better
observation and characterization of processes, such as calcium signaling in
cell culture. Recent advancements in techniques allow the exploration of
previously unattainable insights of dynamical systems, such as the Sparse
Identification of Non-Linear Dynamics (SINDy), overcoming the limitations of
more classic methodologies. The latter requires some prior knowledge of an
effective library of candidate terms, which is not realistic for a real case
study. Using inspiration from fields like traffic density estimation and
control theory, we propose a methodology for characterization and performance
analysis of calcium delivery in a family of cells. In this work, we compare the
performance of the Constrained Regularized Least-Squares Method (CRLSM) and
Physics-Informed Neural Networks (PINN) for system identification and parameter
discovery for governing ordinary differential equations (ODEs). The CRLSM
achieves a fairly good parameter estimate and a good data fit when using the
learned parameters in the Consensus problem. On the other hand, despite the
initial hypothesis, PINNs fail to match the CRLSM performance and, under the
current configuration, do not provide fair parameter estimation. However, we
have only studied a limited number of PINN architectures, and it is expected
that additional hyperparameter tuning, as well as uncertainty quantification,
could significantly improve the performance in future works.

</details>


### [292] [Sequence-Only Prediction of Binding Affinity Changes: A Robust and Interpretable Model for Antibody Engineering](https://arxiv.org/abs/2505.20301)
*Chen Liu,Mingchen Li,Yang Tan,Wenrui Gou,Guisheng Fan,Bingxin Zhou*

Main category: q-bio.QM

TL;DR: The paper introduces ProtAttBA, a deep learning model predicting antibody-antigen binding affinity changes using only sequence information. It uses pre-training and supervised training phases, achieves competitive performance compared to other methods, and offers interpretability through attention scores.


<details>
  <summary>Details</summary>
Motivation: To find an effective way to enhance antibody-antigen binding affinity without relying on costly and time-consuming wet-lab experiments or high-quality complex structures that are often unavailable.

Method: ProtAttBA employs a pre-training phase to learn protein sequence patterns and a supervised training phase using labeled antibody-antigen complex data to train a cross-attention-based regressor for predicting binding affinity changes.

Result: ProtAttBA achieves competitive performance compared to both sequence- and structure-based prediction methods, demonstrating robustness especially with uncertain complex structures. The method also possesses interpretability from the attention mechanism, which can identify critical residues impacting binding affinity.

Conclusion: This work presents a fast and cost-effective computational tool for antibody engineering that has the potential to accelerate the development of novel therapeutic antibodies.

Abstract: A pivotal area of research in antibody engineering is to find effective
modifications that enhance antibody-antigen binding affinity. Traditional
wet-lab experiments assess mutants in a costly and time-consuming manner.
Emerging deep learning solutions offer an alternative by modeling antibody
structures to predict binding affinity changes. However, they heavily depend on
high-quality complex structures, which are frequently unavailable in practice.
Therefore, we propose ProtAttBA, a deep learning model that predicts binding
affinity changes based solely on the sequence information of antibody-antigen
complexes. ProtAttBA employs a pre-training phase to learn protein sequence
patterns, following a supervised training phase using labeled antibody-antigen
complex data to train a cross-attention-based regressor for predicting binding
affinity changes. We evaluated ProtAttBA on three open benchmarks under
different conditions. Compared to both sequence- and structure-based prediction
methods, our approach achieves competitive performance, demonstrating notable
robustness, especially with uncertain complex structures. Notably, our method
possesses interpretability from the attention mechanism. We show that the
learned attention scores can identify critical residues with impacts on binding
affinity. This work introduces a rapid and cost-effective computational tool
for antibody engineering, with the potential to accelerate the development of
novel therapeutic antibodies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [293] [VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion](https://arxiv.org/abs/2505.20794)
*Joon-Seung Choi,Dong-Min Byun,Hyung-Seok Oh,Seong-Whan Lee*

Main category: cs.SD

TL;DR: 提出了一种名为VibESVC的新模型，通过离散小波变换显式提取和操控颤音，实现可控的歌声转换，实验表明该模型能有效转换歌声风格并保持说话人相似性。


<details>
  <summary>Details</summary>
Motivation: 颤音在传达情感和增强音乐深度方面起着关键作用，但由于其动态特性，建模颤音具有挑战性，难以在歌声转换中控制。

Method: 提出VibESVC模型，利用离散小波变换将F0轮廓分解为频率成分，从而显式提取和操控颤音，实现精确转移和颤音控制。

Result: 实验结果表明，VibE-SVC能够有效转换歌声风格，同时保持说话人相似性，主观和客观评估均证实了高质量的转换效果。

Conclusion: VibESVC模型通过显式处理颤音，提高了歌声转换的灵活性和质量。

Abstract: Controlling singing style is crucial for achieving an expressive and natural
singing voice. Among the various style factors, vibrato plays a key role in
conveying emotions and enhancing musical depth. However, modeling vibrato
remains challenging due to its dynamic nature, making it difficult to control
in singing voice conversion. To address this, we propose VibESVC, a
controllable singing voice conversion model that explicitly extracts and
manipulates vibrato using discrete wavelet transform. Unlike previous methods
that model vibrato implicitly, our approach decomposes the F0 contour into
frequency components, enabling precise transfer. This allows vibrato control
for enhanced flexibility. Experimental results show that VibE-SVC effectively
transforms singing styles while preserving speaker similarity. Both subjective
and objective evaluations confirm high-quality conversion.

</details>


### [294] [Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech](https://arxiv.org/abs/2505.20868)
*Nam-Gyu Kim,Deok-Hyeon Cho,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: Spotlight-TTS是一种新的文本转语音方法，通过有声意识风格提取和风格方向调整来强调风格，从而提高语音质量和表达力。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在表达性文本转语音方面取得了进展，但合成高质量的表达性语音仍然具有挑战性。

Method: 提出了一种名为Spotlight-TTS的方法，该方法通过有声意识风格提取（关注与风格高度相关的有声区域，同时保持不同语音区域之间的连续性）和风格方向调整（调整提取的风格方向以优化其与TTS模型的集成）来专门强调风格。

Result: 实验结果表明，与基线模型相比，Spotlight-TTS在表达性、整体语音质量和风格转换能力方面表现出优越的性能。

Conclusion: Spotlight-TTS在提高语音表达性和质量方面优于基线模型，并且其音频样本已公开。

Abstract: Recent advances in expressive text-to-speech (TTS) have introduced diverse
methods based on style embedding extracted from reference speech. However,
synthesizing high-quality expressive speech remains challenging. We propose
Spotlight-TTS, which exclusively emphasizes style via voiced-aware style
extraction and style direction adjustment. Voiced-aware style extraction
focuses on voiced regions highly related to style while maintaining continuity
across different speech regions to improve expressiveness. We adjust the
direction of the extracted style for optimal integration into the TTS model,
which improves speech quality. Experimental results demonstrate that
Spotlight-TTS achieves superior performance compared to baseline models in
terms of expressiveness, overall speech quality, and style transfer capability.
Our audio samples are publicly available.

</details>


### [295] [Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection](https://arxiv.org/abs/2505.20956)
*Shiqi Zhang,Tuomas Virtanen*

Main category: cs.SD

TL;DR: The paper proposes MFFT for BioSED to overcome challenges like limited data and class imbalance, achieving high mAP with minimal annotations.


<details>
  <summary>Details</summary>
Motivation: Bioacoustic sound event detection is essential for biodiversity conservation but faces issues such as sparse events, species diversity, and class imbalance. There's a need for efficient methods using limited labeling budgets.

Method: MFFT, an active learning method combining committee voting disagreement and diversity analysis, is applied. An existing BioSED dataset is refined for evaluating active learning algorithms.

Result: MFFT achieves mAP of 68% in cold-starting and 71% in warm-starting scenarios with only 2.3% of annotations, comparable to fully-supervised results.

Conclusion: MFFT proves effective in cold-start scenarios and for rare species, showing practical value for monitoring endangered species.

Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity
conservation but faces practical challenges during model development and
training: limited amounts of annotated data, sparse events, species diversity,
and class imbalance. To address these challenges efficiently with a limited
labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an
active learning method integrating committee voting disagreement and diversity
analysis. We also refine an existing BioSED dataset specifically for evaluating
active learning algorithms. Experimental results demonstrate that MFFT achieves
a mAP of 68% when cold-starting and 71% when warm-starting (which is close to
the fully-supervised mAP of 75%) while using only 2.3% of the annotations.
Notably, MFFT excels in cold-start scenarios and with rare species, which are
critical for monitoring endangered species, demonstrating its practical value.

</details>


### [296] [Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization](https://arxiv.org/abs/2505.20961)
*Yiyuan Yang,Shitong Xu,Niki Trigoni,Andrew Markham*

Main category: cs.SD

TL;DR: A new 3D SSL framework using sparse cross-attention, pretraining, and adaptive signal coherence metrics is introduced to achieve accurate and efficient localization with fewer microphones.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods have high computational costs and require precise calibration, restricting their use in dynamic or resource-constrained environments.

Method: The method involves a novel 3D SSL framework that uses sparse cross-attention, pretraining, and adaptive signal coherence metrics. It requires fewer input microphones and is fault-tolerant to unreliable microphone positions.

Result: Preliminary experiments show scalability for multi-source localization without additional hardware.

Conclusion: This work enhances SSL by balancing performance and efficiency while improving robustness for real-world applications.

Abstract: Sound source localization (SSL) is a critical technology for determining the
position of sound sources in complex environments. However, existing methods
face challenges such as high computational costs and precise calibration
requirements, limiting their deployment in dynamic or resource-constrained
environments. This paper introduces a novel 3D SSL framework, which uses sparse
cross-attention, pretraining, and adaptive signal coherence metrics, to achieve
accurate and computationally efficient localization with fewer input
microphones. The framework is also fault-tolerant to unreliable or even unknown
microphone position inputs, ensuring its applicability in real-world scenarios.
Preliminary experiments demonstrate its scalability for multi-source
localization without requiring additional hardware. This work advances SSL by
balancing the model's performance and efficiency and improving its robustness
for real-world scenarios.

</details>


### [297] [Training Articulatory Inversion Models for Inter-Speaker Consistency](https://arxiv.org/abs/2505.20529)
*Charles McGhee,Mark J. F. Gales,Kate M. Knill*

Main category: cs.SD

TL;DR: The paper explores if SSL-adapted models trained on single and multi-speaker data can produce consistent articulatory targets across speakers for English and Russian, proposing a new evaluation method and a training method to improve consistency.


<details>
  <summary>Details</summary>
Motivation: To determine whether self-supervised learning models adapted to single and multi-speaker datasets can generate articulatory targets that remain consistent across different speaker identities in English and Russian languages.

Method: Using a novel evaluation method based on minimal pair sets to extract articulatory targets, and introducing a training method aimed at enhancing inter-speaker consistency using only speech data.

Result: The results are not explicitly stated in the abstract but the methods proposed aim to evaluate consistency of articulatory targets across speakers and improve inter-speaker consistency.

Conclusion: Not explicitly stated; however, it is implied that the findings will provide insights into the consistency of articulatory targets produced by SSL-adapted models across different speakers.

Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse
mapping from speech to articulation. Exact articulatory prediction from speech
alone may be impossible, as speakers can choose different forms of articulation
seemingly without reference to their vocal tract structure. However, once a
speaker has selected an articulatory form, their productions vary minimally.
Recent works in AAI have proposed adapting Self-Supervised Learning (SSL)
models to single-speaker datasets, claiming that these single-speaker models
provide a universal articulatory template. In this paper, we investigate
whether SSL-adapted models trained on single and multi-speaker data produce
articulatory targets which are consistent across speaker identities for English
and Russian. We do this through the use of a novel evaluation method which
extracts articulatory targets using minimal pair sets. We also present a
training method which can improve inter-speaker consistency using only speech
data.

</details>


### [298] [MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection](https://arxiv.org/abs/2505.20979)
*Tongyu Lu,Charlotta-Marlena Geist,Jan Melechovsky,Abhinaba Roy,Dorien Herremans*

Main category: cs.SD

TL;DR: The paper proposes MelodySim, a melody-aware music similarity model and dataset for plagiarism detection, which achieves high accuracy on its test set.


<details>
  <summary>Details</summary>
Motivation: To create a melody-aware music similarity model and dataset for plagiarism detection, focusing on melodic similarity while significantly altering other musical tracks.

Method: 1) Construct a dataset by augmenting Slakh2100 with modifications preserving melody such as note splitting, arpeggiation, minor track dropout (excluding bass), and re-instrumentation. 2) Develop a segment-wise melodic-similarity detection model using a MERT encoder and triplet neural network to capture melodic similarity.

Result: A user study confirms that positive pairs contain similar melodies with other musical tracks significantly changed. The model achieves high accuracy on the MelodySim test set.

Conclusion: MelodySim is an effective melody-aware music similarity model and dataset for plagiarism detection.

Abstract: We propose MelodySim, a melody-aware music similarity model and dataset for
plagiarism detection. First, we introduce a novel method to construct a dataset
with focus on melodic similarity. By augmenting Slakh2100; an existing MIDI
dataset, we generate variations of each piece while preserving the melody
through modifications such as note splitting, arpeggiation, minor track dropout
(excluding bass), and re-instrumentation. A user study confirms that positive
pairs indeed contain similar melodies, with other musical tracks significantly
changed. Second, we develop a segment-wise melodic-similarity detection model
that uses a MERT encoder and applies a triplet neural network to capture
melodic similarity. The resultant decision matrix highlights where plagiarism
might occur. Our model achieves high accuracy on the MelodySim test set.

</details>


### [299] [Text-Queried Audio Source Separation via Hierarchical Modeling](https://arxiv.org/abs/2505.21025)
*Xinlei Yin,Xiulian Peng,Xue Jiang,Zhiwei Xiong,Yan Lu*

Main category: cs.SD

TL;DR: 提出HSM-TSS框架解决声学-文本对齐和语义分离的难题，引入双阶段机制和指令处理流水线，实现复杂听觉场景中的高质量音频源分离。


<details>
  <summary>Details</summary>
Motivation: 当前方法在联合建模声学-文本对齐和语义感知分离方面存在困难，并且需要大规模精确标注的数据来弥补低效的跨模态学习和分离问题。

Method: 提出分层分解框架HSM-TSS，将任务分为全局-局部语义引导特征分离和结构保留的声音重建。使用Q-Audio架构对齐音频和文本模态，作为预训练的全局语义编码器。基于预测的全局特征，在AudioMAE特征上执行第二阶段局部语义分离，然后进行声音重建。还提出了一个指令处理流水线，将任意文本查询解析为结构化操作，实现灵活的声音操作。

Result: 该方法在数据高效的训练下实现了最先进的分离性能，并在复杂的听觉场景中保持了与查询高度一致的语义一致性。

Conclusion: HSM-TSS框架通过解耦任务、引入双阶段机制和指令处理流水线，有效解决了现有方法面临的挑战，显著提升了目标音频源分离的性能和灵活性。

Abstract: Target audio source separation with natural language queries presents a
promising paradigm for extracting arbitrary audio events through arbitrary text
descriptions. Existing methods mainly face two challenges, the difficulty in
jointly modeling acoustic-textual alignment and semantic-aware separation
within a blindly-learned single-stage architecture, and the reliance on
large-scale accurately-labeled training data to compensate for inefficient
cross-modal learning and separation. To address these challenges, we propose a
hierarchical decomposition framework, HSM-TSS, that decouples the task into
global-local semantic-guided feature separation and structure-preserving
acoustic reconstruction. Our approach introduces a dual-stage mechanism for
semantic separation, operating on distinct global and local semantic feature
spaces. We first perform global-semantic separation through a global semantic
feature space aligned with text queries. A Q-Audio architecture is employed to
align audio and text modalities, serving as pretrained global-semantic
encoders. Conditioned on the predicted global feature, we then perform the
second-stage local-semantic separation on AudioMAE features that preserve
time-frequency structures, followed by acoustic reconstruction. We also propose
an instruction processing pipeline to parse arbitrary text queries into
structured operations, extraction or removal, coupled with audio descriptions,
enabling flexible sound manipulation. Our method achieves state-of-the-art
separation performance with data-efficient training while maintaining superior
semantic consistency with queries in complex auditory scenes.

</details>


### [300] [Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation](https://arxiv.org/abs/2505.20745)
*Jingping Nie,Dung T. Tran,Karan Thakkar,Vasudha Kowtha,John Huang,Carlos Avendano,Erdrin Azemi,Vikramjit Mitra*

Main category: cs.SD

TL;DR: This paper explores the extent to which auscultation is encoded in pre-trained acoustic representation foundation models (FMs) by conducting a layer-wise investigation using a PCG dataset and HR estimation model. It finds that FM representations offer comparable performance to baseline methods, with the in-house CLAP model outperforming the baseline for HR estimation.


<details>
  <summary>Details</summary>
Motivation: To understand how well auscultation information, particularly heart sounds, is captured in pre-trained acoustic representation FMs.

Method: A layer-wise investigation of six acoustic representation FMs was conducted using a publicly available PCG dataset and an HR estimation model. The study compared these FMs' performance against a baseline method based on acoustic features.

Result: Overall, pre-trained FM representations performed comparably to the baseline method for HR estimation. Notably, the in-house CLAP model surpassed the baseline's performance, achieving lower MAE scores despite a domain mismatch.

Conclusion: Pre-trained acoustic representation FMs, especially the in-house CLAP model, effectively encode auscultation information and can be valuable for acoustics-based vital signs analysis.

Abstract: Auscultation, particularly heart sound, is a non-invasive technique that
provides essential vital sign information. Recently, self-supervised acoustic
representation foundation models (FMs) have been proposed to offer insights
into acoustics-based vital signs. However, there has been little exploration of
the extent to which auscultation is encoded in these pre-trained FM
representations. In this work, using a publicly available phonocardiogram (PCG)
dataset and a heart rate (HR) estimation model, we conduct a layer-wise
investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,
Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP
model. Additionally, we implement the baseline method from Nie et al., 2024
(which relies on acoustic features) and show that overall, representation
vectors from pre-trained foundation models (FMs) offer comparable performance
to the baseline. Notably, HR estimation using the representations from the
audio encoder of the in-house CLAP model outperforms the results obtained from
the baseline, achieving a lower mean absolute error (MAE) across various
train/validation/test splits despite the domain mismatch.

</details>


### [301] [Model as Loss: A Self-Consistent Training Paradigm](https://arxiv.org/abs/2505.21156)
*Saisamarth Rajesh Phaye,Milos Cernak,Andrew Harper*

Main category: cs.SD

TL;DR: The paper proposes Model as Loss, a novel training paradigm for speech enhancement that uses the encoder from the same model as a loss function to guide training.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for speech enhancement rely on handcrafted loss functions or deep feature losses which often fail to capture subtle signal properties essential for optimal performance.

Method: Model as Loss paradigm leverages the encoder's task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal.

Result: Outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets.

Conclusion: Model as Loss is a novel and effective approach for speech enhancement that improves perceptual quality and robustness.

Abstract: Conventional methods for speech enhancement rely on handcrafted loss
functions (e.g., time or frequency domain losses) or deep feature losses (e.g.,
using WavLM or wav2vec), which often fail to capture subtle signal properties
essential for optimal performance. To address this, we propose Model as Loss, a
novel training paradigm that utilizes the encoder from the same model as a loss
function to guide the training.
  The Model as Loss paradigm leverages the encoder's task-specific feature
space, optimizing the decoder to produce output consistent with perceptual and
task-relevant characteristics of the clean signal. By using the encoder's
learned features as a loss function, this framework enforces self-consistency
between the clean reference speech and the enhanced model output. Our approach
outperforms pre-trained deep feature losses on standard speech enhancement
benchmarks, offering better perceptual quality and robust generalization to
both in-domain and out-of-domain datasets.

</details>


### [302] [Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning](https://arxiv.org/abs/2505.21356)
*Whenty Ariyanti,Kuan-Yu Chen,Sabato Marco Siniscalchi,Hsin-Min Wang,Yu Tsao*

Main category: cs.SD

TL;DR: The paper introduces VOQANet and VOQANet+, deep learning frameworks for automated voice quality assessment. VOQANet+ combines Speech Foundation Model embeddings with handcrafted acoustic features, improving performance, interpretability, and robustness to noise.


<details>
  <summary>Details</summary>
Motivation: Perceptual voice quality assessment traditionally relies on subjective expert evaluations using scales like CAPE-V and GRBAS, which are prone to variability between raters. This necessitates the development of automated and objective methods.

Method: VOQANet is a deep learning framework incorporating an attention mechanism and Speech Foundation Model (SFM) to capture high-level acoustic and prosodic information from raw speech. VOQANet+ enhances this by integrating handcrafted acoustic features such as jitter, shimmer, and HNR with SFM embeddings.

Result: Sentence-based inputs perform better than vowel-based ones, particularly in patient-level assessments. VOQANet outperforms baseline methods in RMSE and PCC metrics, while VOQANet+ offers superior performance and maintains robustness in noisy conditions.

Conclusion: Integrating SFM embeddings with domain-specific acoustic features improves interpretability and resilience. VOQANet+ has significant potential for real-world applications, including telehealth, offering an interpretable and noise-resistant alternative to subjective assessments.

Abstract: Objective: Perceptual voice quality assessment plays a critical role in
diagnosing and monitoring voice disorders by providing standardized evaluation
of vocal function. Traditionally, this process relies on expert raters
utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation
of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain
(GRBAS). However, these metrics are inherently subjective and susceptible to
inter-rater variability, motivating the need for automated and objective
assessment methods. Methods: We propose Voice Quality Assessment Network
(VOQANet), a deep learning-based framework with an attention mechanism that
leverages a Speech Foundation Model (SFM) to capture high-level acoustic and
prosodic information from raw speech. To enhance robustness and
interpretability, we present VOQANet+, which integrates handcrafted acoustic
features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM
embeddings. Results: Sentence-based input yields stronger performance than
vowel-based input, especially at the patient level. VOQANet consistently
outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even
better and maintains robustness under noisy conditions. Conclusion: Combining
SFM embeddings with domain-informed acoustic features improves interpretability
and resilience. Significance: VOQANet+ shows strong potential for deployment in
real-world and telehealth settings, addressing the limitations of subjective
perceptual assessments with an interpretable and noise-resilient solution.

</details>


### [303] [VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin](https://arxiv.org/abs/2505.21445)
*Zhiqi Ai,Meixuan Bao,Zhiyong Chen,Zhi Yang,Xinnuo Li,Shugong Xu*

Main category: cs.SD

TL;DR: VoxAging is a large-scale longitudinal dataset that addresses the impact of speaker aging on speaker verification systems, with data collected over 17 years from 293 speakers.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges posed by speaker aging in speaker verification systems and lack of longitudinal data.

Method: Collected VoxAging dataset from 293 speakers over 17 years with weekly recordings; analyzed effects of speaker aging, individual processes, and factors like age group and gender.

Result: Provides a comprehensive study on speaker aging phenomenon and its influence on speaker verification systems.

Conclusion: VoxAging dataset significantly contributes to research in speaker aging and its effects on advanced speaker verification systems.

Abstract: The performance of speaker verification systems is adversely affected by
speaker aging. However, due to challenges in data collection, particularly the
lack of sustained and large-scale longitudinal data for individuals, research
on speaker aging remains difficult. In this paper, we present VoxAging, a
large-scale longitudinal dataset collected from 293 speakers (226 English
speakers and 67 Mandarin speakers) over several years, with the longest time
span reaching 17 years (approximately 900 weeks). For each speaker, the data
were recorded at weekly intervals. We studied the phenomenon of speaker aging
and its effects on advanced speaker verification systems, analyzed individual
speaker aging processes, and explored the impact of factors such as age group
and gender on speaker aging research.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [304] [Scheduling with Uncertain Holding Costs and its Application to Content Moderation](https://arxiv.org/abs/2505.21331)
*Caner Gocmen,Thodoris Lykouris,Deeksha Sinha,Wentao Weng*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In content moderation for social media platforms, the cost of delaying the
review of a content is proportional to its view trajectory, which fluctuates
and is apriori unknown. Motivated by such uncertain holding costs, we consider
a queueing model where job states evolve based on a Markov chain with
state-dependent instantaneous holding costs. We demonstrate that in the
presence of such uncertain holding costs, the two canonical algorithmic
principles, instantaneous-cost ($c\mu$-rule) and expected-remaining-cost
($c\mu/\theta$-rule), are suboptimal. By viewing each job as a Markovian
ski-rental problem, we develop a new index-based algorithm,
Opportunity-adjusted Remaining Cost (OaRC), that adjusts to the opportunity of
serving jobs in the future when uncertainty partly resolves. We show that the
regret of OaRC scales as $\tilde{O}(L^{1.5}\sqrt{N})$, where $L$ is the maximum
length of a job's holding cost trajectory and $N$ is the system size. This
regret bound shows that OaRC achieves asymptotic optimality when the system
size $N$ scales to infinity. Moreover, its regret is independent of the
state-space size, which is a desirable property when job states contain
contextual information. We corroborate our results with an extensive simulation
study based on two holding cost patterns (online ads and user-generated
content) that arise in content moderation for social media platforms. Our
simulations based on synthetic and real datasets demonstrate that OaRC
consistently outperforms existing practice, which is based on the two canonical
algorithmic principles.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [305] [Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software](https://arxiv.org/abs/2505.20303)
*David Hanson*

Main category: cs.SE

TL;DR: The paper explores the landscape of AI-generated code, identifying risks and challenges, while proposing solutions to improve transparency and functionality. It also examines long-term implications on artificial general intelligence and human-AI interaction.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study is the rapid expansion of AI-generated code in software development processes, which necessitates an examination of transparency and safety issues.

Method: The study analyzes market opportunities, discusses challenges related to complexity management, and proposes solutions to enhance transparency and functionality analysis.

Result: The analysis reveals potential risks associated with AI-generated code and highlights the importance of addressing these risks through proactive measures.

Conclusion: Proactive measures are crucial for ensuring the responsible development and deployment of AI in software engineering.

Abstract: As artificial intelligence becomes increasingly integrated into software
development processes, the prevalence and sophistication of AI-generated code
continue to expand rapidly. This study addresses the critical need for
transparency and safety in AI generated code by examining the current
landscape, identifying potential risks, and exploring future implications. We
analyze market opportunities for detecting AI-generated code, discuss the
challenges associated with managing increasing complexity, and propose
solutions to enhance transparency and functionality analysis. Furthermore, this
study investigates the longterm implications of AI generated code, including
its potential role in the development of artificial general intelligence and
its impact on human AI interaction. In conclusion, we emphasize the importance
of proactive measures for ensuring the responsible development and deployment
of AI in software engineering.

</details>


### [306] [Evaluating the Energy-Efficiency of the Code Generated by LLMs](https://arxiv.org/abs/2505.20324)
*Md Arman Islam,Devi Varaprasad Jonnala,Ritika Rekhi,Pratik Pokharel,Siddharth Cilamkoti,Asif Imran,Tevfik Kosar,Bekir Turkkan*

Main category: cs.SE

TL;DR: 尽管大型语言模型（LLMs）生成的代码在功能上通常是正确的，但其性能和能源效率往往低于人类编写的代码。研究发现DeepSeek-v3和GPT-4o生成的代码最节能，而Grok-2和Gemini-1.5-Pro则较差。对于某些算法类别，LLM生成的代码可能比人类编写的标准解决方案多消耗450倍的能量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）生成代码的质量提高，它们在软件行业中被越来越多地采用。然而，研究人员主要关注生成代码的功能正确性，而忽视了其能源效率和环境影响。

Method: 选取来自LeetCode平台的878个编程问题，涵盖不同难度级别和多样算法类别，比较20种流行LLM生成的代码与标准人类编写的解决方案在能源效率上的差异。

Result: 总体而言，LLM生成的代码虽然功能正确，但在性能和能源效率方面通常远不如人类编写的代码。具体来说，DeepSeek-v3和GPT-4o生成的代码最节能，而Grok-2和Gemini-1.5-Pro则表现最差。对于动态规划、回溯和位操作等特定算法类别，LLM生成的代码可能比人类编写的标准解决方案多消耗高达450倍的能量。

Conclusion: LLM生成的代码在能源效率方面仍有显著改进空间，特别是在涉及复杂算法的问题中。未来的研究应更加注重提升代码的能源效率以减少环境影响。

Abstract: As the quality of code generated by Large Language Models (LLMs) improves,
their adoption in the software industry for automated code generation continues
to grow. Researchers primarily focus on enhancing the functional correctness of
the generated code while commonly overlooking its energy efficiency and
environmental impact. This paper investigates the energy efficiency of the code
generated by 20 popular LLMs for 878 programming problems of varying difficulty
levels and diverse algorithmic categories selected from the LeetCode platform
by comparing them against canonical human-written solutions. Although LLMs can
produce functionally correct results in most cases, our findings show that the
performance and energy efficiency of LLM-produced solutions are often far below
those of human-written solutions. Among the studied LLMs, DeepSeek-v3 and
GPT-4o generate the most energy-efficient code, whereas Grok-2 and
Gemini-1.5-Pro are among the least energy-efficient models. On average,
human-generated canonical solutions are approximately 1.17 times more energy
efficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and
over 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific
algorithmic groups such as dynamic programming, backtracking, and bit
manipulation, LLM-generated code can consume up to 450 times more energy than
human-generated canonical solutions.

</details>


### [307] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Main category: cs.SE

TL;DR: C-to-Rust转译对于现代化遗留C代码至关重要。本文介绍了CRUST-Bench，一个包含100个C存储库的数据集，每个存储库都配有手动编写的Rust接口和测试用例，用于验证转译的正确性。评估发现，即使是最佳模型（如OpenAI o1）也只能解决少量任务，表明安全且惯用的Rust生成仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估系统是否能将C代码转译为通过测试的安全Rust代码的数据集。

Method: 创建了一个名为CRUST-Bench的数据集，包含100个C存储库，每个存储库都配有手动编写的Rust接口和测试用例。这些接口和测试用例确保了转译代码的功能正确性和内存安全性。

Result: 评估结果显示，即使是最先进的模型（如OpenAI o1）也只能在单次尝试中解决15个任务，说明该领域仍面临重大挑战。

Conclusion: 改进CRUST-Bench上的表现将有助于开发更强大的转译系统，从而帮助迁移遗留代码库到像Rust这样的确保内存安全的语言。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>


### [308] [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks](https://arxiv.org/abs/2505.20854)
*Xin Zhou,Kisub Kim,Ting Zhang,Martin Weyssow,Luis F. Gomes,Guang Yang,David Lo*

Main category: cs.SE

TL;DR: SWE-Judge is a new evaluation metric that accurately assesses the correctness of software artifacts generated by LLMs, achieving high correlation with human judgments and comparable agreement levels.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating the correctness of generated software artifacts either require significant human effort or fail to accurately reflect actual correctness.

Method: SWE-Judge defines five distinct evaluation strategies as independent judges and uses a dynamic team selection mechanism to identify the most suitable subset of judges to produce a final correctness score.

Result: SWE-Judge consistently achieves higher correlation with human judgments across various benchmarks and tasks, with improvements ranging from 5.9% to 183.8% over existing automatic metrics. It also reaches agreement levels comparable to inter-annotator agreement in certain tasks.

Conclusion: SWE-Judge presents a scalable and reliable alternative to human evaluation for assessing the correctness of generated software artifacts.

Abstract: Large Language Models (LLMs) and other automated techniques have been
increasingly used to support software developers by generating software
artifacts such as code snippets, patches, and comments. However, accurately
assessing the correctness of these generated artifacts remains a significant
challenge. On one hand, human evaluation provides high accuracy but is
labor-intensive and lacks scalability. On the other hand, other existing
automatic evaluation metrics are scalable and require minimal human effort, but
they often fail to accurately reflect the actual correctness of generated
software artifacts.
  In this paper, we present SWE-Judge, the first evaluation metric for
LLM-as-Ensemble-Judge specifically designed to accurately assess the
correctness of generated software artifacts. SWE-Judge first defines five
distinct evaluation strategies, each implemented as an independent judge. A
dynamic team selection mechanism then identifies the most appropriate subset of
judges to produce a final correctness score through ensembling. We evaluate
SWE-Judge across a diverse set of software engineering (SE) benchmarks,
including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess.
These benchmarks span three SE tasks: code generation, automated program
repair, and code summarization. Experimental results demonstrate that SWE-Judge
consistently achieves a higher correlation with human judgments, with
improvements ranging from 5.9% to 183.8% over existing automatic metrics.
Furthermore, SWE-Judge reaches agreement levels with human annotators that are
comparable to inter-annotator agreement in code generation and program repair
tasks. These findings underscore SWE-Judge's potential as a scalable and
reliable alternative to human evaluation.

</details>


### [309] [Towards Conversational Development Environments: Using Theory-of-Mind and Multi-Agent Architectures for Requirements Refinement](https://arxiv.org/abs/2505.20973)
*Keheliya Gallaba,Ali Arabat,Dayi Lin,Mohammed Sayagh,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Foundation Models' challenge in capturing stakeholder requirements for software development is addressed by a novel FM-powered multi-agent system, AlignMind. It enhances FMs with Theory-of-Mind capabilities to consider the perspectives of software makers, allowing it to clarify stakeholder beliefs, desires, and intentions into refined requirements and actionable workflows. Evaluated across 150 use cases, this approach accurately captures stakeholder intents, improving the software development process and paving the way for intent-first development environments.


<details>
  <summary>Details</summary>
Motivation: Foundation Models have shown great potential in natural language tasks, but they struggle to accurately capture stakeholder requirements for software development.

Method: A new FM-powered multi-agent system called AlignMind is introduced. This system uses a cognitive architecture that enhances FMs with Theory-of-Mind capabilities, considering the mental states and perspectives of software makers. This enables the iterative clarification of stakeholder beliefs, desires, and intentions, translating them into refined requirements and actionable natural language workflows.

Result: Through evaluation covering 150 diverse use cases, the approach was demonstrated to accurately capture the intents and requirements of stakeholders, expressing them as specifications and step-by-step plans of action.

Conclusion: The work justifies investments in this area, laying the groundwork for future innovation in building intent-first development environments where software makers can collaborate with AIs to meet their needs.

Abstract: Foundation Models (FMs) have shown remarkable capabilities in various natural
language tasks. However, their ability to accurately capture stakeholder
requirements remains a significant challenge for using FMs for software
development. This paper introduces a novel approach that leverages an
FM-powered multi-agent system called AlignMind to address this issue. By having
a cognitive architecture that enhances FMs with Theory-of-Mind capabilities,
our approach considers the mental states and perspectives of software makers.
This allows our solution to iteratively clarify the beliefs, desires, and
intentions of stakeholders, translating these into a set of refined
requirements and a corresponding actionable natural language workflow in the
often-overlooked requirements refinement phase of software engineering, which
is crucial after initial elicitation. Through a multifaceted evaluation
covering 150 diverse use cases, we demonstrate that our approach can accurately
capture the intents and requirements of stakeholders, articulating them as both
specifications and a step-by-step plan of action. Our findings suggest that the
potential for significant improvements in the software development process
justifies these investments. Our work lays the groundwork for future innovation
in building intent-first development environments, where software makers can
seamlessly collaborate with AIs to create software that truly meets their
needs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [310] [Stochastic Preconditioning for Neural Field Optimization](https://arxiv.org/abs/2505.20473)
*Selena Ling,Merlin Nimier-David,Alec Jacobson,Nicholas Sharp*

Main category: cs.GR

TL;DR: 通过在训练过程中引入空间随机性，可以显著提高神经场的拟合效果，这种方法简单且高效，能够替代或超越定制的层次结构和频率空间构造。


<details>
  <summary>Details</summary>
Motivation: 神经场在视觉计算中具有高度有效性，但其训练过程需要改进以提升性能。

Method: 通过在训练过程中引入空间随机性，隐式地对神经场的模糊版本进行操作，并通过带有高斯分布偏移的采样来评估期望值。该方法处理边界条件并扩展到空间变化的模糊。

Result: 此方法在多种任务（如表面重建和辐射场）上表现出色，几乎匹配或超越了定制层次结构的性能，同时在没有现有层次结构的情况下提供了质量与鲁棒性的直接提升。

Conclusion: 引入空间随机性的方法简单易行，能有效提高神经场的收敛性和鲁棒性，且易于实现。

Abstract: Neural fields are a highly effective representation across visual computing.
This work observes that fitting these fields is greatly improved by
incorporating spatial stochasticity during training, and that this simple
technique can replace or even outperform custom-designed hierarchies and
frequency space constructions. The approach is formalized as implicitly
operating on a blurred version of the field, evaluated in-expectation by
sampling with Gaussian-distributed offsets. Querying the blurred field during
optimization greatly improves convergence and robustness, akin to the role of
preconditioners in numerical linear algebra. This implicit, sampling-based
perspective fits naturally into the neural field paradigm, comes at no
additional cost, and is extremely simple to implement. We describe the basic
theory of this technique, including details such as handling boundary
conditions, and extending to a spatially-varying blur. Experiments demonstrate
this approach on representations including coordinate MLPs, neural hashgrids,
triplanes, and more, across tasks including surface reconstruction and radiance
fields. In settings where custom-designed hierarchies have already been
developed, stochastic preconditioning nearly matches or improves their
performance with a simple and unified approach; in settings without existing
hierarchies it provides an immediate boost to quality and robustness.

</details>


### [311] [Structure from Collision](https://arxiv.org/abs/2505.21335)
*Takuhiro Kaneko*

Main category: cs.GR

TL;DR: The paper introduces Structure from Collision (SfC), a new task to estimate both visible and invisible 3D structures of objects using changes in appearance during collisions. A novel model, SfC-NeRF, is proposed which uses volume annealing to optimize the invisible internal structure.


<details>
  <summary>Details</summary>
Motivation: Current neural 3D representations can only estimate visible external structures from multiview images but struggle with identifying invisible internal structures hidden behind surfaces.

Method: The authors propose SfC-NeRF, a model that estimates the full structure (including invisible internal parts) of an object by analyzing appearance changes during collision through a video sequence under physical, appearance-preserving, and keyframe constraints. They also introduce volume annealing to avoid local optima.

Result: Extensive experiments on 115 diverse objects showed the properties of SfC and demonstrated the effectiveness of SfC-NeRF in estimating complex internal structures with various cavity shapes, locations, sizes, and material properties.

Conclusion: SfC-NeRF successfully addresses the challenge of estimating invisible internal structures, opening new possibilities for understanding object structures beyond what is visible.

Abstract: Recent advancements in neural 3D representations, such as neural radiance
fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate
estimation of 3D structures from multiview images. However, this capability is
limited to estimating the visible external structure, and identifying the
invisible internal structure hidden behind the surface is difficult. To
overcome this limitation, we address a new task called Structure from Collision
(SfC), which aims to estimate the structure (including the invisible internal
structure) of an object from appearance changes during collision. To solve this
problem, we propose a novel model called SfC-NeRF that optimizes the invisible
internal structure of an object through a video sequence under physical,
appearance (i.e., visible external structure)-preserving, and keyframe
constraints. In particular, to avoid falling into undesirable local optima
owing to its ill-posed nature, we propose volume annealing; that is, searching
for global optima by repeatedly reducing and expanding the volume. Extensive
experiments on 115 objects involving diverse structures (i.e., various cavity
shapes, locations, and sizes) and material properties revealed the properties
of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [312] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
*Hudson de Martim*

Main category: cs.CL

TL;DR: This article proposes an adaptation of Graph Retrieval Augmented Generation (Graph RAG) tailored for legal norms analysis, by combining structured knowledge graphs with contextually enriched text segments to address the complexity and volume of legal data. It integrates hierarchical structure and temporal evolution into knowledge graphs, advancing AI in Law.


<details>
  <summary>Details</summary>
Motivation: To effectively analyze and comprehend legal norms with their inherent complexity, vast volume, predefined hierarchical structure, extensive network of references, and multiple temporal versions.

Method: Adaptation of Graph Retrieval Augmented Generation (Graph RAG), integrating hierarchical structure, temporal evolution, and comprehensive Text Units into knowledge graphs for richer representations of legal knowledge.

Result: Promising solution for handling the complexity and interconnectedness of legal data, offering opportunities for more effective systems in legal research, legislative analysis, and decision support.

Conclusion: Graph RAG provides a significant advancement in the field of Artificial Intelligence applied to Law, enhancing capabilities for legal norm comprehension and analysis.

Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.

</details>


### [313] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
*Jarrod Ragsdale,Rajendra Boppana*

Main category: cs.CL

TL;DR: The paper introduces ShIOEnv, a tool that treats command construction as a Markov Decision Process to simulate CLI environments safely. By using context-free grammar and PPO, it improves sample efficiency and generates high-quality datasets used to fine-tune CodeT5, resulting in significant BLEU-4 improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for simulating CLI environments using pre-trained language models are limited by the need for large, frozen models or insufficient public datasets that lack crucial execution data such as exit codes, outputs, and environmental side effects.

Method: ShIOEnv casts command construction as a Markov Decision Process with state as the partially built sequence and actions appending arguments. It executes candidates and returns their exit status, output, and progress toward an objective. A context-free grammar derived from man pages masks invalid arguments, and random and PPO-optimized sampling of action spaces produce exploration strategies.

Result: Grammar masking and PPO significantly improve sample efficiency, producing higher quality datasets. Fine-tuning CodeT5 with policy-generated datasets resulted in 85% BLEU-4 improvements with grammar production constraints and an additional 26% improvement with PPO application.

Conclusion: ShIOEnv and the curated command behavior datasets have been released for future research, providing a valuable resource for enhancing the simulation of CLI environments.

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [314] [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/abs/2505.20315)
*Zhewei Yao,Guoheng Sun,Lukasz Borchmann,Zheyu Shen,Minghang Deng,Bohan Zhai,Hao Zhang,Ang Li,Yuxiong He*

Main category: cs.CL

TL;DR: The paper presents Arctic-Text2SQL-R1, a reinforcement learning framework that generates accurate and executable SQL from natural language using execution correctness as a reward signal. It achieves state-of-the-art results across multiple benchmarks without complex supervision or reward shaping.


<details>
  <summary>Details</summary>
Motivation: Translating natural language into SQL (Test2SQL) is a longstanding challenge where large language models struggle to produce correct and executable SQL for complex queries despite significant improvements in fluency.

Method: Arctic-Text2SQL-R1 uses reinforcement learning with a lightweight reward signal based solely on execution correctness. It avoids brittle intermediate supervision and complex reward shaping, promoting stable training and alignment with the end task. The method also includes carefully curated data, strong supervised initialization, and effective training practices.

Result: Arctic-Text2SQL-R1 achieves state-of-the-art execution accuracy across six diverse Test2SQL benchmarks, including the top position on the BIRD leaderboard. Notably, the 7B model outperforms prior 70B-class systems, demonstrating scalability and efficiency.

Conclusion: Extensive experiments and ablation studies provide practical guidance for future Test2SQL research. The framework demonstrates inference-time robustness through simple extensions like value retrieval and majority voting.

Abstract: Translating natural language into SQL (Test2SQL) is a longstanding challenge
at the intersection of natural language understanding and structured data
access. While large language models (LLMs) have significantly improved fluency
in SQL generation, producing correct and executable SQL--particularly for
complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a
reinforcement learning (RL) framework and model family designed to generate
accurate, executable SQL using a lightweight reward signal based solely on
execution correctness. Our approach avoids brittle intermediate supervision and
complex reward shaping, promoting stable training and alignment with the end
task. Combined with carefully curated data, strong supervised initialization,
and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art
execution accuracy across six diverse Test2SQL benchmarks, including the top
position on the BIRD leaderboard. Notably, our 7B model outperforms prior
70B-class systems, highlighting the framework's scalability and efficiency. We
further demonstrate inference-time robustness through simple extensions like
value retrieval and majority voting. Extensive experiments and ablation studies
offer both positive and negative insights, providing practical guidance for
future Test2SQL research.

</details>


### [315] [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/abs/2505.20318)
*Wang Cai,Hsiu-Yuan Huang,Zhixiang Wang,Yunfang Wu*

Main category: cs.CL

TL;DR: DyVec is proposed to overcome limitations of existing ICV methods by incorporating EQR strategy, dynamic latent segmentation and injection, and REINFORCE-based optimization. Experiments show DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines.


<details>
  <summary>Details</summary>
Motivation: Existing ICV methods are sensitive to ICL-specific factors, use coarse or fragmented representations, and rely on heuristic-based injection positions.

Method: DyVec incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations, applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity, and leverages REINFORCE-based optimization to learn optimal injection positions for each segment.

Result: DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Dynamically segmenting and injecting semantically aggregated latent representations proves effective.

Conclusion: DyVec provides a lightweight and data-efficient solution for inference-time task adaptation.

Abstract: In-Context derived Vector (ICV) methods extract task-relevant representations
from large language models (LLMs) and reinject them during inference, achieving
comparable performance to few-shot In-Context Learning (ICL) without repeated
demonstration processing. However, existing ICV methods remain sensitive to
ICL-specific factors, often use coarse or semantically fragmented
representations as the source of the vector, and rely on heuristic-based
injection positions, limiting their applicability.
  To address these issues, we propose Dynamic Vector (DyVec), which
incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust
semantically aggregated latent representations by mitigating variance
introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to
adaptively partition representations based on task complexity and leverages
REINFORCE-based optimization to learn optimal injection positions for each
segment.
  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior
ICV baselines. Further analysis highlights the effectiveness of dynamically
segmenting and injecting semantically aggregated latent representations. DyVec
provides a lightweight and data-efficient solution for inference-time task
adaptation.

</details>


### [316] [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/abs/2505.20320)
*Satya Narayana Cheetirala,Ganesh Raut,Dhavalkumar Patel,Fabio Sanatana,Robert Freeman,Matthew A Levin,Girish N. Nadkarni,Omar Dawkins,Reba Miller,Randolph M. Steinhagen,Eyal Klang,Prem Timsina*

Main category: cs.CL

TL;DR: The study investigates using RAG with only relevant text segments for long clinical text classification, showing comparable performance to processing entire texts while reducing token usage.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of long text classification in LLMs due to token limits and computational costs, by exploring if a RAG approach can match the performance of processing entire clinical notes.

Method: Split clinical documents into chunks, convert to vector embeddings, store in FAISS index, retrieve top 4,000 words related to classification query, and feed into LLMs for evaluation.

Result: No statistically significant differences in metrics (AUC ROC, precision, recall, F1) between RAG-based approach and whole-text processing (p > 0.05).

Conclusion: RAG can reduce token usage without sacrificing classification accuracy, offering a scalable and cost-effective solution for analyzing long clinical documents.

Abstract: Long text classification is challenging for Large Language Models (LLMs) due
to token limits and high computational costs. This study explores whether a
Retrieval Augmented Generation (RAG) approach using only the most relevant text
segments can match the performance of processing entire clinical notes with
large context LLMs. We begin by splitting clinical documents into smaller
chunks, converting them into vector embeddings, and storing these in a FAISS
index. We then retrieve the top 4,000 words most pertinent to the
classification query and feed these consolidated segments into an LLM. We
evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication
identification task. Metrics such as AUC ROC, precision, recall, and F1 showed
no statistically significant differences between the RAG based approach and
whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can
significantly reduce token usage without sacrificing classification accuracy,
providing a scalable and cost effective solution for analyzing lengthy clinical
documents.

</details>


### [317] [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
*Mathew J. Koretsky,Maya Willey,Adi Asija,Owen Bianchi,Chelsea X. Alvarado,Tanay Nayak,Nicole Kuznetsov,Sungwon Kim,Mike A. Nalls,Daniel Khashabi,Faraz Faghri*

Main category: cs.CL

TL;DR: BiomedSQL is a benchmark for evaluating scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. It reveals performance gaps among models and provides resources to advance text-to-SQL systems.


<details>
  <summary>Details</summary>
Motivation: Current text-to-SQL systems struggle with mapping complex scientific questions into executable SQL, especially when domain-specific reasoning is needed.

Method: Introduced BiomedSQL, a benchmark containing 68,000 question/SQL query/answer triples based on a real-world biomedical knowledge base. Evaluated various LLMs using different prompting strategies.

Result: GPT-o3-mini achieved 59.0% execution accuracy, custom BMSQL reached 62.6%, both significantly lower than the expert baseline of 90.0%.

Conclusion: BiomedSQL offers a new foundation for enhancing text-to-SQL systems to support scientific discovery through robust reasoning.

Abstract: Biomedical researchers increasingly rely on large-scale structured databases
for complex analytical tasks. However, current text-to-SQL systems often
struggle to map qualitative scientific questions into executable SQL,
particularly when implicit domain reasoning is required. We introduce
BiomedSQL, the first benchmark explicitly designed to evaluate scientific
reasoning in text-to-SQL generation over a real-world biomedical knowledge
base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in
a harmonized BigQuery knowledge base that integrates gene-disease associations,
causal inference from omics data, and drug approval records. Each question
requires models to infer domain-specific criteria, such as genome-wide
significance thresholds, effect directionality, or trial phase filtering,
rather than rely on syntactic translation alone. We evaluate a range of open-
and closed-source LLMs across prompting strategies and interaction paradigms.
Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%
execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,
both well below the expert baseline of 90.0%. BiomedSQL provides a new
foundation for advancing text-to-SQL systems capable of supporting scientific
discovery through robust reasoning over structured biomedical knowledge bases.
Our dataset is publicly available at
https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source
at https://github.com/NIH-CARD/biomedsql.

</details>


### [318] [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
*Mengru Wang,Ziwen Xu,Shengyu Mao,Shumin Deng,Zhaopeng Tu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: This paper proposes Steering Target Atoms (STA), a method to isolate and manipulate disentangled knowledge components in language models for enhancing safety and precise control.


<details>
  <summary>Details</summary>
Motivation: Current methods of prompt engineering and steering face challenges due to the intertwined internal representations in large language models, which can limit control precision and cause unintended side effects. Existing solutions using sparse autoencoders have been limited to simple tasks.

Method: The proposed method, Steering Target Atoms (STA), isolates and manipulates disentangled knowledge components within high-dimensional spaces to improve safety and control precision in language model generation.

Result: Experiments show that STA is effective in enhancing safety and control precision. The approach exhibits robustness and flexibility, especially in adversarial scenarios, and is also effective in controlling reasoning in large models.

Conclusion: Steering Target Atoms provide a novel way to enhance safety and reliability in language models by effectively manipulating disentangled knowledge components.

Abstract: Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.

</details>


### [319] [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/abs/2505.20323)
*Shahriar Noroozizadeh,Sayantan Kumar,George H. Chen,Jeremy C. Weiss*

Main category: cs.CL

TL;DR: This paper introduces PMOA-TTS, a large-scale dataset of clinical timelines derived from PubMed Open Access case reports using an LLM-based pipeline. The dataset enables temporal reasoning and longitudinal modeling in biomedical NLP.


<details>
  <summary>Details</summary>
Motivation: Current limitations in large-scale temporally annotated resources for understanding temporal dynamics in clinical narratives motivate the creation of a new dataset.

Method: The method involves converting PubMed Open Access case reports into structured (event, time) timelines via a scalable LLM-based pipeline, including heuristic filtering and prompt-driven extraction with Llama 3.3 and DeepSeek R1.

Result: The dataset contains over 5.6 million timestamped clinical events, shows wide diagnostic and demographic coverage, and achieves high-quality metrics in timeline evaluation. In survival prediction tasks, it demonstrates predictive value with concordance indices up to 0.82 ± 0.01.

Conclusion: PMOA-TTS serves as a valuable resource for timeline extraction, temporal reasoning, and longitudinal modeling in biomedical NLP.

Abstract: Understanding temporal dynamics in clinical narratives is essential for
modeling patient trajectories, yet large-scale temporally annotated resources
remain limited. We present PMOA-TTS, the first openly available dataset of
124,699 PubMed Open Access (PMOA) case reports, each converted into structured
(event, time) timelines via a scalable LLM-based pipeline. Our approach
combines heuristic filtering with Llama 3.3 to identify single-patient case
reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,
resulting in over 5.6 million timestamped clinical events. To assess timeline
quality, we evaluate against a clinician-curated reference set using three
metrics: (i) event-level matching (80% match at a cosine similarity threshold
of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the
Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide
diagnostic and demographic coverage. In a downstream survival prediction task,
embeddings from extracted timelines achieve time-dependent concordance indices
up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally
structured narratives. PMOA-TTS provides a scalable foundation for timeline
extraction, temporal reasoning, and longitudinal modeling in biomedical NLP.
The dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .

</details>


### [320] [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)
*Amirhosein Ghasemabadi,Keith G. Mills,Baochun Li,Di Niu*

Main category: cs.CL

TL;DR: The paper presents Guided by Gut (GG), an efficient self-guided Test-Time Scaling framework for enhancing LLM reasoning without relying on external verifier models. GG improves internal confidence estimates through reinforcement learning and achieves high performance with significantly reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing LLM reasoning, such as using Process Reward Models or Best-of-N sampling, are computationally expensive. The authors aim to develop a more efficient approach that can match or surpass the performance of these methods while reducing resource requirements.

Method: Guided by Gut (GG) is a self-guided TTS framework that uses a lightweight tree search guided by intrinsic LLM signals like token-level confidence and step novelty. It includes a reinforcement learning fine-tuning phase to improve the reliability of internal confidence estimates.

Result: Empirical evaluations show that GG allows smaller models (1.5B parameters) to achieve accuracy comparable to or better than much larger models (32B-70B parameters). It reduces GPU memory usage by up to 10x compared to PRM-based methods, infers 8x faster with 4-5x lower memory usage, and cuts KV cache memory usage by about 50% compared to BoN strategy.

Conclusion: GG provides an efficient alternative to existing LLM reasoning enhancement methods, achieving high performance with significantly reduced computational resources.

Abstract: Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)
reasoning often incur substantial computational costs, primarily due to
extensive reliance on external Process Reward Models (PRMs) or sampling methods
like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient
self-guided TTS framework that achieves PRM-level performance without costly
external verifier models. Our method employs a lightweight tree search guided
solely by intrinsic LLM signals, token-level confidence and step novelty. One
critical innovation is improving the reliability of internal confidence
estimates via a targeted reinforcement learning fine-tuning phase. Empirical
evaluations on challenging mathematical reasoning benchmarks demonstrate that
GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching
or surpassing significantly larger models (e.g., 32B-70B parameters), while
reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG
achieves comparable accuracy with 8x faster inference speeds and 4-5x lower
memory usage. Additionally, GG reduces KV cache memory usage by approximately
50% compared to the BoN strategy, facilitating more efficient and practical
deployment of TTS techniques.

</details>


### [321] [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/abs/2505.20333)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: The paper presents a Multi_Scale Manifold Alignment framework to enhance the interpretability of LLMs by decomposing their latent space into global, intermediate and local semantic manifolds.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in Large Language Models (LLMs) which limits trust in critical applications.

Method: Proposes a Multi_Scale Manifold Alignment framework that uses cross_scale mapping functions with geometric alignment and information preservation, curvature regularization, and hyperparameter tuning.

Result: Theoretical analysis demonstrates that alignment error can be bounded under mild assumptions, providing a unified explanation of how LLMs structure multi-scale semantics.

Conclusion: This framework advances interpretability of LLMs and enables applications such as bias detection and robustness enhancement.

Abstract: Recent advances in Large Language Models (LLMs) have achieved strong
performance, yet their internal reasoning remains opaque, limiting
interpretability and trust in critical applications. We propose a novel
Multi_Scale Manifold Alignment framework that decomposes the latent space into
global, intermediate, and local semantic manifolds capturing themes, context,
and word-level details. Our method introduces cross_scale mapping functions
that jointly enforce geometric alignment (e.g., Procrustes analysis) and
information preservation (via mutual information constraints like MINE or VIB).
We further incorporate curvature regularization and hyperparameter tuning for
stable optimization. Theoretical analysis shows that alignment error, measured
by KL divergence, can be bounded under mild assumptions. This framework offers
a unified explanation of how LLMs structure multi-scale semantics, advancing
interpretability and enabling applications such as bias detection and
robustness enhancement.

</details>


### [322] [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/abs/2505.20334)
*Yixuan Wang,Shiyu Ji,Yijun Liu,Yuzhuang Xu,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper introduces Lookahead Q-Cache (LAQ), a novel KV cache eviction framework for large language models that generates pseudo lookahead queries to improve decoding efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache eviction methods for large language models have inconsistencies with actual inference queries, particularly under tight memory budgets.

Method: Proposes Lookahead Q-Cache (LAQ), which generates low-cost pseudo lookahead queries to approximate true decoding-stage queries, improving the consistency and accuracy of KV cache eviction.

Result: LAQ outperforms existing methods across various budget levels on LongBench and Needle-in-a-Haystack benchmarks, achieving a 1-4 point improvement on LongBench under limited cache budget.

Conclusion: LAQ is complementary to existing approaches and can be flexibly combined for further improvements.

Abstract: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate
decoding by reducing redundant computations. However, the KV cache memory usage
grows substantially with longer text sequences, posing challenges for efficient
deployment. Existing KV cache eviction methods prune tokens using
prefilling-stage attention scores, causing inconsistency with actual inference
queries, especially under tight memory budgets. In this paper, we propose
Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost
pseudo lookahead queries to better approximate the true decoding-stage queries.
By using these lookahead queries as the observation window for importance
estimation, LAQ achieves more consistent and accurate KV cache eviction aligned
with real inference scenarios. Experimental results on LongBench and
Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods
across various budget levels, achieving a 1 $\sim$ 4 point improvement on
LongBench under limited cache budget. Moreover, LAQ is complementary to
existing approaches and can be flexibly combined to yield further improvements.

</details>


### [323] [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/abs/2505.20335)
*Zishun Yu,Shangzhe Li,Xinhua Zhang*

Main category: cs.CL

TL;DR: Large language models have made significant progress in NLP tasks but are computationally expensive. Distillation compresses these models into smaller ones. This paper introduces a general framework for temporal difference-based distillation using the distributional sparsity of the teacher model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational costs associated with large language models by introducing a more efficient distillation method that leverages the distributional sparsity of the teacher model.

Method: A general framework for temporal difference-based distillation is introduced, exploiting the distributional sparsity of the teacher model where most probability mass is assigned to a small subset of tokens.

Result: Practical algorithms derived from this framework show performance improvements.

Conclusion: This new framework provides an effective way to distill large language models into smaller, more efficient ones while maintaining or improving performance.

Abstract: Large language models have led to significant progress across many NLP tasks,
although their massive sizes often incur substantial computational costs.
Distillation has become a common practice to compress these large and highly
capable models into smaller, more efficient ones. Many existing language model
distillation methods can be viewed as behavior cloning from the perspective of
imitation learning or inverse reinforcement learning. This viewpoint has
inspired subsequent studies that leverage (inverse) reinforcement learning
techniques, including variations of behavior cloning and temporal difference
learning methods. Rather than proposing yet another specific temporal
difference method, we introduce a general framework for temporal
difference-based distillation by exploiting the distributional sparsity of the
teacher model. Specifically, it is often observed that language models assign
most probability mass to a small subset of tokens. Motivated by this
observation, we design a temporal difference learning framework that operates
on a reduced action space (a subset of vocabulary), and demonstrate how
practical algorithms can be derived and the resulting performance improvements.

</details>


### [324] [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/abs/2505.20336)
*Yu Zhang,Wanli Jiang,Zhengyu Yang*

Main category: cs.CL

TL;DR: The paper proposes MOSLIM, a new multi-objective alignment method for Large Language Models (LLMs) using a single reward model and policy model. It leverages a multi-head reward model classifying QA pairs and optimizes via a mapping function without preference training during SFT phase.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current methods that involve multiple policies or reward models for diverse human preferences or require preference-specific supervised fine-tuning (SFT) models.

Method: MOSLIM uses a single reward model and policy model with a flexible prompting mechanism. It incorporates a multi-head reward model that classifies QA pairs instead of scoring them and optimizes the policy model through a scalar reward derived from a mapping function converting classification results into reward scores.

Result: MOSLIM outperforms existing multi-objective approaches in most results across several benchmarks while consuming significantly fewer GPU computing resources compared to other policy optimization methods.

Conclusion: MOSLIM provides an efficient and effective solution for multi-objective alignment of LLMs, reducing resource requirements and enhancing performance.

Abstract: The multi-objective alignment of Large Language Models (LLMs) is essential
for ensuring foundational models conform to diverse human preferences. Current
research in this field typically involves either multiple policies or multiple
reward models customized for various preferences, or the need to train a
preference-specific supervised fine-tuning (SFT) model. In this work, we
introduce a novel multi-objective alignment method, MOSLIM, which utilizes a
single reward model and policy model to address diverse objectives. MOSLIM
provides a flexible way to control these objectives through prompting and does
not require preference training during SFT phase, allowing thousands of
off-the-shelf models to be directly utilized within this training framework.
MOSLIM leverages a multi-head reward model that classifies question-answer
pairs instead of scoring them and then optimize policy model with a scalar
reward derived from a mapping function that converts classification results
from reward model into reward scores. We demonstrate the efficacy of our
proposed method across several multi-objective benchmarks and conduct ablation
studies on various reward model sizes and policy optimization methods. The
MOSLIM method outperforms current multi-objective approaches in most results
while requiring significantly fewer GPU computing resources compared with
existing policy optimization methods.

</details>


### [325] [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/abs/2505.20338)
*Cayo Viegas,Rohit Gheyi,Márcio Ribeiro*

Main category: cs.CL

TL;DR: The study explores LLMs' performance on the POSCOMP computer science exam, finding that while they excel at text-based questions, image interpretation remains challenging.


<details>
  <summary>Details</summary>
Motivation: To evaluate the practical utility and guide future developments of LLMs in specialized domains like computer science by testing their proficiency against a challenging benchmark.

Method: Four initial LLMs (ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, Le Chat Mistral Large) were evaluated on the 2022 and 2023 POSCOMP exams, with further analysis including newer models (o1, Gemini 2.5 Pro, Claude 3.7 Sonnet, o3-mini-high) on exams from 2022-2024.

Result: LLMs performed better on text-based questions than on image interpretation tasks, with ChatGPT-4 leading in both years. Newer models showed consistent improvements and surpassed human participants across all three years.

Conclusion: LLMs, especially advanced ones like ChatGPT-4, demonstrate significant potential in handling complex computer science questions, though challenges remain in areas such as image interpretation.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
expanded the capabilities of artificial intelligence in natural language
processing tasks. Despite this progress, their performance in specialized
domains such as computer science remains relatively unexplored. Understanding
the proficiency of LLMs in these domains is critical for evaluating their
practical utility and guiding future developments. The POSCOMP, a prestigious
Brazilian examination used for graduate admissions in computer science promoted
by the Brazlian Computer Society (SBC), provides a challenging benchmark. This
study investigates whether LLMs can match or surpass human performance on the
POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and
Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP
exams. The assessments measured the models' proficiency in handling complex
questions typical of the exam. LLM performance was notably better on text-based
questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led
with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced
(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were
observed in the 2023 exam. ChatGPT-4 achieved the highest performance,
surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly
ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image
interpretation remains a challenge. Given the rapid evolution of LLMs, we
expanded our analysis to include more recent models - o1, Gemini 2.5 Pro,
Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.
These newer models demonstrate further improvements and consistently surpass
both the average and top-performing human participants across all three years.

</details>


### [326] [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/abs/2505.20340)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: The paper introduces Dynamic Manifold Evolution Theory (DMET), which models large language model generation as a controlled dynamical system evolving on a low-dimensional semantic manifold.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework that can explain and control the text generation process of large language models by viewing it as a dynamical system on a semantic manifold.

Method: Modeling large language model generation using dynamic systems theory, specifically casting latent state updates as discrete time Euler approximations of continuous dynamics. Mapping intrinsic energy-driven flows and context-dependent forces onto Transformer components and leveraging Lyapunov stability theory to define empirical metrics for evaluating latent trajectory properties.

Result: Defined three empirical metrics that quantitatively link latent trajectory properties to text fluency, grammaticality, and semantic coherence. Experiments validate DMET's predictions and yield guidelines for balancing creativity and consistency in text generation.

Conclusion: DMET provides a principled approach to understanding and controlling the balance between creativity and consistency in text generation by large language models.

Abstract: We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework
that models large language model generation as a controlled dynamical system
evolving on a low_dimensional semantic manifold. By casting latent_state
updates as discrete time Euler approximations of continuous dynamics, we map
intrinsic energy_driven flows and context_dependent forces onto Transformer
components (residual connections, attention, feed-forward networks). Leveraging
Lyapunov stability theory We define three empirical metrics (state continuity,
clustering quality, topological persistence) that quantitatively link
latent_trajectory properties to text fluency, grammaticality, and semantic
coherence. Extensive experiments across decoding parameters validate DMET's
predictions and yield principled guidelines for balancing creativity and
consistency in text generation.

</details>


### [327] [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/abs/2505.20343)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.CL

TL;DR: The paper investigates gender bias in LLMs, defines entropy bias, and creates a new benchmark dataset. It finds no significant category-level bias but notable differences at the individual question level. A debiasing approach merging gendered responses improves information content.


<details>
  <summary>Details</summary>
Motivation: To explore the existence and persistence of gender bias in popular LLMs and to address discrepancies in information generated for men and women in real-world questions.

Method: Developed a benchmark dataset called RealWorldQuestioning, defined entropy bias, tested four different LLMs, and evaluated responses qualitatively and quantitatively using ChatGPT-4o as 'LLM-as-judge'. Proposed a debiasing approach that merges responses for two genders iteratively.

Result: No significant bias found at category level but substantial differences at individual question level. The debiasing approach improved information content in 78% cases and achieved balanced integration in remaining cases.

Conclusion: Entropy bias exists in LLMs at finer granularity. A simple prompt-based debiasing strategy can effectively reduce bias and improve response quality.

Abstract: We investigate the existence and persistence of a specific type of gender
bias in some of the popular LLMs and contribute a new benchmark dataset,
RealWorldQuestioning (released on HuggingFace ), developed from real-world
questions across four key domains in business and health contexts: education,
jobs, personal financial management, and general health. We define and study
entropy bias, which we define as a discrepancy in the amount of information
generated by an LLM in response to real questions users have asked. We tested
this using four different LLMs and evaluated the generated responses both
qualitatively and quantitatively by using ChatGPT-4o (as "LLM-as-judge"). Our
analyses (metric-based comparisons and "LLM-as-judge" evaluation) suggest that
there is no significant bias in LLM responses for men and women at a category
level. However, at a finer granularity (the individual question level), there
are substantial differences in LLM responses for men and women in the majority
of cases, which "cancel" each other out often due to some responses being
better for males and vice versa. This is still a concern since typical users of
these tools often ask a specific question (only) as opposed to several varied
ones in each of these common yet important areas of life. We suggest a simple
debiasing approach that iteratively merges the responses for the two genders to
produce a final result. Our approach demonstrates that a simple, prompt-based
debiasing strategy can effectively debias LLM outputs, thus producing responses
with higher information content than both gendered variants in 78% of the
cases, and consistently achieving a balanced integration in the remaining
cases.

</details>


### [328] [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347)
*Wenkai Fang,Shunyu Liu,Yang Zhou,Kongcheng Zhang,Tongya Zheng,Kaixuan Chen,Mingli Song,Dacheng Tao*

Main category: cs.CL

TL;DR: The paper introduces Self-play Reinforcement Learning (SeRL), a method to enhance LLM training with limited initial data via self-instruction and self-rewarding modules, showing superior results in reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of requiring high-quality instructions and verifiable rewards for effective RL training of LLMs, especially in specialized domains where such resources are scarce.

Method: SeRL consists of two key modules: 1) self-instruction which generates additional instructions with online filtering for quality assurance, and 2) self-rewarding which uses majority-voting to estimate response rewards without external annotations. Then it performs conventional RL using this generated data.

Result: SeRL outperforms existing methods on various reasoning benchmarks across different LLM backbones, achieving performance comparable to that obtained using high-quality data and verifiable rewards.

Conclusion: SeRL is an effective approach for bootstrapping LLM training with limited initial data, providing a solution for specialized domains lacking high-quality instructions and rewards.

Abstract: Recent advances have demonstrated the effectiveness of Reinforcement Learning
(RL) in improving the reasoning capabilities of Large Language Models (LLMs).
However, existing works inevitably rely on high-quality instructions and
verifiable rewards for effective training, both of which are often difficult to
obtain in specialized domains. In this paper, we propose Self-play
Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial
data. Specifically, SeRL comprises two complementary modules: self-instruction
and self-rewarding. The former module generates additional instructions based
on the available data at each training step, employing robust online filtering
strategies to ensure instruction quality, diversity, and difficulty. The latter
module introduces a simple yet effective majority-voting mechanism to estimate
response rewards for additional instructions, eliminating the need for external
annotations. Finally, SeRL performs conventional RL based on the generated
data, facilitating iterative self-play learning. Extensive experiments on
various reasoning benchmarks and across different LLM backbones demonstrate
that the proposed SeRL yields results superior to its counterparts and achieves
performance on par with those obtained by high-quality data with verifiable
rewards. Our code is available at https://github.com/wantbook-book/SeRL.

</details>


### [329] [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
*Juntong Wu,Zijing Liu,He Cao,Hao Li,Bin Feng,Zishan Shu,Ke Yu,Li Yuan,Yu Li*

Main category: cs.CL

TL;DR: In recent years, protein-text models have been receiving significant attention for their potential in protein generation and understanding. However, current benchmarks have data leakage issues and conventional metrics from NLP are not accurate enough to evaluate model performance in this domain. To address these problems, the researchers reorganized existing datasets and proposed a new evaluation framework based on biological entities. They also introduced a retrieval-enhanced method that surpasses fine-tuned LLMs in protein-to-text generation tasks and performs well in training-free scenarios.


<details>
  <summary>Details</summary>
Motivation: Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. But there are significant data leakage issues present in current benchmarks and conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain.

Method: The researchers reorganized existing datasets and introduced a novel evaluation framework based on biological entities. They proposed a retrieval-enhanced method for protein-to-text generation.

Result: The retrieval-enhanced method significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios.

Conclusion: This work addresses the limitations in current benchmarks and metrics for evaluating protein-text models. The proposed retrieval-enhanced method provides a more effective approach for protein-to-text generation.

Abstract: In recent years, protein-text models have gained significant attention for
their potential in protein generation and understanding. Current approaches
focus on integrating protein-related knowledge into large language models
through continued pretraining and multi-modal alignment, enabling simultaneous
comprehension of textual descriptions and protein sequences. Through a thorough
analysis of existing model architectures and text-based protein understanding
benchmarks, we identify significant data leakage issues present in current
benchmarks. Moreover, conventional metrics derived from natural language
processing fail to accurately assess the model's performance in this domain. To
address these limitations, we reorganize existing datasets and introduce a
novel evaluation framework based on biological entities. Motivated by our
observation, we propose a retrieval-enhanced method, which significantly
outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy
and efficiency in training-free scenarios. Our code and data can be seen at
https://github.com/IDEA-XL/RAPM.

</details>


### [330] [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/abs/2505.20416)
*Zihong Chen,Wanli Jiang,Jinzhe Li,Zhonghang Yuan,Huanjun Kong,Wanli Ouyang,Nanqing Dong*

Main category: cs.CL

TL;DR: GraphGen is a knowledge graph-guided framework that addresses the challenges of synthetic data generation for fine-tuning large language models (LLMs). It constructs fine-grained knowledge graphs, identifies knowledge gaps, and generates high-value QA pairs. GraphGen performs better than conventional methods in closed-book settings.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs requires a lot of high-quality supervised data which is expensive to obtain. Current synthetic data approaches have issues with factual inaccuracies, insufficient coverage, simplistic knowledge structures, and homogenized outputs.

Method: GraphGen constructs a fine-grained knowledge graph from source text, uses expected calibration error metric to identify knowledge gaps in LLMs, prioritizes generation of QA pairs targeting valuable long-tail knowledge, incorporates multi-hop neighborhood sampling to capture complex relations, and employs style-controlled generation for diversifying QA data.

Result: GraphGen outperforms conventional synthetic data methods on knowledge-intensive tasks under closed-book settings.

Conclusion: GraphGen provides a more reliable and comprehensive solution to the data scarcity challenge in supervised fine-tuning.

Abstract: Fine-tuning for large language models (LLMs) typically requires substantial
amounts of high-quality supervised data, which is both costly and
labor-intensive to acquire. While synthetic data generation has emerged as a
promising solution, existing approaches frequently suffer from factual
inaccuracies, insufficient long-tail coverage, simplistic knowledge structures,
and homogenized outputs. To address these challenges, we introduce GraphGen, a
knowledge graph-guided framework designed for three key question-answering (QA)
scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by
constructing a fine-grained knowledge graph from the source text. It then
identifies knowledge gaps in LLMs using the expected calibration error metric,
prioritizing the generation of QA pairs that target high-value, long-tail
knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling
to capture complex relational information and employs style-controlled
generation to diversify the resulting QA data. Experimental results on
knowledge-intensive tasks under closed-book settings demonstrate that GraphGen
outperforms conventional synthetic data methods, offering a more reliable and
comprehensive solution to the data scarcity challenge in supervised
fine-tuning. The code and data are publicly available at
https://github.com/open-sciencelab/GraphGen.

</details>


### [331] [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)
*Arvindh Arun,Sumit Kumar,Mojtaba Nayyeri,Bo Xiong,Ponnurangam Kumaraguru,Antonio Vergari,Steffen Staab*

Main category: cs.CL

TL;DR: SEMMA is a dual-module Knowledge Graph Foundation Model that integrates textual semantics with graph structure using Large Language Models, improving zero-shot reasoning and link prediction on unseen graphs.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Graph Foundation Models focus mainly on graph structures, neglecting the semantic information in textual attributes which could be crucial for better generalization.

Method: SEMMA uses Large Language Models to create semantic embeddings from relation identifiers, forming a textual relation graph that complements the structural graph. This dual approach allows for better transfer of knowledge patterns.

Result: SEMMA outperforms purely structural models like ULTRA in fully inductive link prediction across 54 diverse knowledge graphs. Especially in challenging generalization tasks where test-time relations are unseen, SEMMA is twice as effective.

Conclusion: Textual semantics are vital for generalization in scenarios where structural information alone is insufficient. Future foundation models should integrate both structural and linguistic signals for enhanced knowledge reasoning.

Abstract: Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling
zero-shot reasoning over unseen graphs by learning transferable patterns.
However, most existing KGFMs rely solely on graph structure, overlooking the
rich semantic signals encoded in textual attributes. We introduce SEMMA, a
dual-module KGFM that systematically integrates transferable textual semantics
alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich
relation identifiers, generating semantic embeddings that subsequently form a
textual relation graph, which is fused with the structural component. Across 54
diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully
inductive link prediction. Crucially, we show that in more challenging
generalization settings, where the test-time relation vocabulary is entirely
unseen, structural methods collapse while SEMMA is 2x more effective. Our
findings demonstrate that textual semantics are critical for generalization in
settings where structure alone fails, highlighting the need for foundation
models that unify structural and linguistic signals in knowledge reasoning.

</details>


### [332] [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
*Zhaolin Li,Jan Niehues*

Main category: cs.CL

TL;DR: The paper explores the ability of large language models (LLMs) to learn unseen, low-resource languages through in-context learning (ICL) for Automatic Speech Recognition (ASR). With experiments on four endangered languages, it finds that providing more relevant text samples enhances performance in both language modelling and ASR tasks. A probability-based approach outperforms the traditional instruction-based approach in language learning. ICL enables LLMs to achieve ASR performance comparable to or surpassing dedicated language models while preserving their original capabilities.


<details>
  <summary>Details</summary>
Motivation: Current LLMs only support a small subset of the approximately 7,000 languages spoken worldwide. The research aims to extend the investigation into whether LLMs can learn new, low-resource languages for speech recognition without supervised data.

Method: The study conducts experiments on four diverse endangered languages not included in LLM training data. It evaluates the impact of providing more relevant text samples on both language modeling and ASR tasks, compares probability-based and instruction-based approaches for language learning, and assesses the ASR performance of LLMs using ICL against dedicated language models.

Result: Providing more relevant text samples enhances performance in both language modeling and ASR tasks. The probability-based approach outperforms the traditional instruction-based approach. LLMs using ICL achieve ASR performance comparable to or surpassing dedicated language models for the tested languages.

Conclusion: In-context learning allows LLMs to effectively learn and perform ASR tasks for unseen, low-resource languages, potentially aiding in the preservation of endangered languages while maintaining the LLMs' original capabilities.

Abstract: With approximately 7,000 languages spoken worldwide, current large language
models (LLMs) support only a small subset. Prior research indicates LLMs can
learn new languages for certain tasks without supervised data. We extend this
investigation to speech recognition, investigating whether LLMs can learn
unseen, low-resource languages through in-context learning (ICL). With
experiments on four diverse endangered languages that LLMs have not been
trained on, we find that providing more relevant text samples enhances
performance in both language modelling and Automatic Speech Recognition (ASR)
tasks. Furthermore, we show that the probability-based approach outperforms the
traditional instruction-based approach in language learning. Lastly, we show
ICL enables LLMs to achieve ASR performance that is comparable to or even
surpasses dedicated language models trained specifically for these languages,
while preserving the original capabilities of the LLMs.

</details>


### [333] [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/abs/2505.20482)
*Vibhor Agarwal,Arjoo Gupta,Suparna De,Nishanth Sastry*

Main category: cs.CL

TL;DR: The paper proposes Conversation Kernels to capture conversational context and dependencies in online conversations, improving content analysis by considering the conversation tree structure.


<details>
  <summary>Details</summary>
Motivation: Understanding online conversations is challenging due to short utterances and implicit references. Capturing conversational context and dependencies between posts and comments/replies is crucial for effective content analysis.

Method: Propose two families of Conversation Kernels that explore different parts of the neighborhood of a post within the conversation tree to build relevant conversational context suitable for various tasks.

Result: Evaluated on conversations from slashdot.org with highly varied labels such as 'insightful' or 'funny', demonstrating the general-purpose and flexible nature of Conversation Kernels for different conversation understanding tasks.

Conclusion: Conversation Kernels provide an effective mechanism to discover appropriate conversational context for various aspects of an online post, proving to be adaptable for disparate conversation understanding tasks.

Abstract: Understanding online conversations has attracted research attention with the
growth of social networks and online discussion forums. Content analysis of
posts and replies in online conversations is difficult because each individual
utterance is usually short and may implicitly refer to other posts within the
same conversation. Thus, understanding individual posts requires capturing the
conversational context and dependencies between different parts of a
conversation tree and then encoding the context dependencies between posts and
comments/replies into the language model.
  To this end, we propose a general-purpose mechanism to discover appropriate
conversational context for various aspects about an online post in a
conversation, such as whether it is informative, insightful, interesting or
funny. Specifically, we design two families of Conversation Kernels, which
explore different parts of the neighborhood of a post in the tree representing
the conversation and through this, build relevant conversational context that
is appropriate for each task being considered. We apply our developed method to
conversations crawled from slashdot.org, which allows users to apply highly
different labels to posts, such as 'insightful', 'funny', etc., and therefore
provides an ideal experimental platform to study whether a framework such as
Conversation Kernels is general-purpose and flexible enough to be adapted to
disparately different conversation understanding tasks.

</details>


### [334] [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/abs/2505.20487)
*Roi Cohen,Russa Biswas,Gerard de Melo*

Main category: cs.CL

TL;DR: The paper proposes an informativeness alignment mechanism to improve the informativeness and factuality of LLM-generated text.


<details>
  <summary>Details</summary>
Motivation: LLMs tend to generate factually correct but less informative text, in addition to sometimes generating factually incorrect text.

Method: The informativeness alignment mechanism utilizes recent factual benchmarks to create an objective that prioritizes both correctness and informativeness in generated text.

Result: Training a model with this objective improves not only the informativeness but also the factuality of the generated text.

Conclusion: An informativeness alignment mechanism can enhance both the informativeness and factuality of language models.

Abstract: Factual completeness is a general term that captures how detailed and
informative a factually correct text is. For instance, the factual sentence
``Barack Obama was born in the United States'' is factually correct, though
less informative than the factual sentence ``Barack Obama was born in Honolulu,
Hawaii, United States''. Despite the known fact that LLMs tend to hallucinate
and generate factually incorrect text, they might also tend to choose to
generate factual text that is indeed factually correct and yet less informative
than other, more informative choices. In this work, we tackle this problem by
proposing an informativeness alignment mechanism. This mechanism takes
advantage of recent factual benchmarks to propose an informativeness alignment
objective. This objective prioritizes answers that are both correct and
informative. A key finding of our work is that when training a model to
maximize this objective or optimize its preference, we can improve not just
informativeness but also factuality.

</details>


### [335] [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/abs/2505.20500)
*Naba Rizvi,Harper Strickland,Saleha Ahmedi,Aekta Kallepalli,Isha Khirwadkar,William Wu,Imani N. S. Munyaka,Nedjma Ousidhoum*

Main category: cs.CL

TL;DR: Large language models (LLMs) are used in decision-making tasks and can influence certain perspectives. This study evaluates four LLMs' ability to identify nuanced ableism towards autistic individuals. Results show that while LLMs can recognize autism-related language, they often miss harmful connotations due to reliance on surface-level keyword matching.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models conceptualize ableism and their effectiveness in detecting it in text, especially towards autistic individuals.

Method: Evaluate four LLMs on their ability to identify nuanced ableism directed at autistic individuals by examining the gap between their understanding of terminology and recognizing ableist content in context.

Result: LLMs can identify autism-related language but often miss harmful or offensive connotations. They rely on surface-level keyword matching leading to context misinterpretations, whereas human annotators consider context, speaker identity, and potential impact.

Conclusion: Both LLMs and humans agree on the annotation scheme suggesting binary classification is adequate for evaluating LLM performance.

Abstract: Large language models (LLMs) are increasingly used in decision-making tasks
like r\'esum\'e screening and content moderation, giving them the power to
amplify or suppress certain perspectives. While previous research has
identified disability-related biases in LLMs, little is known about how they
conceptualize ableism or detect it in text. We evaluate the ability of four
LLMs to identify nuanced ableism directed at autistic individuals. We examine
the gap between their understanding of relevant terminology and their
effectiveness in recognizing ableist content in context. Our results reveal
that LLMs can identify autism-related language but often miss harmful or
offensive connotations. Further, we conduct a qualitative comparison of human
and LLM explanations. We find that LLMs tend to rely on surface-level keyword
matching, leading to context misinterpretations, in contrast to human
annotators who consider context, speaker identity, and potential impact. On the
other hand, both LLMs and humans agree on the annotation scheme, suggesting
that a binary classification is adequate for evaluating LLM performance, which
is consistent with findings from prior studies involving human annotators.

</details>


### [336] [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/abs/2505.20506)
*Hawau Olamide Toyin,Rufael Marew,Humaid Alblooshi,Samar M. Magdy,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: This paper introduces ArVoice, a new MSA speech corpus with diacritized transcriptions for multi-speaker speech synthesis and other tasks. It includes professionally recorded data, a modified subset of an existing corpus, and synthetic speech. The total duration is 83.52 hours across 11 voices. They train TTS and voice conversion systems to demonstrate its applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a high-quality multi-speaker Modern Standard Arabic speech corpus with diacritized transcriptions that can be used for various tasks including multi-speaker speech synthesis, speech-based diacritic restoration, voice conversion, and deepfake detection.

Method: ArVoice is composed of three parts: a new professionally recorded set from six voice talents, a modified subset of the Arabic Speech Corpus, and high-quality synthetic speech from two commercial systems. They use this dataset to train three open-source TTS models and two voice conversion systems.

Result: The ArVoice corpus consists of a total of 83.52 hours of speech across 11 voices; around 10 hours consist of human voices from 7 speakers. Training TTS and voice conversion systems on this dataset illustrates its potential use cases.

Conclusion: ArVoice is introduced as a valuable resource for research in multi-speaker speech synthesis and related areas. It is made available for research use.

Abstract: We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech
corpus with diacritized transcriptions, intended for multi-speaker speech
synthesis, and can be useful for other tasks such as speech-based diacritic
restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a
new professionally recorded set from six voice talents with diverse
demographics, (2) a modified subset of the Arabic Speech Corpus; and (3)
high-quality synthetic speech from two commercial systems. The complete corpus
consists of a total of 83.52 hours of speech across 11 voices; around 10 hours
consist of human voices from 7 speakers. We train three open-source TTS and two
voice conversion systems to illustrate the use cases of the dataset. The corpus
is available for research use.

</details>


### [337] [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
*Ziju Shen,Naohao Huang,Fanyi Yang,Yutong Wang,Guoxiong Gao,Tianyi Xu,Jiedong Jiang,Wanyi He,Pu Yang,Mengzhou Sun,Haocheng Ju,Peihao Wu,Bryan Dai,Bin Dong*

Main category: cs.CL

TL;DR: The paper introduces REAL-Prover, an open-source theorem prover for Lean 4 that enhances solving college-level math problems. It uses a fine-tuned language model (REAL-Prover-v1) and a retrieval system (Leansearch-PS). The prover achieves competitive results on ProofNet and sets a new benchmark record on FATE-M.


<details>
  <summary>Details</summary>
Motivation: To advance formal theorem provers from high-school and competition-level mathematics to more advanced college-level mathematics.

Method: Developed REAL-Prover with a fine-tuned large language model (REAL-Prover-v1), integrated with a retrieval system (Leansearch-PS). Created HERALD-AF for data extraction and Jixia-interactive for data collection. Evaluated the prover on ProofNet and introduced a new benchmark FATE-M.

Result: Achieved a 23.7% success rate on ProofNet, comparable to SOTA models. Set a new SOTA success rate of 56.7% on the FATE-M benchmark.

Conclusion: REAL-Prover significantly boosts performance in solving college-level mathematics problems and establishes a new benchmark for algebraic problems.

Abstract: Nowadays, formal theorem provers have made monumental progress on high-school
and competition-level mathematics, but few of them generalize to more advanced
mathematics. In this paper, we present REAL-Prover, a new open-source stepwise
theorem prover for Lean 4 to push this boundary. This prover, based on our
fine-tuned large language model (REAL-Prover-v1) and integrated with a
retrieval system (Leansearch-PS), notably boosts performance on solving
college-level mathematics problems. To train REAL-Prover-v1, we developed
HERALD-AF, a data extraction pipeline that converts natural language math
problems into formal statements, and a new open-source Lean 4 interactive
environment (Jixia-interactive) to facilitate synthesis data collection. In our
experiments, our prover using only supervised fine-tune achieves competitive
results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable
to state-of-the-art (SOTA) models. To further evaluate our approach, we
introduce FATE-M, a new benchmark focused on algebraic problems, where our
prover achieves a SOTA success rate of 56.7% (Pass@64).

</details>


### [338] [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)
*Ting Xu,Zhichao Huang,Jiankai Sun,Shanbo Cheng,Wai Lam*

Main category: cs.CL

TL;DR: The paper introduces SeqPO-SiMT, a new policy optimization framework for Simultaneous Machine Translation (SiMT) that enhances translation quality while reducing latency. Experiments show it outperforms SFT models in COMET scores and Average Lagging, with its 7B LLM results rivaling offline translations from high-performing LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving high-quality translations with reduced latency in simultaneous machine translation (SiMT), unlike traditional methods which are often applied in single-step tasks.

Method: Defines SiMT as a sequential decision-making problem and uses a tailored reward to enhance translation quality while reducing latency. The method contrasts with popular RLHF methods like PPO and DPO, effectively tackling multi-step SiMT tasks.

Result: SeqPO-SiMT achieves significantly higher translation quality with lower latency across six datasets. It surpasses the SFT model by 1.13 points in COMET score and reduces Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. The 7B LLM's SiMT results rival offline translations from high-performing LLMs.

Conclusion: SeqPO-SiMT is an effective framework for improving simultaneous machine translation, offering higher quality translations with reduced latency, even with limited context.

Abstract: We present Sequential Policy Optimization for Simultaneous Machine
Translation (SeqPO-SiMT), a new policy optimization framework that defines the
simultaneous machine translation (SiMT) task as a sequential decision making
problem, incorporating a tailored reward to enhance translation quality while
reducing latency. In contrast to popular Reinforcement Learning from Human
Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in
single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.
This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT
process using a tailored reward. We conduct experiments on six datasets from
diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that
SeqPO-SiMT consistently achieves significantly higher translation quality with
lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning
(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17
in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context
than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly
rival the offline translation of high-performing LLMs, including
Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.

</details>


### [339] [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)
*Jinwu Hu,Zhitian Zhang,Guohao Chen,Xutao Wen,Chao Shuai,Wei Luo,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出了一种针对大语言模型（LLMs）的测试时学习（TTL）范式，称为TLM。通过最小化未标记测试数据的输入困惑度、采用高效的样本选择策略以及使用低秩适应（LoRA）来优化模型性能，从而在领域知识适应方面比原始LLMs提高了至少20%。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在广泛预训练后展现出显著的涌现能力，但在专业化领域泛化和处理多样语言变化（分布迁移）方面仍存在关键局限性。

Method: 1. 提供实证证据和理论见解，表明通过最小化未标记测试数据的输入困惑度可以实现更准确的预测。
2. 将测试时学习过程公式化为输入困惑度最小化，以实现自监督增强LLM性能。
3. 引入样本高效学习策略，主动选择和强调高困惑度样本进行测试时更新。
4. 采用低秩适应（LoRA），而非全参数优化，以减轻灾难性遗忘并确保适应稳定性。

Result: 通过实验验证，在AdaptEval基准上，与原始LLMs相比，TLM在领域知识适应方面的性能提升了至少20%。

Conclusion: 所提出的TLM方法能够有效提升LLMs在特定领域的适应能力，同时保持模型原有的知识和稳定性。

Abstract: While Large Language Models (LLMs) have exhibited remarkable emergent
capabilities through extensive pre-training, they still face critical
limitations in generalizing to specialized domains and handling diverse
linguistic variations, known as distribution shifts. In this paper, we propose
a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically
adapts LLMs to target domains using only unlabeled test data during testing.
Specifically, we first provide empirical evidence and theoretical insights to
reveal that more accurate predictions from LLMs can be achieved by minimizing
the input perplexity of the unlabeled test data. Based on this insight, we
formulate the Test-Time Learning process of LLMs as input perplexity
minimization, enabling self-supervised enhancement of LLM performance.
Furthermore, we observe that high-perplexity samples tend to be more
informative for model optimization. Accordingly, we introduce a Sample
Efficient Learning Strategy that actively selects and emphasizes these
high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic
forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)
instead of full-parameter optimization, which allows lightweight model updates
while preserving more original knowledge from the model. We introduce the
AdaptEval benchmark for TTL and demonstrate through experiments that TLM
improves performance by at least 20% compared to original LLMs on domain
knowledge adaptation.

</details>


### [340] [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/abs/2505.20650)
*Yan Wang,Yang Ren,Lingfei Qian,Xueqing Peng,Keyi Wang,Yi Han,Dongji Feng,Xiao-Yang Liu,Jimin Huang,Qianqian Xie*

Main category: cs.CL

TL;DR: The paper introduces FinTagging, a benchmark for evaluating LLMs in XBRL-based financial reporting. It decomposes XBRL tagging into two subtasks and assesses LLMs' performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capabilities of large language models in structured information extraction and semantic alignment within the context of XBRL-based financial reporting.

Method: Introduced FinTagging which decomposes XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. Assessed diverse set of LLMs under zero-shot settings.

Result: LLMs show strong generalization in information extraction but struggle with fine-grained concept alignment, especially disambiguating closely related taxonomy entries.

Conclusion: Existing LLMs have limitations in fully automating XBRL tagging and there is a need for improved semantic reasoning and schema-aware modeling.

Abstract: We introduce FinTagging, the first full-scope, table-aware XBRL benchmark
designed to evaluate the structured information extraction and semantic
alignment capabilities of large language models (LLMs) in the context of
XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL
tagging as flat multi-class classification and focus solely on narrative text,
FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for
financial entity extraction and FinCL for taxonomy-driven concept alignment. It
requires models to jointly extract facts and align them with the full 10k+
US-GAAP taxonomy across both unstructured text and structured tables, enabling
realistic, fine-grained evaluation. We assess a diverse set of LLMs under
zero-shot settings, systematically analyzing their performance on both subtasks
and overall tagging accuracy. Our results reveal that, while LLMs demonstrate
strong generalization in information extraction, they struggle with
fine-grained concept alignment, particularly in disambiguating closely related
taxonomy entries. These findings highlight the limitations of existing LLMs in
fully automating XBRL tagging and underscore the need for improved semantic
reasoning and schema-aware modeling to meet the demands of accurate financial
disclosure. Code is available at our GitHub repository and data is at our
Hugging Face repository.

</details>


### [341] [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/abs/2505.20654)
*Yi Zhu,Xin Zou,Xindong Wu*

Main category: cs.CL

TL;DR: This paper proposes a new annotation method to create the first Chinese cyberbullying incident detection dataset called CHNCI, which contains 220,676 comments across 91 incidents. An ensemble method based on explanations generation is used to generate pseudo labels that are then judged by human annotators. The authors also propose evaluation criteria for determining if an event constitutes a cyberbullying incident.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for cyberbullying detection focus on polarity of speech (e.g., offensive or non-offensive), which is essentially hate speech detection. However, real-world cyberbullying often gains attention through specific incidents, necessitating a new approach to organizing datasets.

Method: A novel annotation method is proposed to construct a cyberbullying dataset organized by incidents. This includes using an ensemble method combining three cyberbullying detection methods based on explanation generation to produce pseudo labels, followed by human annotation for validation. Evaluation criteria are also established to determine if an event is a cyberbullying incident.

Result: The constructed CHNCI dataset serves as a benchmark for tasks in cyberbullying detection and incident prediction. It consists of 220,676 comments across 91 incidents and is the first Chinese cyberbullying incident detection dataset.

Conclusion: This study represents the first effort in Chinese cyberbullying incident detection, providing a valuable resource and benchmark for future research in this area.

Abstract: Existing cyberbullying detection benchmarks were organized by the polarity of
speech, such as "offensive" and "non-offensive", which were essentially hate
speech detection. However, in the real world, cyberbullying often attracted
widespread social attention through incidents. To address this problem, we
propose a novel annotation method to construct a cyberbullying dataset that
organized by incidents. The constructed CHNCI is the first Chinese
cyberbullying incident detection dataset, which consists of 220,676 comments in
91 incidents. Specifically, we first combine three cyberbullying detection
methods based on explanations generation as an ensemble method to generate the
pseudo labels, and then let human annotators judge these labels. Then we
propose the evaluation criteria for validating whether it constitutes a
cyberbullying incident. Experimental results demonstrate that the constructed
dataset can be a benchmark for the tasks of cyberbullying detection and
incident prediction. To the best of our knowledge, this is the first study for
the Chinese cyberbullying incident detection task.

</details>


### [342] [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/abs/2505.20660)
*Qinzhuo Wu,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: The paper presents BacktrackAgent, a GUI agent with enhanced error detection and recovery capabilities through verifier, judger, and reflector components. It improves task success rate and step accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents lack effective mechanisms for detecting and recovering from errors during interactions within GUI environments.

Method: The BacktrackAgent incorporates a backtracking mechanism including verifier, judger, and reflector modules for error detection and recovery, and uses judgment rewards to enhance performance. A specialized training dataset considering outcome pages after actions is also developed.

Result: BacktrackAgent achieved performance improvements in both task success rate and step accuracy on the Mobile3M and Auto-UI benchmarks.

Conclusion: BacktrackAgent effectively addresses shortcomings of existing GUI agents by incorporating a robust backtracking mechanism, leading to improved task completion efficiency.

Abstract: Graphical User Interface (GUI) agents have gained substantial attention due
to their impressive capabilities to complete tasks through multiple
interactions within GUI environments. However, existing agents primarily focus
on enhancing the accuracy of individual actions and often lack effective
mechanisms for detecting and recovering from errors. To address these
shortcomings, we propose the BacktrackAgent, a robust framework that
incorporates a backtracking mechanism to improve task completion efficiency.
BacktrackAgent includes verifier, judger, and reflector components as modules
for error detection and recovery, while also applying judgment rewards to
further enhance the agent's performance. Additionally, we develop a training
dataset specifically designed for the backtracking mechanism, which considers
the outcome pages after action executions. Experimental results show that
BacktrackAgent has achieved performance improvements in both task success rate
and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be
released upon acceptance.

</details>


### [343] [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/abs/2505.20664)
*Yang He,Xiao Ding,Bibo Cai,Yufei Zhang,Kai Xiong,Zhouhao Sun,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: Self-Route是一种动态推理框架，可以根据模型能力估计自动选择普通或推理模式，通过轻量级预推理阶段和Gradient-10K数据集训练路由，减少30-55%的token消耗，同时保持与推理模型相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管增强推理的大语言模型（RLLMs）在复杂任务中表现显著提升，但它们在简单问题上也使用长推理链，造成不必要的token消耗和资源浪费。

Method: 提出了一种名为Self-Route的动态推理框架，引入轻量级预推理阶段提取隐藏层表示的能力感知嵌入，用于实时评估模型解决问题的能力，并构建了基于模型难度估计的Gradient-10K数据集以训练路由，实现精确的能力边界检测。

Result: 广泛的实验表明，Self-Route在不同参数规模和推理范式的模型上都能达到与推理模型相当的准确性，同时减少了30-55%的token消耗。

Conclusion: Self-Route展示了其在不同模型上的普遍适用性和实际价值，能够在减少资源消耗的同时保持高准确性。

Abstract: While reasoning-augmented large language models (RLLMs) significantly enhance
complex task performance through extended reasoning chains, they inevitably
introduce substantial unnecessary token consumption, particularly for simpler
problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking
phenomenon leads to inefficient resource usage without proportional accuracy
gains. To address this issue, we propose Self-Route, a dynamic reasoning
framework that automatically selects between general and reasoning modes based
on model capability estimation. Our approach introduces a lightweight
pre-inference stage to extract capability-aware embeddings from hidden layer
representations, enabling real-time evaluation of the model's ability to solve
problems. We further construct Gradient-10K, a model difficulty
estimation-based dataset with dense complexity sampling, to train the router
for precise capability boundary detection. Extensive experiments demonstrate
that Self-Route achieves comparable accuracy to reasoning models while reducing
token consumption by 30-55\% across diverse benchmarks. The proposed framework
demonstrates consistent effectiveness across models with different parameter
scales and reasoning paradigms, highlighting its general applicability and
practical value.

</details>


### [344] [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)
*Boyi Zeng,Shixiang Song,Siyuan Huang,Yixuan Wang,He Li,Ziwei He,Xinbing Wang,Zhiyu Li,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本研究通过在单一标记生成步骤中多次调用前向过程，将人类深思熟虑的过程引入语言模型。实验表明，这种方法无需人工注释即可通过自我监督学习实现，并且可以轻松与现有语言模型集成。在多个开源架构和下游任务评估中，该方法表现出高效性和广泛适用性。对于语言建模任务，沉思语言模型的性能可与参数量两倍于它的普通模型相媲美。在9个下游基准测试中，改进后的Pythia模型显著优于官方Pythia模型。值得注意的是，经过增强的Pythia-1B与训练数据量为其十倍的TinyLlama-1.1B相当。


<details>
  <summary>Details</summary>
Motivation: 受人类在表达复杂句子元素之前会深思熟虑以进行更深入的认知处理这一现象的启发，研究者希望将类似的'沉思'过程引入到语言模型中，从而提升其认知处理能力。

Method: 在单个标记生成步骤中反复调用前向过程，在沉思过程中，模型不是生成根据预测分布采样的实际标记，而是根据预测标记分布计算所有标记嵌入的加权和。生成的嵌入随后作为输入反馈用于另一次前向传递。通过自监督学习，模型能够学会这种沉思方式，而无需任何人工注释。

Result: 在三个广泛使用的开源架构（GPT-2、Pythia和LLaMA）以及广泛的下游任务评估中，证明了该方法的有效性和普遍性。对于语言建模任务，沉思语言模型的性能可与参数量两倍于它的普通模型相媲美。在9个下游基准测试中，改进后的Pythia模型显著优于官方Pythia模型。值得注意的是，经过增强的Pythia-1B与训练数据量为其十倍的TinyLlama-1.1B相当。

Conclusion: 研究表明，通过引入沉思过程，语言模型可以在无需额外参数的情况下显著提升性能。这种方法不仅有效，而且具有广泛的适用性，可以轻松与现有的多种语言模型集成。

Abstract: Humans ponder before articulating complex sentence elements, enabling deeper
cognitive processing through focused effort. In this work, we introduce this
pondering process into language models by repeatedly invoking the forward
process within a single token generation step. During pondering, instead of
generating an actual token sampled from the prediction distribution, the model
ponders by yielding a weighted sum of all token embeddings according to the
predicted token distribution. The generated embedding is then fed back as input
for another forward pass. We show that the model can learn to ponder in this
way through self-supervised learning, without any human annotations. Our method
is straightforward and can be seamlessly integrated with various existing
language models. Experiments across three widely used open-source
architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task
evaluations demonstrate the effectiveness and generality of our method. For
language modeling tasks, pondering language models achieve performance
comparable to vanilla models with twice the number of parameters. On 9
downstream benchmarks, our pondering-enhanced Pythia models significantly
outperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is
comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code
is available at https://github.com/LUMIA-Group/PonderingLM.

</details>


### [345] [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/abs/2505.20707)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Deepak Subramani*

Main category: cs.CL

TL;DR: Small Language Models (SLMs) were tested on high school physics problems. Despite decent answer accuracy, their reasoning abilities were often flawed.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of state-of-the-art SLMs in handling complex high school physics problems and to identify their strengths and weaknesses in this domain.

Method: A comprehensive physics dataset was created from the OpenStax High School Physics textbook, annotated according to Bloom's Taxonomy. A novel cultural contextualization approach was applied to some problems. An LLM-as-a-judge framework with Google's Gemini 2.5 Flash was used to evaluate the SLMs' answers and reasoning chains.

Result: Significant differences were found between SLMs. Qwen 3 1.7B had high answer accuracy (85%) but low fully correct reasoning (38%). Mathematical notation format had little effect. Reasoning quality declined with increasing complexity, but consistency was maintained across cultural contexts.

Conclusion: SLMs can often find correct answers but have flawed reasoning, suggesting overreliance on pattern recognition. Future development should focus on enhancing genuine understanding and sound reasoning rather than just answer accuracy.

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, making them promising for educational applications. However,
their capacity for complex reasoning, particularly in domains such as physics,
remains underexplored. This study investigates the high school physics
reasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),
including instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.
We developed a comprehensive physics dataset from the OpenStax High School
Physics textbook, annotated according to Bloom's Taxonomy, with LaTeX and
plaintext mathematical notations. A novel cultural contextualization approach
was applied to a subset, creating culturally adapted problems for Asian,
African, and South American/Australian contexts while preserving core physics
principles. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,
we evaluated answer and reasoning chain correctness, along with calculation
accuracy. The results reveal significant differences between the SLMs. Qwen 3
1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was
substantially low (38%). The format of the mathematical notation had a
negligible impact on performance. SLMs exhibited varied performance across the
physics topics and showed a decline in reasoning quality with increasing
cognitive and knowledge complexity. In particular, the consistency of reasoning
was largely maintained in diverse cultural contexts, especially by better
performing models. These findings indicate that, while SLMs can often find
correct answers, their underlying reasoning is frequently flawed, suggesting an
overreliance on pattern recognition. For SLMs to become reliable educational
tools in physics, future development must prioritize enhancing genuine
understanding and the generation of sound, verifiable reasoning chains over
mere answer accuracy.

</details>


### [346] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)
*Xiaqiang Tang,Jian Li,Keyu Hu,Du Nan,Xiaolong Li,Xi Zhang,Weigao Sun,Sihong Xie*

Main category: cs.CL

TL;DR: This paper addresses faithfulness hallucination in Large Language Models (LLMs), designs a framework to assess cognitive statements' faithfulness, creates a benchmark dataset with insightful statistics, and develops an annotation pipeline for larger benchmarks leading to the CogniBench-L dataset for training cognitive hallucination detection models.


<details>
  <summary>Details</summary>
Motivation: Faithfulness hallucination in LLMs leads to unsupported claims. Current benchmarks lack assessment standards for cognitive statements that infer from given contexts, making consistency evaluation and optimization challenging.

Method: A rigorous framework inspired by legislative evidence assessment is designed to evaluate different levels of faithfulness in cognitive statements. An annotation pipeline automatically creates larger benchmarks for various LLMs, resulting in the CogniBench-L dataset.

Result: The CogniBench-L dataset is developed and can be used to train accurate cognitive hallucination detection models. Insightful statistics are revealed through the benchmark dataset.

Conclusion: The paper presents a framework and dataset to address faithfulness hallucination in LLMs, providing tools for evaluating and improving cognitive statement consistency.

Abstract: Faithfulness hallucination are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standard, existing benchmarks only contain "factual statements" that rephrase
source materials without marking "cognitive statements" that make inference
from the given context, making the consistency evaluation and optimization of
cognitive statements difficult. Inspired by how an evidence is assessed in the
legislative domain, we design a rigorous framework to assess different levels
of faithfulness of cognitive statements and create a benchmark dataset where we
reveal insightful statistics. We design an annotation pipeline to create larger
benchmarks for different LLMs automatically, and the resulting larger-scale
CogniBench-L dataset can be used to train accurate cognitive hallucination
detection model. We release our model and dataset at:
https://github.com/FUTUREEEEEE/CogniBench

</details>


### [347] [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
*Jungyoub Cha,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: SpecExtend enhances speculative decoding for long sequences in LLMs by integrating efficient attention mechanisms and proposing Cross-model Retrieval, accelerating inference up to 2.22x for inputs up to 16K tokens.


<details>
  <summary>Details</summary>
Motivation: To address the degradation of speculative decoding performance on long inputs due to increased attention cost and reduced draft accuracy.

Method: SpecExtend integrates efficient attention mechanisms (FlashAttention and Hybrid Tree Attention) into both draft and target models, and proposes Cross-model Retrieval, a KV cache update strategy using target model's attention scores to select relevant context for the draft model.

Result: Accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens on three long-context understanding datasets.

Conclusion: SpecExtend provides an effective solution for improving speculative decoding performance on long sequences without additional training.

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), but its performance degrades on long inputs
due to increased attention cost and reduced draft accuracy. We introduce
SpecExtend, a drop-in enhancement that improves the performance of speculative
decoding on long sequences without any additional training. SpecExtend
integrates efficient attention mechanisms such as FlashAttention and Hybrid
Tree Attention into both the draft and target models, reducing latency across
all stages. To improve draft accuracy and speed, we propose Cross-model
Retrieval, a novel KV cache update strategy that uses the target model's
attention scores to dynamically select relevant context for the draft model.
Extensive evaluations on three long-context understanding datasets show that
SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x
for inputs up to 16K tokens, providing an effective solution for speculative
decoding of long sequences. The code is available at
https://github.com/jycha98/SpecExtend .

</details>


### [348] [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
*Junsik Kim,Jinwook Park,Kangil Kim*

Main category: cs.CL

TL;DR: In knowledge graph embedding, inconsistency in entity-transformation due to disconnected transformation representations and excessive concentration of entity embeddings is a problem. The paper proposes Relation-Semantics Consistent Filter (RSCF), a plug-in KGE method that ensures more consistent entity-transformation through shared affine transformation, rooted entity-transformation, and normalization of change. RSCF also adds relation transformation and prediction modules to enhance semantics and significantly outperforms state-of-the-art KGE methods in knowledge graph completion tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of inconsistency in embedding differences before and after transformation in knowledge graph embedding, which can lead to loss of valuable inductive bias inherent in the embeddings.

Method: The method proposed is Relation-Semantics Consistent Filter (RSCF), a plug-in KGE method with three features: shared affine transformation of relation embeddings across all relations, rooted entity-transformation adding an entity embedding to its change represented by the transformed vector, and normalization of the change to prevent scale reduction. It also includes relation transformation and prediction modules for enhancing the semantics.

Result: RSCF significantly outperforms state-of-the-art KGE methods in knowledge graph completion tasks with distance-based and tensor decomposition models, showing robustness across all relations and their frequencies.

Conclusion: RSCF ensures more consistent entity-transformation and preserves semantics on embeddings, leading to better performance in knowledge graph completion tasks.

Abstract: In knowledge graph embedding, leveraging relation-specific
entity-transformation has markedly enhanced performance. However, the
consistency of embedding differences before and after transformation remains
unaddressed, risking the loss of valuable inductive bias inherent in the
embeddings. This inconsistency stems from two problems. First, transformation
representations are specified for relations in a disconnected manner, allowing
dissimilar transformations and corresponding entity-embeddings for similar
relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter
Based on Relations) disrupts this consistency through excessive concentration
of entity embeddings under entity-based regularization, generating
indistinguishable score distributions among relations. In this paper, we
introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),
containing more consistent entity-transformation characterized by three
features: 1) shared affine transformation of relation embeddings across all
relations, 2) rooted entity-transformation that adds an entity embedding to its
change represented by the transformed vector, and 3) normalization of the
change to prevent scale reduction. To amplify the advantages of consistency
that preserve semantics on embeddings, RSCF adds relation transformation and
prediction modules for enhancing the semantics. In knowledge graph completion
tasks with distance-based and tensor decomposition models, RSCF significantly
outperforms state-of-the-art KGE methods, showing robustness across all
relations and their frequencies.

</details>


### [349] [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)
*Amr Hegazy,Mostafa Elhoushi,Amr Alanwar*

Main category: cs.CL

TL;DR: 通过引入一个轻量级的可训练控制器网络，该研究提出了一种在推理时有效控制大型语言模型（LLM）不良行为的新方法。此方法无需修改原模型参数，显著提高了对有害输入的拒绝率，并优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 控制大型语言模型（LLM）不良行为的传统方法通常依赖于代价高昂的微调，而现有的激活转向技术缺乏精细和自适应的机制。因此，需要一种更高效且灵活的方法来实现在推理阶段对LLM行为的精确控制。

Method: 研究提出了一种新的推理时控制方法，使用一个轻量级的可训练控制器网络。该网络根据特定中间层激活预测全局缩放因子和层特定权重，动态调整从预计算的“拒绝方向”向量中派生的转向补丁的强度。控制器通过对有害和良性提示的激活进行训练，学会仅对有害输入施加精细的、分层感知的干预。

Result: 实验结果表明，加权转向控制器显著提高了与安全性相关的拒绝率，相较于基础LLM表现出更好的性能。在ToxicChat和In-The-Wild Jailbreak Prompts等安全基准测试中，该方法在Llama-3.1-8B、Llama-3.2-1B和Mistral-7B等多个模型上的表现优于现有方法。

Conclusion: 本研究提出了一种高效的、自适应的推理时控制方法，能够在不改变原始模型参数的情况下实现对LLM行为的精细控制，为解决LLM不良行为提供了一个有前景的方向。

Abstract: Controlling undesirable Large Language Model (LLM) behaviors, such as the
generation of unsafe content or failing to adhere to safety guidelines, often
relies on costly fine-tuning. Activation steering provides an alternative for
inference-time control, but existing methods typically lack fine-grained,
adaptive mechanisms. We introduce a novel approach using a lightweight,
trainable controller network integrated during inference. This controller
network observes specific intermediate LLM activations and predicts both a
global scaling factor and layer-specific weights. The predicted global scaling
factor and layer-specific weights then dynamically modulate the intensity of a
steering patch, derived from a pre-computed "refusal direction" vector, applied
across the LLM's layers during generation. Trained on activations from both
harmful and benign prompts, our controller learns to discriminatively apply
nuanced, layer-aware interventions, activating steering primarily for harmful
inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild
Jailbreak Prompts demonstrate that our weighted steering controller
significantly increases refusal rates compared to the base LLM, achieving
targeted behavioral modification without altering the original model
parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show
our approach outperforms existing methods, presenting an efficient and adaptive
method for fine-grained control over LLM behavior at inference time.

</details>


### [350] [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
*Jiyoung Lee,Seungho Kim,Jieun Han,Jun-Min Lee,Kitaek Kim,Alice Oh,Edward Choi*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) are mostly evaluated on Standard American English (SAE), which can cause fairness issues due to degraded performance on non-standard English varieties. This paper introduces Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness of LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of evaluation of LLMs on non-standard English varieties, which may lead to unequal benefits for users worldwide and raise fairness concerns.

Method: Trans-EnV combines linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora with LLM-based transformations to ensure both linguistic validity and scalability.

Result: Transformed six benchmark datasets into 38 English varieties and evaluated seven state-of-the-art LLMs, revealing significant performance disparities with accuracy decreasing by up to 46.3% on non-standard varieties.

Conclusion: It highlights the importance of comprehensive linguistic robustness evaluation across diverse English varieties.

Abstract: Large Language Models (LLMs) are predominantly evaluated on Standard American
English (SAE), often overlooking the diversity of global English varieties.
This narrow focus may raise fairness concerns as degraded performance on
non-standard varieties can lead to unequal benefits for users worldwide.
Therefore, it is critical to extensively evaluate the linguistic robustness of
LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a
framework that automatically transforms SAE datasets into multiple English
varieties to evaluate the linguistic robustness. Our framework combines (1)
linguistics expert knowledge to curate variety-specific features and
transformation guidelines from linguistic literature and corpora, and (2)
LLM-based transformations to ensure both linguistic validity and scalability.
Using Trans-EnV, we transform six benchmark datasets into 38 English varieties
and evaluate seven state-of-the-art LLMs. Our results reveal significant
performance disparities, with accuracy decreasing by up to 46.3% on
non-standard varieties. These findings highlight the importance of
comprehensive linguistic robustness evaluation across diverse English
varieties. Each construction of Trans-EnV was validated through rigorous
statistical testing and consultation with a researcher in the field of second
language acquisition, ensuring its linguistic validity. Our
\href{https://github.com/jiyounglee-0523/TransEnV}{code} and
\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}
are publicly available.

</details>


### [351] [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)
*Chengyu Wang,Junbing Yan,Wenrui Cai,Yuanhao Yue,Jun Huang*

Main category: cs.CL

TL;DR: The paper introduces EasyDistill, a toolkit for knowledge distillation (KD) of large language models (LLMs), featuring versatile functionalities like data synthesis and supervised fine-tuning, supporting both types of models (System 1 and System 2). It provides robust distilled models, open-sourced datasets, and integrates with Alibaba Cloud's PAI platform.


<details>
  <summary>Details</summary>
Motivation: To create an accessible and comprehensive toolkit for effective black-box and white-box knowledge distillation of LLMs, enabling researchers and practitioners to experiment with state-of-the-art KD strategies and industrial solutions.

Method: Developed a modular toolkit named EasyDistill with functionalities including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques tailored for KD scenarios. The toolkit supports both fast/intuitive and slow/analytical models and integrates with Alibaba Cloud's PAI platform.

Result: EasyDistill offers a series of robust distilled models, KD-based industrial solutions, and corresponding open-sourced datasets for various use cases, making advanced KD techniques more accessible in the NLP community.

Conclusion: The EasyDistill toolkit successfully enables easier experimentation and implementation of state-of-the-art KD strategies for LLMs, enhancing accessibility and impact within the NLP community.

Abstract: In this paper, we present EasyDistill, a comprehensive toolkit designed for
effective black-box and white-box knowledge distillation (KD) of large language
models (LLMs). Our framework offers versatile functionalities, including data
synthesis, supervised fine-tuning, ranking optimization, and reinforcement
learning techniques specifically tailored for KD scenarios. The toolkit
accommodates KD functionalities for both System 1 (fast, intuitive) and System
2 (slow, analytical) models. With its modular design and user-friendly
interface, EasyDistill empowers researchers and industry practitioners to
seamlessly experiment with and implement state-of-the-art KD strategies for
LLMs. In addition, EasyDistill provides a series of robust distilled models and
KD-based industrial solutions developed by us, along with the corresponding
open-sourced datasets, catering to a variety of use cases. Furthermore, we
describe the seamless integration of EasyDistill into Alibaba Cloud's Platform
for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for
LLMs more accessible and impactful within the NLP community.

</details>


### [352] [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/abs/2505.20901)
*Junhyuk Choi,Minju Kim,Yeseon Hong,Bugeun Kim*

Main category: cs.CL

TL;DR: This paper addresses the limitations of previous studies on stereotypes in large vision language models (LVLMs) by introducing new evaluation metrics based on the Stereotype Content Model (SCM) and a new benchmark called BASIC for assessing gender, race, and color stereotypes. Through studying eight LVLMs, they found that SCM-based evaluations effectively capture stereotypes, LVLMs exhibit color stereotypes as well as gender and race ones, and there's an interaction between model architecture and parameter sizes that affects stereotypes.


<details>
  <summary>Details</summary>
Motivation: There are growing concerns about the potential of large vision language models (LVLMs) to learn and generate social biases and stereotypes, but previous studies have limitations in their metrics and datasets.

Method: The study introduces new evaluation metrics based on the Stereotype Content Model (SCM) and proposes a benchmark called BASIC for assessing gender, race, and color stereotypes. These tools are used to conduct a study with eight LVLMs.

Result: 1) SCM-based evaluation is effective in capturing stereotypes; 2) LVLMs exhibit color stereotypes along with gender and race ones; 3) There seems to be an interaction between model architecture and parameter sizes affecting stereotypes.

Conclusion: The new SCM-based evaluation metrics and BASIC benchmark were successfully applied to uncover stereotypes in LVLMs, including previously less studied color stereotypes.

Abstract: As large vision language models(LVLMs) rapidly advance, concerns about their
potential to learn and generate social biases and stereotypes are increasing.
Previous studies on LVLM's stereotypes face two primary limitations: metrics
that overlooked the importance of content words, and datasets that overlooked
the effect of color. To address these limitations, this study introduces new
evaluation metrics based on the Stereotype Content Model (SCM). We also propose
BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM
metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.
As a result, we found three findings. (1) The SCM-based evaluation is effective
in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output
along with gender and race ones. (3) Interaction between model architecture and
parameter sizes seems to affect stereotypes. We release BASIC publicly on
[anonymized for review].

</details>


### [353] [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/abs/2505.20921)
*Injae Na,Keonwoong Noh,Woohwan Jung*

Main category: cs.CL

TL;DR: The paper introduces LLM-AT, a framework that automatically selects appropriate LLM tiers for NLP subtasks to balance cost and performance.


<details>
  <summary>Details</summary>
Motivation: As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance.

Method: LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model until a valid response is obtained. Additionally, an accuracy estimator is proposed which estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records.

Result: Experiments demonstrate that LLM-AT achieves superior performance while reducing costs.

Conclusion: LLM-AT is a practical solution for real-world applications.

Abstract: LLM providers typically offer multiple LLM tiers, varying in performance and
price. As NLP tasks become more complex and modularized, selecting the suitable
LLM tier for each subtask is a key challenge to balance between cost and
performance. To address the problem, we introduce LLM Automatic Transmission
(LLM-AT) framework that automatically selects LLM tiers without training.
LLM-AT consists of Starter, Generator, and Judge. The starter selects the
initial LLM tier expected to solve the given question, the generator produces a
response using the LLM of the selected tier, and the judge evaluates the
validity of the response. If the response is invalid, LLM-AT iteratively
upgrades to a higher-tier model, generates a new response, and re-evaluates
until a valid response is obtained. Additionally, we propose accuracy
estimator, which enables the suitable initial LLM tier selection without
training. Given an input question, accuracy estimator estimates the expected
accuracy of each LLM tier by computing the valid response rate across top-k
similar queries from past inference records. Experiments demonstrate that
LLM-AT achieves superior performance while reducing costs, making it a
practical solution for real-world applications.

</details>


### [354] [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/abs/2505.20925)
*Zhuo Li,Guodong Du,Weiyang Guo,Yigeng Zhou,Xiucheng Li,Wenya Wang,Fangming Liu,Yequan Wang,Deheng Ye,Min Zhang,Jing Li*

Main category: cs.CL

TL;DR: The paper introduces HoE, a lightweight and efficient approach to align LLMs with diverse human preferences without retraining. It uses three hierarchical components and performs well across multiple objectives and preferences.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with multiple objectives is challenging due to the diversity and conflicts in human preferences. Current methods fail to balance trade-offs effectively, often leading to high costs or suboptimal results.

Method: HoE consists of three hierarchical components: LoRA Experts, Router Experts, and Preference Routing. This method eliminates the need for model training while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences.

Result: HoE was evaluated on 14 objectives and 200 different preferences across 6 benchmarks, showing superior performance compared to 15 recent baselines.

Conclusion: HoE provides an effective solution for aligning LLMs with multiple objectives and preferences, achieving optimal Pareto frontiers while maintaining efficiency in parameters and training cost.

Abstract: Aligning large language models (LLMs) to simultaneously satisfy multiple
objectives remains a significant challenge, especially given the diverse and
often conflicting nature of human preferences. Existing alignment methods
struggle to balance trade-offs effectively, often requiring costly retraining
or yielding suboptimal results across the Pareto frontier of preferences. In
this paper, we introduce \textit{HoE}(Hierarchical Mixture-of-Experts), a
\textit{lightweight}, \textit{parameter-efficient}, and \textit{plug-and-play}
approach that eliminates the need for model training, while enabling LLMs to
adapt across the entire Pareto frontier and accommodate diverse user
preferences. In particular, \textit{HoE} consists of three hierarchical
components: LoRA Experts, Router Experts and Preference Routing, reaching
optimal Pareto frontiers and achieving a trade-off between parameter size,
training cost, and performance. We evaluate \textit{HoE} across various tasks
on 14 objectives and 200 different preferences among 6 benchmarks,
demonstrating superior performance over 15 recent baselines. Code is available
in the supplementary materials.

</details>


### [355] [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/abs/2505.20963)
*Felix Krejca,Tobias Kietreiber,Alexander Buchelt,Sebastian Neumaier*

Main category: cs.CL

TL;DR: The paper explores binary classification models (LSTM, CNN, ChatGPT) for automatic content moderation in German newspaper forums using contextual information. CNN and LSTM benefit from context, while ChatGPT's zero-shot classification underperforms.


<details>
  <summary>Details</summary>
Motivation: Current research on hate speech detection often focuses on social media and neglects German-language newspaper forums, especially platform-specific context like user history and article themes.

Method: Developed and evaluated binary classification models (LSTM, CNN, ChatGPT-3.5 Turbo) incorporating contextual information for moderating content in German newspaper forums, using the One Million Posts Corpus from Der Standard.

Result: CNN and LSTM models show improvement with contextual information and are competitive with state-of-the-art methods. ChatGPT's zero-shot classification does not improve with added context and performs worse.

Conclusion: Context-aware models like CNN and LSTM are effective for automatic content moderation in German newspaper forums, while ChatGPT's zero-shot approach is less suitable.

Abstract: The increasing volume of online discussions requires advanced automatic
content moderation to maintain responsible discourse. While hate speech
detection on social media is well-studied, research on German-language
newspaper forums remains limited. Existing studies often neglect
platform-specific context, such as user history and article themes. This paper
addresses this gap by developing and evaluating binary classification models
for automatic content moderation in German newspaper forums, incorporating
contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging
the One Million Posts Corpus from the Austrian newspaper Der Standard, we
assess the impact of context-aware models. Results show that CNN and LSTM
models benefit from contextual information and perform competitively with
state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification
does not improve with added context and underperforms.

</details>


### [356] [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/abs/2505.20971)
*Xiangqing Shen,Fanfan Wang,Rui Xia*

Main category: cs.CL

TL;DR: LLMs excel in reasoning but suffer from hallucinations; KGs offer factual knowledge but lack flexible reasoning. The paper proposes RAR, a framework integrating LLM reasoning with KGs for KGQA through Reasoner, Aligner, and Responser components optimized via Expectation-Maximization algorithm. Experiments show state-of-the-art performance and strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs (hallucinations) and KGs (inflexible reasoning), the authors aim to systematically integrate LLM reasoning with KGs for effective KGQA.

Method: RAR consists of three components: Reasoner generates reasoning chains, Aligner maps them to KG paths, Responser synthesizes answers. It uses a probabilistic model optimized by Expectation-Maximization algorithm iteratively refining reasoning chains and knowledge paths.

Result: Achieved state-of-the-art performance on WebQSP (Hit@1: 93.3%) and CWQ (Hit@1: 91.0%). Human evaluation confirms high-quality reasoning chains aligned with KG paths. Exhibits strong zero-shot generalization and maintains computational efficiency.

Conclusion: RAR effectively integrates LLM reasoning with KGs for KGQA, achieving high performance, interpretability, and computational efficiency.

Abstract: LLMs have demonstrated remarkable capabilities in complex reasoning tasks,
yet they often suffer from hallucinations and lack reliable factual grounding.
Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack
the flexible reasoning abilities of LLMs. In this paper, we present
Reason-Align-Respond (RAR), a novel framework that systematically integrates
LLM reasoning with knowledge graphs for KGQA. Our approach consists of three
key components: a Reasoner that generates human-like reasoning chains, an
Aligner that maps these chains to valid KG paths, and a Responser that
synthesizes the final answer. We formulate this process as a probabilistic
model and optimize it using the Expectation-Maximization algorithm, which
iteratively refines the reasoning chains and knowledge paths. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of RAR,
achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on
WebQSP and CWQ respectively. Human evaluation confirms that RAR generates
high-quality, interpretable reasoning chains well-aligned with KG paths.
Furthermore, RAR exhibits strong zero-shot generalization capabilities and
maintains computational efficiency during inference.

</details>


### [357] [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
*Sebastian Antony Joseph,Syed Murtaza Husain,Stella S. R. Offner,Stéphanie Juneau,Paul Torrey,Adam S. Bolton,Juan P. Farias,Niall Gaffney,Greg Durrett,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在科学研究中的应用潜力巨大，但评估其是否能提供正确的科学洞见具有挑战性。本文介绍了AstroVisBench，一个针对天文学领域的科学计算和可视化的基准测试工具。它评估语言模型创建天文学特定工作流和可视化结果的能力，并使用一种新颖的LLM-as-a-judge流程进行评价。通过AstroVisBench对当前最先进的语言模型进行评估，发现它们在作为天文学研究的有用助手方面存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学研究中有着广泛的应用前景，但在过去的工作中，如何评估这些模型是否能够产生传达正确科学洞见的输出尚未得到解决。

Method: 引入AstroVisBench这一基准测试工具，该工具可以评估语言模型在天文学领域创建特定工作流以处理和分析数据，以及通过复杂图表可视化这些工作流结果的能力。同时，采用一种新颖的LLM-as-a-judge流程来进行评价，并通过五位专业天文学家的注释进行验证。

Result: 通过对当前最先进的语言模型进行评估，发现它们在参与天文学研究并作为有用助手方面存在显著差距。

Conclusion: AstroVisBench为AI科学家提供了强大的端到端评估方法，为可视化工作流的发展提供了前进的道路，这对于从物理学到生物学等广泛领域都至关重要。

Abstract: Large Language Models (LLMs) are being explored for applications in
scientific research, including their capabilities to synthesize literature,
answer research questions, generate research ideas, and even conduct
computational experiments. Ultimately, our goal is for these to help scientists
derive novel scientific insights. In many areas of science, such insights often
arise from processing and visualizing data to understand its patterns. However,
evaluating whether an LLM-mediated scientific workflow produces outputs
conveying the correct scientific insights is challenging to evaluate and has
not been addressed in past work. We introduce AstroVisBench, the first
benchmark for both scientific computing and visualization in the astronomy
domain. AstroVisBench judges a language model's ability to both (1) create
astronomy-specific workflows to process and analyze data and (2) visualize the
results of these workflows through complex plots. Our evaluation of
visualizations uses a novel LLM-as-a-judge workflow, which is validated against
annotation by five professional astronomers. Using AstroVisBench we present an
evaluation of state-of-the-art language models, showing a significant gap in
their ability to engage in astronomy research as useful assistants. This
evaluation provides a strong end-to-end evaluation for AI scientists that
offers a path forward for the development of visualization-based workflows,
which are central to a broad range of domains from physics to biology.

</details>


### [358] [Who Reasons in the Large Language Models?](https://arxiv.org/abs/2505.20993)
*Jie Shao,Jianxin Wu*

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLMs）表现出色，但赋予它们新能力（如数学推理）的过程仍然很大程度上是经验性的且不透明。本文探讨了推理能力是否来自于整个模型、特定模块或仅仅是过拟合的产物。研究表明，推理能力主要归因于Transformer多头自注意力机制中的输出投影模块（oproj）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在明确推理能力在大型语言模型中的来源，以期提升模型效率与专业化程度。

Method: 提出了一套诊断工具Stethoscope for Networks (SfN)，用于探测和分析大型语言模型内部行为，特别是输出投影模块（oproj）的作用。

Result: 证据表明，输出投影模块（oproj）在启用推理方面起着核心作用，而其他模块更侧重于流畅对话。

Conclusion: 这些发现为大型语言模型的可解释性提供了新的视角，并为更高效和专业的训练策略开辟了道路。

Abstract: Despite the impressive performance of large language models (LLMs), the
process of endowing them with new capabilities--such as mathematical
reasoning--remains largely empirical and opaque. A critical open question is
whether reasoning abilities stem from the entire model, specific modules, or
are merely artifacts of overfitting. In this work, we hypothesize that the
reasoning capabilities in well-trained LLMs are primarily attributed to the
output projection module (oproj) in the Transformer's multi-head self-attention
(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for
Networks (SfN), a suite of diagnostic tools designed to probe and analyze the
internal behaviors of LLMs. Using SfN, we provide both circumstantial and
empirical evidence suggesting that oproj plays a central role in enabling
reasoning, whereas other modules contribute more to fluent dialogue. These
findings offer a new perspective on LLM interpretability and open avenues for
more targeted training strategies, potentially enabling more efficient and
specialized LLMs.

</details>


### [359] [Emotion Classification In-Context in Spanish](https://arxiv.org/abs/2505.20571)
*Bipul Thapa,Gabriel Cofre*

Main category: cs.CL

TL;DR: This paper presents a hybrid approach combining TF-IDF with BERT embeddings and a Custom Stacking Ensemble (CSE) model to classify Spanish customer feedback into positive, neutral, and negative emotion categories. The CSE model outperforms individual models and BERT alone, achieving 93.3% test accuracy on a native Spanish dataset.


<details>
  <summary>Details</summary>
Motivation: Classifying customer feedback in Spanish accurately has been challenging due to the loss of semantic integrity and contextual nuances when translating from widely spoken languages to less common ones. Existing methods do not sufficiently preserve the original meaning and sentiment inherent to the Spanish language.

Method: The paper proposes a hybrid method that uses TF-IDF with BERT embeddings to transform Spanish text into rich numerical representations. A Custom Stacking Ensemble (CSE) model is employed, which combines Logistic Regression, KNN, Bagging classifier with LGBM, and AdaBoost as base models, using one-vs-all Logistic Regression as the meta-model.

Result: The CSE model significantly outperforms individual models and BERT alone, achieving a test accuracy of 93.3% on the native Spanish dataset, surpassing the accuracy obtained from translated versions of the data.

Conclusion: The findings highlight the challenges of emotion classification in Spanish and demonstrate the effectiveness of combining vectorization techniques like TF-IDF with BERT embeddings. This approach provides valuable insights for businesses aiming to enhance customer feedback analysis and improve services.

Abstract: Classifying customer feedback into distinct emotion categories is essential
for understanding sentiment and improving customer experience. In this paper,
we classify customer feedback in Spanish into three emotion
categories--positive, neutral, and negative--using advanced NLP and ML
techniques. Traditional methods translate feedback from widely spoken languages
to less common ones, resulting in a loss of semantic integrity and contextual
nuances inherent to the original language. To address this limitation, we
propose a hybrid approach that combines TF-IDF with BERT embeddings,
effectively transforming Spanish text into rich numerical representations that
preserve the semantic depth of the original language by using a Custom Stacking
Ensemble (CSE) approach. To evaluate emotion classification, we utilize a range
of models, including Logistic Regression, KNN, Bagging classifier with LGBM,
and AdaBoost. The CSE model combines these classifiers as base models and uses
a one-vs-all Logistic Regression as the meta-model. Our experimental results
demonstrate that CSE significantly outperforms the individual and BERT model,
achieving a test accuracy of 93.3% on the native Spanish dataset--higher than
the accuracy obtained from the translated version. These findings underscore
the challenges of emotion classification in Spanish and highlight the
advantages of combining vectorization techniques like TF-IDF with BERT for
improved accuracy. Our results provide valuable insights for businesses seeking
to leverage emotion classification to enhance customer feedback analysis and
service improvements.

</details>


### [360] [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/abs/2505.21040)
*Wei Chen,Zhao Zhang,Meng Yuan,Kepeng Xu,Fuzhen Zhuang*

Main category: cs.CL

TL;DR: This paper proposes FCKT, a fine-grained cross-task knowledge transfer framework for targeted sentiment analysis (TSA). FCKT incorporates aspect-level information into sentiment prediction to improve task performance. Experiments on three datasets show its effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations in existing multi-task learning approaches for TSA which predominantly rely on coarse-grained knowledge transfer and lack fine-grained control over aspect-sentiment relationships, often leading to negative transfer.

Method: The method proposed in this paper is FCKT, a fine-grained cross-task knowledge transfer framework. This framework explicitly incorporates aspect-level information into sentiment prediction to achieve more precise knowledge transfer.

Result: Experiments conducted on three datasets demonstrate the effectiveness of FCKT compared to various baselines and large language models (LLMs).

Conclusion: FCKT effectively mitigates negative transfer and enhances task performance in targeted sentiment analysis by achieving fine-grained knowledge transfer.

Abstract: In this paper, we address the task of targeted sentiment analysis (TSA),
which involves two sub-tasks, i.e., identifying specific aspects from reviews
and determining their corresponding sentiments. Aspect extraction forms the
foundation for sentiment prediction, highlighting the critical dependency
between these two tasks for effective cross-task knowledge transfer. While most
existing studies adopt a multi-task learning paradigm to align task-specific
features in the latent space, they predominantly rely on coarse-grained
knowledge transfer. Such approaches lack fine-grained control over
aspect-sentiment relationships, often assuming uniform sentiment polarity
within related aspects. This oversimplification neglects contextual cues that
differentiate sentiments, leading to negative transfer. To overcome these
limitations, we propose FCKT, a fine-grained cross-task knowledge transfer
framework tailored for TSA. By explicitly incorporating aspect-level
information into sentiment prediction, FCKT achieves fine-grained knowledge
transfer, effectively mitigating negative transfer and enhancing task
performance. Experiments on three datasets, including comparisons with various
baselines and large language models (LLMs), demonstrate the effectiveness of
FCKT. The source code is available on https://github.com/cwei01/FCKT.

</details>


### [361] [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)
*Danush Khanna,Pratinav Seth,Sidhaarth Sredharan Murali,Aditya Kumar Guru,Siddharth Shukla,Tanuj Tyagi,Sandeep Chaurasia,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: The paper presents MultiManip dataset for detecting manipulative language in complex conversations and proposes SELF-PERCEPT framework to improve the detection capabilities of LLMs.


<details>
  <summary>Details</summary>
Motivation: Mental manipulation is a critical issue in interpersonal communication, yet large language models (LLMs) struggle with identifying manipulative language in complex, multi-turn, and multi-person conversations due to its nuanced and context-specific nature.

Method: Introduced MultiManip dataset containing 220 dialogues balanced between manipulative and non-manipulative interactions from reality shows. Evaluated state-of-the-art LLMs using various prompting strategies and proposed SELF-PERCEPT, a two-stage prompting framework inspired by Self-Perception Theory.

Result: State-of-the-art LLMs often fail to effectively detect manipulation despite their capabilities. The proposed SELF-PERCEPT framework demonstrates strong performance in detecting multi-person, multi-turn mental manipulation.

Conclusion: The MultiManip dataset and SELF-PERCEPT framework offer advancements in detecting manipulative language within complex conversations.

Abstract: Mental manipulation is a subtle yet pervasive form of abuse in interpersonal
communication, making its detection critical for safeguarding potential
victims. However, due to manipulation's nuanced and context-specific nature,
identifying manipulative language in complex, multi-turn, and multi-person
conversations remains a significant challenge for large language models (LLMs).
To address this gap, we introduce the MultiManip dataset, comprising 220
multi-turn, multi-person dialogues balanced between manipulative and
non-manipulative interactions, all drawn from reality shows that mimic
real-world scenarios. For manipulative interactions, it includes 11 distinct
manipulations depicting real-life scenarios. We conduct extensive evaluations
of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various
prompting strategies. Despite their capabilities, these models often struggle
to detect manipulation effectively. To overcome this limitation, we propose
SELF-PERCEPT, a novel, two-stage prompting framework inspired by
Self-Perception Theory, demonstrating strong performance in detecting
multi-person, multi-turn mental manipulation. Our code and data are publicly
available at https://github.com/danushkhanna/self-percept .

</details>


### [362] [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/abs/2505.20693)
*Praveen Srinivasa Varadhan,Srija Anand,Soma Siddhartha,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: An English Fairytaler model was fine-tuned on Indian languages to evaluate its adaptability. The fine-tuned IN-F5 model is near-human polyglot which enables fluent cross-language speaking. English pretraining helps low-resource TTS reach human parity and the model can synthesize unseen languages with a human-in-the-loop approach.


<details>
  <summary>Details</summary>
Motivation: To understand how an English F5-TTS model adapts to Indian languages and explore its capabilities in polyglot fluency, voice-cloning, style-cloning, and code-mixing.

Method: Compare three training methods: training from scratch, fine-tuning English F5 on Indian data, and fine-tuning on both Indian and English data to prevent forgetting. Fine-tune using only Indian data proved most effective.

Result: The resultant IN-F5 model is a near-human polyglot that allows speakers of one language to fluently speak in another. English pretraining aids low-resource TTS in reaching human parity.

Conclusion: Fine-tuning with only Indian data is most effective. The model can synthesize unseen languages using a human-in-the-loop approach for zero-resource TTS via synthetic data generation.

Abstract: What happens when an English Fairytaler is fine-tuned on Indian languages? We
evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring
polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:
(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and
(iii) fine-tuning on both Indian and English data to prevent forgetting.
Fine-tuning with only Indian data proves most effective and the resultant IN-F5
is a near-human polyglot; that enables speakers of one language (e.g., Odia) to
fluently speak in another (e.g., Hindi). Our results show English pretraining
aids low-resource TTS in reaching human parity. To aid progress in other
low-resource languages, we study data-constrained setups and arrive at a
compute optimal strategy. Finally, we show IN-F5 can synthesize unseen
languages like Bhojpuri and Tulu using a human-in-the-loop approach for
zero-resource TTS via synthetic data generation.

</details>


### [363] [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/abs/2505.21092)
*Daeen Kabir,Minhajur Rahman Chowdhury Mahim,Sheikh Shafayat,Adnan Sadik,Arian Ahmed,Eunsu Kim,Alice Oh*

Main category: cs.CL

TL;DR: The paper introduces BLUCK, a dataset to assess LLMs' performance in Bengali linguistic understanding and cultural knowledge. It consists of 2366 MCQs across 23 categories. Benchmarking with various LLMs shows reasonable overall performance but struggles in Bengali phonetics.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark for evaluating LLMs' capabilities in understanding Bengali language and culture, highlighting the need for resources in mid-resource languages like Bengali.

Method: Developed a dataset named BLUCK with 2366 MCQs from college and job level examinations, covering 23 categories related to Bengali culture, history, and linguistics. Benchmarked using 6 proprietary and 3 open-source LLMs.

Result: LLMs performed reasonably well overall but showed weaknesses in Bengali phonetics. The performance indicates Bengali's status as a mid-resource language.

Conclusion: BLUCK is established as the first MCQ-based evaluation benchmark focusing on native Bengali culture, history, and linguistics, providing valuable insights into LLMs' capabilities in this domain.

Abstract: In this work, we introduce BLUCK, a new dataset designed to measure the
performance of Large Language Models (LLMs) in Bengali linguistic understanding
and cultural knowledge. Our dataset comprises 2366 multiple-choice questions
(MCQs) carefully curated from compiled collections of several college and job
level examinations and spans 23 categories covering knowledge on Bangladesh's
culture and history and Bengali linguistics. We benchmarked BLUCK using 6
proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,
Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that
while these models perform reasonably well overall, they, however, struggles in
some areas of Bengali phonetics. Although current LLMs' performance on Bengali
cultural and linguistic contexts is still not comparable to that of mainstream
languages like English, our results indicate Bengali's status as a mid-resource
language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark
that is centered around native Bengali culture, history, and linguistics.

</details>


### [364] [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
*Stephen Chung,Wenyu Du,Jie Fu*

Main category: cs.CL

TL;DR: Recent studies indicate Reinforcement Learning can boost Large Language Models' reasoning skills in math and coding. Inspired by Dual Process Theory, this paper introduces a QA task with Fast Thinking, Verification, Slow Thinking, and Summarization stages. This method enhances accuracy for Qwen2.5-1.5B (from 24.9% to 27.9%) and DeepSeek-R1-Qwen-1.5B (from 45.9% to 49.8%). Notably, Fast Thinking alone achieves 26.8% accuracy for Qwen2.5-1.5B using fewer than 1000 tokens.


<details>
  <summary>Details</summary>
Motivation: To improve the reasoning capabilities of Large Language Models in areas like math and coding by addressing deficiencies in intuition and verification which lead to imprecise and unconfident search behavior.

Method: A four-stage QA task inspired by Dual Process Theory: Fast Thinking (strict token budget), Verification (evaluating initial response), Slow Thinking (refining the initial response), and Summarization (distilling refinements into precise steps).

Result: Accuracy improved from 24.9% to 27.9% for Qwen2.5-1.5B and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Fast Thinking mode alone achieved 26.8% accuracy using fewer than 1000 tokens for Qwen2.5-1.5B.

Conclusion: Intuition and deliberative reasoning are distinct, complementary systems that benefit from targeted training.

Abstract: Recent studies show that the reasoning capabilities of Large Language Models
(LLMs) can be improved by applying Reinforcement Learning (RL) to
question-answering (QA) tasks in areas such as math and coding. With a long
context length, LLMs may learn to perform search, as indicated by the
self-correction behavior observed in DeepSeek R1. However, this search behavior
is often imprecise and lacks confidence, resulting in long, redundant responses
and highlighting deficiencies in intuition and verification. Inspired by the
Dual Process Theory in psychology, we introduce a simple modification to the QA
task that includes four stages: Fast Thinking, where the LLM must answer within
a strict token budget; Verification, where the model evaluates its initial
response; Slow Thinking, where it refines the initial response with more
deliberation; and Summarization, where it distills the refinement from the
previous stage into precise steps. Our proposed task improves average accuracy
from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for
DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone
achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial
inference efficiency gains. These findings suggest that intuition and
deliberative reasoning are distinct, complementary systems benefiting from
targeted training.

</details>


### [365] [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/abs/2505.21109)
*Bogdan Bogachov,Yaoyao Fiona Zhao*

Main category: cs.CL

TL;DR: An abstract about a new method called Small Language Graph (SLG) which is designed to reduce computational resources for fine-tuning and inference of language models while also addressing hallucination issues. SLG surpasses conventional methods on the Exact Match metric by 3 times and is 1.7 times faster during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Recent domain adaptation techniques for large language models are computationally intensive, and resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing computational resources required for fine-tuning and inference of language models.

Method: This work introduces Small Language Graph (SLG), a lightweight adaptation solution structured as a graph where each node represents a small language model fine-tuned on specific and concise texts.

Result: SLG surpassed conventional fine-tuning methods on the Exact Match metric by 3 times and was 1.7 times faster in the fine-tuning process compared to a larger stand-alone language model.

Conclusion: The findings suggest that SLG offers potential for smaller engineering companies to use generative AI technologies without expensive computational resources and presents an opportunity for distributed AI systems.

Abstract: Despite recent advancements in domain adaptation techniques for large
language models, these methods remain computationally intensive, and the
resulting models can still exhibit hallucination issues. Most existing
adaptation methods do not prioritize reducing the computational resources
required for fine-tuning and inference of language models. Hallucination issues
have gradually decreased with each new model release. However, they remain
prevalent in engineering contexts, where generating well-structured text with
minimal errors and inconsistencies is critical. This work introduces a novel
approach called the Small Language Graph (SLG), which is a lightweight
adaptation solution designed to address the two key challenges outlined above.
The system is structured in the form of a graph, where each node represents a
lightweight expert - a small language model fine-tuned on specific and concise
texts. The results of this study have shown that SLG was able to surpass
conventional fine-tuning methods on the Exact Match metric by 3 times.
Additionally, the fine-tuning process was 1.7 times faster compared to that of
a larger stand-alone language model. These findings introduce a potential for
small to medium-sized engineering companies to confidently use generative AI
technologies, such as LLMs, without the necessity to invest in expensive
computational resources. Also, the graph architecture and the small size of
expert nodes offer a possible opportunity for distributed AI systems, thus
potentially diverting the global need for expensive centralized compute
clusters.

</details>


### [366] [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
*Hanlin Wang,Chak Tou Leong,Jiashuo Wang,Jian Wang,Wenjie Li*

Main category: cs.CL

TL;DR: 为了应对强化学习中延迟奖励的问题，本文提出了Stepwise Progress Attribution (SPA)框架，将最终奖励分解为每一步的贡献，并通过实验验证了其在多个基准测试中的优越性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练LLM代理处理复杂任务时面临延迟奖励的关键挑战，即反馈信号通常只有在任务完成后才可用，这使得早期行动的奖励分配变得困难，从而阻碍了代理的训练。

Method: 提出了一种通用的奖励重新分配框架——Stepwise Progress Attribution (SPA)，该框架将最终奖励分解为每一步的贡献，反映了向整体任务完成的逐步进展。通过训练一个进度估计器，在轨迹上累积每一步的贡献以匹配任务完成情况，并在策略优化过程中，将估计的每步贡献与环境中的动作执行接地信号相结合，作为有效的中间奖励进行代理训练。

Result: 在常见的代理基准测试（包括Webshop、ALFWorld和VirtualHome）上的广泛实验表明，SPA在成功率（平均+2.5％）和接地准确性（平均+1.9％）方面均优于现有最先进方法。进一步分析表明，该方法显著提供了更有效的中间奖励用于RL训练。

Conclusion: Stepwise Progress Attribution (SPA) 是一种有效的奖励重新分配框架，可以显著提高代理训练的成功率和接地准确性，为解决强化学习中的延迟奖励问题提供了一种新方法。

Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents
to handle complex, goal-oriented tasks that require multi-step interactions
with external environments. However, a critical challenge when applying RL to
these agentic tasks arises from delayed rewards: feedback signals are typically
available only after the entire task is completed. This makes it non-trivial to
assign delayed rewards to earlier actions, providing insufficient guidance
regarding environmental constraints and hindering agent training. In this work,
we draw on the insight that the ultimate completion of a task emerges from the
cumulative progress an agent makes across individual steps. We propose Stepwise
Progress Attribution (SPA), a general reward redistribution framework that
decomposes the final reward into stepwise contributions, each reflecting its
incremental progress toward overall task completion. To achieve this, we train
a progress estimator that accumulates stepwise contributions over a trajectory
to match the task completion. During policy optimization, we combine the
estimated per-step contribution with a grounding signal for actions executed in
the environment as the fine-grained, intermediate reward for effective agent
training. Extensive experiments on common agent benchmarks (including Webshop,
ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the
state-of-the-art method in both success rate (+2.5\% on average) and grounding
accuracy (+1.9\% on average). Further analyses demonstrate that our method
remarkably provides more effective intermediate rewards for RL training. Our
code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.

</details>


### [367] [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/abs/2505.21171)
*Rochelle Choenni,Ivan Titov*

Main category: cs.CL

TL;DR: Multilingual LLM performance is often related to model size. Researchers are interested in one-shot pruning methods for efficiency, but pruning can cause performance loss. This study explores the trade-offs between multilinguality and sparsification, proposing M-Wanda to improve performance with minimal additional costs.


<details>
  <summary>Details</summary>
Motivation: To address the issue of performance loss in multilingual LLMs when applying pruning techniques, and to understand the trade-offs between multilinguality and sparsification.

Method: Propose M-Wanda, a pruning method that models cross-lingual variation by incorporating language-aware activation statistics into its pruning criterion and dynamically adjusts layerwise sparsity based on cross-lingual importance.

Result: M-Wanda consistently improves performance at minimal additional costs.

Conclusion: The authors are the first to explicitly optimize pruning to retain multilingual performance and hope to inspire future advances in multilingual pruning.

Abstract: Multilingual LLM performance is often critically dependent on model size.
With an eye on efficiency, this has led to a surge in interest in one-shot
pruning methods that retain the benefits of large-scale pretraining while
shrinking the model size. However, as pruning tends to come with performance
loss, it is important to understand the trade-offs between multilinguality and
sparsification. In this work, we study multilingual performance under different
sparsity constraints and show that moderate ratios already substantially harm
performance. To help bridge this gap, we propose M-Wanda, a pruning method that
models cross-lingual variation by incorporating language-aware activation
statistics into its pruning criterion and dynamically adjusts layerwise
sparsity based on cross-lingual importance. We show that M-Wanda consistently
improves performance at minimal additional costs. We are the first to
explicitly optimize pruning to retain multilingual performance, and hope to
inspire future advances in multilingual pruning.

</details>


### [368] [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189)
*Gleb Mezentsev,Ivan Oseledets*

Main category: cs.CL

TL;DR: 研究发现，冻结的语言模型通过仅两个学习到的嵌入可以在单次前向传递中生成数百个精确的标记，揭示了多标记生成无需迭代解码的能力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在无需自回归生成的情况下是否能够重建长文本。

Method: 提供两个学习到的嵌入，使冻结的LLMs能够在单次前向传递中生成多个标记，并研究这些嵌入的行为及其编码的信息类型。

Result: 冻结的LLMs可以生成数百个准确的标记，且对于给定文本，这些表示并非唯一，但在嵌入空间中形成相连的局部区域。

Conclusion: LLMs具备无需迭代解码的多标记生成能力，且这些表示在嵌入空间中的特性表明学习专用编码器至该空间的潜力。

Abstract: A recent study showed that large language models (LLMs) can reconstruct
surprisingly long texts - up to thousands of tokens - via autoregressive
generation from just one specially trained input embedding. In this work, we
explore whether such reconstruction is possible without autoregression. We show
that frozen LLMs can generate hundreds of accurate tokens in just one forward
pass, when provided with only two learned embeddings. This reveals a surprising
and underexplored capability of LLMs - multi-token generation without iterative
decoding. We investigate the behaviour of these embeddings and provide insight
into the type of information they encode. We also empirically show that
although these representations are not unique for a given text, they form
connected and local regions in embedding space - a property that suggests the
potential of learning a dedicated encoder into that space.

</details>


### [369] [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/abs/2505.21190)
*Jong Hak Moon,Geon Choi,Paloma Rabaey,Min Gwan Kim,Hyuk Gi Hong,Jung-Oh Lee,Hangyul Yoon,Eun Woo Doe,Jiyoun Kim,Harshita Sharma,Daniel C. Castro,Javier Alvarez-Valle,Edward Choi*

Main category: cs.CL

TL;DR: 研究人员引入了LUNGUAGE，一个用于结构化放射报告生成的基准数据集，并提出了LUNGUAGESCORE，一种可解释的评估指标，以支持单个报告和纵向患者级别的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的放射报告评估方法局限于单一报告设置，依赖于无法捕捉精细临床语义和时间依赖性的粗略指标。

Method: 创建了一个包含1,473个标注的胸部X光报告的数据集LUNGUAGE，其中80个报告包含纵向注释。开发了一个两阶段框架，将生成的报告转换为精细的、与模式对齐的结构化表示，同时提出了一种新的评估指标LUNGUAGESCORE。

Result: 实证结果表明，LUNGUAGESCORE有效地支持了结构化报告的评估，能够比较实体、关系和属性级别的结构化输出，同时建模患者时间线上的时间一致性。

Conclusion: LUNGUAGE和LUNGUAGESCORE为顺序放射报告提供了首个基准数据集、结构化框架和评估指标。

Abstract: Radiology reports convey detailed clinical observations and capture
diagnostic reasoning that evolves over time. However, existing evaluation
methods are limited to single-report settings and rely on coarse metrics that
fail to capture fine-grained clinical semantics and temporal dependencies. We
introduce LUNGUAGE,a benchmark dataset for structured radiology report
generation that supports both single-report evaluation and longitudinal
patient-level assessment across multiple studies. It contains 1,473 annotated
chest X-ray reports, each reviewed by experts, and 80 of them contain
longitudinal annotations to capture disease progression and inter-study
intervals, also reviewed by experts. Using this benchmark, we develop a
two-stage framework that transforms generated reports into fine-grained,
schema-aligned structured representations, enabling longitudinal
interpretation. We also propose LUNGUAGESCORE, an interpretable metric that
compares structured outputs at the entity, relation, and attribute level while
modeling temporal consistency across patient timelines. These contributions
establish the first benchmark dataset, structuring framework, and evaluation
metric for sequential radiology reporting, with empirical results demonstrating
that LUNGUAGESCORE effectively supports structured report evaluation. The code
is available at: https://github.com/SuperSupermoon/Lunguage

</details>


### [370] [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/abs/2505.21218)
*Roi Cohen,Omri Fahn,Gerard de Melo*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在预训练过程中可能捕捉到不确定性，这种不确定性可以用来预测任务的正确性。尽管模型规模扩大对捕捉不确定性没有影响，但通过指令调优或[IDK]标记调优将不同类型的不确定性统一为一种，有助于提高模型的正确性预测能力。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型如何在未明确训练的情况下捕捉不确定性，以减少事实错误文本的产生。

Method: 通过考虑不确定性为模型潜在空间中的线性概念，分析LLMs捕捉不确定性的能力，并探讨不同类型不确定性与任务正确性的关系。

Result: 发现LLMs确实能捕捉多种类型的不确定性，这些不确定性与模型避免误导信息的能力相关；模型规模扩大对捕捉不确定性无显著影响；将不确定性类型统一为一种有助于提高正确性预测能力。

Conclusion: 大型语言模型在预训练阶段就能捕捉不确定性，这种能力可以用于预测任务正确性，且通过特定方法统一不确定性类型有助于提升模型性能。

Abstract: Large Language Models are known to capture real-world knowledge, allowing
them to excel in many downstream tasks. Despite recent advances, these models
are still prone to what are commonly known as hallucinations, causing them to
emit unwanted and factually incorrect text. In this work, we study how well
LLMs capture uncertainty, without explicitly being trained for that. We show
that, if considering uncertainty as a linear concept in the model's latent
space, it might indeed be captured, even after only pretraining. We further
show that, though unintuitive, LLMs appear to capture several different types
of uncertainty, each of which can be useful to predict the correctness for a
specific task or benchmark. Furthermore, we provide in-depth results such as
demonstrating a correlation between our correction prediction and the model's
ability to abstain from misinformation using words, and the lack of impact of
model scaling for capturing uncertainty. Finally, we claim that unifying the
uncertainty types as a single one using instruction-tuning or [IDK]-token
tuning is helpful for the model in terms of correctness prediction.

</details>


### [371] [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/abs/2505.21265)
*Ilker Kesen,Jonas F. Lotz,Ingo Ziegler,Phillip Rust,Desmond Elliott*

Main category: cs.CL

TL;DR: 研究人员开发了名为 PIXEL-M4 的模型，该模型通过多语言预训练增强了像素语言模型的能力，使其能更有效地支持多种语言。


<details>
  <summary>Details</summary>
Motivation: 尽管像素语言模型在跨语言迁移任务中表现出色，但多语言预训练领域尚未得到充分探索。

Method: 引入 PIXEL-M4 模型，该模型基于英语、印地语、乌克兰语和简体中文四种语言进行预训练，并在语义和句法任务上进行多语言评估。

Result: PIXEL-M4 在非拉丁字母脚本上的表现优于仅限英语的模型，并且即使在预训练未涉及的语言中也能捕捉到丰富的语言特征。此外，其隐藏表示分析表明，多语言预训练生成了一个与预训练所用语言紧密对齐的语义嵌入空间。

Conclusion: 多语言预训练显著提高了像素语言模型支持多样化语言的能力。

Abstract: Pixel language models operate directly on images of rendered text,
eliminating the need for a fixed vocabulary. While these models have
demonstrated strong capabilities for downstream cross-lingual transfer,
multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model
pretrained on four visually and linguistically diverse languages: English,
Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic
and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart
on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4
captures rich linguistic features, even in languages not seen during
pretraining. Furthermore, an analysis of its hidden representations shows that
multilingual pretraining yields a semantic embedding space closely aligned
across the languages used for pretraining. This work demonstrates that
multilingual pretraining substantially enhances the capability of pixel
language models to effectively support a diverse set of languages.

</details>


### [372] [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/abs/2505.21301)
*Andrea Pedrotti,Giulia Rambelli,Caterina Villani,Marianna Bolognesi*

Main category: cs.CL

TL;DR: This paper explores how well language and vision AI models replicate human categorization by analyzing subordinate-level exemplars for 187 concrete words in Italian, revealing low alignment but domain-dependent performance.


<details>
  <summary>Details</summary>
Motivation: To understand the organization of categories beyond basic levels and evaluate the capability of AI models to generate meaningful exemplars aligned with human category organization.

Method: A new Italian psycholinguistic dataset was created with human-generated exemplars for 187 concrete words. Textual and vision LLMs were then tested on three tasks: exemplar generation, category induction, and typicality judgment.

Result: There is a low alignment between human and AI model exemplars across all tasks, but performance varies significantly across different semantic domains.

Conclusion: AI-generated exemplars have potential for psychological and linguistic research but are constrained by their limited alignment with human category organization.

Abstract: People can categorize the same entity at multiple taxonomic levels, such as
basic (bear), superordinate (animal), and subordinate (grizzly bear). While
prior research has focused on basic-level categories, this study is the first
attempt to examine the organization of categories by analyzing exemplars
produced at the subordinate level. We present a new Italian psycholinguistic
dataset of human-generated exemplars for 187 concrete words. We then use these
data to evaluate whether textual and vision LLMs produce meaningful exemplars
that align with human category organization across three key tasks: exemplar
generation, category induction, and typicality judgment. Our findings show a
low alignment between humans and LLMs, consistent with previous studies.
However, their performance varies notably across different semantic domains.
Ultimately, this study highlights both the promises and the constraints of
using AI-generated exemplars to support psychological and linguistic research.

</details>


### [373] [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)
*Qishuai Zhong,Zongmin Li,Siqi Fan,Aixin Sun*

Main category: cs.CL

TL;DR: 有效使用大型语言模型（LLMs）需要根据用户的年龄、职业和教育水平等社会人口学特征调整响应。本文提出一个框架，评估当属性通过用户资料或对话历史明确或隐含引入时，LLMs的行为适应性。研究发现，大多数模型能根据人口统计变化调整表达的价值观，但一致性有所不同，推理能力较强的模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 目前对大型语言模型（LLMs）行为适应性的评估通常集中在单轮提示上，而实际应用中往往利用对话历史进行情境化。因此，有必要研究LLMs在多轮对话和用户档案中的行为适应性和一致性。

Method: 提出一个框架来评估LLMs的行为适应性，该框架考虑了通过用户档案或对话历史明确或隐含引入的属性。使用多代理管道构建合成数据集，将对话历史与不同的用户档案配对，并采用《价值调查模块》（VSM 2013）中的问题来探测价值表达。

Result: 研究发现大多数模型能够根据人口统计变化调整其表达的价值观，特别是在年龄和教育水平方面。然而，不同模型的一致性有所差异。具有较强推理能力的模型表现出更大的一致性，显示出推理在稳健的社会人口学适应中的重要性。

Conclusion: 大型语言模型的行为适应性可以通过明确的用户档案或隐含的对话历史来影响。模型在调整表达的价值观以适应人口统计变化方面表现出一定的能力，但一致性和效果因模型的推理能力而异。

Abstract: Effective engagement by large language models (LLMs) requires adapting
responses to users' sociodemographic characteristics, such as age, occupation,
and education level. While many real-world applications leverage dialogue
history for contextualization, existing evaluations of LLMs' behavioral
adaptation often focus on single-turn prompts. In this paper, we propose a
framework to evaluate LLM adaptation when attributes are introduced either (1)
explicitly via user profiles in the prompt or (2) implicitly through multi-turn
dialogue history. We assess the consistency of model behavior across these
modalities. Using a multi-agent pipeline, we construct a synthetic dataset
pairing dialogue histories with distinct user profiles and employ questions
from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe
value expression. Our findings indicate that most models adjust their expressed
values in response to demographic changes, particularly in age and education
level, but consistency varies. Models with stronger reasoning capabilities
demonstrate greater alignment, indicating the importance of reasoning in robust
sociodemographic adaptation.

</details>


### [374] [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191)
*Junyan Zhang,Yubo Gao,Yibo Yan,Jungang Li,Zhaorui Hou,Sicheng Tao,Shuliang Liu,Song Dai,Yonghua Hei,Junzhuo Li,Xuming Hu*

Main category: cs.CL

TL;DR: This study explores how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components using HexaInst dataset and SPARCOM framework.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying computational mechanisms driving improvements in LLMs' instruction-following capabilities after fine-tuning.

Method: 1) Identify sparse components in LLMs; 2) Evaluate their functional generality and uniqueness; 3) Compare their alterations systematically through experiments using HexaInst dataset and SPARCOM framework.

Result: Demonstrated functional generality, uniqueness, and the critical role of these sparse components in instruction execution.

Conclusion: Provides deeper insights into how LLMs internalize instruction-following behavior, contributing to the trustworthy LLM community.

Abstract: The finetuning of Large Language Models (LLMs) has significantly advanced
their instruction-following capabilities, yet the underlying computational
mechanisms driving these improvements remain poorly understood. This study
systematically examines how fine-tuning reconfigures LLM computations by
isolating and analyzing instruction-specific sparse components, i.e., neurons
in dense models and both neurons and experts in Mixture-of-Experts (MoE)
architectures. In particular, we introduce HexaInst, a carefully curated and
balanced instructional dataset spanning six distinct categories, and propose
SPARCOM, a novel analytical framework comprising three key contributions: (1) a
method for identifying these sparse components, (2) an evaluation of their
functional generality and uniqueness, and (3) a systematic comparison of their
alterations. Through experiments, we demonstrate functional generality,
uniqueness, and the critical role of these components in instruction execution.
By elucidating the relationship between fine-tuning-induced adaptations and
sparse computational substrates, this work provides deeper insights into how
LLMs internalize instruction-following behavior for the trustworthy LLM
community.

</details>


### [375] [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)
*Xiao Liu,Xinyi Dong,Xinyang Gao,Yansong Feng,Xun Pang*

Main category: cs.CL

TL;DR: Recent advancements in LLMs show promise for generating novel research ideas. This paper explores how augmenting LLMs with relevant data during the idea generation process can enhance the quality of generated ideas, by providing metadata and automatic validation. Experiments in social science show that metadata improves feasibility by 20%, while automatic validation improves overall quality by 7%. A human study also shows that LLM-generated ideas inspire higher quality research proposals.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by novel research ideas generated by LLMs in terms of feasibility and expected effectiveness.

Method: Introduce two ways of incorporating data: (1) providing metadata during the idea generation stage to guide LLMs toward feasible directions, and (2) adding automatic validation during the idea selection stage to assess the empirical plausibility of hypotheses within ideas.

Result: Experiments in the social science domain show that metadata improves the feasibility of generated ideas by 20%, while automatic validation improves the overall quality of selected ideas by 7%.

Conclusion: The work highlights the potential of data-driven research idea generation and the practical utility of LLM-assisted ideation in real-world academic settings.

Abstract: Recent advancements in large language models (LLMs) have shown promise in
generating novel research ideas. However, these ideas often face challenges
related to feasibility and expected effectiveness. This paper explores how
augmenting LLMs with relevant data during the idea generation process can
enhance the quality of generated ideas. We introduce two ways of incorporating
data: (1) providing metadata during the idea generation stage to guide LLMs
toward feasible directions, and (2) adding automatic validation during the idea
selection stage to assess the empirical plausibility of hypotheses within
ideas. We conduct experiments in the social science domain, specifically with
climate negotiation topics, and find that metadata improves the feasibility of
generated ideas by 20%, while automatic validation improves the overall quality
of selected ideas by 7%. A human study shows that LLM-generated ideas, along
with their related data and validation processes, inspire researchers to
propose research ideas with higher quality. Our work highlights the potential
of data-driven research idea generation, and underscores the practical utility
of LLM-assisted ideation in real-world academic settings.

</details>


### [376] [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/abs/2505.21399)
*Hovhannes Tamoyan,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）内部存在一种指导生成内容事实正确性的机制，这种自我意识信号在训练中迅速出现，并在中间层达到高峰，揭示了LLMs的内在自我监控能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决生成内容中的事实错误问题，探索LLMs是否在生成时内部就具备判断事实正确性的能力。

Method: 通过给定主体实体和关系，分析Transformer残差流中线性特征，探讨LLMs能否正确回忆属性形成有效的实体-关系-属性三元组，并调查上下文扰动及不同模型大小和训练动态对自我意识信号的影响。

Result: 发现LLMs在生成时确实编码了决定事实正确性的内部线性特征，这种信号对格式变化具有鲁棒性，并且自我意识在训练过程中快速出现，在中间层达到最高水平。

Conclusion: LLMs具有内在的自我监控能力，这提高了其可解释性和可靠性，为未来改进模型提供了新方向。

Abstract: Factual incorrectness in generated content is one of the primary concerns in
ubiquitous deployment of large language models (LLMs). Prior findings suggest
LLMs can (sometimes) detect factual incorrectness in their generated content
(i.e., fact-checking post-generation). In this work, we provide evidence
supporting the presence of LLMs' internal compass that dictate the correctness
of factual recall at the time of generation. We demonstrate that for a given
subject entity and a relation, LLMs internally encode linear features in the
Transformer's residual stream that dictate whether it will be able to recall
the correct attribute (that forms a valid entity-relation-attribute triplet).
This self-awareness signal is robust to minor formatting variations. We
investigate the effects of context perturbation via different example selection
strategies. Scaling experiments across model sizes and training dynamics
highlight that self-awareness emerges rapidly during training and peaks in
intermediate layers. These findings uncover intrinsic self-monitoring
capabilities within LLMs, contributing to their interpretability and
reliability.

</details>


### [377] [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/abs/2505.21409)
*Dario Satriani,Enzo Veltri,Donatello Santoro,Paolo Papotti*

Main category: cs.CL

TL;DR: Factuality in Large Language Models (LLMs) is challenging, especially for generating structured, multi-record tabular outputs. Current benchmarks often assess short factual answers and overlook this ability. The paper introduces RelationalFactQA, a new benchmark featuring diverse natural language questions and gold-standard tabular answers to evaluate LLMs' knowledge retrieval in a structured format. Experiments reveal that even state-of-the-art LLMs struggle significantly, with performance notably degrading as output dimensionality increases.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the under-explored capability of relational fact retrieval in Large Language Models.

Method: Introducing RelationalFactQA, a benchmark featuring diverse natural language questions (paired with SQL) and gold-standard tabular answers, specifically designed to assess knowledge retrieval in a structured format.

Result: State-of-the-art LLMs do not exceed 25% factual accuracy in generating relational outputs, with performance notably degrading as output dimensionality increases.

Conclusion: The findings underscore critical limitations in current LLMs' ability to synthesize structured factual knowledge and establish RelationalFactQA as a crucial resource for measuring future progress in LLM factuality.

Abstract: Factuality in Large Language Models (LLMs) is a persistent challenge. Current
benchmarks often assess short factual answers, overlooking the critical ability
to generate structured, multi-record tabular outputs from parametric knowledge.
We demonstrate that this relational fact retrieval is substantially more
difficult than isolated point-wise queries, even when individual facts are
known to the model, exposing distinct failure modes sensitive to output
dimensionality (e.g., number of attributes or records). To systematically
evaluate this under-explored capability, we introduce RelationalFactQA, a new
benchmark featuring diverse natural language questions (paired with SQL) and
gold-standard tabular answers, specifically designed to assess knowledge
retrieval in a structured format. RelationalFactQA enables analysis across
varying query complexities, output sizes, and data characteristics. Our
experiments reveal that even state-of-the-art LLMs struggle significantly, not
exceeding 25% factual accuracy in generating relational outputs, with
performance notably degrading as output dimensionality increases. These
findings underscore critical limitations in current LLMs' ability to synthesize
structured factual knowledge and establish RelationalFactQA as a crucial
resource for measuring future progress in LLM factuality.

</details>


### [378] [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/abs/2505.21413)
*Xiao Liu,Da Yin,Zirui Wu,Yansong Feng*

Main category: cs.CL

TL;DR: RefTool is a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks, enabling LLMs to overcome knowledge limitations and demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning.


<details>
  <summary>Details</summary>
Motivation: Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the models' internal knowledge and would fail in domains beyond the LLMs' knowledge scope.

Method: RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems.

Result: Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable.

Conclusion: RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning.

Abstract: Tools enhance the reasoning capabilities of large language models (LLMs) in
complex problem-solving tasks, but not all tasks have available tools. In the
absence of predefined tools, prior works have explored instructing LLMs to
generate tools on their own. However, such approaches rely heavily on the
models' internal knowledge and would fail in domains beyond the LLMs' knowledge
scope. To address this limitation, we propose RefTool, a reference-guided
framework for automatic tool creation that leverages structured external
materials such as textbooks. RefTool consists of two modules: (1) tool
creation, where LLMs generate executable tools from reference content, validate
them using illustrative examples, and organize them hierarchically into a
toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to
select and apply the appropriate tools to solve problems. Experiments on
causality, physics, and chemistry benchmarks demonstrate that RefTool
outperforms existing tool-creation and domain-specific reasoning methods by
11.3% on average accuracy, while being cost-efficient and broadly
generalizable. Analyses reveal that grounding tool creation in references
produces accurate and faithful tools, and that the hierarchical structure
facilitates effective tool selection. RefTool enables LLMs to overcome
knowledge limitations, demonstrating the value of grounding tool creation in
external references for enhanced and generalizable reasoning.

</details>


### [379] [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)
*Bidyarthi Paul,Jalisha Jashim Era,Mirazur Rahman Zim,Tahmid Sattar Aothoi,Faisal Muhammad Shah*

Main category: cs.CL

TL;DR: The paper introduces SOMADHAN, a large Bengali math word problems dataset, evaluates various LLMs on it using CoT prompting, and applies LoRA for efficient fine-tuning. This work aims to advance research in low-resource languages and improve reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Solving complex Bengali MWPs is challenging due to the lack of resources and multi-step reasoning required. Existing models struggle because no human-annotated Bengali dataset has addressed this task before, limiting progress in Bengali mathematical reasoning.

Method: Created SOMADHAN, a dataset of 8792 complex Bengali MWPs with step-by-step solutions. Evaluated multiple LLMs using zero-shot and few-shot prompting with and without CoT reasoning. Applied LoRA for efficient model fine-tuning.

Result: CoT prompting improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting.

Conclusion: This work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs, advancing equitable research in low-resource languages.

Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in
natural language processing (NLP) due to the language's low-resource status and
the multi-step reasoning required. Existing models struggle with complex
Bengali MWPs, largely because no human-annotated Bengali dataset has previously
addressed this task. This gap has limited progress in Bengali mathematical
reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex
Bengali MWPs with manually written, step-by-step solutions. We designed this
dataset to support reasoning-focused evaluation and model development in a
linguistically underrepresented context. Using SOMADHAN, we evaluated a range
of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series
models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with
and without Chain of Thought (CoT) reasoning. CoT prompting consistently
improved performance over standard prompting, especially in tasks requiring
multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with
few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune
models efficiently, enabling them to adapt to Bengali MWPs with minimal
computational cost. Our work fills a critical gap in Bengali NLP by providing a
high-quality reasoning dataset and a scalable framework for solving complex
MWPs. We aim to advance equitable research in low-resource languages and
enhance reasoning capabilities in educational and language technologies.

</details>


### [380] [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)
*Han Xiao,Guozhi Wang,Yuxiang Chai,Zimu Lu,Weifeng Lin,Hao He,Lue Fan,Liuyang Bian,Rui Hu,Liang Liu,Shuai Ren,Yafei Wen,Xiaoxin Chen,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: This paper presents UI-Genie, a self-improving framework for GUI agents that tackles the challenges of verifying trajectory outcomes and obtaining high-quality training data. It introduces a reward model (UI-Genie-RM) with an image-text interleaved architecture and a self-improvement pipeline, achieving state-of-the-art performance on GUI agent benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulties in verifying the outcomes of GUI agent trajectories and the lack of scalable high-quality training data, which are critical challenges in developing effective GUI agents.

Method: UI-Genie employs a reward model (UI-Genie-RM) with an image-text interleaved architecture to process historical context and unify action-level and task-level rewards. The framework also includes a self-improvement pipeline that enhances both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. Training data is generated using strategies like rule-based verification, controlled trajectory corruption, and hard negative mining.

Result: Experimental results demonstrate that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks after three generations of data-model self-improvement.

Conclusion: The authors conclude by presenting UI-Genie as a significant advancement in GUI agent development, offering a robust solution to key challenges. They also open-source the framework and datasets to support further research.

Abstract: In this paper, we introduce UI-Genie, a self-improving framework addressing
two key challenges in GUI agents: verification of trajectory outcome is
challenging and high-quality training data are not scalable. These challenges
are addressed by a reward model and a self-improving pipeline, respectively.
The reward model, UI-Genie-RM, features an image-text interleaved architecture
that efficiently pro- cesses historical context and unifies action-level and
task-level rewards. To sup- port the training of UI-Genie-RM, we develop
deliberately-designed data genera- tion strategies including rule-based
verification, controlled trajectory corruption, and hard negative mining. To
address the second challenge, a self-improvement pipeline progressively expands
solvable complex GUI tasks by enhancing both the agent and reward models
through reward-guided exploration and outcome verification in dynamic
environments. For training the model, we generate UI- Genie-RM-517k and
UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI
agents while demonstrating high-quality synthetic trajectory gen- eration
without manual annotation. Experimental results show that UI-Genie achieves
state-of-the-art performance across multiple GUI agent benchmarks with three
generations of data-model self-improvement. We open-source our complete
framework implementation and generated datasets to facilitate further research
in https://github.com/Euphoria16/UI-Genie.

</details>


### [381] [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/abs/2505.21503)
*Yihan Wang,Qiao Yan,Zhenghao Xing,Lihao Liu,Junjun He,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CL

TL;DR: Large language models (LLMs) have great potential in clinical question answering. However, there is a problem called Silent Agreement where agents may prematurely agree on diagnoses without enough critical analysis. To solve this issue, the authors introduce the concept of Catfish Agent, which is designed to challenge consensus and stimulate deeper reasoning. The effectiveness of the Catfish Agent is evaluated through two mechanisms and it outperforms existing LLMs frameworks.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the recurring issue of Silent Agreement in multi-agent LLM frameworks used for clinical question answering. This issue causes agents to prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases.

Method: The method involves introducing a new concept called Catfish Agent, which is a role-specialized LLM designed to inject structured dissent and counter silent agreement. Two mechanisms are formulated to encourage effective and context-aware interventions: (i) complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) tone-calibrated intervention articulated to balance critique and collaboration.

Result: Evaluations on nine medical Q&A and three medical VQA benchmarks show that the approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.

Conclusion: The conclusion is that the introduction of the Catfish Agent concept significantly enhances diagnostic accuracy in clinical question answering by challenging emerging consensus and stimulating deeper reasoning.

Abstract: Large language models (LLMs) have demonstrated strong potential in clinical
question answering, with recent multi-agent frameworks further improving
diagnostic accuracy via collaborative reasoning. However, we identify a
recurring issue of Silent Agreement, where agents prematurely converge on
diagnoses without sufficient critical analysis, particularly in complex or
ambiguous cases. We present a new concept called Catfish Agent, a
role-specialized LLM designed to inject structured dissent and counter silent
agreement. Inspired by the ``catfish effect'' in organizational psychology, the
Catfish Agent is designed to challenge emerging consensus to stimulate deeper
reasoning. We formulate two mechanisms to encourage effective and context-aware
interventions: (i) a complexity-aware intervention that modulates agent
engagement based on case difficulty, and (ii) a tone-calibrated intervention
articulated to balance critique and collaboration. Evaluations on nine medical
Q&A and three medical VQA benchmarks show that our approach consistently
outperforms both single- and multi-agent LLMs frameworks, including leading
commercial models such as GPT-4o and DeepSeek-R1.

</details>


### [382] [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505)
*Shimao Zhang,Zhejian Lai,Xiang Liu,Shuaijie She,Xiao Liu,Yeyun Gong,Shujian Huang,Jiajun Chen*

Main category: cs.CL

TL;DR: The paper proposes a finer-grained neuron identification algorithm to detect various language neurons and divides LLMs' multilingual inference process into four parts, providing insights into multilingual alignment.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding of multilingual capabilities in LLMs by analyzing language-specific neurons and their role in transferring high-resource language capabilities to low-resource languages.

Method: Proposing a new algorithm for identifying different types of neurons (language-specific, language-related, and language-agnostic) and dividing the internal multilingual inference process into four stages.

Result: Systematic analysis of models before and after alignment, offering empirical results and insights into multilingual alignment and capabilities.

Conclusion: The work provides a comprehensive investigation based on different types of neurons, contributing to a better understanding of multilingual alignment in LLMs.

Abstract: Multilingual Alignment is an effective and representative paradigm to enhance
LLMs' multilingual capabilities, which transfers the capabilities from the
high-resource languages to the low-resource languages. Meanwhile, some
researches on language-specific neurons reveal that there are language-specific
neurons that are selectively activated in LLMs when processing different
languages. This provides a new perspective to analyze and understand LLMs'
mechanisms more specifically in multilingual scenarios. In this work, we
propose a new finer-grained neuron identification algorithm, which detects
language neurons~(including language-specific neurons and language-related
neurons) and language-agnostic neurons. Furthermore, based on the
distributional characteristics of different types of neurons, we divide the
LLMs' internal process for multilingual inference into four parts: (1)
multilingual understanding, (2) shared semantic space reasoning, (3)
multilingual output space transformation, and (4) vocabulary space outputting.
Additionally, we systematically analyze the models before and after alignment
with a focus on different types of neurons. We also analyze the phenomenon of
''Spontaneous Multilingual Alignment''. Overall, our work conducts a
comprehensive investigation based on different types of neurons, providing
empirical results and valuable insights for better understanding multilingual
alignment and multilingual capabilities of LLMs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [383] [Let's Get You Hired: A Job Seeker's Perspective on Multi-Agent Recruitment Systems for Explaining Hiring Decisions](https://arxiv.org/abs/2505.20312)
*Aditya Bhattacharya,Katrien Verbert*

Main category: cs.CY

TL;DR: A multi-agent AI system using LLMs is introduced to guide job seekers in recruitment, offering more transparency than traditional methods. An exploratory study with 4 participants and a qualitative study with 20 participants informed the design and showed the system's benefits.


<details>
  <summary>Details</summary>
Motivation: Traditional applicant selection methods lack transparency and sufficient justifications for recruiting decisions.

Method: A multi-agent AI system using Large Language Models (LLMs) was developed. The design was informed by a two-phased exploratory study with four active job seekers and evaluated through an in-depth qualitative study with 20 active job seekers.

Result: Participants perceived the multi-agent recruitment system as significantly more actionable, trustworthy, and fair compared to traditional methods.

Conclusion: The study provides insights into building user-aligned, multi-agent explainable AI systems across diverse domains.

Abstract: During job recruitment, traditional applicant selection methods often lack
transparency. Candidates are rarely given sufficient justifications for
recruiting decisions, whether they are made manually by human recruiters or
through the use of black-box Applicant Tracking Systems (ATS). To address this
problem, our work introduces a multi-agent AI system that uses Large Language
Models (LLMs) to guide job seekers during the recruitment process. Using an
iterative user-centric design approach, we first conducted a two-phased
exploratory study with four active job seekers to inform the design and
development of the system. Subsequently, we conducted an in-depth, qualitative
user study with 20 active job seekers through individual one-to-one interviews
to evaluate the developed prototype. The results of our evaluation demonstrate
that participants perceived our multi-agent recruitment system as significantly
more actionable, trustworthy, and fair compared to traditional methods. Our
study further helped us uncover in-depth insights into factors contributing to
these perceived user experiences. Drawing from these insights, we offer broader
design implications for building user-aligned, multi-agent explainable AI
systems across diverse domains.

</details>


### [384] [Cultural Awareness in Vision-Language Models: A Cross-Country Exploration](https://arxiv.org/abs/2505.20326)
*Avinash Madasu,Vasudev Lal,Phillip Howard*

Main category: cs.CY

TL;DR: This paper proposes a framework to evaluate Vision-Language Models' (VLMs) encoding of cultural differences and biases concerning race, gender, and physical traits across countries through three retrieval-based tasks.


<details>
  <summary>Details</summary>
Motivation: To systematically understand and address the internal biases of VLMs when deployed in diverse cultural contexts.

Method: Propose three retrieval-based tasks: Race to Country retrieval, Personal Traits to Country retrieval, and Physical Characteristics to Country retrieval to evaluate VLMs' biases.

Result: Findings reveal persistent biases in VLMs that may inadvertently reinforce societal stereotypes.

Conclusion: VLMs have significant cultural biases which need to be understood and mitigated to prevent reinforcing societal stereotypes.

Abstract: Vision-Language Models (VLMs) are increasingly deployed in diverse cultural
contexts, yet their internal biases remain poorly understood. In this work, we
propose a novel framework to systematically evaluate how VLMs encode cultural
differences and biases related to race, gender, and physical traits across
countries. We introduce three retrieval-based tasks: (1) Race to Country
retrieval, which examines the association between individuals from specific
racial groups (East Asian, White, Middle Eastern, Latino, South Asian, and
Black) and different countries; (2) Personal Traits to Country retrieval, where
images are paired with trait-based prompts (e.g., Smart, Honest, Criminal,
Violent) to investigate potential stereotypical associations; and (3) Physical
Characteristics to Country retrieval, focusing on visual attributes like
skinny, young, obese, and old to explore how physical appearances are
culturally linked to nations. Our findings reveal persistent biases in VLMs,
highlighting how visual representations may inadvertently reinforce societal
stereotypes.

</details>


### [385] [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
*Anna Neumann,Elisabeth Kirsten,Muhammad Bilal Zafar,Jatinder Singh*

Main category: cs.CY

TL;DR: The study explores how system prompts in Large Language Models (LLMs) impact model behavior by analyzing the processing of demographic information across six LLMs and 50 demographic groups. It uncovers significant biases originating from hidden, layered system prompt configurations, which can lead to representational and allocative harms undetectable by users. The authors advocate for integrating system prompt analysis into AI auditing processes to mitigate these risks.


<details>
  <summary>Details</summary>
Motivation: System prompts in LLMs guide model behavior and are layered with additions by deployers and third-party developers without transparency or visibility into these changes. As they grow more complex, potential side effects arise, prompting questions about how the positioning of information within prompts affects outputs.

Method: The researchers compared how six commercially available LLMs process demographic information when it is included in system versus user prompts across 50 demographic groups.

Result: Significant biases were found in the models' behavior based on the placement of demographic information, leading to differences in user representation and decision-making scenarios. These biases stem from inaccessible and opaque system-level configurations.

Conclusion: The work highlights the critical need to examine system prompts as part of AI auditing processes due to their potential to introduce biases and downstream harms that users cannot detect or correct.

Abstract: System prompts in Large Language Models (LLMs) are predefined directives that
guide model behaviour, taking precedence over user inputs in text processing
and generation. LLM deployers increasingly use them to ensure consistent
responses across contexts. While model providers set a foundation of system
prompts, deployers and third-party developers can append additional prompts
without visibility into others' additions, while this layered implementation
remains entirely hidden from end-users. As system prompts become more complex,
they can directly or indirectly introduce unaccounted for side effects. This
lack of transparency raises fundamental questions about how the position of
information in different directives shapes model outputs. As such, this work
examines how the placement of information affects model behaviour. To this end,
we compare how models process demographic information in system versus user
prompts across six commercially available LLMs and 50 demographic groups. Our
analysis reveals significant biases, manifesting in differences in user
representation and decision-making scenarios. Since these variations stem from
inaccessible and opaque system-level configurations, they risk
representational, allocative and potential other biases and downstream harms
beyond the user's ability to detect or correct. Our findings draw attention to
these critical issues, which have the potential to perpetuate harms if left
unexamined. Further, we argue that system prompt analysis must be incorporated
into AI auditing processes, particularly as customisable system prompts become
increasingly prevalent in commercial AI deployments.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [386] [CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture](https://arxiv.org/abs/2505.20481)
*Berat Kutay Uğraş,Ömer Nezih Gerek,İbrahim Talha Saygı*

Main category: eess.SP

TL;DR: CardioPatternFormer is a novel Transformer-based model for interpretable ECG classification, employing a sophisticated attention mechanism to identify and classify diverse cardiac patterns while providing clear insights through attention maps.


<details>
  <summary>Details</summary>
Motivation: Accurate ECG interpretation is vital in clinical settings, but complex cardiac data and lack of transparency in AI models limit their utility.

Method: The model, CardioPatternFormer, uses a Transformer architecture inspired by NLP techniques to treat ECGs as the heart's unique 'language'. It employs an advanced attention mechanism to detect and classify various cardiac patterns, excelling at identifying subtle anomalies and distinguishing co-occurring conditions.

Result: CardioPatternFormer demonstrates robust performance on challenging ECG cases, including multi-pathology scenarios. Its interpretability via attention maps allows clinicians to understand its decision-making process.

Conclusion: This work presents a powerful and transparent solution for advanced ECG analysis, promoting trust and aiding informed diagnostic decisions in cardiology.

Abstract: Accurate ECG interpretation is vital, yet complex cardiac data and
"black-box" AI models limit clinical utility. Inspired by Transformer
architectures' success in NLP for understanding sequential data, we frame ECG
as the heart's unique "language" of temporal patterns. We present
CardioPatternFormer, a novel Transformer-based model for interpretable ECG
classification. It employs a sophisticated attention mechanism to precisely
identify and classify diverse cardiac patterns, excelling at discerning subtle
anomalies and distinguishing multiple co-occurring conditions. This
pattern-guided attention provides clear insights by highlighting influential
signal regions, effectively allowing the "heart to talk" through transparent
interpretations. CardioPatternFormer demonstrates robust performance on
challenging ECGs, including complex multi-pathology cases. Its interpretability
via attention maps enables clinicians to understand the model's rationale,
fostering trust and aiding informed diagnostic decisions. This work offers a
powerful, transparent solution for advanced ECG analysis, paving the way for
more reliable and clinically actionable AI in cardiology.

</details>


### [387] [Federated Learning-Distillation Alternation for Resource-Constrained IoT](https://arxiv.org/abs/2505.20456)
*Rafael Valente da Silva,Onel L. Alcaraz López,Richard Demo Souza*

Main category: eess.SP

TL;DR: In this paper, the authors propose FL-distillation alternation (FLDA), which balances model accuracy with communication overhead and energy consumption in IoT networks by alternating between federated distillation (FD) and federated learning (FL) phases.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) encounters challenges in IoT networks due to device limitations in energy and communication resources. These challenges are exacerbated when devices rely on energy harvesting (EH), as energy availability can vary significantly over time. Additionally, large model updates in FL are more susceptible to interference from uncorrelated background traffic in shared wireless environments. Federated distillation (FD) reduces communication overhead and energy consumption but at the cost of reduced model accuracy.

Method: The authors propose FL-distillation alternation (FLDA), where devices alternate between FD and FL phases. This approach aims to balance model information with lower communication overhead and energy consumption per iteration.

Result: FLDA demonstrates higher model accuracy than both FL and FD, achieves faster convergence than FL, saves up to 98% in energy consumption relative to FL, and is less sensitive to interference.

Conclusion: FLDA offers a promising solution for IoT networks by effectively balancing model accuracy, communication overhead, and energy consumption.

Abstract: Federated learning (FL) faces significant challenges in Internet of Things
(IoT) networks due to device limitations in energy and communication resources,
especially when considering the large size of FL models. From an energy
perspective, the challenge is aggravated if devices rely on energy harvesting
(EH), as energy availability can vary significantly over time, influencing the
average number of participating users in each iteration. Additionally, the
transmission of large model updates is more susceptible to interference from
uncorrelated background traffic in shared wireless environments. As an
alternative, federated distillation (FD) reduces communication overhead and
energy consumption by transmitting local model outputs, which are typically
much smaller than the entire model used in FL. However, this comes at the cost
of reduced model accuracy. Therefore, in this paper, we propose FL-distillation
alternation (FLDA). In FLDA, devices alternate between FD and FL phases,
balancing model information with lower communication overhead and energy
consumption per iteration. We consider a multichannel slotted-ALOHA EH-IoT
network subject to background traffic/interference. In such a scenario, FLDA
demonstrates higher model accuracy than both FL and FD, and achieves faster
convergence than FL. Moreover, FLDA achieves target accuracies saving up to 98%
in energy consumption, while also being less sensitive to interference, both
relative to FL.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [388] [An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images](https://arxiv.org/abs/2505.20332)
*Neil Chaudhary,Zaynah Dhunny*

Main category: eess.IV

TL;DR: This paper proposes an AI tool using CNN for breast cancer type identification via histopathological images, showing better performance than existing solutions.


<details>
  <summary>Details</summary>
Motivation: To provide a non-invasive and efficient method to identify breast cancer types, reducing the need for additional tests that delay treatment and increase patient burden.

Method: The model uses CNN architecture with image preprocessing techniques to classify benign and malignant tissues and further subcategorize breast cancer types.

Result: The model outperforms several existing solutions in accuracy, precision, recall, and F1-score when tested on relevant datasets.

Conclusion: Deep learning techniques have significant potential in clinical diagnostics, offering a promising tool for pathologists in breast cancer classification.

Abstract: Accurate identification of breast cancer types plays a critical role in
guiding treatment decisions and improving patient outcomes. This paper presents
an artificial intelligence enabled tool designed to aid in the identification
of breast cancer types using histopathological biopsy images. Traditionally
additional tests have to be done on women who are detected with breast cancer
to find out the types of cancer it is to give the necessary cure. Those tests
are not only invasive but also delay the initiation of treatment and increase
patient burden. The proposed model utilizes a convolutional neural network
(CNN) architecture to distinguish between benign and malignant tissues as well
as accurate subclassification of breast cancer types. By preprocessing the
images to reduce noise and enhance features, the model achieves reliable levels
of classification performance. Experimental results on such datasets
demonstrate the model's effectiveness, outperforming several existing solutions
in terms of accuracy, precision, recall, and F1-score. The study emphasizes the
potential of deep learning techniques in clinical diagnostics and offers a
promising tool to assist pathologists in breast cancer classification.

</details>


### [389] [DiffNMR: Advancing Inpainting of Randomly Sampled Nuclear Magnetic Resonance Signals](https://arxiv.org/abs/2505.20367)
*Sen Yan,Fabrizio Gabellieri,Etienne Goffinet,Filippo Castiglione,Thomas Launey*

Main category: eess.IV

TL;DR: The paper proposes using deep learning, specifically diffusion models, to enhance the reconstruction quality of Non-Uniform Sampling (NUS) NMR spectra, showing potential for improving efficiency and accuracy in NMR spectroscopy.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and lengthy experiments in NMR spectroscopy, while overcoming the limitations of Non-Uniform Sampling (NUS) which can introduce artifacts and degrade spectral quality.

Method: Application of diffusion models to both time-time and time-frequency NUS data to improve the reconstruction quality of NUS spectra.

Result: Achieved satisfactory reconstructions of challenging spectra from the benchmark Artina dataset, demonstrating the superiority of time-frequency domain data over time-time data.

Conclusion: Diffusion models have the potential to enhance the efficiency and accuracy of NMR spectroscopy, with promising avenues for future research.

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy leverages nuclear magnetization
to probe molecules' chemical environment, structure, and dynamics, with
applications spanning from pharmaceuticals to the petroleum industry. Despite
its utility, the high cost of NMR instrumentation, operation and the lengthy
duration of experiments necessitate the development of computational techniques
to optimize acquisition times. Non-Uniform sampling (NUS) is widely employed as
a sub-sampling method to address these challenges, but it often introduces
artifacts and degrades spectral quality, offsetting the benefits of reduced
acquisition times. In this work, we propose the use of deep learning techniques
to enhance the reconstruction quality of NUS spectra. Specifically, we explore
the application of diffusion models, a relatively untapped approach in this
domain. Our methodology involves applying diffusion models to both time-time
and time-frequency NUS data, yielding satisfactory reconstructions of
challenging spectra from the benchmark Artina dataset. This approach
demonstrates the potential of diffusion models to improve the efficiency and
accuracy of NMR spectroscopy as well as the superiority of using a
time-frequency domain data over the time-time one, opening new landscapes for
future studies.

</details>


### [390] [Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants](https://arxiv.org/abs/2505.21019)
*Devran Ugurlu,Shuang Qian,Elliot Fairweather,Charlene Mauger,Bram Ruijsink,Laura Dal Toso,Yu Deng,Marina Strocchi,Reza Razavi,Alistair Young,Pablo Lamata,Steven Niederer,Martin Bishop*

Main category: eess.IV

TL;DR: The paper presents an automatic, open-source pipeline for creating patient-specific left and right ventricular meshes from cardiovascular magnetic resonance images. This pipeline was applied to a large cohort of ~55000 participants from UK Biobank, resulting in the most comprehensive cohort of adult heart models to date.


<details>
  <summary>Details</summary>
Motivation: There is a need for anatomically accurate, patient-specific 3D structural representations of the heart for electro-mechanical simulations or study of disease mechanisms. However, generation of cardiac digital twins at scale is demanding and there are no public repositories of models across demographic groups.

Method: An automatic open-source pipeline was developed for creating patient-specific left and right ventricular meshes from cardiovascular magnetic resonance images. This method was then applied to a large cohort of ~55000 participants from UK Biobank.

Result: The result was the construction of the most comprehensive cohort of adult heart models to date, comprising 1423 representative meshes across sex (male, female), body mass index (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years).

Conclusion: This work represents a significant step forward in the generation of cardiac digital twins, providing a valuable resource for the study of cardiovascular diseases.

Abstract: A cardiac digital twin is a virtual replica of a patient's heart for
screening, diagnosis, prognosis, risk assessment, and treatment planning of
cardiovascular diseases. This requires an anatomically accurate
patient-specific 3D structural representation of the heart, suitable for
electro-mechanical simulations or study of disease mechanisms. However,
generation of cardiac digital twins at scale is demanding and there are no
public repositories of models across demographic groups. We describe an
automatic open-source pipeline for creating patient-specific left and right
ventricular meshes from cardiovascular magnetic resonance images, its
application to a large cohort of ~55000 participants from UK Biobank, and the
construction of the most comprehensive cohort of adult heart models to date,
comprising 1423 representative meshes across sex (male, female), body mass
index (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years). Our code is
available at https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025 ,
and pre-trained networks, representative volumetric meshes with fibers and UVCs
will be made available soon.

</details>


### [391] [Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods](https://arxiv.org/abs/2505.21355)
*Muhammad Imran,Wayne G. Brisbane,Li-Ming Su,Jason P. Joseph,Wei Shao*

Main category: eess.IV

TL;DR: The study explores if AI interpretation of micro-US can outperform clinical screening methods for csPCa, showing improved specificity while maintaining sensitivity.


<details>
  <summary>Details</summary>
Motivation: To determine whether AI interpretation of micro-US can outperform traditional clinical screening methods (PSA and DRE) in detecting csPCa.

Method: A self-supervised convolutional autoencoder extracts deep image features from 2D micro-US slices. Random forest classifiers are trained using five-fold cross-validation to predict csPCa at the slice level. Patients classified as csPCa-positive if 88 or more consecutive slices predicted positive.

Result: The AI-based micro-US model achieved an AUROC of 0.871 compared to 0.753 for the clinical screening model. At a fixed threshold, the micro-US model showed 92.5% sensitivity and 68.1% specificity, while the clinical model had 96.2% sensitivity but only 27.3% specificity.

Conclusion: AI-interpreted micro-US improves specificity while maintaining high sensitivity for csPCa detection, potentially reducing unnecessary biopsies and serving as a low-cost alternative to PSA-based screening.

Abstract: Background and objective: Micro-ultrasound (micro-US) is a novel imaging
modality with diagnostic accuracy comparable to MRI for detecting clinically
significant prostate cancer (csPCa). We investigated whether artificial
intelligence (AI) interpretation of micro-US can outperform clinical screening
methods using PSA and digital rectal examination (DRE). Methods: We
retrospectively studied 145 men who underwent micro-US guided biopsy (79 with
csPCa, 66 without). A self-supervised convolutional autoencoder was used to
extract deep image features from 2D micro-US slices. Random forest classifiers
were trained using five-fold cross-validation to predict csPCa at the slice
level. Patients were classified as csPCa-positive if 88 or more consecutive
slices were predicted positive. Model performance was compared with a
classifier using PSA, DRE, prostate volume, and age. Key findings and
limitations: The AI-based micro-US model and clinical screening model achieved
AUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US
model achieved 92.5% sensitivity and 68.1% specificity, while the clinical
model showed 96.2% sensitivity but only 27.3% specificity. Limitations include
a retrospective single-center design and lack of external validation.
Conclusions and clinical implications: AI-interpreted micro-US improves
specificity while maintaining high sensitivity for csPCa detection. This method
may reduce unnecessary biopsies and serve as a low-cost alternative to
PSA-based screening. Patient summary: We developed an AI system to analyze
prostate micro-ultrasound images. It outperformed PSA and DRE in detecting
aggressive cancer and may help avoid unnecessary biopsies.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [392] [Streamlining Knowledge Graph Creation with PyRML](https://arxiv.org/abs/2505.20949)
*Andrea Giovanni Nuzzolese*

Main category: cs.DB

TL;DR: PyRML is a Python library for building Knowledge Graphs through declarative mappings, integrating with popular data and semantic web libraries while lowering the barrier to entry for KG creation.


<details>
  <summary>Details</summary>
Motivation: Knowledge Graphs are increasingly adopted as a foundational technology for integrating heterogeneous data in domains such as climate science, cultural heritage, and the life sciences. Declarative mapping languages like R2RML and RML have played a central role in enabling scalable and reusable KG construction.

Method: PyRML supports core RML constructs and provides a programmable interface for authoring, executing, and testing mappings directly within Python environments. It integrates with popular data and semantic web libraries (e.g., Pandas and RDFlib).

Result: PyRML bridges the gap between declarative semantics and practical KG engineering by lowering the barrier to entry for KG creation and fostering reproducible, ontology-aligned data integration.

Conclusion: PyRML is a lightweight, Python-native library for building Knowledge Graphs through declarative mappings.

Abstract: Knowledge Graphs (KGs) are increasingly adopted as a foundational technology
for integrating heterogeneous data in domains such as climate science, cultural
heritage, and the life sciences. Declarative mapping languages like R2RML and
RML have played a central role in enabling scalable and reusable KG
construction, offering a transparent means of transforming structured and
semi-structured data into RDF. In this paper, we present PyRML, a lightweight,
Python-native library for building Knowledge Graphs through declarative
mappings. PyRML supports core RML constructs and provides a programmable
interface for authoring, executing, and testing mappings directly within Python
environments. It integrates with popular data and semantic web libraries (e.g.,
Pandas and RDFlib), enabling transparent and modular workflows. By lowering the
barrier to entry for KG creation and fostering reproducible, ontology-aligned
data integration, PyRML bridges the gap between declarative semantics and
practical KG engineering.

</details>


### [393] [LazyVLM: Neuro-Symbolic Approach to Video Analytics](https://arxiv.org/abs/2505.21459)
*Xiangru Jian,Wei Pang,Zhengyuan Dong,Chao Zhang,M. Tamer Özsu*

Main category: cs.DB

TL;DR: Current video analytics methods either lack efficiency or flexibility. LazyVLM is introduced as a neuro-symbolic system that combines the strengths of VLMs and symbolic methods, offering a scalable, efficient, and user-friendly solution for complex video queries.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current video analytics approaches which either struggle with long-context processing and high computational costs (end-to-end Vision Language Models) or depend heavily on manual labeling and rigid rule design (neural-symbolic methods).

Method: LazyVLM decomposes multi-frame video queries into fine-grained operations and offloads the bulk of the processing to efficient relational query execution and vector similarity search, providing a user-friendly semi-structured text interface.

Result: LazyVLM provides a robust, efficient, and user-friendly solution for querying open-domain video data at scale.

Conclusion: LazyVLM addresses the scalability limitations of Vision Language Models while maintaining their flexibility, making it an effective tool for large-scale video analytics.

Abstract: Current video analytics approaches face a fundamental trade-off between
flexibility and efficiency. End-to-end Vision Language Models (VLMs) often
struggle with long-context processing and incur high computational costs, while
neural-symbolic methods depend heavily on manual labeling and rigid rule
design. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics
system that provides a user-friendly query interface similar to VLMs, while
addressing their scalability limitation. LazyVLM enables users to effortlessly
drop in video data and specify complex multi-frame video queries using a
semi-structured text interface for video analytics. To address the scalability
limitations of VLMs, LazyVLM decomposes multi-frame video queries into
fine-grained operations and offloads the bulk of the processing to efficient
relational query execution and vector similarity search. We demonstrate that
LazyVLM provides a robust, efficient, and user-friendly solution for querying
open-domain video data at scale.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [394] [Transfer learning for multifidelity simulation-based inference in cosmology](https://arxiv.org/abs/2505.21215)
*Alex A. Saoulis,Davide Piras,Niall Jeffrey,Alessio Spurio Mancini,Ana M. G. Ferreira,Benjamin Joachimi*

Main category: astro-ph.CO

TL;DR: 通过多保真度迁移学习，结合低保真和高保真模拟，减少高保真模拟需求，从而降低计算成本，同时保持高性能和准确的推理。


<details>
  <summary>Details</summary>
Motivation: 在无法获得闭合形式的似然函数或模型时，基于仿真的推断（SBI）可以进行宇宙学参数估计，但其依赖于机器学习进行神经压缩和密度估计，需要大量的训练数据集，这对于高质量的模拟来说成本过高。

Method: 使用多保真度迁移学习，将低成本的低保真模拟与少量高保真模拟相结合，并在CAMELS多场数据集中的暗物质密度图上展示了该方法。预训练在仅含暗物质的N体模拟上，减少了对高保真水动力模拟的需求。

Result: 根据模型复杂性、后验维度和性能指标的不同，预训练可将高保真水动力模拟的需求减少8到15倍。

Conclusion: 利用更便宜的模拟，该方法可以在大幅降低计算成本的同时，实现高性能和精确的高保真模型推理。

Abstract: Simulation-based inference (SBI) enables cosmological parameter estimation
when closed-form likelihoods or models are unavailable. However, SBI relies on
machine learning for neural compression and density estimation. This requires
large training datasets which are prohibitively expensive for high-quality
simulations. We overcome this limitation with multifidelity transfer learning,
combining less expensive, lower-fidelity simulations with a limited number of
high-fidelity simulations. We demonstrate our methodology on dark matter
density maps from two separate simulation suites in the hydrodynamical CAMELS
Multifield Dataset. Pre-training on dark-matter-only $N$-body simulations
reduces the required number of high-fidelity hydrodynamical simulations by a
factor between $8$ and $15$, depending on the model complexity, posterior
dimensionality, and performance metrics used. By leveraging cheaper
simulations, our approach enables performant and accurate inference on
high-fidelity models while substantially reducing computational costs.

</details>


### [395] [Wavelet Flow For Extragalactic Foreground Simulations](https://arxiv.org/abs/2505.21220)
*M. Mebratu,W. L. K. Wu*

Main category: astro-ph.CO

TL;DR: Wavelet Flow (WF) models are explored for modeling field-level probability distributions of multi-component CMB secondaries, accurately simulating correlated components like CMB lensing convergence and cosmic infrared background.


<details>
  <summary>Details</summary>
Motivation: Extragalactic foregrounds in CMB observations present both opportunities for cosmological insights and challenges as nuisances. Accurate field-level modeling capturing non-Gaussian statistical distributions is crucial for optimal information extraction, especially with the precision of current and upcoming experiments.

Method: The authors use Wavelet Flow (WF) models to jointly train CMB lensing convergence and cosmic infrared background maps. They leverage the multiscale architecture of these models to independently fine-tune model parameters and priors at each scale, optimizing performance across different resolutions.

Result: The trained network generates samples whose average power spectra are within a few percent of the inputs across all scales, and Minkowski functionals are similarly accurate compared to the inputs.

Conclusion: WF models can accurately simulate correlated components of CMB secondaries, supporting improved analysis of cosmological data.

Abstract: Extragalactic foregrounds in cosmic microwave background (CMB) observations
are both a source of cosmological and astrophysical information and a nuisance
to the CMB. Effective field-level modeling that captures their non-Gaussian
statistical distributions is increasingly important for optimal information
extraction, particularly given the precise and low-noise observations from
current and upcoming experiments. We explore the use of Wavelet Flow (WF)
models to tackle the novel task of modeling the field-level probability
distributions of multi-component CMB secondaries. Specifically, we jointly
train correlated CMB lensing convergence ($\kappa$) and cosmic infrared
background (CIB) maps with a WF model and obtain a network that statistically
recovers the input to high accuracy -- the trained network generates samples of
$\kappa$ and CIB fields whose average power spectra are within a few percent of
the inputs across all scales, and whose Minkowski functionals are similarly
accurate compared to the inputs. Leveraging the multiscale architecture of
these models, we fine-tune both the model parameters and the priors at each
scale independently, optimizing performance across different resolutions. These
results demonstrate that WF models can accurately simulate correlated
components of CMB secondaries, supporting improved analysis of cosmological
data. Our code and trained models can be found here
(https://github.com/matiwosm/HybridPriorWavletFlow.git).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [396] [MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems](https://arxiv.org/abs/2505.20824)
*Kai Chen,Taihang Zhen,Hewei Wang,Kailai Liu,Xinfeng Li,Jing Huo,Tianpei Yang,Jinfeng Xu,Wei Dong,Yang Gao*

Main category: cs.MA

TL;DR: The paper introduces MedSentry, a benchmark with 5000 adversarial medical prompts across 25 threat categories and 100 subthemes. It evaluates four multi-agent topologies (Layers, SharedPool, Centralized, and Decentralized) for safety in healthcare LLMs. Findings show differences in vulnerability mechanisms among these architectures. A personality-scale detection and correction mechanism is proposed to mitigate risks.


<details>
  <summary>Details</summary>
Motivation: To ensure the safety of large language models (LLMs) in healthcare, especially within collaborative multi-agent configurations.

Method: Developed MedSentry, a benchmark containing adversarial medical prompts, and an end-to-end attack-defense evaluation pipeline to analyze multi-agent topologies' resistance to attacks from 'dark-personality' agents.

Result: Critical differences in handling information contamination were found among the architectures, with SharedPool being highly susceptible and Decentralized showing greater resilience. A detection and correction mechanism was effective in restoring system safety.

Conclusion: MedSentry provides a rigorous evaluation framework and practical defense strategies for designing safer LLM-based multi-agent systems in medical domains.

Abstract: As large language models (LLMs) are increasingly deployed in healthcare,
ensuring their safety, particularly within collaborative multi-agent
configurations, is paramount. In this paper we introduce MedSentry, a benchmark
comprising 5 000 adversarial medical prompts spanning 25 threat categories with
100 subthemes. Coupled with this dataset, we develop an end-to-end
attack-defense evaluation pipeline to systematically analyze how four
representative multi-agent topologies (Layers, SharedPool, Centralized, and
Decentralized) withstand attacks from 'dark-personality' agents. Our findings
reveal critical differences in how these architectures handle information
contamination and maintain robust decision-making, exposing their underlying
vulnerability mechanisms. For instance, SharedPool's open information sharing
makes it highly susceptible, whereas Decentralized architectures exhibit
greater resilience thanks to inherent redundancy and isolation. To mitigate
these risks, we propose a personality-scale detection and correction mechanism
that identifies and rehabilitates malicious agents, restoring system safety to
near-baseline levels. MedSentry thus furnishes both a rigorous evaluation
framework and practical defense strategies that guide the design of safer
LLM-based multi-agent systems in medical domains.

</details>


### [397] [Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective](https://arxiv.org/abs/2505.20922)
*Yang Zhang,Xinran Li,Jianing Ye,Delin Qu,Shuang Qiu,Chongjie Zhang,Xiu Li,Chenjia Bai*

Main category: cs.MA

TL;DR: Diffusion-Inspired Multi-Agent world model (DIMA) is developed using diffusion models to improve sample efficiency in MARL. It progressively resolves uncertainty while capturing dependencies among agents and achieves state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling environments in MARL is challenging due to the large joint action space and uncertain dynamics.

Method: The approach reduces modeling complexity by focusing on the state space alone at each timestep through sequential agent modeling, aligning with the reverse process in diffusion models.

Result: DIMA significantly outperforms prior world models in terms of final return and sample efficiency, achieving state-of-the-art performance across multiple multi-agent control benchmarks.

Conclusion: DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research.

Abstract: World models have recently attracted growing interest in Multi-Agent
Reinforcement Learning (MARL) due to their ability to improve sample efficiency
for policy learning. However, accurately modeling environments in MARL is
challenging due to the exponentially large joint action space and highly
uncertain dynamics inherent in multi-agent systems. To address this, we reduce
modeling complexity by shifting from jointly modeling the entire state-action
transition dynamics to focusing on the state space alone at each timestep
through sequential agent modeling. Specifically, our approach enables the model
to progressively resolve uncertainty while capturing the structured
dependencies among agents, providing a more accurate representation of how
agents influence the state. Interestingly, this sequential revelation of
agents' actions in a multi-agent system aligns with the reverse process in
diffusion models--a class of powerful generative models known for their
expressiveness and training stability compared to autoregressive or latent
variable models. Leveraging this insight, we develop a flexible and robust
world model for MARL using diffusion models. Our method, Diffusion-Inspired
Multi-Agent world model (DIMA), achieves state-of-the-art performance across
multiple multi-agent control benchmarks, significantly outperforming prior
world models in terms of final return and sample efficiency, including MAMuJoCo
and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent
world models, advancing the frontier of MARL research.

</details>


### [398] [GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation](https://arxiv.org/abs/2505.21154)
*Hailin Zhong,Hanlin Wang,Yujun Ye,Meiyi Zhang,Shengxin Zhu*

Main category: cs.MA

TL;DR: Current recommender systems lack the ability to capture long-term user preference evolution and social influence dynamics. To solve this, a new simulation platform is proposed that integrates human-like cognitive agents and dynamic social interactions. This includes Sim-User Agents with a five-layer cognitive architecture, an ICR2 motivational engine, and a multilayer heterogeneous social graph called GGBond Graph. During operation, agents respond to recommendations from various algorithms, forming a feedback loop.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitation of current personalized recommender systems which rely on static offline data, thus being unable to effectively capture long-term user preference evolution and social influence dynamics in real-world scenarios.

Method: The method involves proposing a high-fidelity social simulation platform that integrates human-like cognitive agents and dynamic social interactions. The system comprises Sim-User Agents with a five-layer cognitive architecture, including episodic memory, affective state transitions, adaptive preference learning, and dynamic trust-risk assessments. Additionally, the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine is introduced, enabling more realistic user decision-making processes. A multilayer heterogeneous social graph (GGBond Graph) is constructed to model evolving social ties and trust dynamics based on interest similarity, personality alignment, and structural homophily.

Result: The result is a stable, multi-round feedback loop where agents autonomously respond to recommendations generated by typical recommender algorithms, deciding whether to consume, rate, and share content while dynamically updating their internal states and social connections. This design provides a controlled, observable environment for evaluating long-term recommender effects.

Conclusion: In conclusion, the innovative design of the proposed simulation platform transcends the limitations of traditional static datasets, providing a means to evaluate long-term recommender effects in a controlled and observable environment.

Abstract: Current personalized recommender systems predominantly rely on static offline
data for algorithm design and evaluation, significantly limiting their ability
to capture long-term user preference evolution and social influence dynamics in
real-world scenarios. To address this fundamental challenge, we propose a
high-fidelity social simulation platform integrating human-like cognitive
agents and dynamic social interactions to realistically simulate user behavior
evolution under recommendation interventions. Specifically, the system
comprises a population of Sim-User Agents, each equipped with a five-layer
cognitive architecture that encapsulates key psychological mechanisms,
including episodic memory, affective state transitions, adaptive preference
learning, and dynamic trust-risk assessments. In particular, we innovatively
introduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine
grounded in psychological and sociological theories, enabling more realistic
user decision-making processes. Furthermore, we construct a multilayer
heterogeneous social graph (GGBond Graph) supporting dynamic relational
evolution, effectively modeling users' evolving social ties and trust dynamics
based on interest similarity, personality alignment, and structural homophily.
During system operation, agents autonomously respond to recommendations
generated by typical recommender algorithms (e.g., Matrix Factorization,
MultVAE, LightGCN), deciding whether to consume, rate, and share content while
dynamically updating their internal states and social connections, thereby
forming a stable, multi-round feedback loop. This innovative design transcends
the limitations of traditional static datasets, providing a controlled,
observable environment for evaluating long-term recommender effects.

</details>


### [399] [Large Language Models Miss the Multi-Agent Mark](https://arxiv.org/abs/2505.21298)
*Emanuele La Malfa,Gabriele La Malfa,Samuele Marro,Jie M. Zhang,Elizabeth Black,Micheal Luck,Philip Torr,Michael Wooldridge*

Main category: cs.MA

TL;DR: 近期关于大型语言模型的多智能体系统（MAS LLMs）的兴趣增加，但很多实现缺乏真正的多智能体特性。本文分析了MAS理论与当前MAS LLMs之间的关键差异，并提出应更好地整合MAS概念以推动研究进展。


<details>
  <summary>Details</summary>
Motivation: 目前许多关于多智能体系统的大型语言模型的研究和实现虽然使用了MAS术语，但并未深入理解其核心原则，可能导致研究进展缓慢或错失机会。

Method: 通过聚焦于代理的社会性、环境设计、协调与通信协议以及测量涌现行为这四个关键领域，系统地分析了MAS理论与当前MAS LLMs实现之间的差异。

Result: 发现许多MAS LLMs缺乏诸如自主性、社会互动和结构化环境等多智能体特征，且往往依赖过度简化的、以LLM为中心的架构。

Conclusion: 建议研究者更好地整合已建立的MAS概念和更精确的术语，以避免误解和错失潜在的研究机会。

Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)
has led to an increase in frameworks leveraging multiple LLMs to tackle complex
tasks. However, much of this literature appropriates the terminology of MAS
without engaging with its foundational principles. In this position paper, we
highlight critical discrepancies between MAS theory and current MAS LLMs
implementations, focusing on four key areas: the social aspect of agency,
environment design, coordination and communication protocols, and measuring
emergent behaviours. Our position is that many MAS LLMs lack multi-agent
characteristics such as autonomy, social interaction, and structured
environments, and often rely on oversimplified, LLM-centric architectures. The
field may slow down and lose traction by revisiting problems the MAS literature
has already addressed. Therefore, we systematically analyse this issue and
outline associated research opportunities; we advocate for better integrating
established MAS concepts and more precise terminology to avoid
mischaracterisation and missed opportunities.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [400] [Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach](https://arxiv.org/abs/2505.21139)
*Subhagata Chattopadhyay,Amit K Chattopadhyay*

Main category: q-bio.PE

TL;DR: The study uses a hybrid machine learning approach to analyze 13 key heart attack risk factors, revealing strong associations and emphasizing the aggravated risk for postmenopausal patients due to estrogen depletion and extraneous stress impacts.


<details>
  <summary>Details</summary>
Motivation: To explore the underlying mechanisms of increased cardiovascular events post-COVID-19 infection, particularly myocardial infarction, in individuals over 40.

Method: A hybrid machine learning approach was employed to analyze epidemiological data combining demographic, biochemical, ECG, and thallium stress-tests. Clustering algorithms categorized populations into 'at-risk' (AR) and 'not-at-risk' (NAR) groups based on 13 key heart attack risk factors.

Result: Strong associations were found between the likelihood of experiencing a heart attack and the 13 studied risk factors. Postmenopausal patients exhibited an aggravated risk due to compromised individual risk factors from estrogen depletion and extraneous stress impacts like anxiety and fear.

Conclusion: Hybrid machine learning models effectively reveal associations in complex epidemiological data, highlighting the need to consider hormonal changes and psychological stress in predicting heart attack risks.

Abstract: The COVID-19 pandemic has significantly increased the incidence of
post-infection cardiovascular events, particularly myocardial infarction, in
individuals over 40. While the underlying mechanisms remain elusive, this study
employs a hybrid machine learning approach to analyze epidemiological data in
assessing 13 key heart attack risk factors and their susceptibility. Based on a
unique dataset that combines demographic, biochemical, ECG, and thallium
stress-tests, this study categorizes distinct subpopulations against varying
risk profiles and then divides the population into 'at-risk' (AR) and
'not-at-risk' (NAR) groups using clustering algorithms. The study reveals
strong association between the likelihood of experiencing a heart attack on the
13 risk factors studied. The aggravated risk for postmenopausal patients
indicates compromised individual risk factors due to estrogen depletion that
may be, further compromised by extraneous stress impacts, like anxiety and
fear, aspects that have traditionally eluded data modeling predictions.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [401] [Fixed-Point Traps and Identity Emergence in Educational Feedback Systems](https://arxiv.org/abs/2505.21038)
*Faruk Alpay*

Main category: math.CT

TL;DR: This paper uses category theory to prove that exam-driven education systems prevent identity formation and creativity by collapsing learning dynamics.


<details>
  <summary>Details</summary>
Motivation: To provide a formal mathematical proof for how exam-based educational systems obstruct the emergence of learner identity and creativity through algebraic and categorical constructs.

Method: Defines Exam-Grade Collapse Systems (EGCS) using Alpay Algebra II and III as functorial constructs with evaluative morphisms recursively collapsing learning dynamics. Proves the nonexistence of nontrivial fixed-point algebras under such systems.

Result: Proves that exam-driven systems create a 'fixed-point trap' leading to creativity suppression, research stagnation, and entropy loss in educational settings.

Conclusion: Applies category theory to demonstrate why modern exam-focused educational systems inhibit identity emergence and creative convergence, offering the first algebraic proof of this obstruction.

Abstract: This paper presents a formal categorical proof that exam-driven educational
systems obstruct identity emergence and block creative convergence. Using the
framework of Alpay Algebra II and III, we define Exam-Grade Collapse Systems
(EGCS) as functorial constructs where learning dynamics $\varphi$ are
recursively collapsed by evaluative morphisms $E$. We prove that under such
collapse regimes, no nontrivial fixed-point algebra $\mu_\varphi$ can exist,
hence learner identity cannot stabilize. This creates a universal fixed-point
trap: all generative functors are entropically folded before symbolic emergence
occurs. Our model mathematically explains the creativity suppression, research
stagnation, and structural entropy loss induced by timed exams and grade-based
feedback. The results apply category theory to expose why modern educational
systems prevent {\phi}-emergence and block observer-invariant self-formation.
This work provides the first provable algebraic obstruction of identity
formation caused by institutional feedback mechanics.

</details>
