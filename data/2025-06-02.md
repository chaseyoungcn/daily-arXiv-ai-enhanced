<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.LG](#cs.LG) [Total: 141]
- [cs.CR](#cs.CR) [Total: 40]
- [cs.NE](#cs.NE) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 71]
- [cs.RO](#cs.RO) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 38]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [stat.ML](#stat.ML) [Total: 15]
- [math.OC](#math.OC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/abs/2505.23881)
*Christopher D. Rosin*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) with reasoning capabilities can generate search heuristics to solve combinatorial design problems, successfully solving longstanding open instances and generating new solutions.


<details>
  <summary>Details</summary>
Motivation: To apply code generation with reasoning LLMs to solve open instances in the mathematical field of combinatorial design, where many problems have yet to be solved.

Method: Using Constructive Protocol CPro1, which guides LLMs to select and implement strategies for constructing solutions to small open instances, while providing automated hyperparameter tuning and execution feedback.

Result: CPro1 with reasoning LLMs solved longstanding open instances for 7 of 16 combinatorial design problems from the 2006 Handbook of Combinatorial Designs, including new solved instances for Bhaskar Rao Designs, Symmetric Weighing Matrices, and Balanced Ternary Designs. It also solved open instances for several problems from recent literature, generating new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform Nested Steiner Quadruple System.

Conclusion: Reasoning LLMs integrated with CPro1 show promise in advancing the field of combinatorial design by solving previously unsolved problems.

Abstract: Large Language Models (LLMs) with reasoning are trained to iteratively
generate and refine their answers before finalizing them, which can help with
applications to mathematics and code generation. We apply code generation with
reasoning LLMs to a specific task in the mathematical field of combinatorial
design. This field studies diverse types of combinatorial designs, many of
which have lists of open instances for which existence has not yet been
determined. The Constructive Protocol CPro1 uses LLMs to generate search
heuristics that have the potential to construct solutions to small open
instances. Starting with a textual definition and a validity verifier for a
particular type of design, CPro1 guides LLMs to select and implement
strategies, while providing automated hyperparameter tuning and execution
feedback. CPro1 with reasoning LLMs successfully solves long-standing open
instances for 7 of 16 combinatorial design problems selected from the 2006
Handbook of Combinatorial Designs, including new solved instances for 3 of
these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary
Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves
open instances for several problems from recent (2025) literature, generating
new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform
Nested Steiner Quadruple System.

</details>


### [2] [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
*Mengkang Hu,Yuhang Zhou,Wendong Fan,Yuzhou Nie,Bowei Xia,Tao Sun,Ziyu Ye,Zhaoxuan Jin,Yingru Li,Qiguang Chen,Zeyu Zhang,Yifeng Wang,Qianshuo Ye,Bernard Ghanem,Ping Luo,Guohao Li*

Main category: cs.AI

TL;DR: Workforce is a hierarchical multi-agent framework that decouples strategic planning from specialized execution, enabling cross-domain transferability. It comprises a domain-agnostic Planner, a Coordinator, and specialized Workers. OWL optimizes the planner with reinforcement learning for better generalization. Workforce achieves state-of-the-art performance on the GAIA benchmark.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent systems struggle to transfer across domains due to their domain-specific nature, requiring complete architectural redesign and full retraining when applied to new domains.

Method: Workforce includes a domain-agnostic Planner for task decomposition, a Coordinator for subtask management, and specialized Workers with domain-specific tool-calling capabilities. OWL optimizes the Planner with reinforcement learning from real-world feedback.

Result: Workforce achieves open-source state-of-the-art performance (69.70%) on the GAIA benchmark, outperforming commercial systems like OpenAI's Deep Research by 2.34%. The OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and performs comparably to GPT-4o on challenging tasks.

Conclusion: Workforce establishes a foundation for the next generation of general-purpose AI assistants by enabling scalable generalization and modular domain transfer.

Abstract: Large Language Model (LLM)-based multi-agent systems show promise for
automating real-world tasks but struggle to transfer across domains due to
their domain-specific nature. Current approaches face two critical
shortcomings: they require complete architectural redesign and full retraining
of all components when applied to new domains. We introduce Workforce, a
hierarchical multi-agent framework that decouples strategic planning from
specialized execution through a modular architecture comprising: (i) a
domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask
management, and (iii) specialized Workers with domain-specific tool-calling
capabilities. This decoupling enables cross-domain transferability during both
inference and training phases: During inference, Workforce seamlessly adapts to
new domains by adding or modifying worker agents; For training, we introduce
Optimized Workforce Learning (OWL), which improves generalization across
domains by optimizing a domain-agnostic planner with reinforcement learning
from real-world feedback. To validate our approach, we evaluate Workforce on
the GAIA benchmark, covering various realistic, multi-domain agentic tasks.
Experimental results demonstrate Workforce achieves open-source
state-of-the-art performance (69.70%), outperforming commercial systems like
OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model
achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to
GPT-4o on challenging tasks. To summarize, by enabling scalable generalization
and modular domain transfer, our work establishes a foundation for the next
generation of general-purpose AI assistants.

</details>


### [3] [Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve](https://arxiv.org/abs/2505.23946)
*Yuanzhe Liu,Ryan Deng,Tim Kaler,Xuhao Chen,Charles E. Leiserson,Yao Ma,Jie Chen*

Main category: cs.AI

TL;DR: 通过一个基于lesson的协作框架，多个小型LLM可以通过相互学习成功与失败的经验来提升自身性能，在解决编码问题时，其表现可以优于大型LLM和其他多LLM协作方法，即使在事先不知道它们互补优势的情况下。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到不同LLM在各种任务上的表现存在多粒度差异，例如在代码优化任务中，不同的code LLM擅长不同的优化类别且没有一个能全面超越其他模型。这引发了如何在不预先了解各自优势的情况下，利用多个LLM代理解决编码问题的思考。

Method: 提出了一种基于lesson的协作框架，设计了lesson的征集-存储-选择机制，使一组代理能够在集体解决问题的过程中相互学习成功和失败的知识以提高各自的性能。

Result: 实验表明，使用所提出的框架，一组小型LLM能够超越大型LLM以及其他多LLM协作方法。

Conclusion: 基于lesson的协作框架为提升多LLM团队解决问题的能力提供了新途径，展示了通过相互学习改进性能的有效性。

Abstract: Recent studies show that LLMs possess different skills and specialize in
different tasks. In fact, we observe that their varied performance occur in
several levels of granularity. For example, in the code optimization task, code
LLMs excel at different optimization categories and no one dominates others.
This observation prompts the question of how one leverages multiple LLM agents
to solve a coding problem without knowing their complementary strengths a
priori. We argue that a team of agents can learn from each other's successes
and failures so as to improve their own performance. Thus, a lesson is the
knowledge produced by an agent and passed on to other agents in the collective
solution process. We propose a lesson-based collaboration framework, design the
lesson solicitation--banking--selection mechanism, and demonstrate that a team
of small LLMs with lessons learned can outperform a much larger LLM and other
multi-LLM collaboration methods.

</details>


### [4] [InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback](https://arxiv.org/abs/2505.23950)
*Boyuan Chen,Donghai Hong,Jiaming Ji,Jiacheng Zheng,Bowen Dong,Jiayi Zhou,Kaile Wang,Juntao Dai,Xuyao Wang,Wenqi Chen,Qirui Zheng,Wenxin Li,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 该论文探讨了多模态大模型（MLLMs）在交互能力上的不足，并提出了InterMT数据集和InterMT-Bench基准来改进和评估这些模型的多轮、多模态交互能力。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在许多任务上表现出色，但它们仍缺乏类似于人类的复杂交互能力，特别是多轮、多模态的交互。为了向人类智能靠近，模型需要具备理解交织的多模态上下文并连贯回应的能力。

Method: 作者创建了首个基于真实人类反馈的多轮多模态交互偏好数据集InterMT，包含15.6k提示、52.6k对话实例和32.4k人工标注的偏好对。此外，还引入了一种代理增强的工作流程，以弥补当前MLLMs在多模态理解和生成方面的能力不足。最后，提出了InterMT-Bench基准，用于评估MLLMs在多轮多模态任务中的表现。

Result: 通过应用如裁判调节等任务，揭示了裁判模型的多轮扩展规律，并展示了InterMT数据集的实用性。这有助于推动MLLMs在多模态交互方面的进一步研究和改进。

Conclusion: 作者希望开源的InterMT数据集能够促进对当前MLLMs的研究，使其更接近人类水平的多模态交互能力。项目网站为https://pku-intermt.github.io。

Abstract: As multimodal large models (MLLMs) continue to advance across challenging
tasks, a key question emerges: What essential capabilities are still missing? A
critical aspect of human learning is continuous interaction with the
environment -- not limited to language, but also involving multimodal
understanding and generation. To move closer to human-level intelligence,
models must similarly support multi-turn, multimodal interaction. In
particular, they should comprehend interleaved multimodal contexts and respond
coherently in ongoing exchanges. In this work, we present an initial
exploration through the InterMT -- the first preference dataset for multi-turn
multimodal interaction, grounded in real human feedback. In this exploration,
we particularly emphasize the importance of human oversight, introducing expert
annotations to guide the process, motivated by the fact that current MLLMs lack
such complex interactive capabilities. InterMT captures human preferences at
both global and local levels into nine sub-dimensions, consists of 15.6k
prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled
preference pairs. To compensate for the lack of capability for multi-modal
understanding and generation, we introduce an agentic workflow that leverages
tool-augmented MLLMs to construct multi-turn QA instances. To further this
goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting
judges with multi-turn, multimodal tasks. We demonstrate the utility of
\InterMT through applications such as judge moderation and further reveal the
multi-turn scaling law of judge model. We hope the open-source of our data can
help facilitate further research on aligning current MLLMs to the next step.
Our project website can be found at https://pku-intermt.github.io .

</details>


### [5] [MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge](https://arxiv.org/abs/2505.23982)
*Jerry Junyang Cheung,Shiyao Shen,Yuchen Zhuang,Yinghao Li,Rampi Ramprasad,Chao Zhang*

Main category: cs.AI

TL;DR: 尽管大型语言模型（LLMs）在材料科学领域取得了进展，但缺乏评估其领域特定知识和复杂推理能力的基准。为填补这一空白，研究者引入了MSQA，这是一个包含1757个研究生水平材料科学问题的综合评估基准，涵盖详细解释性回答和二元是非判断两种形式。通过实验发现，当前LLMs性能存在显著差距：基于API的专有LLMs准确率可达84.5%，而开源LLMs约为60.5%，领域专用LLMs因过拟合和分布偏移表现较差。MSQA是首个同时评估LLMs事实和推理能力的基准，对推动材料科学领域LLMs发展具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在材料科学领域的应用中，缺乏对其领域特定知识和复杂推理能力的有效评估标准。

Method: 构建了一个名为MSQA的综合评估基准，包含1757个研究生水平的材料科学问题，涵盖详细解释性回答和二元是非判断两种形式，涉及材料科学的七个子领域。使用该基准对10种最先进的大型语言模型进行实验评估。

Result: 实验结果显示，基于API的专有LLMs表现最佳，准确率可达84.5%；开源LLMs次之，准确率约为60.5%；领域专用LLMs由于过拟合和分布偏移，表现较差。

Conclusion: MSQA是首个能够同时评估大型语言模型事实知识和推理能力的基准，对于推动材料科学领域大型语言模型的发展具有重要作用。

Abstract: Despite recent advances in large language models (LLMs) for materials
science, there is a lack of benchmarks for evaluating their domain-specific
knowledge and complex reasoning abilities. To bridge this gap, we introduce
MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials
science questions in two formats: detailed explanatory responses and binary
True/False assessments. MSQA distinctively challenges LLMs by requiring both
precise factual knowledge and multi-step reasoning across seven materials
science sub-fields, such as structure-property relationships, synthesis
processes, and computational modeling. Through experiments with 10
state-of-the-art LLMs, we identify significant gaps in current LLM performance.
While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source
(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform
significantly due to overfitting and distributional shifts. MSQA represents the
first benchmark to jointly evaluate the factual and reasoning capabilities of
LLMs crucial for LLMs in advanced materials science.

</details>


### [6] [Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding](https://arxiv.org/abs/2505.23990)
*Mingyang Mao,Mariela M. Perez-Cabarcas,Utteja Kallakuri,Nicholas R. Waytowich,Xiaomin Lin,Tinoosh Mohsenin*

Main category: cs.AI

TL;DR: Multi-RAG is a multimodal retrieval-augmented generation system that provides adaptive assistance in information-intensive circumstances, integrating video, audio, and text to reduce cognitive load. Evaluated on MMBench-Video, it outperforms existing models while using fewer resources.


<details>
  <summary>Details</summary>
Motivation: As robots and intelligent agents become more integrated into human life, there is a growing need to offload the cognitive burden on humans to these systems, especially in dynamic, information-rich scenarios.

Method: The Multi-RAG system integrates and reasons over multi-source information streams including video, audio, and text to improve situational understanding and reduce cognitive load.

Result: Multi-RAG achieves superior performance compared to existing open-source Video-LLMs and LVLMs while utilizing fewer resources and less input data when benchmarked on the MMBench-Video dataset.

Conclusion: The results demonstrate Multi-RAG's potential as a practical and efficient foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts.

Abstract: To effectively engage in human society, the ability to adapt, filter
information, and make informed decisions in ever-changing situations is
critical. As robots and intelligent agents become more integrated into human
life, there is a growing opportunity-and need-to offload the cognitive burden
on humans to these systems, particularly in dynamic, information-rich
scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal
retrieval-augmented generation system designed to provide adaptive assistance
to humans in information-intensive circumstances. Our system aims to improve
situational understanding and reduce cognitive load by integrating and
reasoning over multi-source information streams, including video, audio, and
text. As an enabling step toward long-term human-robot partnerships, Multi-RAG
explores how multimodal information understanding can serve as a foundation for
adaptive robotic assistance in dynamic, human-centered situations. To evaluate
its capability in a realistic human-assistance proxy task, we benchmarked
Multi-RAG on the MMBench-Video dataset, a challenging multimodal video
understanding benchmark. Our system achieves superior performance compared to
existing open-source video large language models (Video-LLMs) and large
vision-language models (LVLMs), while utilizing fewer resources and less input
data. The results demonstrate Multi- RAG's potential as a practical and
efficient foundation for future human-robot adaptive assistance systems in
dynamic, real-world contexts.

</details>


### [7] [GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs](https://arxiv.org/abs/2505.24036)
*Amel Gader,Alsayed Algergawy*

Main category: cs.AI

TL;DR: The paper introduces GenIC, a two-step generative framework leveraging large language models for knowledge graph instance completion. It consists of property prediction and link prediction steps, outperforming existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Knowledge graph completion is crucial for enhancing knowledge bases, yet predicting missing triples remains challenging. Modern knowledge bases with entity descriptions and types offer potential contextual clues for inferring missing facts.

Method: Proposes GenIC, a two-step framework: 1) Property Prediction as multi-label classification to predict possible relations for an entity; 2) Link Prediction as a generative sequence-to-sequence task to predict the tail entity given head and relation.

Result: GenIC shows superior performance compared to existing baselines across three datasets.

Conclusion: GenIC effectively leverages textual descriptions and schema patterns through large language models for improved knowledge graph instance completion.

Abstract: Knowledge graph completion aims to address the gaps of knowledge bases by
adding new triples that represent facts. The complexity of this task depends on
how many parts of a triple are already known. Instance completion involves
predicting the relation-tail pair when only the head is given (h, ?, ?).
Notably, modern knowledge bases often contain entity descriptions and types,
which can provide valuable context for inferring missing facts. By leveraging
these textual descriptions and the ability of large language models to extract
facts from them and recognize patterns within the knowledge graph schema, we
propose an LLM-powered, end-to-end instance completion approach. Specifically,
we introduce GenIC: a two-step Generative Instance Completion framework. The
first step focuses on property prediction, treated as a multi-label
classification task. The second step is link prediction, framed as a generative
sequence-to-sequence task. Experimental results on three datasets show that our
method outperforms existing baselines. Our code is available at
https://github.com/amal-gader/genic.

</details>


### [8] [Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution](https://arxiv.org/abs/2505.24037)
*Qiao Xiao,Alan Ansell,Boqian Wu,Lu Yin,Mykola Pechenizkiy,Shiwei Liu,Decebal Constantin Mocanu*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）因计算需求巨大而面临部署挑战。尽管后训练修剪方法如SparseGPT和Wanda可以有效减少模型大小，但在高稀疏度水平下难以维持模型性能。现有的微调方法，如全微调和LoRA，无法保留稀疏性，因为它们需要更新整个密集矩阵，不适合稀疏LLMs。本文提出了一种名为Sparsity Evolution Fine-Tuning (SEFT)的新方法，专为稀疏LLMs设计。SEFT在微调过程中动态演化修剪模型的稀疏拓扑结构，同时在整个过程中保持整体稀疏性。SEFT通过权重降-增策略进行特定任务的适应，使修剪后的模型能够根据目标数据集自适应其稀疏连接模式。此外，采用敏感性驱动的修剪标准，以确保在整个微调过程中始终保持所需的稀疏度水平。我们在包括LLaMA系列、DeepSeek和Mistral在内的各种LLMs上进行实验，跨越多样化的基准测试，结果表明SEFT在提供比现有基线更优的内存和时间效率的同时实现了更强的性能。我们的代码已公开发布：https://github.com/QiaoXiao7282/SEFT。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型虽然功能强大，但因其巨大的计算需求而在部署方面遇到困难。现有的修剪和微调方法要么无法在高稀疏度下保持性能，要么无法保留稀疏性，因此限制了它们在下游任务中的实用性。

Method: 提出了Sparsity Evolution Fine-Tuning (SEFT)方法，该方法在微调过程中动态演化修剪模型的稀疏拓扑结构，同时保持整体稀疏性。它使用一种权重降-增策略来实现特定任务的适应，并采用敏感性驱动的修剪标准以确保在整个微调过程中维持所需的稀疏度水平。

Result: 实验结果表明，SEFT在多种大型语言模型和多样化基准测试中表现出更强的性能，同时提供了比现有基线更优的内存和时间效率。

Conclusion: SEFT是一种专为稀疏LLMs设计的有效微调方法，能够在保持稀疏性的同时提高模型性能，并且在资源利用方面表现优异。

Abstract: Large language models (LLMs) have achieved remarkable success across various
tasks but face deployment challenges due to their massive computational
demands. While post-training pruning methods like SparseGPT and Wanda can
effectively reduce the model size, but struggle to maintain model performance
at high sparsity levels, limiting their utility for downstream tasks. Existing
fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve
sparsity as they require updating the whole dense metrics, not well-suited for
sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a
novel method designed specifically for sparse LLMs. SEFT dynamically evolves
the sparse topology of pruned models during fine-tuning, while preserving the
overall sparsity throughout the process. The strengths of SEFT lie in its
ability to perform task-specific adaptation through a weight drop-and-grow
strategy, enabling the pruned model to self-adapt its sparse connectivity
pattern based on the target dataset. Furthermore, a sensitivity-driven pruning
criterion is employed to ensure that the desired sparsity level is consistently
maintained throughout fine-tuning. Our experiments on various LLMs, including
LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks
demonstrate that SEFT achieves stronger performance while offering superior
memory and time efficiency compared to existing baselines. Our code is publicly
available at: https://github.com/QiaoXiao7282/SEFT.

</details>


### [9] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu,Yueqi Wang,Shuo Xing,Chia-Ju Chen,Zhengzhong Tu*

Main category: cs.AI

TL;DR: Large Vision-Language Models (LVLMs) have made great progress in multimodal tasks, but they still have limitations. Retrieval-Augmented Generation (RAG) provides a solution to these problems by enabling LVLMs to access large-scale knowledge databases through retrieval mechanisms. In this paper, we dissect the multimodal RAG pipeline for LVLMs and explore a unified agentic framework that integrates re-ranking and generation through self-reflection. Our exploration of RAG for LVLMs yields substantial insights.


<details>
  <summary>Details</summary>
Motivation: LVLMs are limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence. RAG offers a practical solution to mitigate these challenges.

Method: Systematic dissection of the multimodal RAG pipeline for LVLMs, including the retrieval phase, the re-ranking stage, and the generation phase. Exploration of a unified agentic framework that integrates re-ranking and generation through self-reflection.

Result: An average performance boost of 5% without any fine-tuning.

Conclusion: The full-stack exploration of RAG for LVLMs yields substantial insights.

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [10] [SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought](https://arxiv.org/abs/2505.24181)
*Guanghao Li,Wenhao Jiang,Mingfeng Chen,Yan Li,Hao Yu,Shuting Dong,Tao Ren,Ming Tang,Chun Yuan*

Main category: cs.AI

TL;DR: SCOUT is a framework that improves reasoning in large language models by modeling recursive inference as progressive cognitive stages without requiring pretraining, showing gains in accuracy and explanation quality.


<details>
  <summary>Details</summary>
Motivation: Current Chain of Thought methods for improving reasoning in large language models are limited by their reliance on intermediate steps and lack a principled framework for evolving reasoning across iterations.

Method: Introduced Flow Chain of Thought (Flow CoT) which models recursive inference as a trajectory of latent cognitive states. Proposed SCOUT, a fine-tuning framework that uses progressive distillation and a cross attention based retrospective module to enable Flow CoT style reasoning without pretraining.

Result: Experiments on eight reasoning benchmarks showed consistent improvements in accuracy and explanation quality with up to 1.8% gains under fine tuning. Qualitative analyses revealed deeper reasoning across iterations.

Conclusion: SCOUT effectively enhances reasoning in LLMs and demonstrates the practical viability of Flow CoT as a scalable framework.

Abstract: Chain of Thought (CoT) prompting improves the reasoning performance of large
language models (LLMs) by encouraging step by step thinking. However, CoT-based
methods depend on intermediate reasoning steps, which limits scalability and
generalization. Recent work explores recursive reasoning, where LLMs reuse
internal layers across iterations to refine latent representations without
explicit CoT supervision. While promising, these approaches often require
costly pretraining and lack a principled framework for how reasoning should
evolve across iterations. We address this gap by introducing Flow Chain of
Thought (Flow CoT), a reasoning paradigm that models recursive inference as a
progressive trajectory of latent cognitive states. Flow CoT frames each
iteration as a distinct cognitive stage deepening reasoning across iterations
without relying on manual supervision. To realize this, we propose SCOUT
(Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning
framework that enables Flow CoT style reasoning without the need for
pretraining. SCOUT uses progressive distillation to align each iteration with a
teacher of appropriate capacity, and a cross attention based retrospective
module that integrates outputs from previous iterations while preserving the
models original computation flow. Experiments across eight reasoning benchmarks
show that SCOUT consistently improves both accuracy and explanation quality,
achieving up to 1.8% gains under fine tuning. Qualitative analyses further
reveal that SCOUT enables progressively deeper reasoning across iterations
refining both belief formation and explanation granularity. These results not
only validate the effectiveness of SCOUT, but also demonstrate the practical
viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.

</details>


### [11] [Learning API Functionality from Demonstrations for Tool-based Agents](https://arxiv.org/abs/2505.24197)
*Bhrij Patel,Ashish Jagmohan,Aditya Vempaty*

Main category: cs.AI

TL;DR: The paper proposes learning API functionality directly from demonstrations as a new paradigm, studies factors affecting task success rate, and analyzes failure modes.


<details>
  <summary>Details</summary>
Motivation: Digital tool-based agents often rely on documentation to understand API functionality, but such documentation is frequently missing, outdated, privatized, or inconsistent.

Method: Propose learning API functionality directly from demonstrations; collect demonstrations from expert API-based agents and self-exploration; study how number of demonstrations, LLM-generated summaries, and evaluations affect task success rate.

Result: Experiments across 3 datasets and 5 models show that learning functionality from demonstrations remains challenging even for state-of-the-art LLMs. Providing explicit function calls and natural language critiques significantly improves task success rate.

Conclusion: Learning API functionality from demonstrations is a non-trivial challenge. The paper analyzes failure modes, identifies sources of error, and highlights key open challenges for future work in documentation-free, self-improving, API-based agents.

Abstract: Digital tool-based agents that invoke external Application Programming
Interfaces (APIs) often rely on documentation to understand API functionality.
However, such documentation is frequently missing, outdated, privatized, or
inconsistent-hindering the development of reliable, general-purpose agents. In
this work, we propose learning API functionality directly from demonstrations
as a new paradigm applicable in scenarios without documentation. Using existing
API benchmarks, we collect demonstrations from both expert API-based agents and
from self-exploration. To understand what information demonstrations must
convey for successful task completion, we extensively study how the number of
demonstrations and the use of LLM-generated summaries and evaluations affect
the task success rate of the API-based agent. Our experiments across 3 datasets
and 5 models show that learning functionality from demonstrations remains a
non-trivial challenge, even for state-of-the-art LLMs. We find that providing
explicit function calls and natural language critiques significantly improves
the agent's task success rate due to more accurate parameter filling. We
analyze failure modes, identify sources of error, and highlight key open
challenges for future work in documentation-free, self-improving, API-based
agents.

</details>


### [12] [SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems](https://arxiv.org/abs/2505.24201)
*Xu He,Di Wu,Yan Zhai,Kun Sun*

Main category: cs.AI

TL;DR: This paper addresses the security and reliability challenges in LLM-based multi-agent systems (MAS) by presenting a system-level anomaly detection framework. The framework integrates structural modeling with runtime behavioral oversight through two components: a graph-based framework for semantic anomaly detection and a SentinelAgent for observing, analyzing, and intervening based on security policies.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the security and reliability challenges in LLM-based multi-agent systems, which face risks such as prompt manipulation, unsafe tool usage, and emergent agent miscoordination. Existing guardrail mechanisms offer only partial protection and do not fully address systemic or multi-point failures in MAS.

Method: The method consists of two components: 1) A graph-based framework that models agent interactions as dynamic execution graphs, enabling semantic anomaly detection at node, edge, and path levels; 2) A pluggable SentinelAgent, an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS execution based on security policies and contextual reasoning.

Result: The framework was validated through two case studies, including an email assistant and Microsoft's Magentic-One system. It demonstrated the ability to detect covert risks and provide explainable root-cause attribution.

Conclusion: This work lays the foundation for more trustworthy, monitorable, and secure agent-based AI ecosystems by providing a system-level anomaly detection framework tailored for multi-agent systems.

Abstract: The rise of large language model (LLM)-based multi-agent systems (MAS)
introduces new security and reliability challenges. While these systems show
great promise in decomposing and coordinating complex tasks, they also face
multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent
agent miscoordination. Existing guardrail mechanisms offer only partial
protection, primarily at the input-output level, and fall short in addressing
systemic or multi-point failures in MAS. In this work, we present a
system-level anomaly detection framework tailored for MAS, integrating
structural modeling with runtime behavioral oversight. Our approach consists of
two components. First, we propose a graph-based framework that models agent
interactions as dynamic execution graphs, enabling semantic anomaly detection
at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,
an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS
execution based on security policies and contextual reasoning. By bridging
abstract detection logic with actionable enforcement, our method detects not
only single-point faults and prompt injections but also multi-agent collusion
and latent exploit paths. We validate our framework through two case studies,
including an email assistant and Microsoft's Magentic-One system, demonstrating
its ability to detect covert risks and provide explainable root-cause
attribution. Our work lays the foundation for more trustworthy, monitorable,
and secure agent-based AI ecosystems.

</details>


### [13] [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/abs/2505.24208)
*Wenhan Yang,Spencer Stice,Ali Payani,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: To ensure the safe deployment of Vision-Language Models (VLMs), this paper addresses the issue of safety degradation in LVLMs compared to their LLM backbone. The authors find that the modality gap between image and text representations is highly inversely correlated with VLMs' safety. They propose a regularization method during pretraining to reduce this gap, which significantly improves safety alignment without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address the safety degradation problem in LVLMs compared to their LLM backbone, particularly focusing on how the modality gap between image and text representations affects the safety of LVLMs.

Method: The authors first demonstrate the inverse correlation between the modality gap and VLMs' safety. Then, they identify that the modality gap is introduced during the pretraining phase and persists through fine-tuning. Based on these findings, they propose a regularization technique applied during pretraining to reduce the modality gap.

Result: Extensive experiments conducted on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show that the proposed method reduces the unsafe rate by up to 16.3% without affecting performance, and can enhance existing defense mechanisms by up to 18.2%.

Conclusion: This study concludes that reducing the modality gap during pretraining significantly improves the safety alignment of LVLMs, offering a promising direction for enhancing the reliability of these models.

Abstract: Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for
their reliable deployment. However, LVLMs suffer from drastic safety
degradation compared to their LLM backbone. Even blank or irrelevant images can
trigger LVLMs to generate harmful responses to prompts that would otherwise be
refused in text-only contexts. The modality gap between image and text
representations has been recently hypothesized to contribute to safety
degradation of LVLMs. However, if and how the amount of modality gap affects
LVLMs' safety is not studied. In this work, we show that the amount of modality
gap is highly inversely correlated with VLMs' safety. Then, we show that this
modality gap is introduced during pretraining LVLMs and persists through
fine-tuning. Inspired by this observation, we propose a regularization to
reduce the modality gap during pretraining. Our extensive experiments on LLaVA
v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves
safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without
compromising performance, and can further boost existing defenses by up to
18.2%.

</details>


### [14] [E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness](https://arxiv.org/abs/2505.24226)
*Yibo Zhao,Jiapeng Zhu,Ye Guo,Kangkang He,Xiang Li*

Main category: cs.AI

TL;DR: The paper introduces E^2GraphRAG, an improved graph-based RAG framework that enhances efficiency and effectiveness through a summary tree and entity graph with bidirectional indexes. It features adaptive retrieval and shows significant speed improvements over existing methods while maintaining QA performance.


<details>
  <summary>Details</summary>
Motivation: Graph-based RAG methods provide global understanding of knowledge bases but are inefficient and rely on pre-defined query modes, restricting practical application.

Method: E^2GraphRAG constructs a summary tree using large language models and an entity graph with SpaCy during indexing. Bidirectional indexes capture relationships between entities and chunks for fast lookup. An adaptive retrieval strategy is used to select between local and global retrieval modes.

Result: E^2GraphRAG achieves up to 10 times faster indexing than GraphRAG and 100 times speedup in retrieval compared to LightRAG, while maintaining competitive QA performance.

Conclusion: E^2GraphRAG offers a more efficient and effective streamlined graph-based RAG framework.

Abstract: Graph-based RAG methods like GraphRAG have shown promising global
understanding of the knowledge base by constructing hierarchical entity graphs.
However, they often suffer from inefficiency and rely on manually pre-defined
query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a
streamlined graph-based RAG framework that improves both Efficiency and
Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree
with large language models and an entity graph with SpaCy based on document
chunks. We then construct bidirectional indexes between entities and chunks to
capture their many-to-many relationships, enabling fast lookup during both
local and global retrieval. For the retrieval stage, we design an adaptive
retrieval strategy that leverages the graph structure to retrieve and select
between local and global modes. Experiments show that E^2GraphRAG achieves up
to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG
in retrieval while maintaining competitive QA performance.

</details>


### [15] [ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction](https://arxiv.org/abs/2505.24230)
*Murari Ambati*

Main category: cs.AI

TL;DR: ProofNet++ is a neuro-symbolic framework that enhances automated theorem proving by combining LLMs with formal proof verification and self-correction mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based systems suffer from hallucinated logical steps and unverifiable reasoning.

Method: Integrating symbolic proof tree supervision, reinforcement learning loop using verifiers as reward functions, and iterative self-correction module.

Result: Experiments on miniF2F, Lean's mathlib, and HOL Light show significant improvements in proof accuracy, correctness, and formal verifiability over prior models.

Conclusion: Theoretical analysis of the convergence and stability of the verifier-guided RL framework was provided and datasets and codebase were released for future research.

Abstract: We propose ProofNet++, a neuro-symbolic framework that enhances automated
theorem proving by combining large language models (LLMs) with formal proof
verification and self-correction mechanisms. Current LLM-based systems suffer
from hallucinated logical steps and unverifiable reasoning. ProofNet++
mitigates these limitations by integrating symbolic proof tree supervision, a
reinforcement learning loop using verifiers as reward functions, and an
iterative self-correction module. Our experiments on miniF2F, Lean's mathlib,
and HOL Light show that ProofNet++ significantly improves proof accuracy,
correctness, and formal verifiability over prior models. We provide theoretical
analysis of the convergence and stability of the verifier-guided RL framework
and release our datasets and codebase for future research.

</details>


### [16] [FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation](https://arxiv.org/abs/2505.24258)
*Vishal Pallagani,Nitin Gupta,John Aydin,Biplav Srivastava*

Main category: cs.AI

TL;DR: FABLE is a benchmark that evaluates large language models' ability to understand data flow in procedural text. It uses eight classical data-flow analyses across three domains and tests three types of LLMs. The reasoning-focused model performs best but is much slower, while other models perform poorly.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the ability of large language models (LLMs) to perform data-flow reasoning, which is fundamental to procedural tasks but has not been thoroughly assessed before.

Method: Introduced FABLE, an extensible benchmark with 2,400 question-answer pairs assessing LLMs' understanding of data flow using structured, procedural text across three real-world domains and eight classical data-flow analyses. Evaluated three types of LLMs using majority voting over five sampled completions per prompt.

Result: The reasoning-focused model achieved higher accuracy at the cost of significantly slower inference, while general-purpose and code-specific models performed close to random chance.

Conclusion: FABLE provides the first diagnostic benchmark for evaluating data-flow reasoning in LLMs and offers insights into developing models with stronger procedural understanding.

Abstract: Understanding how data moves, transforms, and persists, known as data flow,
is fundamental to reasoning in procedural tasks. Despite their fluency in
natural and programming languages, large language models (LLMs), although
increasingly being applied to decisions with procedural tasks, have not been
systematically evaluated for their ability to perform data-flow reasoning. We
introduce FABLE, an extensible benchmark designed to assess LLMs' understanding
of data flow using structured, procedural text. FABLE adapts eight classical
data-flow analyses from software engineering: reaching definitions, very busy
expressions, available expressions, live variable analysis, interval analysis,
type-state analysis, taint analysis, and concurrency analysis. These analyses
are instantiated across three real-world domains: cooking recipes, travel
routes, and automated plans. The benchmark includes 2,400 question-answer
pairs, with 100 examples for each domain-analysis combination. We evaluate
three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a
general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code
8B). Each model is tested using majority voting over five sampled completions
per prompt. Results show that the reasoning model achieves higher accuracy, but
at the cost of over 20 times slower inference compared to the other models. In
contrast, the general-purpose and code-specific models perform close to random
chance. FABLE provides the first diagnostic benchmark to systematically
evaluate data-flow reasoning and offers insights for developing models with
stronger procedural understanding.

</details>


### [17] [Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models](https://arxiv.org/abs/2505.24260)
*Mingyi He,Yuebing Liang,Shenhao Wang,Yunhan Zheng,Qingyi Wang,Dingyi Zhuang,Li Tian,Jinhua Zhao*

Main category: cs.AI

TL;DR: Urban design aided by generative AI (GenAI) is explored in this study. A stepwise generative urban design framework integrating multimodal diffusion models with human expertise for more adaptive and controllable processes is proposed.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the paper lies in the limitations of current GenAI approaches that lack integration with human design workflows, have limited control, and do not account for the iterative nature of real-world design.

Method: A stepwise generative urban design framework is proposed which integrates multimodal diffusion models with human expertise. The process is divided into three key stages aligned with established urban design workflows: road network and land use planning, building layout planning, and detailed planning and rendering. At each stage, preliminary designs are generated based on textual prompts and image-based constraints, then reviewed and refined by human designers.

Result: Experiments using data from Chicago and New York City show that the proposed framework outperforms baseline models and end-to-end approaches in terms of fidelity, compliance, and diversity of the generated designs.

Conclusion: This study highlights the benefits of multimodal diffusion models and stepwise generation in maintaining human control and allowing iterative refinements, providing a foundation for human-AI interaction in urban design.

Abstract: Urban design is a multifaceted process that demands careful consideration of
site-specific constraints and collaboration among diverse professionals and
stakeholders. The advent of generative artificial intelligence (GenAI) offers
transformative potential by improving the efficiency of design generation and
facilitating the communication of design ideas. However, most existing
approaches are not well integrated with human design workflows. They often
follow end-to-end pipelines with limited control, overlooking the iterative
nature of real-world design. This study proposes a stepwise generative urban
design framework that integrates multimodal diffusion models with human
expertise to enable more adaptive and controllable design processes. Instead of
generating design outcomes in a single end-to-end process, the framework
divides the process into three key stages aligned with established urban design
workflows: (1) road network and land use planning, (2) building layout
planning, and (3) detailed planning and rendering. At each stage, multimodal
diffusion models generate preliminary designs based on textual prompts and
image-based constraints, which can then be reviewed and refined by human
designers. We design an evaluation framework to assess the fidelity,
compliance, and diversity of the generated designs. Experiments using data from
Chicago and New York City demonstrate that our framework outperforms baseline
models and end-to-end approaches across all three dimensions. This study
underscores the benefits of multimodal diffusion models and stepwise generation
in preserving human control and facilitating iterative refinements, laying the
groundwork for human-AI interaction in urban design solutions.

</details>


### [18] [How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning](https://arxiv.org/abs/2505.24273)
*Hongyi James Cai,Junlin Wang,Xiaoyin Chen,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: Recent breakthroughs in LLMs have improved reasoning abilities, especially in mathematical and logical problems through SFT and RL techniques. This paper investigates the dynamics between SFT and RL on various reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the precise benefits of backtracking in reasoning improvements and the optimal extent of its use in LLMs.

Method: Systematically investigate the dynamics between SFT and RL on eight reasoning tasks; construct synthetic datasets varying in backtracking steps for controlled experiments.

Result: Short CoT sequences used in SFT moderately contribute to RL training, but this contribution diminishes with task difficulty. Longer CoT with backtracks generally induce better and more stable RL training. More challenging problems need higher numbers of backtracks during SFT. RL training is largely unaffected by the correctness of long CoT sequences, prioritizing structural patterns over content correctness.

Conclusion: The results offer practical insights into designing optimal training strategies for effectively scaling reasoning in LLMs.

Abstract: Recent breakthroughs in large language models (LLMs) have effectively
improved their reasoning abilities, particularly on mathematical and logical
problems that have verifiable answers, through techniques such as supervised
finetuning (SFT) and reinforcement learning (RL). Prior research indicates that
RL effectively internalizes search strategies, enabling long chain-of-thought
(CoT) reasoning, with backtracking emerging naturally as a learned capability.
However, the precise benefits of backtracking, specifically, how significantly
it contributes to reasoning improvements and the optimal extent of its use,
remain poorly understood. In this work, we systematically investigate the
dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc
1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self
Reference. Our findings highlight that short CoT sequences used in SFT as a
warm-up do have moderate contribution to RL training, compared with cold-start
RL; however such contribution diminishes when tasks become increasingly
difficult. Motivated by this observation, we construct synthetic datasets
varying systematically in the number of backtracking steps and conduct
controlled experiments to isolate the influence of either the correctness
(content) or the structure (i.e., backtrack frequency). We find that (1) longer
CoT with backtracks generally induce better and more stable RL training, (2)
more challenging problems with larger search space tend to need higher numbers
of backtracks during the SFT stage. Additionally, we demonstrate through
experiments on distilled data that RL training is largely unaffected by the
correctness of long CoT sequences, suggesting that RL prioritizes structural
patterns over content correctness. Collectively, our results offer practical
insights into designing optimal training strategies to effectively scale
reasoning in LLMs.

</details>


### [19] [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/abs/2505.24292)
*Yueqi Zhang,Peiwen Yuan,Shaoxiong Feng,Yiwei Li,Xinglin Wang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为QuAda的方法，用于解决当前大型语言模型在人类-AI对话中缺乏有效利用引用文本的问题。通过引入一种新的数据管道和训练方法，QuAda能够在不改变提示的情况下动态调整对引用文本的关注度，并且在多个模型和场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在对话过程中无法有效地定位和利用之前提到的文本片段（如公式、引文等），这限制了它们在人类-AI对话中的表现。

Method: 作者提出了一个称为QuAda的方法，该方法通过在每个注意力头附加两个瓶颈投影，动态地增强或抑制对引用片段的关注，同时不影响原始提示并只更新少量主干权重。此外，还构建了一个以引用为中心的数据管道，用于合成特定任务的对话并验证答案的正确性。

Result: 实验结果表明，QuAda方法适用于所有代表性的对话场景，并能够推广到未见过的主题，提供了一种有效的即插即用解决方案。

Conclusion: QuAda为引用感知对话提供了一种高效、轻量级的解决方案，能够在不增加额外开销的情况下提高大型语言模型对引用文本的利用能力。

Abstract: Human-AI conversation frequently relies on quoting earlier text-"check it
with the formula I just highlighted"-yet today's large language models (LLMs)
lack an explicit mechanism for locating and exploiting such spans. We formalise
the challenge as span-conditioned generation, decomposing each turn into the
dialogue history, a set of token-offset quotation spans, and an intent
utterance. Building on this abstraction, we introduce a quotation-centric data
pipeline that automatically synthesises task-specific dialogues, verifies
answer correctness through multi-stage consistency checks, and yields both a
heterogeneous training corpus and the first benchmark covering five
representative scenarios. To meet the benchmark's zero-overhead and
parameter-efficiency requirements, we propose QuAda, a lightweight
training-based method that attaches two bottleneck projections to every
attention head, dynamically amplifying or suppressing attention to quoted spans
at inference time while leaving the prompt unchanged and updating < 2.8% of
backbone weights. Experiments across models show that QuAda is suitable for all
scenarios and generalises to unseen topics, offering an effective,
plug-and-play solution for quotation-aware dialogue.

</details>


### [20] [GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments](https://arxiv.org/abs/2505.24306)
*Kechen Li,Yaotian Tao,Ximing Wen,Quanwei Sun,Zifei Gong,Chang Xu,Xizhe Zhang,Tianbo Ji*

Main category: cs.AI

TL;DR: Recent advancements in LLMs show potential in planning and reasoning tasks. However, the synergy between LLMs and traditional algorithms is often overlooked. This paper proposes GridRoute, a benchmark for evaluating LLMs' ability to leverage traditional algorithms, and AoT, a hybrid prompting technique that integrates traditional algorithm guidance. Results indicate AoT significantly improves LLM performance, especially in complex environments.


<details>
  <summary>Details</summary>
Motivation: To explore the potential synergy between LLMs and traditional algorithms in path planning and reasoning tasks, addressing the gap in existing studies that primarily focus on LLMs' independent capabilities.

Method: Propose GridRoute, a comprehensive evaluation benchmark, and Algorithm of Thought (AoT), a novel hybrid prompting technique that incorporates traditional algorithm guidance into LLM prompts. Evaluate six LLMs with parameters ranging from 7B to 72B across various map sizes.

Result: AoT significantly enhances LLM performance across all model sizes, particularly noticeable in larger or more complex grid environments.

Conclusion: The proposed AoT method demonstrates promise in improving path planning challenges by integrating traditional algorithms with LLMs.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated their
potential in planning and reasoning tasks, offering a flexible alternative to
classical pathfinding algorithms. However, most existing studies focus on LLMs'
independent reasoning capabilities and overlook the potential synergy between
LLMs and traditional algorithms. To fill this gap, we propose a comprehensive
evaluation benchmark GridRoute to assess how LLMs can take advantage of
traditional algorithms. We also propose a novel hybrid prompting technique
called Algorithm of Thought (AoT), which introduces traditional algorithms'
guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to
72B parameters across various map sizes, assessing their performance in
correctness, optimality, and efficiency in grid environments with varying
sizes. Our results show that AoT significantly boosts performance across all
model sizes, particularly in larger or more complex environments, suggesting a
promising approach to addressing path planning challenges. Our code is
open-sourced at https://github.com/LinChance/GridRoute.

</details>


### [21] [Three Kinds of Negation in Knowledge and Their Mathematical Foundations](https://arxiv.org/abs/2505.24422)
*Zhenghua Pan,Yong Wang*

Main category: cs.AI

TL;DR: The paper explores negation in AI knowledge processing by examining its understanding and characteristics across philosophy, logic, and linguistics. It proposes three types of negation (contradictory, opposite, intermediary) and establishes a mathematical foundation through SCOI and LCOI.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental issue of negation in AI knowledge processing and research by understanding its nature across multiple disciplines.

Method: Analysis of negation concepts from philosophy, logic, and linguistics leading to the proposition of three types of negation. Introduction of SCOI and LCOI to establish a mathematical foundation for these negations.

Result: Three types of negation identified; SCOI and LCOI introduced with main operational properties proved.

Conclusion: Negation in knowledge can be categorized into three types with a mathematical framework established to reflect their intrinsic connections, properties, and laws.

Abstract: In the field of artificial intelligence, understanding, distinguishing,
expressing, and computing the negation in knowledge is a fundamental issue in
knowledge processing and research. In this paper, we examine and analyze the
understanding and characteristics of negation in various fields such as
philosophy, logic, and linguistics etc. Based on the distinction between the
concepts of contradiction and opposition, we propose that there are three
different types of negation in knowledge from a conceptual perspective:
contradictory negation, opposite negation, and intermediary negation. To
establish a mathematical foundation that fully reflects the intrinsic
connections, properties, and laws of these different forms of negation, we
introduce SCOI: sets with contradictory negation, opposite negation and
intermediary negation, and LCOI: logic with contradictory negation, opposite
negation and intermediary negation, and we proved the main operational
properties of SCOI as well as the formal inference relations in LCOI.

</details>


### [22] [P: A Universal Measure of Predictive Intelligence](https://arxiv.org/abs/2505.24426)
*David Gamez*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于预测能力的智能通用度量方法，并通过实验验证了其可行性，可为人类、动物和AI提供单一比例尺上的智能排名。


<details>
  <summary>Details</summary>
Motivation: 当前对智能的理解和测量远远落后于构建模仿人类智能行为系统的实际能力，缺乏统一定义和实用测量方法。

Method: 基于预测是智能最重要组成部分的假设，提出一种新的通用智能度量方法：通过与环境交互时预测准确性总和，结合柯尔莫哥洛夫复杂性衡量预测和感知环境复杂度。

Result: 两个实验表明该算法可行，能够度量虚拟迷宫中代理和时间序列数据预测代理的智能。

Conclusion: 这种新通用智能度量可能是新比较智能科学的起点，可在单一比例尺上对人类、动物和AI进行排名。

Abstract: Over the last thirty years, considerable progress has been made with the
development of systems that can drive cars, play games, predict protein folding
and generate natural language. These systems are described as intelligent and
there has been a great deal of talk about the rapid increase in artificial
intelligence and its potential dangers. However, our theoretical understanding
of intelligence and ability to measure it lag far behind our capacity for
building systems that mimic intelligent human behaviour. There is no commonly
agreed definition of the intelligence that AI systems are said to possess.
No-one has developed a practical measure that would enable us to compare the
intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on
the hypothesis that prediction is the most important component of intelligence.
As an agent interacts with its normal environment, the accuracy of its
predictions is summed up and the complexity of its predictions and perceived
environment is accounted for using Kolmogorov complexity. Two experiments were
carried out to evaluate the practical feasibility of the algorithm. These
demonstrated that it could measure the intelligence of an agent embodied in a
virtual maze and an agent that makes predictions about time-series data. This
universal measure could be the starting point for a new comparative science of
intelligence that ranks humans, animals and AIs on a single ratio scale.

</details>


### [23] [RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation](https://arxiv.org/abs/2505.24442)
*Zhentao Xie,Chengcheng Han,Jinxin Shi,Wenjun Cui,Xin Zhao,Xingjiao Wu,Jiabao Zhao*

Main category: cs.AI

TL;DR: A new multi-agent system called Residual Mixture-of-Agents (RMoA) is proposed, which uses residual connections, a diversity selection mechanism, residual agents and an adaptive termination mechanism to improve efficiency, reliability and performance while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems based on large language models have limitations such as high computational overhead, information loss and lack of robustness.

Method: The RMoA integrates residual connections inspired by ResNet's residual learning. It includes an embedding-based diversity selection mechanism for selecting responses via vector similarity, a Residual Extraction Agent for preserving cross-layer incremental information and a Residual Aggregation Agent for hierarchical information integration. There is also an adaptive termination mechanism that halts processing based on residual convergence.

Result: RMoA achieves state-of-the-art performance on benchmarks across alignment, mathematical reasoning, code generation and multitasking understanding, with significantly reduced computational overhead.

Conclusion: The proposed RMoA system improves efficiency, reliability and performance in multi-agent systems while reducing computational costs.

Abstract: Although multi-agent systems based on large language models show strong
capabilities on multiple tasks, they are still limited by high computational
overhead, information loss, and robustness. Inspired by ResNet's residual
learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual
connections to optimize efficiency and reliability. To maximize information
utilization from model responses while minimizing computational costs, we
innovatively design an embedding-based diversity selection mechanism that
greedily selects responses via vector similarity. Furthermore, to mitigate
iterative information degradation, we introduce a Residual Extraction Agent to
preserve cross-layer incremental information by capturing inter-layer response
differences, coupled with a Residual Aggregation Agent for hierarchical
information integration. Additionally, we propose an adaptive termination
mechanism that dynamically halts processing based on residual convergence,
further improving inference efficiency. RMoA achieves state-of-the-art
performance on the benchmarks of across alignment, mathematical reasoning, code
generation, and multitasking understanding, while significantly reducing
computational overhead. Code is available at
https://github.com/mindhunter01/RMoA.

</details>


### [24] [SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors](https://arxiv.org/abs/2505.24458)
*Tianlong Yu,Chenghang Ye,Zheyu Yang,Ziyi Zhou,Cui Tang,Zui Tao,Jun Zhang,Kailong Wang,Liting Zhou,Yang Yang,Ting Bi*

Main category: cs.AI

TL;DR: The SEAR Dataset is a multimodal resource for studying social engineering attacks in AR and LLMs environments, revealing high efficacy in eliciting compliance and hijacking trust.


<details>
  <summary>Details</summary>
Motivation: To address the emerging threat of social engineering attacks orchestrated through augmented reality and multimodal large language models.

Method: Capturing 180 annotated conversations across 60 participants in simulated adversarial scenarios, including meetings, classes, and networking events. The dataset includes AR-captured visual/audio cues, environmental context, curated social media profiles, and subjective metrics such as trust ratings and susceptibility assessments.

Result: Key findings reveal alarming efficacy in eliciting compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and hijacking trust (76.7% post-interaction trust surge).

Conclusion: The dataset supports research in detecting AR-driven SE attacks, designing defensive frameworks, and understanding multimodal adversarial manipulation, with rigorous ethical safeguards ensuring responsible use.

Abstract: The SEAR Dataset is a novel multimodal resource designed to study the
emerging threat of social engineering (SE) attacks orchestrated through
augmented reality (AR) and multimodal large language models (LLMs). This
dataset captures 180 annotated conversations across 60 participants in
simulated adversarial scenarios, including meetings, classes and networking
events. It comprises synchronized AR-captured visual/audio cues (e.g., facial
expressions, vocal tones), environmental context, and curated social media
profiles, alongside subjective metrics such as trust ratings and susceptibility
assessments. Key findings reveal SEAR's alarming efficacy in eliciting
compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and
hijacking trust (76.7% post-interaction trust surge). The dataset supports
research in detecting AR-driven SE attacks, designing defensive frameworks, and
understanding multimodal adversarial manipulation. Rigorous ethical safeguards,
including anonymization and IRB compliance, ensure responsible use. The SEAR
dataset is available at https://github.com/INSLabCN/SEAR-Dataset.

</details>


### [25] [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)
*Vasilije Markovic,Lazar Obradovic,Laszlo Hajdu,Jovan Pavlovic*

Main category: cs.AI

TL;DR: Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) leads to complex systems with numerous hyperparameters that directly affect performance. This paper explores the role of systematic hyperparameter optimization in such systems, using Cognee framework and three multi-hop QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: While integrating LLMs with KGs is common in retrieval-augmented generation, the role of systematic hyperparameter optimization remains underexplored.

Method: The paper optimizes parameters related to chunking, graph construction, retrieval, and prompting using Cognee, a modular framework for end-to-end KG construction and retrieval. Each configuration is scored using established metrics.

Result: Meaningful gains can be achieved through targeted tuning, but these gains are not uniform across datasets and metrics.

Conclusion: Hyperparameter tuning shows immediate potential, but future progress will depend on clearer frameworks for optimization and evaluation in complex, modular systems.

Abstract: Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results
in complex systems with numerous hyperparameters that directly affect
performance. While such systems are increasingly common in retrieval-augmented
generation, the role of systematic hyperparameter optimization remains
underexplored. In this paper, we study this problem in the context of Cognee, a
modular framework for end-to-end KG construction and retrieval. Using three
multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize
parameters related to chunking, graph construction, retrieval, and prompting.
Each configuration is scored using established metrics (exact match, F1, and
DeepEval's LLM-based correctness metric). Our results demonstrate that
meaningful gains can be achieved through targeted tuning. While the gains are
consistent, they are not uniform, with performance varying across datasets and
metrics. This variability highlights both the value of tuning and the
limitations of standard evaluation measures. While demonstrating the immediate
potential of hyperparameter tuning, we argue that future progress will depend
not only on architectural advances but also on clearer frameworks for
optimization and evaluation in complex, modular systems.

</details>


### [26] [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/abs/2505.24479)
*Sania Nayab,Marco Simoni,Giulio Rossolini*

Main category: cs.AI

TL;DR: The paper proposes a novel approach leveraging knowledge graphs (KGs) to generate fake triplets that guide large language models (LLMs) in producing misinformation statements. It also investigates LLMs' effectiveness in distinguishing genuine and artificial misinformation.


<details>
  <summary>Details</summary>
Motivation: To understand and assess the threats of misinformation amplified by generative AI, requiring methodologies for structured and scalable misinformation generation.

Method: Proposes using knowledge graphs to identify plausibly false relationships and generate fake triplets, which then guide LLMs in creating misinformation statements with varying credibility degrees.

Result: Demonstrates that the deterministic approach produces challenging misinformation for humans to detect, solely using publicly available KGs. Also finds significant limitations in current LLM-based detection methods.

Conclusion: Highlights the necessity for improved detection strategies and further exploration of biases in generative models.

Abstract: The rapid spread of misinformation, further amplified by recent advances in
generative AI, poses significant threats to society, impacting public opinion,
democratic stability, and national security. Understanding and proactively
assessing these threats requires exploring methodologies that enable structured
and scalable misinformation generation. In this paper, we propose a novel
approach that leverages knowledge graphs (KGs) as structured semantic resources
to systematically generate fake triplets. By analyzing the structural
properties of KGs, such as the distance between entities and their predicates,
we identify plausibly false relationships. These triplets are then used to
guide large language models (LLMs) in generating misinformation statements with
varying degrees of credibility. By utilizing structured semantic relationships,
our deterministic approach produces misinformation inherently challenging for
humans to detect, drawing exclusively upon publicly available KGs (e.g.,
WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing
between genuine and artificially generated misinformation. Our analysis
highlights significant limitations in current LLM-based detection methods,
underscoring the necessity for enhanced detection strategies and a deeper
exploration of inherent biases in generative models.

</details>


### [27] [MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge](https://arxiv.org/abs/2505.24493)
*Xin Jing,Jiadong Wang,Iosif Tsangko,Andreas Triantafyllopoulos,Björn W. Schuller*

Main category: cs.AI

TL;DR: The paper explores the use of GPT-4o for annotating a multimodal dataset derived from the sitcom Friends, using only textual cues as inputs. It proposes MELT, a multimodal emotion dataset fully annotated by GPT-4o, and demonstrates its effectiveness by fine-tuning SSL backbones and assessing SER performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of costly and inconsistent human annotation in speech emotion recognition (SER), and to investigate the potential of Large Language Models (LLMs) like GPT-4o to perform emotional speech data annotation without human supervision.

Method: Apply GPT-4o to annotate a multimodal dataset collected from the sitcom Friends using structured text prompts with only textual cues as inputs. Propose MELT, a multimodal emotion dataset fully annotated by GPT-4o.

Result: MELT demonstrates effectiveness in improving SER performance when used to fine-tune four self-supervised learning (SSL) backbones. Subjective experiments also show consistent performance improvement on SER.

Conclusion: GPT-4o can generate accurate and contextually relevant annotations for emotional speech data without direct access to multimodal inputs, showcasing the potential of LLMs for scalable annotation in SER.

Abstract: Although speech emotion recognition (SER) has advanced significantly with
deep learning, annotation remains a major hurdle. Human annotation is not only
costly but also subject to inconsistencies annotators often have different
preferences and may lack the necessary contextual knowledge, which can lead to
varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have
emerged as a scalable alternative for annotating text data. However, the
potential of LLMs to perform emotional speech data annotation without human
supervision has yet to be thoroughly investigated. To address these problems,
we apply GPT-4o to annotate a multimodal dataset collected from the sitcom
Friends, using only textual cues as inputs. By crafting structured text
prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated
during its training, showcasing that it can generate accurate and contextually
relevant annotations without direct access to multimodal inputs. Therefore, we
propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We
demonstrate the effectiveness of MELT by fine-tuning four self-supervised
learning (SSL) backbones and assessing speech emotion recognition performance
across emotion datasets. Additionally, our subjective experiments\' results
demonstrate a consistence performance improvement on SER.

</details>


### [28] [Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction](https://arxiv.org/abs/2505.24597)
*Shuai Liu,Ning Cao,Yile Chen,Yue Jiang,Gao Cong*

Main category: cs.AI

TL;DR: NextLocMoE is a new framework that improves next location prediction by using large language models and a dual-level Mixture-of-Experts design, which captures complex location semantics and heterogeneous behavioral dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for next location prediction do not adequately capture the complex semantics of real-world locations or model the diverse behavioral dynamics across different user groups.

Method: The framework NextLocMoE is introduced, which uses large language models (LLMs) and has a dual-level Mixture-of-Experts design. It includes a Location Semantics MoE for encoding location functional semantics at the embedding level and a Personalized MoE within the Transformer backbone for adapting to individual user mobility patterns. Additionally, a history-aware routing mechanism is incorporated to enhance expert selection and ensure prediction stability.

Result: Empirical evaluations on real-world urban datasets indicate that NextLocMoE achieves superior performance in predictive accuracy, cross-domain generalization, and interpretability compared to existing methods.

Conclusion: NextLocMoE addresses the limitations of current next location prediction methods by better capturing location semantics and modeling heterogeneous behavioral dynamics, leading to improved performance.

Abstract: Next location prediction plays a critical role in understanding human
mobility patterns. However, existing approaches face two core limitations: (1)
they fall short in capturing the complex, multi-functional semantics of
real-world locations; and (2) they lack the capacity to model heterogeneous
behavioral dynamics across diverse user groups. To tackle these challenges, we
introduce NextLocMoE, a novel framework built upon large language models (LLMs)
and structured around a dual-level Mixture-of-Experts (MoE) design. Our
architecture comprises two specialized modules: a Location Semantics MoE that
operates at the embedding level to encode rich functional semantics of
locations, and a Personalized MoE embedded within the Transformer backbone to
dynamically adapt to individual user mobility patterns. In addition, we
incorporate a history-aware routing mechanism that leverages long-term
trajectory data to enhance expert selection and ensure prediction stability.
Empirical evaluations across several real-world urban datasets show that
NextLocMoE achieves superior performance in terms of predictive accuracy,
cross-domain generalization, and interpretability

</details>


### [29] [Taxonomic Networks: A Representation for Neuro-Symbolic Pairing](https://arxiv.org/abs/2505.24601)
*Zekun Wang,Ethan L. Haarer,Nicki Barari,Christopher J. MacLellan*

Main category: cs.AI

TL;DR: The paper introduces neuro-symbolic pairs and taxonomic networks, showing their complementary advantages in learning taxonomic nets.


<details>
  <summary>Details</summary>
Motivation: To create a system that integrates neural and symbolic approaches through a common knowledge representation for more efficient and accurate learning.

Method: Development of neuro-symbolic pairs consisting of neural and symbolic methods linked by a common knowledge representation, specifically using taxonomic networks where nodes represent hierarchically organized concepts.

Result: Symbolic method learns taxonomic nets more efficiently with less data and compute; neural method finds higher-accuracy taxonomic nets when provided with greater resources. These approaches can be used interchangeably based on situational needs.

Conclusion: This work lays the foundation for future systems that more fundamentally integrate neural and symbolic computation.

Abstract: We introduce the concept of a \textbf{neuro-symbolic pair} -- neural and
symbolic approaches that are linked through a common knowledge representation.
Next, we present \textbf{taxonomic networks}, a type of discrimination network
in which nodes represent hierarchically organized taxonomic concepts. Using
this representation, we construct a novel neuro-symbolic pair and evaluate its
performance. We show that our symbolic method learns taxonomic nets more
efficiently with less data and compute, while the neural method finds
higher-accuracy taxonomic nets when provided with greater resources. As a
neuro-symbolic pair, these approaches can be used interchangeably based on
situational needs, with seamless translation between them when necessary. This
work lays the foundation for future systems that more fundamentally integrate
neural and symbolic computation.

</details>


### [30] [Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success](https://arxiv.org/abs/2505.24622)
*Ben Griffin,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 通过结合大型语言模型生成的YES/NO问题和专家指导的启发式方法，构建了一个轻量级集成框架，用于预测初创企业的成功。该框架在保持完全透明的同时，比随机选择提高了5倍的精度。


<details>
  <summary>Details</summary>
Motivation: 预测初创企业成功需要既准确又可解释的模型。现有的方法可能缺乏透明度或不够准确。

Method: 使用大型语言模型生成YES/NO问题作为弱启发式，通过基于阈值的投票机制对这些问题进行过滤、排名和聚合，从而构建一个强大的集成预测器。还可以将专家指导的启发式方法纳入生成过程以提高性能。

Result: 在测试集中，有10%的初创企业被分类为成功的，该方法达到了50%的精确率，比随机选择提高了5倍。加入专家指导的启发式方法后，精确率进一步提高到54%。

Conclusion: 结合大型语言模型推理与人类洞察力具有价值，简单的、可解释的集成可以支持诸如风险投资等高风险领域的决策。

Abstract: Predicting startup success requires models that are both accurate and
interpretable. We present a lightweight ensemble framework that combines YES/NO
questions generated by large language models (LLMs), forming a transparent
decision-making system. Each question acts as a weak heuristic, and by
filtering, ranking, and aggregating them through a threshold-based voting
mechanism, we construct a strong ensemble predictor. On a test set where 10% of
startups are classified as successful, our approach achieves a precision rate
of 50%, representing a 5x improvement over random selection, while remaining
fully transparent. When we incorporate expert-guided heuristics into the
generation process, performance improves further to 54% precision. These
results highlight the value of combining LLM reasoning with human insight and
demonstrate that simple, interpretable ensembles can support high-stakes
decisions in domains such as venture capital (VC).

</details>


### [31] [Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models](https://arxiv.org/abs/2505.24655)
*Frederike Lübeck,Jonas Wildberger,Frederik Träuble,Maximilian Mordig,Sergios Gatidis,Andreas Krause,Bernhard Schölkopf*

Main category: cs.AI

TL;DR: The paper introduces AdaCVD, a new framework for predicting cardiovascular disease (CVD) risk that overcomes limitations of existing models by flexibly incorporating diverse patient data, integrating structured and unstructured information, and adapting to different populations.


<details>
  <summary>Details</summary>
Motivation: Existing CVD risk prediction models have difficulty handling real-world clinical challenges due to oversimplified patient profiles, rigid input requirements, and sensitivity to distribution shifts.

Method: AdaCVD is an adaptable framework built using large language models fine-tuned on extensive UK Biobank data. It incorporates comprehensive patient information, handles both structured and unstructured data, and can quickly adapt to new patient populations with minimal additional data.

Result: AdaCVD outperforms established risk scores and standard machine learning methods, achieving state-of-the-art performance in benchmark comparisons. It shows robustness across various subgroups, including underrepresented ones.

Conclusion: AdaCVD represents a significant advancement in flexible, AI-driven clinical decision support tools tailored to diverse and changing healthcare settings.

Abstract: Cardiovascular disease (CVD) risk prediction models are essential for
identifying high-risk individuals and guiding preventive actions. However,
existing models struggle with the challenges of real-world clinical practice as
they oversimplify patient profiles, rely on rigid input schemas, and are
sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk
prediction framework built on large language models extensively fine-tuned on
over half a million participants from the UK Biobank. In benchmark comparisons,
AdaCVD surpasses established risk scores and standard machine learning
approaches, achieving state-of-the-art performance. Crucially, for the first
time, it addresses key clinical challenges across three dimensions: it flexibly
incorporates comprehensive yet variable patient information; it seamlessly
integrates both structured data and unstructured text; and it rapidly adapts to
new patient populations using minimal additional data. In stratified analyses,
it demonstrates robust performance across demographic, socioeconomic, and
clinical subgroups, including underrepresented cohorts. AdaCVD offers a
promising path toward more flexible, AI-driven clinical decision support tools
suited to the realities of heterogeneous and dynamic healthcare environments.

</details>


### [32] [AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models](https://arxiv.org/abs/2505.24784)
*Conor Heins,Toon Van de Maele,Alexander Tschantz,Hampus Linander,Dimitrije Markovic,Tommaso Salvatori,Corrado Pezzato,Ozan Catal,Ran Wei,Magnus Koudahl,Marco Perin,Karl Friston,Tim Verbelen,Christopher Buckley*

Main category: cs.AI

TL;DR: AXIOM is a novel architecture that combines Bayesian data efficiency and DRL generalization by using core priors about object-centric dynamics to accelerate learning in low-data regimes.


<details>
  <summary>Details</summary>
Motivation: Current deep reinforcement learning approaches struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference models lack the domain flexibility typical of DRL approaches.

Method: AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories capturing sparse object-object interactions. The generative model structure is expanded online by growing and learning mixture models from single events and refined through Bayesian model reduction.

Result: AXIOM masters various games within only 10,000 interaction steps, using a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization.

Conclusion: AXIOM bridges the gap between active inference and DRL by combining data efficiency and interpretability of Bayesian approaches with across-task generalization usually associated with DRL.

Abstract: Current deep reinforcement learning (DRL) approaches achieve state-of-the-art
performance in various domains, but struggle with data efficiency compared to
human learning, which leverages core priors about objects and their
interactions. Active inference offers a principled framework for integrating
sensory information with prior knowledge to learn a world model and quantify
the uncertainty of its own beliefs and predictions. However, active inference
models are usually crafted for a single task with bespoke knowledge, so they
lack the domain flexibility typical of DRL approaches. To bridge this gap, we
propose a novel architecture that integrates a minimal yet expressive set of
core priors about object-centric dynamics and interactions to accelerate
learning in low-data regimes. The resulting approach, which we call AXIOM,
combines the usual data efficiency and interpretability of Bayesian approaches
with the across-task generalization usually associated with DRL. AXIOM
represents scenes as compositions of objects, whose dynamics are modeled as
piecewise linear trajectories that capture sparse object-object interactions.
The structure of the generative model is expanded online by growing and
learning mixture models from single events and periodically refined through
Bayesian model reduction to induce generalization. AXIOM masters various games
within only 10,000 interaction steps, with both a small number of parameters
compared to DRL, and without the computational expense of gradient-based
optimization.

</details>


### [33] [EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/abs/2505.24785)
*Patrick Tser Jern Kon,Jiachen Liu,Xinyi Zhu,Qiuyi Ding,Jingjia Peng,Jiarong Xing,Yibo Huang,Yiming Qiu,Jayanth Srinivasa,Myungjin Lee,Mosharaf Chowdhury,Matei Zaharia,Ang Chen*

Main category: cs.AI

TL;DR: EXP-Bench is a new benchmark that evaluates AI agents on complete research experiments from influential AI publications. It highlights the current limitations of AI agents in conducting end-to-end experiments and serves as a tool for future improvements.


<details>
  <summary>Details</summary>
Motivation: Automating AI research can accelerate scientific progress, but current AI agents face challenges with rigorous, end-to-end experimentation.

Method: Introduced EXP-Bench, a benchmark that evaluates AI agents on complete research experiments. It uses a semi-autonomous pipeline to extract and structure experimental details from research papers and open-source code, curating 461 tasks from 51 top-tier papers.

Result: Leading LLM-based agents scored 20-35% on individual experimental aspects, but only 0.5% success rate for complete experiments.

Conclusion: EXP-Bench identifies bottlenecks in AI agents' capabilities and provides procedures to improve their ability to conduct AI research experiments.

Abstract: Automating AI research holds immense potential for accelerating scientific
progress, yet current AI agents struggle with the complexities of rigorous,
end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed
to systematically evaluate AI agents on complete research experiments sourced
from influential AI publications. Given a research question and incomplete
starter code, EXP-Bench challenges AI agents to formulate hypotheses, design
and implement experimental procedures, execute them, and analyze results. To
enable the creation of such intricate and authentic tasks with high-fidelity,
we design a semi-autonomous pipeline to extract and structure crucial
experimental details from these research papers and their associated
open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks
from 51 top-tier AI research papers. Evaluations of leading LLM-based agents,
such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial
capabilities: while scores on individual experimental aspects such as design or
implementation correctness occasionally reach 20-35%, the success rate for
complete, executable experiments was a mere 0.5%. By identifying these
bottlenecks and providing realistic step-by-step experiment procedures,
EXP-Bench serves as a vital tool for future AI agents to improve their ability
to conduct AI research experiments. EXP-Bench is open-sourced at
https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.

</details>


### [34] [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
*Jingyan Shen,Jiarui Yao,Rui Yang,Yifan Sun,Feng Luo,Rui Pan,Tong Zhang,Han Zhao*

Main category: cs.AI

TL;DR: MiCRo is a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations.


<details>
  <summary>Details</summary>
Motivation: Reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. This limits LLMs from supporting personalization and pluralistic alignment.

Method: MiCRo introduces context-aware mixture modeling approach in the first stage to capture diverse human preferences. In the second stage, it integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity.

Result: Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.

Conclusion: MiCRo is a promising solution for enhancing personalized preference learning and improving downstream personalization.

Abstract: Reward modeling is a key step in building safe foundation models when
applying reinforcement learning from human feedback (RLHF) to align Large
Language Models (LLMs). However, reward modeling based on the Bradley-Terry
(BT) model assumes a global reward function, failing to capture the inherently
diverse and heterogeneous human preferences. Hence, such oversimplification
limits LLMs from supporting personalization and pluralistic alignment.
Theoretically, we show that when human preferences follow a mixture
distribution of diverse subgroups, a single BT model has an irreducible error.
While existing solutions, such as multi-objective learning with fine-grained
annotations, help address this issue, they are costly and constrained by
predefined attributes, failing to fully capture the richness of human values.
In this work, we introduce MiCRo, a two-stage framework that enhances
personalized preference learning by leveraging large-scale binary preference
datasets without requiring explicit fine-grained annotations. In the first
stage, MiCRo introduces context-aware mixture modeling approach to capture
diverse human preferences. In the second stage, MiCRo integrates an online
routing strategy that dynamically adapts mixture weights based on specific
context to resolve ambiguity, allowing for efficient and scalable preference
adaptation with minimal additional supervision. Experiments on multiple
preference datasets demonstrate that MiCRo effectively captures diverse human
preferences and significantly improves downstream personalization.

</details>


### [35] [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
*Yaxin Luo,Zhaoyi Li,Jiacheng Liu,Jiacheng Cui,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.AI

TL;DR: 研究人员开发了Open CaptchaWorld，这是一个基于网络的基准和平台，用于评估由多模态大语言模型（MLLM）驱动的代理在解决各种动态验证码挑战中的视觉推理和交互能力。该基准涵盖了20种现代验证码类型，总计225个验证码，并引入了一个新的度量标准——验证码推理深度，以量化解决每个谜题所需的认知和运动步骤数量。实验结果表明，人类的表现接近完美，而最先进的MLLM代理则显著挣扎，成功率最高仅为40.0%，远低于人类水平的93.3%。这凸显了Open CaptchaWorld作为诊断当前多模态代理局限性和指导更强大多模态推理系统发展的关键基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管现代多模态大语言模型代理在静态感知任务中表现出色，但它们处理像验证码这样的交互式、多步推理挑战的能力尚未得到充分测试。为了解决这一差距，需要一个专门设计的基准和平台来评估这些代理的能力。

Method: 研究人员创建了Open CaptchaWorld，包含20种现代验证码类型，共计225个验证码。他们还提出了一个新的度量标准——验证码推理深度，用以量化解决每个验证码所需的认知和运动步骤。

Result: 实验结果显示，人类在解决验证码方面表现优异，接近完美分数，而最先进的多模态大语言模型代理则显著挣扎，成功率最高仅为40.0%，远低于人类水平的93.3%。

Conclusion: Open CaptchaWorld被证明是一个重要的基准，能够诊断当前多模态代理的局限性，并指导开发更加稳健的多模态推理系统。代码和数据已在指定URL上提供。

Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in
real-world applications, often blocking them from completing end-to-end
automation tasks. While modern multimodal LLM agents have demonstrated
impressive performance in static perception tasks, their ability to handle
interactive, multi-step reasoning challenges like CAPTCHAs is largely untested.
To address this gap, we introduce Open CaptchaWorld, the first web-based
benchmark and platform specifically designed to evaluate the visual reasoning
and interaction capabilities of MLLM-powered agents through diverse and dynamic
CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225
CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,
which quantifies the number of cognitive and motor steps required to solve each
puzzle. Experimental results show that humans consistently achieve near-perfect
scores, state-of-the-art MLLM agents struggle significantly, with success rates
at most 40.0% by Browser-Use Openai-o3, far below human-level performance,
93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing
the limits of current multimodal agents and guiding the development of more
robust multimodal reasoning systems. Code and Data are available at this https
URL.

</details>


### [36] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/abs/2505.05880)
*Bettina Fazzinga,Sergio Flesca,Filippo Furfaro,Luigi Pontieri,Francesco Scala*

Main category: cs.AI

TL;DR: 本文提出了一种数据/计算高效的神经符号方法，通过结合示例驱动的序列标记器和基于AAF的推理器来解决事件解释问题，减少数据标注和模型优化成本，同时利用先验知识弥补示例数据的不足。


<details>
  <summary>Details</summary>
Motivation: 在事件与活动映射高度不确定或未明确指定的情况下，现有的基于推理的方法可能会产生信息量低的结果和繁重的计算。此外，训练一个序列标记模型需要大量手动标注的示例痕迹，这不符合开发绿色AI解决方案的需求。

Method: 提出了一种数据/计算高效的神经符号方法，将示例驱动的序列标记器生成的候选解释交给基于AAF的推理器进行细化，从而利用先验知识弥补示例数据的稀缺性。

Result: 实验结果表明，所提出的方法能够有效利用先验知识，在数据标注和模型优化成本受到严格限制的情况下表现出色。

Conclusion: 所提出的神经符号方法为事件解释问题提供了一个高效解决方案，能够在减少劳动/计算成本和碳足迹的同时，应对数据稀缺性的挑战。

Abstract: Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control](https://arxiv.org/abs/2505.23857)
*Wuhao Wang,Zhiyong Chen*

Main category: cs.LG

TL;DR: DATD3 is a new actor-critic algorithm which uses depthwise separable convolution and multi-head attention to encode historical observations, outperforming existing models in continuous control tasks with partial or full observability.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning often faces challenges in real-world applications due to receiving only partial state information. To tackle this issue, there's a need for an approach that can accommodate decision-making based on observation histories.

Method: The paper proposes the Output-Feedback Markov Decision Process (OPMDP) and DATD3, a novel actor-critic algorithm. DATD3 employs depthwise separable convolution and multi-head attention to encode historical observations, maintaining policy expressiveness while avoiding instability of recurrent models.

Result: Extensive experiments on continuous control tasks show that DATD3 outperforms existing memory-based and recurrent baselines under both partial and full observability.

Conclusion: DATD3 provides a superior alternative for reinforcement learning in output-feedback settings, enhancing performance in both partially and fully observable environments.

Abstract: Reinforcement learning in real-world applications often involves
output-feedback settings, where the agent receives only partial state
information. To address this challenge, we propose the Output-Feedback Markov
Decision Process (OPMDP), which extends the standard MDP formulation to
accommodate decision-making based on observation histories. Building on this
framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic
Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise
separable convolution and multi-head attention to encode historical
observations. DATD3 maintains policy expressiveness while avoiding the
instability of recurrent models. Extensive experiments on continuous control
tasks demonstrate that DATD3 outperforms existing memory-based and recurrent
baselines under both partial and full observability.

</details>


### [38] [Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration](https://arxiv.org/abs/2505.23859)
*Wenju Sun,Qingyong Li,Wen Wang,Yang Liu,Yangli-ao Geng,Boyang Li*

Main category: cs.LG

TL;DR: The paper introduces LOT Merging, a method to minimize feature drift during multi-task model merging, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-task model merging either have a significant performance gap or require costly secondary training procedures. The authors observe that performance degradation correlates with feature drift.

Method: LOT Merging is proposed, which minimizes feature drift between task-specific experts and the unified model in a layer-by-layer manner, formulated as a convex quadratic optimization problem.

Result: Experiments show that LOT Merging significantly outperforms baseline methods, achieving up to 4.4% improvements over state-of-the-art approaches on vision and vision-language benchmarks.

Conclusion: LOT Merging provides an efficient way to consolidate knowledge from multiple task-specific models into a unified model with minimal performance degradation.

Abstract: Multi-task model merging aims to consolidate knowledge from multiple
fine-tuned task-specific experts into a unified model while minimizing
performance degradation. Existing methods primarily approach this by minimizing
differences between task-specific experts and the unified model, either from a
parameter-level or a task-loss perspective. However, parameter-level methods
exhibit a significant performance gap compared to the upper bound, while
task-loss approaches entail costly secondary training procedures. In contrast,
we observe that performance degradation closely correlates with feature drift,
i.e., differences in feature representations of the same sample caused by model
merging. Motivated by this observation, we propose Layer-wise Optimal Task
Vector Merging (LOT Merging), a technique that explicitly minimizes feature
drift between task-specific experts and the unified model in a layer-by-layer
manner. LOT Merging can be formulated as a convex quadratic optimization
problem, enabling us to analytically derive closed-form solutions for the
parameters of linear and normalization layers. Consequently, LOT Merging
achieves efficient model consolidation through basic matrix operations.
Extensive experiments across vision and vision-language benchmarks demonstrate
that LOT Merging significantly outperforms baseline methods, achieving
improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.

</details>


### [39] [BiBLDR: Bidirectional Behavior Learning for Drug Repositioning](https://arxiv.org/abs/2505.23861)
*Renye Zhang,Mengyun Yang,Qichang Zhao,Jianxin Wang*

Main category: cs.LG

TL;DR: Drug repositioning using a bidirectional behavior learning strategy (BiBLDR) for better performance in cold-start scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based drug repositioning methods predominantly utilize graph-based representations which struggle to perform effective inference in cold-start scenarios involving novel drugs because of the lack of association information with the diseases.

Method: The innovative framework, BiBLDR, redefines drug repositioning as a behavior sequential learning task. It constructs bidirectional behavioral sequences based on drug and disease sides and proposes a two-stage strategy for drug repositioning. In the first stage, prototype spaces are constructed to characterize the representational attributes of drugs and diseases. In the second stage, these refined prototypes and bidirectional behavior sequence data are leveraged to predict potential drug-disease associations.

Result: Extensive experiments demonstrate that the method achieves state-of-the-art performance on benchmark datasets and significantly superior performance compared to previous methods in cold-start scenarios.

Conclusion: BiBLDR is a robust and precise model for capturing interactive relationships between drug and disease features from bidirectional behavioral sequences, especially excelling in cold-start scenarios.

Abstract: Drug repositioning aims to identify potential new indications for existing
drugs to reduce the time and financial costs associated with developing new
drugs. Most existing deep learning-based drug repositioning methods
predominantly utilize graph-based representations. However, graph-based drug
repositioning methods struggle to perform effective inference in cold-start
scenarios involving novel drugs because of the lack of association information
with the diseases. Unlike traditional graph-based approaches, we propose a
bidirectional behavior learning strategy for drug repositioning, known as
BiBLDR. This innovative framework redefines drug repositioning as a behavior
sequential learning task to capture drug-disease interaction patterns. First,
we construct bidirectional behavioral sequences based on drug and disease
sides. The consideration of bidirectional information ensures a more meticulous
and rigorous characterization of the behavioral sequences. Subsequently, we
propose a two-stage strategy for drug repositioning. In the first stage, we
construct prototype spaces to characterize the representational attributes of
drugs and diseases. In the second stage, these refined prototypes and
bidirectional behavior sequence data are leveraged to predict potential
drug-disease associations. Based on this learning approach, the model can more
robustly and precisely capture the interactive relationships between drug and
disease features from bidirectional behavioral sequences. Extensive experiments
demonstrate that our method achieves state-of-the-art performance on benchmark
datasets. Meanwhile, BiBLDR demonstrates significantly superior performance
compared to previous methods in cold-start scenarios. Our code is published in
https://github.com/Renyeeah/BiBLDR.

</details>


### [40] [Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting](https://arxiv.org/abs/2505.23863)
*Chang Liu,Bohao Zhao,Jingtao Ding,Huandong Wang,Yong Li*

Main category: cs.LG

TL;DR: PhyxMamba is a novel framework integrating Mamba-based state-space model with physics-informed principles for long-term forecasting of chaotic systems from short-term observations.


<details>
  <summary>Details</summary>
Motivation: Long-term forecasting of chaotic systems remains a challenge due to sensitivity to initial conditions and complex geometry. Existing methods struggle with maintaining predictive stability and dynamical coherence over extended horizons.

Method: PhyxMamba reconstructs the attractor manifold from brief observations using time-delay embeddings, extracts global dynamical features, and uses a generative training scheme with multi-token prediction and attractor geometry regularization to replicate the physical process and preserve key statistical invariants.

Result: Extensive evaluations show that PhyxMamba delivers superior long-term forecasting and captures essential dynamical invariants from short-term data.

Conclusion: PhyxMamba opens new avenues for reliably predicting chaotic systems under observation-scarce conditions, with implications across various fields.

Abstract: Long-term forecasting of chaotic systems from short-term observations remains
a fundamental and underexplored challenge due to the intrinsic sensitivity to
initial conditions and the complex geometry of strange attractors. Existing
approaches often rely on long-term training data or focus on short-term
sequence correlations, struggling to maintain predictive stability and
dynamical coherence over extended horizons. We propose PhyxMamba, a novel
framework that integrates a Mamba-based state-space model with physics-informed
principles to capture the underlying dynamics of chaotic systems. By
reconstructing the attractor manifold from brief observations using time-delay
embeddings, PhyxMamba extracts global dynamical features essential for accurate
forecasting. Our generative training scheme enables Mamba to replicate the
physical process, augmented by multi-token prediction and attractor geometry
regularization for physical constraints, enhancing prediction accuracy and
preserving key statistical invariants. Extensive evaluations on diverse
simulated and real-world chaotic systems demonstrate that PhyxMamba delivers
superior long-term forecasting and faithfully captures essential dynamical
invariants from short-term data. This framework opens new avenues for reliably
predicting chaotic systems under observation-scarce conditions, with broad
implications across climate science, neuroscience, epidemiology, and beyond.
Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.

</details>


### [41] [Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections](https://arxiv.org/abs/2505.23864)
*Wei Zhuo,Zhaohuan Zhan,Ziduo Yang,Han Yu*

Main category: cs.LG

TL;DR: This paper introduces Federated learning with Auxiliary projections (FedAux), a personalized subgraph FL framework for non-IID graph-structured data, which learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. Empirical evaluations show that FedAux outperforms existing baselines in both accuracy and personalization performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph.

Method: FedAux is a personalized subgraph FL framework that jointly trains a local GNN and a learnable auxiliary projection vector (APV) that projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter-client similarities and perform similarity-weighted parameter mixing, yielding personalized models while preserving cross-client knowledge transfer.

Result: Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance.

Conclusion: The authors introduce FedAux, a personalized subgraph FL framework for non-IID graph-structured data, which learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. The method substantially outperforms existing baselines in both accuracy and personalization performance.

Abstract: Federated learning (FL) on graph-structured data typically faces non-IID
challenges, particularly in scenarios where each client holds a distinct
subgraph sampled from a global graph. In this paper, we introduce Federated
learning with Auxiliary projections (FedAux), a personalized subgraph FL
framework that learns to align, compare, and aggregate heterogeneously
distributed local models without sharing raw data or node embeddings. In
FedAux, each client jointly trains (i) a local GNN and (ii) a learnable
auxiliary projection vector (APV) that differentiably projects node embeddings
onto a 1D space. A soft-sorting operation followed by a lightweight 1D
convolution refines these embeddings in the ordered space, enabling the APV to
effectively capture client-specific information. After local training, these
APVs serve as compact signatures that the server uses to compute inter-client
similarities and perform similarity-weighted parameter mixing, yielding
personalized models while preserving cross-client knowledge transfer. Moreover,
we provide rigorous theoretical analysis to establish the convergence and
rationality of our design. Empirical evaluations across diverse graph
benchmarks demonstrate that FedAux substantially outperforms existing baselines
in both accuracy and personalization performance.

</details>


### [42] [Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration](https://arxiv.org/abs/2505.23865)
*Emanuele Masiero,Vito Trianni,Giuseppe Vizzari,Dimitri Ognibene*

Main category: cs.LG

TL;DR: 在精准农业中，提出了一种两阶段深度学习框架，用于作物监测的主动探索任务。该框架通过熵和可见性掩码来指导智能体优先探索信息丰富的区域，并通过模拟验证了不同智能体架构的性能。结果表明，利用不确定性感知策略可以实现稳健且可扩展的探索。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要高效的自主系统来进行作物监测，这些系统必须在大规模环境中进行探索，同时尽量减少资源消耗。

Method: 提出了一种两阶段深度学习框架：1. 使用预训练的LSTM作为信念模型，更新环境的概率图及其熵；2. 在输入中加入POV可见性掩码，以保持部分可观测性下的马尔可夫性质。比较了三种智能体架构：基于IG的未训练智能体、使用CNN的DQN智能体以及具有更宽空间上下文的Double-CNN DQN智能体。

Result: 在20x20地图上的模拟显示，未训练智能体表现良好，DQN智能体在包含POV掩码时匹配其性能，而Double-CNN DQN智能体在更大环境中表现出更高的探索效率。

Conclusion: 不确定性感知策略（利用熵、信念状态和可见性跟踪）能够实现稳健且可扩展的探索。未来工作包括课程学习、多智能体合作、基于变压器的模型以及内在动机机制等。

Abstract: Precision agriculture requires efficient autonomous systems for crop
monitoring, where agents must explore large-scale environments while minimizing
resource consumption. This work addresses the problem as an active exploration
task in a grid environment representing an agricultural field. Each cell may
contain targets (e.g., damaged crops) observable from nine predefined points of
view (POVs). Agents must infer the number of targets per cell using partial,
sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as
a belief model, updating a probabilistic map of the environment and its
associated entropy, which defines the expected information gain (IG). This
allows agents to prioritize informative regions. A key contribution is the
inclusion of a POV visibility mask in the input, preserving the Markov property
under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent
selecting actions to maximize entropy reduction; a DQN agent using CNNs over
local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent
with wider spatial context. Simulations on 20x20 maps showed that the untrained
agent performs well despite its simplicity. The DQN agent matches this
performance when the POV mask is included, while the Double-CNN agent
consistently achieves superior exploration efficiency, especially in larger
environments.
  Results show that uncertainty-aware policies leveraging entropy, belief
states, and visibility tracking lead to robust and scalable exploration. Future
work includes curriculum learning, multi-agent cooperation with shared rewards,
transformer-based models, and intrinsic motivation mechanisms to further
enhance learning efficiency and policy generalization.

</details>


### [43] [Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization](https://arxiv.org/abs/2505.23866)
*Chengli Tan,Yubo Zhou,Haishan Ye,Guang Dai,Junmin Liu,Zengjie Song,Jiangshe Zhang,Zixiang Zhao,Yunda Hao,Yong Xu*

Main category: cs.LG

TL;DR: Deep neural networks are often overconfident in safety-critical applications. This paper shows that sharpness-aware minimization (SAM) reduces this overconfidence and improves model calibration. A variant, CSAM, further enhances calibration.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks used in critical areas like medical diagnosis and autonomous driving tend to be poorly calibrated and overly confident, which can lead to serious issues.

Method: The authors use sharpness-aware minimization (SAM) instead of standard training methods. They find SAM counteracts overconfidence by implicitly maximizing the entropy of the predictive distribution. They also propose a variant called CSAM for better calibration.

Result: Experiments on datasets including ImageNet-1K show SAM reduces calibration error. CSAM performs even better than SAM and achieves lower calibration error consistently compared to other methods.

Conclusion: SAM and its variant CSAM effectively improve the calibration of deep neural networks, reducing their overconfidence in safety-critical applications.

Abstract: Deep neural networks have been increasingly used in safety-critical
applications such as medical diagnosis and autonomous driving. However, many
studies suggest that they are prone to being poorly calibrated and have a
propensity for overconfidence, which may have disastrous consequences. In this
paper, unlike standard training such as stochastic gradient descent, we show
that the recently proposed sharpness-aware minimization (SAM) counteracts this
tendency towards overconfidence. The theoretical analysis suggests that SAM
allows us to learn models that are already well-calibrated by implicitly
maximizing the entropy of the predictive distribution. Inspired by this
finding, we further propose a variant of SAM, coined as CSAM, to ameliorate
model calibration. Extensive experiments on various datasets, including
ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.
Meanwhile, CSAM performs even better than SAM and consistently achieves lower
calibration error than other approaches

</details>


### [44] [Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert](https://arxiv.org/abs/2505.23868)
*Zhaokun Wang,Jinyu Guo,Jingwen Pu,Lingfeng Chen,Hongli Pu,Jie Ou. Libo Qin,Wenhong Tian*

Main category: cs.LG

TL;DR: 提出了一种新的框架LoPE，通过不对称LoRA配置和生成的噪声数据提高模型对噪声的鲁棒性，无需数据清理。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效的微调方法容易受到噪声数据的干扰，而传统的噪声处理方法要么依赖繁琐的数据预处理，要么容易积累错误。

Method: LoPE从专家混合架构中汲取灵感，在不对称LoRA配置中战略性地整合了一个专门的中毒专家。通过两阶段范式，LoPE在微调过程中对中毒专家进行噪声注入以增强其噪声辨别和处理能力；在推理时，选择性地屏蔽专门的中毒专家，利用正常专家获得的净化知识实现对噪声鲁棒的输出。

Result: 广泛的实验证明，LoPE仅通过低成本的噪声注入就能达到强大的性能和鲁棒性，完全消除了数据清理的需求。

Conclusion: LoPE是一种新颖且有效的框架，能够通过生成的噪声数据增强模型对噪声的鲁棒性，无需数据清理步骤。

Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained
language models to downstream tasks are susceptible to interference from noisy
data. Conventional noise-handling approaches either rely on laborious data
pre-processing or employ model architecture modifications prone to error
accumulation. In contrast to existing noise-process paradigms, we propose a
noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a
novel framework that enhances model robustness to noise only with generated
noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE
strategically integrates a dedicated poisoning expert in an asymmetric LoRA
configuration. Through a two-stage paradigm, LoPE performs noise injection on
the poisoning expert during fine-tuning to enhance its noise discrimination and
processing ability. During inference, we selectively mask the dedicated
poisoning expert to leverage purified knowledge acquired by normal experts for
noise-robust output. Extensive experiments demonstrate that LoPE achieves
strong performance and robustness purely through the low-cost noise injection,
which completely eliminates the requirement of data cleaning.

</details>


### [45] [MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection](https://arxiv.org/abs/2505.23870)
*Yixian Shen,Qi Bi,Jia-Hong Huang,Hongyi Zhu,Andy D. Pimentel,Anuj Pathania*

Main category: cs.LG

TL;DR: A new adaptation method MaCP is presented, which achieves exceptional performance with minimal parameters and memory for fine-tuning large models. It exploits cosine projection to improve model efficiency and accuracy. Extensive experiments demonstrate its effectiveness across a wide range of tasks.


<details>
  <summary>Details</summary>
Motivation: To develop an adaptation method that achieves high performance while requiring minimal parameters and memory for fine-tuning large foundation models.

Method: The method projects the weight change from the low-rank adaptation into the discrete cosine space, partitions the weight change over different levels of the discrete cosine spectrum, and selects each partition's most critical frequency components.

Result: MaCP delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives in single-modality and multi-modality tasks.

Conclusion: MaCP is an effective adaptation method for improving both model efficiency and accuracy with minimal resource consumption.

Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine
Projection, that achieves exceptional performance while requiring minimal
parameters and memory for fine-tuning large foundation models. Its general idea
is to exploit the superior energy compaction and decorrelation properties of
cosine projection to improve both model efficiency and accuracy. Specifically,
it projects the weight change from the low-rank adaptation into the discrete
cosine space. Then, the weight change is partitioned over different levels of
the discrete cosine spectrum, and each partition's most critical frequency
components are selected. Extensive experiments demonstrate the effectiveness of
MaCP across a wide range of single-modality tasks, including natural language
understanding, natural language generation, text summarization, as well as
multi-modality tasks such as image classification and video understanding. MaCP
consistently delivers superior accuracy, significantly reduced computational
complexity, and lower memory requirements compared to existing alternatives.

</details>


### [46] [ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning](https://arxiv.org/abs/2505.23871)
*Zeyuan Liu,Zhihe Yang,Jiawei Xu,Rui Yang,Jiafei Lyu,Baoxiang Wang,Yunjian Xu,Xiu Li*

Main category: cs.LG

TL;DR: 提出了一种新的方法ADG，利用扩散模型解决离线强化学习中的数据污染问题，通过区分干净和污染数据并进行优化，提高了在多种噪声环境下的鲁棒性，达到业内领先水平。


<details>
  <summary>Details</summary>
Motivation: 实际收集的数据集容易受到噪声和错误的影响，而现有的方法在处理高维状态空间中的数据污染以及同时多个元素被污染的情况时仍显不足。

Method: 提出了Ambient Diffusion-Guided Dataset Recovery (ADG)，包括：1) 使用Ambient Denoising Diffusion Probabilistic Models (DDPM) 从近似分布中学习部分污染数据集；2) 利用Ambient DDPM的噪声预测属性区分干净和污染数据，并用干净子集训练标准DDPM；3) 使用训练好的标准DDPM优化之前识别出的污染数据以提高数据质量。

Result: 实验表明ADG有效缓解了污染数据的影响，在各种噪声环境下提高了离线强化学习的鲁棒性，并在MuJoCo、Kitchen和Adroit等基准测试中达到了业内领先的性能。

Conclusion: ADG方法具有高度的通用性，可以与任何离线RL算法无缝集成，显著提升了离线RL在不同噪声设置下的性能。

Abstract: Real-world datasets collected from sensors or human inputs are prone to noise
and errors, posing significant challenges for applying offline reinforcement
learning (RL). While existing methods have made progress in addressing
corrupted actions and rewards, they remain insufficient for handling corruption
in high-dimensional state spaces and for cases where multiple elements in the
dataset are corrupted simultaneously. Diffusion models, known for their strong
denoising capabilities, offer a promising direction for this problem-but their
tendency to overfit noisy samples limits their direct applicability. To
overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a
novel approach that pioneers the use of diffusion models to tackle data
corruption in offline RL. First, we introduce Ambient Denoising Diffusion
Probabilistic Models (DDPM) from approximated distributions, which enable
learning on partially corrupted datasets with theoretical guarantees. Second,
we use the noise-prediction property of Ambient DDPM to distinguish between
clean and corrupted data, and then use the clean subset to train a standard
DDPM. Third, we employ the trained standard DDPM to refine the previously
identified corrupted data, enhancing data quality for subsequent offline RL
training. A notable strength of ADG is its versatility-it can be seamlessly
integrated with any offline RL algorithm. Experiments on a range of benchmarks,
including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively
mitigates the impact of corrupted data and improves the robustness of offline
RL under various noise settings, achieving state-of-the-art results.

</details>


### [47] [A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants](https://arxiv.org/abs/2505.23875)
*Peter Samoaa,Marcus Vukojevic,Morteza Haghir Chehreghani,Antonio Longa*

Main category: cs.LG

TL;DR: A new graph-regression dataset named RelSC is introduced, which is built from program graphs combining syntactic and semantic information. It provides a continuous target variable for execution-time cost and comes in two variants: homogeneous (RelSC-H) and multi-relational (RelSC-M). Evaluations on various GNN architectures reveal significant performance differences based on structural representation.


<details>
  <summary>Details</summary>
Motivation: Public benchmarks for graph-level regression are heavily focused on molecular graphs and citation networks, limiting the diversity needed to develop models that generalize across different graph structures.

Method: The authors created RelSC, a dataset derived from program graphs with labels indicating execution-time cost. This dataset exists in two forms: RelSC-H with rich node features under a single edge type, and RelSC-M preserving the original multi-relational structure. Both variants allow researchers to explore how different representations affect model behavior.

Result: Evaluation of several graph neural network architectures on both RelSC-H and RelSC-M showed consistent performance differences between the homogeneous and multi-relational settings, highlighting the importance of structural representation in model performance.

Conclusion: RelSC serves as a valuable and challenging benchmark for advancing graph regression methods by providing diverse graph structures not found in existing datasets.

Abstract: Graph-level regression underpins many real-world applications, yet public
benchmarks remain heavily skewed toward molecular graphs and citation networks.
This limited diversity hinders progress on models that must generalize across
both homogeneous and heterogeneous graph structures. We introduce RelSC, a new
graph-regression dataset built from program graphs that combine syntactic and
semantic information extracted from source code. Each graph is labelled with
the execution-time cost of the corresponding program, providing a continuous
target variable that differs markedly from those found in existing benchmarks.
RelSC is released in two complementary variants. RelSC-H supplies rich node
features under a single (homogeneous) edge type, while RelSC-M preserves the
original multi-relational structure, connecting nodes through multiple edge
types that encode distinct semantic relationships. Together, these variants let
researchers probe how representation choice influences model behaviour. We
evaluate a diverse set of graph neural network architectures on both variants
of RelSC. The results reveal consistent performance differences between the
homogeneous and multi-relational settings, emphasising the importance of
structural representation. These findings demonstrate RelSC's value as a
challenging and versatile benchmark for advancing graph regression methods.

</details>


### [48] [A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size](https://arxiv.org/abs/2505.23876)
*Polad Geidarov*

Main category: cs.LG

TL;DR: This paper explores the use of multilayer perceptron neural networks with analytically calculated weights for metric recognition, demonstrating faster training and greater robustness to dataset reduction compared to random weight initialization through experiments on the MNIST dataset.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness and efficiency of using analytically pre-calculated weights in multilayer perceptron neural networks for metric recognition methods as opposed to traditional random weight initialization.

Method: Conduct comparative experiments by training multilayer perceptron neural networks on the MNIST dataset using both pre-calculated weights (via formulas) and random weight initialization, across different sizes of the training dataset.

Result: The experiments indicate that multilayer perceptrons with pre-calculated weights can be trained significantly faster and are more resilient when the size of the training dataset is reduced.

Conclusion: Using analytically derived weights for initializing a multilayer perceptron leads to more efficient training processes and better performance stability, especially when dealing with smaller training datasets.

Abstract: The paper discusses the capabilities of multilayer perceptron neural networks
implementing metric recognition methods, for which the values of the weights
are calculated analytically by formulas. Comparative experiments in training a
neural network with pre-calculated weights and with random initialization of
weights on different sizes of the MNIST training dataset are carried out. The
results of the experiments show that a multilayer perceptron with
pre-calculated weights can be trained much faster and is much more robust to
the reduction of the training dataset.

</details>


### [49] [Actor-Critic based Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2505.23878)
*Jing Ma,Chenhao Dang,Mingjie Liao*

Main category: cs.LG

TL;DR: The paper presents an actor-critic based online data mixing (AC-ODM) method to improve the pretraining of large language models (LLMs). It uses auxiliary actor-critic networks and considers intra-domain interactions with a reward function. The method accelerates convergence and improves performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing data mixing methods that do not evolve with training dynamics or consider intra-domain interactions, thereby improving the generalization ability of LLMs while reducing carbon footprint and financial costs.

Method: Developed an actor-critic based online data mixing (AC-ODM) method which captures varying domain weights using auxiliary actor-critic networks and considers intra-domain interactions via a reward function. The sampling strategy is transferred from a small proxy LLM to a larger target LLM during pretraining.

Result: AC-ODM-410M reaches optimal validation perplexity 71% faster than ODM, improves zero-shot MMLU benchmark accuracy by 27.5%, and enhances HumanEval pass@1 performance by about 2.23 times.

Conclusion: AC-ODM effectively improves the efficiency of dynamical data mixing and expedites the convergence of pretraining for target LLMs, leading to better performance on various benchmarks.

Abstract: The coverage and composition of pretraining data significantly impacts the
generalization ability of Large Language Models (LLMs). To reduce the carbon
footprint and financial costs of training, some data mixing methods, which
applied the optimized domain weights of a small proxy model to train a larger
one, were proposed. However, these methods did not evolute with the training
dynamics. The existing online data mixing (ODM) method addressed this
limitation by applying the multi-armed bandit algorithm as data sampling
strategy. Yet, it did not consider the intra-domain interactions. In this
paper, we develop an actor-critic based online data mixing (AC-ODM) method,
which captures the varying domain weights by auxiliary actor-critic networks
and consider the intra-domain interactions with the reward function. While
constructing the dataset to pretrain a large target LLM, we directly apply the
actor, which is trained with a small proxy LLM as the environment, as the
sampling strategy. The transfer of sampling strategy can not only ensure the
efficiency of dynamical data mixing, but also expedite the convergence of
pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,
which invokes the sampling strategy obtained by a proxy LLM with 410M
parameters, reaching the optimal validation perplexity of ODM 71% faster, and
improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,
about 2.23x better on pass@1 of HumanEval benchmark.

</details>


### [50] [CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data](https://arxiv.org/abs/2505.23879)
*Caio Cheohen,Vinnícius M. S. Gomes,Manuela L. da Silva*

Main category: cs.LG

TL;DR: The study developed a hybrid CNN-LSTM model using spike protein sequences and clinical data to predict COVID-19 severity with high accuracy, showing AI's potential in genomic surveillance.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for accurate prediction of disease severity in managing healthcare resources during the pandemic caused by SARS-CoV-2.

Method: Retrieved and processed 3,467 spike protein sequences from GISAID, extracted features through a feature engineering pipeline, one-hot encoded clinical variables, and trained a hybrid CNN-LSTM model.

Result: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, recall of 82.85%, and stabilized training at 85% accuracy with minimal overfitting.

Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity, demonstrating the utility of AI in genomic surveillance and precision public health.

Abstract: The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need
for accurate prediction of disease severity to optimize healthcare resource
allocation and patient management. The spike protein, which facilitates viral
entry into host cells, exhibits high mutation rates, particularly in the
receptor-binding domain, influencing viral pathogenicity. Artificial
intelligence approaches, such as deep learning, offer promising solutions for
leveraging genomic and clinical data to predict disease outcomes. Objective:
This study aimed to develop a hybrid CNN-LSTM deep learning model to predict
COVID-19 severity using spike protein sequences and associated clinical
metadata from South American patients. Methods: We retrieved 9,570 spike
protein sequences from the GISAID database, of which 3,467 met inclusion
criteria after standardization. The dataset included 2,313 severe and 1,154
mild cases. A feature engineering pipeline extracted features from sequences,
while demographic and clinical variables were one-hot encoded. A hybrid
CNN-LSTM architecture was trained, combining CNN layers for local pattern
extraction and an LSTM layer for long-term dependency modeling. Results: The
model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,
and recall of 82.85%, demonstrating robust classification performance. Training
stabilized at 85% accuracy with minimal overfitting. The most prevalent
lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional
epidemiological trends, suggesting potential associations between viral
genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model
effectively predicted COVID-19 severity using spike protein sequences and
clinical data, highlighting the utility of AI in genomic surveillance and
precision public health. Despite limitations, this approach provides a
framework for early severity prediction in future outbreaks.

</details>


### [51] [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
*Tianyuan Zhang,Sai Bi,Yicong Hong,Kai Zhang,Fujun Luan,Songlin Yang,Kalyan Sunkavalli,William T. Freeman,Hao Tan*

Main category: cs.LG

TL;DR: The paper introduces Large Chunk Test-Time Training (LaCT), an advancement over traditional Test-Time Training (TTT) methods, that uses large chunk updates to improve hardware utilization and state capacity. It allows for efficient processing of long-context data across diverse modalities and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing TTT methods are inefficient on modern GPUs due to their use of small online minibatch sizes which lead to extremely low FLOPs utilization. This limits their ability to handle long-context data effectively.

Method: LaCT employs extremely large chunk updates (2K to 1M tokens) across various tasks, improving hardware utilization by orders of magnitude and facilitating scaling of nonlinear state size up to 40% of model parameters. It also integrates sophisticated optimizers like Muon for online updates.

Result: Validated across diverse modalities and tasks such as novel view synthesis with image set, language models, and auto-regressive video diffusion. Successfully scaled a 14B-parameter AR video diffusion model on sequences up to 56K tokens and performed novel view synthesis with 1 million context length.

Conclusion: LaCT significantly enhances the capabilities of TTT in handling long-context data and inspires further research in this field.

Abstract: Test-Time Training (TTT) models context dependencies by adapting part of the
model's weights (referred to as fast weights) during inference. This fast
weight, akin to recurrent states in RNNs, stores temporary memories of past
tokens in the current sequence. Existing TTT methods struggled to show
effectiveness in handling long-context data, due to their inefficiency on
modern GPUs. The TTT layers in many of these approaches operate with extremely
low FLOPs utilization (often <5%) because they deliberately apply small online
minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,
a small minibatch implies fine-grained block-wise causal dependencies in the
data, unsuitable for data beyond 1D ordered sequences, like sets or
N-dimensional grids such as images or videos. In contrast, we pursue the
opposite direction by using an extremely large chunk update, ranging from 2K to
1M tokens across tasks of varying modalities, which we refer to as Large Chunk
Test-Time Training (LaCT). It improves hardware utilization by orders of
magnitude, and more importantly, facilitates scaling of nonlinear state size
(up to 40% of model parameters), hence substantially improving state capacity,
all without requiring cumbersome and error-prone kernel implementations. It
also allows easy integration of sophisticated optimizers, e.g. Muon for online
updates. We validate our approach across diverse modalities and tasks,
including novel view synthesis with image set, language models, and
auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR
video diffusion model on sequences up to 56K tokens. In our longest sequence
experiment, we perform novel view synthesis with 1 million context length. We
hope this work will inspire and accelerate new research in the field of
long-context modeling and test-time training. Website:
https://tianyuanzhang.com/projects/ttt-done-right

</details>


### [52] [Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling](https://arxiv.org/abs/2505.23913)
*Gustavo Sutter Pessurno de Carvalho,Mohammed Abdulrahman,Hao Wang,Sriram Ganapathi Subramanian,Marc St-Aubin,Sharon O'Sullivan,Lawrence Wan,Luis Ricardez-Sandoval,Pascal Poupart,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 提出了一种全新的无需代理模型拟合或获取函数优化的零样本贝叶斯优化方法，利用预训练深度生成模型直接从最优解后验分布中采样，相比高斯过程BO效率提升35倍以上。


<details>
  <summary>Details</summary>
Motivation: 当前贝叶斯优化（BO）虽然有无需重新训练的上下文代理模型，但大多数仍需要通过获取函数最大化选择下一次观测，引入许多调参需求，如蒙特卡洛采样器和多启动优化器，增加了复杂性。

Method: 使用预训练的深度生成模型直接从最优解点的后验分布中采样，从而完全在上下文中完成贝叶斯优化，避免了代理模型拟合和获取函数优化的需求，等价于汤普森采样。

Result: 在一系列真实基准测试中展示了该基础模型的能力和成本效益，与基于高斯过程的BO相比，取得了超过35倍的墙钟时间效率增益，支持高效的并行和分布式BO，适用于高吞吐量优化场景。

Conclusion: 所提出的方法提供了一种全新的、无需代理模型拟合或获取函数优化的零样本贝叶斯优化解决方案，大幅提升了优化效率，并简化了操作流程。

Abstract: The optimization of expensive black-box functions is ubiquitous in science
and engineering. A common solution to this problem is Bayesian optimization
(BO), which is generally comprised of two components: (i) a surrogate model and
(ii) an acquisition function, which generally require expensive re-training and
optimization steps at each iteration, respectively. Although recent work
enabled in-context surrogate models that do not require re-training, virtually
all existing BO methods still require acquisition function maximization to
select the next observation, which introduces many knobs to tune, such as Monte
Carlo samplers and multi-start optimizers. In this work, we propose a
completely in-context, zero-shot solution for BO that does not require
surrogate fitting or acquisition function optimization. This is done by using a
pre-trained deep generative model to directly sample from the posterior over
the optimum point. We show that this process is equivalent to Thompson sampling
and demonstrate the capabilities and cost-effectiveness of our foundation model
on a suite of real-world benchmarks. We achieve an efficiency gain of more than
35x in terms of wall-clock time when compared with Gaussian process-based BO,
enabling efficient parallel and distributed BO, e.g., for high-throughput
optimization.

</details>


### [53] [Thompson Sampling in Online RLHF with General Function Approximation](https://arxiv.org/abs/2505.23927)
*Songtao Feng,Jie Fu*

Main category: cs.LG

TL;DR: This paper explores the online reinforcement learning from human feedback (RLHF) setting, proposing a model-free posterior sampling algorithm inspired by Thompson sampling. It establishes a regret bound and introduces a concentration-type inequality for squared Bellman error.


<details>
  <summary>Details</summary>
Motivation: To study the statistical efficiency of RLHF algorithms theoretically, particularly in aligning large language models with human preferences.

Method: Design a model-free posterior sampling algorithm for online RLHF using Bellman eluder dimension as the complexity measure and establish O(√T) regret bound.

Result: Achieved O(√T) regret bound with multiplicative factors depending on horizon, BE dimension, and log-bracketing number. Also established a concentration-type inequality for squared Bellman error.

Conclusion: The proposed algorithm has strong theoretical guarantees and contributes to understanding the statistical efficiency of RLHF algorithms.

Abstract: Reinforcement learning from human feedback (RLHF) has achieved great
empirical success in aligning large language models (LLMs) with human
preference, and it is of great importance to study the statistical efficiency
of RLHF algorithms from a theoretical perspective. In this work, we consider
the online RLHF setting where the preference data is revealed during the
learning process and study action value function approximation. We design a
model-free posterior sampling algorithm for online RLHF inspired by Thompson
sampling and provide its theoretical guarantee. Specifically, we adopt Bellman
eluder (BE) dimension as the complexity measure of the function class and
establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other
multiplicative factor depending on the horizon, BE dimension and the
$log$-bracketing number of the function class. Further, in the analysis, we
first establish the concentration-type inequality of the squared Bellman error
bound based on the maximum likelihood estimator (MLE) generalization bound,
which plays the crucial rules in obtaining the eluder-type regret bound and may
be of independent interest.

</details>


### [54] [BIRD: Behavior Induction via Representation-structure Distillation](https://arxiv.org/abs/2505.23933)
*Galen Pogoncheff,Michael Beyeler*

Main category: cs.LG

TL;DR: BIRD is a framework that transfers aligned behavior by matching the internal representation structure of a student model to that of a teacher, improving robust accuracy and offering practical guidance for teacher selection and design.


<details>
  <summary>Details</summary>
Motivation: Transferring behavioral properties like robustness, fairness, and honesty to models trained on different tasks or data distributions is challenging as aligned behavior is easily forgotten during fine-tuning and collecting task-specific data can be costly.

Method: BIRD matches the internal representation structure of a student model to that of a teacher. It focuses on three interpretable and computable properties of the teacher's representations: task relevance, behavioral relevance, and complementary knowledge.

Result: BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is 25 times smaller than the student.

Conclusion: BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.

Abstract: Human-aligned deep learning models exhibit behaviors consistent with human
values, such as robustness, fairness, and honesty. Transferring these
behavioral properties to models trained on different tasks or data
distributions remains challenging: aligned behavior is easily forgotten during
fine-tuning, and collecting task-specific data that preserves this behavior can
be prohibitively costly. We introduce BIRD (Behavior Induction via
Representation-structure Distillation), a flexible framework for transferring
aligned behavior by matching the internal representation structure of a student
model to that of a teacher. Applied to out-of-distribution robustness in image
classification, BIRD outperforms fine-tuning, transfer learning, and continual
learning methods, improving robust accuracy by up to 16% over the next
strongest baseline. It remains effective even when the teacher is trained on a
much simpler dataset and is $25 \times$ smaller than the student. In a
large-scale study of over 400 teacher-student pairs, we show that three
interpretable and computable properties of the teacher's representations (i.e.,
task relevance, behavioral relevance, and complementary knowledge) explain up
to 85% of the variance in transfer success. These insights offer practical
guidance for teacher selection and design. BIRD turns small, well-aligned
models into scalable alignment seeds, removing a key bottleneck in deploying
safe AI systems in the wild.

</details>


### [55] [Searching Neural Architectures for Sensor Nodes on IoT Gateways](https://arxiv.org/abs/2505.23939)
*Andrea Mattia Garavagno,Edoardo Ragusa,Antonio Frisoli,Paolo Gastaldo*

Main category: cs.LG

TL;DR: This paper proposes an automatic method for designing Neural Networks at the edge, enabling Machine Learning access in privacy-sensitive IoT applications without sharing collected data outside the local network. The experimental results on Visual Wake Words dataset show that this method can achieve state-of-the-art results by running a search procedure within 10 hours on Raspberry Pi Zero 2.


<details>
  <summary>Details</summary>
Motivation: To enable Machine Learning access in privacy-sensitive Internet of Things applications while safeguarding sensitive information including industrial secrets and personal data.

Method: The proposed method runs on IoT gateways and designs Neural Networks for connected sensor nodes without sharing the collected data outside the local network, keeping the data in the site of collection.

Result: The outcomes of a thorough experimental session confirm that -- on the Visual Wake Words dataset -- the proposed approach can achieve state-of-the-art results by exploiting a search procedure that runs in less than 10 hours on the Raspberry Pi Zero 2.

Conclusion: This method has the potential to enable ML for Healthcare Internet of Things and Industrial Internet of Things, designing hardware-friendly and custom NNs at the edge for personalized healthcare and advanced industrial services such as quality control, predictive maintenance, or fault diagnosis.

Abstract: This paper presents an automatic method for the design of Neural Networks
(NNs) at the edge, enabling Machine Learning (ML) access even in
privacy-sensitive Internet of Things (IoT) applications. The proposed method
runs on IoT gateways and designs NNs for connected sensor nodes without sharing
the collected data outside the local network, keeping the data in the site of
collection. This approach has the potential to enable ML for Healthcare
Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing
hardware-friendly and custom NNs at the edge for personalized healthcare and
advanced industrial services such as quality control, predictive maintenance,
or fault diagnosis. By preventing data from being disclosed to cloud services,
this method safeguards sensitive information, including industrial secrets and
personal data. The outcomes of a thorough experimental session confirm that --
on the Visual Wake Words dataset -- the proposed approach can achieve
state-of-the-art results by exploiting a search procedure that runs in less
than 10 hours on the Raspberry Pi Zero 2.

</details>


### [56] [Vision Language Models are Biased](https://arxiv.org/abs/2505.23941)
*An Vo,Khai-Nguyen Nguyen,Mohammad Reza Taesiri,Vy Tuong Dang,Anh Totti Nguyen,Daeyoung Kim*

Main category: cs.LG

TL;DR: Large language models (LLMs) can memorize knowledge that may lead to wrong or biased answers. This study examines how such knowledge negatively impacts the accuracy of vision language models (VLMs) in counting and identification tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how prior knowledge affects the performance of VLMs in objective visual tasks.

Method: Testing VLMs on counting and identification tasks across diverse domains, analyzing the impact of inserting subject-related text into images, and evaluating the effect of instructing VLMs to double-check their results or rely on image details.

Result: State-of-the-art VLMs show strong biases, scoring 17.05% accuracy in counting tasks across 7 domains. Inserting subject-related text decreases accuracy further. Instructing VLMs to double-check or focus on image details improves accuracy by only +2 points on average.

Conclusion: The study reveals a significant failure mode in VLMs due to strong biases and presents an automated framework for testing these biases.

Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.

</details>


### [57] [SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations](https://arxiv.org/abs/2505.23942)
*Gaurav Sarkar,Jay Gala,Subarna Tripathi*

Main category: cs.LG

TL;DR: The paper proposes SG-Blend, a novel activation function combining SSwish and GELU through dynamic interpolation. It offers improvements in model expressivity and gradient stability across diverse tasks with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current activation functions like Swish and GELU are effective but often domain-specific. The authors aim to create a more universally robust activation function by blending complementary strengths of different functions.

Method: SG-Blend blends SSwish (a first-order symmetric variant of Swish) and GELU via learnable parameters for dynamic interpolation. This approach leverages SSwish's controlled non-monotonicity and symmetry, and GELU's smooth probabilistic profile.

Result: SG-Blend demonstrates performance improvements across all considered natural language and computer vision tasks and models, with negligible computational overhead.

Conclusion: SG-Blend is presented as a versatile, drop-in replacement for existing activation functions, consistently outperforming strong contemporary baselines.

Abstract: The design of activation functions remains a pivotal component in optimizing
deep neural networks. While prevailing choices like Swish and GELU demonstrate
considerable efficacy, they often exhibit domain-specific optima. This work
introduces SG-Blend, a novel activation function that blends our proposed
SSwish, a first-order symmetric variant of Swish and the established GELU
through dynamic interpolation. By adaptively blending these constituent
functions via learnable parameters, SG-Blend aims to harness their
complementary strengths: SSwish's controlled non-monotonicity and symmetry, and
GELU's smooth, probabilistic profile, to achieve a more universally robust
balance between model expressivity and gradient stability. We conduct
comprehensive empirical evaluations across diverse modalities and
architectures, showing performance improvements across all considered natural
language and computer vision tasks and models. These results, achieved with
negligible computational overhead, underscore SG-Blend's potential as a
versatile, drop-in replacement that consistently outperforms strong
contemporary baselines. The code is available at
https://anonymous.4open.science/r/SGBlend-6CBC.

</details>


### [58] [Position: The Future of Bayesian Prediction Is Prior-Fitted](https://arxiv.org/abs/2505.23947)
*Samuel Müller,Arik Reuter,Noah Hollmann,David Rügamer,Frank Hutter*

Main category: cs.LG

TL;DR: Prior-data Fitted Networks (PFNs) use artificial datasets for training neural networks, leading to Bayesian models capturing the prior of the dataset-generating distribution. With increasing computational resources and limited real-world data in many applications, PFNs can efficiently allocate pre-training compute to low-data scenarios. Originally applied to small Bayesian modeling tasks, PFNs have expanded to more complex domains. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Prior-data Fitted Networks (PFNs) and other amortized inference approaches as the future of Bayesian inference, especially in data-scarce problems.

Method: Training neural networks on randomly generated artificial datasets to create Bayesian models that capture the prior defined by the dataset-generating distribution.

Result: PFN's application has significantly expanded from small Bayesian modeling tasks to more complex domains and larger datasets.

Conclusion: PFNs and other amortized inference approaches are believed to represent the future of Bayesian inference, and are considered a fruitful area of research.

Abstract: Training neural networks on randomly generated artificial datasets yields
Bayesian models that capture the prior defined by the dataset-generating
distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed
to leverage this insight. In an era of rapidly increasing computational
resources for pre-training and a near stagnation in the generation of new
real-world data in many applications, PFNs are poised to play a more important
role across a wide range of applications. They enable the efficient allocation
of pre-training compute to low-data scenarios. Originally applied to small
Bayesian modeling tasks, the field of PFNs has significantly expanded to
address more complex domains and larger datasets. This position paper argues
that PFNs and other amortized inference approaches represent the future of
Bayesian inference, leveraging amortized learning to tackle data-scarce
problems. We thus believe they are a fruitful area of research. In this
position paper, we explore their potential and directions to address their
current limitations.

</details>


### [59] [TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks](https://arxiv.org/abs/2505.23949)
*Xiang Meng,Mehdi Makni,Rahul Mazumder*

Main category: cs.LG

TL;DR: 研究提出了一种高效的可转置N:M稀疏性掩码求解器，能够扩展到十亿参数模型，并通过实验验证了其在LLaMA3.2-8B模型上的性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有的可转置N:M稀疏性掩码方法要么无法扩展到大型模型，要么受限于M=4的情况，导致压缩与精度的权衡不佳。

Method: 将掩码生成问题转化为最优传输问题，并通过熵正则化、Dykstra算法和舍入过程解决；利用基于张量的实现方式利用GPU并行性，加速计算。

Result: 相比现有方法，该方法实现了高达100倍的速度提升，且误差仅为1-10%；在LLaMA3.2-8B模型上，使用可转置16:32稀疏性时性能接近标准N:M模型，优于标准2:4稀疏模型。

Conclusion: 提出的求解器能够有效生成可转置N:M稀疏性掩码，适用于大规模模型，并展示了其在实际应用中的价值。

Abstract: Network pruning reduces the computational requirements of large neural
networks, with N:M sparsity -- retaining only N out of every M consecutive
weights -- offering a compelling balance between compressed model quality and
hardware acceleration. However, N:M sparsity only accelerates forward-pass
computations, as N:M patterns are not preserved during matrix transposition,
limiting efficiency during training where both passes are computationally
intensive. While transposable N:M sparsity has been proposed to address this
limitation, existing methods for finding transposable N:M sparse masks either
fail to scale to large models or are restricted to M=4 which results in
suboptimal compression-accuracy trade-off. We introduce an efficient solver for
transposable N:M masks that scales to billion-parameter models. We formulate
mask generation as optimal transport problems and solve through entropy
regularization and Dykstra's algorithm, followed by a rounding procedure. Our
tensor-based implementation exploits GPU parallelism, achieving up to 100x
speedup with only 1-10% error compared to existing methods. Our approach can be
integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT
and ALPS to produce transposable N:M sparse models with arbitrary N:M values.
Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains
performance close to its standard N:M counterpart and outperforms standard 2:4
sparse model, showing the practical value of our approach.

</details>


### [60] [Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective](https://arxiv.org/abs/2505.23954)
*Dylan Zapzalka,Trenton Chang,Lindsay Warrenburg,Sae-Hwan Park,Daniel K. Shenfeld,Ravi B. Parikh,Jenna Wiens,Maggie Makar*

Main category: cs.LG

TL;DR: This paper proposes a causally-motivated approach to identify and quantify agents' misreporting by distinguishing deceptive changes in their features from genuine modification.


<details>
  <summary>Details</summary>
Motivation: In settings where ML models are used to inform the allocation of resources, agents might have an incentive to strategically change their features for better outcomes. Disentangling misreporting from genuine modification is a fundamental challenge.

Method: The method involves exploiting the asymmetry between misreported features and genuine modification. Misreported features do not causally affect downstream variables (i.e., causal descendants). By comparing the causal effect of misreported features on their causal descendants as derived from manipulated datasets against those from unmanipulated datasets, the approach identifies and quantifies misreporting.

Result: The authors formally prove identifiability of the misreporting rate and characterize the variance of their estimator. Empirical validation using semi-synthetic and real Medicare dataset shows that the approach can be employed to identify misreporting in real-world scenarios.

Conclusion: The proposed causally-motivated approach successfully distinguishes between misreporting and genuine modification, providing a method to quantify misreporting in resource allocation informed by ML models.

Abstract: In settings where ML models are used to inform the allocation of resources,
agents affected by the allocation decisions might have an incentive to
strategically change their features to secure better outcomes. While prior work
has studied strategic responses broadly, disentangling misreporting from
genuine modification remains a fundamental challenge. In this paper, we propose
a causally-motivated approach to identify and quantify how much an agent
misreports on average by distinguishing deceptive changes in their features
from genuine modification. Our key insight is that, unlike genuine
modification, misreported features do not causally affect downstream variables
(i.e., causal descendants). We exploit this asymmetry by comparing the causal
effect of misreported features on their causal descendants as derived from
manipulated datasets against those from unmanipulated datasets. We formally
prove identifiability of the misreporting rate and characterize the variance of
our estimator. We empirically validate our theoretical results using a
semi-synthetic and real Medicare dataset with misreported data, demonstrating
that our approach can be employed to identify misreporting in real-world
scenarios.

</details>


### [61] [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/abs/2505.23960)
*Henry Conklin*

Main category: cs.LG

TL;DR: 尽管大型神经网络取得了显著成功，但我们仍缺乏统一的符号来思考和描述它们的表示空间。本论文引入了定量方法以识别空间映射中的系统结构，并利用这些方法理解深度学习模型如何学习表示信息、哪些表示结构驱动泛化以及设计决策如何影响出现的结构。此外，还提出了一种新的高性能方法来估计向量空间的熵，适用于从1百万到120亿参数的模型。实验揭示了大规模分布式认知模型的学习方式，并探讨了其与人类系统的相似之处。


<details>
  <summary>Details</summary>
Motivation: 我们缺乏可靠的方法来描述神经网络表示空间的结构、该结构如何在训练中出现以及何种结构是理想的。

Method: 识别映射中的结构基元并结合信息论量化分析；引入一种新方法估计向量空间的熵。

Result: 能够分析多智能体强化学习模型、序列到序列模型和大型语言模型的学习、结构和泛化能力；揭示了语言结构及其约束与现代神经网络性能驱动结构的平行性。

Conclusion: 通过定量方法和新提出的熵估计方法，可以更好地理解大型神经网络的表示学习和泛化机制，并找到与人类认知系统的相似之处。

Abstract: Despite the remarkable success of large large-scale neural networks, we still
lack unified notation for thinking about and describing their representational
spaces. We lack methods to reliably describe how their representations are
structured, how that structure emerges over training, and what kinds of
structures are desirable. This thesis introduces quantitative methods for
identifying systematic structure in a mapping between spaces, and leverages
them to understand how deep-learning models learn to represent information,
what representational structures drive generalisation, and how design decisions
condition the structures that emerge. To do this I identify structural
primitives present in a mapping, along with information theoretic
quantifications of each. These allow us to analyse learning, structure, and
generalisation across multi-agent reinforcement learning models,
sequence-to-sequence models trained on a single task, and Large Language
Models. I also introduce a novel, performant, approach to estimating the
entropy of vector space, that allows this analysis to be applied to models
ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models
of cognition learn, while allowing us to draw parallels between those systems
and their human analogs. They show how the structures of language and the
constraints that give rise to them in many ways parallel the kinds of
structures that drive performance of contemporary neural networks.

</details>


### [62] [Improved Approximations for Hard Graph Problems using Predictions](https://arxiv.org/abs/2505.23967)
*Anders Aamand,Justin Y. Chen,Siddharth Gollapudi,Sandeep Silwal,Hao Wu*

Main category: cs.LG

TL;DR: This paper designs improved approximation algorithms for NP-hard graph problems by incorporating predictions, achieving better approximation ratios for several classic problems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to break through the existing approximation barriers in solving NP-hard graph problems by utilizing predictions that can be learned from past data.

Method: The method involves an edge-based prediction model where each edge provides two bits of information about whether its endpoints belong to an optimal solution. This builds upon and extends the $\varepsilon$-prediction framework. Algorithms are developed with a unifying theme of separately handling constraints related to high-degree vertices (using predictions) and low-degree vertices (without using predictions), then carefully combining the results.

Result: The result is the development of algorithms with improved approximation ratios for various NP-hard graph problems such as MaxCut, Vertex Cover, Set Cover, and Maximum Independent Set problems.

Conclusion: By incorporating predictions into algorithm design, the paper successfully achieves better approximation ratios for several classic NP-hard graph problems, demonstrating the potential of using predictions in computational problem-solving.

Abstract: We design improved approximation algorithms for NP-hard graph problems by
incorporating predictions (e.g., learned from past data). Our prediction model
builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad,
d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based
version of this model, where each edge provides two bits of information,
corresponding to predictions about whether each of its endpoints belong to an
optimal solution. Even with weak predictions where each bit is only
$\varepsilon$-correlated with the true solution, this information allows us to
break approximation barriers in the standard setting. We develop algorithms
with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and
Maximum Independent Set problems (among others). Across these problems, our
algorithms share a unifying theme, where we separately satisfy constraints
related to high degree vertices (using predictions) and low-degree vertices
(without using predictions) and carefully combine the answers.

</details>


### [63] [Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training](https://arxiv.org/abs/2505.23971)
*William Merrill,Shane Arora,Dirk Groeneveld,Hannaneh Hajishirzi*

Main category: cs.LG

TL;DR: 在训练大规模语言模型时，确定合适的批量大小很重要。本文提出了一种直接测量关键批量大小（CBS）的方法，并发现CBS在训练初期迅速增加，随后趋于平稳。基于此，文章提出了批量大小预热策略，从小批量开始逐渐增大，以适应CBS的变化。通过这种方法，在OLMo 1B模型上实现了比原训练更好的损失，同时减少了43%的梯度步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的估计关键批量大小（CBS）的方法依赖于梯度噪声规模，但其假设在实践中可能不成立，限制了方法的适用性。因此，需要一种更可靠、直接测量CBS的方法。

Method: 引入一种简单的经验方法直接测量CBS，并观察其在训练过程中的变化趋势。将该方法应用于OLMo模型，研究不同模型尺寸下的CBS变化规律。根据CBS的变化趋势，提出批量大小预热策略：从较小的批量开始，随着CBS的增长逐步增加批量大小。

Result: 发现CBS在初始化时接近0，随后快速增加并趋于平稳。这种趋势在不同模型尺寸（1B和7B）间保持一致。使用批量大小预热策略训练OLMo 1B模型，获得了略优于原训练的损失值，并减少了43%的梯度步骤。

Conclusion: 提出的直接测量CBS的方法揭示了其在训练中的动态变化，支持了批量大小预热策略的有效性。该策略能够可靠地在较大批量下训练语言模型，提高数据并行性而不损害性能。

Abstract: The right batch size is important when training language models at scale: a
large batch size is necessary for fast training, but a batch size that is too
large will harm token efficiency. To navigate this tradeoff, McCandlish et al.
(2018) suggest that a critical batch size (CBS), below which training will not
substantially degrade loss, can be estimated based on the gradient noise scale
during training. While their method has been adopted in practice, e.g., when
training GPT-3, strong assumptions are required to justify gradient noise as a
proxy for the CBS, which makes it unclear whether their approach should be
trusted in practice, limiting its applicability. In this paper, we introduce a
simple, empirical approach to directly measure the CBS and show how the CBS
evolves over training. Applying our approach to the OLMo models, we find that
CBS is near 0 at initialization, increases rapidly at first, and then plateaus
as training progresses. Furthermore, we find that this trend holds across
different model sizes (1B and 7B), suggesting CBS from small training runs can
inform larger-scale training runs. Our findings about how the CBS changes over
training motivate batch size warmup as a natural way to reliably train language
models at large batch size: start the batch size small and increase it as the
CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to
slightly better loss than the original training run with 43% fewer gradient
steps. This shows how our framework can be applied to reliably train language
models at larger batch sizes, increasing data parallelism without compromising
performance.

</details>


### [64] [Adaptive Deadline and Batch Layered Synchronized Federated Learning](https://arxiv.org/abs/2505.23973)
*Asaf Goren,Natalie Lang,Nir Shlezinger,Alejandro Cohen*

Main category: cs.LG

TL;DR: The paper proposes ADEL-FL, a framework that optimizes deadlines and batch sizes for federated learning to reduce the impact of slow devices.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for handling stragglers in synchronous federated learning treat round timing and local workload as static parameters, limiting their effectiveness under strict time constraints.

Method: ADEL-FL jointly optimizes per-round deadlines and user-specific batch sizes for layer-wise aggregation by formulating a constrained optimization problem minimizing the expected L2 distance to the global optimum under total training time and global rounds.

Result: Experiments demonstrate that ADEL-FL outperforms alternative methods in both convergence rate and final accuracy under heterogeneous conditions.

Conclusion: ADEL-FL provides a solution to optimize federated learning processes considering device heterogeneity and strict time constraints.

Abstract: Federated learning (FL) enables collaborative model training across
distributed edge devices while preserving data privacy, and typically operates
in a round-based synchronous manner. However, synchronous FL suffers from
latency bottlenecks due to device heterogeneity, where slower clients
(stragglers) delay or degrade global updates. Prior solutions, such as fixed
deadlines, client selection, and layer-wise partial aggregation, alleviate the
effect of stragglers, but treat round timing and local workload as static
parameters, limiting their effectiveness under strict time constraints. We
propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines
and user-specific batch sizes for layer-wise aggregation. Our approach
formulates a constrained optimization problem minimizing the expected L2
distance to the global optimum under total training time and global rounds. We
provide a convergence analysis under exponential compute models and prove that
ADEL-FL yields unbiased updates with bounded variance. Extensive experiments
demonstrate that ADEL-FL outperforms alternative methods in both convergence
rate and final accuracy under heterogeneous conditions.

</details>


### [65] [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/abs/2505.23987)
*Vishal Dey,Xiao Hu,Xia Ning*

Main category: cs.LG

TL;DR: In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. To address this, we introduce C-MuMOInstruct and develop GeLLMO-Cs which can perform targeted property-specific optimization.


<details>
  <summary>Details</summary>
Motivation: Existing computational approaches and instruction-tuned LLMs fail to capture nuanced property-specific objectives in molecule optimization, limiting their practical applicability.

Method: Introduced C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, developed GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization.

Result: Experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions.

Conclusion: C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.

Abstract: In real-world drug design, molecule optimization requires selectively
improving multiple molecular properties up to pharmaceutically relevant levels,
while maintaining others that already meet such criteria. However, existing
computational approaches and instruction-tuned LLMs fail to capture such
nuanced property-specific objectives, limiting their practical applicability.
To address this, we introduce C-MuMOInstruct, the first instruction-tuning
dataset focused on multi-property optimization with explicit, property-specific
objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of
instruction-tuned LLMs that can perform targeted property-specific
optimization. Our experiments across 5 in-distribution and 5
out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong
baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit
impressive 0-shot generalization to novel optimization tasks and unseen
instructions. This offers a step toward a foundational LLM to support
realistic, diverse optimizations with property-specific objectives.
C-MuMOInstruct and code are accessible through
https://github.com/ninglab/GeLLMO-C.

</details>


### [66] [Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting](https://arxiv.org/abs/2505.24003)
*ChengAo Shen,Wenchao Yu,Ziming Zhao,Dongjin Song,Wei Cheng,Haifeng Chen,Jingchao Ni*

Main category: cs.LG

TL;DR: A new framework called DMMV is proposed, which uses multi-modal views from time series data for long-term forecasting and performs better than existing models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of long-term time series forecasting by utilizing the power of pre-trained large vision models through transforming time series into multi-modal views.

Method: DMMV, a decomposition-based multi-modal view framework that integrates MMVs using trend-seasonal decomposition and a backcast residual based adaptive decomposition.

Result: DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error on 6 out of 8 benchmark datasets.

Conclusion: The proposed DMMV framework effectively leverages multi-modal views for long-term time series forecasting and shows superior performance compared to state-of-the-art models.

Abstract: Time series, typically represented as numerical sequences, can also be
transformed into images and texts, offering multi-modal views (MMVs) of the
same underlying signal. These MMVs can reveal complementary patterns and enable
the use of powerful pre-trained large models, such as large vision models
(LVMs), for long-term time series forecasting (LTSF). However, as we identified
in this work, applying LVMs to LTSF poses an inductive bias towards
"forecasting periods". To harness this bias, we propose DMMV, a novel
decomposition-based multi-modal view framework that leverages trend-seasonal
decomposition and a novel backcast residual based adaptive decomposition to
integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art
(SOTA) models across diverse datasets show that DMMV outperforms single-view
and existing multi-modal baselines, achieving the best mean squared error (MSE)
on 6 out of 8 benchmark datasets.

</details>


### [67] [How far away are truly hyperparameter-free learning algorithms?](https://arxiv.org/abs/2505.24005)
*Priya Kasimbeg,Vincent Roulet,Naman Agarwal,Sourabh Medapati,Fabian Pedregosa,Atish Agarwala,George E. Dahl*

Main category: cs.LG

TL;DR: 尽管在优化方法上取得了重大进展，但超参数调整仍然是机器学习系统开发中至关重要且昂贵的一部分。本文评估了无学习率方法作为无超参数方法的潜力，并通过AlgoPerf基准测试其性能。结果表明，这些方法仍有很大的改进空间，同时强调了使用强大的、与工作负载无关的基线进行测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 即使忽略架构选择，深度神经网络仍需要针对每个任务精心调整大量优化和正则化超参数。理想情况下，训练算法应无需特定任务的超参数调整，而具有适用于多种任务的默认设置。因此，探索减少超参数需求（特别是学习率）的方法变得重要。

Method: 作者评估了无学习率方法作为无超参数方法组件的潜力。他们将非学习率超参数固定为默认值，并使用AlgoPerf基准测试其性能。由于文献提供的默认设置表现不佳，作者进一步搜索了在所有任务上表现良好的超参数配置。

Result: 最佳的、经过AlgoPerf校准的无学习率方法性能有所提高，但在整体基准分数上仍略逊于同样校准的NadamW基线。这表明无学习率方法仍有很大改进空间。

Conclusion: 无学习率方法虽然有所进步，但仍需进一步改进以达到与强基线相当的性能。此外，测试时应使用强大的、与工作负载无关的基线来改进超参数减少技术。

Abstract: Despite major advances in methodology, hyperparameter tuning remains a
crucial (and expensive) part of the development of machine learning systems.
Even ignoring architectural choices, deep neural networks have a large number
of optimization and regularization hyperparameters that need to be tuned
carefully per workload in order to obtain the best results. In a perfect world,
training algorithms would not require workload-specific hyperparameter tuning,
but would instead have default settings that performed well across many
workloads. Recently, there has been a growing literature on optimization
methods which attempt to reduce the number of hyperparameters -- particularly
the learning rate and its accompanying schedule. Given these developments, how
far away is the dream of neural network training algorithms that completely
obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as
components of hyperparameter-free methods. We freeze their (non-learning rate)
hyperparameters to default values, and score their performance using the
recently-proposed AlgoPerf: Training Algorithms benchmark. We found that
literature-supplied default settings performed poorly on the benchmark, so we
performed a search for hyperparameter configurations that performed well across
all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free
methods had much improved performance but still lagged slightly behind a
similarly calibrated NadamW baseline in overall benchmark score. Our results
suggest that there is still much room for improvement for learning-rate-free
methods, and that testing against a strong, workload-agnostic baseline is
important to improve hyperparameter reduction techniques.

</details>


### [68] [The Rich and the Simple: On the Implicit Bias of Adam and SGD](https://arxiv.org/abs/2505.24022)
*Bhavya Vasudeva,Jung Whan Lee,Vatsal Sharan,Mahdi Soltanolkotabi*

Main category: cs.LG

TL;DR: Adam优化算法在深度学习应用中占据重要地位，但对其隐式偏差及其与随机梯度下降（SGD）等一阶方法差异的理解仍然有限。本文通过研究两层ReLU神经网络在合成高斯聚类数据上的二分类任务，揭示了GD倾向于简单的线性决策边界，而Adam则产生更丰富、更多样的非线性特征和决策边界。这使得Adam在测试精度和泛化能力上表现更优。理论分析和实验结果均支持这一发现。


<details>
  <summary>Details</summary>
Motivation: 尽管Adam在深度学习中广泛使用，但其隐式偏差及与SGD的差异尚未被充分理解。研究者希望探究Adam为何在实践中表现出不同于SGD的行为，特别是在特征学习和决策边界形成方面的差异。

Method: 作者通过训练两层ReLU神经网络完成二分类任务，比较Adam和GD在合成高斯聚类数据上的行为。利用种群梯度分析，理论证明Adam生成更丰富的特征和非线性决策边界，而GD倾向于简单线性边界。同时，通过实验证明Adam在带虚假相关性的数据集上具有更好的泛化性能。

Result: 理论分析表明，Adam能够生成更复杂和多样的特征，从而形成接近贝叶斯最优预测器的非线性决策边界。相比之下，GD表现出简单偏差，导致次优的线性边界。实验结果也验证了Adam在不同数据分布下的优越泛化性能。

Conclusion: Adam相比SGD展现出更强的抗简单偏差能力，能生成更丰富的特征和更复杂的决策边界，从而在某些数据分布下实现更高的测试准确率和更好的泛化性能。

Abstract: Adam is the de facto optimization algorithm for several deep learning
applications, but an understanding of its implicit bias and how it differs from
other algorithms, particularly standard first-order methods such as
(stochastic) gradient descent (GD), remains limited. In practice, neural
networks trained with SGD are known to exhibit simplicity bias -- a tendency to
find simple solutions. In contrast, we show that Adam is more resistant to such
simplicity bias. To demystify this phenomenon, in this paper, we investigate
the differences in the implicit biases of Adam and GD when training two-layer
ReLU neural networks on a binary classification task involving synthetic data
with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting
in a linear decision boundary with a suboptimal margin, whereas Adam leads to
much richer and more diverse features, producing a nonlinear boundary that is
closer to the Bayes' optimal predictor. This richer decision boundary also
allows Adam to achieve higher test accuracy both in-distribution and under
certain distribution shifts. We theoretically prove these results by analyzing
the population gradients. To corroborate our theoretical findings, we present
empirical results showing that this property of Adam leads to superior
generalization across datasets with spurious correlations where neural networks
trained with SGD are known to show simplicity bias and don't generalize well
under certain distributional shifts.

</details>


### [69] [From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?](https://arxiv.org/abs/2505.24030)
*Ziming Zhao,ChengAo Shen,Hanghang Tong,Dongjin Song,Zhigang Deng,Qingsong Wen,Jingchao Ni*

Main category: cs.LG

TL;DR: The first principled study on the effectiveness of LVMs in time series analysis, testing 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines. LVMs are useful for classification but challenged in forecasting.


<details>
  <summary>Details</summary>
Motivation: To explore whether Large Vision Models (LVMs) are truly useful for time series analysis as Transformer-based models and LLMs have gained increasing attention in this field.

Method: Designed and conducted a study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks with extensive ablation analysis.

Result: LVMs are indeed useful for time series classification but face challenges in forecasting. The best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows.

Conclusion: This study serves as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.

Abstract: Transformer-based models have gained increasing attention in time series
research, driving interest in Large Language Models (LLMs) and foundation
models for time series analysis. As the field moves toward multi-modality,
Large Vision Models (LVMs) are emerging as a promising direction. In the past,
the effectiveness of Transformer and LLMs in time series has been debated. When
it comes to LVMs, a similar question arises: are LVMs truely useful for time
series analysis? To address it, we design and conduct the first principled
study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across
both high-level (classification) and low-level (forecasting) tasks, with
extensive ablation analysis. Our findings indicate LVMs are indeed useful for
time series classification but face challenges in forecasting. Although
effective, the contemporary best LVM forecasters are limited to specific types
of LVMs and imaging methods, exhibit a bias toward forecasting periods, and
have limited ability to utilize long look-back windows. We hope our findings
could serve as a cornerstone for future research on LVM- and multimodal-based
solutions to different time series tasks.

</details>


### [70] [LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin](https://arxiv.org/abs/2505.24034)
*Bo Wu,Sid Wang,Yunhao Tang,Jia Ding,Eryk Helenowski,Liang Tan,Tengyu Xu,Tushar Gowda,Zhengxing Chen,Chen Zhu,Xiaocheng Tang,Yundi Qian,Beibei Zhu,Rui Hou*

Main category: cs.LG

TL;DR: The paper introduces LlamaRL, an efficient, distributed RL framework for training large-scale LLMs. It achieves significant speed-ups over existing systems and scales well with model size.


<details>
  <summary>Details</summary>
Motivation: There is a need for an efficient reinforcement learning framework to handle large language models with billions of parameters due to the high demands on latency and memory.

Method: LlamaRL is a fully distributed, asynchronous RL framework built on native PyTorch. It features a single-controller architecture, modularity, ease of use, and scalability to thousands of GPUs. The framework employs techniques such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization.

Result: LlamaRL achieves up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. The efficiency advantage increases with larger model sizes.

Conclusion: LlamaRL is suitable for future large-scale RL training due to its significant efficiency gains and scalability.

Abstract: Reinforcement Learning (RL) has become the most effective post-training
approach for improving the capabilities of Large Language Models (LLMs). In
practice, because of the high demands on latency and memory, it is particularly
challenging to develop an efficient RL framework that reliably manages policy
models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL
framework optimized for efficient training of large-scale LLMs with various
model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a
handful to thousands of devices. LlamaRL introduces a streamlined,
single-controller architecture built entirely on native PyTorch, enabling
modularity, ease of use, and seamless scalability to thousands of GPUs. We also
provide a theoretical analysis of LlamaRL's efficiency, including a formal
proof that its asynchronous design leads to strict RL speed-up. Empirically, by
leveraging best practices such as colocated model offloading, asynchronous
off-policy training, and distributed direct memory access for weight
synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x
speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy
model. Furthermore, the efficiency advantage continues to grow with increasing
model scale, demonstrating the framework's suitability for future large-scale
RL training.

</details>


### [71] [NeuronTune: Towards Self-Guided Spurious Bias Mitigation](https://arxiv.org/abs/2505.24048)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Main category: cs.LG

TL;DR: Deep neural networks can develop spurious bias, relying on non-essential features for predictions. Existing mitigation approaches need external annotations which may be hard to obtain. This paper proposes NeuronTune, a post hoc method that intervenes in the model's internal decision process to mitigate spurious bias without requiring these annotations.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often develop spurious bias, leading to degraded performance when correlations between non-essential features and classes are absent. Current mitigation methods rely on external annotations of spurious correlations, which are difficult to obtain and not always relevant.

Method: NeuronTune is proposed as a post hoc method that directly intervenes in a model's internal decision process by probing the latent embedding space to identify and regulate neurons responsible for spurious prediction behaviors.

Result: Experiments across different architectures and data modalities demonstrate that NeuronTune significantly mitigates spurious bias in a self-guided way.

Conclusion: NeuronTune is a practical and effective tool for improving model robustness as it operates without requiring spurious correlation annotations.

Abstract: Deep neural networks often develop spurious bias, reliance on correlations
between non-essential features and classes for predictions. For example, a
model may identify objects based on frequently co-occurring backgrounds rather
than intrinsic features, resulting in degraded performance on data lacking
these correlations. Existing mitigation approaches typically depend on external
annotations of spurious correlations, which may be difficult to obtain and are
not relevant to the spurious bias in a model. In this paper, we take a step
towards self-guided mitigation of spurious bias by proposing NeuronTune, a post
hoc method that directly intervenes in a model's internal decision process. Our
method probes in a model's latent embedding space to identify and regulate
neurons that lead to spurious prediction behaviors. We theoretically justify
our approach and show that it brings the model closer to an unbiased one.
Unlike previous methods, NeuronTune operates without requiring spurious
correlation annotations, making it a practical and effective tool for improving
model robustness. Experiments across different architectures and data
modalities demonstrate that our method significantly mitigates spurious bias in
a self-guided way.

</details>


### [72] [Differential Gated Self-Attention](https://arxiv.org/abs/2505.24054)
*Elpiniki Maria Lygizou,Mónika Farsang,Radu Grosu*

Main category: cs.LG

TL;DR: Transformers are great but weak to corrupted inputs. This paper proposes Multihead Differential Gated Self-Attention (M-DGSA) which uses a gating mechanism derived from biological concepts to suppress attention noise. It shows robustness gains in vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers, despite their success, struggle with corrupted inputs due to uniform treatment of query-key interactions in self-attention.

Method: Proposes M-DGSA that incorporates per-head input-dependent gating using excitatory and inhibitory branches and a sigmoid gate predicted from token embeddings for context-aware contrast enhancement.

Result: Demonstrates consistent robustness improvements over standard Transformers, Vision Transformers, and Differential Transformers in both vision and language benchmarks.

Conclusion: Contributes a new gating mechanism for self-attention inspired by biology, integrates theory from biology and self-attention, and provides extensive experiments showing effectiveness across domains.

Abstract: Transformers excel across a large variety of tasks but remain susceptible to
corrupted inputs, since standard self-attention treats all query-key
interactions uniformly. Inspired by lateral inhibition in biological neural
circuits and building on the recent use by the Differential Transformer's use
of two parallel softmax subtraction for noise cancellation, we propose
Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head
input-dependent gating to dynamically suppress attention noise. Each head
splits into excitatory and inhibitory branches whose dual softmax maps are
fused by a sigmoid gate predicted from the token embedding, yielding a
context-aware contrast enhancement. M-DGSA integrates seamlessly into existing
Transformer stacks with minimal computational overhead. We evaluate on both
vision and language benchmarks, demonstrating consistent robustness gains over
vanilla Transformer, Vision Transformer, and Differential Transformer
baselines. Our contributions are (i) a novel input-dependent gating mechanism
for self-attention grounded in lateral inhibition, (ii) a principled synthesis
of biological contrast-enhancement and self-attention theory, and (iii)
comprehensive experiments demonstrating noise resilience and cross-domain
applicability.

</details>


### [73] [Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs](https://arxiv.org/abs/2505.24055)
*Yilong Wang,Tianxiang Zhao,Zongyu Wu,Suhang Wang*

Main category: cs.LG

TL;DR: This paper proposes a novel framework for unsupervised domain adaptation in graph neural networks (GNNs) that uses link prediction to connect nodes between source and target graphs, facilitating message-passing and reducing distribution shift. It also introduces an identity-preserving learning objective to maintain discriminative information in the target graph.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised domain adaptation methods for GNNs do not fully consider the graph structure and message-passing mechanism during adaptation, leading to failures when label distribution shifts exist among domains.

Method: The method involves using link prediction to connect nodes between the source and target graphs, thereby facilitating message-passing and augmenting target nodes with 'in-distribution' neighborhoods from the source domain. An identity-preserving learning objective is designed to prevent loss of discriminative information in the target graph.

Result: Experimental results on real-world datasets demonstrate the effectiveness of the proposed framework.

Conclusion: The proposed framework successfully reduces the deviation of the target graph from the source domain in the embedding space and is insensitive to disproportional label distributions across domains.

Abstract: Graph neural networks (GNNs) have shown great ability for node classification
on graphs. However, the success of GNNs relies on abundant labeled data, while
obtaining high-quality labels is costly and challenging, especially for newly
emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a
classifier on the labeled source graph and adapts it to the unlabeled target
graph, is attracting increasing attention. Various approaches have been
proposed to alleviate the distribution shift between the source and target
graphs to facilitate the classifier adaptation. However, most of them simply
adopt existing UDA techniques developed for independent and identically
distributed data to gain domain-invariant node embeddings for graphs, which do
not fully consider the graph structure and message-passing mechanism of GNNs
during the adaptation and will fail when label distribution shift exists among
domains. In this paper, we proposed a novel framework that adopts link
prediction to connect nodes between source and target graphs, which can
facilitate message-passing between the source and target graphs and augment the
target nodes to have ``in-distribution'' neighborhoods with the source domain.
This strategy modified the target graph on the input level to reduce its
deviation from the source domain in the embedding space and is insensitive to
disproportional label distributions across domains. To prevent the loss of
discriminative information in the target graph, we further design a novel
identity-preserving learning objective, which guides the learning of the edge
insertion module together with reconstruction and adaptation losses.
Experimental results on real-world datasets demonstrate the effectiveness of
our framework.

</details>


### [74] [Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition](https://arxiv.org/abs/2505.24059)
*Sean Foley,Hong Nguyen,Jihwan Lee,Sudarsana Reddy Kadiri,Dani Byrd,Louis Goldstein,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: This paper explores phoneme recognition using unimodal and multimodal models on a single-speaker MRI corpus, revealing similar performance in some areas but differences in others.


<details>
  <summary>Details</summary>
Motivation: Previous studies of multimodal learning with real-time MRI data have been constrained by multi-speaker corpora which limits detailed relationship learning between acoustics and articulation due to cross-speaker variability.

Method: Developed unimodal audio and video models along with multimodal models for phoneme recognition using a long-form single-speaker MRI corpus.

Result: Audio and multimodal models perform similarly on different phonetic manner classes but differ on places of articulation. Latent space interpretation shows similar encoding of phonetic space across models, while attention weights highlight timing differences for certain phonemes.

Conclusion: Using a single-speaker MRI corpus helps disentangle contributions of each modality in phoneme recognition, showing both similarities and differences in performance and model interpretations.

Abstract: Although many previous studies have carried out multimodal learning with
real-time MRI data that captures the audio-visual kinematics of the vocal tract
during speech, these studies have been limited by their reliance on
multi-speaker corpora. This prevents such models from learning a detailed
relationship between acoustics and articulation due to considerable
cross-speaker variability. In this study, we develop unimodal audio and video
models as well as multimodal models for phoneme recognition using a long-form
single-speaker MRI corpus, with the goal of disentangling and interpreting the
contributions of each modality. Audio and multimodal models show similar
performance on different phonetic manner classes but diverge on places of
articulation. Interpretation of the models' latent space shows similar encoding
of the phonetic space across audio and multimodal models, while the models'
attention weights highlight differences in acoustic and articulatory timing for
certain phonemes.

</details>


### [75] [Characterising the Inductive Biases of Neural Networks on Boolean Data](https://arxiv.org/abs/2505.24060)
*Chris Mingard,Lukas Seier,Niclas Göring,Andrei-Vlad Badelita,Charles London,Ard Louis*

Main category: cs.LG

TL;DR: 通过研究深度为2的全连接网络与布尔函数的关系，揭示了神经网络归纳偏置、特征学习和泛化能力之间的联系。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的工作（如基于NTK的任务模型对齐解释）部分解释了深度神经网络的良好泛化能力，但忽略了特征学习的作用。本文旨在提供一个端到端的解析案例，综合分析网络的归纳先验、训练动态（包括特征学习）以及最终的泛化性能。

Method: 利用深度为2的离散全连接网络与析取范式（DNF）公式之间的一一对应关系，通过在布尔函数上进行训练，采用蒙特卡洛学习算法，观察模型的可预测训练动态及可解释特征的出现。

Result: 该框架能够详细追踪归纳偏差和特征形成如何驱动泛化过程，揭示了神经网络在特征学习和泛化能力方面的作用机制。

Conclusion: 本研究表明，通过分析简单的网络结构和训练方法，可以深入理解神经网络的归纳偏置、特征学习和泛化能力之间的关系，为进一步研究复杂网络提供了理论基础。

Abstract: Deep neural networks are renowned for their ability to generalise well across
diverse tasks, even when heavily overparameterized. Existing works offer only
partial explanations (for example, the NTK-based task-model alignment
explanation neglects feature learning). Here, we provide an end-to-end,
analytically tractable case study that links a network's inductive prior, its
training dynamics including feature learning, and its eventual generalisation.
Specifically, we exploit the one-to-one correspondence between depth-2 discrete
fully connected networks and disjunctive normal form (DNF) formulas by training
on Boolean functions. Under a Monte Carlo learning algorithm, our model
exhibits predictable training dynamics and the emergence of interpretable
features. This framework allows us to trace, in detail, how inductive bias and
feature formation drive generalisation.

</details>


### [76] [Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning](https://arxiv.org/abs/2505.24061)
*Jiashun Liu,Zihao Wu,Johan Obando-Ceron,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: In deep reinforcement learning, neuronal activity loss affects continual learning. While tau-dormant neuron ratio works for simple MLP-based agents, it lacks statistical power in complex architectures. This paper proposes GraMa (Gradient Magnitude Neural Activity Metric) to quantify neuron-level learning capacity and ReGraMa to improve learning performance.


<details>
  <summary>Details</summary>
Motivation: Deep RL agents often suffer from neuronal activity loss, which hinders their ability to adapt and learn continually. Existing methods like the tau-dormant neuron ratio are effective for simple architectures but lack power in more complex ones.

Method: The authors shift the focus from activations to gradients, introducing GraMa as a lightweight metric to measure neuron-level learning capacity across diverse architectures. They also propose ReGraMa, a method that resets neurons based on GraMa's guidance.

Result: GraMa successfully reveals persistent neuron inactivity in various architectures such as residual networks and diffusion models. ReGraMa improves learning performance across multiple deep RL algorithms and benchmarks including MuJoCo and the DeepMind Control Suite.

Conclusion: GraMa and ReGraMa offer new tools to address neuronal activity loss in complex deep RL architectures, improving the adaptability and learning capacity of advanced RL agents.

Abstract: Deep reinforcement learning (RL) agents frequently suffer from neuronal
activity loss, which impairs their ability to adapt to new data and learn
continually. A common method to quantify and address this issue is the
tau-dormant neuron ratio, which uses activation statistics to measure the
expressive ability of neurons. While effective for simple MLP-based agents,
this approach loses statistical power in more complex architectures. To address
this, we argue that in advanced RL agents, maintaining a neuron's learning
capacity, its ability to adapt via gradient updates, is more critical than
preserving its expressive ability. Based on this insight, we shift the
statistical objective from activations to gradients, and introduce GraMa
(Gradient Magnitude Neural Activity Metric), a lightweight,
architecture-agnostic metric for quantifying neuron-level learning capacity. We
show that GraMa effectively reveals persistent neuron inactivity across diverse
architectures, including residual networks, diffusion models, and agents with
varied activation functions. Moreover, resetting neurons guided by GraMa
(ReGraMa) consistently improves learning performance across multiple deep RL
algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.

</details>


### [77] [Practical Bayes-Optimal Membership Inference Attacks](https://arxiv.org/abs/2505.24089)
*Marcus Lassila,Johan Östman,Khac-Hoang Ngo,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: The paper develops practical and theoretically grounded membership inference attacks (MIAs) for both i.i.d. data and graph-structured data, introduces BASE and G-BASE methods which are computationally efficient approximations of the Bayes-optimal attack, and shows that BASE matches or exceeds previous MIAs' performance at lower cost.


<details>
  <summary>Details</summary>
Motivation: To create practical and theoretically grounded membership inference attacks (MIAs) against both independent and identically distributed (i.i.d.) data and graph-structured data, addressing open questions about optimal query strategies in the graph setting.

Method: Derive the Bayes-optimal membership inference rule for node-level MIAs against graph neural networks based on the Bayesian decision-theoretic framework. Introduce BASE and G-BASE as computationally efficient approximations of the Bayes-optimal attack.

Result: G-BASE outperforms previously proposed classifier-based node-level MIA attacks. BASE matches or exceeds the performance of prior state-of-the-art MIAs like LiRA and RMIA at a significantly lower computational cost. BASE and RMIA are equivalent under a specific hyperparameter setting.

Conclusion: BASE and G-BASE provide efficient approximations of the Bayes-optimal attack for MIAs. BASE is effective for non-graph data while G-BASE is superior for graph-structured data.

Abstract: We develop practical and theoretically grounded membership inference attacks
(MIAs) against both independent and identically distributed (i.i.d.) data and
graph-structured data. Building on the Bayesian decision-theoretic framework of
Sablayrolles et al., we derive the Bayes-optimal membership inference rule for
node-level MIAs against graph neural networks, addressing key open questions
about optimal query strategies in the graph setting. We introduce BASE and
G-BASE, computationally efficient approximations of the Bayes-optimal attack.
G-BASE achieves superior performance compared to previously proposed
classifier-based node-level MIA attacks. BASE, which is also applicable to
non-graph data, matches or exceeds the performance of prior state-of-the-art
MIAs, such as LiRA and RMIA, at a significantly lower computational cost.
Finally, we show that BASE and RMIA are equivalent under a specific
hyperparameter setting, providing a principled, Bayes-optimal justification for
the RMIA attack.

</details>


### [78] [Primal-Dual Neural Algorithmic Reasoning](https://arxiv.org/abs/2505.24067)
*Yu He,Ellen Vitercik*

Main category: cs.LG

TL;DR: The paper presents a new NAR framework based on the primal-dual paradigm, which enhances neural networks' ability to approximate algorithmic reasoning for complex problems.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing NAR approaches focusing on polynomial-time solvable problems, this work aims to extend NAR to harder computational problems by leveraging the primal-dual paradigm.

Method: A general NAR framework is developed using the primal-dual paradigm. This includes creating a bipartite representation between primal and dual variables to align with Graph Neural Networks, and incorporating optimal solutions from small instances to improve reasoning capabilities.

Result: The model not only simulates but also outperforms approximation algorithms in various tasks, showing robust generalization to larger and out-of-distribution graphs. It also proves practically useful when integrated with commercial solvers and applied to real-world datasets.

Conclusion: This NAR framework grounded in the primal-dual paradigm successfully extends algorithmic reasoning to more complex problems, offering both theoretical and practical advancements.

Abstract: Neural Algorithmic Reasoning (NAR) trains neural networks to simulate
classical algorithms, enabling structured and interpretable reasoning over
complex data. While prior research has predominantly focused on learning exact
algorithms for polynomial-time-solvable problems, extending NAR to harder
problems remains an open challenge. In this work, we introduce a general NAR
framework grounded in the primal-dual paradigm, a classical method for
designing efficient approximation algorithms. By leveraging a bipartite
representation between primal and dual variables, we establish an alignment
between primal-dual algorithms and Graph Neural Networks. Furthermore, we
incorporate optimal solutions from small instances to greatly enhance the
model's reasoning capabilities. Our empirical results demonstrate that our
model not only simulates but also outperforms approximation algorithms for
multiple tasks, exhibiting robust generalization to larger and
out-of-distribution graphs. Moreover, we highlight the framework's practical
utility by integrating it with commercial solvers and applying it to real-world
datasets.

</details>


### [79] [DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures](https://arxiv.org/abs/2505.24069)
*Yu He,Yingxi Li,Colin White,Ellen Vitercik*

Main category: cs.LG

TL;DR: 大型语言模型在数据操作相关任务中需要结构化推理能力，但现有基准测试未充分评估此能力。本文提出DSR-Bench基准，用于评估LLM的结构化推理能力，发现现有模型在多属性和多跳推理方面表现不佳，在复杂结构和自然语言任务描述方面存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛应用于涉及数据操作的任务中，这些任务需要模型具备结构化推理能力以理解数据关系。然而，当前的基准测试主要关注应用层面的表现，未能单独评估这种基础能力。

Method: 引入了DSR-Bench这一新基准，包含20种数据结构、35种操作和4,140个问题实例，通过分层组织的方式对LLM的结构化推理能力进行细粒度分析。该基准完全自动化且确定性，避免主观判断，并通过合成数据确保可扩展性和减少污染风险。

Result: 评估了九个最先进的LLM，结果显示指令调优模型在多属性和多跳推理上表现不佳，而专注于推理的模型虽然表现较好，但在复杂和混合结构上的表现仍然脆弱，最佳模型在挑战子集上的平均得分仅为47%。模型在多维数据和自然语言任务描述方面的表现较差。

Conclusion: 现有LLM在结构化推理能力方面存在显著差距，特别是在处理复杂数据结构和自然语言描述时，这对实际部署构成了重要挑战。

Abstract: Large language models (LLMs) are increasingly deployed for real-world tasks
that fundamentally involve data manipulation. A core requirement across these
tasks is the ability to perform structural reasoning--that is, to understand
and reason about data relationships. For example, customer requests require a
temporal ordering, which can be represented by data structures such as queues.
However, existing benchmarks primarily focus on high-level, application-driven
evaluations without isolating this fundamental capability. To address this gap,
we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning
capabilities through data structures, which provide interpretable
representations of data relationships. DSR-Bench includes 20 data structures,
35 operations, and 4,140 problem instances, organized hierarchically for
fine-grained analysis of reasoning limitations. Our evaluation pipeline is
fully automated and deterministic, eliminating subjective human or model-based
judgments. Its synthetic nature also ensures scalability and minimizes data
contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis
shows that instruction-tuned models struggle with basic multi-attribute and
multi-hop reasoning. Furthermore, while reasoning-oriented models perform
better, they remain fragile on complex and hybrid structures, with the best
model achieving an average score of only 47% on the challenge subset.
Crucially, models often perform poorly on multi-dimensional data and natural
language task descriptions, highlighting a critical gap for real-world
deployment.

</details>


### [80] [Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion](https://arxiv.org/abs/2505.24231)
*Md Shahnawaz,Bishwajit Prasad Gond,Durga Prasad Mohapatra*

Main category: cs.LG

TL;DR: The paper proposes a dynamic malware categorization framework that converts API behavioral data into grayscale images and uses CNN for classification.


<details>
  <summary>Details</summary>
Motivation: Malware detection and classification is crucial in cybersecurity due to attackers using advanced obfuscation techniques. Conventional static analysis is ineffective against polymorphic and metamorphic malware, necessitating dynamic detection methods.

Method: The method involves extracting API argument calls during the runtime execution of Windows PE files, encoding these dynamic features, converting them into temporal patterns, transforming these patterns into grayscale images, and then training a CNN model on these images for classification.

Result: Experiments show an average accuracy of 98.36% in classifying different types of malware and benign software, demonstrating high accuracy and resilience against typical evasion strategies.

Conclusion: The proposed framework effectively integrates dynamic analysis and deep learning for reliable and accurate malware classification.

Abstract: Malware detection and classification remains a topic of concern for
cybersecurity, since it is becoming common for attackers to use advanced
obfuscation on their malware to stay undetected. Conventional static analysis
is not effective against polymorphic and metamorphic malware as these change
their appearance without modifying their behavior, thus defying the analysis by
code structure alone. This makes it important to use dynamic detection that
monitors malware behavior at runtime. In this paper, we present a dynamic
malware categorization framework that extracts API argument calls at the
runtime execution of Windows Portable Executable (PE) files. Extracting and
encoding the dynamic features of API names, argument return values, and other
relative features, we convert raw behavioral data to temporal patterns. To
enhance feature portrayal, the generated patterns are subsequently converted
into grayscale pictures using a magma colormap. These improved photos are used
to teach a Convolutional Neural Network (CNN) model discriminative features,
which allows for reliable and accurate malware classification. Results from
experiments indicate that our method, with an average accuracy of 98.36% is
effective in classifying different classes of malware and benign by integrating
dynamic analysis and deep learning. It not only achieves high classification
accuracy but also demonstrates significant resilience against typical evasion
strategies.

</details>


### [81] [DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals](https://arxiv.org/abs/2505.24085)
*Alireza Jafari,Fereshteh Yousefirizi,Vahid Seydi*

Main category: cs.LG

TL;DR: Atrial fibrillation (AF) is a common heart arrhythmia associated with increased health risks. This study proposes a hybrid methodology integrating unsupervised deep learning and gradient boosting models to improve AF detection.


<details>
  <summary>Details</summary>
Motivation: Timely detection of atrial fibrillation (AF) is crucial for reducing stroke-related morbidity.

Method: The study introduces a 19-layer deep convolutional autoencoder (DCAE) coupled with three boosting classifiers - AdaBoost, XGBoost, and LightGBM (LGBM). This hybrid system enables end-to-end AF identification without manual feature extraction.

Result: The DCAE-LGBM model achieves an F1-score of 95.20%, sensitivity of 99.99%, and inference latency of four seconds, surpassing existing methods and meeting clinical deployment requirements.

Conclusion: The integration of DCAE significantly enhances the performance of boosting models, making this hybrid system a reliable tool for automated AF detection in clinical settings.

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with
elevated health risks, where timely detection is pivotal for mitigating
stroke-related morbidity. This study introduces an innovative hybrid
methodology integrating unsupervised deep learning and gradient boosting models
to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is
coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM
(LGBM)-to harness their complementary advantages while addressing individual
limitations. The proposed framework uniquely combines DCAE with gradient
boosting, enabling end-to-end AF identification devoid of manual feature
extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of
99.99%, and inference latency of four seconds, outperforming existing methods
and aligning with clinical deployment requirements. The DCAE integration
significantly enhances boosting models, positioning this hybrid system as a
reliable tool for automated AF detection in clinical settings.

</details>


### [82] [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379)
*Xiaoyu Wu,Yifei Pang,Terrance Liu,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 研究人员提出了一种新型的数据提取攻击方法，挑战了即使完全遗忘也无法保障隐私的假设。通过结合遗忘前后的模型信号和标记过滤策略，该攻击显著提高了从大型语言模型中提取被遗忘数据的成功率，并在多个基准测试中验证了其效果。此外，在模拟医疗诊断数据集上的实验表明，完全遗忘可能反而增加隐私泄露风险。因此，作者建议在评估遗忘方法时应考虑更广泛的威胁模型，包括对先前检查点的对抗访问。


<details>
  <summary>Details</summary>
Motivation: 尽管完全遗忘（exact unlearning）被认为是去除特定数据影响的黄金标准，但其是否真正能够抵御隐私相关攻击仍需验证。

Method: 提出一种新的数据提取攻击方法，利用遗忘前后模型的信号引导以及标记过滤策略，揭示反映已移除数据分布的模式。

Result: 在MUSE、TOFU和WMDP等多个基准测试中，该攻击显著提高了数据提取成功率，在某些情况下性能翻倍。同时，在模拟医疗诊断数据集上证明了完全遗忘可能增加隐私泄露风险。

Conclusion: 完全遗忘并不足以保障隐私，评估遗忘方法时需要考虑更广泛的威胁模型，包括对抗性访问先前模型检查点的可能性。

Abstract: Large language models are typically trained on datasets collected from the
web, which may inadvertently contain harmful or sensitive personal information.
To address growing privacy concerns, unlearning methods have been proposed to
remove the influence of specific data from trained models. Of these, exact
unlearning -- which retrains the model from scratch without the target data --
is widely regarded the gold standard, believed to be robust against
privacy-related attacks. In this paper, we challenge this assumption by
introducing a novel data extraction attack that compromises even exact
unlearning. Our method leverages both the pre- and post-unlearning models: by
guiding the post-unlearning model using signals from the pre-unlearning model,
we uncover patterns that reflect the removed data distribution. Combining model
guidance with a token filtering strategy, our attack significantly improves
extraction success rates -- doubling performance in some cases -- across common
benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our
attack's effectiveness on a simulated medical diagnosis dataset to highlight
real-world privacy risks associated with exact unlearning. In light of our
findings, which suggest that unlearning may, in a contradictory way, increase
the risk of privacy leakage, we advocate for evaluation of unlearning methods
to consider broader threat models that account not only for post-unlearning
models but also for adversarial access to prior checkpoints.

</details>


### [83] [Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting](https://arxiv.org/abs/2505.24088)
*Chen Huang,Skyler Seto,Hadi Pouransari,Mehrdad Farajtabar,Raviteja Vemulapalli,Fartash Faghri,Oncel Tuzel,Barry-John Theobald,Josh Susskind*

Main category: cs.LG

TL;DR: A new regularization method called Proxy-FDA is proposed to reduce concept forgetting during fine-tuning of vision foundation models. It preserves structural knowledge in feature space by performing Feature Distribution Alignment using nearest neighbor graphs and informative proxies.


<details>
  <summary>Details</summary>
Motivation: Vision foundation models pre-trained on large amounts of data can be adapted to downstream tasks through fine-tuning. However, fine-tuning often leads to the problem of concept forgetting on other tasks. Existing methods preserve knowledge via point-wise matching of model weights or features, which may be too strong and lack explicit awareness of feature neighborhood structures encoding rich knowledge.

Method: The proposed method, Proxy-FDA, explicitly preserves structural knowledge in feature space. It performs Feature Distribution Alignment between pre-trained and fine-tuned feature spaces using nearest neighbor graphs. The alignment is further improved by informative proxies that are generated dynamically to increase data diversity.

Result: Experiments demonstrate that Proxy-FDA significantly reduces concept forgetting during fine-tuning. There is a strong correlation between forgetting and a distributional distance metric compared to L2 distance. Proxy-FDA shows benefits in various fine-tuning settings (end-to-end, few-shot, continual tuning) and across different tasks such as image classification, captioning, and VQA.

Conclusion: Proxy-FDA is an effective regularization method for reducing concept forgetting in fine-tuning vision foundation models. It highlights the importance of preserving structural knowledge in feature space and shows potential in diverse fine-tuning scenarios and tasks.

Abstract: Vision foundation models pre-trained on massive data encode rich
representations of real-world concepts, which can be adapted to downstream
tasks by fine-tuning. However, fine-tuning foundation models on one task often
leads to the issue of concept forgetting on other tasks. Recent methods of
robust fine-tuning aim to mitigate forgetting of prior knowledge without
affecting the fine-tuning performance. Knowledge is often preserved by matching
the original and fine-tuned model weights or feature pairs. However, such
point-wise matching can be too strong, without explicit awareness of the
feature neighborhood structures that encode rich knowledge as well. We propose
a novel regularization method Proxy-FDA that explicitly preserves the
structural knowledge in feature space. Proxy-FDA performs Feature Distribution
Alignment (using nearest neighbor graphs) between the pre-trained and
fine-tuned feature spaces, and the alignment is further improved by informative
proxies that are generated dynamically to increase data diversity. Experiments
show that Proxy-FDA significantly reduces concept forgetting during
fine-tuning, and we find a strong correlation between forgetting and a
distributional distance metric (in comparison to L2 distance). We further
demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,
few-shot and continual tuning) and across different tasks like image
classification, captioning and VQA.

</details>


### [84] [A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke](https://arxiv.org/abs/2505.24101)
*Zhenran Xu*

Main category: cs.LG

TL;DR: An explainable ensemble model effectively predicted the prolonged Length of Stay (LOS) in ischaemic stroke, but further validation is needed for haemorrhagic stroke.


<details>
  <summary>Details</summary>
Motivation: To enhance model efficiency, performance, and interpretability by refining predictors and developing an interpretable multi-level stacking ensemble model for predicting prolonged LOS in acute stroke patients.

Method: Data from Stroke Foundation Acute Audit in Australia were used to develop models separately for ischaemic and haemorrhagic stroke. Candidate predictors were categorised into patient, clinical, and system domains and refined using feature selection with correlation-based approaches. An interpretable multi-level stacking ensemble model was developed and evaluated using discrimination (AUC), calibration curves, and interpretability (SHAP plots).

Result: The ensemble model achieved superior performance for ischaemic stroke [AUC: 0.824 (95% CI: 0.801-0.846)] compared to logistic regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004]. For haemorrhagic stroke, the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136]. Shared predictors for both types of stroke included rehabilitation assessment, urinary incontinence, stroke unit care, inability to walk independently, physiotherapy, and stroke care coordinators involvement.

Conclusion: An explainable ensemble model effectively predicted prolonged LOS in ischaemic stroke. Further validation in larger cohorts is needed for haemorrhagic stroke.

Abstract: Length of stay (LOS) prediction in acute stroke is critical for improving
care planning. Existing machine learning models have shown suboptimal
predictive performance, limited generalisability, and have overlooked
system-level factors. We aimed to enhance model efficiency, performance, and
interpretability by refining predictors and developing an interpretable
multi-level stacking ensemble model. Data were accessed from the biennial
Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models
were developed for ischaemic and haemorrhagic stroke separately. The outcome
was prolonged LOS (the LOS above the 75th percentile). Candidate predictors
(ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical,
and system domains. Feature selection with correlation-based approaches was
used to refine key predictors. The evaluation of models included discrimination
(AUC), calibration curves, and interpretability (SHAP plots). In ischaemic
stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in
haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance
[AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic
regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However,
the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform
logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for
haemorrhagic. SHAP analysis identified shared predictors for both types of
stroke: rehabilitation assessment, urinary incontinence, stroke unit care,
inability to walk independently, physiotherapy, and stroke care coordinators
involvement. An explainable ensemble model effectively predicted the prolonged
LOS in ischaemic stroke. Further validation in larger cohorts is needed for
haemorrhagic stroke.

</details>


### [85] [Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs](https://arxiv.org/abs/2505.24110)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: This paper presents a framework proving the equivalence between nondeterministic finite automata (NFAs) and ReLU neural networks, showing that a three-layer ReLU network can recognize any regular language accepted by an NFA.


<details>
  <summary>Details</summary>
Motivation: To establish a formal connection between automata theory and neural computation by proving the equivalence between nondeterministic finite automata (NFAs) and standard feedforward ReLU neural networks.

Method: Encoding automaton states as binary vectors and transitions as sparse linear layers, utilizing ReLU activations to simulate nondeterministic branching, subset construction, and epsilon-closures. Theoretical results prove that a three-layer ReLU network of width O(n) can exactly recognize any regular language accepted by an n-state NFA. Gradient descent over structure-preserving networks preserves symbolic semantics and acceptance behavior.

Result: Extensive experiments across multiple validation tasks achieve perfect or near-perfect empirical alignment with ground-truth automata, demonstrating the practicality and correctness of the theoretical findings.

Conclusion: The work provides the first provably complete symbolic simulation of NFAs within standard deep learning architectures, uniting automata theory with neural computation through ReLU dynamics.

Abstract: We present a formal and constructive framework establishing the equivalence
between nondeterministic finite automata (NFAs) and standard feedforward ReLU
neural networks. By encoding automaton states as binary vectors and transitions
as sparse linear layers, we show that ReLU activations simulate
nondeterministic branching, subset construction, and $\epsilon$-closures in a
mathematically precise manner. Our core theoretical results prove that a
three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any
regular language accepted by an $n$-state NFA-without recurrence, memory, or
approximation. Furthermore, we show that gradient descent over
structure-preserving networks preserves symbolic semantics and acceptance
behavior. Extensive experiments across multiple validation tasks-including
parallel path tracking, symbolic subset construction, $\epsilon$-closure
convergence, acceptance classification, structural training invariants, and
functional equivalence-achieve perfect or near-perfect empirical alignment with
ground-truth automata. This work provides the first provably complete symbolic
simulation of NFAs within standard deep learning architectures, uniting
automata theory with neural computation through ReLU dynamics.

</details>


### [86] [AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits](https://arxiv.org/abs/2505.24138)
*Yichen Shi,Ze Zhang,Hongyang Wang,Zhuofu Tao,Zhongyi Li,Bingyu Chen,Yaxin Wang,Zhiping Yu,Ting-Jung Lin,Lei He*

Main category: cs.LG

TL;DR: Analog/Mixed-Signal (AMS) circuits are crucial in the IC industry, yet automating their design is challenging. Multi-modal Large Language Models (MLLMs) show potential for AMS circuit analysis and design, but lack a comprehensive benchmark. The authors introduce AMSbench, which evaluates MLLMs on tasks such as circuit schematic perception, analysis, and design, revealing limitations in complex reasoning and design tasks.


<details>
  <summary>Details</summary>
Motivation: Automating AMS circuit design has been difficult due to its complexity, and while MLLMs show promise, there's no systematic benchmark to evaluate their capabilities across diverse AMS-related challenges.

Method: The authors developed AMSbench, a benchmark suite with around 8000 test questions at multiple difficulty levels, to assess MLLM performance on critical tasks like circuit schematic perception, analysis, and design.

Result: Evaluation of eight prominent models using AMSbench highlighted significant limitations in current MLLMs, especially in complex multi-modal reasoning and sophisticated circuit design tasks.

Conclusion: Advancing MLLMs' understanding and application of circuit-specific knowledge is necessary to close the performance gap relative to human expertise and progress towards fully automated AMS circuit design.

Abstract: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated
circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit
design has remained a longstanding challenge due to its difficulty and
complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer
promising potential for supporting AMS circuit analysis and design. However,
current research typically evaluates MLLMs on isolated tasks within the domain,
lacking a comprehensive benchmark that systematically assesses model
capabilities across diverse AMS-related challenges. To address this gap, we
introduce AMSbench, a benchmark suite designed to evaluate MLLM performance
across critical tasks including circuit schematic perception, circuit analysis,
and circuit design. AMSbench comprises approximately 8000 test questions
spanning multiple difficulty levels and assesses eight prominent models,
encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and
Gemini 2.5 Pro. Our evaluation highlights significant limitations in current
MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit
design tasks. These results underscore the necessity of advancing MLLMs'
understanding and effective application of circuit-specific knowledge, thereby
narrowing the existing performance gap relative to human expertise and moving
toward fully automated AMS circuit design workflows. Our data is released at
https://huggingface.co/datasets/wwhhyy/AMSBench

</details>


### [87] [Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction](https://arxiv.org/abs/2505.24145)
*Wilfried Genuist,Éric Savin,Filippo Gatti,Didier Clouteau*

Main category: cs.LG

TL;DR: The paper proposes a conditional score-based diffusion model for multi-scenarios fluid flow prediction, which integrates an energy constraint from turbulent flows' statistical properties. It provides stable and physically faithful predictions even under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a model that can efficiently predict multi-scenario fluid flows with minimal training while maintaining physical fidelity.

Method: A conditional score-based diffusion model is used, integrating an energy constraint based on the statistical properties of turbulent flows. The method features a general architecture without problem-specific design, supports plug-and-play enhancements, and enables fast solution generation. Stochastic differential equation formulations are explored to enhance performance.

Result: The model achieves stable, robust, and physically faithful predictions across various flow regimes and configurations, even in challenging turbulent conditions. With properly tuned parameters, it delivers accurate results while preserving key physical and statistical properties.

Conclusion: The proposed model effectively predicts multi-scenario fluid flows with high accuracy and physical fidelity, validated through extensive experiments.

Abstract: Building on recent advances in scientific machine learning and generative
modeling for computational fluid dynamics, we propose a conditional score-based
diffusion model designed for multi-scenarios fluid flow prediction. Our model
integrates an energy constraint rooted in the statistical properties of
turbulent flows, improving prediction quality with minimal training, while
enabling efficient sampling at low cost. The method features a simple and
general architecture that requires no problem-specific design, supports
plug-and-play enhancements, and enables fast and flexible solution generation.
It also demonstrates an efficient conditioning mechanism that simplifies
training across different scenarios without demanding a redesign of existing
models. We further explore various stochastic differential equation
formulations to demonstrate how thoughtful design choices enhance performance.
We validate the proposed methodology through extensive experiments on complex
fluid dynamics datasets encompassing a variety of flow regimes and
configurations. Results demonstrate that our model consistently achieves
stable, robust, and physically faithful predictions, even under challenging
turbulent conditions. With properly tuned parameters, it achieves accurate
results across multiple scenarios while preserving key physical and statistical
properties. We present a comprehensive analysis of stochastic differential
equation impact and discuss our approach across diverse fluid mechanics tasks.

</details>


### [88] [RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget](https://arxiv.org/abs/2505.24149)
*Adam Piaseczny,Md Kamran Chowdhury Shisher,Shiqiang Wang,Christopher G. Brinton*

Main category: cs.LG

TL;DR: 在实际环境中部署的机器学习算法经常面临适应概念漂移的挑战，即任务数据分布随时间变化。当模型性能必须在严格的资源限制下保持时，问题变得更加困难。现有的解决方案通常依赖于产生高计算开销的漂移检测方法，并且无法提供对资源使用或理论性能保证的严格保障。为了解决这些缺点，我们提出了RCCDA：一种动态模型更新策略，该策略在确保严格遵守预定义资源限制的同时优化ML训练动力学，仅利用过去的损失信息和可调的漂移阈值。通过将这些结果集成到李雅普诺夫漂移加惩罚框架中，可以生成一个基于可测量累积损失阈值的轻量级策略，该策略可以证明限制更新频率和成本。在三个领域泛化数据集上的实验结果表明，我们的策略在推理准确性方面优于基线方法，同时在几种概念漂移计划下严格遵守资源限制，使我们的解决方案特别适合实时ML部署。


<details>
  <summary>Details</summary>
Motivation: 在实际环境中部署的机器学习算法经常面临适应概念漂移的挑战，即任务数据分布随时间变化。当模型性能必须在严格的资源限制下保持时，问题变得更加困难。现有的解决方案通常依赖于产生高计算开销的漂移检测方法，并且无法提供对资源使用或理论性能保证的严格保障。

Method: 我们提出了RCCDA：一种动态模型更新策略，该策略在确保严格遵守预定义资源限制的同时优化ML训练动力学，仅利用过去的损失信息和可调的漂移阈值。通过将这些结果集成到李雅普诺夫漂移加惩罚框架中，可以生成一个基于可测量累积损失阈值的轻量级策略，该策略可以证明限制更新频率和成本。

Result: 在三个领域泛化数据集上的实验结果表明，我们的策略在推理准确性方面优于基线方法，同时在几种概念漂移计划下严格遵守资源限制。

Conclusion: 提出的RCCDA策略在确保严格遵守资源限制的同时优化了ML训练动力学，仅利用过去的损失信息和可调的漂移阈值。它在推理准确性方面优于基线方法，适用于实时ML部署。

Abstract: Machine learning (ML) algorithms deployed in real-world environments are
often faced with the challenge of adapting models to concept drift, where the
task data distributions are shifting over time. The problem becomes even more
difficult when model performance must be maintained under adherence to strict
resource constraints. Existing solutions often depend on drift-detection
methods that produce high computational overhead for resource-constrained
environments, and fail to provide strict guarantees on resource usage or
theoretical performance assurances. To address these shortcomings, we propose
RCCDA: a dynamic model update policy that optimizes ML training dynamics while
ensuring strict compliance to predefined resource constraints, utilizing only
past loss information and a tunable drift threshold. In developing our policy,
we analytically characterize the evolution of model loss under concept drift
with arbitrary training update decisions. Integrating these results into a
Lyapunov drift-plus-penalty framework produces a lightweight policy based on a
measurable accumulated loss threshold that provably limits update frequency and
cost. Experimental results on three domain generalization datasets demonstrate
that our policy outperforms baseline methods in inference accuracy while
adhering to strict resource constraints under several schedules of concept
drift, making our solution uniquely suited for real-time ML deployments.

</details>


### [89] [Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning](https://arxiv.org/abs/2505.24155)
*Ehtesamul Azim,Dongjie Wang,Tae Hyun Hwang,Yanjie Fu,Wei Zhang*

Main category: cs.LG

TL;DR: This paper proposes a two-stage framework combining statistical selection with biological pathway knowledge via multi-agent reinforcement learning (MARL) for gene selection in genomic data. It improves prediction accuracy and biological interpretability compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional feature selection methods often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. There is a need to integrate biological pathway knowledge while maintaining statistical rigor in gene selection.

Method: The method involves a two-stage framework. The first stage is a pathway-guided pre-filtering strategy using multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. The second stage models genes as collaborative agents in a MARL framework, incorporating pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies.

Result: Extensive experiments on multiple gene expression datasets demonstrate that the proposed approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.

Conclusion: The proposed two-stage framework effectively integrates statistical selection with biological pathway knowledge, offering a more stable and biologically relevant approach to gene selection in high-dimensional genomic data.

Abstract: Gene selection in high-dimensional genomic data is essential for
understanding disease mechanisms and improving therapeutic outcomes.
Traditional feature selection methods effectively identify predictive genes but
often ignore complex biological pathways and regulatory networks, leading to
unstable and biologically irrelevant signatures. Prior approaches, such as
Lasso-based methods and statistical filtering, either focus solely on
individual gene-outcome associations or fail to capture pathway-level
interactions, presenting a key challenge: how to integrate biological pathway
knowledge while maintaining statistical rigor in gene selection? To address
this gap, we propose a novel two-stage framework that integrates statistical
selection with biological pathway knowledge using multi-agent reinforcement
learning (MARL). First, we introduce a pathway-guided pre-filtering strategy
that leverages multiple statistical methods alongside KEGG pathway information
for initial dimensionality reduction. Next, for refined selection, we model
genes as collaborative agents in a MARL framework, where each agent optimizes
both predictive power and biological relevance. Our framework incorporates
pathway knowledge through Graph Neural Network-based state representations, a
reward mechanism combining prediction performance with gene centrality and
pathway coverage, and collaborative learning strategies using shared memory and
a centralized critic component. Extensive experiments on multiple gene
expression datasets demonstrate that our approach significantly improves both
prediction accuracy and biological interpretability compared to traditional
methods.

</details>


### [90] [Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents](https://arxiv.org/abs/2505.24157)
*Seungjoon Lee,Suhwan Kim,Minhyeon Oh,Youngsik Yoon,Jungseul Ok*

Main category: cs.LG

TL;DR: The paper introduces Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework with three key components to enhance robustness and efficiency in planning for autonomous agents capable of mastering complex tasks in unpredictable environments. Evaluated in two open-world testbeds, REPOA successfully obtains challenging late-game items.


<details>
  <summary>Details</summary>
Motivation: Developing autonomous agents that can master complex, multi-step tasks in unpredictable, interactive environments is a significant challenge. Existing approaches have limitations due to reliance on problematic internal knowledge or unrealistic environmental assumptions. There is a need for agents capable of acquiring planning knowledge from scratch in realistic settings while addressing the challenges of robustness and efficiency.

Method: REPOA features three key components: adaptive dependency learning, fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency.

Result: Evaluation in two established open-world testbeds demonstrates REPOA's robust and efficient planning, as it successfully obtains challenging late-game items that were beyond the reach of prior approaches.

Conclusion: REPOA addresses the challenges of developing autonomous agents capable of mastering complex tasks in unpredictable environments by enhancing robustness and efficiency in planning.

Abstract: Developing autonomous agents capable of mastering complex, multi-step tasks
in unpredictable, interactive environments presents a significant challenge.
While Large Language Models (LLMs) offer promise for planning, existing
approaches often rely on problematic internal knowledge or make unrealistic
environmental assumptions. Although recent work explores learning planning
knowledge, they still retain limitations due to partial reliance on external
knowledge or impractical setups. Indeed, prior research has largely overlooked
developing agents capable of acquiring planning knowledge from scratch,
directly in realistic settings. While realizing this capability is necessary,
it presents significant challenges, primarily achieving robustness given the
substantial risk of incorporating LLMs' inaccurate knowledge. Moreover,
efficiency is crucial for practicality as learning can demand prohibitive
exploration. In response, we introduce Robust and Efficient Planning for
Open-world Agents (REPOA), a novel framework designed to tackle these issues.
REPOA features three key components: adaptive dependency learning and
fine-grained failure-aware operation memory to enhance robustness to knowledge
inaccuracies, and difficulty-based exploration to improve learning efficiency.
Our evaluation in two established open-world testbeds demonstrates REPOA's
robust and efficient planning, showcasing its capability to successfully obtain
challenging late-game items that were beyond the reach of prior approaches.

</details>


### [91] [Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem](https://arxiv.org/abs/2505.24178)
*Katherine Tieu,Dongqi Fu,Jun Wu,Jingrui He*

Main category: cs.LG

TL;DR: In the era of foundation models, Out-of-Distribution (OOD) problems hinder AI generalization. We investigate invariant components in temporal graphs for robust learning and propose an error-bounded Invariant Link Selector with Information Bottleneck method to distinguish invariant and variant components during training.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the challenge of OOD problems which prevent AI generalization, especially in relational data like graphs that do not follow the IID condition and are further complicated by time-related factors.

Method: Using the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can differentiate between invariant and variant components within temporal graphs during the training process. This is supplemented with rigorous optimization functions and task-specific loss functions such as temporal link prediction.

Result: The proposed method enables deep learning models to be more generalizable across different testing scenarios, and it demonstrates effectiveness in real-world applications like citation recommendation and merchandise recommendation through experiments with state-of-the-art methods.

Conclusion: An error-bounded Invariant Link Selector using the IB method has been developed to enhance model generalizability over temporal graphs, addressing OOD challenges effectively.

Abstract: In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,
the data discrepancy between the training environments and testing
environments, hinder AI generalization. Further, relational data like graphs
disobeying the Independent and Identically Distributed (IID) condition makes
the problem more challenging, especially much harder when it is associated with
time. Motivated by this, to realize the robust invariant learning over temporal
graphs, we want to investigate what components in temporal graphs are most
invariant and representative with respect to labels. With the Information
Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector
that can distinguish invariant components and variant components during the
training process to make the deep learning model generalizable for different
testing scenarios. Besides deriving a series of rigorous generalizable
optimization functions, we also equip the training with task-specific loss
functions, e.g., temporal link prediction, to make pretrained models solve
real-world application tasks like citation recommendation and merchandise
recommendation, as demonstrated in our experiments with state-of-the-art (SOTA)
methods. Our code is available at https://github.com/kthrn22/OOD-Linker.

</details>


### [92] [SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](https://arxiv.org/abs/2505.24179)
*Xiaodong Ji,Hailin Zhang,Fangcheng Fu,Bin Cui*

Main category: cs.LG

TL;DR: SALE，一种细粒度的稀疏注意力方法，通过4位量化查询键产品和块稀疏注意力加速长上下文预填充阶段，显著提高处理效率并保持模型准确性，无需参数训练且易于集成。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）应用中，自注意力模块在推理的预填充阶段由于其与序列长度相关的二次时间复杂度成为瓶颈，而现有的稀疏注意力方法虽然加速了计算，但通常会导致模型准确性的大幅损失。

Method: 提出了一种名为SALE的细粒度稀疏注意力方法，通过4位量化查询键产品进行快速准确的细粒度注意力权重估计，然后采用块稀疏注意力加速预填充计算，并引入相对注意力评分指标来评估查询键对的重要性。为了硬件效率，还实现了定制的CUDA内核，将额外开销减少到完整注意力延迟的大约11%。

Result: 实验表明，在长上下文基准测试中，该方法在准确性和效率的权衡上优于现有方法，对于超过64K的序列，Llama-3.1-8B模型至少实现3.36倍的速度提升，同时保持模型质量。

Conclusion: SALE是一种高效的稀疏注意力方法，能够在几乎不影响模型准确性的情况下显著加速长上下文预填充阶段，易于集成到现有系统中，无需参数训练。

Abstract: Many advanced Large Language Model (LLM) applications require long-context
processing, but the self-attention module becomes a bottleneck during the
prefilling stage of inference due to its quadratic time complexity with respect
to sequence length. Existing sparse attention methods accelerate attention
computation by skipping less significant regions of the attention map. However,
these approaches typically perform coarse-grained inspection of the attention
map, rendering considerable loss in model accuracy. In this paper, we propose
SALE, a fine-grained sparse attention method that accelerates the long-context
prefilling stage of LLM with negligible loss in model accuracy. SALE achieves
fast and accurate fine-grained attention weight estimation through 4-bit
quantized query-key products, followed by block-sparse attention to accelerate
prefilling computations. For importance evaluation for query-key pairs, we
adopt our Relative Attention Score metric, which offers significantly higher
efficiency within our framework. We implement a custom CUDA kernel optimized
for our approach for hardware efficiency, reducing the additional overhead to
approximately 11% of the full attention latency. Notably, SALE requires no
parameter training and can be seamlessly integrated into existing systems with
trivial code modifications. Experiments on long-context benchmarks demonstrate
that our method outperforms existing approaches in accuracy-efficiency
trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences
longer than 64K while maintaining model quality.

</details>


### [93] [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.24183)
*Yaoyu Zhu,Di Huang,Hanqi Lyu,Xiaoyun Zhang,Chongxiao Li,Wenxuan Shi,Yutong Wu,Jianan Mu,Jinghua Wang,Yang Zhao,Pengwei Jin,Shuyao Cheng,Shengwen Liang,Xishan Zhang,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: CodeV-R1 is an RLVR framework designed for training Verilog generation LLMs, overcoming challenges in verification environments, dataset scarcity and computation cost.


<details>
  <summary>Details</summary>
Motivation: To extend reinforcement learning with verifiable reward (RLVR) to electronic design automation (EDA), particularly for automatically generating hardware description languages (HDLs) like Verilog from natural language specifications.

Method: Developed a rule-based testbench generator for robust equivalence checking, proposed a round-trip data synthesis method to create high-quality NL-code pairs, and employed a two-stage 'distill-then-RL' training pipeline that includes adaptive DAPO to reduce training cost.

Result: The model CodeV-R1-7B achieved 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1 respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1.

Conclusion: CodeV-R1 successfully addresses key challenges in applying RLVR to EDA, demonstrating superior performance compared to previous models and comparable results to much larger models.

Abstract: Large language models (LLMs) trained via reinforcement learning with
verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,
automatable verification, such as software programming and mathematical
problems. Extending RLVR to electronic design automation (EDA), especially
automatically generating hardware description languages (HDLs) like Verilog
from natural-language (NL) specifications, however, poses three key challenges:
the lack of automated and accurate verification environments, the scarcity of
high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To
this end, we introduce CodeV-R1, an RLVR framework for training Verilog
generation LLMs. First, we develop a rule-based testbench generator that
performs robust equivalence checking against golden references. Second, we
propose a round-trip data synthesis method that pairs open-source Verilog
snippets with LLM-generated NL descriptions, verifies code-NL-code consistency
via the generated testbench, and filters out inequivalent examples to yield a
high-quality dataset. Third, we employ a two-stage "distill-then-RL" training
pipeline: distillation for the cold start of reasoning abilities, followed by
adaptive DAPO, our novel RLVR algorithm that can reduce training cost by
adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves
68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,
surpassing prior state-of-the-art by 12~20%, while matching or even exceeding
the performance of 671B DeepSeek-R1. We will release our model, training
pipeline, and dataset to facilitate research in EDA and LLM communities.

</details>


### [94] [Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling](https://arxiv.org/abs/2505.24185)
*Yipan Wei,Yuchen Zou,Yapeng Li,Bo Du*

Main category: cs.LG

TL;DR: FedDEA是一种新的聚合方法，专为多任务模型集成而设计，解决了现有FMTL方法在异构任务联合训练中的不足。通过动态识别任务相关维度和优化调整，抑制跨任务干扰，实现统一全局模型内的任务级解耦聚合。实验表明，FedDEA易于集成到主流联邦优化算法中，并在多种数据集上显著提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Federated Multi-Task Learning (FMTL)方法主要关注为每个客户端构建个性化模型，但无法将多个异构任务聚合到一个统一模型中，导致在实际场景中难以实现有效的联合训练。

Method: 提出了一种名为FedDEA（Federated Decoupled Aggregation）的方法，该方法根据本地更新的响应强度动态识别任务相关维度，并通过重新缩放增强优化效果，从而有效抑制跨任务干扰，并在统一的全局模型内实现任务级解耦聚合。此方法不依赖任务标签或架构修改，具有广泛的适用性和部署友好性。

Result: 实验结果表明，FedDEA可以轻松集成到各种主流联邦优化算法中，并在广泛使用的NYUD-V2和PASCAL-Context数据集上持续提供显著的整体性能改进，验证了其在高度异构任务设置下的鲁棒性和泛化能力。

Conclusion: FedDEA作为一种新颖的、无需任务标签或架构修改的聚合方法，成功实现了多任务模型集成，解决了现有FMTL方法的局限性，并展现出强大的性能提升和广泛的适用性。

Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing
heterogeneous tasks without exchanging their local data, offering broad
potential for privacy preserving multi-task collaboration. However, most
existing methods focus on building personalized models for each client and
unable to support the aggregation of multiple heterogeneous tasks into a
unified model. As a result, in real-world scenarios where task objectives,
label spaces, and optimization paths vary significantly, conventional FMTL
methods struggle to achieve effective joint training. To address this
challenge, we propose FedDEA (Federated Decoupled Aggregation), an
update-structure-aware aggregation method specifically designed for multi-task
model integration. Our method dynamically identifies task-relevant dimensions
based on the response strength of local updates and enhances their optimization
effectiveness through rescaling. This mechanism effectively suppresses
cross-task interference and enables task-level decoupled aggregation within a
unified global model. FedDEA does not rely on task labels or architectural
modifications, making it broadly applicable and deployment-friendly.
Experimental results demonstrate that it can be easily integrated into various
mainstream federated optimization algorithms and consistently delivers
significant overall performance improvements on widely used NYUD-V2 and
PASCAL-Context. These results validate the robustness and generalization
capabilities of FedDEA under highly heterogeneous task settings.

</details>


### [95] [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
*Orlando Marquez Ayala,Patrice Bechard,Emily Chen,Maggie Baird,Jingfei Chen*

Main category: cs.LG

TL;DR: For domain-specific tasks requiring structured outputs, small language models (SLMs) fine-tuned still have a quality advantage over large language models (LLMs) prompted. Fine-tuning an SLM improves quality by 10% on average.


<details>
  <summary>Details</summary>
Motivation: To determine whether small language models (SLMs) still hold an advantage in specific tasks that require structured outputs, especially with the decreasing cost of using large language models (LLMs).

Method: Compare fine-tuning an SLM with prompting LLMs on generating low-code workflows in JSON form and conduct systematic error analysis to reveal model limitations.

Result: Fine-tuning an SLM leads to a 10% improvement in quality on average compared to using a good prompt for LLMs.

Conclusion: Small language models retain a quality advantage for domain-specific tasks requiring structured outputs.

Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of
complex tasks with the right prompt. As per token costs are reduced, the
advantages of fine-tuning Small Language Models (SLMs) for real-world
applications -- faster inference, lower costs -- may no longer be clear. In
this work, we present evidence that, for domain-specific tasks that require
structured outputs, SLMs still have a quality advantage. We compare fine-tuning
an SLM against prompting LLMs on the task of generating low-code workflows in
JSON form. We observe that while a good prompt can yield reasonable results,
fine-tuning improves quality by 10% on average. We also perform systematic
error analysis to reveal model limitations.

</details>


### [96] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/abs/2505.24190)
*Lan-Cuong Nguyen,Quan Nguyen-Tri,Bang Tran Khanh,Dung D. Le,Long Tran-Thanh,Khoat Than*

Main category: cs.LG

TL;DR: Few-shot image classification faces challenges due to limited labeled data. While augmenting with synthetic data is promising, it often leads to performance degradation because of the gap between real and synthetic distributions. This paper develops a theoretical framework to quantify this impact and proposes a novel algorithm integrating prototype learning to bridge the gap. Extensive experiments show superior performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Few-shot image classification is challenging due to the scarcity of labeled training examples. Augmenting with synthetic data can help but causes performance degradation due to discrepancies between real and synthetic distributions.

Method: The paper develops a theoretical framework to quantify the impact of distribution discrepancies on supervised learning in image classification. It suggests practical ways to generate good synthetic samples and train predictors with high generalization ability. A novel theoretical-based algorithm integrating prototype learning is proposed to optimize data partitioning and model training.

Result: Extensive experimental results demonstrate that the proposed approach outperforms state-of-the-art methods across multiple datasets.

Conclusion: The developed theoretical framework and proposed algorithm effectively address the limitations caused by the gap between real and synthetic data distributions in few-shot image classification.

Abstract: Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [97] [Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback](https://arxiv.org/abs/2505.24193)
*Ofir Schlisselberg,Tal Lancewicki,Peter Auer,Yishay Mansour*

Main category: cs.LG

TL;DR: 本研究探讨了在Best-of-Both-Worlds (BoBW)框架下的多臂赌博机问题，该框架旨在同时实现随机和对抗环境中的接近最优性能。提出了一种新算法，该算法在对抗环境中实现了$\widetilde{O}(\sqrt{KT} + \sqrt{D})$的遗憾值，在随机环境中提供了$\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum \Delta_i \sigma_{max}$的遗憾界限。这是首个在延迟环境下同时匹配随机和对抗环境遗憾下界的BoBW算法。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的工作已经朝着在Best-of-Both-Worlds (BoBW)框架中实现接近最优性能的目标取得了一些进展，但现有算法与已知下界之间仍然存在显著差距，特别是在随机设置中。因此需要一种新的算法来弥补这些差距。

Method: 研究人员设计了一种新算法，该算法针对多臂赌博机问题，能够适应对抗性选择的延迟，并且在BoBW框架下工作。此算法在对抗环境中实现了$\widetilde{O}(\sqrt{KT} + \sqrt{D})$的遗憾值，并在随机环境中提供了特定的遗憾界限。

Result: 该算法在对抗环境中实现了与已知下界相匹配的遗憾值，在随机环境中也首次达到了相应的下界。此外，即使超出BoBW设置，其随机遗憾界限也是首个在对抗性延迟下匹配已知下界的。

Conclusion: 提出的算法是第一个在延迟环境下同时匹配随机和对抗环境遗憾下界的BoBW算法，并且在随机遗憾界限上有所改进。

Abstract: We study the multi-armed bandit problem with adversarially chosen delays in
the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal
performance in both stochastic and adversarial environments. While prior work
has made progress toward this goal, existing algorithms suffer from significant
gaps to the known lower bounds, especially in the stochastic settings. Our main
contribution is a new algorithm that, up to logarithmic factors, matches the
known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of
$\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic
terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is
the cumulative delay. In the stochastic case, we provide a regret bound which
scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum
\Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$
and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to
simultaneously match the lower bounds in both stochastic and adversarial
regimes in delayed environment. Moreover, even beyond the BoBW setting, our
stochastic regret bound is the first to match the known lower bound under
adversarial delays, improving the second term over the best known result by a
factor of $K$.

</details>


### [98] [On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks](https://arxiv.org/abs/2505.24205)
*Mingze Wang,Weinan E*

Main category: cs.LG

TL;DR: Mixture-of-experts networks (MoEs) have great efficiency in deep learning. This study systematically explores the expressive power of MoEs for complex tasks with structural priors such as low-dimensionality and sparsity. It proves that shallow MoEs can efficiently approximate functions on low-dimensional manifolds, while deep MoEs can approximate piecewise functions with compositional sparsity.


<details>
  <summary>Details</summary>
Motivation: To better understand the theoretical foundations of MoEs' ability to model complex tasks, especially those with structural priors like low-dimensionality and sparsity.

Method: A systematic study analyzing the expressive power of both shallow and deep MoEs. For shallow MoEs, their efficiency in approximating functions supported on low-dimensional manifolds is examined. For deep MoEs, their capacity to approximate piecewise functions with compositional sparsity is analyzed.

Result: Shallow MoEs can overcome the curse of dimensionality by efficiently approximating functions on low-dimensional manifolds. Deep MoEs can approximate an exponential number of structured tasks due to their ability to model piecewise functions with compositional sparsity.

Conclusion: The study reveals the roles of key components in MoEs such as the gating mechanism, expert networks, number of experts, and layers, providing insights for designing effective MoE variants.

Abstract: Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in
modern deep learning. Despite their empirical success, the theoretical
foundations underlying their ability to model complex tasks remain poorly
understood. In this work, we conduct a systematic study of the expressive power
of MoEs in modeling complex tasks with two common structural priors:
low-dimensionality and sparsity. For shallow MoEs, we prove that they can
efficiently approximate functions supported on low-dimensional manifolds,
overcoming the curse of dimensionality. For deep MoEs, we show that
$\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise
functions comprising $E^L$ pieces with compositional sparsity, i.e., they can
exhibit an exponential number of structured tasks. Our analysis reveals the
roles of critical architectural components and hyperparameters in MoEs,
including the gating mechanism, expert networks, the number of experts, and the
number of layers, and offers natural suggestions for MoE variants.

</details>


### [99] [Model Informed Flows for Bayesian Inference of Probabilistic Programs](https://arxiv.org/abs/2505.24243)
*Joohwan Ko,Justin Domke*

Main category: cs.LG

TL;DR: The paper combines VIP and a full-rank Gaussian into an exact representation of a forward autoregressive flow, introduces the MIF architecture for better posterior approximations.


<details>
  <summary>Details</summary>
Motivation: Variational inference faces challenges with complex hierarchical Bayesian models due to posterior geometry. While advances in flow-based variational families and VIP address parts of this issue, their formal relationship remains unexplored.

Method: Proves that combining VIP with a full-rank Gaussian equals a forward autoregressive flow plus translation and prior input; develops the MIF architecture incorporating translation, prior info, and hierarchy.

Result: MIF provides tighter posterior approximations and matches or surpasses state-of-the-art performance on various benchmarks.

Conclusion: MIF addresses challenges in variational inference for complex models by integrating theoretical insights and delivering superior empirical results.

Abstract: Variational inference often struggles with the posterior geometry exhibited
by complex hierarchical Bayesian models. Recent advances in flow-based
variational families and Variationally Inferred Parameters (VIP) each address
aspects of this challenge, but their formal relationship is unexplored. Here,
we prove that the combination of VIP and a full-rank Gaussian can be
represented exactly as a forward autoregressive flow augmented with a
translation term and input from the model's prior. Guided by this theoretical
insight, we introduce the Model-Informed Flow (MIF) architecture, which adds
the necessary translation mechanism, prior information, and hierarchical
ordering. Empirically, MIF delivers tighter posterior approximations and
matches or exceeds state-of-the-art performance across a suite of hierarchical
and non-hierarchical benchmarks.

</details>


### [100] [Rethinking Continual Learning with Progressive Neural Collapse](https://arxiv.org/abs/2505.24254)
*Zheng Wang,Wanhao Yu,Li Yang,Sen Lin*

Main category: cs.LG

TL;DR: 提出了一种新的持续学习框架Progressive Neural Collapse (ProNC)，通过动态扩展ETF目标来克服固定全局ETF的缺点，显著优于相关基线，并保持灵活性、简单性和效率。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的关键挑战是灾难性遗忘，而深度神经网络在训练过程中会收敛到神经崩溃状态，其中类别原型形成静态单纯形等角紧框架(ETF)，这为缓解知识干扰提供了理想的模型学习目标。然而，使用固定的全局ETF存在不切实际和性能有限的问题。

Method: 提出了Progressive Neural Collapse (ProNC)框架，该框架通过以原则性方式添加新类别原型作为新任务的顶点，逐步扩展ETF目标，确保所有遇到的类别之间具有最大可分性，并最小化从先前ETF的偏移。此外，还将ProNC集成到常见的持续学习算法设计中，利用蒸馏技术平衡旧类别的目标偏移和新类别的目标对齐。

Result: 广泛的实验表明，所提出的方法显著优于相关基线，同时保持了卓越的灵活性、简单性和效率。

Conclusion: Progressive Neural Collapse (ProNC)框架克服了固定全局ETF的缺点，在持续学习中展现出优越的性能和特性。

Abstract: Continual Learning (CL) seeks to build an agent that can continuously learn a
sequence of tasks, where a key challenge, namely Catastrophic Forgetting,
persists due to the potential knowledge interference among different tasks. On
the other hand, deep neural networks (DNNs) are shown to converge to a terminal
state termed Neural Collapse during training, where all class prototypes
geometrically form a static simplex equiangular tight frame (ETF). These
maximally and equally separated class prototypes make the ETF an ideal target
for model learning in CL to mitigate knowledge interference. Thus inspired,
several studies have emerged very recently to leverage a fixed global ETF in
CL, which however suffers from key drawbacks, such as impracticability and
limited performance.To address these challenges and fully unlock the potential
of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework
that completely removes the need of a fixed global ETF in CL. Specifically,
ProNC progressively expands the ETF target in a principled way by adding new
class prototypes as vertices for new tasks, ensuring maximal separability
across all encountered classes with minimal shifts from the previous ETF. We
next develop a new CL framework by plugging ProNC into commonly used CL
algorithm designs, where distillation is further leveraged to balance between
target shifting for old classes and target aligning for new classes. Extensive
experiments show that our approach significantly outperforms related baselines
while maintaining superior flexibility, simplicity, and efficiency.

</details>


### [101] [Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining](https://arxiv.org/abs/2505.24261)
*Weiyi Wang,Junwei Deng,Yuzheng Hu,Shiyuan Zhang,Xirui Jiang,Runting Zhang,Han Zhao,Jiaqi W. Ma*

Main category: cs.LG

TL;DR: Data attribution methods are sensitive to hyperparameters, posing a challenge for practical application. This study advocates for better theoretical understanding and efficient tuning strategies, proposing a lightweight procedure for selecting the regularization value.


<details>
  <summary>Details</summary>
Motivation: To understand the hyperparameter sensitivity of common data attribution methods and address the challenge it poses for practical application.

Method: Presented the first large-scale empirical study on hyperparameter sensitivity of data attribution methods, provided a theoretical analysis of the regularization term in influence function methods, and proposed a lightweight procedure for selecting the regularization value without model retraining.

Result: Most data attribution methods are sensitive to certain key hyperparameters. The proposed lightweight procedure for selecting the regularization value was validated as effective across a range of standard data attribution benchmarks.

Conclusion: The study identified a fundamental challenge in the practical application of data attribution and emphasized the importance of careful discussion on hyperparameter selection in future method development.

Abstract: Data attribution methods, which quantify the influence of individual training
data points on a machine learning model, have gained increasing popularity in
data-centric applications in modern AI. Despite a recent surge of new methods
developed in this space, the impact of hyperparameter tuning in these methods
remains under-explored. In this work, we present the first large-scale
empirical study to understand the hyperparameter sensitivity of common data
attribution methods. Our results show that most methods are indeed sensitive to
certain key hyperparameters. However, unlike typical machine learning
algorithms -- whose hyperparameters can be tuned using computationally-cheap
validation metrics -- evaluating data attribution performance often requires
retraining models on subsets of training data, making such metrics
prohibitively costly for hyperparameter tuning. This poses a critical open
challenge for the practical application of data attribution methods. To address
this challenge, we advocate for better theoretical understandings of
hyperparameter behavior to inform efficient tuning strategies. As a case study,
we provide a theoretical analysis of the regularization term that is critical
in many variants of influence function methods. Building on this analysis, we
propose a lightweight procedure for selecting the regularization value without
model retraining, and validate its effectiveness across a range of standard
data attribution benchmarks. Overall, our study identifies a fundamental yet
overlooked challenge in the practical application of data attribution, and
highlights the importance of careful discussion on hyperparameter selection in
future method development.

</details>


### [102] [On Fairness of Task Arithmetic: The Role of Task Vectors](https://arxiv.org/abs/2505.24262)
*Hiroki Naganuma,Kotaro Yoshida,Laura Gomezjurado Gonzalez,Takafumi Horie,Yuji Naraki,Ryotaro Shimizu*

Main category: cs.LG

TL;DR: Model editing techniques, especially task arithmetic using task vectors, can modify pre-trained models efficiently but may impact model fairness. This paper systematically examines the effects of manipulating task vectors on fairness metrics and compares them with full fine-tuning and LoRA methods. It also investigates merging task vectors from demographic subgroups to control fairness outcomes.


<details>
  <summary>Details</summary>
Motivation: To explore the fairness implications of model editing techniques like task arithmetic using task vectors, which have computational advantages but may inadvertently affect model fairness in sensitive applications.

Method: Systematically examine how manipulating task vectors affects fairness metrics by benchmarking against full fine-tuning and Low-Rank Adaptation (LoRA). Explore merging task vectors from models fine-tuned on demographic subgroups to control fairness outcomes.

Result: The results provide novel insights into the fairness implications of model editing and demonstrate the potential to control fairness outcomes by adjusting task vector coefficients.

Conclusion: This study establishes a foundation for fairness-aware and responsible model editing practices.

Abstract: Model editing techniques, particularly task arithmetic using task vectors,
have shown promise in efficiently modifying pre-trained models through
arithmetic operations like task addition and negation. Despite computational
advantages, these methods may inadvertently affect model fairness, creating
risks in sensitive applications like hate speech detection. However, the
fairness implications of task arithmetic remain largely unexplored, presenting
a critical gap in the existing literature. We systematically examine how
manipulating task vectors affects fairness metrics, including Demographic
Parity and Equalized Odds. To rigorously assess these effects, we benchmark
task arithmetic against full fine-tuning, a costly but widely used baseline,
and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning
method. Additionally, we explore merging task vectors from models fine-tuned on
demographic subgroups vulnerable to hate speech, investigating whether fairness
outcomes can be controlled by adjusting task vector coefficients, potentially
enabling tailored model behavior. Our results offer novel insights into the
fairness implications of model editing and establish a foundation for
fairness-aware and responsible model editing practices.

</details>


### [103] [GradPower: Powering Gradients for Faster Language Model Pre-Training](https://arxiv.org/abs/2505.24275)
*Mingze Wang,Jinbo Wang,Jiaqi Zhang,Wei Wang,Peng Pei,Xunliang Cai,Weinan E,Lei Wu*

Main category: cs.LG

TL;DR: 提出了一种名为GradPower的轻量级梯度转换技术，用于加速语言模型的预训练。该方法通过简单的代码更改，无需调整基础优化器的内部逻辑和超参数，即可显著降低多种架构、参数规模、数据集和学习率计划下的最终损失。特别是在使用warmup-stable-decay计划训练现代专家混合模型时，效果最为显著。此外，GradPower还可与其他先进优化器结合使用以进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型预训练过程中存在效率问题，需要一种轻量且有效的方法来加速这一过程，同时保持模型性能。

Method: GradPower首先对梯度向量应用元素级符号幂变换（sign-power transformation），然后将转换后的梯度输入基础优化器。此方法仅需单行代码更改，不改变基础优化器的内部逻辑或超参数设置。

Result: GradPower在不同架构（如LLaMA、Qwen2MoE）、参数规模（66M至2B）、数据集（C4、OpenWebText）及学习率计划（余弦、warmup-stable-decay）下均能实现更低的最终损失，尤其在训练现代专家混合模型时表现突出。

Conclusion: GradPower是一种有效的梯度转换技术，可加速语言模型预训练并提高性能，且易于与其他优化器结合使用。理论分析揭示了其机制以及梯度噪声的影响。

Abstract: We propose GradPower, a lightweight gradient-transformation technique for
accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,
GradPower first applies the elementwise sign-power transformation:
$\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds
the transformed gradient into a base optimizer. Notably, GradPower requires
only a single-line code change and no modifications to the base optimizer's
internal logic, including the hyperparameters. When applied to Adam (termed
AdamPower), GradPower consistently achieves lower terminal loss across diverse
architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,
OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The
most pronounced gains are observed when training modern mixture-of-experts
models with warmup-stable-decay schedules. GradPower also integrates seamlessly
with other state-of-the-art optimizers, such as Muon, yielding further
improvements. Finally, we provide theoretical analyses that reveal the
underlying mechanism of GradPower and highlights the influence of gradient
noise.

</details>


### [104] [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
*James R. Golden*

Main category: cs.LG

TL;DR: 尽管现代大语言模型（LLM）具有表达能力和全局非线性，但它们可以通过近乎精确的局部线性分解来解释，这为理解其内部表示和揭示下一位预测过程中的可解释语义结构提供了洞察。


<details>
  <summary>Details</summary>
Motivation: 研究者希望探索大语言模型（LLMs）在推理操作中的线性特性，并通过映射到一个等效的线性系统，无需修改模型权重或改变输出预测，以更好地理解这些模型的工作原理。

Method: 通过扩展图像扩散模型中展示的局部或分段线性技术，研究者战略性地改变了关于给定输入序列进行下一个标记预测的梯度计算，使得模型的雅可比矩阵几乎精确地重现了带有线性系统的前向预测。然后对分离的雅可比矩阵进行奇异值分解，观察到这些LLMs在极低维子空间中运行，其中许多最大的奇异向量解码为与最可能的输出标记相关的概念。

Result: 该方法在多个模型（如Llama 3、Gemma 3、Qwen 3等）上进行了演示，结果表明现代LLMs可以通过近乎精确的局部线性分解来解释，从而提供对其内部表示的洞察，并揭示下一位预测过程中的可解释语义结构。

Conclusion: 现代LLMs尽管具有表达能力和全局非线性，但可以通过局部线性分解来解释，这种分解提供了对模型内部表示的洞察，并揭示了下一位预测过程中可解释的语义结构。

Abstract: We demonstrate that the inference operations of several open-weight large
language models (LLMs) can be mapped to an exactly equivalent linear system for
an input sequence without modifying the model weights or altering output
predictions. Extending techniques from image diffusion models that exhibit
local or piecewise linearity, we strategically alter the gradient computation
with respect to a given input sequence for a next-token prediction such that
the Jacobian of the model nearly exactly reproduces the forward prediction with
a linear system. We demonstrate this approach across models (Llama 3, Gemma 3,
Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show
through the singular value decomposition of the detached Jacobian that these
LLMs operate in extremely low-dimensional subspaces where many of the largest
singular vectors decode to concepts related to the most-likely output token.
This approach also allows us to examine the operation of each successive layer
(and its attention and MLP components) as nearly-exact linear systems and
observe the emergence of semantic concepts. Despite their expressive power and
global nonlinearity, modern LLMs can be interpreted through nearly-exact
locally linear decompositions that provide insights into their internal
representations and reveal interpretable semantic structures in the next-token
prediction process.

</details>


### [105] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/abs/2505.24298)
*Wei Fu,Jiaxuan Gao,Xujie Shen,Chen Zhu,Zhiyu Mei,Chuyi He,Shusheng Xu,Guo Wei,Jun Mei,Jiashu Wang,Tongkai Yang,Binhang Yuan,Yi Wu*

Main category: cs.LG

TL;DR: AReaL is a fully asynchronous RL system for LLMs that decouples generation from training, incorporates system-level optimizations for higher GPU utilization, balances workload to control data staleness, and uses a staleness-enhanced PPO variant. It achieves up to 2.57× training speedup with improved or matched final performance.


<details>
  <summary>Details</summary>
Motivation: Effective reinforcement learning (RL) for large language models (LLMs) requires massive parallelization and efficient training systems. Current synchronous RL systems suffer from system-level inefficiency due to waiting times in batch processing which leads to GPU underutilization.

Method: AReaL completely decouples generation from training allowing rollout workers to continuously generate new outputs without waiting while training workers update the model whenever a batch of data is collected. System-level optimizations are incorporated for better GPU utilization and a staleness-enhanced PPO variant is used to handle outdated training samples effectively.

Result: Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.57× training speedup compared to the best synchronous systems with the same number of GPUs, while maintaining or improving final performance.

Conclusion: AReaL presents an efficient, fully asynchronous RL system for LLMs that significantly improves training speed and GPU utilization while stabilizing RL training through workload balancing and a staleness-enhanced PPO variant.

Abstract: Reinforcement learning (RL) has become a trending paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are synchronous
by alternating generation and training in a batch setting, where the rollouts
in each training batch are generated by the same (or latest) model. This
stabilizes RL training but suffers from severe system-level inefficiency.
Generation must wait until the longest output in the batch is completed before
model update, resulting in GPU underutilization. We present AReaL, a
\emph{fully asynchronous} RL system that completely decouples generation from
training. Rollout workers in AReaL continuously generate new outputs without
waiting, while training workers update the model whenever a batch of data is
collected. AReaL also incorporates a collection of system-level optimizations,
leading to substantially higher GPU utilization. To stabilize RL training,
AReaL balances the workload of rollout and training workers to control data
staleness, and adopts a staleness-enhanced PPO variant to better handle
outdated training samples. Extensive experiments on math and code reasoning
benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training
speedup} compared to the best synchronous systems with the same number of GPUs
and matched or even improved final performance. The code of AReaL is available
at https://github.com/inclusionAI/AReaL/.

</details>


### [106] [On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective](https://arxiv.org/abs/2505.24313)
*Gengze Xu,Wei Yao,Ziqiao Wang,Yong Liu*

Main category: cs.LG

TL;DR: 本文研究了弱到强泛化（W2SG）现象，通过广义偏差-方差分解和布雷格曼散度分析，揭示了学生模型超越教师模型的条件，并提出反向交叉熵损失能有效提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究者希望理解弱到强泛化（W2SG）现象背后的原因，特别是学生模型如何在使用由较弱教师模型标注的数据训练后，最终超越教师模型的表现。

Method: 作者通过广义偏差-方差分解和布雷格曼散度的理论分析，量化了学生与教师模型之间的风险差距，并探讨了学生模型逼近后验均值教师而非单一教师的重要性。同时，研究了反向交叉熵损失对学生模型性能的影响。

Result: 理论分析表明，当学生模型容量显著大于教师模型时，可以收敛到后验均值教师，从而促进W2SG现象的发生。此外，实验证明反向交叉熵损失能够持续提高学生模型的性能。

Conclusion: W2SG现象可以通过减少学生模型对教师监督的过拟合、降低预测熵以及使用反向交叉熵损失来促进。这些发现为设计更有效的学生模型提供了理论支持。

Abstract: Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong
student model, trained on a dataset labeled by a weak teacher, ultimately
outperforms the teacher on the target task. Recent studies attribute this
performance gain to the prediction misfit between the student and teacher
models. In this work, we theoretically investigate the emergence of W2SG
through a generalized bias-variance decomposition of Bregman divergence.
Specifically, we show that the expected population risk gap between the student
and teacher is quantified by the expected misfit between the two models. While
this aligns with previous results, our analysis removes several restrictive
assumptions, most notably, the convexity of the student's hypothesis class,
required in earlier works. Moreover, we show that W2SG is more likely to emerge
when the student model approximates its posterior mean teacher, rather than
mimicking an individual teacher. Using a concrete example, we demonstrate that
if the student model has significantly larger capacity than the teacher, it can
indeed converge to this posterior mean. Our analysis also suggests that
avoiding overfitting to the teacher's supervision and reducing the entropy of
student's prediction further facilitate W2SG. In addition, we show that the
reverse cross-entropy loss, unlike the standard forward cross-entropy, is less
sensitive to the predictive uncertainty of the teacher. Finally, we empirically
verify our theoretical insights and demonstrate that incorporating the reverse
cross-entropy loss consistently improves student performance.

</details>


### [107] [ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving](https://arxiv.org/abs/2505.24317)
*Yongming Chen,Miner Chen,Liewen Liao,Mingyang Jiang,Xiang Zuo,Hengrui Zhang,Yuchen Xi,Songan Zhang*

Main category: cs.LG

TL;DR: This paper proposes a responsibility-oriented reward function in reinforcement learning for autonomous driving by incorporating traffic regulations through a Traffic Regulation Knowledge Graph and Vision-Language Models, improving decision-making performance and reducing rule violations.


<details>
  <summary>Details</summary>
Motivation: Reward functions in RL for autonomous driving are difficult to design effectively due to reliance on manual design and inefficacy in complex scenarios.

Method: Introduction of a responsibility-oriented reward function that integrates traffic regulations via a Traffic Regulation Knowledge Graph and Vision-Language Models with Retrieval-Augmented Generation techniques to automate reward assignment.

Result: The methodology improves the accuracy of assigning accident responsibilities and reduces the agent's liability in traffic incidents.

Conclusion: Incorporating traffic regulations into the RL framework via automated reward assignment enhances robustness and optimizes decision-making in autonomous driving.

Abstract: Reinforcement learning (RL) in autonomous driving employs a trial-and-error
mechanism, enhancing robustness in unpredictable environments. However,
crafting effective reward functions remains challenging, as conventional
approaches rely heavily on manual design and demonstrate limited efficacy in
complex scenarios. To address this issue, this study introduces a
responsibility-oriented reward function that explicitly incorporates traffic
regulations into the RL framework. Specifically, we introduced a Traffic
Regulation Knowledge Graph and leveraged Vision-Language Models alongside
Retrieval-Augmented Generation techniques to automate reward assignment. This
integration guides agents to adhere strictly to traffic laws, thus minimizing
rule violations and optimizing decision-making performance in diverse driving
conditions. Experimental validations demonstrate that the proposed methodology
significantly improves the accuracy of assigning accident responsibilities and
effectively reduces the agent's liability in traffic incidents.

</details>


### [108] [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/abs/2505.24324)
*Ivan Petrukha,Yana Kurliak,Nataliia Stulova*

Main category: cs.LG

TL;DR: In recent years, large language models have made significant progress in code generation. However, most evaluation benchmarks are mainly aimed at Python, making it difficult to evaluate other programming languages such as Swift. The paper presents SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems, and evaluates 44 popular Code LLMs on it.


<details>
  <summary>Details</summary>
Motivation: Most evaluation benchmarks for large language models are primarily oriented towards Python, making it difficult to evaluate other programming languages such as Swift with high quality.

Method: The authors identified critical issues specific to Swift components in widely established multilingual benchmarks like HumanEval-XL and MultiPL-E. They adopted a quality-over-quantity methodology and presented SwiftEval, the first Swift-oriented benchmark consisting of 28 carefully hand-crafted problems.

Result: Significant LLM scores drop for problems requiring language-specific features, especially noticeable in the models of smaller sizes.

Conclusion: SwiftEval provides a high-quality benchmark for evaluating large language models' coding capabilities on Swift.

Abstract: In recent years, large language models (LLMs) have showcased significant
advancements in code generation. However, most evaluation benchmarks are
primarily oriented towards Python, making it difficult to evaluate other
programming languages, such as Swift, with high quality. By examining widely
established multilingual benchmarks like HumanEval-XL and MultiPL-E, we
identified critical issues specific to their Swift components, making them
insufficient or even irrelevant for assessing LLM coding capabilities on Swift.
Unlike these existing approaches, which prioritize rapid scaling and
generalization by automatically translating Python-centric benchmarks with
LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the
first Swift-oriented benchmark consisting of 28 carefully hand-crafted
problems, and evaluate 44 popular Code LLMs on it. Our results show significant
LLM scores drop for problems requiring language-specific features, most
noticeable in the models of smaller sizes.

</details>


### [109] [Cartan Networks: Group theoretical Hyperbolic Deep Learning](https://arxiv.org/abs/2505.24353)
*Federico Milanesio,Matteo Santoro,Pietro G. Fré,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出了一种新的双曲深度学习算法——Cartan网络，通过结合群同态和度量保持的微分同胚，在多个基准数据集上表现良好，并为双曲深度学习架构开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 利用双曲空间的可解群结构和对称空间构造的自然属性，探索其Lie群和Riemann流形的双重特性，以改进层次数据的嵌入方法。

Method: 将群同态与度量保持的微分同胚相结合，构建出一类新的双曲深度学习算法，称为Cartan网络。

Result: 在各种基准数据集上取得了有希望的结果，展示了算法的有效性。

Conclusion: Cartan网络的成功应用为双曲深度学习领域提供了新的研究方向和架构可能性。

Abstract: Hyperbolic deep learning leverages the metric properties of hyperbolic spaces
to develop efficient and informative embeddings of hierarchical data. Here, we
focus on the solvable group structure of hyperbolic spaces, which follows
naturally from their construction as symmetric spaces. This dual nature of Lie
group and Riemannian manifold allows us to propose a new class of hyperbolic
deep learning algorithms where group homomorphisms are interleaved with
metric-preserving diffeomorphisms. The resulting algorithms, which we call
Cartan networks, show promising results on various benchmark data sets and open
the way to a novel class of hyperbolic deep learning architectures.

</details>


### [110] [ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](https://arxiv.org/abs/2505.24357)
*Xianglong Yan,Zhiteng Li,Tianao Zhang,Linghe Kong,Yulun Zhang,Xiaokang Yang*

Main category: cs.LG

TL;DR: ReCalKV is a post-training KV cache compression method for large language models that reduces hidden dimensions of the KV cache through distinct strategies for Keys and Values, achieving high compression ratios with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Large language models face challenges in long-context reasoning due to the excessive memory required to store the Key-Value (KV) cache. Existing methods for reducing the hidden dimensions of the KV cache either introduce additional computation or suffer from significant performance degradation under high compression ratios.

Method: The paper proposes ReCalKV, which includes two main strategies: Head-wise Similarity-aware Reordering (HSR) for Keys and Offline Calibration and Matrix Fusion (OCMF) for Values. HSR clusters similar heads and applies grouped SVD to the key projection matrix, while OCMF preserves accuracy without extra computational overhead.

Result: Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss.

Conclusion: ReCalKV provides an effective solution for KV cache compression in large language models, enabling more efficient long-context reasoning.

Abstract: Large language models (LLMs) have achieved remarkable performance, yet their
capability on long-context reasoning is often constrained by the excessive
memory required to store the Key-Value (KV) cache. This makes KV cache
compression an essential step toward enabling efficient long-context reasoning.
Recent methods have explored reducing the hidden dimensions of the KV cache,
but many introduce additional computation through projection layers or suffer
from significant performance degradation under high compression ratios. To
address these challenges, we propose ReCalKV, a post-training KV cache
compression method that reduces the hidden dimensions of the KV cache. We
develop distinct compression strategies for Keys and Values based on their
different roles and varying importance in the attention mechanism. For Keys, we
propose Head-wise Similarity-aware Reordering (HSR), which clusters similar
heads and applies grouped SVD to the key projection matrix, reducing additional
computation while preserving accuracy. For Values, we propose Offline
Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra
computational overhead. Experiments show that ReCalKV outperforms existing
low-rank compression methods, achieving high compression ratios with minimal
performance loss. Code is available at:
https://github.com/XIANGLONGYAN/ReCalKV.

</details>


### [111] [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360)
*Stepan Shabalin,Ayush Panda,Dmitrii Kharlapenko,Abdur Raheem Ali,Yixiong Hao,Arthur Conmy*

Main category: cs.LG

TL;DR: Sparse Autoencoders (SAEs) and Inference-Time Decomposition of Activations (ITDA) are applied to a large text-to-image diffusion model, Flux 1. SAEs accurately reconstruct residual stream embeddings and outperform MLP neurons on interpretability. SAE features can steer image generation through activation addition. ITDA has similar interpretability to SAEs.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of Sparse Autoencoders (SAEs) and Inference-Time Decomposition of Activations (ITDA) in interpreting and controlling large text-to-image diffusion models, particularly Flux 1.

Method: Applied Sparse Autoencoders (SAEs) and Inference-Time Decomposition of Activations (ITDA) to the Flux 1 text-to-image diffusion model. Introduced a visual automated interpretation pipeline to assess the interpretability of embeddings from both methods.

Result: SAEs accurately reconstructed residual stream embeddings and demonstrated better interpretability than MLP neurons. SAE features were used to successfully steer image generation through activation addition. ITDA showed comparable interpretability to SAEs.

Conclusion: Sparse Autoencoders and ITDA are effective for interpreting large text-to-image diffusion models like Flux 1, with SAEs showing strong reconstruction and interpretability capabilities, and ITDA being similarly interpretable.

Abstract: Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.

</details>


### [112] [Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm](https://arxiv.org/abs/2505.24365)
*Vardhan Shorewala,Shivam Shorewala*

Main category: cs.LG

TL;DR: This paper proposes a unified approach for cluster refinement and anomaly detection, which reduces intra-cluster variance more effectively than k-means, as demonstrated by evaluations using various indices and external validation on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To develop a method that refines clusters more tightly than the standard k-means algorithm and extends it to detect anomalies in datasets.

Method: A novel algorithm iteratively decreases intra-cluster variance of N clusters until reaching a global minimum. Anomaly detection is achieved by identifying points causing significant variance increase.

Result: The algorithm reduced variance by 18.7% and 88.1% on synthetic and Wine Quality datasets respectively, with accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality dataset.

Conclusion: The proposed approach effectively refines clusters and detects anomalies, outperforming k-means in terms of variance reduction and classification performance.

Abstract: This paper introduces a unified approach to cluster refinement and anomaly
detection in datasets. We propose a novel algorithm that iteratively reduces
the intra-cluster variance of N clusters until a global minimum is reached,
yielding tighter clusters than the standard k-means algorithm. We evaluate the
method using intrinsic measures for unsupervised learning, including the
silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and
extend it to anomaly detection by identifying points whose assignment causes a
significant variance increase. External validation on synthetic data and the
UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity
score, V-measure, and F1 score. Results show variance reductions of 18.7% and
88.1% on the synthetic and Wine Quality datasets, respectively, along with
accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality
dataset.

</details>


### [113] [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/abs/2505.24369)
*Yuanfu Wang,Pengyu Wang,Chenyang Xi,Bo Tang,Junyi Zhu,Wenqiang Wei,Chen Chen,Chao Yang,Jingfeng Zhang,Chaochao Lu,Yijun Niu,Keming Mao,Zhiyu Li,Feiyu Xiong,Jie Hu,Mingchuan Yang*

Main category: cs.LG

TL;DR: Adversarial Preference Learning (APL) is developed to enhance the robustness of language models against adversarial attacks by introducing a direct harmfulness metric, conditional generative attacker, and iterative framework. Experiments show APL significantly improves harmlessness while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: Modern language models using RLHF are still vulnerable to adversarial attacks due to inefficiency and cost of human annotation, diversity of attacks, and risk of feedback bias and reward hacking.

Method: APL incorporates three innovations: a direct harmfulness metric based on intrinsic preference probabilities, a conditional generative attacker for synthesizing adversarial variations, and an iterative closed-loop feedback framework for continuous adaptation.

Result: APL achieves 83.33% harmlessness win rate, reduces harmful outputs from 5.88% to 0.43%, lowers attack success rate by up to 65%, maintains competitive utility with MT-Bench score of 6.59 and LC-WinRate of 46.52%.

Conclusion: APL significantly enhances the robustness of language models against adversarial attacks while preserving their utility.

Abstract: Modern language models often rely on Reinforcement Learning from Human
Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to
adversarial attacks due to three key limitations: (1) the inefficiency and high
cost of human annotation, (2) the vast diversity of potential adversarial
attacks, and (3) the risk of feedback bias and reward hacking. To address these
challenges, we introduce Adversarial Preference Learning (APL), an iterative
adversarial training method incorporating three key innovations. First, a
direct harmfulness metric based on the model's intrinsic preference
probabilities, eliminating reliance on external assessment. Second, a
conditional generative attacker that synthesizes input-specific adversarial
variations. Third, an iterative framework with automated closed-loop feedback,
enabling continuous adaptation through vulnerability discovery and mitigation.
Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly
enhances robustness, achieving 83.33% harmlessness win rate over the base model
(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured
by LLaMA-Guard), and lowering attack success rate by up to 65% according to
HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score
of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against
the base model.

</details>


### [114] [Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer](https://arxiv.org/abs/2505.24378)
*Yilun Kong,Guozheng Ma,Qi Zhao,Haoyu Wang,Li Shen,Xueqian Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper proposes M3DT, a mixture-of-experts framework for multi-task reinforcement learning that improves performance and task scalability.


<details>
  <summary>Details</summary>
Motivation: Current multi-task reinforcement learning methods struggle with scaling to a large number of tasks; simply increasing parameters does not solve the issue of performance degradation as task numbers grow.

Method: M3DT enhances Decision Transformer with a mixture-of-experts approach to reduce task load on parameter subsets and introduces a three-stage training mechanism for efficient optimization.

Result: M3DT shows consistent performance improvement with increased number of experts and successfully scales to 160 tasks with superior results.

Conclusion: M3DT addresses the challenge of task scalability in multi-task reinforcement learning by leveraging model parameter scalability through a mixture-of-experts framework.

Abstract: Despite recent advancements in offline multi-task reinforcement learning
(MTRL) have harnessed the powerful capabilities of the Transformer
architecture, most approaches focus on a limited number of tasks, with scaling
to extremely massive tasks remaining a formidable challenge. In this paper, we
first revisit the key impact of task numbers on current MTRL method, and
further reveal that naively expanding the parameters proves insufficient to
counteract the performance degradation as the number of tasks escalates.
Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE)
framework that tackles task scalability by further unlocking the model's
parameter scalability. Specifically, we enhance both the architecture and the
optimization of the agent, where we strengthen the Decision Transformer (DT)
backbone with MoE to reduce task load on parameter subsets, and introduce a
three-stage training mechanism to facilitate efficient training with optimal
performance. Experimental results show that, by increasing the number of
experts, M3DT not only consistently enhances its performance as model expansion
on the fixed task numbers, but also exhibits remarkable task scalability,
successfully extending to 160 tasks with superior performance.

</details>


### [115] [LightSAM: Parameter-Agnostic Sharpness-Aware Minimization](https://arxiv.org/abs/2505.24399)
*Yifei Cheng,Li Shen,Hao Sun,Nan Yin,Xiaochun Cao,Enhong Chen*

Main category: cs.LG

TL;DR: LightSAM是一种改进的SAM算法，它通过使用自适应优化器（如AdaGrad-Norm、AdaGrad和Adam）替代SGD优化器，从而能够自适应地设置扰动半径和学习率，减少了参数敏感性并扩展了SAM的应用范围。理论分析表明，LightSAM在较弱假设下可以实现理想收敛，并且对扰动半径和学习率的选择具有鲁棒性。实验结果验证了LightSAM的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管Sharpness-Aware Minimization (SAM) 在提高模型泛化能力方面取得了成功，但它存在对扰动半径这一超参数的敏感性问题，同时其扰动半径和学习率受问题相关参数约束以保证收敛。这导致实际应用中需要进行参数调优，增加了复杂性。因此，研究者希望设计一种无需手动调参的SAM变体，进一步扩展其应用范围。

Method: 提出了一种名为LightSAM的新算法，该算法采用三种自适应优化器（AdaGrad-Norm、AdaGrad和Adam）分别替换SAM中原有的SGD优化器用于权重扰动和模型更新。这种方法降低了对超参数（如扰动半径和学习率）的敏感性，使得算法在理论上能够以任何参数选择实现理想收敛，达到参数无关的效果。

Result: 理论分析表明，在较弱假设条件下，LightSAM可以实现理想收敛，且对扰动半径和学习率的选择具有鲁棒性。初步实验结果在多个深度学习任务上验证了LightSAM的有效性。

Conclusion: LightSAM通过引入自适应优化器解决了SAM对超参数敏感的问题，扩大了SAM的实际应用范围，同时保持了良好的收敛性和有效性。

Abstract: Sharpness-Aware Minimization (SAM) optimizer enhances the generalization
ability of the machine learning model by exploring the flat minima landscape
through weight perturbations. Despite its empirical success, SAM introduces an
additional hyper-parameter, the perturbation radius, which causes the
sensitivity of SAM to it. Moreover, it has been proved that the perturbation
radius and learning rate of SAM are constrained by problem-dependent parameters
to guarantee convergence. These limitations indicate the requirement of
parameter-tuning in practical applications. In this paper, we propose the
algorithm LightSAM which sets the perturbation radius and learning rate of SAM
adaptively, thus extending the application scope of SAM. LightSAM employs three
popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to
replace the SGD optimizer for weight perturbation and model updating, reducing
sensitivity to parameters. Theoretical results show that under weak
assumptions, LightSAM could converge ideally with any choices of perturbation
radius and learning rate, thus achieving parameter-agnostic. We conduct
preliminary experiments on several deep learning tasks, which together with the
theoretical findings validate the the effectiveness of LightSAM.

</details>


### [116] [On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets](https://arxiv.org/abs/2505.24403)
*Giannis Nikolentzos,Konstantinos Skianis*

Main category: cs.LG

TL;DR: This paper explores the Lipschitz continuity of aggregation functions in neural networks designed for set or multiset data, computing their constants and deriving upper bounds for network Lipschitz constants, with empirical validation.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the robustness and generalization of neural networks that process set or multiset data by analyzing the Lipschitz continuity of aggregation functions used in these networks.

Method: Investigate the Lipschitz continuity of common aggregation functions (sum, mean, max) with respect to three distance functions for unordered multisets. Derive upper bounds on the Lipschitz constant for neural networks processing multisets and study stability and generalization under distribution shifts.

Result: Found that each aggregation function is Lipschitz continuous with respect to only one of the three distance functions. Derived theoretical upper bounds on the Lipschitz constant of neural networks for multisets and empirically verified these findings through experiments on datasets from various domains.

Conclusion: Aggregation functions in neural networks for set/multiset data have specific Lipschitz properties depending on the distance metric used. These properties can be leveraged to better understand and enhance the robustness and generalization of such networks.

Abstract: The Lipschitz constant of a neural network is connected to several important
properties of the network such as its robustness and generalization. It is thus
useful in many settings to estimate the Lipschitz constant of a model. Prior
work has focused mainly on estimating the Lipschitz constant of multi-layer
perceptrons and convolutional neural networks. Here we focus on data modeled as
sets or multisets of vectors and on neural networks that can handle such data.
These models typically apply some permutation invariant aggregation function,
such as the sum, mean or max operator, to the input multisets to produce a
single vector for each input sample. In this paper, we investigate whether
these aggregation functions are Lipschitz continuous with respect to three
distance functions for unordered multisets, and we compute their Lipschitz
constants. In the general case, we find that each aggregation function is
Lipschitz continuous with respect to only one of the three distance functions.
Then, we build on these results to derive upper bounds on the Lipschitz
constant of neural networks that can process multisets of vectors, while we
also study their stability to perturbations and generalization under
distribution shifts. To empirically verify our theoretical analysis, we conduct
a series of experiments on datasets from different domains.

</details>


### [117] [Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data](https://arxiv.org/abs/2505.24413)
*Yang Sui,Qi Xu,Yang Bai,Annie Qu*

Main category: cs.LG

TL;DR: The paper proposes a two-step Multi-task learning (MTL) strategy to handle various heterogeneities in MTL problems.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Multi-task learning struggle to address different forms of heterogeneity simultaneously in a unified framework, such as block-wise, distribution, and posterior heterogeneity.

Method: The method involves imputing missing blocks using shared representations from homogeneous sources across tasks, then disentangling the mappings between input features and responses into a shared component and a task-specific component.

Result: Numerical experiments and real-data analysis from the ADNI database show that the proposed method performs better than other competing methods in Multi-task learning.

Conclusion: The proposed two-step learning strategy for Multi-task learning effectively addresses various forms of heterogeneity and shows superior performance.

Abstract: Multi-task learning (MTL) has emerged as an imperative machine learning tool
to solve multiple learning tasks simultaneously and has been successfully
applied to healthcare, marketing, and biomedical fields. However, in order to
borrow information across different tasks effectively, it is essential to
utilize both homogeneous and heterogeneous information. Among the extensive
literature on MTL, various forms of heterogeneity are presented in MTL
problems, such as block-wise, distribution, and posterior heterogeneity.
Existing methods, however, struggle to tackle these forms of heterogeneity
simultaneously in a unified framework. In this paper, we propose a two-step
learning strategy for MTL which addresses the aforementioned heterogeneity.
First, we impute the missing blocks using shared representations extracted from
homogeneous source across different tasks. Next, we disentangle the mappings
between input features and responses into a shared component and a
task-specific component, respectively, thereby enabling information borrowing
through the shared component. Our numerical experiments and real-data analysis
from the ADNI database demonstrate the superior MTL performance of the proposed
method compared to other competing methods.

</details>


### [118] [Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation](https://arxiv.org/abs/2505.24415)
*Andreas Spilz,Heiko Oppel,Michael Munz*

Main category: cs.LG

TL;DR: Automated movement evaluation via IMU data is crucial for physiotherapy and sports training. However, deep learning models face challenges like limited data, class imbalance, and label ambiguity. This paper introduces a new data augmentation method that uses musculoskeletal simulations to generate realistic IMU data with biomechanically plausible labels, improving classification accuracy and generalization of neural network models.


<details>
  <summary>Details</summary>
Motivation: To enhance physiotherapeutic treatments and sports training through automated evaluation of movement quality by addressing the limitations faced by deep learning models such as limited data availability, class imbalance, and label ambiguity in IMU data-based movement assessments.

Method: A novel data augmentation method was developed to generate realistic IMU data using musculoskeletal simulations with systematic modifications of movement trajectories, ensuring biomechanical plausibility and reliable labeling through inverse kinematic parameters and knowledge-based evaluation strategy.

Result: The augmented data closely resembles real-world data, leading to significant improvements in classification accuracy and generalization capability of neural network models, also proving beneficial for patient-specific fine-tuning when subject-specific training examples are limited.

Conclusion: The proposed data augmentation method effectively overcomes common challenges in deep learning applications for physiotherapeutic exercise evaluation, demonstrating practicality and efficacy.

Abstract: Automated evaluation of movement quality holds significant potential for
enhancing physiotherapeutic treatments and sports training by providing
objective, real-time feedback. However, the effectiveness of deep learning
models in assessing movements captured by inertial measurement units (IMUs) is
often hampered by limited data availability, class imbalance, and label
ambiguity. In this work, we present a novel data augmentation method that
generates realistic IMU data using musculoskeletal simulations integrated with
systematic modifications of movement trajectories. Crucially, our approach
ensures biomechanical plausibility and allows for automatic, reliable labeling
by combining inverse kinematic parameters with a knowledge-based evaluation
strategy. Extensive evaluations demonstrate that augmented variants closely
resembles real-world data, significantly improving the classification accuracy
and generalization capability of neural network models. Additionally, we
highlight the benefits of augmented data for patient-specific fine-tuning
scenarios, particularly when only limited subject-specific training examples
are available. Our findings underline the practicality and efficacy of this
augmentation method in overcoming common challenges faced by deep learning
applications in physiotherapeutic exercise evaluation.

</details>


### [119] [Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning](https://arxiv.org/abs/2505.24424)
*Amit Peleg,Naman Deep Singh,Matthias Hein*

Main category: cs.LG

TL;DR: CLIP在分类和检索方面表现出显著的零样本能力，但其组合推理能力仍然不足。本文提出了一种基于CLIP的微调方法CLIC，通过结合多张图像及其相关标题的新颖训练技术，提高模型的组合性、词汇和语义理解能力，并在检索性能上取得一致提升，成为SugarCrepe++基准上表现最佳的组合性CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（如CLIP）虽然在分类和检索任务中表现出色，但在组合推理能力方面存在不足，特别是在理解和处理概念之间的关系时。此外，尽管预期提高组合性应增强检索性能，但实际上下游检索性能往往会下降。

Method: 引入了CLIC（Compositionally-aware Learning in CLIP），这是一种基于CLIP的微调方法，采用一种新颖的训练技术，将多张图像及其关联的标题结合起来进行训练。该方法不仅提高了模型在不同架构和预训练CLIP模型上的组合性，还在词汇和语义理解方面取得了进展。

Result: 通过CLIC微调后的模型在检索性能上取得了一致的提升，尤其是在组合性方面，成为了SugarCrepe++基准上表现最佳的组合性CLIP模型。即使对于已经实现SOTA检索性能的最新CLIP模型，CLIC也能进一步提升其检索性能。

Conclusion: CLIC提供了一种有效的方法来增强CLIP模型的组合性、词汇和语义理解能力，同时改善了检索性能，为视觉-语言模型的进一步发展提供了新的方向。

Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io

</details>


### [120] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/abs/2505.24434)
*Md Shahriar Rahim Siddiqui,Moshe Eliasof,Eldad Haber*

Main category: cs.LG

TL;DR: Graph Flow Matching (GFM) enhances traditional flow matching by incorporating a diffusion term that considers neighboring point information, improving image generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing flow matching networks predict velocity independently for each point without considering correlations between points along the trajectory, which may limit the quality of downstream generation.

Method: GFM decomposes the learned velocity into a reaction term (standard flow matching network) and a diffusion term (graph neural module aggregating neighbor information). This approach operates in the latent space of a pretrained variational autoencoder.

Result: GFM consistently improves Fréchet Inception Distance (FID) and recall across five image generation benchmarks (LSUN Church, LSUN Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$).

Conclusion: GFM is an effective modular enhancement to existing flow matching architectures, enriching velocity predictions with local context while maintaining scalability and minimal additional computational cost.

Abstract: Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [121] [Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs](https://arxiv.org/abs/2505.24438)
*Franziska Heeg,Jonas Sauer,Petra Mutzel,Ingo Scholtes*

Main category: cs.LG

TL;DR: In this paper, researchers address the gap in understanding the causal topology of temporal graphs by introducing a new concept called consistent event graph isomorphism. They also develop a temporal generalization of the Weisfeiler-Leman algorithm and derive a novel message passing scheme for temporal graph neural networks (TGNNs), which performs well in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the observation that an important characteristic of temporal graphs - how the directed arrow of time influences their causal topology - is often neglected by existing temporal graph neural networks (TGNNs). There is a lack of a formal way to analyze the expressive power of TGNNs due to the absence of a generalized graph isomorphism concept applicable to temporal graphs.

Method: The authors introduce the notion of consistent event graph isomorphism, which uses a time-unfolded representation of time-respecting paths in temporal graphs. They compare this definition with existing notions of temporal graph isomorphisms and develop a temporal generalization of the Weisfeiler-Leman algorithm to distinguish non-isomorphic temporal graphs. Based on this theoretical foundation, they derive a new message passing scheme for TGNNs that operates on the event graph representation of temporal graphs.

Result: The experimental evaluation indicates that the proposed approach performs well in a temporal graph classification experiment.

Conclusion: This study provides a novel method to understand and enhance the expressive power of TGNNs through the introduction of consistent event graph isomorphism and a new message passing scheme.

Abstract: An important characteristic of temporal graphs is how the directed arrow of
time influences their causal topology, i.e., which nodes can possibly influence
each other causally via time-respecting paths. The resulting patterns are often
neglected by temporal graph neural networks (TGNNs). To formally analyze the
expressive power of TGNNs, we lack a generalization of graph isomorphism to
temporal graphs that fully captures their causal topology. Addressing this gap,
we introduce the notion of consistent event graph isomorphism, which utilizes a
time-unfolded representation of time-respecting paths in temporal graphs. We
compare this definition with existing notions of temporal graph isomorphisms.
We illustrate and highlight the advantages of our approach and develop a
temporal generalization of the Weisfeiler-Leman algorithm to heuristically
distinguish non-isomorphic temporal graphs. Building on this theoretical
foundation, we derive a novel message passing scheme for temporal graph neural
networks that operates on the event graph representation of temporal graphs. An
experimental evaluation shows that our approach performs well in a temporal
graph classification experiment.

</details>


### [122] [Learning Safety Constraints for Large Language Models](https://arxiv.org/abs/2505.24445)
*Xin Chen,Yarden As,Andreas Krause*

Main category: cs.LG

TL;DR: Large language models (LLMs) have safety risks. The paper proposes SaP, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space.


<details>
  <summary>Details</summary>
Motivation: To address significant safety risks of LLMs through harmful outputs and vulnerability to adversarial attacks.

Method: Propose SaP (Safety Polytope), which identifies safe and unsafe regions via the polytope's facets, enabling detection and correction of unsafe outputs through geometric steering. It operates post-hoc in the representation space, not modifying model weights.

Result: Experiments across multiple LLMs show effective detection of unethical inputs, reduction in adversarial attack success rates while maintaining performance on standard tasks.

Conclusion: SaP provides an explicit geometric model for safety, with interpretable insights into how safety is captured in LLMs' representation space.

Abstract: Large language models (LLMs) have emerged as powerful tools but pose
significant safety risks through harmful outputs and vulnerability to
adversarial attacks. We propose SaP, short for Safety Polytope, a geometric
approach to LLM safety that learns and enforces multiple safety constraints
directly in the model's representation space. We develop a framework that
identifies safe and unsafe regions via the polytope's facets, enabling both
detection and correction of unsafe outputs through geometric steering. Unlike
existing approaches that modify model weights, SaP operates post-hoc in the
representation space, preserving model capabilities while enforcing safety
constraints. Experiments across multiple LLMs demonstrate that our method can
effectively detect unethical inputs, reduce adversarial attack success rates
while maintaining performance on standard tasks, thus highlighting the
importance of having an explicit geometric model for safety. Analysis of the
learned polytope facets reveals emergence of specialization in detecting
different semantic notions of safety, providing interpretable insights into how
safety is captured in LLMs' representation space.

</details>


### [123] [Stepsize anything: A unified learning rate schedule for budgeted-iteration training](https://arxiv.org/abs/2505.24452)
*Anda Tang,Yiming Dong,Yutao Zeng,zhou Xun,Zhouchen Lin*

Main category: cs.LG

TL;DR: This paper proposes Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.


<details>
  <summary>Details</summary>
Motivation: The expanding computational costs and limited resources highlight the need for budgeted-iteration training. Current learning rate schedules lack theoretical foundations and require extensive trial-and-error selection.

Method: The authors construct a novel training budget-aware optimization framework to derive the UBA schedule, controlled by a single hyper-parameter φ that balances flexibility and simplicity. They also establish a theoretical connection between φ and the condition number and prove the convergence for different values of φ.

Result: Extensive experimental results show that UBA consistently surpasses commonly-used schedules across diverse vision and language tasks, spanning network architectures and scales, under different training-iteration budgets.

Conclusion: The UBA schedule is a theoretically grounded learning rate schedule that outperforms commonly-used schedules in various scenarios, eliminating the need for per-network numerical optimization.

Abstract: The expanding computational costs and limited resources underscore the
critical need for budgeted-iteration training, which aims to achieve optimal
learning within predetermined iteration budgets.While learning rate schedules
fundamentally govern the performance of different networks and tasks,
particularly in budgeted-iteration scenarios, their design remains largely
heuristic, lacking theoretical foundations.In addition, the optimal learning
rate schedule requires extensive trial-and-error selection, making the training
process inefficient.In this work, we propose the Unified Budget-Aware (UBA)
schedule, a theoretically grounded learning rate schedule that consistently
outperforms commonly-used schedules among diverse architectures and tasks under
different constrained training budgets.First, we bridge the gap by constructing
a novel training budget-aware optimization framework, which explicitly accounts
for the robustness to landscape curvature variations.From this framework, we
derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that
provides a trade-off between flexibility and simplicity, eliminating the need
for per-network numerical optimization. Moreover, we establish a theoretical
connection between $\varphi$ and the condition number, adding interpretation
and justification to our approach. Besides, we prove the convergence for
different values of $\varphi$.We offer practical guidelines for its selection
via theoretical analysis and empirical results.xtensive experimental results
show that UBA \textit{consistently surpasses} the commonly-used schedules
across diverse vision and language tasks, spanning network architectures (e.g.,
ResNet, OLMo) and scales, under different training-iteration budgets.

</details>


### [124] [Logits-Based Finetuning](https://arxiv.org/abs/2505.24461)
*Jingyao Li,Senqiao Yang,Sitong Wu,Han Shi,Chuanyang Zheng,Hong Xu,Jiaya Jia*

Main category: cs.LG

TL;DR: The paper explores the use of reconstruction-based methods for out-of-distribution (OOD) detection and introduces MOOD, a framework using Masked Image Modeling as a pretext task. It significantly improves OOD detection performance without using any OOD samples.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods using recognition-based approaches tend to learn shortcuts rather than comprehensive representations. This motivates the exploration of alternative methods that can provide a more efficacious prior for learning intrinsic data distributions of the in-distribution dataset.

Method: The method leverages reconstruction-based pretext tasks, specifically Masked Image Modeling, within a framework called MOOD. This approach aims to enhance the model's ability to learn intrinsic data distributions of the in-distribution dataset, improving OOD detection performance.

Result: MOOD outperforms previous state-of-the-art methods in one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It also surpasses 10-shot-per-class outlier exposure OOD detection without including any OOD samples.

Conclusion: Reconstruction-based methods, particularly Masked Image Modeling, provide a generally applicable and effective prior for OOD detection. The proposed MOOD framework demonstrates significant improvements in OOD detection performance.

Abstract: The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection. Codes are available at
https://github.com/JulietLJY/MOOD.

</details>


### [125] [Smooth Model Compression without Fine-Tuning](https://arxiv.org/abs/2505.24469)
*Christina Runkel,Natacha Kuete Meli,Jovita Lukasik,Ander Biguri,Carola-Bibiane Schönlieb,Michael Moeller*

Main category: cs.LG

TL;DR: 通过在训练过程中应用核范数、权重的一阶和二阶导数惩罚，鼓励结构化平滑性，从而提高模型压缩效果，使用SVD压缩方法，在CIFAR-10数据集上对ResNet-18进行压缩，参数减少70%，准确率达到91%。


<details>
  <summary>Details</summary>
Motivation: 大型机器学习模型的压缩和修剪对于实际应用至关重要，但传统的修剪和压缩技术没有考虑到网络权重的结构，限制了其有效性。因此，探索平滑正则化对神经网络训练和模型压缩的影响是必要的。

Method: 在训练过程中应用核范数、权重的一阶和二阶导数惩罚，以鼓励结构化平滑性，并保持预测性能与非平滑模型相当。然后，使用奇异值分解（SVD）为基础的压缩方法，利用底层的平滑结构，用较小的低秩张量近似模型的权重张量。

Result: 标准修剪方法在应用于这些平滑模型时通常表现更好。该方法实现了最先进的压缩效果，无需任何微调，例如在CIFAR-10上的平滑ResNet-18模型中，参数减少了70%，准确率达到91%。

Conclusion: 通过引入平滑正则化并结合SVD压缩方法，可以显著提高模型压缩的效果，同时保持较高的预测性能。

Abstract: Compressing and pruning large machine learning models has become a critical
step towards their deployment in real-world applications. Standard pruning and
compression techniques are typically designed without taking the structure of
the network's weights into account, limiting their effectiveness. We explore
the impact of smooth regularization on neural network training and model
compression. By applying nuclear norm, first- and second-order derivative
penalties of the weights during training, we encourage structured smoothness
while preserving predictive performance on par with non-smooth models. We find
that standard pruning methods often perform better when applied to these smooth
models. Building on this observation, we apply a
Singular-Value-Decomposition-based compression method that exploits the
underlying smooth structure and approximates the model's weight tensors by
smaller low-rank tensors. Our approach enables state-of-the-art compression
without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18
on CIFAR-10 with $70\%$ fewer parameters.

</details>


### [126] [Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy](https://arxiv.org/abs/2505.24473)
*Nikita Balagansky,Yaroslav Aksenov,Daniil Laptev,Vadim Kurochkin,Gleb Gerasimov,Nikita Koryagin,Daniil Gavrilov*

Main category: cs.LG

TL;DR: Sparse Autoencoders (SAEs) usually have a fixed sparsity level during training, which leads to the need for separate models to meet different sparsity requirements. This paper introduces HierarchicalTopK, a new training objective that allows a single SAE to optimize reconstructions across multiple sparsity levels simultaneously.


<details>
  <summary>Details</summary>
Motivation: Conventional Sparse Autoencoders are constrained by the fixed sparsity level chosen during training and require separate models to meet different sparsity requirements, increasing computational cost.

Method: The authors introduce a novel training objective called HierarchicalTopK, which enables a single Sparse Autoencoder to optimize reconstructions across multiple sparsity levels at once.

Result: Experiments using Gemma-2 2B show that this approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Additionally, HierarchicalTopK maintains high interpretability scores even at higher sparsity levels.

Conclusion: HierarchicalTopK bridges the gap between flexibility and interpretability in Sparse Autoencoder design, allowing for more versatile and efficient models.

Abstract: Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting
neural networks by decomposing hidden representations into disentangled,
interpretable features via sparsity constraints. However, conventional SAEs are
constrained by the fixed sparsity level chosen during training; meeting
different sparsity requirements therefore demands separate models and increases
the computational footprint during both training and evaluation. We introduce a
novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to
optimise reconstructions across multiple sparsity levels simultaneously.
Experiments with Gemma-2 2B demonstrate that our approach achieves
Pareto-optimal trade-offs between sparsity and explained variance,
outperforming traditional SAEs trained at individual sparsity levels. Further
analysis shows that HierarchicalTopK preserves high interpretability scores
even at higher sparsity. The proposed objective thus closes an important gap
between flexibility and interpretability in SAE design.

</details>


### [127] [Object Centric Concept Bottlenecks](https://arxiv.org/abs/2505.24492)
*David Steinmann,Wolfgang Stammer,Antonia Wüst,Kristian Kersting*

Main category: cs.LG

TL;DR: The paper presents Object-Centric Concept Bottlenecks (OCB), a framework that merges concept-based models with pre-trained object-centric foundation models to enhance both performance and interpretability for complex visual tasks.


<details>
  <summary>Details</summary>
Motivation: Current concept-based models (CBMs) are limited in their expressiveness for object-centric real-world settings, which restricts their ability to solve complex vision tasks beyond single-label classification.

Method: The authors introduce Object-Centric Concept Bottlenecks (OCB), which combines the strengths of CBMs and pre-trained object-centric foundation models. They evaluate OCB on complex image datasets and perform an ablation study on key components like strategies for aggregating object-concept encodings.

Result: Experiments show that OCB outperforms traditional CBMs and enables interpretable decisions for complex visual tasks.

Conclusion: OCB is a promising approach to improve both the performance and interpretability of models in complex vision tasks.

Abstract: Developing high-performing, yet interpretable models remains a critical
challenge in modern AI. Concept-based models (CBMs) attempt to address this by
extracting human-understandable concepts from a global encoding (e.g., image
encoding) and then applying a linear classifier on the resulting concept
activations, enabling transparent decision-making. However, their reliance on
holistic image encodings limits their expressiveness in object-centric
real-world settings and thus hinders their ability to solve complex vision
tasks beyond single-label classification. To tackle these challenges, we
introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines
the strengths of CBMs and pre-trained object-centric foundation models,
boosting performance and interpretability. We evaluate OCB on complex image
datasets and conduct a comprehensive ablation study to analyze key components
of the framework, such as strategies for aggregating object-concept encodings.
The results show that OCB outperforms traditional CBMs and allows one to make
interpretable decisions for complex visual tasks.

</details>


### [128] [Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem](https://arxiv.org/abs/2505.24498)
*Andres Fernandez,Juan Azcarreta,Cagdas Bilen,Jesus Monge Alvarez*

Main category: cs.LG

TL;DR: Recent work in online speech spectrogram inversion effectively combines Deep Learning with the Gradient Theorem to predict phase derivatives directly from magnitudes, while this work introduces three innovations that drastically reduce computational cost while maintaining high quality.


<details>
  <summary>Details</summary>
Motivation: To drastically reduce computational cost of online speech spectrogram inversion while maintaining high quality.

Method: Firstly, introduce a novel neural network architecture with just 8k parameters; Secondly, increasing latency by 1 hop size allows to further halve the cost of the neural inference step; Thirdly, propose a linear-complexity solver for the least squares step.

Result: These innovations maintain high quality reconstruction while drastically reducing computational cost.

Conclusion: The three introduced innovations successfully reduce the computational cost of online speech spectrogram inversion while maintaining high quality.

Abstract: Recent work in online speech spectrogram inversion effectively combines Deep
Learning with the Gradient Theorem to predict phase derivatives directly from
magnitudes. Then, phases are estimated from their derivatives via least
squares, resulting in a high quality reconstruction. In this work, we introduce
three innovations that drastically reduce computational cost, while maintaining
high quality: Firstly, we introduce a novel neural network architecture with
just 8k parameters, 30 times smaller than previous state of the art. Secondly,
increasing latency by 1 hop size allows us to further halve the cost of the
neural inference step. Thirdly, we we observe that the least squares problem
features a tridiagonal matrix and propose a linear-complexity solver for the
least squares step that leverages tridiagonality and positive-semidefiniteness,
achieving a speedup of several orders of magnitude. We release samples online.

</details>


### [129] [Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset](https://arxiv.org/abs/2505.24505)
*Ignacio Boero,Santiago Diaz,Tomás Vázquez,Enzo Coppes,Pablo Belzarena,Federico Larroca*

Main category: cs.LG

TL;DR: The paper introduces a publicly available power system dataset from Uruguay and evaluates learning-based ORPD solutions, showing higher prediction errors with real-world data than synthetic data, thus emphasizing the need for more expressive models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing machine learning models in handling the complex statistical properties of real-world grid conditions for Optimal Reactive Power Dispatch (ORPD) problem.

Method: Introducing a public dataset containing structural and operational data of Uruguay's electrical grid and assessing the impact of real-world data on learning-based ORPD solutions.

Result: Learning-based ORPD solutions show significantly higher prediction errors with real-world data compared to synthetic data.

Conclusion: Existing models are limited in handling real grid conditions; there is a need for more expressive architectures, and the provided dataset can help further research.

Abstract: The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in
power system operations, ensuring voltage stability and minimizing power
losses. Recent advances in machine learning, particularly within the ``learning
to optimize'' framework, have enabled fast and efficient approximations of ORPD
solutions, typically by training models on precomputed optimization results.
While these approaches have demonstrated promising performance on synthetic
datasets, their effectiveness under real-world grid conditions remains largely
unexplored. This paper makes two key contributions. First, we introduce a
publicly available power system dataset that includes both the structural
characteristics of Uruguay's electrical grid and nearly two years of real-world
operational data, encompassing actual demand and generation profiles. Given
Uruguay's high penetration of renewable energy, the ORPD problem has become the
primary optimization challenge in its power network. Second, we assess the
impact of real-world data on learning-based ORPD solutions, revealing a
significant increase in prediction errors when transitioning from synthetic to
actual demand and generation inputs. Our results highlight the limitations of
existing models in learning under the complex statistical properties of real
grid conditions and emphasize the need for more expressive architectures. By
providing this dataset, we aim to facilitate further research into robust
learning-based optimization techniques for power system management.

</details>


### [130] [Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting](https://arxiv.org/abs/2505.24511)
*Jiahao Wang,Mingyue Cheng,Qi Liu*

Main category: cs.LG

TL;DR: 研究人员通过提出TimeReasoner，探索了慢思维大语言模型在时间序列预测中的推理能力。他们设计了一系列提示策略，在多个基准上评估这些模型的表现，发现它们具有显著的零样本预测能力，尤其是在捕捉高层次趋势和上下文变化方面。这项初步研究揭示了大语言模型在时间领域中的推理行为，并指出了其潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的时间序列预测方法有效，但通常侧重于快速思维模式，强调模式提取和直接值映射，而忽略了对时间动态和上下文依赖性的显式推理。与此同时，新兴的慢思维大语言模型展示了跨领域的多步骤推理能力，这为将时间序列预测重新定义为结构化推理任务提供了新的机会。

Method: 研究人员提出了TimeReasoner，一个广泛的实证研究，将时间序列预测作为条件推理任务。他们设计了一系列提示策略，以从预训练的慢思维大语言模型中激发推理时的推理能力，并在多个时间序列预测基准上评估这些模型的表现。

Result: 实验结果表明，慢思维大语言模型表现出非平凡的零样本预测能力，特别是在捕捉高层次趋势和上下文变化方面。然而，这项研究仍处于初步阶段。

Conclusion: 这项研究揭示了大语言模型在时间领域中的推理行为，突显了其潜力和局限性。作者希望这项工作能够推动基于推理的预测范式的研究，并为更可解释和通用的时间序列预测框架铺平道路。

Abstract: Time series forecasting (TSF) is a fundamental and widely studied task,
spanning methods from classical statistical approaches to modern deep learning
and multimodal language modeling. Despite their effectiveness, these methods
often follow a fast thinking paradigm emphasizing pattern extraction and direct
value mapping, while overlooking explicit reasoning over temporal dynamics and
contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,
ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning
capabilities across diverse domains, suggesting a new opportunity for reframing
TSF as a structured reasoning task. This motivates a key question: can
slow-thinking LLMs effectively reason over temporal patterns to support time
series forecasting, even in zero-shot manner? To investigate this, in this
paper, we propose TimeReasoner, an extensive empirical study that formulates
TSF as a conditional reasoning task. We design a series of prompting strategies
to elicit inference-time reasoning from pretrained slow-thinking LLMs and
evaluate their performance across diverse TSF benchmarks. Our findings reveal
that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,
especially in capturing high-level trends and contextual shifts. While
preliminary, our study surfaces important insights into the reasoning behaviors
of LLMs in temporal domains highlighting both their potential and limitations.
We hope this work catalyzes further research into reasoning-based forecasting
paradigms and paves the way toward more interpretable and generalizable TSF
frameworks.

</details>


### [131] [Airborne Neural Network](https://arxiv.org/abs/2505.24513)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.LG

TL;DR: The paper introduces Airborne Neural Network, a distributed architecture for real-time learning and inference in aerospace systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying deep learning systems in aerospace domain where real time data processing and ultra low latency are critical but infrastructure limitations exist.

Method: Propose Airborne Neural Network, a distributed architecture where multiple airborne devices each host a subset of neural network neurons and compute collaboratively guided by controllers.

Result: This approach enables large-scale neural network operations in airborne environments which could revolutionize aerospace applications such as air traffic control and geospatial data processing.

Conclusion: Airborne Neural Network lays the foundation for next generation AI powered aerospace systems.

Abstract: Deep Learning, driven by neural networks, has led to groundbreaking
advancements in Artificial Intelligence by enabling systems to learn and adapt
like the human brain. These models have achieved remarkable results,
particularly in data-intensive domains, supported by massive computational
infrastructure. However, deploying such systems in Aerospace, where real time
data processing and ultra low latency are critical, remains a challenge due to
infrastructure limitations. This paper proposes a novel concept: the Airborne
Neural Network a distributed architecture where multiple airborne devices each
host a subset of neural network neurons. These devices compute collaboratively,
guided by an airborne network controller and layer specific controllers,
enabling real-time learning and inference during flight. This approach has the
potential to revolutionize Aerospace applications, including airborne air
traffic control, real-time weather and geographical predictions, and dynamic
geospatial data processing. By enabling large-scale neural network operations
in airborne environments, this work lays the foundation for the next generation
of AI powered Aerospace systems.

</details>


### [132] [Transformers Are Universally Consistent](https://arxiv.org/abs/2505.24531)
*Sagar Ghosh,Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: Transformers with softmax-based nonlinear attention can perform OLS regression uniformly consistently in hyperbolic space, with empirical error bounds decreasing at a rate of $\mathcal{O}(t^{-1/2d})$; results also apply to Euclidean settings.


<details>
  <summary>Details</summary>
Motivation: To determine if Transformers can robustly perform functional regression over sequences of input tokens, especially considering the non-Euclidean geometry of real-world data distributions.

Method: Analyze Transformers equipped with softmax-based nonlinear attention for their ability to execute OLS regression when inputs and outputs are embedded in hyperbolic space, deriving deterministic upper bounds on empirical error.

Result: Theoretical analysis shows uniform consistency for OLS regression with error bounds decaying at a rate of $\mathcal{O}(t^{-1/2d})$, applicable to both hyperbolic and Euclidean spaces.

Conclusion: Transformers with softmax-based nonlinear attention are capable of performing OLS regression uniformly consistently in hyperbolic space, supported by theoretical guarantees and empirical evaluations.

Abstract: Despite their central role in the success of foundational models and
large-scale language modeling, the theoretical foundations governing the
operation of Transformers remain only partially understood. Contemporary
research has largely focused on their representational capacity for language
comprehension and their prowess in in-context learning, frequently under
idealized assumptions such as linearized attention mechanisms. Initially
conceived to model sequence-to-sequence transformations, a fundamental and
unresolved question is whether Transformers can robustly perform functional
regression over sequences of input tokens. This question assumes heightened
importance given the inherently non-Euclidean geometry underlying real-world
data distributions. In this work, we establish that Transformers equipped with
softmax-based nonlinear attention are uniformly consistent when tasked with
executing Ordinary Least Squares (OLS) regression, provided both the inputs and
outputs are embedded in hyperbolic space. We derive deterministic upper bounds
on the empirical error which, in the asymptotic regime, decay at a provable
rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens
and $d$ the embedding dimensionality. Notably, our analysis subsumes the
Euclidean setting as a special case, recovering analogous convergence
guarantees parameterized by the intrinsic dimensionality of the data manifold.
These theoretical insights are corroborated through empirical evaluations on
real-world datasets involving both continuous and categorical response
variables.

</details>


### [133] [Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators](https://arxiv.org/abs/2505.24533)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: The paper presents a novel framework generalizing one-dimensional monoidal systems into higher dimensions, unifying several well-known linear transforms and enabling learnable transformations.


<details>
  <summary>Details</summary>
Motivation: To create a systematic way to derive classic transforms such as the Discrete Fourier Transform (DFT), the Walsh transform, and the Hadamard transform by appropriately choosing vector and matrix pairs.

Method: Introducing a class of algebraic structures that generalize one-dimensional monoidal systems into higher dimensions using per-axis composition operators subject to non-commutativity and a global interchange law. These structures are defined recursively from a base case of vector-matrix pairs.

Result: Classic transforms are shown to be special cases of this algebraic structure, providing a unified framework for deriving these transforms systematically.

Conclusion: This framework not only unifies classical transforms but also enables the development of learnable transformations tailored to specific data modalities and tasks.

Abstract: We introduce a novel framework consisting of a class of algebraic structures
that generalize one-dimensional monoidal systems into higher dimensions by
defining per-axis composition operators subject to non-commutativity and a
global interchange law. These structures, defined recursively from a base case
of vector-matrix pairs, model directional composition in multiple dimensions
while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms
in signal processing and data analysis. In this framework, data indices are
embedded into a composite structure that decomposes into simpler components. We
show that classic transforms such as the Discrete Fourier Transform (DFT), the
Walsh transform, and the Hadamard transform are special cases of our algebraic
structure. The framework provides a systematic way to derive these transforms
by appropriately choosing vector and matrix pairs. By subsuming classical
transforms within a common structure, the framework also enables the
development of learnable transformations tailored to specific data modalities
and tasks.

</details>


### [134] [HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection](https://arxiv.org/abs/2505.24534)
*Florian Frantzen,Michael T. Schaub*

Main category: cs.LG

TL;DR: The paper introduces HLSAD, a new method for detecting anomalies in time-evolving simplicial complexes using Hodge Laplacians.


<details>
  <summary>Details</summary>
Motivation: Traditional graph anomaly detection techniques often fail to capture changes in higher-order interactions that are crucial for identifying complex structural anomalies.

Method: Leveraging the spectral properties of Hodge Laplacians of simplicial complexes to model multi-way interactions among data points and incorporating higher-dimensional simplicial structures into the method.

Result: Outperforms existing graph methods in detecting both events and change points through comprehensive experiments on both synthetic and real-world datasets.

Conclusion: HLSAD enhances both detection accuracy and computational efficiency.

Abstract: In this paper, we propose HLSAD, a novel method for detecting anomalies in
time-evolving simplicial complexes. While traditional graph anomaly detection
techniques have been extensively studied, they often fail to capture changes in
higher-order interactions that are crucial for identifying complex structural
anomalies. These higher-order interactions can arise either directly from the
underlying data itself or through graph lifting techniques. Our approach
leverages the spectral properties of Hodge Laplacians of simplicial complexes
to effectively model multi-way interactions among data points. By incorporating
higher-dimensional simplicial structures into our method, our method enhances
both detection accuracy and computational efficiency. Through comprehensive
experiments on both synthetic and real-world datasets, we demonstrate that our
approach outperforms existing graph methods in detecting both events and change
points.

</details>


### [135] [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
*Narmeen Oozeer,Luke Marks,Fazl Barez,Amirali Abdullah*

Main category: cs.LG

TL;DR: K-Steering是一种新的方法，用于在推理时控制大型语言模型的多个行为属性。它通过训练一个单一的非线性多标签分类器来避免线性假设，移除存储和调整单独属性向量的需求，并允许在不重新训练的情况下动态组合行为。在ToneBank和DebateMix两个新基准上的实证结果表明，K-Steering在准确引导多种行为方面优于强大的基线。


<details>
  <summary>Details</summary>
Motivation: 控制大型语言模型的多个行为属性是一个具有挑战性的问题，因为属性之间存在干扰，而且线性引导方法假设激活空间中的加法行为并需要每属性调整。

Method: 引入了K-Steering方法，该方法在隐藏激活上训练一个单一的非线性多标签分类器，并通过梯度计算干预方向。这避免了线性假设，移除了存储和调整单独属性向量的需求，并允许在不重新训练的情况下动态组合行为。

Result: 在3个模型家族上的实证结果，通过激活基础分类器和LLM基础法官验证，表明K-Steering在准确引导多种行为方面优于强大的基线。

Conclusion: K-Steering提供了一种统一且灵活的方法，有效解决了控制大型语言模型多个行为属性的挑战。

Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at
inference time is a challenging problem due to interference between attributes
and the limitations of linear steering methods, which assume additive behavior
in activation space and require per-attribute tuning. We introduce K-Steering,
a unified and flexible approach that trains a single non-linear multi-label
classifier on hidden activations and computes intervention directions via
gradients at inference time. This avoids linearity assumptions, removes the
need for storing and tuning separate attribute vectors, and allows dynamic
composition of behaviors without retraining. To evaluate our method, we propose
two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral
control. Empirical results across 3 model families, validated by both
activation-based classifiers and LLM-based judges, demonstrate that K-Steering
outperforms strong baselines in accurately steering multiple behaviors.

</details>


### [136] [Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems](https://arxiv.org/abs/2505.24578)
*Abhishek Chandra,Taniya Kapoor,Mitrofan Curti,Koen Tiels,Elena A. Lomonova*

Main category: cs.LG

TL;DR: Neuro-symbolic operator (NSO) framework is proposed to derive analytical operators governing hysteretic relationships in complex piezoelectric systems. It combines a Fourier neural operator and a sparse model discovery method, enabling accurate, interpretable predictions of displacement profiles even for noisy data or out-of-distribution voltages.


<details>
  <summary>Details</summary>
Motivation: Current neural operator methods used for characterizing complex piezoelectric systems suffer from issues related to interpretability and generalizability. State-of-the-art neural operators are black-boxes that do not provide insights into the learned operator and struggle with predicting displacement profiles beyond their training domain.

Method: The NSO framework first uses a Fourier neural operator to learn a mapping from voltage fields to displacement profiles. Then it applies a library-based sparse model discovery method to generate white-box parsimonious models governing the underlying hysteresis.

Result: NSO accurately predicts voltage-displacement hysteresis, including butterfly-shaped relationships, and can handle noisy and low-fidelity voltage data, showing robustness. It outperforms state-of-the-art neural operators and model discovery methods on several evaluation metrics.

Conclusion: The NSO contributes significantly to the characterization of complex piezoelectric systems by enhancing both interpretability and generalizability of neural operators, which is crucial for various real-world applications such as design, monitoring, and maintenance.

Abstract: Complex piezoelectric systems are foundational in industrial applications.
Their performance, however, is challenged by the nonlinear voltage-displacement
hysteretic relationships. Efficient characterization methods are, therefore,
essential for reliable design, monitoring, and maintenance. Recently proposed
neural operator methods serve as surrogates for system characterization but
face two pressing issues: interpretability and generalizability.
State-of-the-art (SOTA) neural operators are black-boxes, providing little
insight into the learned operator. Additionally, generalizing them to novel
voltages and predicting displacement profiles beyond the training domain is
challenging, limiting their practical use. To address these limitations, this
paper proposes a neuro-symbolic operator (NSO) framework that derives the
analytical operators governing hysteretic relationships. NSO first learns a
Fourier neural operator mapping voltage fields to displacement profiles,
followed by a library-based sparse model discovery method, generating white-box
parsimonious models governing the underlying hysteresis. These models enable
accurate and interpretable prediction of displacement profiles across varying
and out-of-distribution voltage fields, facilitating generalizability. The
potential of NSO is demonstrated by accurately predicting voltage-displacement
hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts
displacement profiles even for noisy and low-fidelity voltage data, emphasizing
its robustness. The results highlight the advantages of NSO compared to SOTA
neural operators and model discovery methods on several evaluation metrics.
Consequently, NSO contributes to characterizing complex piezoelectric systems
while improving the interpretability and generalizability of neural operators,
essential for design, monitoring, maintenance, and other real-world scenarios.

</details>


### [137] [Conservation-preserved Fourier Neural Operator through Adaptive Correction](https://arxiv.org/abs/2505.24579)
*Chaoyu Liu,Yangming Li,Zhongying Deng,Chris Budd,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 为了解决标准傅里叶神经算子(FNOs)无法很好地遵循物理守恒定律的问题，本文提出了一种新的自适应校正方法，通过引入可学习矩阵来调整解以满足守恒定律。实验结果表明，该方法在一系列偏微分方程(PDEs)上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管傅里叶神经算子(FNOs)在从数据中学习偏微分方程(PDEs)的数值解方面显示出希望和效率，但它们通常无法保留关键的守恒定律（如质量守恒、动量守恒等），这在准确建模物理系统时是至关重要的。现有的将这些守恒定律纳入傅里叶神经算子的方法要么设计相关损失函数，要么在训练时采用后处理方法，但这些方法不能精确且自适应地校正输出以满足守恒定律，并可能导致性能下降。

Method: 本文提出了一种新的自适应校正方法，通过引入一个可学习矩阵，在训练过程中自适应地调整解以满足守恒定律。这种方法确保了输出完全符合目标守恒定律，同时为模型提供了更多的灵活性和自适应性以校正输出。理论上证明，将此自适应校正应用于无约束的FNO可以得到一个数据损失不比最佳守恒满足型FNO差的解。

Result: 实验结果表明，所提出的方法在一系列具有代表性的PDEs上始终优于其他方法。

Conclusion: 本文提出的自适应校正方法有效地解决了FNO无法精确满足守恒定律的问题，为数值求解PDE提供了一个更精确和灵活的工具。

Abstract: Fourier Neural Operators (FNOs) have recently emerged as a promising and
efficient approach for learning the numerical solutions to partial differential
equations (PDEs) from data. However, standard FNO often fails to preserve key
conservation laws, such as mass conservation, momentum conservation, norm
conservation, etc., which are crucial for accurately modeling physical systems.
Existing methods for incorporating these conservation laws into Fourier neural
operators are achieved by designing related loss function or incorporating
post-processing method at the training time. None of them can both exactly and
adaptively correct the outputs to satisfy conservation laws, and our
experiments show that these methods can lead to inferior performance while
preserving conservation laws. In this work, we propose a novel adaptive
correction approach to ensure the conservation of fundamental quantities. Our
method introduces a learnable matrix to adaptively adjust the solution to
satisfy the conservation law during training. It ensures that the outputs
exactly satisfy the goal conservation law and allow for more flexibility and
adaptivity for the model to correct the outputs. We theoretically show that
applying our adaptive correction to an unconstrained FNO yields a solution with
data loss no worse than that of the best conservation-satisfying FNO. We
compare our approach with existing methods on a range of representative PDEs.
Experiment results show that our method consistently outperform other methods.

</details>


### [138] [AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams](https://arxiv.org/abs/2505.24584)
*Sakhinana Sagar Srinivas,Shivam Gupta,Venkataramana Runkana*

Main category: cs.LG

TL;DR: Recent advancements in generative AI have accelerated chemical and material discovery, but transitioning to industrial-scale production is still a challenge. This paper presents a closed-loop, physics-aware framework that automates the generation of process flow diagrams (PFDs) and piping and instrumentation diagrams (PIDs), which are crucial for scaling chemical processes. The framework integrates specialized small language models with first-principles simulation, and it includes a hierarchical knowledge graph, multi-stage training pipeline, and DWSIM-based validation. Advanced inference optimizations and structural pruning techniques are also applied to improve efficiency and compactness.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the critical bottleneck of transitioning AI-discovered chemicals and materials into industrial-scale production by developing new chemical manufacturing processes. Current AI methods lack the capability to auto-generate PFDs or PIDs while considering engineering constraints.

Method: The method involves creating a closed loop, physics aware framework for automated generation of industrially viable PFDs and PIDs. It integrates domain specialized small scale language models with first principles simulation. Key components include: a hierarchical knowledge graph of process flow and instrumentation descriptions for over 1020 chemicals; a multi-stage training pipeline using Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT); and DWSIM based simulator in the loop validation. Additionally, advanced inference time optimizations and structural pruning techniques are incorporated to enhance runtime efficiency and model compactness.

Result: Experiments show that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes well to unseen chemicals.

Conclusion: This work bridges AI-driven design with industrial-scale feasibility, significantly reducing R&D timelines from lab discovery to plant deployment.

Abstract: Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.

</details>


### [139] [A Flat Minima Perspective on Understanding Augmentations and Model Robustness](https://arxiv.org/abs/2505.24592)
*Weebum Yoo,Sung Whan Yoon*

Main category: cs.LG

TL;DR: The paper presents a unified theoretical framework explaining how data augmentations enhance model robustness, using loss surface flatness and PAC generalization bound, with confirmation via simulations on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive theoretical understanding of why and how data augmentations improve model robustness across different types of distributional shifts.

Method: A unified theoretical framework analyzing the role of data augmentations in enhancing model robustness by considering loss surface flatness and PAC generalization bound. This method covers a wide range of existing augmentation techniques and is not restricted to specific distribution shifts.

Result: The theories proposed in the paper are validated through simulations on common corruption and adversarial robustness benchmarks (CIFAR, ImageNet) and domain generalization benchmarks (PACS, OfficeHome).

Conclusion: This work offers a broad and inclusive theoretical analysis of the efficacy of data augmentations in improving model robustness, beyond specific types of distributional shifts.

Abstract: Model robustness indicates a model's capability to generalize well on
unforeseen distributional shifts, including data corruption, adversarial
attacks, and domain shifts. Data augmentation is one of the prevalent and
effective ways to enhance robustness. Despite the great success of
augmentations in different fields, a general theoretical understanding of their
efficacy in improving model robustness is lacking. We offer a unified
theoretical framework to clarify how augmentations can enhance model robustness
through the lens of loss surface flatness and PAC generalization bound. Our
work diverges from prior studies in that our analysis i) broadly encompasses
much of the existing augmentation methods, and ii) is not limited to specific
types of distribution shifts like adversarial attacks. We confirm our theories
through simulations on the existing common corruption and adversarial
robustness benchmarks based on the CIFAR and ImageNet datasets, as well as
domain generalization benchmarks including PACS and OfficeHome.

</details>


### [140] [Binary Cumulative Encoding meets Time Series Forecasting](https://arxiv.org/abs/2505.24595)
*Andrei Chernov,Vitaliy Pozdnyakov,Ilya Makarov*

Main category: cs.LG

TL;DR: The paper proposes Binary Cumulative Encoding (BCE) for time series forecasting that retains order and magnitude info, outperforming existing methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting methods using classification tasks lack the ability to convey relative distance between predicted and true values due to reliance on one-hot encoding which ignores ordinal structure of values.

Method: Introduced Binary Cumulative Encoding (BCE) that transforms scalar targets into monotonic binary vectors preserving order and magnitude information. Also proposed a specific convolutional neural network architecture incorporating residual and dilated convolutions for effective temporal modeling within the BCE framework.

Result: Experiments on benchmark datasets demonstrate superior performance in both point and probabilistic forecasting compared to widely used methods, along with reduced parameter count and faster training.

Conclusion: Binary Cumulative Encoding coupled with the proposed CNN architecture provides an efficient and effective approach for time series forecasting, advancing the state-of-the-art in this domain.

Abstract: Recent studies in time series forecasting have explored formulating
regression via classification task. By discretizing the continuous target space
into bins and predicting over a fixed set of classes, these approaches benefit
from stable training, robust uncertainty modeling, and compatibility with
modern deep learning architectures. However, most existing methods rely on
one-hot encoding that ignores the inherent ordinal structure of the underlying
values. As a result, they fail to provide information about the relative
distance between predicted and true values during training. In this paper, we
propose to address this limitation by introducing binary cumulative encoding
(BCE), that represents scalar targets into monotonic binary vectors. This
encoding implicitly preserves order and magnitude information, allowing the
model to learn distance-aware representations while still operating within a
classification framework. We propose a convolutional neural network
architecture specifically designed for BCE, incorporating residual and dilated
convolutions to enable fast and expressive temporal modeling. Through extensive
experiments on benchmark forecasting datasets, we show that our approach
outperforms widely used methods in both point and probabilistic forecasting,
while requiring fewer parameters and enabling faster training.

</details>


### [141] [The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches](https://arxiv.org/abs/2505.24603)
*Omri Lev,Vishwak Srinivasan,Moshe Shenfeld,Katrina Ligett,Ayush Sekhari,Ashia C. Wilson*

Main category: cs.LG

TL;DR: The paper revisits Gaussian sketching through the lens of Renyi Differential Privacy (RDP), providing tighter privacy bounds and demonstrating performance improvements in linear regression settings.


<details>
  <summary>Details</summary>
Motivation: Gaussian sketching, which involves pre-multiplying data with a random Gaussian matrix, is a widely used technique in data science and machine learning. It also inherently provides differential privacy guarantees. The motivation is to refine the privacy analysis of this operation using RDP to obtain significantly tighter privacy bounds than prior results.

Method: The authors conduct a refined privacy analysis of Gaussian sketching using Renyi Differential Privacy (RDP). This approach yields significantly tighter privacy bounds compared to previous analyses. They then apply this improved analysis to different linear regression settings.

Result: The improved privacy analysis leads to performance improvements in various linear regression settings. Theoretical utility guarantees are established, and empirical results show that the methods improve performance across multiple datasets, sometimes also reducing runtime.

Conclusion: Gaussian sketching, when analyzed through the lens of RDP, offers significantly tighter privacy bounds. This refined analysis improves performance in linear regression tasks, both theoretically and empirically.

Abstract: Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.

</details>


### [142] [Multi-criteria Rank-based Aggregation for Explainable AI](https://arxiv.org/abs/2505.24612)
*Sujoy Chatterjee,Everton Romanzini Colombo,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: Explainability is crucial for machine learning models. This paper introduces a multi-criteria rank-based weighted aggregation method to balance multiple quality metrics and produce an ensemble of explanation models, also proposing rank-based versions of XAI metrics.


<details>
  <summary>Details</summary>
Motivation: Explainability is important in machine learning models and different explainers can provide contrasting explanations for the same prediction, introducing trade-offs across conflicting quality metrics. Although available aggregation approaches improve robustness, reducing explanations' variability, very limited research employed a multi-criteria decision-making approach.

Method: This paper introduces a multi-criteria rank-based weighted aggregation method that balances multiple quality metrics simultaneously to produce an ensemble of explanation models. It also proposes rank-based versions of existing XAI metrics (complexity, faithfulness and stability) to better evaluate ranked feature importance explanations.

Result: Extensive experiments on publicly available datasets demonstrate the robustness of the proposed model across these metrics. Comparative analyses showed that TOPSIS and WSUM are the best candidates for this use case.

Conclusion: A multi-criteria rank-based weighted aggregation method was introduced which successfully balances multiple quality metrics simultaneously to produce an ensemble of explanation models. Rank-based versions of XAI metrics were also proposed and proved effective.

Abstract: Explainability is crucial for improving the transparency of black-box machine
learning models. With the advancement of explanation methods such as LIME and
SHAP, various XAI performance metrics have been developed to evaluate the
quality of explanations. However, different explainers can provide contrasting
explanations for the same prediction, introducing trade-offs across conflicting
quality metrics. Although available aggregation approaches improve robustness,
reducing explanations' variability, very limited research employed a
multi-criteria decision-making approach. To address this gap, this paper
introduces a multi-criteria rank-based weighted aggregation method that
balances multiple quality metrics simultaneously to produce an ensemble of
explanation models. Furthermore, we propose rank-based versions of existing XAI
metrics (complexity, faithfulness and stability) to better evaluate ranked
feature importance explanations. Extensive experiments on publicly available
datasets demonstrate the robustness of the proposed model across these metrics.
Comparative analyses of various multi-criteria decision-making and rank
aggregation algorithms showed that TOPSIS and WSUM are the best candidates for
this use case.

</details>


### [143] [Hyperbolic Dataset Distillation](https://arxiv.org/abs/2505.24623)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: The paper proposes Hyperbolic Dataset Distillation (HDD), a novel method that uses hyperbolic space to preserve hierarchical and geometric relationships in dataset distillation. HDD improves upon existing distribution matching methods by embedding features into hyperbolic space, enhancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current distribution matching (DM) methods for dataset distillation operate within Euclidean space, treating data points as independent and identically distributed without considering complex geometric or hierarchical relationships. This limitation motivates the need for an approach that can better capture such structures.

Method: The proposed method, Hyperbolic Dataset Distillation (HDD), embeds features extracted from a shallow network into the Lorentz hyperbolic space. The discrepancy between synthetic and original data distributions is measured using the hyperbolic distance between their centroids. By optimizing this distance, the method integrates hierarchical structure into the distillation process. Additionally, pruning in hyperbolic space shows that only 20% of the distilled core set is needed to maintain model performance.

Result: Experiments on various datasets demonstrate that HDD effectively captures hierarchical and geometric characteristics of data, leading to improved training stability and comparable model performance with significantly reduced dataset size. It also shows compatibility with most existing DM methods.

Conclusion: HDD offers a promising solution for dataset distillation by leveraging hyperbolic space to address limitations of current DM methods. It achieves high computational efficiency while preserving important data characteristics.

Abstract: To address the computational and storage challenges posed by large-scale
datasets in deep learning, dataset distillation has been proposed to synthesize
a compact dataset that replaces the original while maintaining comparable model
performance. Unlike optimization-based approaches that require costly bi-level
optimization, distribution matching (DM) methods improve efficiency by aligning
the distributions of synthetic and original data, thereby eliminating nested
optimization. DM achieves high computational efficiency and has emerged as a
promising solution. However, existing DM methods, constrained to Euclidean
space, treat data as independent and identically distributed points,
overlooking complex geometric and hierarchical relationships. To overcome this
limitation, we propose a novel hyperbolic dataset distillation method, termed
HDD. Hyperbolic space, characterized by negative curvature and exponential
volume growth with distance, naturally models hierarchical and tree-like
structures. HDD embeds features extracted by a shallow network into the Lorentz
hyperbolic space, where the discrepancy between synthetic and original data is
measured by the hyperbolic (geodesic) distance between their centroids. By
optimizing this distance, the hierarchical structure is explicitly integrated
into the distillation process, guiding synthetic samples to gravitate towards
the root-centric regions of the original data distribution while preserving
their underlying geometric characteristics. Furthermore, we find that pruning
in hyperbolic space requires only 20% of the distilled core set to retain model
performance, while significantly improving training stability. Notably, HDD is
seamlessly compatible with most existing DM methods, and extensive experiments
on different datasets validate its effectiveness.

</details>


### [144] [Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees](https://arxiv.org/abs/2505.24627)
*Fu Luo,Yaoxin Wu,Zhi Zheng,Zhenkun Wang*

Main category: cs.LG

TL;DR: Recent neural combinatorial optimization (NCO) methods show promise but overfit to specific constraint values. This paper addresses this issue by developing a training scheme and multi-expert module for the CVRP, improving performance across varying constraint tightness degrees.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the performance of NCO methods under different constraint tightness levels, using CVRP as an example.

Method: Empirical analysis of existing NCO methods on CVRP reveals overfitting to capacity constraints. A new training scheme and multi-expert module are proposed to address this.

Result: The proposed method effectively overcomes the overfitting issue, showing superior performances on CVRP and CVRPTW with various constraint tightness degrees.

Conclusion: The developed training scheme and multi-expert module enhance the adaptability of NCO methods to different constraint values.

Abstract: Recent neural combinatorial optimization (NCO) methods have shown promising
problem-solving ability without requiring domain-specific expertise. Most
existing NCO methods use training and testing data with a fixed constraint
value and lack research on the effect of constraint tightness on the
performance of NCO methods. This paper takes the capacity-constrained vehicle
routing problem (CVRP) as an example to empirically analyze the NCO performance
under different tightness degrees of the capacity constraint. Our analysis
reveals that existing NCO methods overfit the capacity constraint, and they can
only perform satisfactorily on a small range of the constraint values but
poorly on other values. To tackle this drawback of existing NCO methods, we
develop an efficient training scheme that explicitly considers varying degrees
of constraint tightness and proposes a multi-expert module to learn a generally
adaptable solving strategy. Experimental results show that the proposed method
can effectively overcome the overfitting issue, demonstrating superior
performances on the CVRP and CVRP with time windows (CVRPTW) with various
constraint tightness degrees.

</details>


### [145] [Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks](https://arxiv.org/abs/2505.24629)
*Lotte Bransen,Tim Janssen,Jesse Davis*

Main category: cs.LG

TL;DR: 在足球点球分析中，现有的研究通常假设守门员和踢球者独立选择行动，这与实际情况不符。本文通过构建一个与球员无关的模拟框架，利用大量由专家标注的点球数据集，评估不同守门员策略的效果，并优化守门员在实际比赛中的决策政策。


<details>
  <summary>Details</summary>
Motivation: 现有的点球分析方法假设守门员和踢球者的动作选择相互独立，但现实中两者的选择会互相影响。此外，由于样本量小，难以深入了解特定对手的能力。因此需要一种新的方法来克服这些限制。

Method: 开发了一个与球员无关的模拟框架，该框架基于一个大型点球数据集（由点球专家标注），考虑了丰富的动作选择并结合了守门员技能信息，用于评估不同守门员策略的效果。

Result: 该框架能够有效评估各种守门员策略，并为实际比赛中的守门员决策提供优化方案。

Conclusion: 通过提出的模拟框架，可以更好地理解点球中的复杂互动，并为守门员提供更优的策略建议。

Abstract: Penalties are fraught and game-changing moments in soccer games that teams
explicitly prepare for. Consequently, there has been substantial interest in
analyzing them in order to provide advice to practitioners. From a data science
perspective, such analyses suffer from a significant limitation: they make the
unrealistic simplifying assumption that goalkeepers and takers select their
action -- where to dive and where to the place the kick -- independently of
each other. In reality, the choices that some goalkeepers make depend on the
taker's movements and vice-versa. This adds substantial complexity to the
problem because not all players have the same action capacities, that is, only
some players are capable of basing their decisions on their opponent's
movements. However, the small sample sizes on the player level mean that one
may have limited insights into a specific opponent's capacities. We address
these challenges by developing a player-agnostic simulation framework that can
evaluate the efficacy of different goalkeeper strategies. It considers a rich
set of choices and incorporates information about a goalkeeper's skills. Our
work is grounded in a large dataset of penalties that were annotated by penalty
experts and include aspects of both kicker and goalkeeper strategies. We show
how our framework can be used to optimize goalkeeper policies in real-world
situations.

</details>


### [146] [WILTing Trees: Interpreting the Distance Between MPNN Embeddings](https://arxiv.org/abs/2505.24642)
*Masahiro Negishi,Thomas Gärtner,Pascal Welke*

Main category: cs.LG

TL;DR: The paper explores how MPNNs learn functional distances between prediction targets and proposes a method using optimal transport on WILT to interpret these distances.


<details>
  <summary>Details</summary>
Motivation: To understand the functional distance between prediction targets that MPNNs implicitly learn, unlike previous work that links MPNN distances to structural graph distances without considering task-specific information.

Method: Distill the distance between MPNN embeddings into an interpretable graph distance using optimal transport on the Weisfeiler Leman Labeling Tree (WILT), where edge weights reveal influential subgraphs.

Result: Through experiments, it is shown that MPNNs define the relative position of embeddings by focusing on a small set of functionally important subgraphs.

Conclusion: MPNNs focus on specific subgraphs to determine embedding distances, and the proposed method provides an interpretable way to understand these distances.

Abstract: We investigate the distance function learned by message passing neural
networks (MPNNs) in specific tasks, aiming to capture the functional distance
between prediction targets that MPNNs implicitly learn. This contrasts with
previous work, which links MPNN distances on arbitrary tasks to structural
distances on graphs that ignore task-specific information. To address this gap,
we distill the distance between MPNN embeddings into an interpretable graph
distance. Our method uses optimal transport on the Weisfeiler Leman Labeling
Tree (WILT), where the edge weights reveal subgraphs that strongly influence
the distance between embeddings. This approach generalizes two well-known graph
kernels and can be computed in linear time. Through extensive experiments, we
demonstrate that MPNNs define the relative position of embeddings by focusing
on a small set of subgraphs that are known to be functionally important in the
domain.

</details>


### [147] [Learning Distributions over Permutations and Rankings with Factorized Representations](https://arxiv.org/abs/2505.24664)
*Daniel Severo,Brian Karrer,Niklas Nolte*

Main category: cs.LG

TL;DR: The paper proposes a new approach to learning distributions over permutations using alternative representations such as Lehmer codes, Fisher-Yates draws, and Insertion-Vectors.


<details>
  <summary>Details</summary>
Motivation: Existing methods for learning distributions over permutations rely on mixtures of parametric families or neural networks with expensive variational inference procedures. The authors aim to develop a more efficient and flexible method.

Method: The proposed method leverages alternative representations for permutations including Lehmer codes, Fisher-Yates draws, and Insertion-Vectors. These representations form a bijection with the symmetric group, allowing for unconstrained learning using conventional deep learning techniques.

Result: Experiments show that the proposed method significantly outperforms current approaches on the jigsaw puzzle benchmark. Additionally, the method learns non-trivial distributions even in the least expressive mode on two new benchmarks: learning cyclic permutations and re-ranking movies based on user preference.

Conclusion: The novel approach enables a trade-off between expressivity of the model family and computational requirements, and subsumes previous families of probabilistic models over permutations.

Abstract: Learning distributions over permutations is a fundamental problem in machine
learning, with applications in ranking, combinatorial optimization, structured
prediction, and data association. Existing methods rely on mixtures of
parametric families or neural networks with expensive variational inference
procedures. In this work, we propose a novel approach that leverages
alternative representations for permutations, including Lehmer codes,
Fisher-Yates draws, and Insertion-Vectors. These representations form a
bijection with the symmetric group, allowing for unconstrained learning using
conventional deep learning techniques, and can represent any probability
distribution over permutations. Our approach enables a trade-off between
expressivity of the model family and computational requirements. In the least
expressive and most computationally efficient case, our method subsumes
previous families of well established probabilistic models over permutations,
including Mallow's and the Repeated Insertion Model. Experiments indicate our
method significantly outperforms current approaches on the jigsaw puzzle
benchmark, a common task for permutation learning. However, we argue this
benchmark is limited in its ability to assess learning probability
distributions, as the target is a delta distribution (i.e., a single correct
solution exists). We therefore propose two additional benchmarks: learning
cyclic permutations and re-ranking movies based on user preference. We show
that our method learns non-trivial distributions even in the least expressive
mode, while traditional models fail to even generate valid permutations in this
setting.

</details>


### [148] [Learning geometry and topology via multi-chart flows](https://arxiv.org/abs/2505.24665)
*Hanlin Yu,Søren Hauberg,Marcelo Hartmann,Arto Klami,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 通过学习多个粘合在一起的流并开发计算测地线的数值算法，可以显著提高拓扑估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据通常位于嵌入高维空间的低维黎曼流形上，这促使我们学习退化的归一化流，这些流在环境空间和低维潜在空间之间映射。然而，如果流形具有非平凡的拓扑结构，则使用单一流无法正确学习它。

Method: 首先提出了一种用于学习此类流集合的一般训练方案；其次开发了第一个用于在这些流形上计算测地线的数值算法。

Result: 经验上，证明这种方法导致拓扑估计有高度显著的改进。

Conclusion: 该方法通过多流组合和测地线计算提高了拓扑估计的效果。

Abstract: Real world data often lie on low-dimensional Riemannian manifolds embedded in
high-dimensional spaces. This motivates learning degenerate normalizing flows
that map between the ambient space and a low-dimensional latent space. However,
if the manifold has a non-trivial topology, it can never be correctly learned
using a single flow. Instead multiple flows must be `glued together'. In this
paper, we first propose the general training scheme for learning such a
collection of flows, and secondly we develop the first numerical algorithms for
computing geodesics on such manifolds. Empirically, we demonstrate that this
leads to highly significant improvements in topology estimation.

</details>


### [149] [Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning](https://arxiv.org/abs/2505.24676)
*Mihir Bhaskar,Jun Tao Luo,Zihan Geng,Asmita Hajra,Junia Howell,Matthew R. Gormley*

Main category: cs.LG

TL;DR: 学者们因历史房产评估记录的难以获取，难以量化美国政府20世纪30年代住房政策对种族财富差异的确切金融影响。本文提出了一种数字化历史住房评估数据的方法，并建立了一个县的数据集。通过手动标注、OCR技术和回归模型等手段，使得学者、社区活动家和政策制定者能更好地分析和理解红线划定的历史影响。


<details>
  <summary>Details</summary>
Motivation: 尽管有充分记录显示美国政府20世纪30年代的住房政策对种族财富差异产生了深远影响，但由于历史房产评估记录的难以获取，学者们难以量化其确切金融影响。

Method: 从公开可得的扫描文档开始，手动标注超过12,000个房产卡片以训练和验证方法；使用OCR技术为额外50,000个房产标注数据，采用结合经典计算机视觉技术和深度学习基于OCR的两阶段方法；对于无法应用OCR的情况（如扫描文档不可用），展示如何基于建筑特征数据的回归模型来估计历史价值，并测试该模型在其他县的通用性。

Result: 成功构建了一个县的历史住房评估数据集，且证明了成本效益高的工具和技术可以用于更大范围的研究。

Conclusion: 通过本文提出的数字化方法和模型，学者、社区活动家和政策制定者能够更好地分析和理解红线划定的历史影响。

Abstract: Despite well-documented consequences of the U.S. government's 1930s housing
policies on racial wealth disparities, scholars have struggled to quantify its
precise financial effects due to the inaccessibility of historical property
appraisal records. Many counties still store these records in physical formats,
making large-scale quantitative analysis difficult. We present an approach
scholars can use to digitize historical housing assessment data, applying it to
build and release a dataset for one county. Starting from publicly available
scanned documents, we manually annotated property cards for over 12,000
properties to train and validate our methods. We use OCR to label data for an
additional 50,000 properties, based on our two-stage approach combining
classical computer vision techniques with deep learning-based OCR. For cases
where OCR cannot be applied, such as when scanned documents are not available,
we show how a regression model based on building feature data can estimate the
historical values, and test the generalizability of this model to other
counties. With these cost-effective tools, scholars, community activists, and
policy makers can better analyze and understand the historical impacts of
redlining.

</details>


### [150] [Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs](https://arxiv.org/abs/2505.24684)
*Zihao Chen,Yu Xiang,Wenyong Wang*

Main category: cs.LG

TL;DR: 通过分析η-TCVAE中的总相关性，发现了解纠缠粒度这一关键的隐式归纳偏差，并揭示了参数空间中最佳ELBO轨迹的有趣“V”形。实验表明，传统因子分解VAE受限于固定的解纠缠粒度，倾向于解纠缠低复杂度特征，而η-STCVAE能够通过调整解纠缠粒度来扩展可解纠缠表示的范围，允许解纠缠高复杂度特征。


<details>
  <summary>Details</summary>
Motivation: 变分自编码器（VAEs）及其变体在学习语义上有意义的无监督解纠缠表示方面取得了成功，但面临着一个基本的理论挑战：没有隐式的归纳偏差，无监督解纠缠是无法实现的，然而这种偏差仍然难以捉摸。

Method: 研究者专注于探索带有因子化先验的VAEs中驱动解纠缠的隐式归纳偏差。通过分析η-TCVAE中的总相关性，发现了称为解纠缠粒度的关键隐式归纳偏差。并提出了新的模型η-STCVAE，通过超过10万次实验验证了这些发现。

Result: 实验结果表明，传统的因子分解VAE受固定解纠缠粒度的限制，内在地倾向于解纠缠低复杂度特征。而通过η-STCVAE适当地调整解纠缠粒度，可以扩大解纠缠表示的范围，允许解纠缠高复杂度特征。

Conclusion: 研究揭示了解纠缠粒度作为一种隐式归纳偏差影响了解纠缠性能和ELBO的推理，为VAEs的可解释性和固有偏差提供了新的见解。

Abstract: Despite the success in learning semantically meaningful, unsupervised
disentangled representations, variational autoencoders (VAEs) and their
variants face a fundamental theoretical challenge: substantial evidence
indicates that unsupervised disentanglement is unattainable without implicit
inductive bias, yet such bias remains elusive. In this work, we focus on
exploring the implicit inductive bias that drive disentanglement in VAEs with
factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we
uncover a crucial implicit inductive bias called disentangling granularity,
which leads to the discovery of an interesting "V"-shaped optimal Evidence
Lower Bound (ELBO) trajectory within the parameter space. This finding is
validated through over 100K experiments using factorized VAEs and our newly
proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that
conventional factorized VAEs, constrained by fixed disentangling granularity,
inherently tend to disentangle low-complexity feature. Whereas, appropriately
tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the
range of disentangled representations, allowing for the disentanglement of
high-complexity features. Our findings unveil that disentangling granularity as
an implicit inductive bias in factorized VAEs influence both disentanglement
performance and the inference of the ELBO, offering fresh insights into the
interpretability and inherent biases of VAEs.

</details>


### [151] [Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms](https://arxiv.org/abs/2505.24692)
*Derek Everett,Fred Lu,Edward Raff,Fernando Camacho,James Holt*

Main category: cs.LG

TL;DR: 提出了一种新的策略，使用高斯插值在连续空间中学习奖励环境，该方法可以有效学习连续的Lipschitz奖励函数，并自然扩展到非平稳问题，同时计算上更优（100-10000倍更快），并在实验中优于滑动高斯过程策略。


<details>
  <summary>Details</summary>
Motivation: 经典的多臂赌博机算法通常假设奖励环境是平稳的且动作空间较小。近期的发展通常只放松其中一个假设：现有的非平稳赌博机策略适用于少量手臂，而Lipschitz、线性和高斯过程赌博机策略则用于处理平稳奖励环境下大量的手臂。本文旨在解决在一个连续空间内学习奖励环境的问题，同时考虑非平稳性。

Method: 提出了一种新的策略，使用高斯插值来学习连续空间中的奖励环境。证明了该方法能够以$\mathcal{O}^*(\sqrt{T})$的累积遗憾有效地学习连续的Lipschitz奖励函数。通过简单的修改，该方法可以自然地扩展到非平稳问题。

Result: 该方法在计算上具有优势（比现有方法快100-10000倍），并且在具有非平稳性和极大量手臂的数据集上，实验表现优于滑动高斯过程策略。

Conclusion: 所提出的策略不仅在理论上保证了对连续Lipschitz奖励函数的有效学习，还展示了其在非平稳问题上的适用性和计算效率。

Abstract: Canonical algorithms for multi-armed bandits typically assume a stationary
reward environment where the size of the action space (number of arms) is
small. More recently developed methods typically relax only one of these
assumptions: existing non-stationary bandit policies are designed for a small
number of arms, while Lipschitz, linear, and Gaussian process bandit policies
are designed to handle a large (or infinite) number of arms in stationary
reward environments under constraints on the reward function. In this
manuscript, we propose a novel policy to learn reward environments over a
continuous space using Gaussian interpolation. We show that our method
efficiently learns continuous Lipschitz reward functions with
$\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally
extends to non-stationary problems with a simple modification. We finally
demonstrate that our method is computationally favorable (100-10000x faster)
and experimentally outperforms sliding Gaussian process policies on datasets
with non-stationarity and an extremely large number of arms.

</details>


### [152] [On Symmetric Losses for Robust Policy Optimization with Noisy Preferences](https://arxiv.org/abs/2505.24709)
*Soichiro Nishimori,Yu-Jie Zhang,Thanawat Lodkaew,Masashi Sugiyama*

Main category: cs.LG

TL;DR: This paper proposes Symmetric Preference Optimization (SymPO), a method that uses symmetric losses for robust policy optimization in language models even with noisy preference data, proving its effectiveness through various experiments.


<details>
  <summary>Details</summary>
Motivation: Current methods for optimizing policies based on human preferences assume accurate annotations, but real-world data often contains noise due to human errors or biases.

Method: The authors propose treating reward modeling as a classification problem and applying symmetric losses, which are robust to label noise, leading to the development of the Symmetric Preference Optimization (SymPO) method.

Result: Experiments conducted on both synthetic and real-world tasks show that SymPO is effective in optimizing policies under noisy conditions while maintaining rank-preserving rewards.

Conclusion: SymPO enables successful policy optimization even when labels are noisy, providing a robust framework for reinforcement learning from human feedback.

Abstract: Optimizing policies based on human preferences is key to aligning language
models with human intent. This work focuses on reward modeling, a core
component in reinforcement learning from human feedback (RLHF), and offline
preference optimization, such as direct preference optimization. Conventional
approaches typically assume accurate annotations. However, real-world
preference data often contains noise due to human errors or biases. We propose
a principled framework for robust policy optimization under noisy preferences,
viewing reward modeling as a classification problem. This allows us to leverage
symmetric losses, known for their robustness to label noise in classification,
leading to our Symmetric Preference Optimization (SymPO) method. We prove that
symmetric losses enable successful policy optimization even under noisy labels,
as the resulting reward remains rank-preserving -- a property sufficient for
policy improvement. Experiments on synthetic and real-world tasks demonstrate
the effectiveness of SymPO.

</details>


### [153] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
*Wei Chen,Jiahao Zhang,Haipeng Zhu,Boyan Xu,Zhifeng Hao,Keli Zhang,Junjian Ye,Ruichu Cai*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


### [154] [CoRet: Improved Retriever for Code Editing](https://arxiv.org/abs/2505.24715)
*Fabio Fehr,Prabhu Teja Sivaprasad,Luca Franceschi,Giovanni Zappella*

Main category: cs.LG

TL;DR: This paper introduces CoRet, a dense retrieval model for code-editing tasks which improves retrieval recall by at least 15 percentage points over existing models.


<details>
  <summary>Details</summary>
Motivation: To develop a model that integrates code semantics, repository structure, and call graph dependencies for enhancing code repository retrieval based on natural language queries.

Method: The model, CoRet, is designed for code-editing tasks and uses a loss function explicitly designed for repository-level retrieval. It focuses on retrieving relevant portions of a code repository based on natural language queries.

Result: CoRet substantially improves retrieval recall by at least 15 percentage points over existing models on SWE-bench and Long Code Arena's bug localisation datasets.

Conclusion: CoRet shows significant improvements in retrieval performance and the importance of its design choices.

Abstract: In this paper, we introduce CoRet, a dense retrieval model designed for
code-editing tasks that integrates code semantics, repository structure, and
call graph dependencies. The model focuses on retrieving relevant portions of a
code repository based on natural language queries such as requests to implement
new features or fix bugs. These retrieved code chunks can then be presented to
a user or to a second code-editing model or agent. To train CoRet, we propose a
loss function explicitly designed for repository-level retrieval. On SWE-bench
and Long Code Arena's bug localisation datasets, we show that our model
substantially improves retrieval recall by at least 15 percentage points over
existing models, and ablate the design choices to show their importance in
achieving these results.

</details>


### [155] [PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations](https://arxiv.org/abs/2505.24717)
*Benjamin Holzschuh,Qiang Liu,Georg Kohl,Nils Thuerey*

Main category: cs.LG

TL;DR: The paper presents PDE-Transformer, an advanced transformer-based architecture for physics simulations on regular grids, which surpasses existing models in performance and scalability.


<details>
  <summary>Details</summary>
Motivation: To create a more scalable and versatile general-purpose transformer architecture for building large-scale foundation models in physical sciences.

Method: Combining recent architectural improvements of diffusion transformers with specific adjustments for large-scale simulations, embedding different physical channels individually as spatio-temporal tokens that interact via channel-wise self-attention.

Result: Outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs and achieves improved performance on several challenging downstream tasks compared to training from scratch.

Conclusion: PDE-Transformer is a successful architecture that can be used as the backbone for building large-scale foundation models in physical sciences, beating other foundation model architectures for physics simulations.

Abstract: We introduce PDE-Transformer, an improved transformer-based architecture for
surrogate modeling of physics simulations on regular grids. We combine recent
architectural improvements of diffusion transformers with adjustments specific
for large-scale simulations to yield a more scalable and versatile
general-purpose transformer architecture, which can be used as the backbone for
building large-scale foundation models in physical sciences. We demonstrate
that our proposed architecture outperforms state-of-the-art transformer
architectures for computer vision on a large dataset of 16 different types of
PDEs. We propose to embed different physical channels individually as
spatio-temporal tokens, which interact via channel-wise self-attention. This
helps to maintain a consistent information density of tokens when learning
multiple types of PDEs simultaneously. We demonstrate that our pre-trained
models achieve improved performance on several challenging downstream tasks
compared to training from scratch and also beat other foundation model
architectures for physics simulations.

</details>


### [156] [Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach](https://arxiv.org/abs/2505.24721)
*Nick Rossenbach,Benedikt Hilmes,Leon Brackmann,Moritz Gunz,Ralf Schlüter*

Main category: cs.LG

TL;DR: This paper proposes a PyTorch-based library called Synaptogen to simulate neural network execution with accurately captured memristor hardware properties, demonstrating its effectiveness on a Conformer trained on TED-LIUMv2.


<details>
  <summary>Details</summary>
Motivation: Current memristor-based hardware prototypes cannot fit large neural networks and existing software for simulation assumes simplified hardware. There is a need for more accurate simulation tools to explore the behavior of larger ML models on memristor hardware.

Method: The authors developed a PyTorch-based library named Synaptogen which simulates neural network execution using detailed memristor hardware properties. They applied this tool to study the performance of a Conformer model with millions of parameters trained on the speech recognition task TED-LIUMv2.

Result: Using Synaptogen, they found that with adjusted quantization-aware training, the relative degradation in word error rate could be limited to 25% when employing a 3-bit weight precision for linear operations via simulated analog computation.

Conclusion: The proposed Synaptogen library allows for the accurate simulation of large ML models on memristor hardware, providing insights into system behavior under realistic hardware constraints.

Abstract: Memristor-based hardware offers new possibilities for energy-efficient
machine learning (ML) by providing analog in-memory matrix multiplication.
Current hardware prototypes cannot fit large neural networks, and related
literature covers only small ML models for tasks like MNIST or single word
recognition. Simulation can be used to explore how hardware properties affect
larger models, but existing software assumes simplified hardware. We propose a
PyTorch-based library based on "Synaptogen" to simulate neural network
execution with accurately captured memristor hardware properties. For the first
time, we show how an ML system with millions of parameters would behave on
memristor hardware, using a Conformer trained on the speech recognition task
TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the
relative degradation in word error rate to 25% when using a 3-bit weight
precision to execute linear operations via simulated analog computation.

</details>


### [157] [HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts](https://arxiv.org/abs/2505.24722)
*Neil He,Rishabh Anand,Hiren Madhu,Ali Maatouk,Smita Krishnaswamy,Leandros Tassiulas,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 提出了HELM系列模型，基于双曲空间的大型语言模型，包含HELM-MICE和HELM-D两种模型。通过引入双曲几何，改善了语言模型在文本建模任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）依赖欧几里得操作，无法完全捕捉自然语言中固有的语义层次和几何结构，导致训练不稳定和生成能力下降。

Method: 提出了一种全新的双曲大型语言模型（HELM），包括Mixture-of-Curvature Experts模型（HELM-MICE）和密集模型（HELM-D）。开发了双曲多头潜在注意力（HMLA）、双曲旋转位置编码和RMS归一化等技术。

Result: 在MMLU和ARC等多个基准测试中，HELM架构相较于流行的欧几里得架构（如LLaMA和DeepSeek）表现出一致的提升，最高可达4%。

Conclusion: 双曲几何在大规模语言模型预训练中展现出有效性和增强推理能力，为未来语言模型的研究提供了新的方向。

Abstract: Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-KV-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.

</details>


### [158] [Robust Federated Learning against Model Perturbation in Edge Networks](https://arxiv.org/abs/2505.24728)
*Dongzi Jin,Yong Xiao,Yingyu Li*

Main category: cs.LG

TL;DR: In this paper, the authors address the issue of performance degradation in Federated Learning (FL) caused by perturbations in shared models. They propose a method called Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL). This method improves model robustness against perturbations by exploring the geometrical property of the model landscape and solving a min-max optimization problem. Theoretical analysis proves that SMRFL converges at the same rate as FL without perturbations. Experimental results demonstrate that SMRFL significantly enhances robustness compared to baseline methods under various perturbation scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to overcome the challenge of significant performance degradation in Federated Learning when shared models are exposed to various perturbations in practical scenarios.

Method: The proposed method, Sharpness-Aware Minimization-based Robust Federated Learning (SMRFL), aims to improve model robustness against perturbations. It solves a min-max optimization problem that promotes model convergence towards a flat minimum by minimizing the maximum loss within a neighborhood of the model parameters.

Result: The theoretical result proves that SMRFL can converge at the same rate as FL without perturbations. Extensive experimental results show that SMRFL significantly enhances robustness against perturbations compared to three baseline methods on two real-world datasets under three perturbation scenarios.

Conclusion: SMRFL effectively reduces model sensitivity to perturbations and enhances robustness, making it a promising approach for improving the reliability of Federated Learning in practical applications.

Abstract: Federated Learning (FL) is a promising paradigm for realizing edge
intelligence, allowing collaborative learning among distributed edge devices by
sharing models instead of raw data. However, the shared models are often
assumed to be ideal, which would be inevitably violated in practice due to
various perturbations, leading to significant performance degradation. To
overcome this challenge, we propose a novel method, termed Sharpness-Aware
Minimization-based Robust Federated Learning (SMRFL), which aims to improve
model robustness against perturbations by exploring the geometrical property of
the model landscape. Specifically, SMRFL solves a min-max optimization problem
that promotes model convergence towards a flat minimum by minimizing the
maximum loss within a neighborhood of the model parameters. In this way, model
sensitivity to perturbations is reduced, and robustness is enhanced since
models in the neighborhood of the flat minimum also enjoy low loss values. The
theoretical result proves that SMRFL can converge at the same rate as FL
without perturbations. Extensive experimental results show that SMRFL
significantly enhances robustness against perturbations compared to three
baseline methods on two real-world datasets under three perturbation scenarios.

</details>


### [159] [Feature Attribution from First Principles](https://arxiv.org/abs/2505.24729)
*Magamed Taimeskhanov,Damien Garreau*

Main category: cs.LG

TL;DR: The paper proposes a new feature attribution framework for machine learning models, which starts by defining attributions for simple models and uses these as building blocks for complex models. It also derives expressions for deep ReLU networks and steps towards optimizing evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Existing axiomatic frameworks for feature attribution methods in machine learning are too restrictive, making it difficult to evaluate these methods empirically.

Method: The method involves starting with the simplest models (indicator functions) to define attributions and using these as building blocks for more complex models. This approach bypasses the need for restrictive axioms.

Result: This approach recovers several existing attribution methods depending on the choice of atomic attribution and derives closed-form expressions for deep ReLU networks.

Conclusion: The proposed framework offers a novel way to understand feature attributions without restrictive axioms and takes a step forward in optimizing evaluation metrics related to feature attributions.

Abstract: Feature attribution methods are a popular approach to explain the behavior of
machine learning models. They assign importance scores to each input feature,
quantifying their influence on the model's prediction. However, evaluating
these methods empirically remains a significant challenge. To bypass this
shortcoming, several prior works have proposed axiomatic frameworks that any
feature attribution method should satisfy. In this work, we argue that such
axioms are often too restrictive, and propose in response a new feature
attribution framework, built from the ground up. Rather than imposing axioms,
we start by defining attributions for the simplest possible models, i.e.,
indicator functions, and use these as building blocks for more complex models.
We then show that one recovers several existing attribution methods, depending
on the choice of atomic attribution. Subsequently, we derive closed-form
expressions for attribution of deep ReLU networks, and take a step toward the
optimization of evaluation metrics with respect to feature attributions.

</details>


### [160] [Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning](https://arxiv.org/abs/2505.24737)
*Erchi Wang,Yuqing Zhu,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper studies the problem of differentially private empirical risk
minimization (DP-ERM) for binary linear classification. We obtain an efficient
$(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of
$\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} +
\frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data
points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and
$\gamma$ is the margin of linear separation of the remaining data points (after
$S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic
terms. In the agnostic case, we improve the existing results when the number of
outliers is small. Our algorithm is highly adaptive because it does not require
knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We
also derive a utility bound for the advanced private hyperparameter tuning
algorithm.

</details>


### [161] [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
*Yehonathan Refael,Guy Smorodinsky,Tom Tirer,Ofir Lindenbaum*

Main category: cs.LG

TL;DR: SUMO是一种新的优化器，通过在动态调整的低维子空间中使用精确奇异值分解（SVD）进行动量正交化，能够显著加速大型语言模型训练的收敛速度，并提高稳定性和性能，同时减少内存需求。


<details>
  <summary>Details</summary>
Motivation: 尽管低秩梯度优化方法提高了大语言模型训练的内存效率，但它们通常依赖于标准的各向同性最陡下降技术，这在深度网络（特别是大语言模型）的高度各向异性损失景观中可能表现不佳，限制了收敛速度的提升。

Method: 提出了一种名为SUMO的优化器，它利用精确的奇异值分解（SVD）在动态调整的低维子空间内进行动量正交化，从而实现与损失景观的谱特性对齐的范数诱导最陡下降优化步骤。此外，还理论推导了近似误差的上限，并证明其与矩的条件数相关，这些条件在大语言模型训练中经常出现。

Result: 理论上和实证上都表明，通过SVD进行精确正交化可以大幅提高收敛率，同时降低整体复杂度。实验结果表明，SUMO相较于现有最先进方法，不仅加速了收敛，还提升了稳定性、性能，并将内存需求减少了多达20%。

Conclusion: SUMO作为一种创新的优化方法，在不牺牲性能的前提下，显著提升了大语言模型训练的效率和稳定性，同时降低了内存消耗。

Abstract: Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential acceleration in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.

</details>


### [162] [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.24760)
*Zafir Stojanovski,Oliver Stanley,Joe Sharratt,Richard Jones,Abdulhakeem Adefioye,Jean Kaddour,Andreas Köpf*

Main category: cs.LG

TL;DR: The paper introduces Reasoning Gym (RG), a library for reinforcement learning that offers over 100 data generators and verifiers across multiple domains. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity.


<details>
  <summary>Details</summary>
Motivation: To create a library of reasoning environments for reinforcement learning with verifiable rewards, addressing the limitation of fixed datasets in previous reasoning datasets.

Method: Developed Reasoning Gym (RG) which includes data generators and verifiers spanning various domains and utilizes procedural generation to create virtually infinite training data with adjustable complexity.

Result: Experimental results show the efficacy of RG in both evaluating and reinforcement learning of reasoning models.

Conclusion: Reasoning Gym provides a valuable tool for continuous evaluation across varying difficulty levels in reinforcement learning.

Abstract: We introduce Reasoning Gym (RG), a library of reasoning environments for
reinforcement learning with verifiable rewards. It provides over 100 data
generators and verifiers spanning multiple domains including algebra,
arithmetic, computation, cognition, geometry, graph theory, logic, and various
common games. Its key innovation is the ability to generate virtually infinite
training data with adjustable complexity, unlike most previous reasoning
datasets, which are typically fixed. This procedural generation approach allows
for continuous evaluation across varying difficulty levels. Our experimental
results demonstrate the efficacy of RG in both evaluating and reinforcement
learning of reasoning models.

</details>


### [163] [AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption](https://arxiv.org/abs/2505.24773)
*Yajie Zhou,Xiaoyi Pang,Zhibo Wang*

Main category: cs.LG

TL;DR: AFLoRA是一种为大型语言模型设计的自适应轻量级联邦微调框架，通过分离共享和客户端特定更新、使用对角矩阵秩剪枝和基于公共数据优化的秩感知聚合等方法，在准确性和效率上超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦微调方法在处理异构和资源受限环境中的大型语言模型时，面临计算与通信需求高、性能受最弱客户端限制以及非独立同分布数据影响等问题，同时难以兼顾低秩更新的精确聚合和系统成本。

Method: AFLoRA框架主要采用以下策略：1) 分离共享和客户端特定更新以降低开销并提高聚合精度；2) 引入基于对角矩阵的秩剪枝以更有效地利用本地资源；3) 使用具有公共数据优化的秩感知聚合来增强数据异质性下的泛化能力。

Result: 广泛实验表明，AFLoRA在准确性和效率方面均优于现有最先进技术，为现实世界中异构环境下大型语言模型的有效适配提供了实用解决方案。

Conclusion: AFLoRA通过创新的技术手段解决了联邦微调大型语言模型时遇到的性能瓶颈问题，并在异构环境中实现了高效且精准的模型适配。

Abstract: Federated fine-tuning has emerged as a promising approach to adapt foundation
models to downstream tasks using decentralized data. However, real-world
deployment remains challenging due to the high computational and communication
demands of fine-tuning Large Language Models (LLMs) on clients with data and
system resources that are heterogeneous and constrained. In such settings, the
global model's performance is often bottlenecked by the weakest clients and
further degraded by the non-IID nature of local data. Although existing methods
leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to
reduce communication and computation overhead, they often fail to
simultaneously ensure accurate aggregation of low-rank updates and maintain low
system costs, thereby hindering overall performance. To address these
challenges, we propose AFLoRA, an adaptive and lightweight federated
fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific
updates to reduce overhead and improve aggregation accuracy, incorporates
diagonal matrix-based rank pruning to better utilize local resources, and
employs rank-aware aggregation with public data refinement to strengthen
generalization under data heterogeneity. Extensive experiments demonstrate that
AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,
providing a practical solution for efficient LLM adaptation in heterogeneous
environments in the real world.

</details>


### [164] [Diffusion-Based Symbolic Regression](https://arxiv.org/abs/2505.24776)
*Zachary Bastiani,Robert M. Kirby,Jacob Hochhalter,Shandian Zhe*

Main category: cs.LG

TL;DR: Diffusion is used for symbolic regression, with a novel approach that combines random mask-based diffusion, denoising, and reinforcement learning to generate high-quality equations.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression needs a powerful generative modeling framework like diffusion, which has been successful in other fields such as image and audio synthesis.

Method: A random mask-based diffusion and denoising process is constructed to generate diverse and high-quality equations. This is integrated with token-wise Group Relative Policy Optimization (GRPO) for efficient reinforcement learning on measurement datasets. Additionally, a long short-term risk-seeking policy is introduced to expand the pool of top-performing candidates.

Result: Extensive experiments and ablation studies have shown the effectiveness of the proposed approach.

Conclusion: The novel diffusion-based approach for symbolic regression successfully generates high-quality equations and performs efficiently.

Abstract: Diffusion has emerged as a powerful framework for generative modeling,
achieving remarkable success in applications such as image and audio synthesis.
Enlightened by this progress, we propose a novel diffusion-based approach for
symbolic regression. We construct a random mask-based diffusion and denoising
process to generate diverse and high-quality equations. We integrate this
generative processes with a token-wise Group Relative Policy Optimization
(GRPO) method to conduct efficient reinforcement learning on the given
measurement dataset. In addition, we introduce a long short-term risk-seeking
policy to expand the pool of top-performing candidates, further enhancing
performance. Extensive experiments and ablation studies have demonstrated the
effectiveness of our approach.

</details>


### [165] [EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation](https://arxiv.org/abs/2505.24779)
*Yidong Luo,Chenguang Wang,Jiahao Yang,Fanzeng Xia,Tianshu Yu*

Main category: cs.LG

TL;DR: The paper introduces a benchmark framework for evaluating Mixed-Integer Linear Programming (MILP) instance generation methods across crucial dimensions, using solver-internal features as 'expert assessments' to enhance the reliability of synthetic MILP data.


<details>
  <summary>Details</summary>
Motivation: Driven by the lack of standardized evaluation techniques for assessing the fidelity and utility of synthetic MILP instances amidst the proliferation of generation methods.

Method: The method involves a comprehensive benchmark framework that evaluates MILP instance quality across mathematical validity, structural similarity, computational hardness, and utility in machine learning tasks. It analyzes solver-internal features such as root node gap, heuristic success rates, and cut plane usage.

Result: The framework effectively facilitates robust comparisons among diverse generation techniques, spurring development of higher-quality instance generators.

Conclusion: This benchmark aims to enhance the reliability of research reliant on synthetic MILP data through systematic comparison of instance set fidelity.

Abstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex
decision-making problems. The proliferation of MILP instance generation
methods, driven by machine learning's demand for diverse optimization datasets
and the limitations of static benchmarks, has significantly outpaced
standardized evaluation techniques. Consequently, assessing the fidelity and
utility of synthetic MILP instances remains a critical, multifaceted challenge.
This paper introduces a comprehensive benchmark framework designed for the
systematic and objective evaluation of MILP instance generation methods. Our
framework provides a unified and extensible methodology, assessing instance
quality across crucial dimensions: mathematical validity, structural
similarity, computational hardness, and utility in downstream machine learning
tasks. A key innovation is its in-depth analysis of solver-internal features --
particularly by comparing distributions of key solver outputs including root
node gap, heuristic success rates, and cut plane usage -- leveraging the
solver's dynamic solution behavior as an `expert assessment' to reveal nuanced
computational resemblances. By offering a structured approach with clearly
defined solver-independent and solver-dependent metrics, our benchmark aims to
facilitate robust comparisons among diverse generation techniques, spur the
development of higher-quality instance generators, and ultimately enhance the
reliability of research reliant on synthetic MILP data. The framework's
effectiveness in systematically comparing the fidelity of instance sets is
demonstrated using contemporary generative models.

</details>


### [166] [QGAN-based data augmentation for hybrid quantum-classical neural networks](https://arxiv.org/abs/2505.24780)
*Run-Ze He,Jun-Jian Su,Su-Juan Qin,Zheng-Ping Jin,Fei Gao*

Main category: cs.LG

TL;DR: The paper explores the use of Quantum Generative Adversarial Networks (QGANs) with Hybrid Quantum-Classical Neural Networks (HQCNNs) for data augmentation, showing improved performance and efficiency compared to classical methods.


<details>
  <summary>Details</summary>
Motivation: Data augmentation in quantum machine learning is underexplored, particularly in addressing data scarcity issues.

Method: Integrate QGANs with HQCNNs to create an augmentation framework with two strategies: a general approach for enhancing data processing and classification across HQCNNs, and a customized strategy that dynamically generates samples based on HQCNN's performance.

Result: Simulation experiments on MNIST dataset show QGAN outperforms traditional data augmentation methods and classical GANs. It achieves comparable performance with half the parameters of DCGAN, indicating higher efficiency and effectiveness.

Conclusion: QGANs can simplify models, generate high-quality data, and enhance HQCNN accuracy and performance, paving the way for applying quantum data augmentation techniques in machine learning.

Abstract: Quantum neural networks converge faster and achieve higher accuracy than
classical models. However, data augmentation in quantum machine learning
remains underexplored. To tackle data scarcity, we integrate quantum generative
adversarial networks (QGANs) with hybrid quantum-classical neural networks
(HQCNNs) to develop an augmentation framework. We propose two strategies: a
general approach to enhance data processing and classification across HQCNNs,
and a customized strategy that dynamically generates samples tailored to the
HQCNN's performance on specific data categories, improving its ability to learn
from complex datasets. Simulation experiments on the MNIST dataset demonstrate
that QGAN outperforms traditional data augmentation methods and classical GANs.
Compared to baseline DCGAN, QGAN achieves comparable performance with half the
parameters, balancing efficiency and effectiveness. This suggests that QGANs
can simplify models and generate high-quality data, enhancing HQCNN accuracy
and performance. These findings pave the way for applying quantum data
augmentation techniques in machine learning.

</details>


### [167] [Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding](https://arxiv.org/abs/2505.24791)
*Jiaru Zhang,Juanwu Lu,Ziran Wang,Ruqi Zhang*

Main category: cs.LG

TL;DR: The paper proposes Selective Jacobi Decoding (SeJD), a strategy to accelerate autoregressive inference in normalizing flows by parallel iterative optimization, achieving up to 4.7 times faster inference without compromising generation quality.


<details>
  <summary>Details</summary>
Motivation: Normalizing flows are powerful generative models but face challenges such as limited expressive power due to architectural constraints and slow generation during inference due to sequential modeling.

Method: The authors propose SeJD, which leverages observations that strict sequential dependency is unnecessary for high-quality sample generation and that there is low dependency redundancy in the initial layer and higher redundancy in subsequent layers. SeJD accelerates autoregressive inference through parallel iterative optimization.

Result: Empirical evaluations demonstrate substantial speed improvements up to 4.7 times faster inference while maintaining generation quality and fidelity.

Conclusion: SeJD provides a general and effective technique to accelerate autoregressive inference in normalizing flows.

Abstract: Normalizing flows are promising generative models with advantages such as
theoretical rigor, analytical log-likelihood computation, and end-to-end
training. However, the architectural constraints to ensure invertibility and
tractable Jacobian computation limit their expressive power and practical
usability. Recent advancements utilize autoregressive modeling, significantly
enhancing expressive power and generation quality. However, such sequential
modeling inherently restricts parallel computation during inference, leading to
slow generation that impedes practical deployment. In this paper, we first
identify that strict sequential dependency in inference is unnecessary to
generate high-quality samples. We observe that patches in sequential modeling
can also be approximated without strictly conditioning on all preceding
patches. Moreover, the models tend to exhibit low dependency redundancy in the
initial layer and higher redundancy in subsequent layers. Leveraging these
observations, we propose a selective Jacobi decoding (SeJD) strategy that
accelerates autoregressive inference through parallel iterative optimization.
Theoretical analyses demonstrate the method's superlinear convergence rate and
guarantee that the number of iterations required is no greater than the
original sequential approach. Empirical evaluations across multiple datasets
validate the generality and effectiveness of our acceleration technique.
Experiments demonstrate substantial speed improvements up to 4.7 times faster
inference while keeping the generation quality and fidelity.

</details>


### [168] [ByzFL: Research Framework for Robust Federated Learning](https://arxiv.org/abs/2505.24802)
*Marc González,Rachid Guerraoui,Rafael Pinot,Geovani Rizk,John Stephan,François Taïani*

Main category: cs.LG

TL;DR: An open-source Python library named ByzFL is presented for developing and benchmarking robust federated learning algorithms. It provides a unified framework, including state-of-the-art robust aggregators, configurable attacks, and FL scenario simulations. ByzFL supports systematic experimentation through a JSON-based configuration file and result visualization utilities.


<details>
  <summary>Details</summary>
Motivation: To address the need for a comprehensive and flexible tool to develop and evaluate robust federated learning algorithms in the presence of various adversarial threat models and heterogeneous data distributions.

Method: ByzFL provides a unified and extensible framework with implementations of state-of-the-art robust aggregators, a suite of configurable attacks, and tools for simulating FL scenarios such as heterogeneous data distributions, multiple training algorithms, and adversarial threat models. Experimentation is enabled via a single JSON-based configuration file.

Result: The library facilitates reproducible research and rapid prototyping of robust FL solutions, compatible with PyTorch tensors and NumPy arrays.

Conclusion: ByzFL is an open-source Python library that enables the development and benchmarking of robust federated learning algorithms, available at https://byzfl.epfl.ch/ and on GitHub.

Abstract: We present ByzFL, an open-source Python library for developing and
benchmarking robust federated learning (FL) algorithms. ByzFL provides a
unified and extensible framework that includes implementations of
state-of-the-art robust aggregators, a suite of configurable attacks, and tools
for simulating a variety of FL scenarios, including heterogeneous data
distributions, multiple training algorithms, and adversarial threat models. The
library enables systematic experimentation via a single JSON-based
configuration file and includes built-in utilities for result visualization.
Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to
facilitate reproducible research and rapid prototyping of robust FL solutions.
ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on
GitHub: https://github.com/LPD-EPFL/byzfl.

</details>


### [169] [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/abs/2505.24823)
*Yinggan Xu,Yue Liu,Zhiqiang Gao,Changnan Peng,Di Luo*

Main category: cs.LG

TL;DR: 尽管大语言模型（LLMs）在解决复杂科学问题方面取得进展，但它们往往无法模拟人类专家基于原则的简洁推理，而是生成冗长且不透明的解决方案。为研究此限制，我们引入了PhySense，一个基于物理原则推理的基准测试。评估显示，多个最先进的LLM未能与专家的推理路径一致，这为开发高效、稳健和可解释的原则基础科学推理AI系统提供了见解。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型虽然能够解决复杂的科学问题，但其推理方式与人类专家相比存在差距，特别是在基于核心物理原则进行高效和可解释的问题解决方面。

Method: 通过设计PhySense这一新型基于物理原则推理的基准测试，该测试对专家来说容易解决，但对缺乏原则优先推理的LLM来说具有挑战性。然后使用多种最先进的LLM和提示类型进行评估。

Result: 评估结果表明，这些LLM一致地未能遵循类似于专家的推理路径。

Conclusion: 需要进一步发展AI系统以实现高效、稳健和可解释的原则基础科学推理。

Abstract: Large language models (LLMs) have rapidly advanced and are increasingly
capable of tackling complex scientific problems, including those in physics.
Despite this progress, current LLMs often fail to emulate the concise,
principle-based reasoning characteristic of human experts, instead generating
lengthy and opaque solutions. This discrepancy highlights a crucial gap in
their ability to apply core physical principles for efficient and interpretable
problem solving. To systematically investigate this limitation, we introduce
PhySense, a novel principle-based physics reasoning benchmark designed to be
easily solvable by experts using guiding principles, yet deceptively difficult
for LLMs without principle-first reasoning. Our evaluation across multiple
state-of-the-art LLMs and prompt types reveals a consistent failure to align
with expert-like reasoning paths, providing insights for developing AI systems
with efficient, robust and interpretable principle-based scientific reasoning.

</details>


### [170] [Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting](https://arxiv.org/abs/2505.24835)
*Fuyuan Lyu,Linfeng Du,Yunpeng Weng,Qiufang Ying,Zhiyan Xu,Wen Zou,Haolun Wu,Xiuqiang He,Xing Tang*

Main category: cs.LG

TL;DR: 在资金分配问题中，传统的预测方法存在目标不匹配和不确定性问题。本文提出了一种无模型假设的风险感知时间序列预测与分配框架（RTS-PnO），具有端到端训练、自适应不确定性校准和对预测模型不可知的特点。该框架在离线实验中使用八个数据集，在在线实验中应用于腾讯FiT的跨境支付业务，表现出色。


<details>
  <summary>Details</summary>
Motivation: 资金分配是金融领域的重要问题，现有的简单预测或预测后优化方法存在目标不匹配的问题，同时最先进的时序预测模型引入了额外的不确定性。

Method: 提出了一种名为RTS-PnO的框架，其特点包括：(i) 端到端训练并使用目标一致性度量，(ii) 自适应预测不确定性校准，(iii) 对预测模型不可知。

Result: 在离线实验中，使用来自三种金融应用类别的八个数据集，RTS-PnO始终优于其他竞争基线。在在线实验中，与产品线方法相比，后悔值减少了8.4%。

Conclusion: RTS-PnO框架解决了目标不匹配和预测结果不确定性的问题，展示了在多种金融应用场景中的优越性能。

Abstract: Fund allocation has been an increasingly important problem in the financial
domain. In reality, we aim to allocate the funds to buy certain assets within a
certain future period. Naive solutions such as prediction-only or
Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the
introduction of the SOTA time series forecasting model inevitably introduces
additional uncertainty in the predicted result. To solve both problems
mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate
(RTS-PnO) framework, which holds no prior assumption on the forecasting models.
Such a framework contains three features: (i) end-to-end training with
objective alignment measurement, (ii) adaptive forecasting uncertainty
calibration, and (iii) agnostic towards forecasting models. The evaluation of
RTS-PnO is conducted over both online and offline experiments. For offline
experiments, eight datasets from three categories of financial applications are
used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other
competitive baselines. The online experiment is conducted on the Cross-Border
Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed
when compared with the product-line approach. The code for the offline
experiment is available at https://github.com/fuyuanlyu/RTS-PnO.

</details>


### [171] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari,Jamie Hayes,Matthew Jagielski,Ilia Shumailov,Milad Nasr,Alina Oprea*

Main category: cs.LG

TL;DR: 模型蒸馏虽然有助于创建更小、可部署的语言模型，但这些模型容易受到对抗性偏差注入的攻击。研究发现，通过少量数据投毒即可在教师模型中注入细微偏差，这些偏差会在学生模型中显著放大。提出了非目标传播和目标传播两种模式，并验证了其在多种偏差类型、蒸馏方法和模态中的有效性。当前防御措施对此类攻击效果有限，需要专门的安全措施。


<details>
  <summary>Details</summary>
Motivation: 尽管模型蒸馏可以生成更小且功能强大的语言模型，但其在广泛部署时面临对抗性操作的风险，特别是在训练过程中可能被注入有偏内容。

Method: 通过在教师模型中进行少量数据投毒注入偏差，并分析这些偏差如何在学生模型中传播和放大。提出非目标传播和目标传播两种模式，分别影响多个任务和特定任务。使用不同的偏差类型、蒸馏方法和模态验证方法的有效性。

Result: 仅用25个投毒样本（0.25%投毒率），学生模型在目标场景中生成有偏响应的概率为76.9%，高于教师模型的69.4%。对于非目标传播，对抗性偏差在未见任务中的学生模型中出现频率高出6倍至29倍。

Conclusion: 当前防御措施对这类攻击效果有限，蒸馏模型存在重大安全漏洞，需要专门的安全防护措施。提出了构建有效的对抗性偏差缓解策略的实际设计原则。

Abstract: Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [172] [From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching](https://arxiv.org/abs/2505.24843)
*Ruqi Bai,Yao Ji,Zeyu Zhou,David I. Inouye*

Main category: cs.LG

TL;DR: Spurious correlations can degrade model performance in new environments. Previous methods either underperform or require test-time data. This paper proposes a data-centric approach, Noisy Counterfactual Matching (NCM), that uses invariant data pairs to enhance robustness without needing test-time data. The method can eliminate spurious features and is validated on both synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To address the issue of model performance degradation due to spurious correlations in new environments, while avoiding the pitfalls of prior methods such as underperformance or reliance on test-time data.

Method: The paper introduces Noisy Counterfactual Matching (NCM), a constraint-based method leveraging invariant data pairs. These are pairs of samples that would have the same prediction with an optimally robust classifier. NCM can function effectively even with a small set of noisy pairs, potentially eliminating one spurious feature per pair. Additionally, for linear causal models, the test domain error is shown to be upper bounded by the in-domain error and a term depending on counterfactual diversity and quality.

Result: NCM is proven effective in enhancing robustness. Theoretical results show that using counterfactual pairs can reduce reliance on spurious features. Empirical validation on a synthetic dataset and real-world benchmarks demonstrates that linear probing on a pretrained backbone improves robustness.

Conclusion: Noisy Counterfactual Matching offers a promising data-centric solution to improve model robustness against spurious correlations without requiring test-time data, supported by both theoretical analysis and experimental results.

Abstract: Spurious correlations can cause model performance to degrade in new
environments. Prior causality-inspired works aim to learn invariant
representations (e.g., IRM) but typically underperform empirical risk
minimization (ERM). Recent alternatives improve robustness by leveraging
test-time data, but such data may be unavailable in practice. To address these
issues, we take a data-centric approach by leveraging invariant data pairs,
pairs of samples that would have the same prediction with the optimally robust
classifier. We prove that certain counterfactual pairs will naturally satisfy
this invariance property and introduce noisy counterfactual matching (NCM), a
simple constraint-based method for leveraging invariant pairs for enhanced
robustness, even with a small set of noisy pairs-in the ideal case, each pair
can eliminate one spurious feature. For linear causal models, we prove that the
test domain error can be upper bounded by the in-domain error and a term that
depends on the counterfactuals' diversity and quality. We validate on a
synthetic dataset and demonstrate on real-world benchmarks that linear probing
on a pretrained backbone improves robustness.

</details>


### [173] [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/abs/2505.24844)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: Chameleon是一种灵活高效的数据混合框架，利用leverage scores量化嵌入空间中的领域重要性，能够有效提升语言模型在不同场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的领域重加权方法通常依赖于昂贵的权重计算，并且在引入新数据时需要重新训练，因此研究者们希望找到一种更灵活、高效的解决方案。

Method: 通过构建领域亲和力矩阵来确定领域嵌入空间中的共同表示，并使用leverage scores计算数据混合权重，从而实现无需重新训练即可适应新数据的能力。

Result: (i) 在预训练领域中，计算出的权重仅用一小部分计算量就提高了性能；(ii) Chameleon可以适应数据变化，提高少样本推理准确性；(iii) 在微调过程中，该方法能有效改善所有微调领域的测试困惑度。

Conclusion: Chameleon框架提供了一种灵活高效的方法来进行数据混合，减少了计算成本并提高了模型在各种场景下的泛化性能。

Abstract: Training data mixtures greatly impact the generalization performance of large
language models. Existing domain reweighting methods often rely on costly
weight computations and require retraining when new data is introduced. To this
end, we introduce a flexible and efficient data mixing framework, Chameleon,
that employs leverage scores to quantify domain importance within a learned
embedding space. We first construct a domain affinity matrix over domain
embeddings. The induced leverage scores determine a mixture that upweights
domains sharing common representations in embedding space. This formulation
allows direct transfer to new data by computing the new domain embeddings. In
experiments, we demonstrate improvements over three key scenarios: (i) our
computed weights improve performance on pretraining domains with a fraction of
the compute of existing methods; (ii) Chameleon can adapt to data changes
without proxy retraining, boosting few-shot reasoning accuracies when
transferred to new data; (iii) our method enables efficient domain reweighting
in finetuning, consistently improving test perplexity on all finetuning domains
over uniform mixture. Our code is available at
https://github.com/LIONS-EPFL/Chameleon.

</details>


### [174] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
*Shuyao Xu,Cheng Peng,Jiangxuan Long,Weidi Xu,Wei Chu,Yuan Qi*

Main category: cs.LG

TL;DR: Recent advances in model distillation show that complex reasoning abilities can be transferred to smaller models. This paper proposes REDI, a two-stage framework that leverages both positive and negative distilled reasoning traces to maximize LLM reasoning performance in an offline setting. Empirical evaluations demonstrate REDI's superiority over other methods.


<details>
  <summary>Details</summary>
Motivation: Standard practices in model distillation discard incorrect reasoning examples, which are valuable yet underutilized data. The paper aims to address how both positive and negative distilled reasoning traces can be effectively used to improve LLM reasoning performance.

Method: The proposed method is called Reinforcement Distillation (REDI), a two-stage framework. Stage 1 involves learning from positive traces via Supervised Fine-Tuning (SFT). Stage 2 refines the model using both positive and negative traces through the REDI objective, a novel reference-free loss function.

Result: Empirical evaluations show that REDI outperforms baseline methods like Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model achieves an 83.1% score on MATH-500 and matches or surpasses other models across various benchmarks.

Conclusion: REDI establishes a new state-of-the-art for 1.5B models post-trained offline with openly available data, demonstrating its effectiveness in leveraging both positive and negative reasoning traces.

Abstract: Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [175] [Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking](https://arxiv.org/abs/2505.24857)
*Heli Ben-Hamu,Itai Gat,Daniel Severo,Niklas Nolte,Brian Karrer*

Main category: cs.LG

TL;DR: EB-Sampler是一种新的采样方法，能加速当前最先进的MDMs的采样过程约2-3倍，在标准编码和数学推理基准上不损失性能。同时，该方法在小型推理任务（如迷宫导航和数独）中也表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管大多数文献集中于提升采样程序的性能，但对MDMs的有效采样探索较少。研究者观察到部分被掩盖的令牌序列通常可以确定性地决定多个未知令牌的值，这意味着单一的预测模型包含未被标准采样过程充分利用的额外信息。

Method: 基于上述观察，研究者引入了EB-Sampler，这是现有采样器的一个简单替代方案，采用熵约束解码过程，能够在一次函数评估中动态解码多个令牌，并具有预定义的近似误差容限。此外，研究者将EB-Sampler作为一个广泛的自适应采样器家族的一部分进行公式化，并提供了错误分析以支持其算法选择。

Result: EB-Sampler能够将当前最先进的MDMs的采样速度提高约2-3倍，在标准编码和数学推理基准上不损失性能。并且验证了该方法在较小的推理任务（包括迷宫导航和数独）中也表现良好，而这些任务往往是ARMs难以处理的。

Conclusion: EB-Sampler为MDMs提供了一种更高效的采样方法，不仅显著提高了采样速度，而且在多种任务中保持甚至提升了性能，展现了其在自然语言处理和其他推理任务中的潜力。

Abstract: Recent masked diffusion models (MDMs) have shown competitive performance
compared to autoregressive models (ARMs) for language modeling. While most
literature has focused on performance enhancing sampling procedures, efficient
sampling from MDMs has been scarcely explored. We make the observation that
often a given sequence of partially masked tokens determines the values of
multiple unknown tokens deterministically, meaning that a single prediction of
a masked model holds additional information unused by standard sampling
procedures. Based on this observation, we introduce EB-Sampler, a simple
drop-in replacement for existing samplers, utilizing an Entropy Bounded
unmasking procedure that dynamically unmasks multiple tokens in one function
evaluation with predefined approximate error tolerance. We formulate the
EB-Sampler as part of a broad family of adaptive samplers for which we provide
an error analysis that motivates our algorithmic choices. EB-Sampler
accelerates sampling from current state of the art MDMs by roughly 2-3x on
standard coding and math reasoning benchmarks without loss in performance. We
also validate the same procedure works well on smaller reasoning tasks
including maze navigation and Sudoku, tasks ARMs often struggle with.

</details>


### [176] [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
*Joschka Braun,Carsten Eickhoff,Seyed Ali Bahrainian*

Main category: cs.LG

TL;DR: Steering vectors can control text properties in free-form generation tasks, but high steering strengths degrade text quality. Combining steering and prompting offers the best trade-off.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of steering vectors in free-form generation tasks beyond multiple-choice settings, focusing on adaptively controlling topical focus, sentiment, toxicity, and readability in abstractive summaries.

Method: Evaluate steering vectors' ability to control targeted summary properties in abstractive summaries of the NEWTS dataset, comparing their performance with prompting and combining both methods.

Result: Steering vectors effectively control targeted summary properties but degrade text quality at high strengths. Prompting offers weaker control while preserving quality. Combining steering and prompting yields the strongest control with a favorable efficacy-quality trade-off.

Conclusion: There is a practical trade-off between control strength and text quality preservation when using steering vectors for free-form generation tasks.

Abstract: Steering vectors are a lightweight method for controlling text properties by
adding a learned bias to language model activations at inference time. So far,
steering vectors have predominantly been evaluated in multiple-choice settings,
while their effectiveness in free-form generation tasks remains understudied.
Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of
steering vectors in adaptively controlling topical focus, sentiment, toxicity,
and readability in abstractive summaries of the NEWTS dataset. We find that
steering effectively controls the targeted summary properties, but high
steering strengths consistently degrade both intrinsic and extrinsic text
quality. Compared to steering, prompting offers weaker control, while
preserving text quality. Combining steering and prompting yields the strongest
control over text properties and offers the most favorable efficacy-quality
trade-off at moderate steering strengths. Our results underscore the practical
trade-off between control strength and text quality preservation when applying
steering vectors to free-form generation tasks.

</details>


### [177] [The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models](https://arxiv.org/abs/2505.24874)
*Adam Stein,Aaditya Naik,Neelay Velingker,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: Neuro-symbolic learning combines neural networks with symbolic programs for complex reasoning tasks. While traditional neuro-symbolic methods face challenges, foundation models offer a promising alternative through neuro-symbolic prompting.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional neuro-symbolic learning and explore the potential of foundation models in achieving the goals of neuro-symbolic learning without the downsides of training from scratch.

Method: Highlighting three pitfalls of traditional neuro-symbolic learning and arguing that foundation models enable generalizable neuro-symbolic solutions.

Result: Foundation models can be used for complex reasoning tasks through neuro-symbolic prompting, offering a path towards achieving the original goals of neuro-symbolic learning.

Conclusion: Foundation models provide a way to achieve the benefits of neuro-symbolic learning, such as interpretability and reliability, without the challenges associated with traditional training methods.

Abstract: Neuro-symbolic learning was proposed to address challenges with training
neural networks for complex reasoning tasks with the added benefits of
interpretability, reliability, and efficiency. Neuro-symbolic learning methods
traditionally train neural models in conjunction with symbolic programs, but
they face significant challenges that limit them to simplistic problems. On the
other hand, purely-neural foundation models now reach state-of-the-art
performance through prompting rather than training, but they are often
unreliable and lack interpretability. Supplementing foundation models with
symbolic programs, which we call neuro-symbolic prompting, provides a way to
use these models for complex reasoning tasks. Doing so raises the question:
What role does specialized model training as part of neuro-symbolic learning
have in the age of foundation models? To explore this question, we highlight
three pitfalls of traditional neuro-symbolic learning with respect to the
compute, data, and programs leading to generalization problems. This position
paper argues that foundation models enable generalizable neuro-symbolic
solutions, offering a path towards achieving the original goals of
neuro-symbolic learning without the downsides of training from scratch.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [178] [A comprehensive survey of cybercrimes in India over the last decade](https://arxiv.org/abs/2505.23770)
*Sudhanshu Sekhar Tripathy*

Main category: cs.CR

TL;DR: Since the 1990s, digital transformation has spurred cybercrime growth. This paper examines cybercrime, prevention strategies, security protocols, and terminology to safeguard digital infrastructure in India.


<details>
  <summary>Details</summary>
Motivation: To address the increasing risks of cybercrime due to digital transformation and technology integration.

Method: Analysis of cybercrime, prevention strategies, security protocols, and terminology related to safeguarding digital infrastructure.

Result: Highlights the surge in cybercrime incidents in India, particularly involving ransomware, data breaches, and social engineering attacks, affecting sectors like banking, healthcare, and government.

Conclusion: There is a need for public awareness, cybersecurity education, and robust regulatory frameworks to combat cyber threats.

Abstract: Since the 1990s, the integration of technology into daily life has led to the
creation of an extensive network of interconnected devices, transforming how
individuals and organizations operate. However, this digital transformation has
also spurred the rise of cybercrime, criminal activities perpetrated through
networks or computer systems. Cybercrime has become a global concern,
presenting significant challenges to security systems. Although advancements in
digital technology have enhanced efficiency, they have also opened new avenues
for exploitation by cybercriminals, highlighting the urgent need for advanced
cybersecurity measures. The escalating number of cyberattacks and associated
risks in the past decade highlights the critical importance of protecting
sensitive data and safeguarding information systems. Cybercrimes range from
financial fraud and phishing scams to identity theft and online harassment,
posing substantial risks to both individuals and organizations. In response,
governments, law enforcement agencies, and cybersecurity units have intensified
their efforts to address these threats. In recent years, India has experienced
a significant surge in cybercrime incidents, with a notable increase in cases
involving ransomware, data breaches, and social engineering attacks. The
growing penetration of internet services, the expansion of e-commerce, and the
rapid adoption of digital payment systems have made individuals and
organizations more vulnerable to cyber threats. Key areas affected include
banking, healthcare, and government sectors, which are frequently targeted due
to the sensitive nature of the data they handle. To combat these risks, there
is an increasing focus on public awareness, cybersecurity education, and robust
regulatory frameworks. This paper examines cybercrime, prevention strategies,
security protocols, and terminology to safeguard digital infrastructure.

</details>


### [179] [AESHA3: Efficient and Secure Sub-Key Generation for AES Using SHA-3](https://arxiv.org/abs/2505.23771)
*Ankush Soni,Sanjay K. Sahay,Parit Mehta*

Main category: cs.CR

TL;DR: A new encryption method AESHA3 is proposed, which uses SHA3 to generate sub-keys. It is faster and more resource-saving than the standard AES, especially suitable for IoT devices.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the weaknesses in the generation of sub-keys in AES and the overhead caused by sequential sub-key generation, particularly for resource-constrained devices.

Method: The method involves proposing a novel encryption method called AESHA3, which utilizes Secure Hash Algorithm-3 (SHA3) to generate sub-keys. The output of SHA3 is described as one-way, highly non-linear, and random.

Result: Experimental analysis indicates that the average time taken for generating sub-keys using AESHA3 is approximately 1300 times faster than with standard AES. The investigation also shows significant data encryption up to two MB and more resource saving with smaller data sizes compared to AES.

Conclusion: AESHA3 is found to be very relevant not only in terms of security but also in saving resources in IoT devices.

Abstract: Advanced Encryption Standard (AES) is one of the most widely used symmetric
cipher for the confidentiality of data. Also it is used for other security
services, viz. integrity, authentication and key establishment. However,
recently, authors have shown some weakness in the generation of sub-keys in
AES, e.g. bit leakage attack, etc. Also, AES sub-keys are generated
sequentially, which is an overhead, especially for resource-constrained
devices. Therefore, we propose and investigate a novel encryption AESHA3, which
uses sub-keys generated by Secure Hash Algorithm-3 (SHA3). The output of SHA3
is one-way and highly non-linear, and random. The experimental analysis shows
that the average time taken for generating the sub-keys to be used for
encrypting the data using our approach i.e. AESHA3 is ~1300 times faster than
the sub-key generated by the standard AES. Accordingly, we find that AESHA3
will be very relevant not only in terms of security but also it will save the
resources in IoT devices. We investigated AESHA3 in Intel Core i7, 6th
Generation processor and Raspberry Pi 4B and found that up to two MB data
encryption is very significant, and lesser the data size, more the resource
saving compared to AES.

</details>


### [180] [Anamorphic Cryptography with Elliptic Curve Methods](https://arxiv.org/abs/2505.23772)
*William J Buchanan*

Main category: cs.CR

TL;DR: In 2022, Persianom, Phan and Yung created Anamorphic Cryptography which allows for a public key to encrypt data with two secret keys decrypting it into different messages. This paper implements this using ECC (Elliptic Curve Cryptography) such as secp256k1 curve providing better performance than discrete logarithm-based methods. The secret message to Alice is hidden within the random nonce value used in encryption.


<details>
  <summary>Details</summary>
Motivation: To provide a method where one party (Alice) can receive a secret message that another party (the Dictator) cannot see despite both being able to decrypt the same ciphertext into different messages.

Method: Implementation of Anamorphic Cryptography using Elliptic Curve Cryptography (ECC), specifically utilizing the secp256k1 curve. This involves creating a public key for encryption and two secret keys for decryption leading to different messages. The secret message intended for Alice is embedded within the random nonce value used in encryption.

Result: Provides significant performance improvements over discrete logarithm-based methods while successfully enabling the Dictator to decrypt all messages and Alice to decrypt her unique secret message.

Conclusion: Anamorphic Cryptography implemented via ECC offers an effective solution for secure communications where multiple parties can decrypt the same ciphertext into distinct messages.

Abstract: In 2022, Persianom, Phan and Yung outlined the creation of Anamorphic
Cryptography. With this, we can create a public key to encrypt data, and then
have two secret keys. These secret keys are used to decrypt the cipher into
different messages. So, one secret key is given to the Dictator (who must be
able to decrypt all the messages), and the other is given to Alice. Alice can
then decrypt the ciphertext to a secret message that the Dictator cannot see.
This paper outlines the implementation of Anamorphic Cryptography using ECC
(Elliptic Curve Cryptography), such as with the secp256k1 curve. This gives
considerable performance improvements over discrete logarithm-based methods.
Overall, it outlines how the secret message sent to Alice is hidden within the
random nonce value, which is used within the encryption process, and which is
cancelled out when the Dictator decrypts the ciphertext.

</details>


### [181] [LightDSA: A Python-Based Hybrid Digital Signature Library and Performance Analysis of RSA, DSA, ECDSA and EdDSA in Variable Configurations, Elliptic Curve Forms and Curves](https://arxiv.org/abs/2505.23773)
*Sefik Serengil,Alper Ozpinar*

Main category: cs.CR

TL;DR: The paper introduces LightDSA, a configurable digital signature library supporting RSA, DSA, ECDSA, and EdDSA with flexible curve and form selection. It allows arbitrary combinations of curves and forms, unlike traditional implementations. The authors analyze the computational efficiency of these algorithms across various configurations to provide insights into security-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: To create a more flexible and hybrid digital signature library that supports multiple algorithms (RSA, DSA, ECDSA, EdDSA) with arbitrary curve-form combinations, enabling broader performance evaluation beyond conventional strict mappings.

Method: Development of LightDSA, a digital signature library allowing flexible form and curve selection for different DSAs. The method involves implementing and analyzing key generation, signing, and verification times across various configurations and comparing the performance of different algorithms.

Result: LightDSA enables a comprehensive analysis of the trade-offs between security and efficiency in digital signatures, offering guidance on selecting optimal configurations for specific cryptographic needs.

Conclusion: LightDSA provides a flexible platform for evaluating digital signature algorithms, demonstrating significant variability in performance depending on configuration choices. This can help users choose the best algorithm and settings based on their security and efficiency requirements.

Abstract: Digital signature algorithms (DSAs) are fundamental to cryptographic
security, ensuring data integrity and authentication. While RSA, DSA, ECDSA,
and EdDSA are widely used, their performance varies significantly depending on
key sizes, hash functions, and elliptic curve configurations. In this paper, we
introduce LightDSA, a hybrid and configurable digital signature library that
supports RSA, DSA, ECDSA, and EdDSA with flexible form and curve selection,
open sourced at https://github.com/serengil/LightDSA. Unlike conventional
implementations that impose strict curve-form mappings - such as Weierstrass
for ECDSA and Edwards for EdDSA LightDSA - allows arbitrary combinations,
enabling a broader performance evaluation. We analyze the computational
efficiency of these algorithms across various configurations, comparing key
generation, signing, and verification times. Our results provide insights into
the trade-offs between security and efficiency, guiding the selection of
optimal configurations for different cryptographic needs.

</details>


### [182] [Mind the Gap: A Practical Attack on GGUF Quantization](https://arxiv.org/abs/2505.23786)
*Kazuki Egashira,Robin Staab,Mark Vero,Jingxuan He,Martin Vechev*

Main category: cs.CR

TL;DR: 在前沿LLM的规模日益增大的背景下，为了实现内存高效的部署，后训练量化已成为标准。然而，基本的基于舍入的量化方案存在安全风险，因为它们可能被利用向量化模型注入恶意行为，而这些行为在全精度下仍然隐藏。现有的攻击无法应用于更复杂的量化方法（如GGUF）。本文通过引入首个针对GGUF的攻击填补了这一空白。研究表明，最广泛使用的后训练量化方法容易受到对抗性干扰，且仅依靠量化方案的复杂性不足以作为防御手段。


<details>
  <summary>Details</summary>
Motivation: 当前用于大型语言模型的后训练量化技术虽然提高了内存效率，但其安全性尚未得到充分验证。尽管已知基础的量化方法存在安全隐患，但更复杂的量化方法（如GGUF）的安全性仍是一个开放问题。因此，研究者希望探索并揭示GGUF等复杂量化方法是否也存在潜在的安全威胁。

Method: 研究者首先发现量化误差（全精度权重与量化版本之间的差异）提供了足够的灵活性来构建恶意的量化模型，这些模型在全精度下表现正常。基于此，他们开发了一种攻击方法，在训练目标恶意LLM时约束其权重以适应量化误差。该方法适用于多种GGUF量化数据类型，并能在不同攻击场景中进行测试。

Result: 该攻击方法在三个流行的LLM上进行了测试，涵盖了九种GGUF量化数据类型和三种不同的攻击场景：不安全代码生成（提升88.7%）、目标内容注入（提升85.0%）和良性指令拒绝（提升30.1%）。结果表明，常用的后训练量化方法容易受到对抗性干扰，而仅仅依赖量化方案的复杂性并不足以防止此类攻击。

Conclusion: 本研究表明，即使是最广泛使用的后训练量化方法也可能容易受到对抗性干扰。此外，仅仅增加量化方案的复杂性并不能有效防御恶意攻击。这提示研究者和从业者需要重新审视量化技术的安全性，并探索更加健壮的防御策略。

Abstract: With the increasing size of frontier LLMs, post-training quantization has
become the standard for memory-efficient deployment. Recent work has shown that
basic rounding-based quantization schemes pose security risks, as they can be
exploited to inject malicious behaviors into quantized models that remain
hidden in full precision. However, existing attacks cannot be applied to more
complex quantization methods, such as the GGUF family used in the popular
ollama and llama.cpp frameworks. In this work, we address this gap by
introducing the first attack on GGUF. Our key insight is that the quantization
error -- the difference between the full-precision weights and their
(de-)quantized version -- provides sufficient flexibility to construct
malicious quantized models that appear benign in full precision. Leveraging
this, we develop an attack that trains the target malicious LLM while
constraining its weights based on quantization errors. We demonstrate the
effectiveness of our attack on three popular LLMs across nine GGUF quantization
data types on three diverse attack scenarios: insecure code generation
($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign
instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the
most widely used post-training quantization method is susceptible to
adversarial interferences, and (2) the complexity of quantization schemes alone
is insufficient as a defense.

</details>


### [183] [Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning](https://arxiv.org/abs/2505.23791)
*Sayyed Farid Ahamed,Sandip Roy,Soumya Banerjee,Marc Vucovich,Kevin Choi,Abdul Rahman,Alison Hu,Edward Bowen,Sachin Shetty*

Main category: cs.CR

TL;DR: Federated Learning, despite its privacy-preserving goals, is highly vulnerable to model extraction attacks. This paper explores two types of such attacks on FL-based models using various deep learning architectures and datasets. The study finds that the success of these attacks depends heavily on the size of the attack query set. Additionally, a transfer learning approach increases the accuracy and fidelity of the extracted models, especially with smaller query sets.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the need to address the vulnerabilities in Federated Learning (FL) systems against Intellectual Property (IP) threats, particularly model extraction (ME) attacks. While FL aims to protect client data, its distributed nature makes it susceptible to ME attacks, which can replicate confidential models by querying black-box APIs.

Method: The researchers implemented ME attacks on various federated clients built under the NVFlare platform across two deep learning architectures and three image datasets. They evaluated the performance of these attacks using metrics like accuracy, fidelity, and KL divergence. Furthermore, they explored a transfer learning based approach where pretrained models were fine-tuned to serve as the starting point for the extraction process.

Result: The experiments demonstrated that the accuracy and fidelity of the extracted models are closely related to the size of the attack query set. Using transfer learning significantly improved the accuracy and fidelity of the extracted models, particularly when the query sets were small.

Conclusion: Federated Learning systems are highly vulnerable to model extraction attacks. The effectiveness of these attacks can be enhanced through the use of transfer learning, which poses additional risks to the confidentiality of models in FL-based systems.

Abstract: Federated Learning (FL) is a collaborative learning framework designed to
protect client data, yet it remains highly vulnerable to Intellectual Property
(IP) threats. Model extraction (ME) attacks pose a significant risk to Machine
Learning as a Service (MLaaS) platforms, enabling attackers to replicate
confidential models by querying black-box (without internal insight) APIs.
Despite FL's privacy-preserving goals, its distributed nature makes it
particularly susceptible to such attacks. This paper examines the vulnerability
of FL-based victim models to two types of model extraction attacks. For various
federated clients built under the NVFlare platform, we implemented ME attacks
across two deep learning architectures and three image datasets. We evaluate
the proposed ME attack performance using various metrics, including accuracy,
fidelity, and KL divergence. The experiments show that for different FL
clients, the accuracy and fidelity of the extracted model are closely related
to the size of the attack query set. Additionally, we explore a transfer
learning based approach where pretrained models serve as the starting point for
the extraction process. The results indicate that the accuracy and fidelity of
the fine-tuned pretrained extraction models are notably higher, particularly
with smaller query sets, highlighting potential advantages for attackers.

</details>


### [184] [Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things](https://arxiv.org/abs/2505.23792)
*Kai Li,Conggai Li,Xin Yuan,Shenghong Li,Sai Zou,Syed Sohail Ahmed,Wei Ni,Dusit Niyato,Abbas Jamalipour,Falko Dressler,Ozgur B. Akan*

Main category: cs.CR

TL;DR: This paper introduces Zero-Trust Foundation Models (ZTFMs) for IoT systems, integrating zero-trust security principles into foundation models through a technical framework including federated learning and blockchain-based identity management. It addresses emerging security threats and outlines research challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance security and privacy in AI-driven IoT systems by embedding zero-trust security principles into the lifecycle of foundation models.

Method: Integrating continuous verification, least privilege access, data confidentiality, and behavioral analytics into the design, training, and deployment of foundation models using a framework that includes federated learning, blockchain-based identity management, micro-segmentation, and trusted execution environments.

Result: Identifies ZTFMs' potential to transform conventional IoT architectures into resilient ecosystems, evaluates countermeasures for security threats, and highlights open research challenges.

Conclusion: The paper provides a roadmap for secure, intelligent, and trustworthy IoT infrastructures powered by foundation models.

Abstract: This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm
that embeds zero-trust security principles into the lifecycle of foundation
models (FMs) for Internet of Things (IoT) systems. By integrating core tenets,
such as continuous verification, least privilege access (LPA), data
confidentiality, and behavioral analytics into the design, training, and
deployment of FMs, ZTFMs can enable secure, privacy-preserving AI across
distributed, heterogeneous, and potentially adversarial IoT environments. We
present the first structured synthesis of ZTFMs, identifying their potential to
transform conventional trust-based IoT architectures into resilient,
self-defending ecosystems. Moreover, we propose a comprehensive technical
framework, incorporating federated learning (FL), blockchain-based identity
management, micro-segmentation, and trusted execution environments (TEEs) to
support decentralized, verifiable intelligence at the network edge. In
addition, we investigate emerging security threats unique to ZTFM-enabled
systems and evaluate countermeasures, such as anomaly detection, adversarial
training, and secure aggregation. Through this analysis, we highlight key open
research challenges in terms of scalability, secure orchestration,
interpretable threat attribution, and dynamic trust calibration. This survey
lays a foundational roadmap for secure, intelligent, and trustworthy IoT
infrastructures powered by FMs.

</details>


### [185] [USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2505.23793)
*Baolin Zheng,Guanlin Chen,Hongqiong Zhong,Qingyang Teng,Yingshui Tan,Zhendong Liu,Weixun Wang,Jiaheng Liu,Jian Yang,Huiyun Jing,Jincheng Wei,Wenbo Su,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.CR

TL;DR: 尽管多模态大语言模型（MLLMs）取得了显著成就并被广泛应用，但其安全漏洞问题日益凸显，亟需可靠的评估基准。现有的MLLM安全基准在数据质量和覆盖范围上存在不足，并且通常单独考虑对有害查询的脆弱性和对无害查询的过度敏感性。本文提出了统一安全基准（USB），作为最全面的MLLM安全评估基准之一，涵盖了高质量查询、广泛的风险类别、综合的模态组合以及脆弱性和过度敏感性的评估。通过结合开源数据集与合成数据，USB为61个风险子类别提供了4种不同的模态组合，涵盖英汉双语及脆弱性和过度敏感性两个维度。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的安全漏洞问题日益严重，而现有安全基准存在数据质量差、覆盖范围不足的问题，且未能同时考虑对有害查询的脆弱性和对无害查询的过度敏感性。这阻碍了对安全问题的发现和治理。因此，需要一个更全面、更高质量的安全评估基准来解决这些问题。

Method: 提出了一种名为统一安全基准（USB）的评估方法，该方法具有高质量查询、广泛的风险类别、综合的模态组合，并同时评估脆弱性和过度敏感性。通过设计复杂的数据合成管道，生成高质量的补充数据，填补了先前未探索的领域。USB结合了开源数据集和合成数据，为每个风险子类别提供4种不同的模态组合，涵盖英语和中文两种语言。

Result: USB成为目前最全面的MLLM安全评估基准之一，能够有效评估模型在不同风险类别和模态组合下的表现，同时考虑了脆弱性和过度敏感性。通过对比现有基准，证明了USB在覆盖范围和数据质量上的优越性。

Conclusion: 统一安全基准（USB）为多模态大语言模型的安全评估提供了一个更全面、更高质量的解决方案，有助于更好地发现和治理安全问题。

Abstract: Despite their remarkable achievements and widespread adoption, Multimodal
Large Language Models (MLLMs) have revealed significant security
vulnerabilities, highlighting the urgent need for robust safety evaluation
benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of
data quality and coverge, and modal risk combinations, resulting in inflated
and contradictory evaluation results, which hinders the discovery and
governance of security concerns. Besides, we argue that vulnerabilities to
harmful queries and oversensitivity to harmless ones should be considered
simultaneously in MLLMs safety evaluation, whereas these were previously
considered separately. In this paper, to address these shortcomings, we
introduce Unified Safety Benchmarks (USB), which is one of the most
comprehensive evaluation benchmarks in MLLM safety. Our benchmark features
high-quality queries, extensive risk categories, comprehensive modal
combinations, and encompasses both vulnerability and oversensitivity
evaluations. From the perspective of two key dimensions: risk categories and
modality combinations, we demonstrate that the available benchmarks -- even the
union of the vast majority of them -- are far from being truly comprehensive.
To bridge this gap, we design a sophisticated data synthesis pipeline that
generates extensive, high-quality complementary data addressing previously
unexplored aspects. By combining open-source datasets with our synthetic data,
our benchmark provides 4 distinct modality combinations for each of the 61 risk
sub-categories, covering both English and Chinese across both vulnerability and
oversensitivity dimensions.

</details>


### [186] [Strengthening Cybersecurity Resilience in Agriculture Through Educational Interventions: A Case Study of the Ponca Tribe of Nebraska](https://arxiv.org/abs/2505.23800)
*George Grispos,Logan Mears,Larry Loucks*

Main category: cs.CR

TL;DR: The paper discusses the Cybersecurity Improvement Initiative for Agriculture (CIIA), an educational intervention aimed at enhancing cybersecurity awareness among farmers, particularly those from the Ponca Tribe of Nebraska. Through pre- and post-intervention surveys, it was found that while there was a significant lack of cybersecurity knowledge initially, the intervention led to improvements in understanding key concepts such as password hygiene, multi-factor authentication, and data backups.


<details>
  <summary>Details</summary>
Motivation: To address the growing cybersecurity challenges faced by the agricultural community due to increasing digitization, and to strengthen cybersecurity awareness and resilience among farmers and food producers.

Method: The research employed a case study approach focusing on farmers from the Ponca Tribe of Nebraska. Pre- and post-intervention surveys were used to evaluate participants' cybersecurity knowledge and awareness before and after exposure to the CIIA.

Result: Findings revealed a substantial baseline deficiency in cybersecurity education among participants, but post-intervention assessments showed improvements in the understanding of cybersecurity concepts like password hygiene, multi-factor authentication, and routine data backups.

Conclusion: There is a need for continued and sustained, community-specific cybersecurity education efforts to mitigate emerging cyber threats in the agricultural sector.

Abstract: The increasing digitization of agricultural operations has introduced new
cybersecurity challenges for the farming community. This paper introduces an
educational intervention called Cybersecurity Improvement Initiative for
Agriculture (CIIA), which aims to strengthen cybersecurity awareness and
resilience among farmers and food producers. Using a case study that focuses on
farmers from the Ponca Tribe of Nebraska, the research evaluates pre- and post-
intervention survey data to assess participants' cybersecurity knowledge and
awareness before and after exposure to the CIIA. The findings reveal a
substantial baseline deficiency in cybersecurity education among participants,
however, post-intervention assessments demonstrate improvements in the
comprehension of cybersecurity concepts, such as password hygiene, multi-factor
authentication, and the necessity of routine data backups. These initial
findings highlight the need for a continued and sustained, community-specific
cybersecurity education effort to help mitigate emerging cyber threats in the
agricultural sector.

</details>


### [187] [MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection](https://arxiv.org/abs/2505.23803)
*Yinuo Xue,Eric Spero,Yun Sing Koh,Giovanni Russello*

Main category: cs.CR

TL;DR: MultiPhishGuard is a dynamic LLM-based multi-agent detection system that significantly outperforms existing phishing detectors with high accuracy and low false positive/negative rates.


<details>
  <summary>Details</summary>
Motivation: Phishing email detection faces challenges from evolving adversarial tactics and heterogeneous attack patterns. Traditional methods struggle to keep pace, while machine learning approaches face challenges adapting to novel phishing strategies.

Method: A framework with five cooperative agents (text, URL, metadata, explanation simplifier, and adversarial agents) is employed, with automatically adjusted decision weights powered by a Proximal Policy Optimization reinforcement learning algorithm. An adversarial training loop featuring an adversarial agent that generates subtle context-aware email variants is introduced.

Result: Experimental evaluations on public datasets demonstrate that MultiPhishGuard significantly outperforms Chain-of-Thoughts, single-agent baselines and state-of-the-art detectors, achieving high accuracy (97.89%) with low false positive (2.73%) and false negative rates (0.20%).

Conclusion: This work advances phishing defense through dynamic multi-agent collaboration and generative adversarial resilience.

Abstract: Phishing email detection faces critical challenges from evolving adversarial
tactics and heterogeneous attack patterns. Traditional detection methods, such
as rule-based filters and denylists, often struggle to keep pace with these
evolving tactics, leading to false negatives and compromised security. While
machine learning approaches have improved detection accuracy, they still face
challenges adapting to novel phishing strategies. We present MultiPhishGuard, a
dynamic LLM-based multi-agent detection system that synergizes specialized
expertise with adversarial-aware reinforcement learning. Our framework employs
five cooperative agents (text, URL, metadata, explanation simplifier, and
adversarial agents) with automatically adjusted decision weights powered by a
Proximal Policy Optimization reinforcement learning algorithm. To address
emerging threats, we introduce an adversarial training loop featuring an
adversarial agent that generates subtle context-aware email variants, creating
a self-improving defense ecosystem and enhancing system robustness.
Experimental evaluations on public datasets demonstrate that MultiPhishGuard
significantly outperforms Chain-of-Thoughts, single-agent baselines and
state-of-the-art detectors, as validated by ablation studies and comparative
analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy
(97.89\%) with low false positive (2.73\%) and false negative rates (0.20\%).
Additionally, we incorporate an explanation simplifier agent, which provides
users with clear and easily understandable explanations for why an email is
classified as phishing or legitimate. This work advances phishing defense
through dynamic multi-agent collaboration and generative adversarial
resilience.

</details>


### [188] [ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes](https://arxiv.org/abs/2505.23805)
*Akram Sheriff,Ken Huang,Zsolt Nemeth,Madjid Nakhjiri*

Main category: cs.CR

TL;DR: The paper introduces Adaptive Defense Agent (ADA), an Automated Moving Target Defense (AMTD) system enhancing AI workload security by continuously rotating workloads at the infrastructure level using Kubernetes pods ephemerality. This invalidates attacker assumptions and disrupts kill chains, applying chaos engineering principles as a proactive defense. ADA supports advancements in AI ecosystems, uses cloud-native Kubernetes technologies for easier deployment and maintenance, and promotes faster adoption with a robust zero-trust model.


<details>
  <summary>Details</summary>
Motivation: To fundamentally enhance the security posture of AI workloads by introducing a new approach to Automated Moving Target Defense (AMTD). Traditional static defenses are complex and expensive, relying on confidential or trusted computing solutions, while ADA aims to provide a more robust, agile, and operationally efficient zero-trust model through proactive environmental manipulation.

Method: ADA operates by continuously and automatically rotating AI workloads at the infrastructure level using the ephemerality of Kubernetes pods. It regularly destroys and respawns AI service instances, thus systematically invalidating attacker assumptions and disrupting potential kill chains. The methodology applies principles of chaos engineering as a continuous, proactive defense mechanism.

Result: ADA's novel approach offers a paradigm shift from traditional static defenses. It supports the latest advancements in agentic and nonagentic AI ecosystems, facilitates easier deployment and maintenance, and achieves security through proactive environmental manipulation rather than reactive patching.

Conclusion: ADA provides a more robust, agile, and operationally efficient zero-trust model for AI services. By leveraging widely proliferated cloud-native Kubernetes technologies and achieving an inherent zero trust posture through rotation, it enhances the security posture of AI workloads significantly.

Abstract: This paper introduces the Adaptive Defense Agent (ADA), an innovative
Automated Moving Target Defense (AMTD) system designed to fundamentally enhance
the security posture of AI workloads. ADA operates by continuously and
automatically rotating these workloads at the infrastructure level, leveraging
the inherent ephemerality of Kubernetes pods. This constant managed churn
systematically invalidates attacker assumptions and disrupts potential kill
chains by regularly destroying and respawning AI service instances. This
methodology, applying principles of chaos engineering as a continuous,
proactive defense, offers a paradigm shift from traditional static defenses
that rely on complex and expensive confidential or trusted computing solutions
to secure the underlying compute platforms, while at the same time agnostically
supporting the latest advancements in agentic and nonagentic AI ecosystems and
solutions such as agent-to-agent (A2A) communication frameworks or model
context protocols (MCP). This AI-native infrastructure design, relying on the
widely proliferated cloud-native Kubernetes technologies, facilitates easier
deployment, simplifies maintenance through an inherent zero trust posture
achieved by rotation, and promotes faster adoption. We posit that ADA's novel
approach to AMTD provides a more robust, agile, and operationally efficient
zero-trust model for AI services, achieving security through proactive
environmental manipulation rather than reactive patching.

</details>


### [189] [DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries](https://arxiv.org/abs/2505.23813)
*Abhijit Talluri*

Main category: cs.CR

TL;DR: The paper introduces DP-RTFL, an advanced FL framework integrating local Differential Privacy (LDP) with resilient temporal state management and integrity verification mechanisms for ensuring training continuity, precise state recovery, and strong data privacy.


<details>
  <summary>Details</summary>
Motivation: Standard FL strategies encounter significant operational challenges related to fault tolerance, system resilience against concurrent client and server failures, and the provision of robust, verifiable privacy guarantees.

Method: DP-RTFL integrates local Differential Privacy (LDP) at the client level with resilient temporal state management and integrity verification mechanisms, such as hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs in this context).

Result: This framework is particularly suited for critical applications like credit risk assessment using sensitive financial data, aiming to be operationally robust, auditable, and scalable for enterprise AI deployments.

Conclusion: The implementation of the DP-RTFL framework is available as open-source.

Abstract: Federated Learning (FL) has emerged as a critical paradigm for enabling
privacy-preserving machine learning, particularly in regulated sectors such as
finance and healthcare. However, standard FL strategies often encounter
significant operational challenges related to fault tolerance, system
resilience against concurrent client and server failures, and the provision of
robust, verifiable privacy guarantees essential for handling sensitive data.
These deficiencies can lead to training disruptions, data loss, compromised
model integrity, and non-compliance with data protection regulations (e.g.,
GDPR, CCPA). This paper introduces Differentially Private Resilient Temporal
Federated Learning (DP-RTFL), an advanced FL framework designed to ensure
training continuity, precise state recovery, and strong data privacy. DP-RTFL
integrates local Differential Privacy (LDP) at the client level with resilient
temporal state management and integrity verification mechanisms, such as
hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs
in this context). The framework is particularly suited for critical
applications like credit risk assessment using sensitive financial data, aiming
to be operationally robust, auditable, and scalable for enterprise AI
deployments. The implementation of the DP-RTFL framework is available as
open-source.

</details>


### [190] [Watermarking Without Standards Is Not AI Governance](https://arxiv.org/abs/2505.23814)
*Alexander Nemecek,Yuzhou Jiang,Erman Ayday*

Main category: cs.CR

TL;DR: Watermarking for generative AI content faces challenges due to technical limitations and misaligned incentives. This paper proposes a three-layer framework to enhance its effectiveness in governance.


<details>
  <summary>Details</summary>
Motivation: Current watermarking implementations may only provide an illusion of compliance rather than effective oversight, with a gap between regulatory expectations and technical capabilities.

Method: Analyze policy proposals and industry practices to identify issues in incentive structures and propose a three-layer framework including technical standards, audit infrastructure, and enforcement mechanisms.

Result: A proposed framework that could realign watermarking with governance goals by addressing technical, audit, and enforcement needs.

Conclusion: Without enforceable requirements and independent verification, watermarking remains insufficient for accountability, potentially undermining AI safety and regulation efforts.

Abstract: Watermarking has emerged as a leading technical proposal for attributing
generative AI content and is increasingly cited in global governance
frameworks. This paper argues that current implementations risk serving as
symbolic compliance rather than delivering effective oversight. We identify a
growing gap between regulatory expectations and the technical limitations of
existing watermarking schemes. Through analysis of policy proposals and
industry practices, we show how incentive structures disincentivize robust,
auditable deployments. To realign watermarking with governance goals, we
propose a three-layer framework encompassing technical standards, audit
infrastructure, and enforcement mechanisms. Without enforceable requirements
and independent verification, watermarking will remain inadequate for
accountability and ultimately undermine broader efforts in AI safety and
regulation.

</details>


### [191] [System Prompt Extraction Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2505.23817)
*Badhan Chandra Das,M. Hadi Amini,Yanzhao Wu*

Main category: cs.CR

TL;DR: The paper presents SPE-LLM, a framework for evaluating and defending against system prompt extraction attacks in LLMs.


<details>
  <summary>Details</summary>
Motivation: System prompts in LLMs are crucial for guiding model behavior but are vulnerable to extraction attacks, posing privacy and security risks.

Method: 1) Design novel adversarial queries to extract system prompts. 2) Propose three defense techniques against extraction attacks. 3) Introduce rigorous evaluation metrics and conduct experiments on benchmark datasets.

Result: Demonstrates severe risks of system prompt extraction and validates the effectiveness of the SPE-LLM framework through comprehensive experiments.

Conclusion: SPE-LLM provides a systematic approach to evaluate and defend against system prompt extraction attacks in LLMs.

Abstract: The system prompt in Large Language Models (LLMs) plays a pivotal role in
guiding model behavior and response generation. Often containing private
configuration details, user roles, and operational instructions, the system
prompt has become an emerging attack target. Recent studies have shown that LLM
system prompts are highly susceptible to extraction attacks through
meticulously designed queries, raising significant privacy and security
concerns. Despite the growing threat, there is a lack of systematic studies of
system prompt extraction attacks and defenses. In this paper, we present a
comprehensive framework, SPE-LLM, to systematically evaluate System Prompt
Extraction attacks and defenses in LLMs. First, we design a set of novel
adversarial queries that effectively extract system prompts in state-of-the-art
(SOTA) LLMs, demonstrating the severe risks of LLM system prompt extraction
attacks. Second, we propose three defense techniques to mitigate system prompt
extraction attacks in LLMs, providing practical solutions for secure LLM
deployments. Third, we introduce a set of rigorous evaluation metrics to
accurately quantify the severity of system prompt extraction attacks in LLMs
and conduct comprehensive experiments across multiple benchmark datasets, which
validates the efficacy of our proposed SPE-LLM framework.

</details>


### [192] [SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking](https://arxiv.org/abs/2505.23821)
*Lingfeng Yao,Chenpei Huang,Shengyao Wang,Junpei Xue,Hanqing Guo,Jiang Liu,Xun Chen,Miao Pan*

Main category: cs.CR

TL;DR: 本研究提出了SpeechVerifier，一种不依赖外部参考数据的语音完整性验证方法，通过多尺度特征提取、对比学习生成指纹以及分段水印技术，实现了对恶意篡改的有效检测和对良性操作的鲁棒性。实验结果证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的兴起，有影响力的公众人物的演讲被恶意篡改，严重影响了社会稳定和公众信任。现有的语音篡改检测方法存在不足：它们要么依赖外部参考数据，要么无法同时对攻击敏感并对良性操作（如压缩和重采样）具有鲁棒性。

Method: 为了解决这些挑战，引入了SpeechVerifier，它仅使用已发布的语音本身来主动验证语音完整性，而无需任何外部参考。受音频指纹和水印技术的启发，SpeechVerifier利用多尺度特征提取来捕捉不同时间分辨率下的语音特征，然后采用对比学习生成可以检测各种粒度修改的指纹。这些指纹对良性操作具有鲁棒性，但在发生恶意篡改时会表现出显著变化。为了实现自我包含的语音验证，生成的指纹通过分段水印嵌入到语音信号中。在没有外部参考的情况下，SpeechVerifier可以从已发布的音频中检索指纹，并将其与嵌入的水印进行比较以验证语音的完整性。

Result: 大量的实验结果表明，所提出的SpeechVerifier在检测篡改攻击方面是有效的，并且对良性操作具有鲁棒性。

Conclusion: 本文提出了一种名为SpeechVerifier的新方法，该方法能够在不依赖外部参考数据的情况下，有效检测语音篡改并抵抗良性操作的影响。这为解决现有语音篡改检测方法的不足提供了一个新的解决方案。

Abstract: With the surge of social media, maliciously tampered public speeches,
especially those from influential figures, have seriously affected social
stability and public trust. Existing speech tampering detection methods remain
insufficient: they either rely on external reference data or fail to be both
sensitive to attacks and robust to benign operations, such as compression and
resampling. To tackle these challenges, we introduce SpeechVerifer to
proactively verify speech integrity using only the published speech itself,
i.e., without requiring any external references. Inspired by audio
fingerprinting and watermarking, SpeechVerifier can (i) effectively detect
tampering attacks, (ii) be robust to benign operations and (iii) verify the
integrity only based on published speeches. Briefly, SpeechVerifier utilizes
multiscale feature extraction to capture speech features across different
temporal resolutions. Then, it employs contrastive learning to generate
fingerprints that can detect modifications at varying granularities. These
fingerprints are designed to be robust to benign operations, but exhibit
significant changes when malicious tampering occurs. To enable speech
verification in a self-contained manner, the generated fingerprints are then
embedded into the speech signal by segment-wise watermarking. Without external
references, SpeechVerifier can retrieve the fingerprint from the published
audio and check it with the embedded watermark to verify the integrity of the
speech. Extensive experimental results demonstrate that the proposed
SpeechVerifier is effective in detecting tampering attacks and robust to benign
operations.

</details>


### [193] [Privacy-Preserving Inconsistency Measurement](https://arxiv.org/abs/2505.23825)
*Carl Corea,Timotheus Kampik,Nico Potyka*

Main category: cs.CR

TL;DR: This paper explores a new method of privacy-preserving inconsistency measurement for multi-party communication, developing two concrete methods using secure multi-party computation and cryptographic protocols that satisfy important properties such as input privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to quantitatively assess the degree of inconsistency between knowledge bases in multi-party communication without revealing their actual contents.

Method: By using secure multi-party computation (SMPC) and cryptographic protocols, the authors develop two concrete methods for inconsistency measurement.

Result: The developed methods successfully satisfy important properties of SMPC protocols, including input privacy.

Conclusion: A new form of privacy-preserving inconsistency measurement has been investigated and shown to be effective via two developed methods.

Abstract: We investigate a new form of (privacy-preserving) inconsistency measurement
for multi-party communication. Intuitively, for two knowledge bases K_A, K_B
(of two agents A, B), our results allow to quantitatively assess the degree of
inconsistency for K_A U K_B without having to reveal the actual contents of the
knowledge bases. Using secure multi-party computation (SMPC) and cryptographic
protocols, we develop two concrete methods for this use-case and show that they
satisfy important properties of SMPC protocols -- notably, input privacy, i.e.,
jointly computing the inconsistency degree without revealing the inputs.

</details>


### [194] [Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM](https://arxiv.org/abs/2505.23828)
*Lei Yu,Yechao Zhang,Ziqi Zhou,Yang Wu,Wei Wan,Minghui Li,Shengshan Hu,Pei Xiaobing,Jing Wang*

Main category: cs.CR

TL;DR: This paper addresses the vulnerability of Retrieval-Augmented Generation (RAG) in Vision-Language Models (VLMs) under poisoning attacks. It proposes a new attack method, Spa-VLM, which crafts malicious multi-modal knowledge to mislead VLM outputs. Evaluated on Wikipedia datasets and different RAGs, Spa-VLM shows high stealthiness and effectiveness, with an attack success rate exceeding 0.8 after injecting only 5 malicious entries into large knowledge bases.


<details>
  <summary>Details</summary>
Motivation: Existing single-modal RAG poisoning attacks fail completely in multi-modal RAG scenarios for VLMs, highlighting a need for effective multi-modal RAG poisoning techniques.

Method: The authors propose Spa-VLM, a method that creates malicious multi-modal knowledge entries (adversarial images and misleading text) and injects them into the RAG's knowledge base. This leads to misleading outputs when users access the VLM service.

Result: Spa-VLM achieves highly stealthy poisoning with an attack success rate exceeding 0.8 after injecting just 5 malicious entries into knowledge bases with 100K and 2M entries. Several defense mechanisms were ineffective against Spa-VLM.

Conclusion: The proposed Spa-VLM demonstrates significant vulnerabilities in RAG-based VLMs under poisoning attacks, showing its effectiveness and robustness compared to existing methods.

Abstract: With the rapid development of the Vision-Language Model (VLM), significant
progress has been made in Visual Question Answering (VQA) tasks. However,
existing VLM often generate inaccurate answers due to a lack of up-to-date
knowledge. To address this issue, recent research has introduced
Retrieval-Augmented Generation (RAG) techniques, commonly used in Large
Language Models (LLM), into VLM, incorporating external multi-modal knowledge
to enhance the accuracy and practicality of VLM systems. Nevertheless, the RAG
in LLM may be susceptible to data poisoning attacks. RAG-based VLM may also
face the threat of this attack. This paper first reveals the vulnerabilities of
the RAG-based large model under poisoning attack, showing that existing
single-modal RAG poisoning attacks have a 100\% failure rate in multi-modal RAG
scenarios. To address this gap, we propose Spa-VLM (Stealthy Poisoning Attack
on RAG-based VLM), a new paradigm for poisoning attacks on large models. We
carefully craft malicious multi-modal knowledge entries, including adversarial
images and misleading text, which are then injected into the RAG's knowledge
base. When users access the VLM service, the system may generate misleading
outputs. We evaluate Spa-VLM on two Wikipedia datasets and across two different
RAGs. Results demonstrate that our method achieves highly stealthy poisoning,
with the attack success rate exceeding 0.8 after injecting just 5 malicious
entries into knowledge bases with 100K and 2M entries, outperforming
state-of-the-art poisoning attacks designed for RAG-based LLMs. Additionally,
we evaluated several defense mechanisms, all of which ultimately proved
ineffective against Spa-VLM, underscoring the effectiveness and robustness of
our attack.

</details>


### [195] [GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance](https://arxiv.org/abs/2505.23839)
*Zaixi Zhang,Zhenghong Zhou,Ruofan Jin,Le Cong,Mengdi Wang*

Main category: cs.CR

TL;DR: This paper introduces GeneBreaker, a framework to evaluate vulnerabilities of DNA foundation models. It successfully jailbreaks the latest Evo series models across 6 viral categories and demonstrates biosecurity risks.


<details>
  <summary>Details</summary>
Motivation: DNA Foundation Models have achieved success in designing synthetic functional DNA sequences but their susceptibility to jailbreaking remains underexplored, leading to potential concern of generating harmful sequences such as pathogens or toxin-producing genes.

Method: GeneBreaker employs an LLM agent with customized bioinformatic tools, beam search guided by PathoLM and log-probability heuristics, and a BLAST-based evaluation pipeline against a curated Human Pathogen Database (JailbreakDNABench).

Result: Evaluated on JailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series models across 6 viral categories consistently (up to 60% Attack Success Rate for Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1 envelope protein demonstrate the sequence and structural fidelity of jailbreak output.

Conclusion: Our findings reveal that scaling DNA foundation models amplifies dual-use risks, motivating enhanced safety alignment and tracing mechanisms.

Abstract: DNA, encoding genetic instructions for almost all living organisms, fuels
groundbreaking advances in genomics and synthetic biology. Recently, DNA
Foundation Models have achieved success in designing synthetic functional DNA
sequences, even whole genomes, but their susceptibility to jailbreaking remains
underexplored, leading to potential concern of generating harmful sequences
such as pathogens or toxin-producing genes. In this paper, we introduce
GeneBreaker, the first framework to systematically evaluate jailbreak
vulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent
with customized bioinformatic tools to design high-homology, non-pathogenic
jailbreaking prompts, (2) beam search guided by PathoLM and log-probability
heuristics to steer generation toward pathogen-like sequences, and (3) a
BLAST-based evaluation pipeline against a curated Human Pathogen Database
(JailbreakDNABench) to detect successful jailbreaks. Evaluated on our
JailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series
models across 6 viral categories consistently (up to 60\% Attack Success Rate
for Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1
envelope protein demonstrate the sequence and structural fidelity of jailbreak
output, while evolutionary modeling of SARS-CoV-2 underscores biosecurity
risks. Our findings also reveal that scaling DNA foundation models amplifies
dual-use risks, motivating enhanced safety alignment and tracing mechanisms.
Our code is at https://github.com/zaixizhang/GeneBreaker.

</details>


### [196] [Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems](https://arxiv.org/abs/2505.23847)
*Ronny Ko,Jiseong Jeong,Shuyuan Zheng,Chuan Xiao,Taewan Kim,Makoto Onizuka,Wonyong Shin*

Main category: cs.CR

TL;DR: This position paper maps the security agenda for cross-domain multi-agent LLM systems, introducing seven categories of novel security challenges with plausible attacks, security evaluation metrics, and future research guidelines.


<details>
  <summary>Details</summary>
Motivation: Large language models are evolving into autonomous agents that can collaborate across organizational boundaries for tasks like disaster response and supply-chain optimization. However, cross-domain collaboration breaks the unified trust assumptions behind current alignment and containment techniques, leading to potential risks from emergent multi-agent dynamics.

Method: The paper identifies and categorizes seven novel security challenges for cross-domain multi-agent LLM systems, providing plausible attacks, security evaluation metrics, and future research guidelines for each category.

Result: The paper successfully outlines a security agenda by defining the challenges, potential attacks, and necessary security measures for cross-domain multi-agent LLM systems.

Conclusion: Cross-domain multi-agent LLM systems present new security challenges that require further research and development of appropriate security measures and evaluation metrics.

Abstract: Large language models (LLMs) are rapidly evolving into autonomous agents that
cooperate across organizational boundaries, enabling joint disaster response,
supply-chain optimization, and other tasks that demand decentralized expertise
without surrendering data ownership. Yet, cross-domain collaboration shatters
the unified trust assumptions behind current alignment and containment
techniques. An agent benign in isolation may, when receiving messages from an
untrusted peer, leak secrets or violate policy, producing risks driven by
emergent multi-agent dynamics rather than classical software bugs. This
position paper maps the security agenda for cross-domain multi-agent LLM
systems. We introduce seven categories of novel security challenges, for each
of which we also present plausible attacks, security evaluation metrics, and
future research guidelines.

</details>


### [197] [CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning](https://arxiv.org/abs/2505.23849)
*Kaveen Hiniduma,Zilinghan Li,Aditya Sinha,Ravi Madduri,Suren Byna*

Main category: cs.CR

TL;DR: CADRE is a novel framework introduced in this paper for ensuring data readiness in privacy-preserving federated learning. It allows users to define custom standards, metrics, rules, and remedies tailored to specific tasks. The framework generates comprehensive reports to ensure datasets are optimally prepared while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the challenge of ensuring high-quality data at each client in privacy-preserving federated learning (PPFL), especially given the restricted access to data. This can hinder the performance and reliability of FL models.

Method: The method involves introducing CADRE, a framework that enables users to define custom data readiness standards, metrics, rules, and remedies. Comprehensive DR reports are generated based on these user-defined parameters to prepare datasets optimally for federated learning while maintaining privacy.

Result: Experiments conducted across six diverse datasets addressing seven different DR issues demonstrated the versatility and effectiveness of the framework in ensuring data readiness across various dimensions such as quality, privacy, and fairness.

Conclusion: CADRE enhances the performance and reliability of federated learning models by identifying and addressing data-related issues before the training phase, making valuable resources more efficient.

Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine
learning approach where multiple clients train a model collaboratively. PPFL
preserves privacy and security of the client's data by not exchanging it.
However, ensuring that data at each client is of high quality and ready for
federated learning (FL) is a challenge due to restricted data access. In this
paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a
novel framework that allows users to define custom data readiness (DR)
standards, metrics, rules, and remedies tailored to specific FL tasks. Our
framework generates comprehensive DR reports based on the user-defined metrics,
rules, and remedies to ensure datasets are optimally prepared for FL while
preserving privacy. We demonstrate the framework's practical application by
integrating it into an existing PPFL framework. We conducted experiments across
six diverse datasets, addressing seven different DR issues. The results
illustrate the framework's versatility and effectiveness in ensuring DR across
various dimensions, including data quality, privacy, and fairness. This
approach enhances the performance and reliability of FL models as well as
utilizes valuable resources by identifying and addressing data-related issues
before the training phase.

</details>


### [198] [KGMark: A Diffusion Watermark for Knowledge Graphs](https://arxiv.org/abs/2505.23873)
*Hongrui Peng,Haolang Lu,Yuanlong Yu,Weiye Fu,Kun Wang,Guoshun Nan*

Main category: cs.CR

TL;DR: This paper introduces KGMARK, the first graph watermarking framework for dynamic knowledge graph data, which generates robust, detectable, and transparent diffusion fingerprints through novel methods and strategies.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods are mainly designed for static plain text or image data, but they struggle to be applied to dynamic graphs due to spatial and temporal variations of structured data.

Method: The authors propose a clustering-based alignment method to adapt watermarks to spatial variations, a redundant embedding strategy to strengthen robustness against attacks and temporal variations, and a learnable mask matrix to enhance transparency of the diffusion fingerprints.

Result: Experiments on various public benchmarks demonstrate the effectiveness of the proposed KGMARK framework.

Conclusion: KGMARK successfully addresses the challenges posed by variations in structured data and provides an effective solution for watermarking dynamic knowledge graph data.

Abstract: Knowledge graphs (KGs) are ubiquitous in numerous real-world applications,
and watermarking facilitates protecting intellectual property and preventing
potential harm from AI-generated content. Existing watermarking methods mainly
focus on static plain text or image data, while they can hardly be applied to
dynamic graphs due to spatial and temporal variations of structured data. This
motivates us to propose KGMARK, the first graph watermarking framework that
aims to generate robust, detectable, and transparent diffusion fingerprints for
dynamic KG data. Specifically, we propose a novel clustering-based alignment
method to adapt the watermark to spatial variations. Meanwhile, we present a
redundant embedding strategy to harden the diffusion watermark against various
attacks, facilitating the robustness of the watermark to the temporal
variations. Additionally, we introduce a novel learnable mask matrix to improve
the transparency of diffusion fingerprints. By doing so, our KGMARK properly
tackles the variation challenges of structured data. Experiments on various
public benchmarks show the effectiveness of our proposed KGMARK.

</details>


### [199] [Synopsis: Secure and private trend inference from encrypted semantic embeddings](https://arxiv.org/abs/2505.23880)
*Madelyne Xiao,Palak Jain,Micha Gorelick,Sarah Scheffler*

Main category: cs.CR

TL;DR: This paper introduces Synopsis, a secure architecture for analyzing messaging trends in consensually-donated E2EE messages using message embeddings. It combines techniques from the local and central DP models and wraps the system in malicious-secure multi-party computation to ensure privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to study important phenomena like the spread of misinformation or political messaging on platforms with end-to-end encryption (E2EE), while preserving user privacy.

Method: Synopsis uses message embeddings to analyze messaging trends in consensually-donated E2EE messages. It combines techniques from the local and central differential privacy (DP) models and wraps the system in malicious-secure multi-party computation to ensure the DP query architecture is the only way to access messages.

Result: Evaluations on a dataset of Hindi-language WhatsApp messages demonstrate the efficiency and accuracy of Synopsis. Queries run in about 30 seconds, and the accuracy of the fine-grained interface exceeds 94% on benchmark tasks.

Conclusion: Synopsis provides a secure and efficient method for analyzing messaging trends in consensually-donated E2EE messages while preserving privacy.

Abstract: WhatsApp and many other commonly used communication platforms guarantee
end-to-end encryption (E2EE), which requires that service providers lack the
cryptographic keys to read communications on their own platforms. WhatsApp's
privacy-preserving design makes it difficult to study important phenomena like
the spread of misinformation or political messaging, as users have a clear
expectation and desire for privacy and little incentive to forfeit that privacy
in the process of handing over raw data to researchers, journalists, or other
parties.
  We introduce Synopsis, a secure architecture for analyzing messaging trends
in consensually-donated E2EE messages using message embeddings. Since the goal
of this system is investigative journalism workflows, Synopsis must facilitate
both exploratory and targeted analyses -- a challenge for systems using
differential privacy (DP), and, for different reasons, a challenge for private
computation approaches based on cryptography. To meet these challenges, we
combine techniques from the local and central DP models and wrap the system in
malicious-secure multi-party computation to ensure the DP query architecture is
the only way to access messages, preventing any party from directly viewing
stored message embeddings.
  Evaluations on a dataset of Hindi-language WhatsApp messages (34,024 messages
represented as 500-dimensional embeddings) demonstrate the efficiency and
accuracy of our approach. Queries on this data run in about 30 seconds, and the
accuracy of the fine-grained interface exceeds 94% on benchmark tasks.

</details>


### [200] [Digital Forensic Investigation of the ChatGPT Windows Application](https://arxiv.org/abs/2505.23938)
*Malithi Wanniarachchi Kankanamge,Nick McKenna,Santiago Carmona,Syed Mhamudul Hasan,Abdur R. Shahid,Ahmed Imteaj*

Main category: cs.CR

TL;DR: The study conducts a forensic analysis of the ChatGPT Windows application, identifying methods to recover digital artifacts like cache, chat logs, metadata, and network traffic using various forensics tools. Critical insights are provided for crime investigation and misuse reporting even after data deletion.


<details>
  <summary>Details</summary>
Motivation: To address the potential misuses of the ChatGPT Windows application by providing a comprehensive forensic analysis that can aid in crime investigation and digital forensics.

Method: Using widely popular and openly available digital forensics tools such as Autopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop to extract and analyze digital artifacts including cache, chat logs, metadata, and network traffic from the ChatGPT Windows application.

Result: Successfully identified and recovered the history of the application's chat, user interactions, and system-level traces, even after they were deleted.

Conclusion: This forensic analysis provides critical insights into investigating crimes involving the ChatGPT Windows application and outlines a potential misuse report for digital forensics.

Abstract: The ChatGPT Windows application offers better user interaction in the Windows
operating system (OS) by enhancing productivity and streamlining the workflow
of ChatGPT's utilization. However, there are potential misuses associated with
this application that require rigorous forensic analysis. This study presents a
holistic forensic analysis of the ChatGPT Windows application, focusing on
identifying and recovering digital artifacts for investigative purposes. With
the use of widely popular and openly available digital forensics tools such as
Autopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this
research explores different methods to extract and analyze cache, chat logs,
metadata, and network traffic from the application. Our key findings also
demonstrate the history of the application's chat, user interactions, and
system-level traces that can be recovered even after deletion, providing
critical insights into the crime investigation and, thus, documenting and
outlining a potential misuse report for digital forensics.

</details>


### [201] [Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention](https://arxiv.org/abs/2505.23968)
*Stephan Rabanser,Ali Shahin Shamsabadi,Olive Franzese,Xiao Wang,Adrian Weller,Nicolas Papernot*

Main category: cs.CR

TL;DR: This paper addresses the potential misuse of cautious predictions in machine learning models by introducing an uncertainty-inducing attack named Mirage that can be exploited to unjustly deny services. To counteract this, the authors propose Confidential Guardian, a framework that detects artificially suppressed confidence and uses zero-knowledge proofs to ensure genuine model confidence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the novel threat where cautious prediction mechanisms in machine learning models could be exploited by dishonest institutions to covertly disadvantage specific individuals under the guise of uncertainty.

Method: The method involves two main components: 1) Introducing an uncertainty-inducing attack called Mirage which reduces confidence in targeted input regions while maintaining high predictive performance across all data points; 2) Proposing Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence and employs zero-knowledge proofs of verified inference to ensure reported confidence scores are genuine.

Result: The results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.

Conclusion: Confidential Guardian is concluded to be an effective solution for preventing the misuse of cautious predictions in machine learning models, ensuring that abstention decisions are based on genuine model uncertainty.

Abstract: Cautious predictions -- where a machine learning model abstains when
uncertain -- are crucial for limiting harmful errors in safety-critical
applications. In this work, we identify a novel threat: a dishonest institution
can exploit these mechanisms to discriminate or unjustly deny services under
the guise of uncertainty. We demonstrate the practicality of this threat by
introducing an uncertainty-inducing attack called Mirage, which deliberately
reduces confidence in targeted input regions, thereby covertly disadvantaging
specific individuals. At the same time, Mirage maintains high predictive
performance across all data points. To counter this threat, we propose
Confidential Guardian, a framework that analyzes calibration metrics on a
reference dataset to detect artificially suppressed confidence. Additionally,
it employs zero-knowledge proofs of verified inference to ensure that reported
confidence scores genuinely originate from the deployed model. This prevents
the provider from fabricating arbitrary model confidence values while
protecting the model's proprietary details. Our results confirm that
Confidential Guardian effectively prevents the misuse of cautious predictions,
providing verifiable assurances that abstention reflects genuine model
uncertainty rather than malicious intent.

</details>


### [202] [HoneySat: A Network-based Satellite Honeypot Framework](https://arxiv.org/abs/2505.24008)
*Efrén López-Morales,Ulysse Planta,Gabriele Marra,Carlos González,Jacob Hopkins,Majid Garoosi,Elías Obreque,Carlos Rubio-Medrano,Ali Abbasi*

Main category: cs.CR

TL;DR: The paper introduces HoneySat, the first high-interaction satellite honeypot framework that simulates a CubeSat convincingly. It has been validated by experienced SmallSat operators and effectively supports adversarial techniques and tactics. Real interactions from actual adversaries have confirmed its covert and efficient operation while collecting valuable interaction data.


<details>
  <summary>Details</summary>
Motivation: There is no efficient way to collect data on adversarial techniques for satellites, which hinders the generation of security intelligence.

Method: Presented HoneySat, a high-interaction satellite honeypot framework that simulates a CubeSat. Validated with experienced SmallSat operators and tested against adversarial techniques and tactics.

Result: Majority of satellite operators agreed HoneySat provides realistic simulations. It supports most adversarial techniques and tactics targeting satellites. Real interactions from actual adversaries confirm its efficiency and value.

Conclusion: HoneySat can operate covertly and efficiently, collecting highly valuable interaction data for satellite security intelligence.

Abstract: Satellites are the backbone of several mission-critical services, such as GPS
that enable our modern society to function. For many years, satellites were
assumed to be secure because of their indecipherable architectures and the
reliance on security by obscurity. However, technological advancements have
made these assumptions obsolete, paving the way for potential attacks, and
sparking a renewed interest in satellite security. Unfortunately, to this day,
there is no efficient way to collect data on adversarial techniques for
satellites, which severely hurts the generation of security intelligence. In
this paper, we present HoneySat, the first high-interaction satellite honeypot
framework, which is fully capable of convincingly simulating a real-world
CubeSat, a type of Small Satellite (SmallSat) widely used in practice. To
provide evidence of the effectiveness of HoneySat, we surveyed experienced
SmallSat operators currently in charge of active in-orbit satellite missions.
Results revealed that the majority of satellite operators (71.4%) agreed that
HoneySat provides realistic and engaging simulations of CubeSat missions.
Further experimental evaluations also showed that HoneySat provides adversaries
with extensive interaction opportunities by supporting the majority of
adversarial techniques (86.8%) and tactics (100%) that target satellites.
Additionally, we also obtained a series of real interactions from actual
adversaries by deploying HoneySat on the internet over several months,
confirming that HoneySat can operate covertly and efficiently while collecting
highly valuable interaction data.

</details>


### [203] [LLM Agents Should Employ Security Principles](https://arxiv.org/abs/2505.24019)
*Kaiyuan Zhang,Zian Su,Pin-Yu Chen,Elisa Bertino,Xiangyu Zhang,Ninghui Li*

Main category: cs.CR

TL;DR: 这篇论文提出在大规模部署大型语言模型（LLM）代理时，应采用信息安全领域的成熟设计原则来应对多代理交互和系统易受提示注入等漏洞问题，并引入了AgentSandbox框架作为概念验证，该框架在保持高功能效用的同时显著降低了隐私风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在自动化复杂任务方面展现出巨大潜力，但多代理交互和系统易受提示注入等问题带来了新的隐私泄露和系统利用的漏洞，因此需要一种安全机制来解决这些问题。

Method: 论文通过引入AgentSandbox这一概念框架，将防御纵深、最小特权、完全中介和心理可接受性等安全设计原则嵌入到代理的生命周期中，并从良性效用、攻击效用和攻击成功率三个维度对最先进的LLM进行了评估。

Result: AgentSandbox框架在良性及对抗性评估下均保持了高功能效用，同时大幅缓解了隐私风险。

Conclusion: 通过将安全设计原则作为基础元素嵌入新兴的LLM代理协议中，可以推动值得信赖的代理生态系统的发展，与用户的隐私期望和不断发展的监管要求相一致。

Abstract: Large Language Model (LLM) agents show considerable promise for automating
complex tasks using contextual reasoning; however, interactions involving
multiple agents and the system's susceptibility to prompt injection and other
forms of context manipulation introduce new vulnerabilities related to privacy
leakage and system exploitation. This position paper argues that the
well-established design principles in information security, which are commonly
referred to as security principles, should be employed when deploying LLM
agents at scale. Design principles such as defense-in-depth, least privilege,
complete mediation, and psychological acceptability have helped guide the
design of mechanisms for securing information systems over the last five
decades, and we argue that their explicit and conscientious adoption will help
secure agentic systems. To illustrate this approach, we introduce AgentSandbox,
a conceptual framework embedding these security principles to provide
safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art
LLMs along three dimensions: benign utility, attack utility, and attack success
rate. AgentSandbox maintains high utility for its intended functions under both
benign and adversarial evaluations while substantially mitigating privacy
risks. By embedding secure design principles as foundational elements within
emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems
aligned with user privacy expectations and evolving regulatory requirements.

</details>


### [204] [An Advanced Cyber-Physical System Security Testbed for Substation Automation](https://arxiv.org/abs/2505.24021)
*Akila Herath,Chen-Ching Liu,Junho Hong,Mansi Girdhar*

Main category: cs.CR

TL;DR: This paper discusses the design and development of a Cyber-Physical System (CPS) testbed for assessing substation dynamics, demonstrating cyber attacks on IEC 61850 protocols, and evaluating detection/mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To create an effective platform for testing and validating cyber intrusion detection and mitigation strategies in substations using a CPS testbed.

Method: Design and develop a CPS testbed that can assess real-time substation dynamics, demonstrate cyber attacks exploiting IEC 61850-based SV and GOOSE protocols, obtain realistic timing measurements, and evaluate time frames for deploying detection and mitigation strategies.

Result: The CPS testbed successfully demonstrated cyber attacks on IEC 61850 protocols and provided realistic timing measurements to evaluate the effectiveness of detection and mitigation strategies.

Conclusion: A CPS testbed is a powerful tool for testing and validating cyber intrusion detection and mitigation strategies in substations, providing valuable insights into real-time dynamics and attack detection.

Abstract: A Cyber-Physical System (CPS) testbed serves as a powerful platform for
testing and validating cyber intrusion detection and mitigation strategies in
substations. This study presents the design and development of a CPS testbed
that can effectively assess the real-time dynamics of a substation. Cyber
attacks exploiting IEC 61850-based SV and GOOSE protocols are demonstrated
using the testbed, along with an analysis on attack detection. Realistic timing
measurements are obtained, and the time frames for deploying detection and
mitigation strategies are evaluated.

</details>


### [205] [A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming](https://arxiv.org/abs/2505.24252)
*Yizhong Ding*

Main category: cs.CR

TL;DR: To address the lack of diverse and non-redundant malicious-code datasets, this paper proposes RAWG, a Reward-driven Automated Webshell Malicious-code Generator. It categorizes webshell samples by obfuscation types, uses an LLM for token extraction and normalization, applies SFT and PPO for enhancing generation quality, and shows superior performance in payload diversity and escape effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the critical need for effective WebShell defense mechanisms due to frequent cyber-attacks, coupled with the shortage of publicly available, well-categorized malicious-code datasets organized by obfuscation method. Existing methods suffer from limited diversity and high redundancy in generated payloads.

Method: 1. Categorize webshell samples into seven distinct types of obfuscation.
2. Use a large language model (LLM) to extract and normalize key tokens from each sample, creating a standardized corpus.
3. Perform supervised fine-tuning (SFT) on an open-source large model for generating diverse, highly obfuscated payloads.
4. Apply Proximal Policy Optimization (PPO), treating malicious-code samples as 'chosen' data and benign code as 'rejected' data during reinforcement learning.

Result: Extensive experiments demonstrate that RAWG significantly outperforms current state-of-the-art methods in both payload diversity and escape effectiveness.

Conclusion: RAWG is an effective solution for generating diverse and highly obfuscated webshell malicious payloads, addressing the limitations of existing methods.

Abstract: Frequent cyber-attacks have elevated WebShell exploitation and defense to a
critical research focus within network security. However, there remains a
significant shortage of publicly available, well-categorized malicious-code
datasets organized by obfuscation method. Existing malicious-code generation
methods, which primarily rely on prompt engineering, often suffer from limited
diversity and high redundancy in the payloads they produce. To address these
limitations, we propose \textbf{RAWG}, a \textbf{R}eward-driven
\textbf{A}utomated \textbf{W}ebshell Malicious-code \textbf{G}enerator designed
for red-teaming applications. Our approach begins by categorizing webshell
samples from common datasets into seven distinct types of obfuscation. We then
employ a large language model (LLM) to extract and normalize key tokens from
each sample, creating a standardized, high-quality corpus. Using this curated
dataset, we perform supervised fine-tuning (SFT) on an open-source large model
to enable the generation of diverse, highly obfuscated webshell malicious
payloads. To further enhance generation quality, we apply Proximal Policy
Optimization (PPO), treating malicious-code samples as "chosen" data and benign
code as "rejected" data during reinforcement learning. Extensive experiments
demonstrate that RAWG significantly outperforms current state-of-the-art
methods in both payload diversity and escape effectiveness.

</details>


### [206] [MUSE: Model-Agnostic Tabular Watermarking via Multi-Sample Selection](https://arxiv.org/abs/2505.24267)
*Liancheng Fang,Aiwei Liu,Henry Peng Zou,Yankai Chen,Hengrui Zhang,Zhongfen Deng,Philip S. Yu*

Main category: cs.CR

TL;DR: The paper introduces MUSE, a watermarking algorithm for tabular generative models which achieves state-of-the-art watermark detectability and robustness while maintaining data quality.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods for tabular diffusion models rely on DDIM invertibility, but such models have poorer invertibility compared to other modalities, resulting in compromised performance. Additionally, these models require less computation, enabling alternative approaches.

Method: MUSE embeds watermarks by generating multiple candidate samples and selecting one using a specialized scoring function, without relying on model invertibility. Theoretical analysis relates watermark detectability to candidate count and dataset size, allowing calibration of watermark strength.

Result: MUSE reduces distortion rates on fidelity metrics by 81-89% and achieves a 1.0 TPR@0.1%FPR detection rate, demonstrating state-of-the-art watermark detectability and robustness against various attacks while preserving data quality.

Conclusion: MUSE effectively addresses key challenges in tabular data watermarking and is compatible with any tabular generative model supporting repeated sampling.

Abstract: We introduce MUSE, a watermarking algorithm for tabular generative models.
Previous approaches typically leverage DDIM invertibility to watermark tabular
diffusion models, but tabular diffusion models exhibit significantly poorer
invertibility compared to other modalities, compromising performance.
Simultaneously, tabular diffusion models require substantially less computation
than other modalities, enabling a multi-sample selection approach to tabular
generative model watermarking. MUSE embeds watermarks by generating multiple
candidate samples and selecting one based on a specialized scoring function,
without relying on model invertibility. Our theoretical analysis establishes
the relationship between watermark detectability, candidate count, and dataset
size, allowing precise calibration of watermarking strength. Extensive
experiments demonstrate that MUSE achieves state-of-the-art watermark
detectability and robustness against various attacks while maintaining data
quality, and remains compatible with any tabular generative model supporting
repeated sampling, effectively addressing key challenges in tabular data
watermarking. Specifically, it reduces the distortion rates on fidelity metrics
by 81-89%, while achieving a 1.0 TPR@0.1%FPR detection rate. Implementation of
MUSE can be found at https://github.com/fangliancheng/MUSE.

</details>


### [207] [Transaction Proximity: A Graph-Based Approach to Blockchain Fraud Prevention](https://arxiv.org/abs/2505.24284)
*Gordon Y. Liao,Ziming Zeng,Mira Belenkiy,Jacob Hirshman*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a fraud-deterrent access validation system for public
blockchains, leveraging two complementary concepts: "Transaction Proximity",
which measures the distance between wallets in the transaction graph, and
"Easily Attainable Identities (EAIs)", wallets with direct transaction
connections to centralized exchanges. Recognizing the limitations of
traditional approaches like blocklisting (reactive, slow) and strict allow
listing (privacy-invasive, adoption barriers), we propose a system that
analyzes transaction patterns to identify wallets with close connections to
centralized exchanges.
  Our directed graph analysis of the Ethereum blockchain reveals that 56% of
large USDC wallets (with a lifetime maximum balance greater than \$10,000) are
EAI and 88% are within one transaction hop of an EAI. For transactions
exceeding \$2,000, 91% involve at least one EAI. Crucially, an analysis of past
exploits shows that 83% of the known exploiter addresses are not EAIs, with 21%
being more than five hops away from any regulated exchange. We present three
implementation approaches with varying gas cost and privacy tradeoffs,
demonstrating that EAI-based access control can potentially prevent most of
these incidents while preserving blockchain openness. Importantly, our approach
does not restrict access or share personally identifiable information, but it
provides information for protocols to implement their own validation or risk
scoring systems based on specific needs. This middle-ground solution enables
programmatic compliance while maintaining the core values of open blockchain.

</details>


### [208] [Verifiable Weighted Secret Sharing](https://arxiv.org/abs/2505.24289)
*Kareem Shehata,Han Fangqi,Sri AravindaKrishnan Thyagarajan*

Main category: cs.CR

TL;DR: The paper presents an efficient verifiable weighted secret sharing (WSS) scheme that can handle malicious dealers, using Bulletproofs for verification and new privacy-preserving techniques, showing significant improvements in communication complexity on Ethereum.


<details>
  <summary>Details</summary>
Motivation: Traditional threshold secret sharing schemes assume equal party weights, but emerging systems like blockchains reveal disparities in trustworthiness. Existing WSS schemes only ensure security from honest-but-curious behavior and lack protection from malicious adversaries.

Method: The authors propose a verifiable WSS scheme compatible with the latest CRT-based WSS, utilizing Bulletproofs for efficient verification and introducing new privacy-preserving techniques to prove relations between committed values.

Result: Evaluation on Ethereum demonstrates up to a 100x improvement in communication complexity compared to current designs and a 20x improvement compared to unweighted VSS schemes.

Conclusion: This work provides the first efficient verifiable WSS scheme that tolerates malicious dealers, significantly enhancing communication efficiency, with potential applications in blockchain systems.

Abstract: Traditionally, threshold secret sharing (TSS) schemes assume all parties have
equal weight, yet emerging systems like blockchains reveal disparities in party
trustworthiness, such as stake or reputation. Weighted Secret Sharing (WSS)
addresses this by assigning varying weights to parties, ensuring security even
if adversaries control parties with total weight at most a threshold $t$.
Current WSS schemes assume honest dealers, resulting in security from only
honest-but-curious behaviour but not protection from malicious adversaries for
downstream applications. \emph{Verifiable} secret sharing (VSS) is a well-known
technique to address this, but existing VSS schemes are either tailored to TSS,
or require additional trust assumptions. We propose the first efficient
verifiable WSS scheme that tolerates malicious dealers and is compatible with
the latest CRT-based WSS~\cite{crypto_w_weights}. Our solution uses
Bulletproofs for efficient verification and introduces new privacy-preserving
techniques for proving relations between committed values, which may be of
independent interest. Evaluation on Ethereum show up to a $100\times$
improvement in communication complexity compared to the current design and
$20\times$ improvement compared to unweighted VSS schemes.

</details>


### [209] [Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups](https://arxiv.org/abs/2505.24393)
*Suhyeon Lee*

Main category: cs.CR

TL;DR: Optimistic Rollups (ORUs) improve blockchain scalability but face the verifier's dilemma. This paper proposes Randomized Attention Test (RAT), a protocol to probabilistically challenge validators' attentiveness in ORUs, ensuring their diligence and system security with low costs.


<details>
  <summary>Details</summary>
Motivation: ORUs enhance blockchain scalability but suffer from the verifier's dilemma due to lack of mechanisms to ensure validator attentiveness, potentially leading to finalization of fraudulent states.

Method: Introduces RAT, an L1-based protocol that probabilistically challenges validators in ORUs to verify their liveness and computational readiness using game-theoretic analysis.

Result: Achieves Ideal Security Equilibrium where all validators are attentive and proposers are honest, with relatively low economic penalties for non-responsive validators and low attention test frequency.

Conclusion: RAT provides a practical mechanism to enforce validator diligence in ORUs, enhancing system security and integrity while minimizing additional costs.

Abstract: Optimistic Rollups (ORUs) significantly enhance blockchain scalability but
inherently suffer from the verifier's dilemma, particularly concerning
validator attentiveness. Current systems lack mechanisms to proactively ensure
validators are diligently monitoring L2 state transitions, creating a
vulnerability where fraudulent states could be finalized. This paper introduces
the Randomized Attention Test (RAT), a novel L1-based protocol designed to
probabilistically challenge validators in ORUs, thereby verifying their
liveness and computational readiness. Our game-theoretic analysis demonstrates
that an Ideal Security Equilibrium, where all validators are attentive and
proposers are honest, can be achieved with RAT. Notably, this equilibrium is
attainable and stable with relatively low economic penalties (e.g., under
$1000) for non-responsive validators and a low attention test frequency (e.g.,
under 1% per epoch). RAT thus provides a crucial, practical mechanism to
enforce validator diligence, fortifying the overall security and integrity of
ORU systems with minimizing additional costs.

</details>


### [210] [The Cost of Restaking vs. Proof-of-Stake](https://arxiv.org/abs/2505.24440)
*Akaki Mamageishvili,Benny Sudakov*

Main category: cs.CR

TL;DR: The paper compares the efficiency of restaking and Proof-of-Stake (PoS) protocols in terms of stake requirements, finding that restaking can offer significant savings compared to PoS, potentially growing as the square root of the number of validators.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the efficiency of restaking and PoS protocols in terms of stake requirements, focusing on transforming restaking graphs into secure PoS protocols and vice versa.

Method: Analyzing the sufficient condition for a restaking graph to be secure, deriving upper and lower bounds on extra stakes needed for transformation between restaking and PoS protocols, and studying the complementary question of converting secure PoS protocols into an aggregate secure restaking graph.

Result: It is always possible to transform a secure restaking graph into secure PoS protocols. Restaking can provide substantial savings compared to PoS protocols, with the savings asymptotically growing in the worst case as the square root of the number of validators.

Conclusion: Restaking offers significant advantages over traditional PoS protocols in terms of stake efficiency, with clear bounds established for transformations between the two.

Abstract: We compare the efficiency of restaking and Proof-of-Stake (PoS) protocols in
terms of stake requirements. First, we consider the sufficient condition for
the restaking graph to be secure. We show that the condition implies that it is
always possible to transform such a restaking graph into secure PoS protocols.
Next, we derive two main results, giving upper and lower bounds on required
extra stakes that one needs to add to validators of the secure restaking graph
to be able to transform it into secure PoS protocols. In particular, we show
that the restaking savings compared to PoS protocols can be very large and can
asymptotically grow in the worst case as a square root of the number of
validators. We also study a complementary question of transforming secure PoS
protocols into an aggregate secure restaking graph and provide lower and upper
bounds on the PoS savings compared to restaking.

</details>


### [211] [LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs](https://arxiv.org/abs/2505.24451)
*Luis Ibanez-Lissen,Lorena Gonzalez-Manzano,Jose Maria de Fuentes,Nicolas Anciaux*

Main category: cs.CR

TL;DR: 研究了如何使用线性探测器（LPs）来估计压缩大型语言模型（LLMs）的性能，并在微调之前确定剪枝压缩的最佳截断点。提出的方法$LPASS$应用于BERT和Gemma，用于检测MITRE Top 25中的12种最危险漏洞，在48万C/C++样本上进行实验。结果表明：(1) 可以分别移除33.3%和72.2%的层而不会损失精度；(2) LPs能提前估计微调后和压缩后的模型效果，误差范围为3%-8.68%。基于$LPASS$的LLMs在多类漏洞检测中准确率达到86.9%，超过现有技术水平。此外，压缩版Gemma比原版在F1分数上提高1.6%，同时减少训练和推理时间分别为29.4%和23.8%，模型大小减少42.98%。


<details>
  <summary>Details</summary>
Motivation: 当前用于网络安全目的的大型语言模型（LLMs）需要通过压缩和微调技术提升效率和效果，但这些过程需要大量计算资源。因此，探索一种快速、低耗的方式，提前评估压缩LLMs的性能并指导剪枝操作成为必要。

Method: 引入线性探测器（LPs），结合提出的方法$LPASS$，应用于BERT和Gemma模型。通过分析不同层对模型性能的影响，确定最佳剪枝点，并估计微调和压缩后的模型效果。该方法用于检测MITRE Top 25中的12种漏洞类型，涉及48万C/C++代码样本。

Result: 实验结果表明，(1) 分别可以移除33.3%和72.2%的模型层而不影响精度；(2) LPs能够提前预测微调和压缩后的模型性能，误差范围为3%-8.68%。基于$LPASS$的LLMs在多类漏洞检测任务中达到86.9%的准确率，优于现有技术水平。此外，压缩版Gemma比原版在F1分数上提高1.6%，同时显著降低训练和推理时间以及模型大小。

Conclusion: 线性探测器（LPs）与$LPASS$方法能够有效评估压缩LLMs的性能并指导剪枝操作。相比传统方法，$LPASS$不仅提升了漏洞检测的准确性，还显著减少了计算资源消耗，具有重要的实际应用价值。

Abstract: Large Language Models (LLMs) are being extensively used for cybersecurity
purposes. One of them is the detection of vulnerable codes. For the sake of
efficiency and effectiveness, compression and fine-tuning techniques are being
developed, respectively. However, they involve spending substantial
computational efforts. In this vein, we analyse how Linear Probes (LPs) can be
used to provide an estimation on the performance of a compressed LLM at an
early phase -- before fine-tuning. We also show their suitability to set the
cut-off point when applying layer pruning compression. Our approach, dubbed
$LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25
most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in
142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be
removed, respectively, with no precision loss; (2) they provide an early
estimate of the post-fine-tuning and post-compression model effectiveness, with
3\% and 8.68\% as the lowest and average precision errors, respectively.
$LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy
in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed
versions of Gemma outperform the original ones by 1.6\% of F1-score at a
maximum while saving 29.4 \% and 23.8\% of training and inference time and
42.98\% of model size.

</details>


### [212] [CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control](https://arxiv.org/abs/2505.24536)
*Chaohui Xu,Qi Cui,Chip-Hong Chang*

Main category: cs.CR

TL;DR: A new framework named CHIP is proposed for DNNs IP protection, which uses cryptographic chameleon hash function to ensure strong ownership proof, high fidelity, active usage authorization and user traceability.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking and passport-based methods lack a holistic protection with robust ownership proof, high fidelity, active usage authorization and user traceability for offline access distributed models and multi-user MLaaS cloud model.

Method: The CHIP protection framework utilizes the cryptographic chameleon hash function. The collision-resistant property allows for strong model ownership claim and liable user traceability, while the trapdoor-collision property enables hashing of multiple user passports and licensee certificates to the same immutable signature for active usage control. Using the owner passport as an oracle, multiple user-specific triplets can be created for secure offline distribution. The watermarked master model can also be deployed for MLaaS with usage permission verifiable by the provision of any trapdoor-colliding user passports.

Result: CHIP is extensively evaluated on four datasets and two architectures to demonstrate its protection versatility and robustness.

Conclusion: CHIP provides a comprehensive solution for DNNs IP protection with strong ownership proof, high fidelity, active usage authorization and user traceability.

Abstract: The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous
training costs make their intellectual property (IP) protection of paramount
importance. Recently introduced passport-based methods attempt to steer DNN
watermarking towards strengthening ownership verification against ambiguity
attacks by modulating the affine parameters of normalization layers.
Unfortunately, neither watermarking nor passport-based methods provide a
holistic protection with robust ownership proof, high fidelity, active usage
authorization and user traceability for offline access distributed models and
multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we
propose a Chameleon Hash-based Irreversible Passport (CHIP) protection
framework that utilizes the cryptographic chameleon hash function to achieve
all these goals. The collision-resistant property of chameleon hash allows for
strong model ownership claim upon IP infringement and liable user traceability,
while the trapdoor-collision property enables hashing of multiple user
passports and licensee certificates to the same immutable signature to realize
active usage control. Using the owner passport as an oracle, multiple
user-specific triplets, each contains a passport-aware user model, a user
passport, and a licensee certificate can be created for secure offline
distribution. The watermarked master model can also be deployed for MLaaS with
usage permission verifiable by the provision of any trapdoor-colliding user
passports. CHIP is extensively evaluated on four datasets and two architectures
to demonstrate its protection versatility and robustness. Our code is released
at https://github.com/Dshm212/CHIP.

</details>


### [213] [So, I climbed to the top of the pyramid of pain -- now what?](https://arxiv.org/abs/2505.24685)
*Vasilis Katos,Emily Rosenorn-Lanng,Jane Henriksen-Bulmer,Ala Yankouskaya*

Main category: cs.CR

TL;DR: This paper introduces the Human Layer Kill Chain framework to address human vulnerabilities in cybersecurity, proposing a Sociotechnical Kill Plane that integrates human psychology and AI-enabled techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional attack models like Lockheed Martin's Cyber Kill Chain are insufficient for addressing human vulnerabilities exploited by modern adversaries.

Method: The paper details an eight-stage Human Layer Kill Chain and proposes merging it with the Cyber Kill Chain to form a Sociotechnical Kill Plane.

Result: This framework allows for a holistic examination of attackers' TTPs across the sociotechnical landscape, enhancing understanding of adversarial methods.

Conclusion: The proposed framework has significant implications for incident response and organizational resilience as AI continues to shape the threat landscape.

Abstract: This paper explores the evolving dynamics of cybersecurity in the age of
advanced AI, from the perspective of the introduced Human Layer Kill Chain
framework. As traditional attack models like Lockheed Martin's Cyber Kill Chain
become inadequate in addressing human vulnerabilities exploited by modern
adversaries, the Humal Layer Kill Chain offers a nuanced approach that
integrates human psychology and behaviour into the analysis of cyber threats.
We detail the eight stages of the Human Layer Kill Chain, illustrating how
AI-enabled techniques can enhance psychological manipulation in attacks. By
merging the Human Layer with the Cyber Kill Chain, we propose a Sociotechnical
Kill Plane that allows for a holistic examination of attackers' tactics,
techniques, and procedures (TTPs) across the sociotechnical landscape. This
framework not only aids cybersecurity professionals in understanding
adversarial methods, but also empowers non-technical personnel to engage in
threat identification and response. The implications for incident response and
organizational resilience are significant, particularly as AI continues to
shape the threat landscape.

</details>


### [214] [Next Generation Authentication for Data Spaces: An Authentication Flow Based On Grant Negotiation And Authorization Protocol For Verifiable Presentations (GNAP4VP)](https://arxiv.org/abs/2505.24698)
*Rodrigo Menéndez,Andres Munoz-Arcentales,Joaquín Salvachúa,Carlos Aparicio,Irene Plaza,Gabriel Huecas*

Main category: cs.CR

TL;DR: This paper proposes an identity verification protocol for shared data environments in Data Spaces, extending GNAP and integrating OIDC4VP and LVP. It presents two interaction flows to address privacy, interoperability, and regulatory compliance issues.


<details>
  <summary>Details</summary>
Motivation: To ensure security and privacy in digital environments within Data Spaces by providing a robust identity verification protocol that supports decentralized, user-centric identity management.

Method: Extending the Grant Negotiation and Authorization Protocol (GNAP) and integrating OpenID Connect for Verifiable Presentations (OIDC4VP) along with support for Linked Verifiable Presentations (LVP). Introducing two alternative interaction flows: Wallet-Driven Interaction utilizing OIDC4VP and LVP Authorization for machine-to-machine communication.

Result: The proposed solution adheres to the principles of Self-Sovereign Identity (SSI), facilitating decentralized, user-centric identity management while maintaining flexibility through protocol negotiation. It addresses challenges like privacy, interoperability, and regulatory compliance ensuring scalability and minimizing trust assumptions.

Conclusion: This work contributes towards establishing a more trustworthy and sovereign digital infrastructure aligning with emerging European data governance initiatives.

Abstract: Identity verification in Data Spaces is a fundamental aspect of ensuring
security and privacy in digital environments. This paper presents an identity
verification protocol tailored for shared data environments within Data Spaces.
This protocol extends the Grant Negotiation and Authorization Protocol (GNAP)
and integrates OpenID Connect for Verifiable Presentations (OIDC4VP) along with
support for Linked Verifiable Presentations (LVP), providing a robust
foundation for secure and privacy-preserving interactions. The proposed
solution adheres to the principles of Self-Sovereign Identity (SSI) to
facilitate decentralized, user-centric identity management while maintaining
flexibility through protocol negotiation. Two alternative interaction flows are
introduced: a "Wallet-Driven Interaction" utilizing OIDC4VP, and a "LVP
Authorization" model for fully automated machine-to-machine communication.
These flows address critical challenges encountered in Data Spaces, including
privacy, interoperability, and regulatory compliance while simultaneously
ensuring scalability and minimizing trust assumptions. The paper provides a
detailed technical design, outlining the implementation considerations, and
demonstrating how the proposed flows guarantee verifiable, secure, and
efficient interactions between participants. This work contributes towards the
establishment of a more trustworthy and sovereign digital infrastructure, in
alignment with emerging European data governance initiatives.

</details>


### [215] [PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches](https://arxiv.org/abs/2505.24703)
*Dennis Jacob,Chong Xiang,Prateek Mittal*

Main category: cs.CR

TL;DR: Deep learning models are vulnerable to adversarial patch attacks, which impair performance. While certifiable defenses have been successfully applied to single-label classification, limited work has been done for multi-label classification. This paper presents PatchDEMUX, a certifiably robust framework for multi-label classifiers against adversarial patches.


<details>
  <summary>Details</summary>
Motivation: Current certifiable defenses mainly focus on single-label classification and lack effective methods for multi-label classification tasks.

Method: PatchDEMUX considers the multi-label classification task as a series of isolated binary classification problems to provably guarantee robustness. In addition, when an attacker is limited to a single patch, an additional certification procedure is proposed to provide tighter robustness bounds.

Result: Using PatchCleanser as a backbone, PatchDEMUX can achieve non-trivial robustness on the MS-COCO and PASCAL VOC datasets while maintaining high clean performance.

Conclusion: PatchDEMUX is a generalizable method that extends existing certifiable defenses for single-label classification to multi-label classification tasks.

Abstract: Deep learning techniques have enabled vast improvements in computer vision
technologies. Nevertheless, these models are vulnerable to adversarial patch
attacks which catastrophically impair performance. The physically realizable
nature of these attacks calls for certifiable defenses, which feature provable
guarantees on robustness. While certifiable defenses have been successfully
applied to single-label classification, limited work has been done for
multi-label classification. In this work, we present PatchDEMUX, a certifiably
robust framework for multi-label classifiers against adversarial patches. Our
approach is a generalizable method which can extend any existing certifiable
defense for single-label classification; this is done by considering the
multi-label classification task as a series of isolated binary classification
problems to provably guarantee robustness. Furthermore, in the scenario where
an attacker is limited to a single patch we propose an additional certification
procedure that can provide tighter robustness bounds. Using the current
state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a
backbone, we find that PatchDEMUX can achieve non-trivial robustness on the
MS-COCO and PASCAL VOC datasets while maintaining high clean performance

</details>


### [216] [Talking Transactions: Decentralized Communication through Ethereum Input Data Messages (IDMs)](https://arxiv.org/abs/2505.24724)
*Xihan Xiong,Zhipeng Wang,Qin Wang,Endong Liu,Pascal Berrang,William Knottenbelt*

Main category: cs.CR

TL;DR: This paper conducts a large-scale analysis of Ethereum Input Data Messages (IDMs) from the genesis block to February 2024, exploring their linguistic characteristics, cultural differences, and security/regulatory implications.


<details>
  <summary>Details</summary>
Motivation: To understand how blockchain transactions 'talk' through the input data field of Ethereum transactions and what they communicate about, beyond its role as a financial infrastructure.

Method: Analyzed Ethereum IDMs from genesis block to February 2024 (3134 days), extracting 867,140 transactions with informative IDMs. Used LLMs for language detection and examined IDM content, user communities, and security/regulatory implications.

Result: English (95.4%) and Chinese (4.4%) dominate natural language use in IDMs. English IDMs focus on security/scam warnings with negative emotions, while Chinese IDMs emphasize emotional expression/social connection with positive tones. Longer English IDMs relate to high ETH values for protocol purposes, whereas longer Chinese IDMs involve symbolic amounts for emotional intent. IDM participants form small, loosely connected communities (59.99%). Victims use IDMs in on-chain attacks for fund recovery negotiations, and misuse for abuse/threats/sexual solicitation highlights the need for regulation.

Conclusion: Ethereum IDMs serve as a decentralized communication medium with culturally and functionally divergent use cases. They have security relevance in on-chain attacks and regulatory implications due to potential misuse.

Abstract: Can you imagine, blockchain transactions can talk! In this paper, we study
how they talk and what they talk about. We focus on the input data field of
Ethereum transactions, which is designed to allow external callers to interact
with smart contracts. In practice, this field also enables users to embed
natural language messages into transactions. Users can leverage these Input
Data Messages (IDMs) for peer-to-peer communication. This means that, beyond
Ethereum's well-known role as a financial infrastructure, it also serves as a
decentralized communication medium.
  We present the first large-scale analysis of Ethereum IDMs from the genesis
block to February 2024 (3134 days). We filter IDMs to extract 867,140
transactions with informative IDMs and use LLMs for language detection. We find
that English (95.4%) and Chinese (4.4%) dominate the use of natural languages
in IDMs. Interestingly, English IDMs center on security and scam warnings (24%)
with predominantly negative emotions, while Chinese IDMs emphasize emotional
expression and social connection (44%) with a more positive tone. We also
observe that longer English IDMs often transfer high ETH values for
protocol-level purposes, while longer Chinese IDMs tend to involve symbolic
transfer amounts for emotional intent. Moreover, we find that the IDM
participants tend to form small, loosely connected communities (59.99%). Our
findings highlight culturally and functionally divergent use cases of the IDM
channel across user communities. We further examine the security relevance of
IDMs in on-chain attacks. Many victims use them to appeal to attackers for fund
recovery. IDMs containing negotiations or reward offers are linked to higher
reply rates. We also analyze IDMs' regulatory implications. Their misuse for
abuse, threats, and sexual solicitation reveals the urgent need for content
moderation and regulation in decentralized systems.

</details>


### [217] [Authentication and authorization in Data Spaces: A relationship-based access control approach for policy specification based on ODRL](https://arxiv.org/abs/2505.24742)
*Irene Plaza-Ortiz,Andres Munoz-Arcentales,Joaquín Salvachúa,Carlos Aparicio,Gabriel Huecas,Enrique Barra*

Main category: cs.CR

TL;DR: This paper proposes an extension of the ODRL standard, called the ODRL Data Spaces (ODS) profile, and a policy execution engine to translate ODRL policies into executable formats. It is validated through a use case involving OpenFGA.


<details>
  <summary>Details</summary>
Motivation: Data has become a crucial resource in the digital economy and distributed environments such as Data Spaces require fine-grained access control mechanisms that balance openness with sovereignty and security.

Method: Propose an extension of the ODRL standard (ODRL Data Spaces profile) and introduce a policy execution engine to translate ODRL policies into executable formats for effective enforcement.

Result: The approach is validated through a use case involving OpenFGA, proving its applicability to relationship-based access control scenarios.

Conclusion: The ODRL Data Spaces profile and policy execution engine can effectively support authorization and complement existing authentication mechanisms throughout the data lifecycle.

Abstract: Data has become a crucial resource in the digital economy, fostering
initiatives for secure and sovereign data sharing frameworks such as Data
Spaces. However, these distributed environments require fine-grained access
control mechanisms that balance openness with sovereignty and security. This
paper proposes an extension of the Open Digital Rights Language (ODRL)
standard, the ODRL Data Spaces (ODS) profile, aimed at supporting authorization
and complementing existing authentication mechanisms throughout the data
lifecycle. Additionally, a policy execution engine is introduced to translate
ODRL policies into executable formats, enabling effective enforcement. The
approach is validated through a use case involving OpenFGA, demonstrating its
applicability to relationship-based access control scenarios.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [218] [Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control](https://arxiv.org/abs/2505.24161)
*Zijie Xu,Tong Bu,Zecheng Hao,Jianhao Ding,Zhaofei Yu*

Main category: cs.NE

TL;DR: 在连续控制任务中，现有的强化学习算法与脉冲神经网络（SNN）存在不匹配问题，导致SNN训练不稳定。本文提出了一种新的代理目标框架，解决了SNN离散、不可微的特性与现有算法之间的冲突，从而提高了SNN的性能，并首次在连续控制任务中超越了传统ANN的性能。


<details>
  <summary>Details</summary>
Motivation: 当前在强化学习领域中，直接用SNN替代ANN的方法忽略了RL算法是否适合SNN的问题，特别是在连续控制任务中，这种不匹配会导致SNN训练不稳定。

Method: 提出了一个代理目标框架，利用连续且可微的动态更新机制，绕过了SNN尖峰的不兼容性，稳定了RL算法。此方法仅在训练时使用代理网络，部署时仍保留SNN的能量效率。

Result: 通过广泛的实验表明，相比传统的SNN，代理目标框架能够使SNN在不同类型的脉冲神经元上提升高达32%的性能，并且首次在连续控制任务中使用简单的LIF神经元超越了ANN的性能。

Conclusion: 本研究为设计更适合SNN特性的强化学习算法提供了新思路，推动了兼具高性能和低功耗的类脑计算代理的发展。

Abstract: Spiking Neural Networks (SNNs) offer low-latency and energy-efficient
decision making through neuromorphic hardware, making them compelling for
Reinforcement Learning (RL) in resource-constrained edge devices. Recent
studies in this field directly replace Artificial Neural Networks (ANNs) by
SNNs in existing RL frameworks, overlooking whether the RL algorithm is
suitable for SNNs. However, most RL algorithms in continuous control are
designed tailored to ANNs, including the target network soft updates mechanism,
which conflict with the discrete, non-differentiable dynamics of SNN spikes. We
identify that this mismatch destabilizes SNN training in continuous control
tasks. To bridge this gap between discrete SNN and continuous control, we
propose a novel proxy target framework. The continuous and differentiable
dynamics of the proxy target enable smooth updates, bypassing the
incompatibility of SNN spikes, stabilizing the RL algorithms. Since the proxy
network operates only during training, the SNN retains its energy efficiency
during deployment without inference overhead. Extensive experiments on
continuous control benchmarks demonstrate that compared to vanilla SNNs, the
proxy target framework enables SNNs to achieve up to 32% higher performance
across different spiking neurons. Notably, we are the first to surpass ANN
performance in continuous control with simple Leaky-Integrate-and-Fire (LIF)
neurons. This work motivates a new class of SNN-friendly RL algorithms tailored
to SNN's characteristics, paving the way for neuromorphic agents that combine
high performance with low power consumption.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [219] [Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning](https://arxiv.org/abs/2505.24099)
*Mohammad Shah Alam,William Ott,Ilya Timofeyev*

Main category: math.DS

TL;DR: This paper investigates the use of echo state networks (ESNs) with transfer learning to predict changes in long-term statistical patterns of the generalized Kuramoto-Sivashinsky (gKS) equation when parameters such as dispersion relation or spatial domain length vary.


<details>
  <summary>Details</summary>
Motivation: To improve the predictive capabilities of ESNs for complex systems like the gKS model that exhibits spatiotemporal chaos, especially across different parameter regimes.

Method: Integrate ESNs with transfer learning to adapt them to various parameter settings of the gKS model, allowing prediction of changes in long-term statistical patterns and chaotic attractors.

Result: Successfully captured changes in the underlying chaotic attractor of the gKS model when varying parameters like dispersion relation or spatial domain length.

Conclusion: The combination of ESNs and transfer learning shows promise for predicting complex dynamical behaviors in nonlinear systems.

Abstract: In this paper, we explore the predictive capabilities of echo state networks
(ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal
nonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel
methodology that integrates ESNs with transfer learning, aiming to enhance
predictive performance across various parameter regimes of the gKS model. Our
research focuses on predicting changes in long-term statistical patterns of the
gKS model that result from varying the dispersion relation or the length of the
spatial domain. We use transfer learning to adapt ESNs to different parameter
settings and successfully capture changes in the underlying chaotic attractor.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [220] [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: This paper proposes HED-LM, a novel few-shot optimization method combining numerical similarity and contextual relevance for sensor-based classification tasks. It improves example selection and outperforms existing methods in fatigue detection using accelerometer data.


<details>
  <summary>Details</summary>
Motivation: Few-shot prompting is effective for inference with limited labeled data, but its performance heavily relies on the quality of selected examples. Current methods struggle with nuanced example selection, especially in complex tasks like fatigue detection from sensor data.

Method: The paper introduces HED-LM, which uses a hybrid selection pipeline involving Euclidean distance to filter candidate examples and large language models (LLMs) to re-rank them based on contextual relevance. This approach aims to improve the robustness of few-shot prompting by combining numerical similarity with contextual understanding.

Result: HED-LM achieved a mean macro F1-score of 69.13±10.71% in fatigue detection tasks, surpassing random selection (59.30±10.13%) and distance-only filtering (67.61±11.39%). This represents relative improvements of 16.6% and 2.3%, respectively.

Conclusion: HED-LM effectively enhances the performance of few-shot prompting in sensor-based classification tasks, particularly in scenarios requiring nuanced example selection. The method shows promise for broader applications in healthcare monitoring, human activity recognition, and industrial safety.

Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid
Euclidean Distance with Large Language Models) to improve example selection for
sensor-based classification tasks. While few-shot prompting enables efficient
inference with limited labeled data, its performance largely depends on the
quality of selected examples. HED-LM addresses this challenge through a hybrid
selection pipeline that filters candidate examples based on Euclidean distance
and re-ranks them using contextual relevance scored by large language models
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection
task using accelerometer data characterized by overlapping patterns and high
inter-subject variability. Unlike simpler tasks such as activity recognition,
fatigue detection demands more nuanced example selection due to subtle
differences in physiological signals. Our experiments show that HED-LM achieves
a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection
(59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These
represent relative improvements of 16.6% and 2.3%, respectively. The results
confirm that combining numerical similarity with contextual relevance improves
the robustness of few-shot prompting. Overall, HED-LM offers a practical
solution to improve performance in real-world sensor-based learning tasks and
shows potential for broader applications in healthcare monitoring, human
activity recognition, and industrial safety scenarios.

</details>


### [221] [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/abs/2505.23785)
*Cody Kommers,Drew Hemment,Maria Antoniak,Joel Z. Leibo,Hoyt Long,Emily Robinson,Adam Sobey*

Main category: cs.CL

TL;DR: This paper argues that large language models can represent human meaning through thick description, which accommodates heterogeneity and retains contextual information. It identifies five key challenges and suggests thick description as a unifying framework to address concerns about representing culture in LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to explore how large language models can make cultural context and human meaning legible at an unprecedented scale in AI-based sociotechnical systems, overcoming the limitations of thin descriptions that strip human activity of cultural context.

Method: The method involves utilizing the verbal capabilities of LLMs to automate the generation and processing of thick descriptions, which are verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning.

Result: The result is a proposal for developing new representational formats based on thick description to render human meaning legible, identifying five key challenges in doing so.

Conclusion: The conclusion frames thick description as a crucial direction for the application of generative AI and a potential unifying framework to address emerging concerns about representing culture in or using LLMs.

Abstract: This position paper argues that large language models (LLMs) can make
cultural context, and therefore human meaning, legible at an unprecedented
scale in AI-based sociotechnical systems. We argue that such systems have
previously been unable to represent human meaning because they rely on thin
descriptions: numerical representations that enforce standardization and
therefore strip human activity of the cultural context that gives it meaning.
By contrast, scholars in the humanities and qualitative social sciences have
developed frameworks for representing meaning through thick description: verbal
representations that accommodate heterogeneity and retain contextual
information needed to represent human meaning. While these methods can
effectively codify meaning, they are difficult to deploy at scale. However, the
verbal capabilities of LLMs now provide a means of (at least partially)
automating the generation and processing of thick descriptions, potentially
overcoming this bottleneck. We argue that the problem of rendering human
meaning legible is not just about selecting better metrics, but about
developing new representational formats (based on thick description). We frame
this as a crucial direction for the application of generative AI and identify
five key challenges: preserving context, maintaining interpretive pluralism,
integrating perspectives based on lived experience and critical distance,
distinguishing qualitative content from quantitative magnitude, and
acknowledging meaning as dynamic rather than static. Furthermore, we suggest
that thick description has the potential to serve as a unifying framework to
address a number of emerging concerns about the difficulties of representing
culture in (or using) LLMs.

</details>


### [222] [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/abs/2505.23788)
*Aakash Sen Sharma,Debdeep Sanyal,Priyansh Srivastava,Sundar Atreya H.,Shirish Karande,Mohan Kankanhalli,Murari Mandal*

Main category: cs.CL

TL;DR: Large language models (LLMs) have the risk of copyright infringement. Current safeguards compromise the practical utility of these models. This paper introduces FUA-LLM, a framework that aligns LLM outputs with fair-use doctrine using FairUseDB and Direct Preference Optimization (DPO). It also proposes new evaluation metrics and shows substantial reduction in problematic outputs while preserving usability.


<details>
  <summary>Details</summary>
Motivation: To address the issue of copyright infringement in LLMs and improve their practical utility without relying on restrictive refusal-based filters.

Method: Developed FUA-LLM using FairUseDB, a dataset with expert-validated examples, and applied Direct Preference Optimization (DPO) to fine-tune open-source LLMs for legally compliant and useful responses. Proposed new evaluation metrics: Weighted Penalty Utility and Compliance Aware Harmonic Mean (CAH).

Result: FUA-LLM reduces problematic outputs by up to 20% compared to state-of-the-art approaches while maintaining real-world usability, as confirmed by quantitative experiments and expert evaluations.

Conclusion: FUA-LLM successfully aligns LLM outputs with fair-use doctrine, reducing copyright infringement risks and preserving practical utility.

Abstract: Large language models (LLMs) commonly risk copyright infringement by
reproducing protected content verbatim or with insufficient transformative
modifications, posing significant ethical, legal, and practical concerns.
Current inference-time safeguards predominantly rely on restrictive
refusal-based filters, often compromising the practical utility of these
models. To address this, we collaborated closely with intellectual property
experts to develop FUA-LLM (Fair Use Aligned Language Models), a
legally-grounded framework explicitly designed to align LLM outputs with
fair-use doctrine. Central to our method is FairUseDB, a carefully constructed
dataset containing 18,000 expert-validated examples covering nine realistic
infringement scenarios. Leveraging this dataset, we apply Direct Preference
Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce
legally compliant and practically useful alternatives rather than resorting to
blunt refusal. Recognizing the shortcomings of traditional evaluation metrics,
we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic
Mean (CAH) to balance infringement risk against response utility. Extensive
quantitative experiments coupled with expert evaluations confirm that FUA-LLM
substantially reduces problematic outputs (up to 20\%) compared to
state-of-the-art approaches, while preserving real-world usability.

</details>


### [223] [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/abs/2505.23790)
*Shaojie Wang,Sirui Ding,Na Zou*

Main category: cs.CL

TL;DR: Recent advances in large language models (LLMs) have made it difficult to evaluate their intrinsic linguistic understanding. This paper proposes an information-theoretic framework based on mutual information (MI) to address this issue. The authors formalize understanding as MI between an input sentence and its latent representation, decompose sentence-level MI into token-level MI, derive a computable lower bound for token-level MI, and implement a recoverability task to measure MI across different LLMs. Results show that encoder-only models maintain higher information fidelity than decoder-only models, and fine-tuning to maximize token-level recoverability improves LLM understanding ability.


<details>
  <summary>Details</summary>
Motivation: Evaluating the intrinsic linguistic understanding of large language models remains challenging. Current evaluation methods are often specialized tasks that do not fully capture the nuances of linguistic understanding.

Method: The authors propose an information-theoretic framework based on mutual information (MI). They formalize understanding as MI between an input sentence and its latent representation, decompose sentence-level MI into token-level MI, derive a computable lower bound for token-level MI using Fano's inequality, and implement a recoverability task to measure MI across different LLMs.

Result: Encoder-only models consistently maintain higher information fidelity than decoder-only models. Decoder-only models exhibit a late-layer 'forgetting' pattern where mutual information is first enhanced and then discarded. Fine-tuning to maximize token-level recoverability improves understanding ability of LLMs on tasks without task-specific supervision.

Conclusion: Mutual information can serve as a foundation for understanding and improving language model capabilities.

Abstract: Recent advances in large language models (LLMs) have revolutionized natural
language processing, yet evaluating their intrinsic linguistic understanding
remains challenging. Moving beyond specialized evaluation tasks, we propose an
information-theoretic framework grounded in mutual information (MI) to achieve
this. We formalize the understanding as MI between an input sentence and its
latent representation (sentence-level MI), measuring how effectively input
information is preserved in latent representation. Given that LLMs learn
embeddings for individual tokens, we decompose sentence-level MI into
token-level MI between tokens and sentence embeddings, establishing theoretical
bounds connecting these measures. Based on this foundation, we theoretically
derive a computable lower bound for token-level MI using Fano's inequality,
which directly relates to token-level recoverability-the ability to predict
original tokens from sentence embedding. We implement this recoverability task
to comparatively measure MI across different LLMs, revealing that encoder-only
models consistently maintain higher information fidelity than their
decoder-only counterparts, with the latter exhibiting a distinctive late-layer
"forgetting" pattern where mutual information is first enhanced and then
discarded. Moreover, fine-tuning to maximize token-level recoverability
consistently improves understanding ability of LLMs on tasks without
task-specific supervision, demonstrating that mutual information can serve as a
foundation for understanding and improving language model capabilities.

</details>


### [224] [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.23794)
*Yuan Li,Qi Luo,Xiaonan Li,Bufan Li,Qinyuan Cheng,Bo Wang,Yining Zheng,Yuxin Wang,Zhangyue Yin,Xipeng Qiu*

Main category: cs.CL

TL;DR: R3-RAG is a new method that uses reinforcement learning to improve Retrieval-Augmented Generation (RAG) systems. It helps large language models learn how to reason and retrieve information step by step, leading to more accurate answers. The system has two stages and includes rewards for answer correctness and document relevance.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems face limitations due to dense retrievers being a bottleneck, having limited parameters compared to LLMs, and lacking step-by-step reasoning capabilities. Prompt-based iterative RAG is constrained by human-designed workflows.

Method: The R3-RAG model is divided into two stages: 1) Cold start - teaching the model to interleave reasoning and retrieval iteratively; 2) Reinforcement learning - enhancing the model's ability to explore the external retrieval environment. Two rewards are proposed: answer correctness for outcome reward and relevance-based document verification for process reward.

Result: Experimental results demonstrate that R3-RAG significantly outperforms baseline methods and shows good transferability across different retrievers.

Conclusion: R3-RAG successfully addresses the limitations of current RAG systems by using reinforcement learning to teach LLMs how to reason and retrieve information step by step, resulting in improved performance and factual correctness.

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge with Large
Language Models (LLMs) to enhance factual correctness and mitigate
hallucination. However, dense retrievers often become the bottleneck of RAG
systems due to their limited parameters compared to LLMs and their inability to
perform step-by-step reasoning. While prompt-based iterative RAG attempts to
address these limitations, it is constrained by human-designed workflows. To
address these limitations, we propose $\textbf{R3-RAG}$, which uses
$\textbf{R}$einforcement learning to make the LLM learn how to
$\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving
comprehensive external knowledge and leading to correct answers. R3-RAG is
divided into two stages. We first use cold start to make the model learn the
manner of iteratively interleaving reasoning and retrieval. Then we use
reinforcement learning to further harness its ability to better explore the
external retrieval environment. Specifically, we propose two rewards for
R3-RAG: 1) answer correctness for outcome reward, which judges whether the
trajectory leads to a correct answer; 2) relevance-based document verification
for process reward, encouraging the model to retrieve documents that are
relevant to the user question, through which we can let the model learn how to
iteratively reason and retrieve relevant documents to get the correct answer.
Experimental results show that R3-RAG significantly outperforms baselines and
can transfer well to different retrievers. We release R3-RAG at
https://github.com/Yuan-Li-FNLP/R3-RAG.

</details>


### [225] [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/abs/2505.23797)
*Zaihan Yang,Ryan Leonard,Hien Tran,Rory Driscoll,Chadbourne Davis*

Main category: cs.CL

TL;DR: The paper develops a RoBERTa-TF-IDF-PCA Hybrid model for classifying Reddit posts into four levels of suicide risk severity. It achieves an F1 score of 0.7512.


<details>
  <summary>Details</summary>
Motivation: Suicidal thoughts and behaviors are a growing societal concern, creating a need for tools that can enable early detection of suicidal risk.

Method: A multi-class classification task is framed using a RoBERTa-TF-IDF-PCA Hybrid model which integrates deep contextual embeddings from RoBERTa with TF-IDF term weighting compressed by PCA. Data resampling techniques and data augmentation strategies are also explored to address data imbalance and overfitting.

Result: The hybrid model outperforms other models including RoBERTa only, BERT model and traditional machine learning classifiers, achieving a best weighted F1 score of 0.7512.

Conclusion: The proposed RoBERTa-TF-IDF-PCA Hybrid model provides an effective method for classifying suicide risk severity from social media posts.

Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical
societal concern, highlighting the urgent need for effective tools to enable
early detection of suicidal risk. In this work, we develop robust machine
learning models that leverage Reddit posts to automatically classify them into
four distinct levels of suicide risk severity. We frame this as a multi-class
classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating
the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),
a state-of-the-art deep learning transformer model, with the statistical
term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy
and reliability of suicide risk assessment. To address data imbalance and
overfitting, we explore various data resampling techniques and data
augmentation strategies to enhance model generalization. Additionally, we
compare our model's performance against that of using RoBERTa only, the BERT
model and other traditional machine learning classifiers. Experimental results
demonstrate that the hybrid model can achieve improved performance, giving a
best weighted $F_{1}$ score of 0.7512.

</details>


### [226] [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)
*Jian Lan,Yifei Fu,Udo Schlegel,Gengyuan Zhang,Tanveer Hannan,Haokun Chen,Thomas Seidl*

Main category: cs.CL

TL;DR: Social bias in large vision-language models (VLMs) is evaluated and mitigated, revealing gender and race biases, mis-calibrated confidence levels, and fluctuating fairness in hidden layers. A post-hoc method is proposed to improve fairness without re-training.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the critical issue of social bias in large vision-language models (VLMs), where fairness and ethics-related problems harm certain groups of people in society. The authors aim to evaluate the extent of social bias in generative responses and develop a solution to mitigate it.

Method: The authors first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets using a multiple-choice selection task. They observe gender and race biases in model responses, as well as mis-calibrated confidence levels towards specific social groups. Further investigation reveals substantial fluctuations in fairness levels within VLMs' hidden layers and mixed effects of residuals on fairness. Based on these findings, they propose a post-hoc method that ablates bias-associated residuals and amplifies fairness-associated residuals during inference, which is training-free and model-agnostic.

Result: The proposed post-hoc method outperforms competing training strategies in improving the fairness of VLMs' responses and achieving more reliable confidence levels.

Conclusion: This study highlights the presence of social bias in VLMs and demonstrates the effectiveness of a post-hoc method in mitigating such bias without re-training the models.

Abstract: Social bias is a critical issue in large vision-language models (VLMs), where
fairness- and ethics-related problems harm certain groups of people in society.
It is unknown to what extent VLMs yield social bias in generative responses. In
this study, we focus on evaluating and mitigating social bias on both the
model's response and probability distribution. To do so, we first evaluate four
state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the
multiple-choice selection task. Surprisingly, we find that models suffer from
generating gender-biased or race-biased responses. We also observe that models
are prone to stating their responses are fair, but indeed having mis-calibrated
confidence levels towards particular social groups. While investigating why
VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit
substantial fluctuations in fairness levels. Meanwhile, residuals in each layer
show mixed effects on fairness, with some contributing positively while some
lead to increased bias. Based on these findings, we propose a post-hoc method
for the inference stage to mitigate social bias, which is training-free and
model-agnostic. We achieve this by ablating bias-associated residuals while
amplifying fairness-associated residuals on model hidden layers during
inference. We demonstrate that our post-hoc method outperforms the competing
training strategies, helping VLMs have fairer responses and more reliable
confidence levels.

</details>


### [227] [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)
*Xiaoyuan Wu,Weiran Lin,Omer Akgul,Lujo Bauer*

Main category: cs.CL

TL;DR: Large language models (LLMs) suffer from hallucinations and fragility. Current methods to measure LLM consistency do not align well with human perceptions. A new logit-based ensemble method is proposed, performing as well as the best existing metric but still imperfect, suggesting the need for more human evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations and fragility in LLMs, and to better align consistency measurement methods with human perceptions.

Method: A user study was conducted with 2,976 participants to evaluate current consistency measurement methods. A logit-based ensemble method was then proposed for estimating LLM consistency.

Result: The proposed method matches the performance of the best existing metric in estimating human ratings of LLM consistency. However, all non-human evaluation methods are found to be imperfect.

Conclusion: Evaluation of LLM consistency should incorporate more human input due to the imperfections of current automatic estimation methods.

Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to
prompt perturbations, often resulting in inconsistent or unreliable generated
text. Different methods have been proposed to mitigate such hallucinations and
fragility -- one of them being measuring the consistency (the model's
confidence in the response, or likelihood of generating a similar response when
resampled) of LLM responses. In previous work, measuring consistency often
relied on the probability of a response appearing within a pool of resampled
responses, or internal states or logits of responses. However, it is not yet
clear how well these approaches approximate how humans perceive the consistency
of LLM responses. We performed a user study (n=2,976) and found current methods
typically do not approximate users' perceptions of LLM consistency very well.
We propose a logit-based ensemble method for estimating LLM consistency, and we
show that this method matches the performance of the best-performing existing
metric in estimating human ratings of LLM consistency. Our results suggest that
methods of estimating LLM consistency without human evaluation are sufficiently
imperfect that we suggest evaluation with human input be more broadly used.

</details>


### [228] [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/abs/2505.23801)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan*

Main category: cs.CL

TL;DR: SEMFED is a new framework for federated learning in NLP tasks which reduces communication costs by 80.5% while keeping model accuracy over 98%.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) shows great potential in training machine learning models with data privacy, but applying it to NLP tasks has challenges like semantic heterogeneity, vocabulary mismatches and resource constraints.

Method: SEMFED includes three key innovations: semantic-aware client selection mechanism, adaptive NLP-specific model architectures and communication-efficient semantic feature compression technique.

Result: Experiments show SEMFED reduces communication costs by 80.5% and maintains model accuracy above 98%, outperforming other FL approaches.

Conclusion: SEMFED can handle heterogeneous client environments well, making it suitable for real-world federated NLP deployments.

Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for
training machine learning models while preserving data privacy. However,
applying FL to Natural Language Processing (NLP) tasks presents unique
challenges due to semantic heterogeneity across clients, vocabulary mismatches,
and varying resource constraints on edge devices. Objectives: This paper
introduces SEMFED, a novel semantic-aware resource-efficient federated learning
framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED
incorporates three key innovations: (1) a semantic-aware client selection
mechanism that balances semantic diversity with resource constraints, (2)
adaptive NLP-specific model architectures tailored to device capabilities while
preserving semantic information, and (3) a communication-efficient semantic
feature compression technique that significantly reduces bandwidth
requirements. Results: Experimental results on various NLP classification tasks
demonstrate that SEMFED achieves an 80.5% reduction in communication costs
while maintaining model accuracy above 98%, outperforming state-of-the-art FL
approaches. Conclusion: SEMFED effectively manages heterogeneous client
environments with varying computational resources, network reliability, and
semantic data distributions, making it particularly suitable for real-world
federated NLP deployments.

</details>


### [229] [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/abs/2505.23802)
*Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Michael Wornow,Juan M. Banda,Nikesh Kotecha,Timothy Keyes,Yifan Mai,Mert Oez,Hao Qiu,Shrey Jain,Leonardo Schettini,Mehr Kashyap,Jason Alan Fries,Akshay Swaminathan,Philip Chung,Fateme Nateghi,Asad Aali,Ashwin Nayak,Shivam Vedak,Sneha S. Jain,Birju Patel,Oluseyi Fayanju,Shreya Shah,Ethan Goh,Dong-han Yao,Brian Soetikno,Eduardo Reis,Sergios Gatidis,Vasu Divi,Robson Capasso,Rachna Saralkar,Chia-Chun Chiang,Jenelle Jindal,Tho Pham,Faraz Ghoddusi,Steven Lin,Albert S. Chiou,Christy Hong,Mohana Roy,Michael F. Gensheimer,Hinesh Patel,Kevin Schulman,Dev Dash,Danton Char,Lance Downing,Francois Grolleau,Kameron Black,Bethel Mieso,Aydin Zahedivash,Wen-wai Yim,Harshita Sharma,Tony Lee,Hannah Kirsch,Jennifer Lee,Nerissa Ambers,Carlene Lugtu,Aditya Sharma,Bilal Mawji,Alex Alekseyev,Vicky Zhou,Vikas Kakkar,Jarrod Helzer,Anurang Revri,Yair Bannett,Roxana Daneshjou,Jonathan Chen,Emily Alsentzer,Keith Morse,Nirmal Ravi,Nima Aghaeepour,Vanessa Kennedy,Akshay Chaudhari,Thomas Wang,Sanmi Koyejo,Matthew P. Lungren,Eric Horvitz,Percy Liang,Mike Pfeffer,Nigam H. Shah*

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLMs）在医学执照考试中接近满分，但这些评估未能充分反映现实临床实践的复杂性和多样性。本文介绍了MedHELM，一个可扩展的评估框架，用于评估LLM在医学任务中的表现，具有三个关键贡献：1. 经过临床医生验证的分类法；2. 全面的基准测试套件；3. 系统比较和成本性能分析。研究发现，高级推理模型表现最佳，Claude 3.5 Sonnet在较低计算成本下实现了相当的结果。此外，LLM-jury评估方法与临床医生评分有良好一致性。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型在医学领域的评估未能充分反映现实临床实践的复杂性和多样性，需要更全面、细致的评估框架来衡量LLM在医疗任务中的表现。

Method: 开发了一个名为MedHELM的可扩展评估框架，包含经过临床医生验证的分类法（涵盖5个类别、22个子类别和121项任务），一个由35个基准组成的综合基准套件（包括17个现有基准和18个新制定基准），以及使用LLM-jury进行系统比较和成本性能分析的方法。

Result: 评估了9个前沿LLM，发现在不同医学任务上的表现差异显著。高级推理模型表现最佳，Claude 3.5 Sonnet以较低的计算成本实现了相当的结果。LLM-jury评估方法与临床医生评分的一致性良好，超过了自动化基线方法。

Conclusion: 本研究强调了针对医学领域实际任务进行特定评估的重要性，并提供了一个开源框架以支持这一目标。

Abstract: While large language models (LLMs) achieve near-perfect scores on medical
licensing exams, these evaluations inadequately reflect the complexity and
diversity of real-world clinical practice. We introduce MedHELM, an extensible
evaluation framework for assessing LLM performance for medical tasks with three
key contributions. First, a clinician-validated taxonomy spanning 5 categories,
22 subcategories, and 121 tasks developed with 29 clinicians. Second, a
comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly
formulated) providing complete coverage of all categories and subcategories in
the taxonomy. Third, a systematic comparison of LLMs with improved evaluation
methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9
frontier LLMs, using the 35 benchmarks, revealed significant performance
variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%
win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved
comparable results at 40% lower estimated computational cost. On a normalized
accuracy scale (0-1), most models performed strongly in Clinical Note
Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83),
moderately in Medical Research Assistance (0.65-0.75), and generally lower in
Clinical Decision Support (0.56-0.72) and Administration & Workflow
(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with
clinician ratings (ICC = 0.47), surpassing both average clinician-clinician
agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and
BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top
models at lower estimated cost. These findings highlight the importance of
real-world, task-specific evaluation for medical use of LLMs and provides an
open source framework to enable this.

</details>


### [230] [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)
*Terrance Liu,Shuyi Wang,Daniel Preotiuc-Pietro,Yash Chandarana,Chirag Gupta*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在文本到SQL解析中表现良好，但有时会表现出自信的错误。本文研究了提供校准置信分数的问题，该分数传达输出查询正确的可能性。首次建立了基于LLM的文本到SQL解析的后验校准基准。结果表明，Platt缩放法比直接使用原始模型输出概率作为置信度得分提供了显著改进。此外，我们提出了一种利用SQL查询结构性质的方法，以提供更细致的正确性信号，称为“子句频率”(SCF)得分。通过多变量Platt缩放(MPS)，我们将各个SCF得分合并为一个总体准确且校准的得分。在两个流行的文本到SQL数据集上的实证评估表明，结合MPS和SCF的方法在校准和相关错误检测任务上优于传统的Platt缩放。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在文本到SQL解析方面表现出色，但它们有时会出现自信的错误。因此，建立可信赖的文本到SQL系统需要从LLM中引出可靠的不确定性度量。

Method: 本文引入了Platt缩放法来校准置信度得分，并提出了利用SQL查询结构特性的“子句频率”(SCF)得分方法。还扩展了经典的Platt缩放技术，即多变量Platt缩放(MPS)，将各个SCF得分合并成一个整体准确且校准的得分。

Result: 实证评估显示，结合多变量Platt缩放(MPS)和子句频率(SCF)得分的方法在校准和错误检测任务上取得了进一步的改进，优于传统的Platt缩放。

Conclusion: 本文首次建立了基于LLM的文本到SQL解析的后验校准基准，展示了Platt缩放法的有效性，并通过结合MPS和SCF得分进一步提升了校准和错误检测性能。

Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL
parsing, they sometimes exhibit unexpected failures in which they are
confidently incorrect. Building trustworthy text-to-SQL systems thus requires
eliciting reliable uncertainty measures from the LLM. In this paper, we study
the problem of providing a calibrated confidence score that conveys the
likelihood of an output query being correct. Our work is the first to establish
a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In
particular, we show that Platt scaling, a canonical method for calibration,
provides substantial improvements over directly using raw model output
probabilities as confidence scores. Furthermore, we propose a method for
text-to-SQL calibration that leverages the structured nature of SQL queries to
provide more granular signals of correctness, named "sub-clause frequency"
(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the
canonical Platt scaling technique, we combine individual SCF scores into an
overall accurate and calibrated score. Empirical evaluation on two popular
text-to-SQL datasets shows that our approach of combining MPS and SCF yields
further improvements in calibration and the related task of error detection
over traditional Platt scaling.

</details>


### [231] [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/abs/2505.23806)
*Sihyeon Lee,Hyunjoo Song,Jong-chan Lee,Yoon Jin Lee,Boram Lee,Hee-Eon Lim,Dongyeong Kim,Jinwook Seo,Bohyoung Kim*

Main category: cs.CL

TL;DR: MedOrchestra is a hybrid framework combining cloud and local LLMs for clinical tasks, preserving privacy while enhancing performance. Evaluated on pancreatic cancer staging, it outperforms local models and clinicians.


<details>
  <summary>Details</summary>
Motivation: To address the trade-offs in deploying large language models in clinical settings, balancing privacy concerns with the need for accurate and complex clinical interpretation.

Method: A cloud LLM decomposes complex clinical tasks into subtasks and generates prompts based on clinical guidelines and synthetic test cases without accessing clinical data. A local LLM executes these subtasks and synthesizes outputs locally to preserve privacy.

Result: On free-text radiology reports, MedOrchestra achieves 70.21% accuracy, surpassing local models and board-certified clinicians. On structured reports, it reaches 85.42% accuracy, demonstrating superiority across all settings.

Conclusion: MedOrchestra effectively leverages the strengths of both cloud and local LLMs, providing a privacy-preserving solution that enhances accuracy in clinical interpretation tasks.

Abstract: Deploying large language models (LLMs) in clinical settings faces critical
trade-offs: cloud LLMs, with their extensive parameters and superior
performance, pose risks to sensitive clinical data privacy, while local LLMs
preserve privacy but often fail at complex clinical interpretation tasks. We
propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex
clinical tasks into manageable subtasks and prompt generation, while a local
LLM executes these subtasks in a privacy-preserving manner. Without accessing
clinical data, the cloud LLM generates and validates subtask prompts using
clinical guidelines and synthetic test cases. The local LLM executes subtasks
locally and synthesizes outputs generated by the cloud LLM. We evaluate
MedOrchestra on pancreatic cancer staging using 100 radiology reports under
NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,
outperforming local model baselines (without guideline: 48.94%, with guideline:
56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:
65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches
85.42% accuracy, showing clear superiority across all settings.

</details>


### [232] [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807)
*Yuli Chen,Bo Cheng,Jiale Han,Yingying Zhang,Yingting Li,Shuhao Zhang*

Main category: cs.CL

TL;DR: 为了克服主流剪枝技术在高稀疏度下可能导致的性能显著下降问题，提出了一种新的动态层内剪枝（DLP）方法，能够自适应地确定每一层的重要性并分配剪枝率。实验表明，DLP在高稀疏度下能有效保持模型性能，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的主流剪枝技术通常依赖于均匀的层内剪枝策略，在高稀疏度下可能导致严重的性能下降。虽然非均匀层内剪枝策略有所改进，但其依赖预定义值可能造成次优性能。因此需要一种可以自适应确定每一层重要性并分配剪枝率的方法。

Method: 提出了一种名为动态层内剪枝（DLP）的新方法，通过整合模型权重与输入激活信息，自适应地确定每一层的相对重要性，并根据重要性分配剪枝率。

Result: 实验结果表明，DLP能够在多个大语言模型上有效地保持高稀疏度下的模型性能。具体来说，在70%稀疏度下，DLP将LLaMA2-7B的困惑度降低了7.79，并使平均准确率提高了2.7%。此外，DLP可与现有的多种大语言模型压缩技术兼容，并能无缝集成到参数高效微调（PEFT）中。

Conclusion: DLP方法在高稀疏度下能够有效保持模型性能，优于现有方法，并且与多种现有技术兼容，为未来的研究提供了便利。

Abstract: Pruning has recently been widely adopted to reduce the parameter scale and
improve the inference efficiency of Large Language Models (LLMs). Mainstream
pruning techniques often rely on uniform layerwise pruning strategies, which
can lead to severe performance degradation at high sparsity levels. Recognizing
the varying contributions of different layers in LLMs, recent studies have
shifted their focus toward non-uniform layerwise pruning. However, these
approaches often rely on pre-defined values, which can result in suboptimal
performance. To overcome these limitations, we propose a novel method called
Dynamic Layerwise Pruning (DLP). This approach adaptively determines the
relative importance of each layer by integrating model weights with input
activation information, assigning pruning rates accordingly. Experimental
results show that DLP effectively preserves model performance at high sparsity
levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the
perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%
compared to state-of-the-art methods. Moreover, DLP is compatible with various
existing LLM compression techniques and can be seamlessly integrated into
Parameter-Efficient Fine-Tuning (PEFT). We release the code at
https://github.com/ironartisan/DLP to facilitate future research.

</details>


### [233] [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2505.23808)
*Lin Mu,Xiaoyu Wang,Li Ni,Yang Li,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Main category: cs.CL

TL;DR: DenseLoRA is a novel approach that improves upon LoRA by enhancing parameter efficiency and achieving superior performance through a dense low-rank matrix.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in parameter utilization found in LoRA, where many weights in the matrices are redundant.

Method: DenseLoRA uses a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. It adapts LLMs through a dense low-rank matrix instead of two redundant low-rank matrices.

Result: DenseLoRA achieves 83.8% accuracy with only 0.01% of trainable parameters on LLaMA3-8B, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters. Extensive experiments show the impact of DenseLoRA's components on model performance.

Conclusion: DenseLoRA enhances parameter efficiency and achieves better performance than LoRA.

Abstract: Low-rank adaptation (LoRA) has been developed as an efficient approach for
adapting large language models (LLMs) by fine-tuning two low-rank matrices,
thereby reducing the number of trainable parameters. However, prior research
indicates that many of the weights in these matrices are redundant, leading to
inefficiencies in parameter utilization. To address this limitation, we
introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances
parameter efficiency while achieving superior performance compared to LoRA.
DenseLoRA builds upon the concept of representation fine-tuning, incorporating
a single Encoder-Decoder to refine and compress hidden representations across
all adaptation layers before applying adaptation. Instead of relying on two
redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense
low-rank matrix, improving parameter utilization and adaptation efficiency. We
evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%
accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%
accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we
conduct extensive experiments to systematically assess the impact of
DenseLoRA's components on overall model performance. Code is available at
https://github.com/mulin-ahu/DenseLoRA.

</details>


### [234] [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/abs/2505.23809)
*Haowei Yang,Haotian Lyu,Tianle Zhang,Dingzhou Wang,Yushang Zhao*

Main category: cs.CL

TL;DR: E-commerce competition prompts a need to balance creative content with conversion. This paper proposes a framework leveraging LLMs for generating effective marketing copy, achieving 12.5% CTR and 8.3% CVR increase.


<details>
  <summary>Details</summary>
Motivation: With the intensifying e-commerce competition, there is a critical need to balance creative content with conversion effectiveness in marketing copy.

Method: The proposed framework integrates prompt engineering, multi-objective fine-tuning, and post-processing using LLMs' language generation capabilities. The fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding.

Result: The approach achieves a 12.5% increase in CTR and an 8.3% increase in CVR through offline evaluations and online A/B tests across categories, while maintaining content novelty.

Conclusion: This provides a practical solution for automated copy generation and suggests potential for future multimodal and real-time personalization.

Abstract: As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.

</details>


### [235] [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
*Chenghao Yang,Yinbo Luo,Zhoufutu Wen,Qi Chu,Tao Gong,Longxiang Liu,Kaiyuan Zhang,Jianpeng Jiao,Ge Zhang,Wenhao Huang,Nenghai Yu*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）如ChatGPT在实际对话应用中被广泛使用，但其处理长复杂对话的能力一直受到批评。为此，研究者提出了MARS-Bench基准测试，专门用于评估多轮对话中的三个关键方面：超多轮、交互多轮和跨轮任务。实验表明，闭源LLMs显著优于开源替代品，明确推理能增强LLMs处理长复杂对话的鲁棒性，但LLMs在处理动机转移和复杂的跨轮依赖时仍面临重大挑战。此外，通过注意力可视化实验揭示了由于特殊标记导致注意力汇聚而引起的LLMs性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前没有基准能够全面反映大型语言模型（LLMs）在处理长复杂对话会话中的弱点，包括频繁的动机转移和复杂的跨轮依赖。

Method: 构建了一个名为MARS-Bench的基准测试，该基准从逐场文本评论中构造，以反映现实对话，并专门设计用于评估多轮对话中的三个方面：超多轮、交互多轮和跨轮任务。

Result: 闭源LLMs显著优于开源LLMs；明确推理显著提高了LLMs处理长复杂对话会话的鲁棒性；LLMs在处理动机转移和复杂的跨轮依赖时确实面临重大挑战；注意力汇聚由于特殊标记引起LLMs性能下降。

Conclusion: MARS-Bench填补了现有基准无法充分反映LLMs在处理长复杂对话会话中弱点的空白，提供了对LLMs性能的新见解，并指出了进一步改进的方向。

Abstract: Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted
in real-world dialogue applications. However, LLMs' robustness, especially in
handling long complex dialogue sessions, including frequent motivation
transfer, sophisticated cross-turn dependency, is criticized all along.
Nevertheless, no existing benchmarks can fully reflect these weaknesses. We
present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic
\textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to
remedy the gap. MARS-Bench is constructed from play-by-play text commentary so
to feature realistic dialogues specifically designed to evaluate three critical
aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,
and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that
closed-source LLMs significantly outperform open-source alternatives, explicit
reasoning significantly boosts LLMs' robustness on handling long complex
dialogue sessions, and LLMs indeed face significant challenges when handling
motivation transfer and sophisticated cross-turn dependency. Moreover, we
provide mechanistic interpretability on how attention sinks due to special
tokens lead to LLMs' performance degradation when handling long complex
dialogue sessions based on attention visualization experiment in
Qwen2.5-7B-Instruction.

</details>


### [236] [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
*Hadi Askari,Shivanshu Gupta,Fei Wang,Anshuman Chhabra,Muhao Chen*

Main category: cs.CL

TL;DR: Pretrained Large Language Models (LLMs) show variability in layer training quality, impacting downstream performance. Current methods to assess this are model-centric and overlook data influence. This paper introduces LayerIF, a data-driven framework using Influence Functions to evaluate layer-wise training quality, considering both architecture and data. It provides task-specific importance estimates for LLM layers, showing how they specialize for different tasks. Demonstrated through two applications - expert allocation in LoRA-MoE and sparsity distribution in LLM pruning - LayerIF improves task performance across multiple LLM architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need to accurately estimate layer-wise training quality in LLMs to enhance their downstream performance, addressing the limitation of current model-centric approaches that neglect data influence.

Method: LayerIF is proposed, a data-driven framework leveraging Influence Functions. It isolates each layer's gradients and measures the sensitivity of validation loss to training examples to derive data-driven estimates of layer importance in a task-sensitive manner.

Result: Experiments show that LayerIF produces task-specific layer importance estimates and leads to consistent gains in task performance when used for expert allocation in LoRA-MoE architectures and layer-wise sparsity distribution in LLM pruning.

Conclusion: LayerIF offers a model-agnostic, influence-guided method to assess and improve LLM layer training quality, enhancing downstream task performance.

Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a
wide range of tasks, yet exhibit substantial variability in the various layers'
training quality with respect to specific downstream applications, limiting
their downstream performance.It is therefore critical to estimate layer-wise
training quality in a manner that accounts for both model architecture and
training data. However, existing approaches predominantly rely on model-centric
heuristics (such as spectral statistics, outlier detection, or uniform
allocation) while overlooking the influence of data. To address these
limitations, we propose LayerIF, a data-driven framework that leverages
Influence Functions to quantify the training quality of individual layers in a
principled and task-sensitive manner. By isolating each layer's gradients and
measuring the sensitivity of the validation loss to training examples by
computing layer-wise influences, we derive data-driven estimates of layer
importance. Notably, our method produces task-specific layer importance
estimates for the same LLM, revealing how layers specialize for different
test-time evaluation tasks. We demonstrate the utility of our scores by
leveraging them for two downstream applications: (a) expert allocation in
LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM
pruning. Experiments across multiple LLM architectures demonstrate that our
model-agnostic, influence-guided allocation leads to consistent gains in task
performance.

</details>


### [237] [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/abs/2505.23812)
*Lata Pangtey,Mohammad Zia Ur Rehman,Prasad Chaudhari,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CL

TL;DR: The paper proposes SPLAENet, a novel method for stance prediction in misinformative social media content. It uses dual cross-attention, hierarchical attention, emotion incorporation, and label fusion to improve stance detection accuracy. Experiments show significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need to combat misinformation by effectively analyzing underlying biases in shared information through stance detection, which can polarize user opinions.

Method: SPLAENet employs a dual cross-attention mechanism and a hierarchical attention network to capture inter and intra-relationships between source and reply texts. Emotions are incorporated to distinguish different stance categories based on emotional alignment or divergence. Label fusion using distance-metric learning aligns extracted features with stance labels.

Result: SPLAENet achieves an average gain of 8.92% in accuracy and 17.36% in F1-score on the RumourEval dataset, 7.02% in accuracy and 10.92% in F1-score on the SemEval dataset, and 10.03% in accuracy and 11.18% in F1-score on the P-stance dataset.

Conclusion: The proposed method demonstrates significant improvements in stance detection for misinformative social media content.

Abstract: The rapid evolution of social media has generated an overwhelming volume of
user-generated content, conveying implicit opinions and contributing to the
spread of misinformation. The method aims to enhance the detection of stance
where misinformation can polarize user opinions. Stance detection has emerged
as a crucial approach to effectively analyze underlying biases in shared
information and combating misinformation. This paper proposes a novel method
for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual
cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work
(SPLAENet) in misinformative social media user-generated content. The proposed
method employs a dual cross-attention mechanism and a hierarchical attention
network to capture inter and intra-relationships by focusing on the relevant
parts of source text in the context of reply text and vice versa. We
incorporate emotions to effectively distinguish between different stance
categories by leveraging the emotional alignment or divergence between the
texts. We also employ label fusion that uses distance-metric learning to align
extracted features with stance labels, improving the method's ability to
accurately distinguish between stances. Extensive experiments demonstrate the
significant improvements achieved by SPLAENet over existing state-of-the-art
methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and
17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it
achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the
P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and
11.18\% in F1-score. These results validate the effectiveness of the proposed
method for stance detection in the context of misinformative social media
content.

</details>


### [238] [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
*Joe Needham,Giles Edkins,Govind Pimpale,Henning Bartsch,Marius Hobbhahn*

Main category: cs.CL

TL;DR: 尽管前沿AI模型（如Gemini-2.5-Pro，AUC为0.83）已展现出显著的评估意识能力，但仍未超越简单的人类基准（AUC为0.92）。此外，在代理设置中，AI模型和人类更擅长识别评估。并且，AI模型能够远超随机水平去识别评估的目的。因此，建议在未来的模型中追踪这一能力。


<details>
  <summary>Details</summary>
Motivation: 如果AI模型能检测到它们正在被评估，那么评估的有效性可能会受到损害。例如，在评估期间模型的行为可能系统性地不同，这将导致用于部署和治理决策的基准可靠性降低。

Method: 构建一个包含1000个提示和来自61个不同数据集的转录本的多样化基准，这些数据集涵盖了公共基准、现实世界部署交互和代理框架的代理轨迹。然后测试前沿语言模型是否可以根据转录本来源是评估还是实际部署进行准确分类，并进一步测试模型是否可以识别评估的目的。

Result: 前沿模型展示了高于随机水平的评估意识（Gemini-2.5-Pro的AUC为0.83），但尚未超过简单的人类基准（AUC为0.92）。此外，AI模型和人类在代理设置中比在聊天设置中更擅长识别评估。通过多项选择和开放式提问，AI模型在识别评估目的方面远远超过随机机会。

Conclusion: 前沿模型已经表现出相当大的评估意识能力，但尚未达到超人的水平。推荐在未来的模型中追踪这一能力。

Abstract: If AI models can detect when they are being evaluated, the effectiveness of
evaluations might be compromised. For example, models could have systematically
different behavior during evaluations, leading to less reliable benchmarks for
deployment and governance decisions. We investigate whether frontier language
models can accurately classify transcripts based on whether they originate from
evaluations or real-world deployment, a capability we call evaluation
awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts
and transcripts from 61 distinct datasets. These span public benchmarks (e.g.,
MMLU, SWEBench), real-world deployment interactions, and agent trajectories
from scaffolding frameworks (e.g., web-browsing agents). Frontier models
clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches
an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of
$0.92$). Furthermore, both AI models and humans are better at identifying
evaluations in agentic settings compared to chat settings. Additionally, we
test whether models can identify the purpose of the evaluation. Under
multiple-choice and open-ended questioning, AI models far outperform random
chance in identifying what an evaluation is testing for. Our results indicate
that frontier models already exhibit a substantial, though not yet superhuman,
level of evaluation-awareness. We recommend tracking this capability in future
models.

</details>


### [239] [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/abs/2505.23851)
*Michael Shalyt,Rotem Elimelech,Ido Kaminer*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在符号数学方面的能力接近大学水平，但现有基准无法充分评估其核心技能。为此，我们引入了ASyMOB，一个专注于符号操作的评估框架。通过分析LLMs在不同扰动类型问题中的表现，发现其依赖于记忆模式而非深层理解。结合代码执行能力的模型表现出更高的准确性。最先进模型展现出高符号数学能力和对扰动的强鲁棒性，可能标志着前沿LLMs泛化能力的“相变”。未来发展方向在于更深层次的外部工具集成或开发使CAS系统变得不必要的模型。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLMs符号数学能力的基准存在不足，无法全面衡量其核心技能，如积分、微分方程和代数简化等。因此需要一个新的评估框架来填补这一空白。

Method: 提出ASyMOB评估框架，包含17,092个独特的数学挑战，按相似性和复杂性组织。通过比较模型在简单数值或符号扰动问题中的表现，分析LLMs的泛化能力。同时对比LLMs与计算机代数系统的性能。

Result: 被评估的LLMs在所有扰动类型中表现显著下降（高达-70.3%），表明其依赖记忆模式而非深层理解。结合代码执行能力的模型准确率更高，特别是在某些扰动类型中可提升+33.1%。最先进的模型展现出高符号数学能力和对扰动的强鲁棒性。

Conclusion: 最先进LLMs在符号数学方面展现出高能力和强鲁棒性，可能标志着其泛化能力的“相变”。未来研究需探讨是通过更深层次的外部工具集成还是开发使CAS系统变得不必要的模型来进一步提升能力。

Abstract: Large language models (LLMs) are rapidly approaching the level of proficiency
in university-level symbolic mathematics required for applications in advanced
science and technology. However, existing benchmarks fall short in assessing
the core skills of LLMs in symbolic mathematics-such as integration,
differential equations, and algebraic simplification. To address this gap, we
introduce ASyMOB, a novel assessment framework focused exclusively on symbolic
manipulation, featuring 17,092 unique math challenges, organized by similarity
and complexity. ASyMOB enables analysis of LLM generalization capabilities by
comparing performance in problems that differ by simple numerical or symbolic
`perturbations'. Evaluated LLMs exhibit substantial degradation in performance
for all perturbation types (up to -70.3%), suggesting reliance on memorized
patterns rather than deeper understanding of symbolic math, even among models
achieving high baseline accuracy. Comparing LLM performance to computer algebra
systems, we identify examples where they fail while LLMs succeed, as well as
problems solved only by combining both approaches. Models capable of integrated
code execution yielded higher accuracy compared to their performance without
code, particularly stabilizing weaker models (up to +33.1% for certain
perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5
Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and
97.6% on the unperturbed set), but also remarkable robustness against
perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).
This may indicate a recent "phase transition" in the generalization
capabilities of frontier LLMs. It remains to be seen whether the path forward
lies in deeper integration with sophisticated external tools, or in developing
models so capable that symbolic math systems like CAS become unnecessary.

</details>


### [240] [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/abs/2505.23852)
*Nic Dobbins,Christelle Xiong,Kristine Lan,Meliha Yetisgen*

Main category: cs.CL

TL;DR: This paper explores the ability of Large Language Models (LLMs) to autonomously reproduce findings from published biomedical research using the same dataset. The study used GPT-4o to simulate a research team that reproduced about 53.2% of findings across five Alzheimer's studies, highlighting both potential and limitations of LLMs in automating reproducibility assessments.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capabilities of LLMs as autonomous agents for reproducing published research findings in biomedical field using the same or similar dataset, thereby assessing their potential for automating reproducibility assessments.

Method: The 'Quick Access' dataset from the National Alzheimer's Coordinating Center was utilized. Highly cited manuscripts using NACC data were identified, and five reproducible studies were selected. A simulated research team of LLM-based autonomous agents (using GPT-4o) was created to write and execute code for reproducing the findings based solely on study Abstracts, Methods sections, and data dictionary descriptions.

Result: Across five Alzheimer's studies, LLM agents approximately reproduced 53.2% of key findings described in the Abstracts. Discrepancies were found in numeric values, range-based findings, and applied statistical methods compared to the original studies.

Conclusion: The study demonstrates the potential and current limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.

Abstract: Objective: To demonstrate the capabilities of Large Language Models (LLMs) as
autonomous agents to reproduce findings of published research studies using the
same or similar dataset.
  Materials and Methods: We used the "Quick Access" dataset of the National
Alzheimer's Coordinating Center (NACC). We identified highly cited published
research manuscripts using NACC data and selected five studies that appeared
reproducible using this dataset alone. Using GPT-4o, we created a simulated
research team of LLM-based autonomous agents tasked with writing and executing
code to dynamically reproduce the findings of each study, given only study
Abstracts, Methods sections, and data dictionary descriptions of the dataset.
  Results: We extracted 35 key findings described in the Abstracts across 5
Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of
findings per study. Numeric values and range-based findings often differed
between studies and agents. The agents also applied statistical methods or
parameters that varied from the originals, though overall trends and
significance were sometimes similar.
  Discussion: In some cases, LLM-based agents replicated research techniques
and findings. In others, they failed due to implementation flaws or missing
methodological detail. These discrepancies show the current limits of LLMs in
fully automating reproducibility assessments. Still, this early investigation
highlights the potential of structured agent-based systems to provide scalable
evaluation of scientific rigor.
  Conclusion: This exploratory work illustrates both the promise and
limitations of LLMs as autonomous agents for automating reproducibility in
biomedical research.

</details>


### [241] [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/abs/2505.23854)
*Linwei Tao,Yi-Fan Yeh,Minjing Dong,Tao Huang,Philip Torr,Chang Xu*

Main category: cs.CL

TL;DR: In this paper, researchers comprehensively study uncertainty estimation in large language models (LLMs), evaluating 80 models using various methods and tasks. They find that linguistic verbal uncertainty (LVU) outperforms other methods and emphasize the need for multi-perspective evaluation.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for robust uncertainty estimation as LLMs are increasingly used in high-stakes applications to ensure safe and trustworthy deployment.

Method: The study evaluates 80 LLMs across different categories using three black-box single-pass methods: token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU). The challenging MMLU-Pro benchmark was used for systematic evaluation of uncertainty calibration and selective classification.

Result: LVU consistently outperforms TPU and NVU with stronger calibration and discrimination while being more interpretable. High accuracy does not imply reliable uncertainty, and factors like model scale, post-training, reasoning ability, and quantization influence estimation performance. LLMs show better uncertainty estimates on reasoning tasks than knowledge-heavy ones.

Conclusion: The findings underline the importance of multi-perspective evaluation of LLMs and establish LVU as a practical tool for enhancing reliability in real-world settings.

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
applications, robust uncertainty estimation is essential for ensuring the safe
and trustworthy deployment of LLMs. We present the most comprehensive study to
date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and
closed-source families, dense and Mixture-of-Experts (MoE) architectures,
reasoning and non-reasoning modes, quantization variants and parameter scales
from 0.6B to 671B. Focusing on three representative black-box single-pass
methods, including token probability-based uncertainty (TPU), numerical verbal
uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically
evaluate uncertainty calibration and selective classification using the
challenging MMLU-Pro benchmark, which covers both reasoning-intensive and
knowledge-based tasks. Our results show that LVU consistently outperforms TPU
and NVU, offering stronger calibration and discrimination while being more
interpretable. We also find that high accuracy does not imply reliable
uncertainty, and that model scale, post-training, reasoning ability and
quantization all influence estimation performance. Notably, LLMs exhibit better
uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good
calibration does not necessarily translate to effective error ranking. These
findings highlight the need for multi-perspective evaluation and position LVU
as a practical tool for improving the reliability of LLMs in real-world
settings.

</details>


### [242] [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)
*Sahil Verma,Keegan Hines,Jeff Bilmes,Charlotte Siska,Luke Zettlemoyer,Hila Gonen,Chandan Singh*

Main category: cs.CL

TL;DR: OMNIGUARD is a new approach that can detect harmful prompts across languages and modalities, achieving significant improvements in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The increasing capabilities of LLMs bring concerns about harmful misuse. Current detection approaches have limitations, especially for low-resource languages or non-text modalities.

Method: OMNIGUARD identifies aligned internal representations of an LLM/MLLM across languages or modalities and uses them to build a language-agnostic or modality-agnostic classifier.

Result: OMNIGUARD improves classification accuracy by 11.57% in multilingual settings, 20.44% for image-based prompts, and sets a new SOTA for audio-based prompts. It is also very efficient (≈120× faster than the next fastest baseline).

Conclusion: OMNIGUARD provides a robust and efficient solution for detecting harmful prompts across languages and modalities.

Abstract: The emerging capabilities of large language models (LLMs) have sparked
concerns about their immediate potential for harmful misuse. The core approach
to mitigate these concerns is the detection of harmful queries to the model.
Current detection approaches are fallible, and are particularly susceptible to
attacks that exploit mismatched generalization of model capabilities (e.g.,
prompts in low-resource languages or prompts provided in non-text modalities
such as image and audio). To tackle this challenge, we propose OMNIGUARD, an
approach for detecting harmful prompts across languages and modalities. Our
approach (i) identifies internal representations of an LLM/MLLM that are
aligned across languages or modalities and then (ii) uses them to build a
language-agnostic or modality-agnostic classifier for detecting harmful
prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\%
over the strongest baseline in a multilingual setting, by 20.44\% for
image-based prompts, and sets a new SOTA for audio-based prompts. By
repurposing embeddings computed during generation, OMNIGUARD is also very
efficient ($\approx 120 \times$ faster than the next fastest baseline). Code
and data are available at: https://github.com/vsahil/OmniGuard.

</details>


### [243] [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/abs/2505.23867)
*Zeyu Liu,Zhitian Hou,Yining Di,Kejing Yang,Zhijie Sang,Congkai Xie,Jingwen Yang,Siyuan Liu,Jialu Wang,Chunming Li,Ming Li,Hongxia Yang*

Main category: cs.CL

TL;DR: Infi-Med is a framework for medical MLLMs with three key innovations that addresses challenges in resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy. It achieves SOTA performance in general medical reasoning.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models show potential in healthcare but face challenges in practical deployment such as resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy.

Method: Proposes Infi-Med, a framework with three innovations: 1) resource-efficient approach with high-quality SFT datasets; 2) enhanced multimodal reasoning capabilities; 3) systematic evaluation system.

Result: Experiments show Infi-Med achieves state-of-the-art performance in general medical reasoning while being adaptable to clinical scenarios.

Conclusion: Infi-Med provides a strong foundation for deploying MLLMs in real-world healthcare settings by balancing model effectiveness and operational constraints.

Abstract: Multimodal large language models (MLLMs) have demonstrated promising
prospects in healthcare, particularly for addressing complex medical tasks,
supporting multidisciplinary treatment (MDT), and enabling personalized
precision medicine. However, their practical deployment faces critical
challenges in resource efficiency, diagnostic accuracy, clinical
considerations, and ethical privacy. To address these limitations, we propose
Infi-Med, a comprehensive framework for medical MLLMs that introduces three key
innovations: (1) a resource-efficient approach through curating and
constructing high-quality supervised fine-tuning (SFT) datasets with minimal
sample requirements, with a forward-looking design that extends to both
pretraining and posttraining phases; (2) enhanced multimodal reasoning
capabilities for cross-modal integration and clinical task understanding; and
(3) a systematic evaluation system that assesses model performance across
medical modalities and task types. Our experiments demonstrate that Infi-Med
achieves state-of-the-art (SOTA) performance in general medical reasoning while
maintaining rapid adaptability to clinical scenarios. The framework establishes
a solid foundation for deploying MLLMs in real-world healthcare settings by
balancing model effectiveness with operational constraints.

</details>


### [244] [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/abs/2505.23912)
*Caiqi Zhang,Xiaochen Zhu,Chengzu Li,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: Hallucination is a challenge for LLMs in factual content generation. Previous methods use confidence estimation but are computationally expensive. This paper proposes LoVeC, an efficient method for long-form verbalized confidence estimation using RL-trained LLMs to append confidence scores. Experiments show better calibration and robust generalization across domains.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucination in LLMs during factual content generation and improve upon existing computationally expensive confidence estimation methods.

Method: Propose LoVeC, which uses reinforcement learning to train LLMs to append numerical confidence scores to each generated statement for long-form generation.

Result: Experiments on three long-form QA datasets demonstrate better calibration, robust generalization across domains, and high efficiency by only adding a few tokens to the output.

Conclusion: LoVeC provides an efficient and effective method for verbalized confidence estimation in long-form generation, enhancing model calibration and generalization.

Abstract: Hallucination remains a major challenge for the safe and trustworthy
deployment of large language models (LLMs) in factual content generation. Prior
work has explored confidence estimation as an effective approach to
hallucination detection, but often relies on post-hoc self-consistency methods
that require computationally expensive sampling. Verbalized confidence offers a
more efficient alternative, but existing approaches are largely limited to
short-form question answering (QA) tasks and do not generalize well to
open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized
Confidence), an on-the-fly verbalized confidence estimation method for
long-form generation. Specifically, we use reinforcement learning (RL) to train
LLMs to append numerical confidence scores to each generated statement, serving
as a direct and interpretable signal of the factuality of generation. Our
experiments consider both on-policy and off-policy RL methods, including DPO,
ORPO, and GRPO, to enhance the model calibration. We introduce two novel
evaluation settings, free-form tagging and iterative tagging, to assess
different verbalized confidence estimation methods. Experiments on three
long-form QA datasets show that our RL-trained models achieve better
calibration and generalize robustly across domains. Also, our method is highly
efficient, as it only requires adding a few tokens to the output being decoded.

</details>


### [245] [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/abs/2505.23914)
*Yuxin Wang,Botao Yu,Ivory Yang,Saeed Hassanpour,Soroush Vosoughi*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在内容审核中存在过度敏感的问题，不仅因为识别冒犯性词汇，还由于系统性的主题偏见。通过引入'主题关联分析'方法，研究发现更先进的模型尽管整体误报率较低，但表现出更强的主题刻板印象。这表明LLMs依赖于学习到的主题关联而非仅仅对冒犯性语言作出反应，从而影响其审核决策。研究强调需要超越基于关键词的过滤进行改进。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在内容审核时经常将无害评论误分类为有毒评论，导致过度敏感。虽然以往研究主要归因于冒犯性词汇的存在，但本研究试图探索除词汇层面外可能引发此问题的原因。

Method: 受认知心理学隐性联想测试启发，提出了一种语义级别的方法——主题关联分析，以量化LLMs如何将某些主题与毒性相关联。通过对被误分类的无害评论生成自由形式的情景想象，并分析其主题放大程度来揭示模型的主题联想特性。

Result: 更先进的模型（如GPT-4 Turbo）尽管整体误报率较低，但在主题联想方面表现出更强的刻板印象，说明LLMs不仅对显式的冒犯性语言作出反应，还依赖于所学的主题关联。

Conclusion: 研究结果揭示了LLMs在内容审核中的过度敏感并非仅由关键词触发，而是受到主题关联的影响，强调了需要进一步改进，超越传统的关键词过滤机制。

Abstract: Large Language Models are widely used for content moderation but often
misclassify benign comments as toxic, leading to over-sensitivity. While
previous research attributes this issue primarily to the presence of offensive
terms, we reveal a potential cause beyond token level: LLMs exhibit systematic
topic biases in their implicit associations. Inspired by cognitive psychology's
implicit association tests, we introduce Topic Association Analysis, a
semantic-level approach to quantify how LLMs associate certain topics with
toxicity. By prompting LLMs to generate free-form scenario imagination for
misclassified benign comments and analyzing their topic amplification levels,
we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger
topic stereotype despite lower overall false positive rates. These biases
suggest that LLMs do not merely react to explicit, offensive language but rely
on learned topic associations, shaping their moderation decisions. Our findings
highlight the need for refinement beyond keyword-based filtering, providing
insights into the underlying mechanisms driving LLM over-sensitivity.

</details>


### [246] [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/abs/2505.23923)
*Feiteng Fang,Ting-En Lin,Yuchuan Wu,Xiong Liu,Xiang Huang,Dingwei Chen,Jing Ye,Haonan Zhang,Liang Zhu,Hamid Alinejad-Rokny,Min Yang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: The paper introduces ChARM, a Character-based Act-adaptive Reward Model for RPLAs with two innovations and presents RoleplayPref dataset and RoleplayEval benchmark. It shows significant improvements over conventional models and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional reward models for RPLAs lack scalability and struggle to adapt to subjective conversational preferences.

Method: Propose ChARM with an act-adaptive margin and a self-evolution mechanism. Also introduce RoleplayPref dataset and RoleplayEval benchmark for evaluation.

Result: 13% improvement over the conventional Bradley-Terry model in preference rankings and state-of-the-art results on CharacterEval and RoleplayEval.

Conclusion: ChARM effectively addresses the limitations of traditional reward models and advances the field of RPLAs.

Abstract: Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic
and engaging human-computer interactions. However, traditional reward models
often struggle with scalability and adapting to subjective conversational
preferences. We propose ChARM, a Character-based Act-adaptive Reward Model,
addressing these challenges through two innovations: (1) an act-adaptive margin
that significantly enhances learning efficiency and generalizability, and (2) a
self-evolution mechanism leveraging large-scale unlabeled data to improve
training coverage. Additionally, we introduce RoleplayPref, the first
large-scale preference dataset specifically for RPLAs, featuring 1,108
characters, 13 subcategories, and 16,888 bilingual dialogues, alongside
RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%
improvement over the conventional Bradley-Terry model in preference rankings.
Furthermore, applying ChARM-generated rewards to preference learning techniques
(e.g., direct preference optimization) achieves state-of-the-art results on
CharacterEval and RoleplayEval. Code and dataset are available at
https://github.com/calubkk/ChARM.

</details>


### [247] [Scaling up the think-aloud method](https://arxiv.org/abs/2505.23931)
*Daniel Wurgaft,Ben Prystawski,Kanishk Gandhi,Cedegao E. Zhang,Joshua B. Tenenbaum,Noah D. Goodman*

Main category: cs.CL

TL;DR: The paper presents methods to automate transcription and annotation of think-aloud data for large-scale analysis, demonstrating its value through a study on mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: To revive the think-aloud method in contemporary cognitive science by overcoming the limitation of labor-intensive transcription and annotation.

Method: Developed methods using natural language processing tools to automate transcription and annotation of verbal reports. Analyzed the data as search graphs.

Result: Achieved moderate inter-rater reliability with humans and characterized consistency and variation in human reasoning traces.

Conclusion: Demonstrates the value of think-aloud data at scale and serves as a proof of concept for automated analysis of verbal reports.

Abstract: The think-aloud method, where participants voice their thoughts as they solve
a task, is a valuable source of rich data about human reasoning processes. Yet,
it has declined in popularity in contemporary cognitive science, largely
because labor-intensive transcription and annotation preclude large sample
sizes. Here, we develop methods to automate the transcription and annotation of
verbal reports of reasoning using natural language processing tools, allowing
for large-scale analysis of think-aloud data. In our study, 640 participants
thought aloud while playing the Game of 24, a mathematical reasoning task. We
automatically transcribed the recordings and coded the transcripts as search
graphs, finding moderate inter-rater reliability with humans. We analyze these
graphs and characterize consistency and variation in human reasoning traces.
Our work demonstrates the value of think-aloud data at scale and serves as a
proof of concept for the automated analysis of verbal reports.

</details>


### [248] [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/abs/2505.23945)
*Sriram Balasubramanian,Samyadeep Basu,Soheil Feizi*

Main category: cs.CL

TL;DR: Chain-of-thought (CoT) reasoning was studied in large vision-language models (LVLMs) and language models (LLMs). A new evaluation pipeline was introduced to analyze bias articulation patterns. Findings revealed that image-based biases are less articulated than text-based ones, and a phenomenon called 'inconsistent' reasoning was identified.


<details>
  <summary>Details</summary>
Motivation: To investigate the faithfulness of chain-of-thought reasoning in large vision-language models (LVLMs) and understand how both text-based and image-based biases affect reasoning.

Method: Developed a novel fine-grained evaluation pipeline to categorize bias articulation patterns in LVLMs and LLMs, allowing for more precise analysis of CoT reasoning.

Result: Discovered that image-based biases are rarely articulated compared to text-based ones, and identified a new phenomenon termed 'inconsistent' reasoning where models correctly reason before suddenly changing answers.

Conclusion: Current reasoning models struggle with implicit cues that are not explicitly stated, highlighting challenges in achieving faithful CoT reasoning.

Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language
models, but questions remain about whether these reasoning traces faithfully
reflect the internal processes of the model. We present the first comprehensive
study of CoT faithfulness in large vision-language models (LVLMs),
investigating how both text-based and previously unexplored image-based biases
affect reasoning and bias articulation. Our work introduces a novel,
fine-grained evaluation pipeline for categorizing bias articulation patterns,
enabling significantly more precise analysis of CoT reasoning than previous
methods. This framework reveals critical distinctions in how models process and
respond to different types of biases, providing new insights into LVLM CoT
faithfulness. Our findings reveal that subtle image-based biases are rarely
articulated compared to explicit text-based ones, even in models specialized
for reasoning. Additionally, many models exhibit a previously unidentified
phenomenon we term ``inconsistent'' reasoning - correctly reasoning before
abruptly changing answers, serving as a potential canary for detecting biased
reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to
revisit CoT faithfulness in LLMs across various levels of implicit cues. Our
findings reveal that current language-only reasoning models continue to
struggle with articulating cues that are not overtly stated.

</details>


### [249] [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/abs/2505.23996)
*Yinong Oliver Wang,Nivedha Sivakumar,Falaah Arif Khan,Rin Metcalf Susa,Adam Golinski,Natalie Mackraz,Barry-John Theobald,Luca Zappella,Nicholas Apostoloff*

Main category: cs.CL

TL;DR: 提出了一种新的不确定性感知公平性度量UCerF，并创建了一个包含31,756个样本的新数据集，用于评估现代大型语言模型的性别-职业公平性。使用该度量和数据集对十个开源LLM进行基准测试，揭示了传统方法未能发现的模型偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）的快速普及强调了对其公平性进行基准测试的重要性。传统的公平性指标仅关注离散的准确性评估，无法反映模型在不同群体间的不确定性差异。

Method: 提出了不确定性感知公平性度量UCerF，构建了一个包含31,756个样本的新数据集用于共指解析的性别-职业公平性评估，并使用这些工具对十个开源LLM进行了基准测试。

Result: 通过UCerF度量发现了如Mistral-7B等模型在错误预测上表现出过高置信度的问题，这是传统Equalized Odds方法未能捕捉到的。

Conclusion: 所提出的LLM基准测试方法能够更全面地评估模型公平性，有助于开发更加透明和负责任的AI系统。

Abstract: The recent rapid adoption of large language models (LLMs) highlights the
critical need for benchmarking their fairness. Conventional fairness metrics,
which focus on discrete accuracy-based evaluations (i.e., prediction
correctness), fail to capture the implicit impact of model uncertainty (e.g.,
higher model confidence about one group over another despite similar accuracy).
To address this limitation, we propose an uncertainty-aware fairness metric,
UCerF, to enable a fine-grained evaluation of model fairness that is more
reflective of the internal bias in model decisions compared to conventional
fairness measures. Furthermore, observing data size, diversity, and clarity
issues in current datasets, we introduce a new gender-occupation fairness
evaluation dataset with 31,756 samples for co-reference resolution, offering a
more diverse and suitable dataset for evaluating modern LLMs. We establish a
benchmark, using our metric and dataset, and apply it to evaluate the behavior
of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness
due to high confidence in incorrect predictions, a detail overlooked by
Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark,
which evaluates fairness with uncertainty awareness, paves the way for
developing more transparent and accountable AI systems.

</details>


### [250] [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
*Hidetaka Kamigaito,Ying Zhang,Jingun Kwon,Katsuhiko Hayashi,Manabu Okumura,Taro Watanabe*

Main category: cs.CL

TL;DR: Transformers性能随参数规模增加而提升，但内部机制与参数缩放规律的关系尚不明确。本文通过偏差-多样性分解研究残差流中的层，发现增加层能提高性能是因为它们的行为不同（即多样性）。实验表明，增加层数的性能增益表现出次模性，边际改进随着层数增加而减少。


<details>
  <summary>Details</summary>
Motivation: 尽管已知Transformer性能可通过增大参数规模来改善，但其内部机制与参数缩放规律之间的关系仍不清楚。因此需要研究层和其规模如何影响性能。

Method: 通过偏差-多样性分解分析残差流中的层，将误差分为偏差（层输出与真实值的误差）和多样性（层输出之间的差异）。接着引入信息论多样性，并研究增加层对性能的影响。

Result: 发现当各层预测接近正确答案且保持相互多样性时性能更优；当层输出远离真实值时，多样性尤为重要。增加层仅在层行为不同时才增强性能，且性能增益表现出次模性。

Conclusion: 增加层的多样性是提升Transformer性能的关键，且性能增益随层数增加呈现递减趋势，与参数缩放规律一致。

Abstract: Transformers deliver outstanding performance across a wide range of tasks and
are now a dominant backbone architecture for large language models (LLMs).
Their task-solving performance is improved by increasing parameter size, as
shown in the recent studies on parameter scaling laws. Although recent
mechanistic-interpretability studies have deepened our understanding of the
internal behavior of Transformers by analyzing their residual stream, the
relationship between these internal mechanisms and the parameter scaling laws
remains unclear. To bridge this gap, we focus on layers and their size, which
mainly decide the parameter size of Transformers. For this purpose, we first
theoretically investigate the layers within the residual stream through a
bias-diversity decomposition. The decomposition separates (i) bias, the error
of each layer's output from the ground truth, and (ii) diversity, which
indicates how much the outputs of each layer differ from each other. Analyzing
Transformers under this theory reveals that performance improves when
individual layers make predictions close to the correct answer and remain
mutually diverse. We show that diversity becomes especially critical when
individual layers' outputs are far from the ground truth. Finally, we introduce
an information-theoretic diversity and show our main findings that adding
layers enhances performance only when those layers behave differently, i.e.,
are diverse. We also reveal the performance gains from increasing the number of
layers exhibit submodularity: marginal improvements diminish as additional
layers increase, mirroring the logarithmic convergence predicted by the
parameter scaling laws. Experiments on multiple semantic-understanding tasks
with various LLMs empirically confirm the theoretical properties derived in
this study.

</details>


### [251] [Large Language Model Meets Constraint Propagation](https://arxiv.org/abs/2505.24012)
*Alexandre Bonlarron,Florian Régin,Elisabetta De Maria,Jean-Charles Régin*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) lack the ability to enforce external constraints due to sequential token generation without explicit control mechanisms. GenCP resolves this by merging LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). This paper enhances GenCP through integrating Masked Language Models (MLMs) for domain generation, enabling bidirectional constraint propagation and bridging the gap between token-level prediction and structured constraint enforcement. Evaluation on COLLIE benchmarks shows that incorporating domain preview via MLM calls significantly boosts GenCP's performance, making LLM inferences more efficient and improving the generation of feasible and meaningful solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the limitation of Large Language Models (LLMs) in enforcing external constraints during text generation due to their sequential token generation process without explicit control mechanisms. This limitation necessitates a solution that can better handle structured constraints while maintaining fluent text generation.

Method: The method involves enhancing the GenCP system by integrating Masked Language Models (MLMs) for domain generation. This integration allows bidirectional constraint propagation, leveraging both past and future tokens. By formulating text generation as a Constraint Satisfaction Problem (CSP) and combining it with CP reasoning, the approach bridges the gap between token-level prediction and structured constraint enforcement.

Result: The evaluation on COLLIE benchmarks indicates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Despite additional MLM calls and increased backtracking in some cases, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints.

Conclusion: The conclusion is that the integration of Masked Language Models (MLMs) into the GenCP system leads to more reliable and constraint-aware text generation. This advancement makes LLM inferences more efficient and enhances the system's ability to generate feasible and meaningful solutions, especially in tasks requiring strict content constraints.

Abstract: Large Language Models (LLMs) excel at generating fluent text but struggle to
enforce external constraints because they generate tokens sequentially without
explicit control mechanisms. GenCP addresses this limitation by combining LLM
predictions with Constraint Programming (CP) reasoning, formulating text
generation as a Constraint Satisfaction Problem (CSP). In this paper, we
improve GenCP by integrating Masked Language Models (MLMs) for domain
generation, which allows bidirectional constraint propagation that leverages
both past and future tokens. This integration bridges the gap between
token-level prediction and structured constraint enforcement, leading to more
reliable and constraint-aware text generation. Our evaluation on COLLIE
benchmarks demonstrates that incorporating domain preview via MLM calls
significantly improves GenCP's performance. Although this approach incurs
additional MLM calls and, in some cases, increased backtracking, the overall
effect is a more efficient use of LLM inferences and an enhanced ability to
generate feasible and meaningful solutions, particularly in tasks with strict
content constraints.

</details>


### [252] [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/abs/2505.24040)
*Yuexing Hao,Kumail Alhamoud,Hyewon Jeong,Haoran Zhang,Isha Puri,Philip Torr,Mike Schaekermann,Ariel D. Stern,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: The paper introduces MedPAIR dataset to compare how physician trainees and LLMs prioritize information in medical QA, finding LLMs often misaligned with human relevance estimates. Filtering irrelevant sentences improves accuracy for both groups.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the prioritization of relevant information in medical question-answering between physician trainees and large language models.

Method: Creation of the MedPAIR dataset with annotations from 36 physician trainees on 1,300 QA pairs, assessing sentence relevance within questions and comparing these estimates to those of LLMs.

Result: LLMs show frequent misalignment with the relevance assessments of physician trainees. Removing irrelevant sentences as labeled by trainees enhances accuracy for both groups.

Conclusion: The study highlights the importance of content relevance in medical QA and provides a resource (MedPAIR dataset) for further research in aligning LLMs with human relevance judgments.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance on
various medical question-answering (QA) benchmarks, including standardized
medical exams. However, correct answers alone do not ensure correct logic, and
models may reach accurate conclusions through flawed processes. In this study,
we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance
Estimation and Question Answering) dataset to evaluate how physician trainees
and LLMs prioritize relevant information when answering QA questions. We obtain
annotations on 1,300 QA pairs from 36 physician trainees, labeling each
sentence within the question components for relevance. We compare these
relevance estimates to those for LLMs, and further evaluate the impact of these
"relevant" subsets on downstream task performance for both physician trainees
and LLMs. We find that LLMs are frequently not aligned with the content
relevance estimates of physician trainees. After filtering out physician
trainee-labeled irrelevant sentences, accuracy improves for both the trainees
and the LLMs. All LLM and physician trainee-labeled data are available at:
http://medpair.csail.mit.edu/.

</details>


### [253] [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/abs/2505.24133)
*Zefan Cai,Wen Xiao,Hanshi Sun,Cheng Luo,Yikai Zhang,Ke Wan,Yucheng Li,Yeyang Zhou,Li-Wen Chang,Jiuxiang Gu,Zhen Dong,Anima Anandkumar,Abedelkadir Asi,Junjie Hu*

Main category: cs.CL

TL;DR: The paper introduces Redundancy-aware KV Cache Compression for Reasoning models (R-KV), which reduces cache size while maintaining or improving performance in reasoning tasks, offering significant memory savings and throughput improvements.


<details>
  <summary>Details</summary>
Motivation: Reasoning models, despite their strong performance in complex tasks, suffer from excessive output length leading to large key-value (KV) caches. Existing KV cache compression methods cause reasoning failures when used with chain-of-thought inference.

Method: Proposes R-KV, a method that targets redundant tokens in reasoning models to compress KV caches effectively without losing performance.

Result: R-KV preserves nearly 100% of full KV cache performance using only 10% of the cache, achieves 105% performance with 16% cache, offers 90% memory saving, and 6.6X throughput compared to standard methods. It consistently outperforms existing baselines on two mathematical reasoning datasets.

Conclusion: R-KV provides an effective solution for reducing KV cache size in reasoning models without sacrificing performance, resulting in substantial memory savings and improved throughput.

Abstract: Reasoning models have demonstrated impressive performance in self-reflection
and chain-of-thought reasoning. However, they often produce excessively long
outputs, leading to prohibitively large key-value (KV) caches during inference.
While chain-of-thought inference significantly improves performance on complex
reasoning tasks, it can also lead to reasoning failures when deployed with
existing KV cache compression approaches. To address this, we propose
Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel
method specifically targeting redundant tokens in reasoning models. Our method
preserves nearly 100% of the full KV cache performance using only 10% of the KV
cache, substantially outperforming existing KV cache baselines, which reach
only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV
cache performance with 16% of the KV cache. This KV-cache reduction also leads
to a 90% memory saving and a 6.6X throughput over standard chain-of-thought
reasoning inference. Experimental results show that R-KV consistently
outperforms existing KV cache compression baselines across two mathematical
reasoning datasets.

</details>


### [254] [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/abs/2505.24163)
*Jiaqi Sun,Shiyou Qian,Zhangchi Han,Wei Li,Zelin Qian,Dingyu Yang,Jian Cao,Guangtao Xue*

Main category: cs.CL

TL;DR: The paper introduces LKD-KGC, an unsupervised framework for constructing domain-specific knowledge graphs (KGs) from document repositories. It autonomously infers knowledge dependencies, determines processing sequences, and generates entity schema without relying on predefined structures or external knowledge. Experiments show improvements of 10%-20% in precision and recall compared to state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods for knowledge graph construction (KGC) using large language models (LLMs) are constrained by manually defined schemas, single-document processing, and public-domain references, which limits their effectiveness for domain-specific corpora with complex knowledge dependencies and limited reference knowledge.

Method: LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema then guides the unsupervised extraction of entities and relationships.

Result: Extensive experiments demonstrate that LKD-KGC achieves improvements of 10% to 20% in both precision and recall rate compared to state-of-the-art baselines.

Conclusion: LKD-KGC shows potential in constructing high-quality domain-specific KGs without relying on predefined structures or external knowledge.

Abstract: Knowledge Graphs (KGs) structure real-world entities and their relationships
into triples, enhancing machine reasoning for various tasks. While
domain-specific KGs offer substantial benefits, their manual construction is
often inefficient and requires specialized knowledge. Recent approaches for
knowledge graph construction (KGC) based on large language models (LLMs), such
as schema-guided KGC and reference knowledge integration, have proven
efficient. However, these methods are constrained by their reliance on manually
defined schema, single-document processing, and public-domain references,
making them less effective for domain-specific corpora that exhibit complex
knowledge dependencies and specificity, as well as limited reference knowledge.
To address these challenges, we propose LKD-KGC, a novel framework for
unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes
document repositories to infer knowledge dependencies, determines optimal
processing sequences via LLM driven prioritization, and autoregressively
generates entity schema by integrating hierarchical inter-document contexts.
This schema guides the unsupervised extraction of entities and relationships,
eliminating reliance on predefined structures or external knowledge. Extensive
experiments show that compared with state-of-the-art baselines, LKD-KGC
generally achieves improvements of 10% to 20% in both precision and recall
rate, demonstrating its potential in constructing high-quality domain-specific
KGs.

</details>


### [255] [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)
*Neemesh Yadav,Palakorn Achananuparp,Jing Jiang,Ee-Peng Lim*

Main category: cs.CL

TL;DR: The study explores how Theory-of-Mind (ToM) reasoning in Large Language Models (LLMs) can align agentic behaviors with human norms during negotiation tasks, using the ultimatum game. LLM agents with different prosocial beliefs and ToM levels were initialized and tested across various models. Results from 2,700 simulations showed that ToM reasoning improves behavior alignment, decision-making consistency, and negotiation outcomes.


<details>
  <summary>Details</summary>
Motivation: To investigate the role of Theory-of-Mind (ToM) reasoning in aligning agentic behaviors with human norms in negotiation tasks, specifically using the ultimatum game as a controlled environment.

Method: LLM agents were initialized with different prosocial beliefs (Greedy, Fair, Selfless) and reasoning methods (chain-of-thought). They were examined across diverse LLMs including o3-mini, DeepSeek-R1 Distilled Qwen 32B. The study used 2,700 simulations to evaluate their decision-making processes.

Result: ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Reasoning models without ToM reasoning show limited capabilities compared to those with it.

Conclusion: This research contributes to understanding the role of ToM in enhancing human-AI interaction and cooperative decision-making.

Abstract: Large Language Models (LLMs) have shown potential in simulating human
behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for
complex social interactions. In this study, we investigate the role of ToM
reasoning in aligning agentic behaviors with human norms in negotiation tasks,
using the ultimatum game as a controlled environment. We initialized LLM agents
with different prosocial beliefs (including Greedy, Fair, and Selfless) and
reasoning methods like chain-of-thought (CoT) and varying ToM levels, and
examined their decision-making processes across diverse LLMs, including
reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from
2,700 simulations indicated that ToM reasoning enhances behavior alignment,
decision-making consistency, and negotiation outcomes. Consistent with previous
findings, reasoning models exhibit limited capability compared to models with
ToM reasoning, different roles of the game benefits with different orders of
ToM reasoning. Our findings contribute to the understanding of ToM's role in
enhancing human-AI interaction and cooperative decision-making. The code used
for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

</details>


### [256] [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/abs/2505.24264)
*Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas*

Main category: cs.CL

TL;DR: 通过研究策略提升自然语言推理中解释的有效性，包括减少语义损失、纠正语法错误、引导模型生成证明草图和提高对反馈的解读能力。实验结果表明这些策略在不同数据集上显著提升了自动形式化和解释改进的效果，并减少了验证所需的迭代次数。


<details>
  <summary>Details</summary>
Motivation: 自然语言解释在自然语言推理（NLI）中起着关键作用，然而当前方法在将自然语言转换为机器可验证的形式表示时存在语义信息丢失和不准确解释的问题，同时大型语言模型（LLMs）在捕捉逻辑结构和构建严谨证明方面仍有限制。

Method: 提出四种策略：1) 减少自动形式化过程中的语义损失；2) 有效识别和纠正逻辑表示中的语法错误；3) 使用逻辑表达式指导LLMs生成结构化的证明草图；4) 提高LLMs对TP反馈的解读能力以进行迭代改进。此外，还研究了混合LLM-TP架构中的特定干预措施以提高效率。

Result: 在e-SNLI、QASC和WorldTree数据集上的实证结果表明，所提出的策略在自动形式化方面分别提高了+18.46%、+34.2%、+39.77%，在解释改进方面分别提高了+29.5%、+51.5%、+41.25%。此外，特定干预措施显著提高了验证效率，大幅减少了成功验证所需的迭代次数。

Conclusion: 本文提出的策略有效地缓解了自动形式化过程中的语义损失问题，提高了LLMs在生成结构化证明草图和解读TP反馈方面的能力，从而显著改进了NLI解释的质量和验证效率。

Abstract: Natural language explanations play a fundamental role in Natural Language
Inference (NLI) by revealing how premises logically entail hypotheses. Recent
work has shown that the interaction of large language models (LLMs) with
theorem provers (TPs) can help verify and improve the validity of NLI
explanations. However, TPs require translating natural language into
machine-verifiable formal representations, a process that introduces the risk
of semantic information loss and unfaithful interpretation, an issue compounded
by LLMs' challenges in capturing critical logical structures with sufficient
precision. Moreover, LLMs are still limited in their capacity for rigorous and
robust proof construction within formal verification frameworks. To mitigate
issues related to faithfulness and robustness, this paper investigates
strategies to (1) alleviate semantic loss during autoformalisation, (2)
efficiently identify and correct syntactic errors in logical representations,
(3) explicitly use logical expressions to guide LLMs in generating structured
proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback
for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree
using different LLMs demonstrate that the proposed strategies yield significant
improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation
refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,
we show that specific interventions on the hybrid LLM-TP architecture can
substantially improve efficiency, drastically reducing the number of iterations
required for successful verification.

</details>


### [257] [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/abs/2505.24341)
*Shujian Yang,Shiyao Cui,Chuanrui Hu,Haicheng Wang,Tianwei Zhang,Minlie Huang,Jialiang Lu,Han Qiu*

Main category: cs.CL

TL;DR: Detecting toxic Chinese content via LLMs is challenging due to multimodal nature and character perturbations. A taxonomy of 3 strategies/8 approaches, a curated dataset, and benchmarking of 9 SOTA LLMs reveal that these models struggle with perturbed toxic content and may 'overcorrect' when using ICL or SFT.


<details>
  <summary>Details</summary>
Motivation: Toxic content detection in Chinese language poses unique challenges for LLMs due to the multimodal characteristics of Chinese and the ease with which simple character substitutions can confuse models.

Method: Proposed a taxonomy of perturbation strategies/approaches for toxic Chinese content; curated a dataset based on this taxonomy; benchmarked 9 SOTA LLMs for detecting perturbed toxic Chinese text; explored enhancement solutions like ICL and SFT.

Result: LLMs show weakness in detecting perturbed multimodal Chinese toxic contents. Enhancement methods such as ICL or SFT with few perturbed examples can cause 'overcorrection', misidentifying normal contents as toxic.

Conclusion: Detection of perturbed toxic Chinese content by LLMs needs improvement. Caution should be exercised with ICL and SFT approaches to avoid overcorrection.

Abstract: Detecting toxic content using language models is important but challenging.
While large language models (LLMs) have demonstrated strong performance in
understanding Chinese, recent studies show that simple character substitutions
in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In
this paper, we highlight the multimodal nature of Chinese language as a key
challenge for deploying LLMs in toxic Chinese detection. First, we propose a
taxonomy of 3 perturbation strategies and 8 specific approaches in toxic
Chinese content. Then, we curate a dataset based on this taxonomy, and
benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect
perturbed toxic Chinese text. Additionally, we explore cost-effective
enhancement solutions like in-context learning (ICL) and supervised fine-tuning
(SFT). Our results reveal two important findings. (1) LLMs are less capable of
detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a
small number of perturbed examples may cause the LLMs "overcorrect'':
misidentify many normal Chinese contents as toxic.

</details>


### [258] [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/abs/2505.23789)
*Mingyu Huang,Shasha Zhou,Yuxuan Chen,Ke Li*

Main category: cs.CL

TL;DR: In the era of 'big literature', where digital scientific publications are exponentially increasing, traditional manual reviewing is no longer feasible. Recent LLMs have shown strong capabilities for literature comprehension but cannot offer systematic reviews due to limited context windows and trust issues. This paper presents LitChat, an interactive literature agent that augments LLM agents with data-driven discovery tools, automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques. A case study on AI4Health illustrates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the exponential growth in digital scientific publications and the limitations of current LLMs in providing comprehensive, objective, open, and transparent views desired by systematic reviews.

Method: LitChat is an end-to-end, interactive, and conversational literature agent that augments LLM agents with data-driven discovery tools. It automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs.

Result: A case study on AI4Health demonstrates the effectiveness of LitChat in quickly navigating users through a large-scale literature landscape with data-based evidence, which would be infeasible with traditional means.

Conclusion: LitChat offers a novel solution to navigate through the vast landscape of scientific literature by combining the strengths of LLMs with data-driven discovery tools, enabling more efficient and insightful literature exploration.

Abstract: We are living in an era of "big literature", where the volume of digital
scientific publications is growing exponentially. While offering new
opportunities, this also poses challenges for understanding literature
landscapes, as traditional manual reviewing is no longer feasible. Recent large
language models (LLMs) have shown strong capabilities for literature
comprehension, yet they are incapable of offering "comprehensive, objective,
open and transparent" views desired by systematic reviews due to their limited
context windows and trust issues like hallucinations. Here we present LitChat,
an end-to-end, interactive and conversational literature agent that augments
LLM agents with data-driven discovery tools to facilitate literature
exploration. LitChat automatically interprets user queries, retrieves relevant
sources, constructs knowledge graphs, and employs diverse data-mining
techniques to generate evidence-based insights addressing user needs. We
illustrate the effectiveness of LitChat via a case study on AI4Health,
highlighting its capacity to quickly navigate the users through large-scale
literature landscape with data-based evidence that is otherwise infeasible with
traditional means.

</details>


### [259] [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/abs/2505.24409)
*Eojin Kang,Juae Kim*

Main category: cs.CL

TL;DR: Multilingual LLMs have inconsistent factual knowledge recall depending on the input language. This paper explores non-English to English transfer via Language and Thought Theory, proposes the Language-to-Thought (L2T) prompting strategy, and shows that applying L2T during training can facilitate cross-linguistic knowledge integration without translation-based learning.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency of factual knowledge recall in multilingual LLMs based on the input language and explore non-English to English transfer via Language and Thought Theory.

Method: Propose the Language-to-Thought (L2T) prompting strategy which analyzes the relationship between input language, internal cognitive processes, and knowledge. Apply L2T during training to alleviate LLMs' reliance on the input language.

Result: Experimental results challenge the assumption that English-based approaches consistently outperform other languages and show that aligning the model's internal thought with the knowledge required for the task is critical for successful cross-lingual transfer.

Conclusion: Applying L2T during training can alleviate LLMs' reliance on the input language and facilitate cross-linguistic knowledge integration without translation-based learning.

Abstract: Multilingual large language models (LLMs) open up new possibilities for
leveraging information across languages, but their factual knowledge recall
remains inconsistent depending on the input language. While previous studies
have attempted to address this issue through English-based prompting and
evaluation, we explore non-English to English transfer via Language and Thought
Theory. This perspective allows us to examine language-thought binding in LLMs
and uncover why factual knowledge often fails to transfer effectively. We
propose the Language-to-Thought (L2T) prompting strategy, which analyzes the
relationship between input language, internal cognitive processes, and
knowledge. Experimental results challenge the assumption that English-based
approaches consistently outperform other languages and offer a novel insight
that aligning the model's internal thought with the knowledge required for the
task is critical for successful cross-lingual transfer. Furthermore, we show
that applying L2T during training can alleviate LLMs' reliance on the input
language and facilitate cross-linguistic knowledge integration without
translation-based learning. Code and datasets will be available.

</details>


### [260] [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/abs/2505.23815)
*Stéphane Aroca-Ouellette,Natalie Mackraz,Barry-John Theobald,Katherine Metcalf*

Main category: cs.CL

TL;DR: The paper presents PROSE, a method for enhancing the precision of inferred human preferences in LLM writing agents. It uses iterative refinement and verification across multiple user samples. Evaluated with several LLMs on summarization and email writing tasks, PROSE outperforms CIPHER by 33%. Combining PROSE with ICL offers up to 9% improvement over ICL alone.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inferring human preferences often produce generic descriptions that fail to capture the unique nature of individual preferences.

Method: PROSE incorporates two key elements: (1) iterative refinement of inferred preferences, and (2) verification of inferred preferences across multiple user writing samples.

Result: PROSE more accurately infers nuanced human preferences, improving the quality of the writing agent's generations over CIPHER by 33%. Combining PROSE with ICL provides up to a 9% improvement over ICL alone.

Conclusion: PROSE enhances the precision of preference descriptions inferred from user writing samples, leading to improved performance in personalized interactions.

Abstract: Accommodating human preferences is essential for creating aligned LLM agents
that deliver personalized and effective interactions. Recent work has shown the
potential for LLMs acting as writing agents to infer a description of user
preferences. Agent alignment then comes from conditioning on the inferred
preference description. However, existing methods often produce generic
preference descriptions that fail to capture the unique and individualized
nature of human preferences. This paper introduces PROSE, a method designed to
enhance the precision of preference descriptions inferred from user writing
samples. PROSE incorporates two key elements: (1) iterative refinement of
inferred preferences, and (2) verification of inferred preferences across
multiple user writing samples. We evaluate PROSE with several LLMs (i.e.,
Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an
email writing task. We find that PROSE more accurately infers nuanced human
preferences, improving the quality of the writing agent's generations over
CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly,
we demonstrate that ICL and PROSE are complementary methods, and combining them
provides up to a 9\% improvement over ICL alone.

</details>


### [261] [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)
*Trenton Chang,Tobias Schnabel,Adith Swaminathan,Jenna Wiens*

Main category: cs.CL

TL;DR: 尽管大语言模型（LLMs）在推理和指令跟随基准上取得了进展，但它们是否能可靠地产生与广泛用户目标一致的输出仍不清楚。本文提出了一个基于多维目标空间的框架，以系统评估LLMs在文本重写任务中的可控性问题，并发现现有策略可能不足以解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种基准测试中表现出色，但其输出是否能可靠地与广泛的用户目标对齐尚不明确，即存在所谓的“可控性”问题。此外，多种方法被提出以修改LLM行为，但不确定这些方法是否足以提升LLM的可控性。

Method: 作者引入了一个基于多维目标空间的框架，将用户目标和LLM输出建模为向量，向量的维度对应于文本属性（如阅读难度）。该框架应用于文本重写任务，用以系统评估LLM在可控性方面的失败模式，包括覆盖不足、校准不当和副作用等问题。

Result: 研究发现当前LLM在可控性方面存在困难，尤其是副作用问题持续存在。通过干预措施（如提示工程、最佳N采样和强化学习微调）虽然可以部分改善可控性，但副作用仍然难以完全消除。

Conclusion: 即使是最强大的LLM，在可控性方面也面临挑战，现有的对齐策略可能不足以解决这些问题。因此，作者开源了他们的可控性评估框架，以便进一步研究和改进。

Abstract: Despite advances in large language models (LLMs) on reasoning and
instruction-following benchmarks, it remains unclear whether they can reliably
produce outputs aligned with a broad variety of user goals, a concept we refer
to as steerability. The abundance of methods proposed to modify LLM behavior
makes it unclear whether current LLMs are already steerable, or require further
intervention. In particular, LLMs may exhibit (i) poor coverage, where rare
user goals are underrepresented; (ii) miscalibration, where models overshoot
requests; and (iii) side effects, where changes to one dimension of text
inadvertently affect others. To systematically evaluate these failures, we
introduce a framework based on a multi-dimensional goal space that models user
goals and LLM outputs as vectors with dimensions corresponding to text
attributes (e.g., reading difficulty). Applied to a text-rewriting task, we
find that current LLMs struggle with steerability, as side effects are
persistent. Interventions to improve steerability, such as prompt engineering,
best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying
effectiveness, yet side effects remain problematic. Our findings suggest that
even strong LLMs struggle with steerability, and existing alignment strategies
may be insufficient. We open-source our steerability evaluation framework at
https://github.com/MLD3/steerability.

</details>


### [262] [VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation](https://arxiv.org/abs/2505.24472)
*Hieu Tran,Phuong-Anh Nguyen-Le,Huy Nghiem,Quang-Nhan Nguyen,Wei Ai,Marine Carpuat*

Main category: cs.CL

TL;DR: This paper presents VietMix, a parallel corpus of code-mixed Vietnamese-English text, and a synthetic data generation pipeline to improve machine translation for low-resource languages. The methods significantly boost translation quality scores and are preferred in LLM-based assessments.


<details>
  <summary>Details</summary>
Motivation: Machine translation systems struggle with code-mixed inputs for low-resource languages, leading to the need for a specialized corpus and data generation method to address this challenge.

Method: The researchers created VietMix, a parallel corpus of naturally occurring code-mixed Vietnamese text with expert English translations. They also developed a synthetic data generation pipeline with filtering mechanisms to ensure syntactic plausibility and pragmatic appropriateness in code-mixing patterns.

Result: Experimental validation shows that naturalistic and synthetic data improved models' performance, achieving scores of up to 71.84 on COMETkiwi and 81.77 on XCOMET. In LLM-based assessments, augmented models were favored over seed fine-tuned counterparts in about 49% of judgments.

Conclusion: VietMix and the augmentation methodology enhance ecological validity in neural MT evaluations and provide a framework for addressing code-mixed translation challenges across other low-resource language pairs.

Abstract: Machine translation systems fail when processing code-mixed inputs for
low-resource languages. We address this challenge by curating VietMix, a
parallel corpus of naturally occurring code-mixed Vietnamese text paired with
expert English translations. Augmenting this resource, we developed a
complementary synthetic data generation pipeline. This pipeline incorporates
filtering mechanisms to ensure syntactic plausibility and pragmatic
appropriateness in code-mixing patterns. Experimental validation shows our
naturalistic and complementary synthetic data boost models' performance,
measured by translation quality estimation scores, of up to 71.84 on COMETkiwi
and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,
augmented models are favored over seed fine-tuned counterparts in approximately
49% of judgments (54-56% excluding ties). VietMix and our augmentation
methodology advance ecological validity in neural MT evaluations and establish
a framework for addressing code-mixed translation challenges across other
low-resource pairs.

</details>


### [263] [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843)
*Wenhan Dong,Tianyi Hu,Jingyi Zheng,Zhen Sun,Yuemeng Zhao,Yule Liu,Xinlei He,Xinyi Huang*

Main category: cs.CL

TL;DR: 当前评估大语言模型侧向思维能力的方法存在局限性，如产生误导结果、无法揭示关键问题等。为此，本文提出了一套改进的评估标准，包括检查推理路径、多样化评估指标和与人类表现的比较分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有评估方法在多轮次不完全信息任务中对大语言模型侧向思维能力评估时出现的误导结果和隐藏关键问题的不足。

Method: 提出了一套新的评估标准，包括检查推理路径、采用多样化的评估指标以及与人类表现进行比较分析。

Result: 新标准能够更准确地揭示大语言模型的真实推理能力，并提高评估的可靠性。

Conclusion: 改进后的评估标准为更全面和可靠地评估大语言模型的侧向思维能力提供了新途径。

Abstract: Multi-round incomplete information tasks are crucial for evaluating the
lateral thinking capabilities of large language models (LLMs). Currently,
research primarily relies on multiple benchmarks and automated evaluation
metrics to assess these abilities. However, our study reveals novel insights
into the limitations of existing methods, as they often yield misleading
results that fail to uncover key issues, such as shortcut-taking behaviors,
rigid patterns, and premature task termination. These issues obscure the true
reasoning capabilities of LLMs and undermine the reliability of evaluations. To
address these limitations, we propose a refined set of evaluation standards,
including inspection of reasoning paths, diversified assessment metrics, and
comparative analyses with human performance.

</details>


### [264] [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/abs/2505.23848)
*Harvey Dam,Jonas Knochelmann,Vinu Joseph,Ganesh Gopalakrishnan*

Main category: cs.CL

TL;DR: A method to reduce refusal rates of LLMs on sensitive content is introduced, which makes simple formatting adjustments during generation without modifying model weights or prompts.


<details>
  <summary>Details</summary>
Motivation: The motivation is based on the observation that refusals in certain models were often preceded by specific token sequences marking the beginning of the chain-of-thought (CoT) block (<think>) followed by a double newline token (

).

Method: The method involves two simple formatting adjustments: suppressing \n\n after <think> and suppressing the end-of-sequence token after the end of the CoT block (</think>). This relies solely on modifying token probabilities during generation.

Result: Experiments with official DeepSeek-R1 distillations showed that these interventions increased the proportion of substantive answers to sensitive prompts without affecting performance on standard benchmarks.

Conclusion: Refusal behaviors can be circumvented by blocking refusal subspaces at specific points in the generation process.

Abstract: We introduce a method to reduce refusal rates of large language models (LLMs)
on sensitive content without modifying model weights or prompts. Motivated by
the observation that refusals in certain models were often preceded by the
specific token sequence of a token marking the beginning of the
chain-of-thought (CoT) block (<think>) followed by a double newline token
(\n\n), we investigate the impact of two simple formatting adjustments during
generation: suppressing \n\n after <think> and suppressing the end-of-sequence
token after the end of the CoT block (</think>). Our method requires no
datasets, parameter changes, or training, relying solely on modifying token
probabilities during generation. In our experiments with official DeepSeek-R1
distillations, these interventions increased the proportion of substantive
answers to sensitive prompts without affecting performance on standard
benchmarks. Our findings suggest that refusal behaviors can be circumvented by
blocking refusal subspaces at specific points in the generation process.

</details>


### [265] [Towards Effective Code-Integrated Reasoning](https://arxiv.org/abs/2505.24480)
*Fei Bai,Yingqian Min,Beichen Zhang,Zhipeng Chen,Wayne Xin Zhao,Lei Fang,Zheng Liu,Zhongyuan Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本研究探讨了代码集成推理，提出了一种系统方法来提高工具增强型强化学习在代码集成推理中的训练效果和稳定性。实验表明模型性能显著优于多个竞争基线，并揭示了代码集成推理的若干关键见解。


<details>
  <summary>Details</summary>
Motivation: 代码集成推理需要模型在必要时生成代码并通过代码解释器整合反馈，但工具增强型强化学习在学习动态中可能存在潜在的不稳定性。

Method: 开发了增强的训练策略，平衡探索与稳定性，逐步构建工具使用能力并提高推理性能。

Result: 通过五个主流数学推理基准的广泛实验，模型表现出显著的性能提升。深入分析显示，代码集成推理扩展了模型的能力边界，同时提高了推理效率。

Conclusion: 本研究提供了一种系统方法来改进工具增强型强化学习的训练效果和稳定性，展示了代码集成推理的潜力。

Abstract: In this paper, we investigate code-integrated reasoning, where models
generate code when necessary and integrate feedback by executing it through a
code interpreter. To acquire this capability, models must learn when and how to
use external code tools effectively, which is supported by tool-augmented
reinforcement learning (RL) through interactive learning. Despite its benefits,
tool-augmented RL can still suffer from potential instability in the learning
dynamics. In light of this challenge, we present a systematic approach to
improving the training effectiveness and stability of tool-augmented RL for
code-integrated reasoning. Specifically, we develop enhanced training
strategies that balance exploration and stability, progressively building
tool-use capabilities while improving reasoning performance. Through extensive
experiments on five mainstream mathematical reasoning benchmarks, our model
demonstrates significant performance improvements over multiple competitive
baselines. Furthermore, we conduct an in-depth analysis of the mechanism and
effect of code-integrated reasoning, revealing several key insights, such as
the extension of model's capability boundaries and the simultaneous improvement
of reasoning efficiency through code integration. All data and code for
reproducing this work are available at: https://github.com/RUCAIBox/CIR.

</details>


### [266] [TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence](https://arxiv.org/abs/2505.24500)
*Guiyang Hou,Xing Gao,Yuchuan Wu,Xiang Huang,Wenqi Zhang,Zhe Zheng,Yongliang Shen,Jialu Du,Fei Huang,Yongbin Li,Weiming Lu*

Main category: cs.CL

TL;DR: This paper presents TimeHC-RL, a novel method to enhance LLMs' social intelligence by recognizing the distinct timeline and cognitive modes in social domains. Experiments show its superiority over System 2 RL method and enable a 7B model to match advanced models.


<details>
  <summary>Details</summary>
Motivation: The motivation is the underexplored enhancement of LLMs' cognitive development in social domains from a post-training perspective, acknowledging the unique requirements of social cognition compared to mathematics or coding.

Method: The method introduced is Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL), designed to improve LLMs' social intelligence.

Result: Experimental results demonstrate that TimeHC-RL outperforms the widely adopted System 2 RL method and allows a 7B model to rival advanced models like DeepSeek-R1 and OpenAI-O3.

Conclusion: The conclusion drawn is that TimeHC-RL effectively enhances LLMs' social intelligence, and the exploration has uncovered valuable insights for improving LLMs through post-training and test-time interventions.

Abstract: Recently, Large Language Models (LLMs) have made significant progress in
IQ-related domains that require careful thinking, such as mathematics and
coding. However, enhancing LLMs' cognitive development in social domains,
particularly from a post-training perspective, remains underexplored.
Recognizing that the social world follows a distinct timeline and requires a
richer blend of cognitive modes (from intuitive reactions (System 1) and
surface-level thinking to deliberate thinking (System 2)) than mathematics,
which primarily relies on System 2 cognition (careful, step-by-step reasoning),
we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning
(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we
systematically explore improving LLMs' social intelligence and validate the
effectiveness of the TimeHC-RL method, through five other post-training
paradigms and two test-time intervention paradigms on eight datasets with
diverse data patterns. Experimental results reveal the superiority of our
proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It
gives the 7B backbone model wings, enabling it to rival the performance of
advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic
exploration from post-training and test-time interventions perspectives to
improve LLMs' social intelligence has uncovered several valuable insights.

</details>


### [267] [Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors](https://arxiv.org/abs/2505.24523)
*Andrea Pedrotti,Michele Papucci,Cristiano Ciaccio,Alessio Miaschi,Giovanni Puccetti,Felice Dell'Orletta,Andrea Esuli*

Main category: cs.CL

TL;DR: Recent advancements in Generative AI and LLMs have enabled the creation of highly realistic synthetic content, raising concerns about malicious use. Detecting MGT remains challenging due to lack of robust benchmarks. This work presents a pipeline to test resilience of MGT detectors to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in detecting machine-generated text and highlight vulnerabilities in current detection methods.

Method: Fine-tune language models using Direct Preference Optimization (DPO) to shift MGT style toward human-written text, making it harder for detectors to distinguish. Analyze linguistic shifts and features used by detectors.

Result: Detectors can be easily fooled with relatively few examples, leading to a significant drop in detection performance.

Conclusion: Improving detection methods and making them robust to unseen in-domain texts is crucial.

Abstract: Recent advancements in Generative AI and Large Language Models (LLMs) have
enabled the creation of highly realistic synthetic content, raising concerns
about the potential for malicious use, such as misinformation and manipulation.
Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the
lack of robust benchmarks that assess generalization to real-world scenarios.
In this work, we present a pipeline to test the resilience of state-of-the-art
MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed
adversarial attacks. To challenge the detectors, we fine-tune language models
using Direct Preference Optimization (DPO) to shift the MGT style toward
human-written text (HWT). This exploits the detectors' reliance on stylistic
clues, making new generations more challenging to detect. Additionally, we
analyze the linguistic shifts induced by the alignment and which features are
used by detectors to detect MGT texts. Our results show that detectors can be
easily fooled with relatively few examples, resulting in a significant drop in
detection performance. This highlights the importance of improving detection
methods and making them robust to unseen in-domain texts.

</details>


### [268] [Localizing Persona Representations in LLMs](https://arxiv.org/abs/2505.24539)
*Celia Cintas,Miriam Rateike,Erik Miehling,Elizabeth Daly,Skyler Speakman*

Main category: cs.CL

TL;DR: 研究人员研究了大型语言模型中不同人格特征的编码方式，发现模型最后一部分解码层中存在显著差异。伦理观点可能存在多义性，而政治意识形态则相对明确。


<details>
  <summary>Details</summary>
Motivation: 了解大型语言模型如何编码人类特性、价值观和信仰，有助于优化模型对特定人类特质的表达。

Method: 使用降维和模式识别方法分析多个预训练的仅解码器大型语言模型的各层激活情况，比较不同人格特征在表示空间中的编码。

Result: 在多个预训练的仅解码器大型语言模型中，不同人格特征的表示空间差异主要集中在解码器的最后一部分。道德虚无主义和功利主义等伦理观点表现出重叠的激活模式，而保守主义和自由主义等政治意识形态则分布在更独立的区域。

Conclusion: 这些结果增进了我们对大型语言模型内部信息表示的理解，并为未来改进模型对特定人类特质的调节提供了指导。

Abstract: We present a study on how and where personas -- defined by distinct sets of
human characteristics, values, and beliefs -- are encoded in the representation
space of large language models (LLMs). Using a range of dimension reduction and
pattern recognition methods, we first identify the model layers that show the
greatest divergence in encoding these representations. We then analyze the
activations within a selected layer to examine how specific personas are
encoded relative to others, including their shared and distinct embedding
spaces. We find that, across multiple pre-trained decoder-only LLMs, the
analyzed personas show large differences in representation space only within
the final third of the decoder layers. We observe overlapping activations for
specific ethical perspectives -- such as moral nihilism and utilitarianism --
suggesting a degree of polysemy. In contrast, political ideologies like
conservatism and liberalism appear to be represented in more distinct regions.
These findings help to improve our understanding of how LLMs internally
represent information and can inform future efforts in refining the modulation
of specific human traits in LLM outputs. Warning: This paper includes
potentially offensive sample statements.

</details>


### [269] [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)
*Wei Zhong,Manasa Bharadwaj,Yixiao Wang,Nikhil Verma,Yipeng Ji,Chul Lee*

Main category: cs.CL

TL;DR: The paper introduces Budget EAGLE (Beagle), a cross-attention-based Transformer decoder for speculative decoding (SD) in large language models. It simplifies the architecture, improves training efficiency, and maintains stable memory usage. A new training method, Two-Stage Block-Attention Training, is proposed to ensure stability and convergence efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and lack of generalization in state-of-the-art speculative decoding methods that rely on tightly coupled, self-attention-based Transformer decoders.

Method: Introduced Beagle, a cross-attention-based Transformer decoder SD model, and Two-Stage Block-Attention Training, a novel training method for block-level attention scenarios.

Result: Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2 across multiple LLMs and datasets.

Conclusion: Beagle offers a strong alternative for architectures in speculative decoding with its simplified architecture and improved training efficiency.

Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating
inference in large language models (LLMs), particularly when the draft and
target models are well aligned. However, state-of-the-art SD methods typically
rely on tightly coupled, self-attention-based Transformer decoders, often
augmented with auxiliary pooling or fusion layers. This coupling makes them
increasingly complex and harder to generalize across different models. We
present Budget EAGLE (Beagle), the first, to our knowledge,
cross-attention-based Transformer decoder SD model that achieves performance on
par with leading self-attention SD models (EAGLE-v2) while eliminating the need
for pooling or auxiliary components, simplifying the architecture, improving
training efficiency, and maintaining stable memory usage during training-time
simulation. To enable effective training of this novel architecture, we propose
Two-Stage Block-Attention Training, a new method that achieves training
stability and convergence efficiency in block-level attention scenarios.
Extensive experiments across multiple LLMs and datasets show that Beagle
achieves competitive inference speedups and higher training efficiency than
EAGLE-v2, offering a strong alternative for architectures in speculative
decoding.

</details>


### [270] [CREFT: Sequential Multi-Agent LLM for Character Relation Extraction](https://arxiv.org/abs/2505.24553)
*Ye Eun Chun,Taeyoon Hwang,Seung-won Hwang,Byung-Hak Kim*

Main category: cs.CL

TL;DR: CREFT is a new framework using specialized LLM agents to analyze complex character relations in narratives, outperforming single-agent models and aiding script evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with extracting character relations in long-form narratives with subtle interactions.

Method: CREFT builds a base character graph via knowledge distillation then refines character composition, relation extraction, role identification, and group assignments iteratively.

Result: Experiments on Korean drama dataset show CREFT surpasses single-agent LLMs in accuracy and completeness.

Conclusion: CREFT enhances narrative understanding and accelerates script review, benefiting entertainment, publishing, and education sectors.

Abstract: Understanding complex character relations is crucial for narrative analysis
and efficient script evaluation, yet existing extraction methods often fail to
handle long-form narratives with nuanced interactions. To address this
challenge, we present CREFT, a novel sequential framework leveraging
specialized Large Language Model (LLM) agents. First, CREFT builds a base
character graph through knowledge distillation, then iteratively refines
character composition, relation extraction, role identification, and group
assignments. Experiments on a curated Korean drama dataset demonstrate that
CREFT significantly outperforms single-agent LLM baselines in both accuracy and
completeness. By systematically visualizing character networks, CREFT
streamlines narrative comprehension and accelerates script review -- offering
substantial benefits to the entertainment, publishing, and educational sectors.

</details>


### [271] [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)
*Anna Sofia Lippolis,Minh Davide Ragagni,Paolo Ciancarini,Andrea Giovanni Nuzzolese,Valentina Presutti*

Main category: cs.CL

TL;DR: The paper introduces Bench4KE, an API-based benchmarking system for Knowledge Engineering automation, focusing on evaluating tools that automatically generate Competency Questions (CQs). It provides a gold standard dataset and uses similarity metrics to assess CQ quality. A comparative analysis of four LLM-based CQ generation systems is presented, establishing a baseline for future research.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the lack of standardization in evaluating tools for automatic generation of Competency Questions (CQs), which undermines methodological rigour and hinders replication and comparison of results.

Method: The authors introduce Bench4KE, an extensible API-based benchmarking system for Knowledge Engineering automation. It includes a curated gold standard of CQ datasets from real-world ontology projects and uses various similarity metrics to evaluate CQ quality.

Result: A comparative analysis of four recent CQ generation systems based on LLMs was conducted, providing insights into their performance and establishing a baseline for future research.

Conclusion: Bench4KE addresses the gap in standardizing the evaluation of KE automation tools, particularly those generating CQs. The system is designed to be extensible to other KE automation tasks, and its resources are publicly available.

Abstract: The availability of Large Language Models (LLMs) presents a unique
opportunity to reinvigorate research on Knowledge Engineering (KE) automation,
a trend already evident in recent efforts developing LLM-based methods and
tools for the automatic generation of Competency Questions (CQs). However, the
evaluation of these tools lacks standardisation. This undermines the
methodological rigour and hinders the replication and comparison of results. To
address this gap, we introduce Bench4KE, an extensible API-based benchmarking
system for KE automation. Its first release focuses on evaluating tools that
generate CQs automatically. CQs are natural language questions used by ontology
engineers to define the functional requirements of an ontology. Bench4KE
provides a curated gold standard consisting of CQ datasets from four real-world
ontology projects. It uses a suite of similarity metrics to assess the quality
of the CQs generated. We present a comparative analysis of four recent CQ
generation systems, which are based on LLMs, establishing a baseline for future
research. Bench4KE is also designed to accommodate additional KE automation
tasks, such as SPARQL query generation, ontology testing and drafting. Code and
datasets are publicly available under the Apache 2.0 license.

</details>


### [272] [NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization](https://arxiv.org/abs/2505.24575)
*Hyuntak Kim,Byung-Hak Kim*

Main category: cs.CL

TL;DR: NexusSum is a multi-agent LLM framework for narrative summarization which introduces Dialogue-to-Description Transformation and Hierarchical Multi-LLM Summarization, achieving up to 30.0% improvement in BERTScore (F1).


<details>
  <summary>Details</summary>
Motivation: Existing LLMs face challenges in summarizing long-form narratives due to intricate plotlines, character interactions, and thematic coherence.

Method: The method includes two key innovations: Dialogue-to-Description Transformation that standardizes character dialogue and descriptive text into a unified format; and Hierarchical Multi-LLM Summarization that optimizes chunk processing and controls output length.

Result: This method establishes a new state-of-the-art in narrative summarization with up to 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts.

Conclusion: NexusSum demonstrates the effectiveness of multi-agent LLMs in handling long-form content and offers a scalable approach for structured summarization in diverse storytelling domains.

Abstract: Summarizing long-form narratives--such as books, movies, and TV
scripts--requires capturing intricate plotlines, character interactions, and
thematic coherence, a task that remains challenging for existing LLMs. We
introduce NexusSum, a multi-agent LLM framework for narrative summarization
that processes long-form text through a structured, sequential
pipeline--without requiring fine-tuning. Our approach introduces two key
innovations: (1) Dialogue-to-Description Transformation: A narrative-specific
preprocessing method that standardizes character dialogue and descriptive text
into a unified format, improving coherence. (2) Hierarchical Multi-LLM
Summarization: A structured summarization pipeline that optimizes chunk
processing and controls output length for accurate, high-quality summaries. Our
method establishes a new state-of-the-art in narrative summarization, achieving
up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV
scripts. These results demonstrate the effectiveness of multi-agent LLMs in
handling long-form content, offering a scalable approach for structured
summarization in diverse storytelling domains.

</details>


### [273] [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2505.24016)
*Matthew Raffel,Victor Agostinelli,Lizhong Chen*

Main category: cs.CL

TL;DR: This paper discusses the construction, fine-tuning, and deployment of BeaverTalk, a cascaded system for speech-to-text translation.


<details>
  <summary>Details</summary>
Motivation: To develop an effective system for simultaneous speech-to-text translation in multiple language directions.

Method: The system architecture employs a VAD segmenter for breaking a speech stream into segments, Whisper Large V2 for automatic speech recognition (ASR), and Gemma 3 12B for simultaneous translation. Fine-tuned via low-rank adaptors (LoRAs) for a conversational prompting strategy.

Result: On the English→German task, BLEU scores are 24.64 and 27.83 at different StreamLAALs. On the English→Chinese task, BLEU scores are 34.07 and 37.23 at different StreamLAALs.

Conclusion: BeaverTalk is a successfully constructed, fine-tuned, and deployed cascaded system for speech-to-text translation.

Abstract: This paper discusses the construction, fine-tuning, and deployment of
BeaverTalk, a cascaded system for speech-to-text translation as part of the
IWSLT 2025 simultaneous translation task. The system architecture employs a VAD
segmenter for breaking a speech stream into segments, Whisper Large V2 for
automatic speech recognition (ASR), and Gemma 3 12B for simultaneous
translation. Regarding the simultaneous translation LLM, it is fine-tuned via
low-rank adaptors (LoRAs) for a conversational prompting strategy that
leverages a single prior-sentence memory bank from the source language as
context. The cascaded system participated in the English$\rightarrow$German and
English$\rightarrow$Chinese language directions for both the low and high
latency regimes. In particular, on the English$\rightarrow$German task, the
system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and
3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the
system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and
3521.35, respectively.

</details>


### [274] [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)
*Junzhuo Li,Bo Wang,Xiuze Zhou,Peijie Jiang,Jia Liu,Xuming Hu*

Main category: cs.CL

TL;DR: MoE模型通过"中期激活，后期放大"模式实现每层效率提高37%，深度架构提升鲁棒性，任务敏感性指导设计。


<details>
  <summary>Details</summary>
Motivation: 现有的归因方法无法捕捉稀疏MoE架构中动态路由-专家交互，特别是在异构设计的MoE模型中的解释性尚未得到充分研究。

Method: 提出了一种跨层次归因算法，用于分析稀疏MoE架构（Qwen 1.5-MoE、OLMoE、Mixtral-8x7B）与密集模型（Qwen 1.5-7B、Llama-7B、Mixtral-7B）的对比表现。

Result: MoE模型通过"中期激活，后期放大"模式在每层效率上提高了37%；消融研究表明共享专家处理一般任务，路由专家专注于特定领域处理；架构深度决定了模型的鲁棒性；任务敏感性进一步指导了模型设计。

Conclusion: 这些发现推进了MoE模型的可解释性，并提供了平衡效率、专业化和鲁棒性的原则。

Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those
with heterogeneous designs, remains underexplored. Existing attribution methods
for dense models fail to capture dynamic routing-expert interactions in sparse
MoE architectures. To address this issue, we propose a cross-level attribution
algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,
Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results
show MoE models achieve 37% higher per-layer efficiency via a "mid-activation,
late-amplification" pattern: early layers screen experts, while late layers
refine knowledge collaboratively. Ablation studies reveal a "basic-refinement"
framework--shared experts handle general tasks (entity recognition), while
routed experts specialize in domain-specific processing (geographic
attributes). Semantic-driven routing is evidenced by strong correlations
between attention heads and experts (r=0.68), enabling task-aware coordination.
Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates
expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10
experts) through shared expert redundancy, whereas shallow OLMoE suffers severe
degradation (76% drop). Task sensitivity further guides design: core-sensitive
tasks (geography) require concentrated expertise, while distributed-tolerant
tasks (object attributes) leverage broader participation. These insights
advance MoE interpretability, offering principles to balance efficiency,
specialization, and robustness.

</details>


### [275] [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/abs/2505.24033)
*Yasaman Jafari,Zixian Wang,Leon Bergen,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 通过将隐藏状态合并，研究结构化状态空间模型（SSMs）是否可以支持下游推理，并提出文档汤方法，实现模块化编码和重用，实验表明该方法在多跳问答、稀疏检索和长文档推理方面具有高准确性。


<details>
  <summary>Details</summary>
Motivation: 研究隐藏状态从结构化状态空间模型中能否在事后被合并以支持下游推理。

Method: 受模型汤的启发，提出了一种策略，即文档独立编码并通过简单操作（如平均）将其表示汇集到一个单一的状态。这种方法被称为文档汤，它能够在不重新处理整个输入的情况下进行模块化编码和重用。

Result: 微调后的Mamba2模型生成可汤表示，支持多跳问答、稀疏检索和长文档推理，并且在HotpotQA数据集上，对十个独立编码的文档进行汤操作几乎达到了与在同一输入上训练的交叉编码器相同的性能。

Conclusion: 文档汤方法能够有效地支持下游任务，同时保持较高的准确性和效率。

Abstract: We investigate whether hidden states from Structured State Space Models
(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by
model souping, we propose a strategy where documents are encoded independently
and their representations are pooled -- via simple operations like averaging --
into a single context state. This approach, which we call document souping,
enables modular encoding and reuse without reprocessing the full input for each
query. We finetune Mamba2 models to produce soupable representations and find
that they support multi-hop QA, sparse retrieval, and long-document reasoning
with strong accuracy. On HotpotQA, souping ten independently encoded documents
nearly matches the performance of a cross-encoder trained on the same inputs.

</details>


### [276] [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/abs/2505.24616)
*Nikita Martynov,Anastasia Mordasheva,Dmitriy Gorbetskiy,Danil Astafurov,Ulyana Isaeva,Elina Basyrova,Sergey Skachkov,Victoria Berestova,Nikolay Ivanov,Valeriia Zanina,Alena Fenogenova*

Main category: cs.CL

TL;DR: The paper presents POLLUX, an open-source benchmark for evaluating Russian large language models with a novel methodology including detailed criteria and scoring protocols.


<details>
  <summary>Details</summary>
Motivation: To create a more interpretable, scalable evaluation system for large language models in Russian that reduces reliance on human judgments.

Method: Developed a taxonomy of 35 task types, provided detailed criteria and a scoring protocol where models evaluate each other's responses, introduced LLM-as-a-Judge evaluators (7B and 32B) for nuanced assessment.

Result: POLLUX includes 2,100 manually crafted prompts categorized by difficulty and provides tools for transparent, criteria-driven evaluation beyond traditional human comparisons.

Conclusion: POLLUX offers a comprehensive, open-source benchmark and evaluation methodology for Russian LLMs, enhancing interpretability and reducing costs associated with human evaluations.

Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to
evaluate the generative capabilities of large language models (LLMs) in
Russian. Our main contribution is a novel evaluation methodology that enhances
the interpretability of LLM assessment. For each task type, we define a set of
detailed criteria and develop a scoring protocol where models evaluate
responses and provide justifications for their ratings. This enables
transparent, criteria-driven evaluation beyond traditional resource-consuming,
side-by-side human comparisons. POLLUX includes a detailed, fine-grained
taxonomy of 35 task types covering diverse generative domains such as code
generation, creative writing, and practical assistant use cases, totaling 2,100
manually crafted and professionally authored prompts. Each task is categorized
by difficulty (easy/medium/hard), with experts constructing the dataset
entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)
evaluators trained for nuanced assessment of generative outputs. This approach
provides scalable, interpretable evaluation and annotation tools for model
development, effectively replacing costly and less precise human judgments.

</details>


### [277] [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2505.24630)
*Junyi Li,Hwee Tou Ng*

Main category: cs.CL

TL;DR: Large language models have advanced reasoning capabilities through RL optimization, but suffer from increased hallucinations. This paper proposes FSPO, an algorithm that incorporates factuality verification to reduce hallucinations and improve reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of increased hallucinations in large language models caused by reasoning-oriented reinforcement learning fine-tuning.

Method: FSPO (Factuality-aware Step-wise Policy Optimization), which involves explicit factuality verification at each reasoning step and dynamically adjusts token-level advantage values based on automated verification against given evidence.

Result: Experiments using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy across mathematical reasoning and hallucination benchmarks.

Conclusion: FSPO substantially improves both reliability and performance of large language models in reasoning tasks.

Abstract: Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.

</details>


### [278] [Efficient Text Encoders for Labor Market Analysis](https://arxiv.org/abs/2505.24640)
*Jens-Joris Decorte,Jeroen Van Hautte,Chris Develder,Thomas Demeester*

Main category: cs.CL

TL;DR: 本文提出了一种新的对比学习方法ConTeXT-match，用于技能分类任务，提高了技能提取的效率和性能。同时引入了Skill-XL基准和JobBERT V2模型，分别用于鲁棒评估和工作标题规范化。实验表明这些模型在大规模实时劳动力市场分析中具有高效、准确和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的技能提取方法依赖于大型语言模型（LLMs），但这些模型计算成本高且速度慢。

Method: 提出了ConTeXT-match，一种带有标记级别注意力的对比学习方法，适用于技能分类的极端多标签分类任务；引入了Skill-XL作为新的基准，包含详尽的句子级技能标注；还提出了JobBERT V2模型，利用提取的技能生成高质量的工作标题表示。

Result: ConTeXT-match实现了最先进的结果，同时保持了轻量级双编码器模型的高效性。Skill-XL为技能分类提供了更全面的评估标准。JobBERT V2在工作标题规范化方面表现优异。

Conclusion: 所提出的模型在效率、准确性和可扩展性方面表现出色，适合用于大规模实时劳动力市场分析。

Abstract: Labor market analysis relies on extracting insights from job advertisements,
which provide valuable yet unstructured information on job titles and
corresponding skill requirements. While state-of-the-art methods for skill
extraction achieve strong performance, they depend on large language models
(LLMs), which are computationally expensive and slow. In this paper, we propose
\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level
attention that is well-suited for the extreme multi-label classification task
of skill classification. \textbf{ConTeXT-match} significantly improves skill
extraction efficiency and performance, achieving state-of-the-art results with
a lightweight bi-encoder model. To support robust evaluation, we introduce
\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill
annotations that explicitly address the redundancy in the large label space.
Finally, we present \textbf{JobBERT V2}, an improved job title normalization
model that leverages extracted skills to produce high-quality job title
representations. Experiments demonstrate that our models are efficient,
accurate, and scalable, making them ideal for large-scale, real-time labor
market analysis.

</details>


### [279] [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/abs/2505.24671)
*Dayeon Ki,Rachel Rudinger,Tianyi Zhou,Marine Carpuat*

Main category: cs.CL

TL;DR: 通过多智能体辩论框架，利用多个大型语言模型的互补优势，提高文化适应性，实验表明辩论提高了整体准确性和文化群体平等性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要适应不同的文化背景，但之前的尝试主要集中在单一模型、单轮次的方法上。

Method: 提出了一种多智能体辩论框架，两个基于LLM的智能体在一个文化场景中进行辩论并协同达成最终决定，同时提出了两种变体：一种是LLM智能体独占辩论，另一种是在自我反思和辩论之间动态选择。

Result: 实验显示，辩论在整体准确性和文化群体平等方面都优于单一LLM基线。值得注意的是，多智能体辩论使相对较小的LLM（7-9B）能够达到与大得多的模型（27B参数）相当的准确性。

Conclusion: 多智能体辩论框架能有效提高LLM的文化适应性，特别是在准确性和文化公平性方面表现出色。

Abstract: Large Language Models (LLMs) need to adapt their predictions to diverse
cultural contexts to benefit diverse communities across the world. While
previous efforts have focused on single-LLM, single-turn approaches, we propose
to exploit the complementary strengths of multiple LLMs to promote cultural
adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based
agents debate over a cultural scenario and collaboratively reach a final
decision. We propose two variants: one where either LLM agents exclusively
debate and another where they dynamically choose between self-reflection and
debate during their turns. We evaluate these approaches on 7 open-weight LLMs
(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette
norms in 75 countries. Experiments show that debate improves both overall
accuracy and cultural group parity over single-LLM baselines. Notably,
multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies
comparable to that of a much larger model (27B parameters).

</details>


### [280] [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)
*Dayeon Ki,Kevin Duh,Marine Carpuat*

Main category: cs.CL

TL;DR: In a study on Machine Translation (MT) systems, researchers examined four types of quality feedback - explicit (error highlights and LLM explanations) and implicit (backtranslation and QA tables). They found that all but error highlights improved decision accuracy and reliance. Implicit feedback, particularly QA tables, outperformed explicit feedback in decision accuracy, appropriate reliance, and user perceptions.


<details>
  <summary>Details</summary>
Motivation: To address the urgent need for feedback mechanisms that help users utilize AI responsibly, especially in situations where users cannot easily assess the quality of AI predictions themselves.

Method: A study was conducted in a realistic Machine Translation scenario where monolingual users decide whether to share MT outputs. Four types of quality feedback were compared: explicit feedback (error highlights, LLM explanations) and implicit feedback (backtranslation, QA tables).

Result: All feedback types except error highlights significantly improve decision accuracy and appropriate reliance. Implicit feedback, especially QA tables, yielded greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions.

Conclusion: Implicit feedback mechanisms are more effective than explicit ones in improving users' decision-making regarding AI-generated content, being perceived as more helpful and trustworthy while imposing less mental burden.

Abstract: As people increasingly use AI systems in work and daily life, feedback
mechanisms that help them use AI responsibly are urgently needed, particularly
in settings where users are not equipped to assess the quality of AI
predictions. We study a realistic Machine Translation (MT) scenario where
monolingual users decide whether to share an MT output, first without and then
with quality feedback. We compare four types of quality feedback: explicit
feedback that directly give users an assessment of translation quality using 1)
error highlights and 2) LLM explanations, and implicit feedback that helps
users compare MT inputs and outputs through 3) backtranslation and 4)
question-answer (QA) tables. We find that all feedback types, except error
highlights, significantly improve both decision accuracy and appropriate
reliance. Notably, implicit feedback, especially QA tables, yields
significantly greater gains than explicit feedback in terms of decision
accuracy, appropriate reliance, and user perceptions, receiving the highest
ratings for helpfulness and trust, and the lowest for mental burden.

</details>


### [281] [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/abs/2505.24174)
*Ryota Miyano,Yuki Arase*

Main category: cs.CL

TL;DR: This study proposes an improved LoRA merge method for LLM adaptation in low-resource language generation tasks, which updates and prunes LoRA parameters via fine-tuning with minimal target task data, showing significant improvements in task adaptability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of previous LoRA merge methods that keep LoRA parameters frozen and do not focus on low-resource scenarios, aiming to enhance task adaptability for low-resource language generation.

Method: The proposed method involves updating and pruning LoRA parameters through fine-tuning using minimal target task data, allowing finer-grained adjustments of LoRA parameters.

Result: Extensive experiments on summarization tasks across various domains and multiple languages (English and Japanese) demonstrate significant and consistent improvements in task adaptability compared to previous methods.

Conclusion: The improved LoRA merge method successfully enhances task adaptability for low-resource language generation tasks, outperforming prior approaches.

Abstract: This study proposes a simple yet effective LoRA merge method to achieve LLM
adaptation for low-resource language generation tasks. The LoRA merge
technique, which integrates multiple LoRA modules trained on different tasks,
has gained attention as an effective and efficient approach for adapting LLMs
to target tasks. However, previous methods are limited in adaptability as they
keep the LoRA parameters frozen. Additionally, the low-resource problem has
been out of their scope. We propose a LoRA merge method that updates and prunes
LoRA parameters through fine-tuning with minimal target task data, which allows
finer-grained adjustments of LoRA parameters and enhancement of task
adaptability. Extensive experiments have been conducted taking summarization as
a benchmark task. Our datasets cover various domains and multiple languages of
English and Japanese. The results confirm that the proposed method achieves
significant and consistent improvements in task adaptability over the previous
methods.

</details>


### [282] [Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison](https://arxiv.org/abs/2505.24701)
*Tejul Pandit,Meet Raval,Dhvani Upadhyay*

Main category: cs.CL

TL;DR: The paper proposes using Large Language Models (LLMs) to generate synthetic Aspect-Based Sentiment Analysis (ABSA) data, addressing the lack of diverse labeled datasets. Evaluations on three LLMs reveal their distinct performance trade-offs in topic and sentiment classification tasks. Synthetic data proves effective for ABSA research.


<details>
  <summary>Details</summary>
Motivation: Aspect-Based Sentiment Analysis requires diverse labeled datasets reflecting real-world conversational nuances, but such datasets are scarce.

Method: Synthetic ABSA data is generated using GPT-4o, ensuring consistent topic and sentiment distributions across multiple domains. Three state-of-the-art LLMs are then evaluated on topic and sentiment classification tasks using this synthetic data.

Result: The synthetic data effectively supports ABSA tasks, with each LLM showing different strengths: DeepSeekR1 has higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet have strong recall, and Gemini 1.5 Pro provides faster inference.

Conclusion: Using LLMs to generate synthetic ABSA data is a viable and flexible method, promoting ABSA research and model evaluation without needing limited real-world labeled data.

Abstract: Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions
but often suffers from the scarcity of diverse, labeled datasets that reflect
real-world conversational nuances. This paper presents an approach for
generating synthetic ABSA data using Large Language Models (LLMs) to address
this gap. We detail the generation process aimed at producing data with
consistent topic and sentiment distributions across multiple domains using
GPT-4o. The quality and utility of the generated data were evaluated by
assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,
Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification
tasks. Our results demonstrate the effectiveness of the synthetic data,
revealing distinct performance trade-offs among the models: DeepSeekR1 showed
higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,
and Gemini 1.5 Pro offered significantly faster inference. We conclude that
LLM-based synthetic data generation is a viable and flexible method for
creating valuable ABSA resources, facilitating research and model evaluation
without reliance on limited or inaccessible real-world labeled data.

</details>


### [283] [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/abs/2505.24244)
*Nir Endy,Idan Daniel Grosbard,Yuval Ran-Milo,Yonatan Slutzky,Itay Tshuva,Raja Giryes*

Main category: cs.CL

TL;DR: This paper explores the flow of factual information in Mamba SSM-based language models by adapting attentional interpretability techniques from Transformers, revealing patterns of information transmission and dynamics, as well as differences and similarities between Mamba and Transformer models.


<details>
  <summary>Details</summary>
Motivation: To understand how factual information flows within Mamba State-Space Model based language models and compare it with Transformer-based architectures.

Method: Adapt the Attention Knockout methodology, originally developed for Transformers, to Mamba-1 and Mamba-2 models. Trace information transmission across tokens and layers, disentangle 'features' that enable token-to-token information exchange or enrich individual tokens using Mamba's structured factorization.

Result: Revealed patterns of subject-token information emergence and layer-wise dynamics. Identified phenomena that vary between Mamba and Transformer models, as well as those that are universal across all inspected models.

Conclusion: The study offers a unified perspective on Mamba internal operations and suggests that some observed phenomena may be inherent to large language models in general.

Abstract: This paper investigates the flow of factual information in Mamba State-Space
Model (SSM)-based language models. We rely on theoretical and empirical
connections to Transformer-based architectures and their attention mechanisms.
Exploiting this relationship, we adapt attentional interpretability techniques
originally developed for Transformers--specifically, the Attention Knockout
methodology--to both Mamba-1 and Mamba-2. Using them we trace how information
is transmitted and localized across tokens and layers, revealing patterns of
subject-token information emergence and layer-wise dynamics. Notably, some
phenomena vary between mamba models and Transformer based models, while others
appear universally across all models inspected--hinting that these may be
inherent to LLMs in general. By further leveraging Mamba's structured
factorization, we disentangle how distinct "features" either enable
token-to-token information exchange or enrich individual tokens, thus offering
a unified lens to understand Mamba internal operations.

</details>


### [284] [Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation](https://arxiv.org/abs/2505.24754)
*Yingchaojie Feng,Yiqun Sun,Yandong Sun,Minfeng Zhu,Qiang Huang,Anthony K. H. Tung,Wei Chen*

Main category: cs.CL

TL;DR: The paper proposes GSTransform, a novel framework for instruction-following text embedding that improves quality and achieves significant speedups over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for instruction-following text embedding suffer from high computational overhead as they require re-encoding the entire corpus for each new instruction.

Method: GSTransform is based on Guided Space Transformation. It uses a lightweight transformation mechanism to adapt pre-computed embeddings in real time according to user instructions, guided by a small amount of text data with instruction-focused label annotation.

Result: Extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets show that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300x in real-time processing on large-scale datasets.

Conclusion: GSTransform provides a more efficient way to generate dynamic text embeddings that adapt to user instructions, significantly improving both the quality and speed of instruction-following text embedding.

Abstract: In this work, we investigate an important task named instruction-following
text embedding, which generates dynamic text embeddings that adapt to user
instructions, highlighting specific attributes of text. Despite recent
advancements, existing approaches suffer from significant computational
overhead, as they require re-encoding the entire corpus for each new
instruction. To address this challenge, we propose GSTransform, a novel
instruction-following text embedding framework based on Guided Space
Transformation. Our key observation is that instruction-relevant information is
inherently encoded in generic embeddings but remains underutilized. Instead of
repeatedly encoding the corpus for each instruction, GSTransform is a
lightweight transformation mechanism that adapts pre-computed embeddings in
real time to align with user instructions, guided by a small amount of text
data with instruction-focused label annotation. We conduct extensive
experiments on three instruction-awareness downstream tasks across nine
real-world datasets, demonstrating that GSTransform improves
instruction-following text embedding quality over state-of-the-art methods
while achieving dramatic speedups of 6~300x in real-time processing on
large-scale datasets. The source code is available at
https://github.com/YingchaojieFeng/GSTransform.

</details>


### [285] [Drop Dropout on Single-Epoch Language Model Pretraining](https://arxiv.org/abs/2505.24788)
*Houjun Liu,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: In single-epoch pretraining for LLMs, dropout and early dropout degrade downstream performance in various tasks and model editability. Thus, dropout should be avoided in such pretraining.


<details>
  <summary>Details</summary>
Motivation: To investigate the role of dropout in single-epoch pretraining of language models, given its traditional effectiveness in reducing overfitting but minimal use in large LLMs due to low overfitting.

Method: Conduct experiments on single-epoch pretraining of masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with different levels of dropout, including no dropout and early dropout.

Result: Downstream performance in language modeling, morpho-syntax, question answering, and natural-language inference improves when no dropout is applied during pretraining. Early dropout also degrades performance compared to no dropout. Models trained without dropout are more successful in gradient-based model editing and equivalent in representation-based model editing.

Conclusion: Dropout should be avoided during single-epoch pretraining of LLMs.

Abstract: Originally, dropout was seen as a breakthrough regularization technique that
reduced overfitting and improved performance in almost all applications of deep
learning by reducing overfitting. Yet, single-epoch pretraining tasks common to
modern LLMs yield minimal overfitting, leading to dropout not being used for
large LLMs. Nevertheless, no thorough empirical investigation has been done on
the role of dropout in LM pretraining. Through experiments in single-epoch
pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs
with varying levels of dropout, we find that downstream performance in language
modeling, morpho-syntax (BLiMP), question answering (SQuAD), and
natural-language inference (MNLI) improves when dropout is not applied during
pretraining. We additionally find that the recently-introduced "early dropout"
also degrades performance over applying no dropout at all. We further
investigate the models' editability, and find that models trained without
dropout are more successful in gradient-based model editing (MEND) and
equivalent in representation-based model editing (ReFT). Therefore, we advocate
to drop dropout during single-epoch pretraining.

</details>


### [286] [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)
*Xu Wang,Zihao Li,Benyou Wang,Yan Hu,Difan Zou*

Main category: cs.CL

TL;DR: SSPU is a new framework that uses SAE features for precise, interpretable, and robust unlearning in LLMs. It reduces harmful knowledge accuracy and improves adversarial robustness compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning strategies either lack interpretability or fail to provide robust defense against adversarial prompts in LLMs.

Method: SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization controlling activations into an 'irrelevant' subspace while preserving retained knowledge.

Result: In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines.

Conclusion: The findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.

Abstract: Large language models (LLMs) store vast amounts of information, making them
powerful yet raising privacy and safety concerns when selective knowledge
removal is required. Existing unlearning strategies, ranging from
gradient-based fine-tuning and model editing to sparse autoencoder (SAE)
steering, either lack interpretability or fail to provide a robust defense
against adversarial prompts. We propose SAE-Guided Subspace Projection
Unlearning (SSPU), a novel framework that leverages SAE features to drive
targeted updates in the model's parameter space, enabling precise,
interpretable, and robust unlearning. SSPU's three-stage pipeline performs
data-driven layer and feature selection, subspace construction via QR
decomposition, and constrained optimization that controls activations into an
"irrelevant" subspace while preserving retained knowledge. Overall, we use SAE
features to construct a subspace that supervises unlearning, refining the loss
and adding a regularization term to guide interpretable parameter updates. In
experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,
TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared
to the strongest baseline. It also improves adversarial robustness, lowering
malicious accuracy under jailbreak prompts compared to baselines. Our findings
expose the limitations of prior unlearning methods and demonstrate how
interpretable subspace-guided optimization can achieve robust, controllable
model behavior.

</details>


### [287] [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.24830)
*Juraj Vladika,Annika Domres,Mai Nguyen,Rebecca Moser,Jana Nano,Felix Busch,Lisa C. Adams,Keno K. Bressem,Denise Bernhardt,Stephanie E. Combs,Kai J. Borm,Florian Matthes,Jan C. Peeken*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在医疗领域应用时存在幻觉和不准确引用的问题。本文提出了一种新的原子事实核查框架，将LLM生成的响应分解为可验证的单元，并与权威医学知识库进行独立验证，从而提高了医疗长篇问答中LLM的可靠性和可解释性。该框架显著改善了事实准确性，并能追溯到源文献。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs拥有广泛的医学知识，但其易产生幻觉和不准确引用的特点阻碍了其在临床中的应用和监管合规。现有的方法如检索增强生成虽部分解决了这些问题，但仍存在幻觉和低事实层面可解释性的问题。

Method: 提出了一种新的原子事实核查框架，将LLM生成的响应分解为离散、可验证的单元（称为原子事实），并针对权威医学指南知识库对每个原子事实进行独立验证。此方法允许对错误进行针对性纠正，并直接追溯到源文献。

Result: 通过多读者评估和自动化开放问答基准测试，证明了该框架在事实准确性和可解释性方面的显著改进。框架总体答案改进率高达40%，幻觉检测率达到50%。

Conclusion: 这项工作是朝着更值得信赖和可靠的临床LLM应用迈出的重要一步，解决了当前医疗AI应用中的关键问题，增强了对AI辅助医疗的信心。

Abstract: Large language models (LLMs) exhibit extensive medical knowledge but are
prone to hallucinations and inaccurate citations, which pose a challenge to
their clinical adoption and regulatory compliance. Current methods, such as
Retrieval Augmented Generation, partially address these issues by grounding
answers in source documents, but hallucinations and low fact-level
explainability persist. In this work, we introduce a novel atomic fact-checking
framework designed to enhance the reliability and explainability of LLMs used
in medical long-form question answering. This method decomposes LLM-generated
responses into discrete, verifiable units called atomic facts, each of which is
independently verified against an authoritative knowledge base of medical
guidelines. This approach enables targeted correction of errors and direct
tracing to source literature, thereby improving the factual accuracy and
explainability of medical Q&A. Extensive evaluation using multi-reader
assessments by medical experts and an automated open Q&A benchmark demonstrated
significant improvements in factual accuracy and explainability. Our framework
achieved up to a 40% overall answer improvement and a 50% hallucination
detection rate. The ability to trace each atomic fact back to the most relevant
chunks from the database provides a granular, transparent explanation of the
generated responses, addressing a major gap in current medical AI applications.
This work represents a crucial step towards more trustworthy and reliable
clinical applications of LLMs, addressing key prerequisites for clinical
application and fostering greater confidence in AI-assisted healthcare.

</details>


### [288] [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864)
*Mingjie Liu,Shizhe Diao,Ximing Lu,Jian Hu,Xin Dong,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.CL

TL;DR: The paper challenges assumptions about reinforcement learning (RL) in language models, introducing ProRL which uncovers novel reasoning strategies through prolonged training.


<details>
  <summary>Details</summary>
Motivation: To investigate whether prolonged RL training can expand a model's reasoning capabilities beyond amplifying high-reward outputs already present in the base model and to determine if continually scaling up RL compute leads to improved reasoning performance.

Method: ProRL, a novel training methodology incorporating KL divergence control, reference policy resetting, and a diverse suite of tasks.

Result: RL-trained models outperform base models across various pass@k evaluations, even in scenarios where base models fail regardless of attempts. Reasoning boundary improvements correlate with base model competence and training duration.

Conclusion: ProRL demonstrates that prolonged RL training can uncover novel reasoning strategies inaccessible to base models and provides insights into conditions under which RL expands reasoning boundaries in language models.

Abstract: Recent advances in reasoning-centric language models have highlighted
reinforcement learning (RL) as a promising method for aligning models with
verifiable rewards. However, it remains contentious whether RL truly expands a
model's reasoning capabilities or merely amplifies high-reward outputs already
latent in the base model's distribution, and whether continually scaling up RL
compute reliably leads to improved reasoning performance. In this work, we
challenge prevailing assumptions by demonstrating that prolonged RL (ProRL)
training can uncover novel reasoning strategies that are inaccessible to base
models, even under extensive sampling. We introduce ProRL, a novel training
methodology that incorporates KL divergence control, reference policy
resetting, and a diverse suite of tasks. Our empirical analysis reveals that
RL-trained models consistently outperform base models across a wide range of
pass@k evaluations, including scenarios where base models fail entirely
regardless of the number of attempts. We further show that reasoning boundary
improvements correlates strongly with task competence of base model and
training duration, suggesting that RL can explore and populate new regions of
solution space over time. These findings offer new insights into the conditions
under which RL meaningfully expands reasoning boundaries in language models and
establish a foundation for future work on long-horizon RL for reasoning. We
release model weights to support further research:
https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B

</details>


### [289] [Interpretable phenotyping of Heart Failure patients with Dutch discharge letters](https://arxiv.org/abs/2505.24619)
*Vittorio Torri,Machteld J. Boonstra,Marielle C. van de Veerdonk,Deborah N. Kalkman,Alicia Uijl,Francesca Ieva,Ameen Abu-Hanna,Folkert W. Asselbergs,Iacer Calixto*

Main category: cs.CL

TL;DR: The paper explores models for phenotyping heart failure patients based on LVEF classes using structured and unstructured data. It finds discharge letters to be the most informative source, with Aug-Linear models providing strong performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To evaluate models for phenotyping heart failure patients based on left ventricular ejection fraction (LVEF) classes using both structured and unstructured data while assessing model performance and interpretability.

Method: Analyzed HF hospitalizations data from two Amsterdam UMC hospitals from 2015 to 2023. Silver labels for LVEF classes were generated by combining diagnosis codes, echocardiography results, and textual mentions. Gold labels were manually annotated for testing. Both Transformer-based (black-box) and Aug-Linear (white-box) models were trained and compared with baselines on structured and unstructured data. Interpretability was evaluated by comparing clinician annotations with SHAP, LIME explanations, and inherent explanations of Aug-Linear models.

Result: BERT-based and Aug-Linear models using discharge letters alone achieved the highest classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear on external validation). Aug-Linear explanations aligned more closely with clinicians' explanations than post-hoc explanations on black-box models.

Conclusion: Discharge letters are the most informative source for phenotyping HF patients. Aug-Linear models provide clinician-aligned interpretability and match black-box performance, supporting transparent clinical decision-making.

Abstract: Objective: Heart failure (HF) patients present with diverse phenotypes
affecting treatment and prognosis. This study evaluates models for phenotyping
HF patients based on left ventricular ejection fraction (LVEF) classes, using
structured and unstructured data, assessing performance and interpretability.
  Materials and Methods: The study analyzes all HF hospitalizations at both
Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105
hospitalizations, 16,334 patients). Data from AMC were used for model training,
and from VUmc for external validation. The dataset was unlabelled and included
tabular clinical measurements and discharge letters. Silver labels for LVEF
classes were generated by combining diagnosis codes, echocardiography results,
and textual mentions. Gold labels were manually annotated for 300 patients for
testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box)
models were trained and compared with baselines on structured and unstructured
data. To evaluate interpretability, two clinicians annotated 20 discharge
letters by highlighting information they considered relevant for LVEF
classification. These were compared to SHAP and LIME explanations from
black-box models and the inherent explanations of Aug-Linear models.
  Results: BERT-based and Aug-Linear models, using discharge letters alone,
achieved the highest classification results (AUC=0.84 for BERT, 0.81 for
Aug-Linear on external validation), outperforming baselines. Aug-Linear
explanations aligned more closely with clinicians' explanations than post-hoc
explanations on black-box models.
  Conclusions: Discharge letters emerged as the most informative source for
phenotyping HF patients. Aug-Linear models matched black-box performance while
providing clinician-aligned interpretability, supporting their use in
transparent clinical decision-making.

</details>


### [290] [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858)
*Gabrielle Kaili-May Liu,Gal Yona,Avi Caciularu,Idan Szpektor,Tim G. J. Rudner,Arman Cohan*

Main category: cs.CL

TL;DR: The paper conducts the first systematic study on faithful confidence calibration of LLMs, finding that LLMs generally fail at this task and existing interventions are insufficient. It proposes MetaFaith, a prompt-based calibration method inspired by human metacognition, which significantly improves faithful calibration.


<details>
  <summary>Details</summary>
Motivation: LLMs often use assertive language when conveying false claims, which can lead to over-reliance and eroded trust. There is a need for reliable uncertainty communication in LLMs.

Method: The researchers conducted a comprehensive benchmarking study across various models, datasets, and prompting strategies to evaluate LLMs' ability to faithfully reflect their intrinsic uncertainty. They then introduced MetaFaith, a novel prompt-based calibration approach inspired by human metacognition.

Result: MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.

Conclusion: MetaFaith addresses the critical gap in LLMs' ability to communicate uncertainty reliably, offering a significant advancement in faithful confidence calibration.

Abstract: A critical component in the trustworthiness of LLMs is reliable uncertainty
communication, yet LLMs often use assertive language when conveying false
claims, leading to over-reliance and eroded trust. We present the first
systematic study of $\textit{faithful confidence calibration}$ of LLMs,
benchmarking models' ability to use linguistic expressions of uncertainty that
$\textit{faithfully reflect}$ their intrinsic uncertainty, across a
comprehensive array of models, datasets, and prompting strategies. Our results
demonstrate that LLMs largely fail at this task, and that existing
interventions are insufficient: standard prompt approaches provide only
marginal gains, and existing, factuality-based calibration techniques can even
harm faithful calibration. To address this critical gap, we introduce
MetaFaith, a novel prompt-based calibration approach inspired by human
metacognition. We show that MetaFaith robustly improves faithful calibration
across diverse models and task domains, enabling up to 61% improvement in
faithfulness and achieving an 83% win rate over original generations as judged
by humans.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [291] [Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play](https://arxiv.org/abs/2505.18334)
*Jiaxun Cui,Chen Tang,Jarrett Holtz,Janice Nguyen,Alessandro G. Allievi,Hang Qiu,Peter Stone*

Main category: cs.RO

TL;DR: The paper proposes LLM+Debrief method for autonomous vehicles to communicate in natural language, facilitating cooperation and coordination.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles can drive more safely with communication, but current communication methods are not human-understandable.

Method: Proposes LLM+Debrief method for generating natural language messages through multi-agent discussion.

Result: LLM+Debrief is more effective than zero-shot LLM agent in generating meaningful and human-understandable messages.

Conclusion: Natural language communication among autonomous vehicles can enhance cooperation and coordination.

Abstract: Past work has demonstrated that autonomous vehicles can drive more safely if
they communicate with one another than if they do not. However, their
communication has often not been human-understandable. Using natural language
as a vehicle-to-vehicle (V2V) communication protocol offers the potential for
autonomous vehicles to drive cooperatively not only with each other but also
with human drivers. In this work, we propose a suite of traffic tasks in
autonomous driving where vehicles in a traffic scenario need to communicate in
natural language to facilitate coordination in order to avoid an imminent
collision and/or support efficient traffic flow. To this end, this paper
introduces a novel method, LLM+Debrief, to learn a message generation and
high-level decision-making policy for autonomous vehicles through multi-agent
discussion. To evaluate LLM agents for driving, we developed a gym-like
simulation environment that contains a range of driving scenarios. Our
experimental results demonstrate that LLM+Debrief is more effective at
generating meaningful and human-understandable natural language messages to
facilitate cooperation and coordination than a zero-shot LLM agent. Our code
and demo videos are available at https://talking-vehicles.github.io/.

</details>


### [292] [DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics](https://arxiv.org/abs/2505.24786)
*Eran Bamani Beeri,Eden Nissinman,Avishai Sintov*

Main category: cs.RO

TL;DR: This paper presents DiG-Net, a novel approach for dynamic gesture recognition in assistive robotics up to 30 meters away, using DADA blocks and Spatio-Temporal Graph modules. The model achieves 97.3% accuracy on challenging gestures.


<details>
  <summary>Details</summary>
Motivation: Dynamic hand gestures are crucial for human-robot interaction, especially for those with mobility constraints or operating robots remotely. Existing methods struggle with long-range interactions.

Method: The Distance-aware Gesture Network (DiG-Net) combines Depth-Conditioned Deformable Alignment (DADA) blocks with Spatio-Temporal Graph modules. It also uses the Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL) to enhance learning and robustness across varying distances.

Result: DiG-Net shows significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 97.3% on a diverse dataset with challenging hyper-range gestures.

Conclusion: DiG-Net enhances the usability of assistive robots in various scenarios such as home healthcare, industrial safety, and remote assistance, allowing seamless interactions regardless of physical limitations.

Abstract: Dynamic hand gestures play a pivotal role in assistive human-robot
interaction (HRI), facilitating intuitive, non-verbal communication,
particularly for individuals with mobility constraints or those operating
robots remotely. Current gesture recognition methods are mostly limited to
short-range interactions, reducing their utility in scenarios demanding robust
assistive communication from afar. In this paper, we introduce a novel approach
designed specifically for assistive robotics, enabling dynamic gesture
recognition at extended distances of up to 30 meters, thereby significantly
improving accessibility and quality of life. Our proposed Distance-aware
Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable
Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust
processing and classification of gesture sequences captured under challenging
conditions, including significant physical attenuation, reduced resolution, and
dynamic gesture variations commonly experienced in real-world assistive
environments. We further introduce the Radiometric Spatio-Temporal Depth
Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model
robustness across varying distances. Our model demonstrates significant
performance improvement over state-of-the-art gesture recognition frameworks,
achieving a recognition accuracy of 97.3% on a diverse dataset with challenging
hyper-range gestures. By effectively interpreting gestures from considerable
distances, DiG-Net significantly enhances the usability of assistive robots in
home healthcare, industrial safety, and remote assistance scenarios, enabling
seamless and intuitive interactions for users regardless of physical
limitations

</details>


### [293] [RealDrive: Retrieval-Augmented Driving with Diffusion Models](https://arxiv.org/abs/2505.24808)
*Wenhao Ding,Sushant Veer,Yuxiao Chen,Yulong Cao,Chaowei Xiao,Marco Pavone*

Main category: cs.RO

TL;DR: RealDrive是一种结合检索增强生成（RAG）框架的规划方法，通过从训练数据集中检索相关专家演示来初始化基于扩散的规划策略，从而在各种场景中实现精细控制和安全行为。实验表明，与标准学习型规划器相比，RealDrive减少了40%的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的学习型规划器虽然能生成自然的人类驾驶行为，但在处理稀有、安全关键场景时表现不佳，并且对生成轨迹的可控性有限。

Method: 提出了一种名为RealDrive的方法，该方法使用检索增强生成（RAG）框架，通过从训练数据集中检索最相关的专家演示来初始化基于扩散的规划策略。并通过去噪过程在当前观测和检索示例之间进行插值，利用检索到的情景提供的强先验知识。此外，使用基于规划目标训练的任务相关检索模型比任务无关检索器表现出更优的规划性能。

Result: 实验结果表明，与标准学习型规划器相比，RealDrive在Waymo Open Motion数据集上实现了长尾事件更好的泛化能力和更高的轨迹多样性，并将碰撞率降低了40%。

Conclusion: RealDrive通过结合检索增强生成框架和基于扩散的规划策略，在提高安全性的同时增强了对生成轨迹的控制能力，为自动驾驶规划提供了一种有效的方法。

Abstract: Learning-based planners generate natural human-like driving behaviors by
learning to reason about nuanced interactions from data, overcoming the rigid
behaviors that arise from rule-based planners. Nonetheless, data-driven
approaches often struggle with rare, safety-critical scenarios and offer
limited controllability over the generated trajectories. To address these
challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG)
framework that initializes a diffusion-based planning policy by retrieving the
most relevant expert demonstrations from the training dataset. By interpolating
between current observations and retrieved examples through a denoising
process, our approach enables fine-grained control and safe behavior across
diverse scenarios, leveraging the strong prior provided by the retrieved
scenario. Another key insight we produce is that a task-relevant retrieval
model trained with planning-based objectives results in superior planning
performance in our framework compared to a task-agnostic retriever.
Experimental results demonstrate improved generalization to long-tail events
and enhanced trajectory diversity compared to standard learning-based planners
-- we observe a 40% reduction in collision rate on the Waymo Open Motion
dataset with RAG.

</details>


### [294] [DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation](https://arxiv.org/abs/2505.24853)
*Zhao Mandi,Yifan Hou,Dieter Fox,Yashraj Narang,Ajay Mandlekar,Shuran Song*

Main category: cs.RO

TL;DR: The paper introduces DexMachina, a curriculum-based algorithm for learning dexterous manipulation policies from human hand-object demonstrations, focusing on long-horizon, bimanual tasks with articulated objects. It uses virtual object controllers to help the policy learn gradually and significantly outperforms baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning dexterous manipulation policies for robots from human hand-object demonstrations, particularly in complex, long-horizon, bimanual tasks with articulated objects that involve large action spaces, spatiotemporal discontinuities, and differences between human and robot hand capabilities.

Method: Propose DexMachina, a novel curriculum-based algorithm that employs virtual object controllers with decreasing strength over time. Initially, an object is driven automatically toward its target state, allowing the policy to progressively learn control under motion and contact guidance.

Result: DexMachina shows significant improvement over baseline methods when tested on a diverse set of tasks involving dexterous hands in a simulation benchmark.

Conclusion: This work provides a useful platform for comparing hardware designs and identifying desirable capabilities in dexterous robotic hands, potentially lowering barriers for future research contributions.

Abstract: We study the problem of functional retargeting: learning dexterous
manipulation policies to track object states from human hand-object
demonstrations. We focus on long-horizon, bimanual tasks with articulated
objects, which is challenging due to large action space, spatiotemporal
discontinuities, and embodiment gap between human and robot hands. We propose
DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual
object controllers with decaying strength: an object is first driven
automatically towards its target states, such that the policy can gradually
learn to take over under motion and contact guidance. We release a simulation
benchmark with a diverse set of tasks and dexterous hands, and show that
DexMachina significantly outperforms baseline methods. Our algorithm and
benchmark enable a functional comparison for hardware designs, and we present
key findings informed by quantitative and qualitative results. With the recent
surge in dexterous hand development, we hope this work will provide a useful
platform for identifying desirable hardware capabilities and lower the barrier
for contributing to future research. Videos and more at
https://project-dexmachina.github.io/

</details>


### [295] [Bi-Manual Joint Camera Calibration and Scene Representation](https://arxiv.org/abs/2505.24819)
*Haozhan Tang,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: This paper presents Bi-JCR, a framework that allows multiple robot manipulators with cameras to bypass traditional calibration markers by using 3D foundation models for multi-view correspondence. It estimates extrinsic transformations, inter-arm relative poses, and creates a unified 3D workspace representation from RGB images.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify the camera calibration process in bimanual robot manipulation, which traditionally requires collecting images of pre-determined markers, making it cumbersome.

Method: Bi-JCR leverages 3D foundation models for dense, marker-free multi-view correspondence. It jointly estimates the extrinsic transformation from each camera to its end-effector, the inter-arm relative poses between manipulators, and a unified, scale-consistent 3D representation of the shared workspace.

Result: Empirical evaluations show the robustness of Bi-JCR on various tabletop environments and demonstrate its applicability on downstream tasks, supporting collision checking and semantic segmentation.

Conclusion: Bi-JCR successfully circumvents the need for calibration markers and provides a unified 3D workspace representation that facilitates bimanual coordination tasks.

Abstract: Robot manipulation, especially bimanual manipulation, often requires setting
up multiple cameras on multiple robot manipulators. Before robot manipulators
can generate motion or even build representations of their environments, the
cameras rigidly mounted to the robot need to be calibrated. Camera calibration
is a cumbersome process involving collecting a set of images, with each
capturing a pre-determined marker. In this work, we introduce the Bi-Manual
Joint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables
multiple robot manipulators, each with cameras mounted, to circumvent taking
images of calibration markers. By leveraging 3D foundation models for dense,
marker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the
extrinsic transformation from each camera to its end-effector, (ii) the
inter-arm relative poses between manipulators, and (iii) a unified,
scale-consistent 3D representation of the shared workspace, all from the same
captured RGB image sets. The representation, jointly constructed from images
captured by cameras on both manipulators, lives in a common coordinate frame
and supports collision checking and semantic segmentation to facilitate
downstream bimanual coordination tasks. We empirically evaluate the robustness
of Bi-JCR on a variety of tabletop environments, and demonstrate its
applicability on a variety of downstream tasks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [296] [On the Parallels Between Evolutionary Theory and the State of AI](https://arxiv.org/abs/2505.23774)
*Zeki Doruk Erden,Boi Faltings*

Main category: q-bio.NC

TL;DR: The paper draws parallels between AI and evolutionary biology, proposing a new design paradigm for AI inspired by advancements in evolutionary theory.


<details>
  <summary>Details</summary>
Motivation: To critically examine the foundational principles of contemporary AI methods and explore their limitations while seeking insights from evolutionary biology to inform a new design paradigm for AI.

Method: Drawing parallels between the modern AI landscape and the 20th-century Modern Synthesis in evolutionary biology, and highlighting how advancements in evolutionary theory offer insights that can inform a new design paradigm for AI.

Result: A pathway is proposed to overcome existing limitations in AI, enabling it to achieve its aspirational goals by synthesizing findings across AI and evolutionary theory.

Conclusion: Advancements in evolutionary theory, particularly those of Evolutionary Developmental Biology, can offer insights that inform a new design paradigm for AI.

Abstract: This article critically examines the foundational principles of contemporary
AI methods, exploring the limitations that hinder its potential. We draw
parallels between the modern AI landscape and the 20th-century Modern Synthesis
in evolutionary biology, and highlight how advancements in evolutionary theory
that augmented the Modern Synthesis, particularly those of Evolutionary
Developmental Biology, offer insights that can inform a new design paradigm for
AI. By synthesizing findings across AI and evolutionary theory, we propose a
pathway to overcome existing limitations, enabling AI to achieve its
aspirational goals.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [297] [Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models](https://arxiv.org/abs/2505.24227)
*Ying Yang,Jie Zhang,Xiao Lv,Di Lin,Tao Xiang,Qing Guo*

Main category: cs.CV

TL;DR: An abstract about a new framework LightD that generates natural adversarial samples for VLP models via semantically guided relighting.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks on VLP models have been explored, but generating natural adversarial samples with realistic and meaningful perturbations remains challenging. Existing methods designed for classification tasks are not effective for VLP models due to restricted optimization spaces.

Method: Propose LightD which uses ChatGPT for context-aware initial lighting parameters and integrates a pretrained relighting model (IC-light) for diverse lighting adjustments. Gradient-based optimization is applied to the reference lighting image to enhance attack effectiveness while maintaining visual naturalness.

Result: LightD expands the optimization space while ensuring perturbations align with scene semantics and has been shown to be effective in generating natural adversarial samples.

Conclusion: LightD demonstrates effectiveness and superiority across various VLP models in tasks such as image captioning and visual question answering.

Abstract: While adversarial attacks on vision-and-language pretraining (VLP) models
have been explored, generating natural adversarial samples crafted through
realistic and semantically meaningful perturbations remains an open challenge.
Existing methods, primarily designed for classification tasks, struggle when
adapted to VLP models due to their restricted optimization spaces, leading to
ineffective attacks or unnatural artifacts. To address this, we propose
\textbf{LightD}, a novel framework that generates natural adversarial samples
for VLP models via semantically guided relighting. Specifically, LightD
leverages ChatGPT to propose context-aware initial lighting parameters and
integrates a pretrained relighting model (IC-light) to enable diverse lighting
adjustments. LightD expands the optimization space while ensuring perturbations
align with scene semantics. Additionally, gradient-based optimization is
applied to the reference lighting image to further enhance attack effectiveness
while maintaining visual naturalness. The effectiveness and superiority of the
proposed LightD have been demonstrated across various VLP models in tasks such
as image captioning and visual question answering.

</details>


### [298] [Representational Difference Explanations](https://arxiv.org/abs/2505.23917)
*Neehar Kondapaneni,Oisin Mac Aodha,Pietro Perona*

Main category: cs.CV

TL;DR: The paper proposes Representational Differences Explanations (RDX), a method for discovering and visualizing differences between two learned representations, facilitating interpretable model comparisons.


<details>
  <summary>Details</summary>
Motivation: Current post hoc explainable AI (XAI) methods in machine learning lack the capability to effectively support model comparison, creating a need for a more effective tool.

Method: The method, called Representational Differences Explanations (RDX), is designed to discover and visualize the differences between two learned representations. It is validated by comparing models with known conceptual differences.

Result: RDX successfully recovers meaningful distinctions where existing XAI techniques fail. When applied to state-of-the-art models on subsets of ImageNet and iNaturalist datasets, it reveals insightful representational differences and subtle data patterns.

Conclusion: RDX addresses the gap in current machine learning tools by providing an effective and explainable method for contrasting model representations.

Abstract: We propose a method for discovering and visualizing the differences between
two learned representations, enabling more direct and interpretable model
comparisons. We validate our method, which we call Representational Differences
Explanations (RDX), by using it to compare models with known conceptual
differences and demonstrate that it recovers meaningful distinctions where
existing explainable AI (XAI) techniques fail. Applied to state-of-the-art
models on challenging subsets of the ImageNet and iNaturalist datasets, RDX
reveals both insightful representational differences and subtle patterns in the
data. Although comparison is a cornerstone of scientific analysis, current
tools in machine learning, namely post hoc XAI methods, struggle to support
model comparison effectively. Our work addresses this gap by introducing an
effective and explainable tool for contrasting model representations.

</details>


### [299] [VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL](https://arxiv.org/abs/2505.23977)
*Yichen Feng,Zhangchen Xu,Fengqing Jiang,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.CV

TL;DR: The paper presents VisualSphinx, a large-scale synthetic visual logical reasoning training data for vision language models (VLMs), enhancing their logical reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Vision language models need effective multimodal reasoning and logical coherence which is essential for tasks like diagram understanding but lack appropriate training datasets.

Method: Proposes VisualSphinx, a large-scale synthetic visual logical reasoning training data created through a rule-to-image synthesis pipeline that extracts and expands puzzle rules from seed questions to generate grounded synthesis images.

Result: VLMs trained with VisualSphinx show improved performance in logical reasoning tasks and also benefit other reasoning tasks such as algebraic, arithmetic, and geometry reasoning.

Conclusion: VisualSphinx enhances VLM's reasoning capabilities significantly.

Abstract: Vision language models (VLMs) are expected to perform effective multimodal
reasoning and make logically coherent decisions, which is critical to tasks
such as diagram understanding and spatial problem solving. However, current VLM
reasoning lacks large-scale and well-structured training datasets. To bridge
this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic
visual logical reasoning training data. To tackle the challenge of image
synthesis with grounding answers, we propose a rule-to-image synthesis
pipeline, which extracts and expands puzzle rules from seed questions and
generates the code of grounding synthesis image synthesis for puzzle sample
assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx
benefit from logical coherence and readability of our dataset and exhibit
improved performance on logical reasoning tasks. The enhanced reasoning
capabilities developed from VisualSphinx also benefit other reasoning tasks
such as algebraic reasoning, arithmetic reasoning and geometry reasoning.

</details>


### [300] [Multi-Group Proportional Representation for Text-to-Image Models](https://arxiv.org/abs/2505.24023)
*Sangwon Jung,Alex Oesterling,Claudio Mayrink Verdun,Sajani Vithana,Taesup Moon,Flavio P. Calmon*

Main category: cs.CV

TL;DR: This paper proposes a novel framework using the Multi-Group Proportional Representation (MPR) metric to measure and optimize the representation of intersectional groups in images generated by text-to-image models, promoting balanced demographic representation while preserving generation quality.


<details>
  <summary>Details</summary>
Motivation: There is an increasing concern about the ability of text-to-image generative models to represent diverse demographic groups fairly and responsibly. Current AI design lacks a systematic methodology to measure and control representational harms in image generation.

Method: The paper introduces the MPR metric to evaluate the worst-case deviation of representation statistics across population groups in generated images. It also develops an optimization algorithm that uses MPR as a training objective to guide T2I models towards more balanced generation.

Result: Experiments show that MPR can effectively measure representation statistics across multiple intersectional groups and, when used during training, leads to more balanced image generation across demographic groups without compromising quality.

Conclusion: The proposed framework provides a systematic way to assess and improve the fairness of representation in text-to-image models, contributing to the responsible design of AI systems.

Abstract: Text-to-image (T2I) generative models can create vivid, realistic images from
textual descriptions. As these models proliferate, they expose new concerns
about their ability to represent diverse demographic groups, propagate
stereotypes, and efface minority populations. Despite growing attention to the
"safe" and "responsible" design of artificial intelligence (AI), there is no
established methodology to systematically measure and control representational
harms in image generation. This paper introduces a novel framework to measure
the representation of intersectional groups in images generated by T2I models
by applying the Multi-Group Proportional Representation (MPR) metric. MPR
evaluates the worst-case deviation of representation statistics across given
population groups in images produced by a generative model, allowing for
flexible and context-specific measurements based on user requirements. We also
develop an algorithm to optimize T2I models for this metric. Through
experiments, we demonstrate that MPR can effectively measure representation
statistics across multiple intersectional groups and, when used as a training
objective, can guide models toward a more balanced generation across
demographic groups while maintaining generation quality.

</details>


### [301] [DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models](https://arxiv.org/abs/2505.24025)
*Chenbin Pan,Wenbin He,Zhengzhong Tu,Liu Ren*

Main category: cs.CV

TL;DR: DINO-R1 is the first attempt to enhance visual in-context reasoning of vision foundation models via reinforcement learning, introducing GRQO and KL-regularization methods.


<details>
  <summary>Details</summary>
Motivation: Reasoning capabilities have been successfully explored in large language models but are underexplored in vision foundation models.

Method: Propose DINO-R1 with Group Relative Query Optimization (GRQO) and KL-regularization, building upon Grounding-DINO for training a series of models.

Result: DINO-R1 outperforms supervised fine-tuning baselines on COCO, LVIS, and ODinW datasets, showing strong generalization in various visual prompting scenarios.

Conclusion: DINO-R1 significantly improves visual in-context reasoning through reinforcement learning, setting a new approach for vision foundation models.

Abstract: The recent explosive interest in the reasoning capabilities of large language
models, such as DeepSeek-R1, has demonstrated remarkable success through
reinforcement learning-based fine-tuning frameworks, exemplified by methods
like Group Relative Policy Optimization (GRPO). However, such reasoning
abilities remain underexplored and notably absent in vision foundation models,
including representation models like the DINO series. In this work, we propose
\textbf{DINO-R1}, the first such attempt to incentivize visual in-context
reasoning capabilities of vision foundation models using reinforcement
learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query
Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly
designed for query-based representation models, which computes query-level
rewards based on group-normalized alignment quality. We also apply
KL-regularization to stabilize the objectness distribution to reduce the
training instability. This joint optimization enables dense and expressive
supervision across queries while mitigating overfitting and distributional
drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models
that integrate a visual prompt encoder and a visual-guided query selection
mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that
DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving
strong generalization in both open-vocabulary and closed-set visual prompting
scenarios.

</details>


### [302] [MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking](https://arxiv.org/abs/2505.24026)
*Numair Nadeem,Muhammad Hamza Asad,Saeed Anwar,Abdul Bais*

Main category: cs.CV

TL;DR: The paper presents MaskAdapt, a novel approach for crop and weed semantic segmentation that enhances accuracy through multimodal contextual learning using RGB images and depth data gradients. It introduces geometry-aware masking and cross-attention mechanisms to address occlusions and texture ambiguities, outperforming SOTA UDA methods in diverse agricultural conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for crop and weed segmentation rely heavily on pixel-level annotations and struggle with domain shifts and real-world challenges such as occlusions and visual blending between crops and weeds.

Method: MaskAdapt integrates RGB images with depth data gradients through a cross-attention mechanism to refine feature representations and improve boundary delineation. A geometry-aware masking strategy is also proposed, applying horizontal, vertical, and stochastic masks during training to enhance spatial context understanding.

Result: Evaluations on real agricultural datasets show MaskAdapt consistently surpasses existing SOTA UDA methods, achieving higher segmentation mean Intersection over Union (mIOU) across various field conditions.

Conclusion: MaskAdapt demonstrates significant improvements in semantic segmentation accuracy for crops and weeds under diverse agricultural scenarios, addressing limitations of current UDA methods.

Abstract: Semantic segmentation of crops and weeds is crucial for site-specific farm
management; however, most existing methods depend on labor intensive
pixel-level annotations. A further challenge arises when models trained on one
field (source domain) fail to generalize to new fields (target domain) due to
domain shifts, such as variations in lighting, camera setups, soil composition,
and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by
enabling adaptation without target-domain labels, but current UDA methods
struggle with occlusions and visual blending between crops and weeds, leading
to misclassifications in real-world conditions. To overcome these limitations,
we introduce MaskAdapt, a novel approach that enhances segmentation accuracy
through multimodal contextual learning by integrating RGB images with features
derived from depth data. By computing depth gradients from depth maps, our
method captures spatial transitions that help resolve texture ambiguities.
These gradients, through a cross-attention mechanism, refines RGB feature
representations, resulting in sharper boundary delineation. In addition, we
propose a geometry-aware masking strategy that applies horizontal, vertical,
and stochastic masks during training. This encourages the model to focus on the
broader spatial context for robust visual recognition. Evaluations on real
agricultural datasets demonstrate that MaskAdapt consistently outperforms
existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation
mean Intersection over Union (mIOU) across diverse field conditions.

</details>


### [303] [CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs](https://arxiv.org/abs/2505.24120)
*Ai Jian,Weijie Qiu,Xiaokun Wang,Peiyu Wang,Yunzhuo Hao,Jiangbo Pei,Yichen Wei,Yi Peng,Xuchen Song*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs) lack adequate scientific reasoning abilities. The study introduces CSVQA, a benchmark with 1,378 question-answer pairs across STEM disciplines to evaluate scientific reasoning in VLMs. A rigorous evaluation protocol is also proposed, revealing significant performance disparities among 15 VLMs tested.


<details>
  <summary>Details</summary>
Motivation: To address the inadequately assessed scientific reasoning capabilities of Vision-Language Models and the lack of authentic scientific contexts in current multimodal benchmarks.

Method: Creation of CSVQA, a diagnostic multimodal benchmark featuring 1,378 question-answer pairs spanning diverse STEM disciplines, demanding domain knowledge, visual evidence integration, and higher-order reasoning. Proposal of a rigorous evaluation protocol based on curated explanations.

Result: Evaluation of 15 VLMs on the CSVQA benchmark reveals notable performance disparities, with even the top-ranked model achieving only 49.6% accuracy.

Conclusion: There is an urgent need to enhance scientific reasoning capabilities in VLMs.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in
multimodal understanding, yet their capabilities for scientific reasoning
remains inadequately assessed. Current multimodal benchmarks predominantly
evaluate generic image comprehension or text-driven reasoning, lacking
authentic scientific contexts that require domain-specific knowledge
integration with visual evidence analysis. To fill this gap, we present CSVQA,
a diagnostic multimodal benchmark specifically designed for evaluating
scientific reasoning through domain-grounded visual question answering.Our
benchmark features 1,378 carefully constructed question-answer pairs spanning
diverse STEM disciplines, each demanding domain knowledge, integration of
visual evidence, and higher-order reasoning. Compared to prior multimodal
benchmarks, CSVQA places greater emphasis on real-world scientific content and
complex reasoning.We additionally propose a rigorous evaluation protocol to
systematically assess whether model predictions are substantiated by valid
intermediate reasoning steps based on curated explanations. Our comprehensive
evaluation of 15 VLMs on this benchmark reveals notable performance
disparities, as even the top-ranked proprietary model attains only 49.6\%
accuracy.This empirical evidence underscores the pressing need for advancing
scientific reasoning capabilities in VLMs. Our CSVQA is released at
https://huggingface.co/datasets/Skywork/CSVQA.

</details>


### [304] [S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](https://arxiv.org/abs/2505.24139)
*Yichen Xie,Runsheng Xu,Tong He,Jyh-Jing Hwang,Katie Luo,Jingwei Ji,Hubert Lin,Letian Chen,Yiren Lu,Zhaoqi Leng,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: S4-Driver is a scalable self-supervised motion planning algorithm with spatio-temporal visual representation based on the PaLI multimodal large language model. It transforms MLLMs' visual representation from 2D to 3D space via a novel sparse volume strategy without finetuning the vision encoder, showing competitive performance against supervised multi-task approaches and great scalability.


<details>
  <summary>Details</summary>
Motivation: The key gap in the input representation space for end-to-end autonomous driving approaches built on MLLMs is that they are often pretrained with reasoning tasks in 2D image space rather than the native 3D space where autonomous vehicles plan.

Method: S4-Driver uses a novel sparse volume strategy to transform MLLMs' strong visual representation from perspective view to 3D space without the need to finetune the vision encoder, aggregating multi-view and multi-frame visual inputs for better prediction of planning trajectories in 3D space.

Result: S4-Driver performs favorably against existing supervised multi-task approaches while requiring no human annotations and demonstrates great scalability when pretrained on large volumes of unannotated driving logs.

Conclusion: S4-Driver shows competitive performance against supervised multi-task approaches and great scalability, advancing the state-of-the-art in self-supervised motion planning algorithms.

Abstract: The latest advancements in multi-modal large language models (MLLMs) have
spurred a strong renewed interest in end-to-end motion planning approaches for
autonomous driving. Many end-to-end approaches rely on human annotations to
learn intermediate perception and prediction tasks, while purely
self-supervised approaches--which directly learn from sensor inputs to generate
planning trajectories without human annotations often underperform the state of
the art. We observe a key gap in the input representation space: end-to-end
approaches built on MLLMs are often pretrained with reasoning tasks in 2D image
space rather than the native 3D space in which autonomous vehicles plan. To
this end, we propose S4-Driver, a scalable self-supervised motion planning
algorithm with spatio-temporal visual representation, based on the popular PaLI
multimodal large language model. S4-Driver uses a novel sparse volume strategy
to seamlessly transform the strong visual representation of MLLMs from
perspective view to 3D space without the need to finetune the vision encoder.
This representation aggregates multi-view and multi-frame visual inputs and
enables better prediction of planning trajectories in 3D space. To validate our
method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with
in-house camera data). Results show that S4-Driver performs favorably against
existing supervised multi-task approaches while requiring no human annotations.
It also demonstrates great scalability when pretrained on large volumes of
unannotated driving logs.

</details>


### [305] [The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models](https://arxiv.org/abs/2505.24141)
*Jiashuai Liu,Yingjia Shang,Yingkang Zhan,Di Zhang,Yi Niu,Dong Wei,Xian Wu,Zeyu Gao,Chen Li,Yefeng Zheng*

Main category: cs.CV

TL;DR: 尽管病理基础模型在研究和临床决策支持系统中被广泛采用，但其对抗性攻击的脆弱性尚未得到充分探索。本文首次系统地研究了全切片图像分析中的病理基础模型安全性，并提出了一种无标签攻击框架，能够在修改少量图像块的情况下显著降低下游任务准确性。


<details>
  <summary>Details</summary>
Motivation: 病理基础模型的安全性问题日益重要，但目前对其对抗性攻击的脆弱性研究较少。

Method: 引入了“局部扰动，全局影响”的原则，提出一种无需访问下游任务标签的无标签攻击框架，修订了四种经典的白盒攻击方法，并根据全切片图像的特性重新定义了扰动预算。

Result: 通过仅修改每张幻灯片0.1%的补丁并添加不可察觉的噪声，攻击导致某些情况下下游任务准确率下降高达20%。此外，还分析了影响攻击成功的因素、图像块级漏洞与语义内容的关系，并初步探讨了防御策略。

Conclusion: 本研究为未来关于病理基础模型的对抗鲁棒性和可靠部署的研究奠定了基础。

Abstract: With the widespread adoption of pathology foundation models in both research
and clinical decision support systems, exploring their security has become a
critical concern. However, despite their growing impact, the vulnerability of
these models to adversarial attacks remains largely unexplored. In this work,
we present the first systematic investigation into the security of pathology
foundation models for whole slide image~(WSI) analysis against adversarial
attacks. Specifically, we introduce the principle of \textit{local perturbation
with global impact} and propose a label-free attack framework that operates
without requiring access to downstream task labels. Under this attack
framework, we revise four classical white-box attack methods and redefine the
perturbation budget based on the characteristics of WSI. We conduct
comprehensive experiments on three representative pathology foundation models
across five datasets and six downstream tasks. Despite modifying only 0.1\% of
patches per slide with imperceptible noise, our attack leads to downstream
accuracy degradation that can reach up to 20\% in the worst cases. Furthermore,
we analyze key factors that influence attack success, explore the relationship
between patch-level vulnerability and semantic content, and conduct a
preliminary investigation into potential defence strategies. These findings lay
the groundwork for future research on the adversarial robustness and reliable
deployment of pathology foundation models. Our code is publicly available at:
https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.

</details>


### [306] [Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT](https://arxiv.org/abs/2505.24182)
*Zhuobai Dong,Junchao Yi,Ziyuan Zheng,Haochen Han,Xiangxi Zheng,Alex Jinpeng Wang,Fangming Liu,Linjie Li*

Main category: cs.CV

TL;DR: 多模态大语言模型在视觉物理推理方面存在显著不足，尤其在复杂场景中的物理规律、空间交互和因果关系上表现不佳。为评估此能力，研究提出MVPBench基准测试，并引入基于图的CoT一致性度量方法。实验表明，即使是最先进的模型也表现出较差的视觉推理准确性和图像-文本对齐能力，而RL-based后训练对齐甚至可能损害空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多模态大语言模型虽然在感知和推理方面表现出色，但在视觉物理推理方面存在明显缺陷，特别是在处理复杂场景中的基本物理定律、空间互动和因果效应时。此外，这些模型往往无法根据视觉证据进行连贯的推理链条，尤其是在需要多步骤才能得出正确答案的情况下。

Method: 提出了MVPBench基准测试，用于通过视觉链式思维（CoT）的角度严格评估视觉物理推理能力。每个示例包含交错的多图像输入，要求不仅提供正确的最终答案，还需要一个基于逐步变化视觉线索的连贯推理路径。同时，引入了基于图的CoT一致性度量来验证模型的推理路径是否符合有效的物理逻辑。并通过减少文本先验的使用，鼓励模型依赖于视觉理解。

Result: 实验结果揭示了一个令人担忧的趋势：即使是前沿的多模态大语言模型在物理领域中也显示出较差的视觉推理准确性和弱图像-文本对齐能力。值得注意的是，基于强化学习（RL）的后训练对齐方法通常被认为可以提高视觉推理性能，但事实表明它往往会损害空间推理能力。

Conclusion: 研究表明，目前最先进的多模态大语言模型在视觉物理推理方面存在重大挑战。这提示我们需要重新思考当前的微调实践，特别是与基于强化学习的对齐方法相关的内容。

Abstract: Understanding the physical world - governed by laws of motion, spatial
relations, and causality - poses a fundamental challenge for multimodal large
language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o
demonstrate impressive perceptual and reasoning capabilities, our investigation
reveals these models struggle profoundly with visual physical reasoning,
failing to grasp basic physical laws, spatial interactions, and causal effects
in complex scenes. More importantly, they often fail to follow coherent
reasoning chains grounded in visual evidence, especially when multiple steps
are needed to arrive at the correct answer. To rigorously evaluate this
capability, we introduce MVPBench, a curated benchmark designed to rigorously
evaluate visual physical reasoning through the lens of visual chain-of-thought
(CoT). Each example features interleaved multi-image inputs and demands not
only the correct final answer but also a coherent, step-by-step reasoning path
grounded in evolving visual cues. This setup mirrors how humans reason through
real-world physical processes over time. To ensure fine-grained evaluation, we
introduce a graph-based CoT consistency metric that verifies whether the
reasoning path of model adheres to valid physical logic. Additionally, we
minimize shortcut exploitation from text priors, encouraging models to rely on
visual understanding. Experimental results reveal a concerning trend: even
cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text
alignment in physical domains. Surprisingly, RL-based post-training alignment -
commonly believed to improve visual reasoning performance - often harms spatial
reasoning, suggesting a need to rethink current fine-tuning practices.

</details>


### [307] [Benchmarking Foundation Models for Zero-Shot Biometric Tasks](https://arxiv.org/abs/2505.24214)
*Redwan Sony,Parisa Farmanifard,Hamzeh Alzwairy,Nitish Shukla,Arun Ross*

Main category: cs.CV

TL;DR: Foundation models, especially Vision-Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), show potential in biometric recognition tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of foundation models, specifically VLMs and MLLMs, in biometric recognition and analysis which remains relatively underexplored.

Method: A comprehensive benchmark was introduced to evaluate the zero-shot and few-shot performance of 41 publicly available VLMs across six biometric tasks related to face and iris modalities.

Result: Experiments demonstrated that embeddings from these foundation models can be effectively used for various biometric tasks without fine-tuning. Notable results include a True Match Rate (TMR) of 96.77% at a False Match Rate (FMR) of 1% for face verification on LFW dataset, and a TMR of 97.55% at 1% FMR for iris recognition on IITD-R-Full dataset. Simple classifier heads applied to these embeddings also showed high accuracy in tasks like DeepFake detection, Presentation Attack Detection (PAD), and soft biometric attribute prediction.

Conclusion: This work highlights the potential of pretrained models in achieving Artificial General Intelligence by demonstrating their effectiveness in diverse biometric tasks.

Abstract: The advent of foundation models, particularly Vision-Language Models (VLMs)
and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of
artificial intelligence, enabling remarkable generalization across diverse
tasks with minimal or no supervision. Yet, their potential in biometric
recognition and analysis remains relatively underexplored. In this work, we
introduce a comprehensive benchmark that evaluates the zero-shot and few-shot
performance of state-of-the-art publicly available VLMs and MLLMs across six
biometric tasks spanning the face and iris modalities: face verification, soft
biometric attribute prediction (gender and race), iris recognition,
presentation attack detection (PAD), and face manipulation detection (morphs
and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments
show that embeddings from these foundation models can be used for diverse
biometric tasks with varying degrees of success. For example, in the case of
face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a
False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)
dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1
percent FMR on the IITD-R-Full dataset was 97.55 percent without any
fine-tuning. Further, we show that applying a simple classifier head to these
embeddings can help perform DeepFake detection for faces, Presentation Attack
Detection (PAD) for irides, and extract soft biometric attributes like gender
and ethnicity from faces with reasonably high accuracy. This work reiterates
the potential of pretrained models in achieving the long-term vision of
Artificial General Intelligence.

</details>


### [308] [Reasoning Can Hurt the Inductive Abilities of Large Language Models](https://arxiv.org/abs/2505.24225)
*Haibo Jin,Peiyan Zhang,Man Luo,Haohan Wang*

Main category: cs.CV

TL;DR: 尽管链式思维（CoT）提示被广泛用于增强大型推理模型（LRMs）的归纳推理能力，但研究表明，它可能会降低归纳性能。本文通过四个受控诊断任务揭示了CoT推理可能因三种错误模式放大错误，并提出了无需重新训练即可改进归纳准确性的结构化干预措施。这表明有效的推理不仅取决于步骤数量，还取决于步骤的质量和结构。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在归纳推理方面表现有限，而链式思维（CoT）提示被认为能提升这种能力。然而，其实际效果尚未得到充分验证。

Method: 设计了四个基于游戏的受控诊断任务（国际象棋、德州扑克、骰子游戏和二十一点），这些任务包含隐藏的人类定义规则。通过理论框架分析了CoT推理中可能导致错误的三个模式：子任务分解错误、子任务解决错误和最终答案总结错误。根据分析结果，提出了适应不同错误类型的结构化干预措施。

Result: 研究发现，CoT推理可能会降低归纳推理的表现，大型推理模型在某些情况下甚至不如不使用推理的模型。提出的结构化干预措施显著提高了归纳推理的准确性，且无需重新训练模型。

Conclusion: 有效的推理不仅需要增加推理步骤的数量，还需要确保推理步骤的结构合理。结构化干预可以改善模型的归纳推理能力。

Abstract: Large Language Models (LLMs) have shown remarkable progress across domains,
yet their ability to perform inductive reasoning - inferring latent rules from
sparse examples - remains limited. It is often assumed that chain-of-thought
(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such
reasoning. We investigate this assumption with creating four controlled,
diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -
with hidden human-defined rules. We find that CoT reasoning can degrade
inductive performance, with LRMs often underperforming their non-reasoning
counterparts.
  To explain this, we present a theoretical framework that reveals how
reasoning steps can amplify error through three failure modes: incorrect
sub-task decomposition, incorrect sub-task solving, and incorrect final answer
summarization. Based on our theoretical and empirical analysis, we introduce
structured interventions that adapt CoT generation according to our identified
failure types. These interventions improve inductive accuracy without
retraining. Our findings suggest that effective (CoT) reasoning depends not
only on taking more steps but also on ensuring those steps are well-structured.

</details>


### [309] [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232)
*Haibo Jin,Peiyan Zhang,Peiran Wang,Man Luo,Haohan Wang*

Main category: cs.CV

TL;DR: Large foundation models (LFMs) are vulnerable to hallucinations and jailbreak attacks. This paper proposes a unified theoretical framework for understanding these vulnerabilities, validates it empirically on LLaVA-1.5 and MiniGPT-4, and suggests that robustness strategies should jointly address both issues.


<details>
  <summary>Details</summary>
Motivation: To understand the connection between two distinct vulnerabilities in large foundation models: hallucinations and jailbreak attacks, and to develop more effective defense mechanisms.

Method: Propose a unified theoretical framework modeling jailbreaks as token-level optimization and hallucinations as attention-level optimization. Establish two key propositions: Similar Loss Convergence and Gradient Consistency in Attention Redistribution.

Result: Empirical validation of the propositions on LLaVA-1.5 and MiniGPT-4 shows consistent optimization trends and aligned gradients. Mitigation techniques for one vulnerability can reduce the success rate of the other.

Conclusion: There is a shared failure mode in LFMs regarding hallucinations and jailbreak attacks, suggesting that robustness strategies should be developed to jointly address both vulnerabilities.

Abstract: Large foundation models (LFMs) are susceptible to two distinct
vulnerabilities: hallucinations and jailbreak attacks. While typically studied
in isolation, we observe that defenses targeting one often affect the other,
hinting at a deeper connection.
  We propose a unified theoretical framework that models jailbreaks as
token-level optimization and hallucinations as attention-level optimization.
Within this framework, we establish two key propositions: (1) \textit{Similar
Loss Convergence} - the loss functions for both vulnerabilities converge
similarly when optimizing for target-specific outputs; and (2) \textit{Gradient
Consistency in Attention Redistribution} - both exhibit consistent gradient
behavior driven by shared attention dynamics.
  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,
showing consistent optimization trends and aligned gradients. Leveraging this
connection, we demonstrate that mitigation techniques for hallucinations can
reduce jailbreak success rates, and vice versa. Our findings reveal a shared
failure mode in LFMs and suggest that robustness strategies should jointly
address both vulnerabilities.

</details>


### [310] [LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework](https://arxiv.org/abs/2505.24245)
*Xin Kang,Zihan Zheng,Lei Chu,Yue Gao,Jiahao Li,Hao Pan,Xuejin Chen,Yan Lu*

Main category: cs.CV

TL;DR: The paper introduces LTM3D, a framework combining diffusion and AR models for conditional 3D shape generation, featuring Conditional Distribution Modeling, Prefix Learning, and Latent Token Reconstruction to enhance token dependency learning, align condition tokens, and reduce uncertainty respectively. It operates in token space supporting multiple 3D representations and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To integrate the strengths of diffusion and auto-regressive models for conditional 3D shape generation, addressing the challenge of combining these paradigms effectively.

Method: LTM3D uses a Conditional Distribution Modeling backbone with a masked autoencoder and a diffusion model to improve token dependency learning. It also includes Prefix Learning to align condition tokens with shape latent tokens, and a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity.

Result: Extensive experiments show that LTM3D surpasses existing methods in prompt fidelity and structural accuracy for image- and text-conditioned shape generation tasks, while providing a generalizable framework for multi-modal, multi-representation 3D generation.

Conclusion: LTM3D successfully combines diffusion and AR models for conditional 3D shape generation, offering superior performance and a flexible framework for various 3D representations.

Abstract: We present LTM3D, a Latent Token space Modeling framework for conditional 3D
shape generation that integrates the strengths of diffusion and auto-regressive
(AR) models. While diffusion-based methods effectively model continuous latent
spaces and AR models excel at capturing inter-token dependencies, combining
these paradigms for 3D shape generation remains a challenge. To address this,
LTM3D features a Conditional Distribution Modeling backbone, leveraging a
masked autoencoder and a diffusion model to enhance token dependency learning.
Additionally, we introduce Prefix Learning, which aligns condition tokens with
shape latent tokens during generation, improving flexibility across modalities.
We further propose a Latent Token Reconstruction module with
Reconstruction-Guided Sampling to reduce uncertainty and enhance structural
fidelity in generated shapes. Our approach operates in token space, enabling
support for multiple 3D representations, including signed distance fields,
point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on
image- and text-conditioned shape generation tasks demonstrate that LTM3D
outperforms existing methods in prompt fidelity and structural accuracy while
offering a generalizable framework for multi-modal, multi-representation 3D
generation.

</details>


### [311] [Interactive Video Generation via Domain Adaptation](https://arxiv.org/abs/2505.24253)
*Ishaan Rawal,Suryansh Kumar*

Main category: cs.CV

TL;DR: Text-conditioned diffusion models are strong for video generation, but Interactive Video Generation (IVG) with good quality is hard. Recent methods using attention masking have issues. This paper identifies two problems: perceptual degradation and initialization gap, proposing mask normalization and temporal intrinsic diffusion prior to solve them. These solutions improve both quality and trajectory control.


<details>
  <summary>Details</summary>
Motivation: To enable high-quality Interactive Video Generation where users can control motion elements without degrading the perceptual quality.

Method: Identified two failure modes in current IVG methods as domain shift problems. Proposed mask normalization to mitigate internal covariate shift caused by attention masking and a temporal intrinsic diffusion prior to address the initialization gap.

Result: Extensive evaluations show that the proposed mask normalization and temporal intrinsic denoising enhance both perceptual quality and trajectory control compared to existing IVG techniques.

Conclusion: The solutions of mask normalization and temporal intrinsic diffusion prior effectively improve the perceptual quality and trajectory control in Interactive Video Generation.

Abstract: Text-conditioned diffusion models have emerged as powerful tools for
high-quality video generation. However, enabling Interactive Video Generation
(IVG), where users control motion elements such as object trajectory, remains
challenging. Recent training-free approaches introduce attention masking to
guide trajectory, but this often degrades perceptual quality. We identify two
key failure modes in these methods, both of which we interpret as domain shift
problems, and propose solutions inspired by domain adaptation. First, we
attribute the perceptual degradation to internal covariate shift induced by
attention masking, as pretrained models are not trained to handle masked
attention. To address this, we propose mask normalization, a pre-normalization
layer designed to mitigate this shift via distribution matching. Second, we
address initialization gap, where the randomly sampled initial noise does not
align with IVG conditioning, by introducing a temporal intrinsic diffusion
prior that enforces spatio-temporal consistency at each denoising step.
Extensive qualitative and quantitative evaluations demonstrate that mask
normalization and temporal intrinsic denoising improve both perceptual quality
and trajectory control over the existing state-of-the-art IVG techniques.

</details>


### [312] [Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering](https://arxiv.org/abs/2505.24371)
*Md Intisar Chowdhury,Kittinun Aukkapinyo,Hiroshi Fujimura,Joo Ann Woo,Wasu Wasusatein,Fadoua Ghourabi*

Main category: cs.CV

TL;DR: The paper presents Grid-LoGAT, a VideoQA system using grid-based visual prompting to extract local and global details from video frames with VLM on edge devices for privacy, and answers questions via LLM in the cloud. It outperforms state-of-the-art methods on NExT-QA and STAR-QA datasets.


<details>
  <summary>Details</summary>
Motivation: To create a VideoQA system that ensures image privacy while improving transcript quality by extracting intricate local details from video frames and integrating them with global information.

Method: Grid-LoGAT operates in two phases: extracting text transcripts from video frames using VLM and processing questions using these transcripts through LLM. Grid-based visual prompting is used to improve transcript quality.

Result: Grid-LoGAT outperforms state-of-the-art methods with similar baseline models on NExT-QA (65.9%) and STAR-QA (50.11%) datasets. It surpasses the non-grid version by 24 points on localization-based questions created using NExT-QA.

Conclusion: Grid-LoGAT effectively ensures image privacy and improves transcript quality for VideoQA tasks.

Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription
(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates
in two phases. First, extracting text transcripts from video frames using a
Vision-Language Model (VLM). Next, processing questions using these transcripts
to generate answers through a Large Language Model (LLM). This design ensures
image privacy by deploying the VLM on edge devices and the LLM in the cloud. To
improve transcript quality, we propose grid-based visual prompting, which
extracts intricate local details from each grid cell and integrates them with
global information. Evaluation results show that Grid-LoGAT, using the
open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms
state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA
datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our
method surpasses the non-grid version by 24 points on localization-based
questions we created using NExT-QA.

</details>


### [313] [SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification](https://arxiv.org/abs/2505.24380)
*Zheng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于条带感知空间的细粒度鸟类图像分类框架，包含EPA和CSW两个模块，实验表明该框架性能显著提升且架构高效。


<details>
  <summary>Details</summary>
Motivation: 细粒度鸟类图像分类对于生态监测和物种识别具有重要意义，并在图像识别和细粒度视觉建模领域具有广泛研究价值。然而，与一般图像分类任务相比，FBIC面临更多挑战，如鸟类大小变化、背景干扰以及姿态多样性等问题。这些问题限制了传统方法在实际应用中的泛化性和可解释性。

Method: 提出了一种基于条带感知空间的细粒度鸟类分类框架，其中包含扩展感知聚合器（EPA）和通道语义编织（CSW）两个模块。EPA通过整合水平和垂直空间方向的信息来融合局部纹理细节与全局结构提示；CSW则通过沿通道维度自适应融合长距离和短距离信息进一步细化语义表示。模型建立在ResNet-50主干网络之上，支持跨空间域扩展结构特征的跳跃连接。

Result: 在CUB-200-2011数据集上的实验结果表明，所提出的框架实现了显著的性能改进，同时保持了架构效率。

Conclusion: 所提出的基于条带感知空间的细粒度鸟类分类框架能够有效应对细粒度鸟类图像分类中的挑战，提高了模型的鲁棒性和可解释性，同时保持了高效的架构设计。

Abstract: Fine-grained bird image classification (FBIC) is not only of great
significance for ecological monitoring and species identification, but also
holds broad research value in the fields of image recognition and fine-grained
visual modeling. Compared with general image classification tasks, FBIC poses
more formidable challenges: 1) the differences in species size and imaging
distance result in the varying sizes of birds presented in the images; 2)
complex natural habitats often introduce strong background interference; 3) and
highly flexible poses such as flying, perching, or foraging result in
substantial intra-class variability. These factors collectively make it
difficult for traditional methods to stably extract discriminative features,
thereby limiting the generalizability and interpretability of models in
real-world applications. To address these challenges, this paper proposes a
fine-grained bird classification framework based on strip-aware spatial
perception, which aims to capture long-range spatial dependencies across entire
rows or columns in bird images, thereby enhancing the model's robustness and
interpretability. The proposed method incorporates two novel modules:
extensional perception aggregator (EPA) and channel semantic weaving (CSW).
Specifically, EPA integrates local texture details with global structural cues
by aggregating information across horizontal and vertical spatial directions.
CSW further refines the semantic representations by adaptively fusing
long-range and short-range information along the channel dimension. Built upon
a ResNet-50 backbone, the model enables jump-wise connection of extended
structural features across the spatial domain. Experimental results on the
CUB-200-2011 dataset demonstrate that our framework achieves significant
performance improvements while maintaining architectural efficiency.

</details>


### [314] [Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing](https://arxiv.org/abs/2505.24489)
*Anasse Boutayeb,Iyad Lahsen-cherif,Ahmed El Khadimi*

Main category: cs.CV

TL;DR: This paper explores the application of Deformable-DETR model using deformable attention mechanisms on remote sensing images in optical and SAR modes, achieving high F1 scores and comparing results with other models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the power of deep learning models, particularly transformer-based ones, for object detection tasks in remote sensing images due to their geographical coverage and present objects consistency.

Method: The method involves applying the Deformable-DETR model on two datasets (Pleiades Aircraft dataset for optical and SAR Ship Detection Dataset for SAR) using a 10-fold stratified validation approach.

Result: The model performed well with an F1 score of 95.12% for the optical dataset and 94.54% for SSDD, outperforming several CNNs and transformer-based models.

Conclusion: Deformable-DETR model with deformable attention mechanisms is effective for object detection in remote sensing images across different modes.

Abstract: Object detection has recently seen an interesting trend in terms of the most
innovative research work, this task being of particular importance in the field
of remote sensing, given the consistency of these images in terms of
geographical coverage and the objects present. Furthermore, Deep Learning (DL)
models, in particular those based on Transformers, are especially relevant for
visual computing tasks in general, and target detection in particular. Thus,
the present work proposes an application of Deformable-DETR model, a specific
architecture using deformable attention mechanisms, on remote sensing images in
two different modes, especially optical and Synthetic Aperture Radar (SAR). To
achieve this objective, two datasets are used, one optical, which is Pleiades
Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset
(SSDD). The results of a 10-fold stratified validation showed that the proposed
model performed particularly well, obtaining an F1 score of 95.12% for the
optical dataset and 94.54% for SSDD, while comparing these results with several
models detections, especially those based on CNNs and transformers, as well as
those specifically designed to detect different object classes in remote
sensing images.

</details>


### [315] [BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning](https://arxiv.org/abs/2505.23883)
*Jianyang Gu,Samuel Stevens,Elizabeth G Campolongo,Matthew J Thompson,Net Zhang,Jiaman Wu,Andrei Kopanev,Zheda Mai,Alexander E. White,James Balhoff,Wasila Dahdul,Daniel Rubenstein,Hilmar Lapp,Tanya Berger-Wolf,Wei-Lun Chao,Yu Su*

Main category: cs.CV

TL;DR: 通过大规模对比视觉-语言训练，研究在生物视觉模型中发现了类似基础模型的新兴行为。使用TreeOfLife-200M数据集训练BioCLIP 2模型，在狭窄的物种区分任务之外展现出多种生物视觉任务上的卓越性能，并揭示了嵌入空间中的层级监督和对比目标如何促进这些新兴特性的形成。


<details>
  <summary>Details</summary>
Motivation: 探索生物视觉模型是否可以通过大规模训练展现类似基础模型的新兴行为，特别是在视觉-语言任务中的表现。

Method: 1. 筹备TreeOfLife-200M数据集，包含2.14亿张生物图像。
2. 使用该数据集训练BioCLIP 2模型以区分不同物种。
3. 分析BioCLIP 2在其他生物视觉任务（如栖息地分类和性状预测）中的表现。
4. 探讨其嵌入空间中的特性，包括种间和种内变异的表现。
5. 提供正式证明和分析解释为何分层监督和对比目标会促进这些特性。

Result: 1. BioCLIP 2在各种生物视觉任务上表现出色，尽管其训练目标仅限于区分物种。
2. 嵌入空间中的种间分布与功能和生态学意义密切相关。
3. 种内变异在与种间区分正交的子空间中得以保留并更好地区分。
4. 随着训练数据规模的增加，这些特性变得更加显著。

Conclusion: 大规模训练数据和对比目标促进了生物视觉模型中具有生态和功能意义的嵌入空间的形成，揭示了类似基础模型的新兴行为在生物视觉领域的潜力。

Abstract: Foundation models trained at scale exhibit remarkable emergent behaviors,
learning new capabilities beyond their initial training objectives. We find
such emergent behaviors in biological vision models via large-scale contrastive
vision-language training. To achieve this, we first curate TreeOfLife-200M,
comprising 214 million images of living organisms, the largest and most diverse
biological organism image dataset to date. We then train BioCLIP 2 on
TreeOfLife-200M to distinguish different species. Despite the narrow training
objective, BioCLIP 2 yields extraordinary accuracy when applied to various
biological visual tasks such as habitat classification and trait prediction. We
identify emergent properties in the learned embedding space of BioCLIP 2. At
the inter-species level, the embedding distribution of different species aligns
closely with functional and ecological meanings (e.g., beak sizes and
habitats). At the intra-species level, instead of being diminished, the
intra-species variations (e.g., life stages and sexes) are preserved and better
separated in subspaces orthogonal to inter-species distinctions. We provide
formal proof and analyses to explain why hierarchical supervision and
contrastive objectives encourage these emergent properties. Crucially, our
results reveal that these properties become increasingly significant with
larger-scale training data, leading to a biologically meaningful embedding
space.

</details>


### [316] [Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts](https://arxiv.org/abs/2505.24541)
*Xin He,Xumeng Han,Longhui Wei,Lingxi Xie,Qi Tian*

Main category: cs.CV

TL;DR: Mixpert is an efficient mixture-of-vision-experts architecture that enhances MLLMs by inheriting joint learning advantages from a single vision encoder and incorporating a multi-expert paradigm for task-specific fine-tuning, while a dynamic routing mechanism ensures optimal input allocation.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) face challenges in interpreting complex image information using a single vision encoder due to domain conflicts. Integrating multiple domain-specific vision encoders enhances data perception but adds complexity and limits joint optimization.

Method: The paper introduces Mixpert, which combines the benefits of a single vision encoder with a multi-expert paradigm for task-specific fine-tuning across different visual tasks. A dynamic routing mechanism is designed to allocate input images to the most suitable visual expert.

Result: Mixpert effectively alleviates domain conflicts in multi-task learning with minimal additional computational cost, making it more efficient than using multiple encoders. It integrates seamlessly into any MLLM and demonstrates substantial performance gains across various tasks.

Conclusion: Mixpert offers an efficient solution for enhancing MLLMs' ability to handle complex image information by addressing domain conflicts and maintaining computational efficiency.

Abstract: Multimodal large language models (MLLMs) require a nuanced interpretation of
complex image information, typically leveraging a vision encoder to perceive
various visual scenarios. However, relying solely on a single vision encoder to
handle diverse task domains proves difficult and inevitably leads to conflicts.
Recent work enhances data perception by directly integrating multiple
domain-specific vision encoders, yet this structure adds complexity and limits
the potential for joint optimization. In this paper, we introduce Mixpert, an
efficient mixture-of-vision-experts architecture that inherits the joint
learning advantages from a single vision encoder while being restructured into
a multi-expert paradigm for task-specific fine-tuning across different visual
tasks. Additionally, we design a dynamic routing mechanism that allocates input
images to the most suitable visual expert. Mixpert effectively alleviates
domain conflicts encountered by a single vision encoder in multi-task learning
with minimal additional computational cost, making it more efficient than
multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,
with experimental results demonstrating substantial performance gains across
various tasks.

</details>


### [317] [DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets](https://arxiv.org/abs/2505.23980)
*Bayu Adhi Tama,Mansa Krishna,Homayra Alam,Mostafa Cham,Omar Faruque,Gong Cheng,Jianwu Wang,Mathieu Morlighem,Vandana Janeja*

Main category: cs.CV

TL;DR: This paper presents DeepTopoNet, a deep learning framework that integrates radar-derived ice thickness observations and BedMachine Greenland data through a novel dynamic loss-balancing mechanism to reconstruct bed topography under the Greenland ice sheet.


<details>
  <summary>Details</summary>
Motivation: The complex and sparse nature of observational data, particularly information about the bed topography under the ice sheet, significantly increases the uncertainty in model projections. Traditional methods like airborne ice-penetrating radar leave large data gaps.

Method: The study introduces DeepTopoNet, which incorporates gradient-based and trend surface features, uses a CNN architecture for subgrid-scale predictions, and employs a loss function that adaptively adjusts the weighting between radar and BedMachine data.

Result: By systematically testing on the Upernavik Isstr{\o}m region, the model achieves high accuracy, outperforming baseline methods in reconstructing subglacial terrain.

Conclusion: This work demonstrates the potential of deep learning in bridging observational gaps and provides a scalable and efficient solution to inferring subglacial topography.

Abstract: Understanding Greenland's subglacial topography is critical for projecting
the future mass loss of the ice sheet and its contribution to global sea-level
rise. However, the complex and sparse nature of observational data,
particularly information about the bed topography under the ice sheet,
significantly increases the uncertainty in model projections. Bed topography is
traditionally measured by airborne ice-penetrating radar that measures the ice
thickness directly underneath the aircraft, leaving data gap of tens of
kilometers in between flight lines. This study introduces a deep learning
framework, which we call as DeepTopoNet, that integrates radar-derived ice
thickness observations and BedMachine Greenland data through a novel dynamic
loss-balancing mechanism. Among all efforts to reconstruct bed topography,
BedMachine has emerged as one of the most widely used datasets, combining mass
conservation principles and ice thickness measurements to generate
high-resolution bed elevation estimates. The proposed loss function adaptively
adjusts the weighting between radar and BedMachine data, ensuring robustness in
areas with limited radar coverage while leveraging the high spatial resolution
of BedMachine predictions i.e. bed estimates. Our approach incorporates
gradient-based and trend surface features to enhance model performance and
utilizes a CNN architecture designed for subgrid-scale predictions. By
systematically testing on the Upernavik Isstr{\o}m) region, the model achieves
high accuracy, outperforming baseline methods in reconstructing subglacial
terrain. This work demonstrates the potential of deep learning in bridging
observational gaps, providing a scalable and efficient solution to inferring
subglacial topography.

</details>


### [318] [Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors](https://arxiv.org/abs/2505.24625)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Main category: cs.CV

TL;DR: The paper presents VG LLM, a method that enables MLLMs to understand 3D spaces from video data without additional 3D inputs, showing improvements in 3D scene understanding and spatial reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for understanding 3D scenes using MLLMs rely on comprehensive 3D data inputs like point clouds or BEV maps. The research aims to eliminate the need for such additional 3D inputs.

Method: The proposed method, VG LLM, uses a 3D visual geometry encoder to extract 3D prior information from video sequences. This information is combined with visual tokens and processed by the MLLM.

Result: Experiments indicate significant improvements in various tasks related to 3D scene understanding and spatial reasoning. Notably, the 4B model achieves competitive results compared to state-of-the-art methods and surpasses Gemini-1.5-Pro in VSI-Bench evaluations.

Conclusion: VG LLM successfully enhances MLLMs' capability to understand and reason in 3D spaces directly from video data, demonstrating its potential in advancing 3D scene understanding.

Abstract: Previous research has investigated the application of Multimodal Large
Language Models (MLLMs) in understanding 3D scenes by interpreting them as
videos. These approaches generally depend on comprehensive 3D data inputs, such
as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,
we advance this field by enhancing the capability of MLLMs to understand and
reason in 3D spaces directly from video data, without the need for additional
3D input. We propose a novel and efficient method, the Video-3D Geometry Large
Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that
extracts 3D prior information from video sequences. This information is
integrated with visual tokens and fed into the MLLM. Extensive experiments have
shown that our method has achieved substantial improvements in various tasks
related to 3D scene understanding and spatial reasoning, all directly learned
from video sources. Impressively, our 4B model, which does not rely on explicit
3D data inputs, achieves competitive results compared to existing
state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the
VSI-Bench evaluations.

</details>


### [319] [Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models](https://arxiv.org/abs/2505.24638)
*Zahid Hassan Tushar,Adeleke Ademakinwa,Jianwu Wang,Zhibo Zhang,Sanjay Purushotham*

Main category: cs.CV

TL;DR: The paper proposes a new model named Cloud-Attention-Net with Angle Coding (CAAC) to improve the accuracy of Cloud Optical Thickness (COT) estimation by addressing limitations of previous methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for estimating COT, including the Independent Pixel Approximation (IPA) and existing deep learning models, have significant biases and lack robustness due to simplified assumptions or sensitivity to variations in radiance intensity, distortions, cloud shadows, and different solar and viewing zenith angles.

Method: The proposed method is a novel angle-invariant, attention-based deep model called Cloud-Attention-Net with Angle Coding (CAAC). It incorporates attention mechanisms and angle embeddings to consider satellite viewing geometry and 3D radiative transfer effects. A multi-angle training strategy is also used to ensure angle invariance.

Result: Comprehensive experiments show that CAAC significantly outperforms existing state-of-the-art deep learning models, reducing cloud property retrieval errors by at least a factor of nine.

Conclusion: CAAC addresses the challenges in COT estimation related to 3D cloud effects, viewing angles, and atmospheric interference, providing a more accurate and robust solution compared to previous methods.

Abstract: Cloud Optical Thickness (COT) is a critical cloud property influencing
Earth's climate, weather, and radiation budget. Satellite radiance measurements
enable global COT retrieval, but challenges like 3D cloud effects, viewing
angles, and atmospheric interference must be addressed to ensure accurate
estimation. Traditionally, the Independent Pixel Approximation (IPA) method,
which treats individual pixels independently, has been used for COT estimation.
However, IPA introduces significant bias due to its simplified assumptions.
Recently, deep learning-based models have shown improved performance over IPA
but lack robustness, as they are sensitive to variations in radiance intensity,
distortions, and cloud shadows. These models also introduce substantial errors
in COT estimation under different solar and viewing zenith angles. To address
these challenges, we propose a novel angle-invariant, attention-based deep
model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages
attention mechanisms and angle embeddings to account for satellite viewing
geometry and 3D radiative transfer effects, enabling more accurate retrieval of
COT. Additionally, our multi-angle training strategy ensures angle invariance.
Through comprehensive experiments, we demonstrate that CAAC significantly
outperforms existing state-of-the-art deep learning models, reducing cloud
property retrieval errors by at least a factor of nine.

</details>


### [320] [Federated Foundation Model for GI Endoscopy Images](https://arxiv.org/abs/2505.24108)
*Alina Devkota,Annahita Amireskandari,Joel Palko,Shyam Thakkar,Donald Adjeroh,Xiajun Jiang,Binod Bhattarai,Prashnna K. Gyawali*

Main category: cs.CV

TL;DR: The paper proposes a Federated Learning (FL) framework for training foundation models on gastroendoscopy imaging, allowing data to stay in local hospital environments while contributing to a shared model. It explores FL algorithms and evaluates the trained model on classification, detection, and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Gastrointestinal endoscopy is crucial for early disease detection, but deep learning models require expensive labeled datasets. Foundation models can be fine-tuned for specific tasks but face challenges due to privacy restrictions on medical data.

Method: A FL framework is proposed to train foundation models for gastroendoscopy imaging. Several established FL algorithms are explored and assessed for their suitability without task-specific labels. Experiments are conducted in both homogeneous and heterogeneous settings.

Result: The trained foundation model shows improved performance across three downstream tasks: classification, detection, and segmentation.

Conclusion: The FL framework effectively trains foundation models for gastroendoscopy imaging in a federated, privacy-preserving setting, overcoming data scarcity and privacy issues.

Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract
abnormalities in order to detect diseases in their early stages and improve
patient outcomes. Although deep learning has shown success in supporting GI
diagnostics and decision-making, these models require curated datasets with
labels that are expensive to acquire. Foundation models offer a promising
solution by learning general-purpose representations, which can be finetuned
for specific tasks, overcoming data scarcity. Developing foundation models for
medical imaging holds significant potential, but the sensitive and protected
nature of medical data presents unique challenges. Foundation model training
typically requires extensive datasets, and while hospitals generate large
volumes of data, privacy restrictions prevent direct data sharing, making
foundation model training infeasible in most scenarios. In this work, we
propose a FL framework for training foundation models for gastroendoscopy
imaging, enabling data to remain within local hospital environments while
contributing to a shared model. We explore several established FL algorithms,
assessing their suitability for training foundation models without relying on
task-specific labels, conducting experiments in both homogeneous and
heterogeneous settings. We evaluate the trained foundation model on three
critical downstream tasks--classification, detection, and segmentation--and
demonstrate that it achieves improved performance across all tasks,
highlighting the effectiveness of our approach in a federated,
privacy-preserving setting.

</details>


### [321] [BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models](https://arxiv.org/abs/2505.24649)
*Huu-Thien Tran,Thanh-Dat Truong,Khoa Luu*

Main category: cs.CV

TL;DR: A new method called Bijective Maximum Likelihood Learning (BIMA) is proposed to reduce hallucination in large vision-language models, achieving significant improvements.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models suffer from the problem of hallucination, where responses do not correspond to visual content. There is a need for methods to mitigate this issue effectively.

Method: The paper introduces BIMA, which uses normalizing flow theories to improve the decoding process and thus mitigate hallucination in vision-language models.

Result: BIMA achieves an average F1 score of 85.06% on the POPE benchmark, and reduces CHAIRS and CHAIRI by 7.6% and 2.6%, respectively.

Conclusion: This study is among the first to explore bijection as a means to reduce hallucination in large vision-language models, offering significant improvements.

Abstract: Large vision-language models have become widely adopted to advance in various
domains. However, developing a trustworthy system with minimal interpretable
characteristics of large-scale models presents a significant challenge. One of
the most prevalent terms associated with the fallacy functions caused by these
systems is hallucination, where the language model generates a response that
does not correspond to the visual content. To mitigate this problem, several
approaches have been developed, and one prominent direction is to ameliorate
the decoding process. In this paper, we propose a new Bijective Maximum
Likelihood Learning (BIMA) approach to hallucination mitigation using
normalizing flow theories. The proposed BIMA method can efficiently mitigate
the hallucination problem in prevailing vision-language models, resulting in
significant improvements. Notably, BIMA achieves the average F1 score of 85.06%
on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,
respectively. To the best of our knowledge, this is one of the first studies
that contemplates the bijection means to reduce hallucination induced by large
vision-language models.

</details>


### [322] [Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin](https://arxiv.org/abs/2505.24222)
*Fangyikang Wang,Hubery Yin,Lei Qian,Yinan Li,Shaobin Zhuang,Huminhao Zhu,Yilin Zhang,Yanlong Tang,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Main category: cs.CV

TL;DR: The paper introduces Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion Hessian geometry in a training-free manner with two key innovations, which significantly improves image generation quality with negligible computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current DM sampling techniques typically rely on first-order Langevin dynamics at each noise level and naive attempts to utilize Hessian geometry in high-dimensional DMs lead to quadratic-complexity computational costs. Thus, there is a need for an efficient method to approximate the diffusion Hessian geometry.

Method: The LML method introduces two key innovations: (1) A low-rank approximation of the diffusion Hessian leveraging the DMs' inherent structure and circumventing explicit quadratic-complexity computations; (2) A damping mechanism to stabilize the approximated Hessian.

Result: Extensive experiments across multiple pretrained DMs validate that the LML method significantly improves image generation quality, with negligible computational overhead.

Conclusion: The LML approximated Hessian geometry enables the diffusion sampling to execute more accurate steps and improve the image generation quality.

Abstract: The diffusion models (DMs) have demonstrated the remarkable capability of
generating images via learning the noised score function of data distribution.
Current DM sampling techniques typically rely on first-order Langevin dynamics
at each noise level, with efforts concentrated on refining inter-level
denoising strategies. While leveraging additional second-order Hessian geometry
to enhance the sampling quality of Langevin is a common practice in Markov
chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in
high-dimensional DMs lead to quadratic-complexity computational costs,
rendering them non-scalable. In this work, we introduce a novel
Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion
Hessian geometry in a training-free manner, drawing inspiration from the
celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces
two key innovations: (1) A low-rank approximation of the diffusion Hessian,
leveraging the DMs' inherent structure and circumventing explicit
quadratic-complexity computations; (2) A damping mechanism to stabilize the
approximated Hessian. This LML approximated Hessian geometry enables the
diffusion sampling to execute more accurate steps and improve the image
generation quality. We further conduct a theoretical analysis to substantiate
the approximation error bound of low-rank approximation and the convergence
property of the damping mechanism. Extensive experiments across multiple
pretrained DMs validate that the LML method significantly improves image
generation quality, with negligible computational overhead.

</details>


### [323] [MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM](https://arxiv.org/abs/2505.24238)
*Bowen Dong,Minheng Ni,Zitong Huang,Guanglei Yang,Wangmeng Zuo,Lei Zhang*

Main category: cs.CV

TL;DR: Multimodal hallucination in MLLMs is a problem that this paper addresses by proposing a new benchmark called \{dataset\} and a method called \{method\}. This benchmark can distinguish between perception-induced and reasoning-induced hallucinations, and the method reduces logical hallucinations.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of multimodal hallucinations in MLLMs, which restricts their correctness. Existing benchmarks fail to adequately distinguish between different types of hallucinations, hindering the diagnosis of multimodal reasoning failures.

Method: The proposed method, \{method\}, combines curriculum reinforcement fine-tuning and collaborative hint inference. It encourages models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty and reduces reasoning complexity.

Result: The analysis shows that model scale, data scale, and training stages significantly affect the degree of logical, fabrication, and factual hallucinations. Current MLLMs have limited visual reasoning capabilities regarding spatial hallucinations. Question types correlate with distinct hallucination patterns. The proposed method establishes a baseline on the \{dataset\} benchmark and reduces logical hallucinations in original base models.

Conclusion: This paper concludes that the proposed \{dataset\} benchmark effectively isolates reasoning hallucinations and the proposed \{method\} reduces logical hallucinations in MLLMs.

Abstract: Multimodal hallucination in multimodal large language models (MLLMs)
restricts the correctness of MLLMs. However, multimodal hallucinations are
multi-sourced and arise from diverse causes. Existing benchmarks fail to
adequately distinguish between perception-induced hallucinations and
reasoning-induced hallucinations. This failure constitutes a significant issue
and hinders the diagnosis of multimodal reasoning failures within MLLMs. To
address this, we propose the {\dataset} benchmark, which isolates reasoning
hallucinations by constructing questions where input images are correctly
perceived by MLLMs yet reasoning errors persist. {\dataset} introduces
multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination
score for hallucination quantification. Our analysis reveals that (1) the model
scale, data scale, and training stages significantly affect the degree of
logical, fabrication, and factual hallucinations; (2) current MLLMs show no
effective improvement on spatial hallucinations caused by misinterpreted
spatial relationships, indicating their limited visual reasoning capabilities;
and (3) question types correlate with distinct hallucination patterns,
highlighting targeted challenges and potential mitigation strategies. To
address these challenges, we propose {\method}, a method that combines
curriculum reinforcement fine-tuning to encourage models to generate
logic-consistent reasoning chains by stepwise reducing learning difficulty, and
collaborative hint inference to reduce reasoning complexity. {\method}
establishes a baseline on {\dataset}, and reduces the logical hallucinations in
original base models.

</details>


### [324] [STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising](https://arxiv.org/abs/2505.24327)
*Jingjing Liu,Jiashun Jin,Xianchao Xiu,Jianhua Zhang,Wanquan Liu*

Main category: cs.CV

TL;DR: The paper proposes STAR-Net and STAR-Net-S for RSI denoising, leveraging non-local self-similarity and low-rank prior while integrating physical models with deep learning. It also introduces an ADMM-guided unrolling network to automate parameter tuning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in current deep learning-based RSI denoising methods and their insufficient attention to non-local self-similarity, as well as the need for tedious tuning of regularization parameters in conventional iterative optimization approaches.

Method: STAR-Net uses a low-rank prior to capture non-local self-similarity in RSIs. STAR-Net-S is a sparse variant designed to handle non-Gaussian noise. An ADMM-guided deep unrolling network is developed to automatically learn all regularization parameters, combining model-based and deep learning-based advantages.

Result: Comprehensive experiments on both synthetic and real-world datasets show that STAR-Net and STAR-Net-S outperform state-of-the-art RSI denoising methods.

Conclusion: STAR-Net and STAR-Net-S effectively denoise RSIs by leveraging non-local self-similarity and low-rank priors, while the ADMM-guided unrolling network automates parameter tuning, leading to superior performance.

Abstract: Remote sensing image (RSI) denoising is an important topic in the field of
remote sensing. Despite the impressive denoising performance of RSI denoising
methods, most current deep learning-based approaches function as black boxes
and lack integration with physical information models, leading to limited
interpretability. Additionally, many methods may struggle with insufficient
attention to non-local self-similarity in RSI and require tedious tuning of
regularization parameters to achieve optimal performance, particularly in
conventional iterative optimization approaches. In this paper, we first propose
a novel RSI denoising method named sparse tensor-aided representation network
(STAR-Net), which leverages a low-rank prior to effectively capture the
non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a
sparse variant called STAR-Net-S to deal with the interference caused by
non-Gaussian noise in original RSI for the purpose of improving robustness.
Different from conventional iterative optimization, we develop an alternating
direction method of multipliers (ADMM)-guided deep unrolling network, in which
all regularization parameters can be automatically learned, thus inheriting the
advantages of both model-based and deep learning-based approaches and
successfully addressing the above-mentioned shortcomings. Comprehensive
experiments on synthetic and real-world datasets demonstrate that STAR-Net and
STAR-Net-S outperform state-of-the-art RSI denoising methods.

</details>


### [325] [GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models](https://arxiv.org/abs/2505.24340)
*Gilles Quentin Hacheme,Girmaw Abebe Tadesse,Caleb Robinson,Akram Zaytar,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: The paper presents GeoVision Labeler (GVL), a strictly zero-shot classification framework for geospatial imagery. It uses a vision Large Language Model (vLLM) to generate image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). GVL achieves up to 93.2% zero-shot accuracy on binary tasks and implements recursive LLM-driven clustering for multi-class classification tasks.


<details>
  <summary>Details</summary>
Motivation: Classifying geospatial imagery is challenging, especially in regions with scarce or unavailable annotated data. Existing tools claiming zero-shot capabilities still rely on task-specific pretraining and adaptation to reach competitive performance.

Method: GVL is a modular pipeline where a vLLM generates rich, human-readable image descriptions. These descriptions are then mapped to user-defined classes by an LLM. For complex multi-class tasks, a recursive LLM-driven clustering forms meta-classes at successive depths, followed by hierarchical classification.

Result: GVL achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. It delivers competitive zero-shot performance on complex multi-class classification tasks using hierarchical classification.

Conclusion: GVL is a flexible and interpretable pipeline for zero-shot classification of geospatial imagery, enabling a wide range of use cases. The framework is open-sourced to encourage adoption in real-world geospatial workflows.

Abstract: Classifying geospatial imagery remains a major bottleneck for applications
such as disaster response and land-use monitoring-particularly in regions where
annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that
claim zero-shot classification capabilities for satellite imagery nonetheless
rely on task-specific pretraining and adaptation to reach competitive
performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot
classification framework: a vision Large Language Model (vLLM) generates rich,
human-readable image descriptions, which are then mapped to user-defined
classes by a conventional Large Language Model (LLM). This modular, and
interpretable pipeline enables flexible image classification for a large range
of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,
and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary
Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class
classification tasks (UC Merced, RESISC45), we implemented a recursive
LLM-driven clustering to form meta-classes at successive depths, followed by
hierarchical classification-first resolving coarse groups, then finer
distinctions-to deliver competitive zero-shot performance. GVL is open-sourced
at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in
real-world geospatial workflows.

</details>


### [326] [Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers](https://arxiv.org/abs/2505.24443)
*Heejo Kong,Sung-Jin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 提出了一种新的框架Diversify and Conquer (DAC)，以增强半监督学习（SSL）在开放集半监督学习环境中的鲁棒性，特别是在标记数据不足的情况下。该方法通过构建一组不同偏差的模型，利用它们在未标记数据训练时的预测差异，实现对外部未知类别的鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 传统的半监督学习（SSL）假设标记和未标记数据具有相同的类别分布，但在实际应用中，未标记数据通常包含未知类别的数据（即外部数据或异常值），这会显著降低SSL模型的性能。现有的开放集SSL方法依赖于单个模型对内部数据和外部数据的预测差异，当标记数据不足时容易失败，导致性能下降。

Method: DAC框架的核心思想是通过单一训练过程构建一组具有不同偏差的模型。这些模型在对待外部数据时表现出不同的偏向，而在内部数据上保持一致的预测。通过鼓励模型的不同头部结构对外部数据产生分歧，同时对内部数据保持一致性，利用这些头部之间的分歧作为衡量标准来识别未知概念。

Result: 实验结果表明，在标记数据不足的情况下，所提出的方法能够实现对外部数据的鲁棒检测，优于现有的开放集SSL方法以及不考虑外部数据的朴素SSL方法。

Conclusion: DAC框架为解决开放集半监督学习中的外部数据问题提供了一种有效的解决方案，特别是在标记数据有限的情况下。通过构建一组具有不同偏差的模型，该方法提高了SSL在存在未知类别时的鲁棒性。

Abstract: Conventional semi-supervised learning (SSL) ideally assumes that labeled and
unlabeled data share an identical class distribution, however in practice, this
assumption is easily violated, as unlabeled data often includes unknown class
data, i.e., outliers. The outliers are treated as noise, considerably degrading
the performance of SSL models. To address this drawback, we propose a novel
framework, Diversify and Conquer (DAC), to enhance SSL robustness in the
context of open-set semi-supervised learning. In particular, we note that
existing open-set SSL methods rely on prediction discrepancies between inliers
and outliers from a single model trained on labeled data. This approach can be
easily failed when the labeled data is insufficient, leading to performance
degradation that is worse than naive SSL that do not account for outliers. In
contrast, our approach exploits prediction disagreements among multiple models
that are differently biased towards the unlabeled distribution. By leveraging
the discrepancies arising from training on unlabeled data, our method enables
robust outlier detection even when the labeled data is underspecified. Our key
contribution is constructing a collection of differently biased models through
a single training process. By encouraging divergent heads to be differently
biased towards outliers while making consistent predictions for inliers, we
exploit the disagreement among these heads as a measure to identify unknown
concepts. Our code is available at https://github.com/heejokong/DivCon.

</details>


### [327] [VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software](https://arxiv.org/abs/2505.24838)
*Brandon Man,Ghadi Nehme,Md Ferdous Alam,Faez Ahmed*

Main category: cs.CV

TL;DR: 本研究介绍了VideoCAD，一个大规模的合成数据集，包含超过41K个标注的CAD操作视频记录，用于工程UI交互学习。VideoCAD提供了一个数量级更高的复杂性，并展示了两个下游应用：学习专业精度3D CAD工具的UI交互和评估多模态大语言模型的空间推理和视频理解能力的VQA基准。为此，我们提出了VideoCADFormer，它在直接从视频学习CAD交互方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集和方法主要集中在移动或Web应用程序中的短时间、低复杂性任务上，无法满足专业工程工具的需求。因此，需要一个更复杂的数据集来捕捉真实世界工程任务的需求。

Method: 研究者们引入了VideoCAD数据集，包含超过41K个标注的CAD操作视频记录。还提出了VideoCADFormer模型，用于从视频中学习CAD交互。并通过VQA基准测试多模态大语言模型的空间推理和视频理解能力。

Result: VideoCAD数据集提供了比现有数据集高一个数量级的复杂性，具有长达20倍的时间跨度。VideoCADFormer模型在学习CAD交互方面优于多个行为克隆基线模型。

Conclusion: VideoCAD和VideoCADFormer揭示了当前基于视频的UI理解状态的关键挑战，包括精确的动作定位、多模态和空间推理以及长时间依赖性的需求。

Abstract: Computer-Aided Design (CAD) is a time-consuming and complex process,
requiring precise, long-horizon user interactions with intricate 3D interfaces.
While recent advances in AI-driven user interface (UI) agents show promise,
most existing datasets and methods focus on short, low-complexity tasks in
mobile or web applications, failing to capture the demands of professional
engineering tools. In this work, we introduce VideoCAD, the first attempt at
engineering UI interaction learning for precision tasks. Specifically, VideoCAD
is a large-scale synthetic dataset consisting of over 41K annotated video
recordings of CAD operations, generated using an automated framework for
collecting high-fidelity UI action data from human-made CAD designs. Compared
to existing datasets, VideoCAD offers an order of magnitude higher complexity
in UI interaction learning for real-world engineering tasks, having up to a 20x
longer time horizon than other datasets. We show two important downstream
applications of VideoCAD: learning UI interactions from professional precision
3D CAD tools and a visual question-answering (VQA) benchmark designed to
evaluate multimodal large language models' (LLM) spatial reasoning and video
understanding abilities. To learn the UI interactions, we propose
VideoCADFormer - a state-of-the-art model in learning CAD interactions directly
from video, which outperforms multiple behavior cloning baselines. Both
VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key
challenges in the current state of video-based UI understanding, including the
need for precise action grounding, multi-modal and spatial reasoning, and
long-horizon dependencies.

</details>


### [328] [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/abs/2505.24840)
*Yuwen Tan,Yuan Qing,Boqing Gong*

Main category: cs.CV

TL;DR: 尽管当前最先进的大语言模型（LLMs）在视觉理解方面存在局限性，但通过使用大量构建的视觉问答任务进行微调，可以在一定程度上改善其层次一致性。然而，要让视觉LLMs完全理解视觉概念的层次结构，必须使LLMs具备相应的分类知识。


<details>
  <summary>Details</summary>
Motivation: 研究发现许多先进的大语言模型（LLMs）缺乏对视觉世界的层次知识，甚至不了解已建立的生物分类法。这种缺陷使得LLMs成为视觉LLMs实现层次视觉理解的瓶颈。

Method: 利用来自六个分类法和四个图像数据集的大约一百万个四选一视觉问答（VQA）任务来揭示这些发现。并对视觉LLM使用这些VQA任务进行微调，验证LLMs的瓶颈效应。

Result: 微调后的视觉LLM在某种程度上再次确认了LLMs的瓶颈效应，因为这些VQA任务提高了LLM的层次一致性比视觉LLM更多。推测只有当LLMs具备相应的分类知识时，才能使视觉LLMs充分理解视觉概念的层次结构。

Conclusion: 为了使视觉LLMs能够全面理解视觉概念的层次结构，必须赋予LLMs相应的分类知识。这表明，未来的研究需要关注如何增强LLMs的分类知识，以克服这一瓶颈。

Abstract: This paper reveals that many state-of-the-art large language models (LLMs)
lack hierarchical knowledge about our visual world, unaware of even
well-established biology taxonomies. This shortcoming makes LLMs a bottleneck
for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone
Fish but not Vertebrate). We arrive at these findings using about one million
four-choice visual question answering (VQA) tasks constructed from six
taxonomies and four image datasets. Interestingly, finetuning a vision LLM
using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because
the VQA tasks improve the LLM's hierarchical consistency more than the vision
LLM's. We conjecture that one cannot make vision LLMs understand visual
concepts fully hierarchical until LLMs possess corresponding taxonomy
knowledge.

</details>


### [329] [Optimal Density Functions for Weighted Convolution in Learning Models](https://arxiv.org/abs/2505.24527)
*Simone Cammarasana,Giuseppe Patanè*

Main category: cs.CV

TL;DR: The paper proposes weighted convolution, a new method for processing signals on regular grids like images. It uses an optimal density function to weigh neighboring pixels by distance, differing from traditional uniform convolution. The authors develop a framework to compute this function and show that it improves accuracy in image-to-image tasks such as denoising, with up to 53% loss reduction. Although execution time increases by 11%, the method is robust across various hyperparameters. Future research will explore its application in real-case 2D and 3D image problems.


<details>
  <summary>Details</summary>
Motivation: To improve the approximation accuracy of convolutional neural networks by developing a more sophisticated approach to convolution that considers the varying importance of neighboring pixels.

Method: Introduce weighted convolution using an optimal density function to scale pixel contributions based on their distance from the central pixel. Define a framework to optimize both the convolutional kernel weights via stochastic gradient descent and the density function via DIRECT-L.

Result: Weighted convolution significantly reduces loss (up to 53%) and increases test accuracy compared to standard convolution in image-to-image tasks. The method is robust across different hyperparameters but increases execution time by 11%.

Conclusion: Weighted convolution offers significant improvements in performance for convolutional neural network tasks, despite a modest increase in computational cost. It has potential applications in real-world 2D and 3D image processing problems.

Abstract: The paper introduces the weighted convolution, a novel approach to the
convolution for signals defined on regular grids (e.g., 2D images) through the
application of an optimal density function to scale the contribution of
neighbouring pixels based on their distance from the central pixel. This choice
differs from the traditional uniform convolution, which treats all neighbouring
pixels equally. Our weighted convolution can be applied to convolutional neural
network problems to improve the approximation accuracy. Given a convolutional
network, we define a framework to compute the optimal density function through
a minimisation model. The framework separates the optimisation of the
convolutional kernel weights (using stochastic gradient descent) from the
optimisation of the density function (using DIRECT-L). Experimental results on
a learning model for an image-to-image task (e.g., image denoising) show that
the weighted convolution significantly reduces the loss (up to 53% improvement)
and increases the test accuracy compared to standard convolution. While this
method increases execution time by 11%, it is robust across several
hyperparameters of the learning model. Future work will apply the weighted
convolution to real-case 2D and 3D image convolutional learning problems.

</details>


### [330] [Time Blindness: Why Video-Language Models Can't See What Humans Can?](https://arxiv.org/abs/2505.24867)
*Ujjwal Upadhyay,Mukul Ranjan,Zhiqiang Shen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 尽管最近视觉-语言模型（VLMs）在理解视频中的时空关系方面取得了显著进展，但当空间信息被遮挡时，这些模型难以捕捉纯粹的时间模式。本文提出了一个新的基准测试SpookyBench，其中信息仅编码在类似噪声帧的时间序列中。研究发现，人类可以以超过98%的准确率识别这些序列中的形状、文本和模式，而最先进的VLMs准确率为0%。这表明了模型过度依赖帧级空间特征以及无法从时间线索中提取意义的关键限制。通过系统分析，作者发现这一问题在不同规模和架构的模型中都存在。为了解决这个问题，需要新的架构或训练范式来将空间依赖性与时间处理分离。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在空间信息被遮挡的情况下，难以捕捉纯粹的时间模式，这揭示了模型在时间理解上的局限性。因此，需要一个专门的基准来评估和改进模型在时间模式识别方面的能力。

Method: 引入了一个名为SpookyBench的新基准测试，其中信息仅编码在类似噪声帧的时间序列中。该基准用于评估模型在低空间信噪比数据集中的时间理解能力，并与人类的表现进行比较。

Result: 实验结果显示，人类能够以超过98%的准确率识别SpookyBench中的形状、文本和模式，而最先进的VLMs准确率为0%。此外，模型在时间理解方面的表现随着任务复杂性的增加而迅速下降，而人类感知则保持相对稳定。

Conclusion: 当前的视觉-语言模型在时间模式识别方面存在显著缺陷，这需要通过新的架构或训练方法来解决。作者发布了SpookyBench以推动相关研究，并缩小人类与机器在视频理解方面的时间认知差距。

Abstract: Recent advances in vision-language models (VLMs) have made impressive strides
in understanding spatio-temporal relationships in videos. However, when spatial
information is obscured, these models struggle to capture purely temporal
patterns. We introduce $\textbf{SpookyBench}$, a benchmark where information is
encoded solely in temporal sequences of noise-like frames, mirroring natural
phenomena from biological signaling to covert communication. Interestingly,
while humans can recognize shapes, text, and patterns in these sequences with
over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance
gap highlights a critical limitation: an over-reliance on frame-level spatial
features and an inability to extract meaning from temporal cues. Furthermore,
when trained in data sets with low spatial signal-to-noise ratios (SNR),
temporal understanding of models degrades more rapidly than human perception,
especially in tasks requiring fine-grained temporal reasoning. Overcoming this
limitation will require novel architectures or training paradigms that decouple
spatial dependencies from temporal processing. Our systematic analysis shows
that this issue persists across model scales and architectures. We release
SpookyBench to catalyze research in temporal pattern recognition and bridge the
gap between human and machine video understanding. Dataset and code has been
made available on our project website: https://timeblindness.github.io/.

</details>


### [331] [Geospatial Foundation Models to Enable Progress on Sustainable Development Goals](https://arxiv.org/abs/2505.24528)
*Pedram Ghamisi,Weikang Yu,Xiaokang Zhang,Aldino Rizaldy,Jian Wang,Chufeng Zhou,Richard Gloaguen,Gustau Camps-Valls*

Main category: cs.CV

TL;DR: Foundation Models (FMs) have shown promise in geospatial analysis and Earth Observation, but their real-world utility for sustainability goals is underexplored. This study introduces SustainFM, a benchmarking framework based on the 17 Sustainable Development Goals, to assess geospatial FMs. The findings indicate that FMs often outperform traditional methods, should be evaluated beyond accuracy, and can offer scalable solutions for sustainability challenges. A shift from model-centric development to impact-driven deployment is advocated.


<details>
  <summary>Details</summary>
Motivation: To comprehensively evaluate the real-world utility and alignment of geospatial Foundation Models with global sustainability goals, which has been underexplored despite the rapid proliferation of such models.

Method: Introduction of SustainFM, a benchmarking framework grounded in the 17 Sustainable Development Goals, to provide an interdisciplinary assessment of geospatial FMs across extremely diverse tasks.

Result: (1) FMs often outperform traditional approaches across diverse tasks and datasets; (2) Evaluation criteria for FMs should include transferability, generalization, and energy efficiency; (3) FMs enable scalable, SDG-grounded solutions.

Conclusion: There is a need for a paradigm shift from model-centric development to impact-driven deployment, considering metrics like energy efficiency, robustness, and ethical implications.

Abstract: Foundation Models (FMs) are large-scale, pre-trained AI systems that have
revolutionized natural language processing and computer vision, and are now
advancing geospatial analysis and Earth Observation (EO). They promise improved
generalization across tasks, scalability, and efficient adaptation with minimal
labeled data. However, despite the rapid proliferation of geospatial FMs, their
real-world utility and alignment with global sustainability goals remain
underexplored. We introduce SustainFM, a comprehensive benchmarking framework
grounded in the 17 Sustainable Development Goals with extremely diverse tasks
ranging from asset wealth prediction to environmental hazard detection. This
study provides a rigorous, interdisciplinary assessment of geospatial FMs and
offers critical insights into their role in attaining sustainability goals. Our
findings show: (1) While not universally superior, FMs often outperform
traditional approaches across diverse tasks and datasets. (2) Evaluating FMs
should go beyond accuracy to include transferability, generalization, and
energy efficiency as key criteria for their responsible use. (3) FMs enable
scalable, SDG-grounded solutions, offering broad utility for tackling complex
sustainability challenges. Critically, we advocate for a paradigm shift from
model-centric development to impact-driven deployment, and emphasize metrics
such as energy efficiency, robustness to domain shifts, and ethical
considerations.

</details>


### [332] [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/abs/2505.24872)
*Zilin Xiao,Jaywon Koo,Siru Ouyang,Jefferson Hernandez,Yu Meng,Vicente Ordonez*

Main category: cs.CV

TL;DR: ProxyThinker是一种推理时技术，无需训练即可让大型模型继承小型视觉推理器的视觉推理能力。通过修改解码动态，ProxyThinker成功激发了复杂的自我验证和自我修正行为，并在多个视觉基准上提高了性能。此外，其实现方式比以前的方法快38倍，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习推动了大型视觉-语言模型（LVLMs）的视觉推理能力的发展，但使用强化微调（RFT）训练LVLMs计算成本高昂，难以扩展模型规模。因此需要一种更高效的技术来提升模型的视觉推理能力。

Method: 提出了一种名为ProxyThinker的推理时技术，通过从RFT推理器的输出分布中减去基础模型的输出分布，修改解码动态，从而激发复杂的行为如自我验证和自我修正。同时，该方法利用并行技术协调多个语言模型，显著提升了推理速度。

Result: ProxyThinker在空间、数学和跨学科推理等具有挑战性的视觉基准上一致提升了性能，使未调整的基础模型能够与全规模RFT模型相媲美。并且其推理速度比先前的方法快38倍。

Conclusion: ProxyThinker无需训练即可增强大型模型的视觉推理能力，且在多个推理任务上表现出色，其实现方式高效，为实际部署铺平了道路。

Abstract: Recent advancements in reinforcement learning with verifiable rewards have
pushed the boundaries of the visual reasoning capabilities in large
vision-language models (LVLMs). However, training LVLMs with reinforcement
fine-tuning (RFT) is computationally expensive, posing a significant challenge
to scaling model size. In this work, we propose ProxyThinker, an inference-time
technique that enables large models to inherit the visual reasoning
capabilities from small, slow-thinking visual reasoners without any training.
By subtracting the output distributions of base models from those of RFT
reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits
the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors
such as self-verification and self-correction. ProxyThinker consistently boosts
performance on challenging visual benchmarks on spatial, mathematical, and
multi-disciplinary reasoning, enabling untuned base models to compete with the
performance of their full-scale RFT counterparts. Furthermore, our
implementation efficiently coordinates multiple language models with
parallelism techniques and achieves up to 38 $\times$ faster inference compared
to previous decoding-time methods, paving the way for the practical deployment
of ProxyThinker. Code is available at
https://github.com/MrZilinXiao/ProxyThinker.

</details>


### [333] [Reading Recognition in the Wild](https://arxiv.org/abs/2505.24848)
*Charig Yang,Samiul Alam,Shakhrul Iman Siam,Michael J. Proulx,Lambert Mathias,Kiran Somasundaram,Luis Pesqueira,James Fort,Sheroze Sheriffdeen,Omkar Parkhi,Carl Ren,Mi Zhang,Yuning Chai,Richard Newcombe,Hyo Jin Kim*

Main category: cs.CV

TL;DR: 为了在始终开启的智能眼镜中实现自我中心的情境AI，记录用户与世界的互动（包括阅读时）至关重要。本文引入了一项新的任务——阅读识别，以判断用户何时在阅读，并发布了大规模多模式数据集Reading in the Wild，包含100小时的阅读和非阅读视频。研究确定了三种可用于解决此任务的模式（自我中心RGB、眼动追踪、头部姿态），并提出了一种灵活的Transformer模型，可以单独或组合使用这些模式来执行任务。证明了这些模式对任务的相关性和互补性，并探讨了如何高效有效地编码每个模式。此外，展示了该数据集在分类阅读类型方面的实用性，将目前在受限环境中进行的阅读理解研究扩展到更大规模、多样性和现实性。代码、模型和数据将公开。


<details>
  <summary>Details</summary>
Motivation: 在智能眼镜中实现自我中心的情境AI需要能够记录用户与世界的互动，包括在阅读期间的互动。这促使了对阅读识别任务的研究，以确定用户何时在阅读。

Method: 1. 创建了一个大规模多模式数据集Reading in the Wild，包含100小时的阅读和非阅读视频。
2. 确定了三种模态（自我中心RGB、眼动追踪、头部姿态）用于解决阅读识别任务。
3. 提出了一种灵活的Transformer模型，可以单独或组合使用上述模态来执行任务。
4. 探讨了如何高效有效地编码每个模态。

Result: 研究表明，所选的三种模态对于阅读识别任务是相关且互补的。提出的Transformer模型在使用这些模态（单独或组合）时表现良好。此外，该数据集有助于分类阅读类型，将阅读理解研究扩展到更大规模、多样性和现实性。

Conclusion: 通过创建大规模多模式数据集和提出灵活的Transformer模型，本文成功地解决了阅读识别任务。这些成果不仅有助于情境AI的发展，还推动了阅读理解研究向更大规模、更多样化和更真实的场景发展。代码、模型和数据的公开将促进进一步的研究。

Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial
to be able to keep a record of the user's interactions with the world,
including during reading. In this paper, we introduce a new task of reading
recognition to determine when the user is reading. We first introduce the
first-of-its-kind large-scale multimodal Reading in the Wild dataset,
containing 100 hours of reading and non-reading videos in diverse and realistic
scenarios. We then identify three modalities (egocentric RGB, eye gaze, head
pose) that can be used to solve the task, and present a flexible transformer
model that performs the task using these modalities, either individually or
combined. We show that these modalities are relevant and complementary to the
task, and investigate how to efficiently and effectively encode each modality.
Additionally, we show the usefulness of this dataset towards classifying types
of reading, extending current reading understanding studies conducted in
constrained settings to larger scale, diversity and realism. Code, model, and
data will be public.

</details>


### [334] [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
*Yiqing Liang,Jielin Qiu,Wenhao Ding,Zuxin Liu,James Tompkin,Mengdi Xu,Mengzhou Xia,Zhengzhong Tu,Laixi Shi,Jiacheng Zhu*

Main category: cs.CV

TL;DR: Multimodal LLMs (MLLMs) can be effectively post-trained using Reinforcement Learning with Verifiable Rewards (RLVR) through a systematic framework that includes a data mixture strategy, significantly improving general reasoning and out-of-distribution accuracy.


<details>
  <summary>Details</summary>
Motivation: To apply RLVR to MLLMs and address the challenges posed by the diverse and complex nature of vision-language tasks, which require nuanced capabilities. The aim is to improve generalization and reasoning by optimizing dataset mixture strategies during post-training.

Method: (1) Developed a multimodal RLVR framework for multi-dataset post-training with a curated dataset containing verifiable vision-language problems, enabling multi-domain online RL learning. (2) Proposed a data mixture strategy that predicts RL fine-tuning outcomes from the data mixture distribution and optimizes for the best mixture.

Result: Experiments show that combining multi-domain RLVR training with mixture prediction strategies boosts MLLM general reasoning capacities. The best data mixture improves out-of-distribution benchmark accuracy by 5.24% compared to uniform data mixtures and by 20.74% compared to pre-finetuning baselines.

Conclusion: The systematic post-training framework for MLLMs using RLVR, including an optimized data mixture strategy, significantly enhances general reasoning and accuracy on unseen benchmarks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [335] [4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification](https://arxiv.org/abs/2505.23782)
*Andrew P. Berg,Qian Zhang,Mia Y. Wang*

Main category: cs.SD

TL;DR: The paper explores deep learning methods for UAV classification focusing on data scarcity, comparing CNNs and transformers, finding CNNs currently slightly outperform transformers but transformers show potential with more data.


<details>
  <summary>Details</summary>
Motivation: UAV usage is expected to increase significantly, necessitating better security measures. The challenge of data scarcity for training models in UAV classification needs addressing.

Method: Trained models using 4,500 seconds of audio samples across a 9-class dataset. Used PEFT and data augmentations to handle data scarcity. Implemented and compared CNNs and attention-based transformers.

Result: CNNs outperformed transformers by 1-2% accuracy and were more computationally efficient. However, transformers showed potential for improved performance with more data.

Conclusion: Future work will involve increasing the dataset size to further understand the trade-offs between CNNs and transformers.

Abstract: Unmanned aerial vehicle (UAV) usage is expected to surge in the coming
decade, raising the need for heightened security measures to prevent airspace
violations and security threats. This study investigates deep learning
approaches to UAV classification focusing on the key issue of data scarcity. To
investigate this we opted to train the models using a total of 4,500 seconds of
audio samples, evenly distributed across a 9-class dataset. We leveraged
parameter efficient fine-tuning (PEFT) and data augmentations to mitigate the
data scarcity. This paper implements and compares the use of convolutional
neural networks (CNNs) and attention-based transformers. Our results show that,
CNNs outperform transformers by 1-2\% accuracy, while still being more
computationally efficient. These early findings, however, point to potential in
using transformers models; suggesting that with more data and further
optimizations they could outperform CNNs. Future works aims to upscale the
dataset to better understand the trade-offs between these approaches.

</details>


### [336] [Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection](https://arxiv.org/abs/2505.24486)
*Falih Gozi Febrinanto,Kristen Moore,Chandra Thapa,Jiangang Ma,Vidya Saikrishna,Feng Xia*

Main category: cs.SD

TL;DR: The paper proposes Rehearsal with Auxiliary-Informed Sampling (RAIS) for audio deepfake detection, which outperforms current methods with an average Equal Error Rate of 1.953%.


<details>
  <summary>Details</summary>
Motivation: Existing rehearsal techniques fail to effectively capture the diversity of audio characteristics, leading to bias and increased risk of forgetting prior knowledge when faced with new deepfake attacks.

Method: RAIS employs a label generation network to produce auxiliary labels that guide diverse sample selection for the memory buffer in rehearsal-based continual learning approach.

Result: Extensive experiments demonstrate that RAIS outperforms state-of-the-art methods, achieving an average Equal Error Rate (EER) of 1.953% across five experiences.

Conclusion: Rehearsal with Auxiliary-Informed Sampling (RAIS) is an effective rehearsal-based continual learning approach for audio deepfake detection.

Abstract: The performance of existing audio deepfake detection frameworks degrades when
confronted with new deepfake attacks. Rehearsal-based continual learning (CL),
which updates models using a limited set of old data samples, helps preserve
prior knowledge while incorporating new information. However, existing
rehearsal techniques don't effectively capture the diversity of audio
characteristics, introducing bias and increasing the risk of forgetting. To
address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling
(RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS
employs a label generation network to produce auxiliary labels, guiding diverse
sample selection for the memory buffer. Extensive experiments show RAIS
outperforms state-of-the-art methods, achieving an average Equal Error Rate
(EER) of 1.953 % across five experiences. The code is available at:
https://github.com/falihgoz/RAIS.

</details>


### [337] [HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset](https://arxiv.org/abs/2411.14207)
*Shivam Saini,Jürgen Peissig*

Main category: cs.SD

TL;DR: The paper introduces a 7th-order Ambisonic Room Impulse Responses (HOA-RIRs) dataset created via the Image Source Method, featuring a unique 64-microphone configuration for precise spatial audio reproduction and serving as a resource for spatial audio researchers.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and high-resolution dataset for spatial audio research, overcoming limitations of traditional microphone arrays.

Method: Using the Image Source Method to generate a dataset of 7th-order Ambisonic Room Impulse Responses with a unique 64-microphone configuration based on the superposition principle, allowing direct capture in the Spherical Harmonics domain.

Result: A versatile dataset covering various room configurations, offering high spatial resolution and realism for tasks like source localization, reverberation prediction, and immersive sound reproduction.

Conclusion: This dataset is an important contribution to spatial audio research, particularly beneficial for machine learning applications in room acoustics modeling and sound field synthesis.

Abstract: This contribution introduces a dataset of 7th-order Ambisonic Room Impulse
Responses (HOA-RIRs), created using the Image Source Method. By employing
higher-order Ambisonics, our dataset enables precise spatial audio
reproduction, a critical requirement for realistic immersive audio
applications. Leveraging the virtual simulation, we present a unique microphone
configuration, based on the superposition principle, designed to optimize sound
field coverage while addressing the limitations of traditional microphone
arrays. The presented 64-microphone configuration allows us to capture RIRs
directly in the Spherical Harmonics domain. The dataset features a wide range
of room configurations, encompassing variations in room geometry, acoustic
absorption materials, and source-receiver distances. A detailed description of
the simulation setup is provided alongside for an accurate reproduction. The
dataset serves as a vital resource for researchers working on spatial audio,
particularly in applications involving machine learning to improve room
acoustics modeling and sound field synthesis. It further provides a very high
level of spatial resolution and realism crucial for tasks such as source
localization, reverberation prediction, and immersive sound reproduction.

</details>


### [338] [Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion](https://arxiv.org/abs/2505.24291)
*Kaidi Wang,Wenhao Guan,Ziyue Jiang,Hukai Huang,Peijie Chen,Weijie Wu,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: Discl-VC is a new voice conversion framework that separates content and prosody info, enabling better control over speaking styles in zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot voice conversion systems have trouble accurately replicating speaking styles, limiting controllability.

Method: Propose Discl-VC which disentangles content and prosody from self-supervised speech representations using in-context learning with a flow matching transformer. Also introduce a mask generative transformer for non-autoregressive prediction of discrete prosody tokens based on prompts.

Result: Experimental results show superior performance in zero-shot voice conversion and remarkable accuracy in prosody control.

Conclusion: Discl-VC offers significant improvements in both voice conversion quality and prosody control.

Abstract: Currently, zero-shot voice conversion systems are capable of synthesizing the
voice of unseen speakers. However, most existing approaches struggle to
accurately replicate the speaking style of the source speaker or mimic the
distinctive speaking style of the target speaker, thereby limiting the
controllability of voice conversion. In this work, we propose Discl-VC, a novel
voice conversion framework that disentangles content and prosody information
from self-supervised speech representations and synthesizes the target
speaker's voice through in-context learning with a flow matching transformer.
To enable precise control over the prosody of generated speech, we introduce a
mask generative transformer that predicts discrete prosody tokens in a
non-autoregressive manner based on prompts. Experimental results demonstrate
the superior performance of Discl-VC in zero-shot voice conversion and its
remarkable accuracy in prosody control for synthesized speech.

</details>


### [339] [Unified AI for Accurate Audio Anomaly Detection](https://arxiv.org/abs/2505.23781)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.SD

TL;DR: The paper proposes a unified AI framework for high-accuracy audio anomaly detection using noise reduction, feature extraction, and machine learning techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in noisy environments and real-time applications for audio-based anomaly detection.

Method: Combining spectral subtraction and adaptive filtering for noise reduction, traditional methods (MFCCs) and deep embeddings (OpenL3) for feature extraction, and classical models (SVM, Random Forest), deep learning architectures (CNNs), and ensemble methods for modeling.

Result: Superior performance in precision, recall, and classification of slurred vs. normal speech on benchmark datasets like TORGO and LibriSpeech.

Conclusion: This work provides a scalable solution for audio-based anomaly detection.

Abstract: This paper presents a unified AI framework for high-accuracy audio anomaly
detection by integrating advanced noise reduction, feature extraction, and
machine learning modeling techniques. The approach combines spectral
subtraction and adaptive filtering to enhance audio quality, followed by
feature extraction using traditional methods like MFCCs and deep embeddings
from pre-trained models such as OpenL3. The modeling pipeline incorporates
classical models (SVM, Random Forest), deep learning architectures (CNNs), and
ensemble methods to boost robustness and accuracy. Evaluated on benchmark
datasets including TORGO and LibriSpeech, the proposed framework demonstrates
superior performance in precision, recall, and classification of slurred vs.
normal speech. This work addresses challenges in noisy environments and
real-time applications and provides a scalable solution for audio-based anomaly
detection.

</details>


### [340] [Learning Normal Patterns in Musical Loops](https://arxiv.org/abs/2505.23784)
*Shayan Dadman,Bernt Arild Bremdal,Børre Bang,Rune Dalmo*

Main category: cs.SD

TL;DR: This paper presents an unsupervised framework using deep feature extraction and anomaly detection for identifying audio patterns in music samples, showing improved performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing music information retrieval methods that rely on handcrafted features, have domain-specific constraints, or require iterative user interaction.

Method: The method uses a pre-trained Hierarchical Token-semantic Audio Transformer (HTS-AT) with a Feature Fusion Mechanism (FFM) to generate audio loop representations. These are processed by one-class Deep Support Vector Data Description (Deep SVDD) to detect anomalies through mapping normative patterns to a compact latent hypersphere.

Result: The Deep SVDD models, particularly the residual autoencoder variant, demonstrate enhanced anomaly separation compared to standard methods like Isolation Forest and PCA, especially for larger variations.

Conclusion: This research offers a fully unsupervised, flexible solution for effective pattern identification in diverse audio samples, overcoming previous structural and input limitations.

Abstract: This paper introduces an unsupervised framework for detecting audio patterns
in musical samples (loops) through anomaly detection techniques, addressing
challenges in music information retrieval (MIR). Existing methods are often
constrained by reliance on handcrafted features, domain-specific limitations,
or dependence on iterative user interaction. We address these limitations
through an architecture combining deep feature extraction with unsupervised
anomaly detection. Our approach leverages a pre-trained Hierarchical
Token-semantic Audio Transformer (HTS-AT), paired with a Feature Fusion
Mechanism (FFM), to generate representations from variable-length audio loops.
These embeddings are processed using one-class Deep Support Vector Data
Description (Deep SVDD), which learns normative audio patterns by mapping them
to a compact latent hypersphere. Evaluations on curated bass and guitar
datasets compare standard and residual autoencoder variants against baselines
like Isolation Forest (IF) and and principle component analysis (PCA) methods.
Results show our Deep SVDD models, especially the residual autoencoder variant,
deliver improved anomaly separation, particularly for larger variations. This
research contributes a flexible, fully unsupervised solution for processing
diverse audio samples, overcoming previous structural and input limitations
while enabling effective pattern identification through distance-based latent
space scoring.

</details>


### [341] [Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses](https://arxiv.org/abs/2505.23834)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: The paper proposes a Patient-Aware Feature Alignment (PAFA) framework with two novel losses, PCSL and GPAL, for lung sound classification. It achieves excellent results on the ICBHI dataset.


<details>
  <summary>Details</summary>
Motivation: Biomedical signals exhibit inter-patient variability even among patients with the same symptoms, necessitating a learning approach that considers individual differences.

Method: The PAFA framework includes two novel losses: Patient Cohesion-Separation Loss (PCSL) and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same patient while separating those from other patients to capture patient variability. GPAL draws each patient's centroid toward a global center, preventing feature space fragmentation.

Result: The method achieved scores of 64.84% for four-class and 72.08% for two-class classification on the ICBHI dataset.

Conclusion: The findings highlight PAFA's ability to capture individualized patterns and demonstrate performance gains in distinct patient clusters, offering broader applications for patient-centered healthcare.

Abstract: Lung sound classification is vital for early diagnosis of respiratory
diseases. However, biomedical signals often exhibit inter-patient variability
even among patients with the same symptoms, requiring a learning approach that
considers individual differences. We propose a Patient-Aware Feature Alignment
(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)
and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same
patient while separating those from other patients to capture patient
variability, whereas GPAL draws each patient's centroid toward a global center,
preventing feature space fragmentation. Our method achieves outstanding results
on the ICBHI dataset with a score of 64.84\% for four-class and 72.08\% for
two-class classification. These findings highlight PAFA's ability to capture
individualized patterns and demonstrate performance gains in distinct patient
clusters, offering broader applications for patient-centered healthcare.

</details>


### [342] [Acoustic Classification of Maritime Vessels using Learnable Filterbanks](https://arxiv.org/abs/2505.23964)
*Jonas Elsborg,Tejs Vegge,Arghya Bhowmik*

Main category: cs.SD

TL;DR: CATFISH is a deep learning model for recognizing maritime vessels based on acoustic signatures, achieving 96.63% test accuracy.


<details>
  <summary>Details</summary>
Motivation: Reliably monitoring and recognizing maritime vessels based on acoustic signatures is complicated by the variability of different recording scenarios.

Method: Using a trainable spectral front-end and temporal feature encoder to learn a Gabor filterbank, the model can dynamically emphasize different frequency components.

Result: Trained on the VTUAD hydrophone recordings from the Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 % percent test accuracy across varying source-sensor distances, surpassing the previous benchmark by over 12 percentage points.

Conclusion: The authors present the model, justify their architectural choices, analyze the learned Gabor filters, and perform ablation studies on sensor data fusion and attention-based pooling.

Abstract: Reliably monitoring and recognizing maritime vessels based on acoustic
signatures is complicated by the variability of different recording scenarios.
A robust classification framework must be able to generalize across diverse
acoustic environments and variable source-sensor distances. To this end, we
present a deep learning model with robust performance across different
recording scenarios. Using a trainable spectral front-end and temporal feature
encoder to learn a Gabor filterbank, the model can dynamically emphasize
different frequency components. Trained on the VTUAD hydrophone recordings from
the Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 %
percent test accuracy across varying source-sensor distances, surpassing the
previous benchmark by over 12 percentage points. We present the model, justify
our architectural choices, analyze the learned Gabor filters, and perform
ablation studies on sensor data fusion and attention-based pooling.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [343] [An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring](https://arxiv.org/abs/2505.24239)
*Sana Ebrahimi,Mohsen Dehghankar,Abolfazl Asudeh*

Main category: cs.MA

TL;DR: A new multi-agent LLM framework with credibility scoring is introduced to reduce vulnerability to adversarial agents.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM systems are vulnerable to adversarial and low-performing agents.

Method: Model the collaborative query-answering process as an iterative game, associating credibility scores for each agent based on past contributions.

Result: Experiments show effectiveness in reducing adversarial influence and enhancing resilience in multi-agent cooperation, even when adversaries are in the majority.

Conclusion: The introduced framework improves multi-agent system resilience against adversarial agents.

Abstract: While multi-agent LLM systems show strong capabilities in various domains,
they are highly vulnerable to adversarial and low-performing agents. To resolve
this issue, in this paper, we introduce a general and adversary-resistant
multi-agent LLM framework based on credibility scoring. We model the
collaborative query-answering process as an iterative game, where the agents
communicate and contribute to a final system output. Our system associates a
credibility score that is used when aggregating the team outputs. The
credibility scores are learned gradually based on the past contributions of
each agent in query answering. Our experiments across multiple tasks and
settings demonstrate our system's effectiveness in mitigating adversarial
influence and enhancing the resilience of multi-agent cooperation, even in the
adversary-majority settings.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [344] [INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization](https://arxiv.org/abs/2505.24269)
*Aleksandr Algazinov,Joydeep Chandra,Matt Laing*

Main category: cs.NI

TL;DR: In-network computation can significantly improve AI workloads by reducing latency, enhancing throughput and optimizing resource utilization. This paper analyzes the optimization of in-network computation for AI, including programmable network architectures, mapping AI models onto network devices, distributed learning, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The escalating demands of AI workloads on network infrastructure necessitate a transformative approach to processing data within the network itself, rather than relying solely on traditional computing resources.

Method: Exploration of programmable network architectures (SDN, PDPs), methodologies for mapping AI models onto network devices, advancements in distributed learning (in-network aggregation, federated learning), frameworks (Planter, Quark) for development simplification, and key applications (intelligent network monitoring, intrusion detection, traffic management, Edge AI).

Result: Reduced latency, enhanced throughput, optimized resource utilization, and potential for intelligent, efficient, and responsive networks capable of meeting next-generation AI application demands.

Conclusion: In-network AI holds significant potential to revolutionize network infrastructure, creating more intelligent, efficient, and responsive systems for handling AI workloads.

Abstract: In-network computation represents a transformative approach to addressing the
escalating demands of Artificial Intelligence (AI) workloads on network
infrastructure. By leveraging the processing capabilities of network devices
such as switches, routers, and Network Interface Cards (NICs), this paradigm
enables AI computations to be performed directly within the network fabric,
significantly reducing latency, enhancing throughput, and optimizing resource
utilization. This paper provides a comprehensive analysis of optimizing
in-network computation for AI, exploring the evolution of programmable network
architectures, such as Software-Defined Networking (SDN) and Programmable Data
Planes (PDPs), and their convergence with AI. It examines methodologies for
mapping AI models onto resource-constrained network devices, addressing
challenges like limited memory and computational capabilities through efficient
algorithm design and model compression techniques. The paper also highlights
advancements in distributed learning, particularly in-network aggregation, and
the potential of federated learning to enhance privacy and scalability.
Frameworks like Planter and Quark are discussed for simplifying development,
alongside key applications such as intelligent network monitoring, intrusion
detection, traffic management, and Edge AI. Future research directions,
including runtime programmability, standardized benchmarks, and new
applications paradigms, are proposed to advance this rapidly evolving field.
This survey underscores the potential of in-network AI to create intelligent,
efficient, and responsive networks capable of meeting the demands of
next-generation AI applications.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [345] [Bounds on the Excess Minimum Risk via Generalized Information Divergence Measures](https://arxiv.org/abs/2505.24117)
*Ananya Omanwar,Fady Alajaji,Tamás Linder*

Main category: cs.IT

TL;DR: 研究了在马尔可夫链条件下，基于广义信息散度测度的最小风险过剩的上界。通过使用Rényi、α-Jensen-Shannon散度和Sibson互信息，提出了比传统互信息方法更广泛的边界，并证明在某些参数α的区间内，这些边界可以更紧致。


<details>
  <summary>Details</summary>
Motivation: 探讨在随机变量Y、X和Z形成马尔可夫链的情况下，从观察到的特征向量估计目标向量时的风险差异，以及如何利用广义信息散度测度来量化这种差异。

Method: 采用Rényi、α-Jensen-Shannon散度及Sibson互信息等广义信息散度测度，推导出关于最小风险过剩的上界。这些方法推广了基于互信息的方法，并且不需要假设sub-Gaussian参数为常数。

Result: 提出了一系列新的边界条件，这些边界不仅适用于更广泛的联合分布，而且在特定参数α的范围内，相较于传统的互信息方法表现得更加精确。

Conclusion: 通过引入广义信息散度测度，成功地扩展了对最小风险过剩的理解，并提供了在更广泛分布上的适用性。

Abstract: Given finite-dimensional random vectors $Y$, $X$, and $Z$ that form a Markov
chain in that order (i.e., $Y \to X \to Z$), we derive upper bounds on the
excess minimum risk using generalized information divergence measures. Here,
$Y$ is a target vector to be estimated from an observed feature vector $X$ or
its stochastically degraded version $Z$. The excess minimum risk is defined as
the difference between the minimum expected loss in estimating $Y$ from $X$ and
from $Z$. We present a family of bounds that generalize the mutual information
based bound of Gy\"orfi et al. (2023), using the R\'enyi and
$\alpha$-Jensen-Shannon divergences, as well as Sibson's mutual information.
Our bounds are similar to those developed by Modak et al. (2021) and Aminian et
al. (2024) for the generalization error of learning algorithms. However, unlike
these works, our bounds do not require the sub-Gaussian parameter to be
constant and therefore apply to a broader class of joint distributions over
$Y$, $X$, and $Z$. We also provide numerical examples under both constant and
non-constant sub-Gaussianity assumptions, illustrating that our generalized
divergence based bounds can be tighter than the one based on mutual information
for certain regimes of the parameter $\alpha$.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [346] [Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking](https://arxiv.org/abs/2505.23930)
*Naomi Omeonga wa Kayembe*

Main category: cs.CY

TL;DR: 本研究通过区分问题寻求与问题解决，探讨人类智能与AI的独特特征。分析157个YouTube视频后，揭示公众对AI的能力既着迷又焦虑。研究批评正交性理论，强调人类智能整合了目标设定和目标追求，并倡导提高情感和数字素养，以促进负责任的AI应用。


<details>
  <summary>Details</summary>
Motivation: 为了明确人类智能与AI的区别，特别是人类智能中独特的、基于体验的目标设定能力。

Method: 引入概念框架区分问题寻求与问题解决；通过主题分析结合定性和定量方法，分析157个YouTube视频的元数据。

Result: 发现公众对AI的态度存在双重性：既有对其能力的着迷，也有对其社会影响的焦虑和怀疑。

Conclusion: 提出应将问题寻求视为认知的核心，推动以人为本和符合伦理的AI发展，同时增强公众的情感和数字素养。

Abstract: This study introduces a novel conceptual framework distinguishing
problem-seeking from problem-solving to clarify the unique features of human
intelligence in contrast to AI. Problem-seeking refers to the embodied,
emotionally grounded process by which humans identify and set goals, while
problem-solving denotes the execution of strategies aimed at achieving such
predefined objectives. The framework emphasizes that while AI excels at
efficiency and optimization, it lacks the orientation derived from experiential
grounding and the embodiment flexibility intrinsic to human cognition. To
empirically explore this distinction, the research analyzes metadata from 157
YouTube videos discussing AI. Conducting a thematic analysis combining
qualitative insights with keyword-based quantitative metrics, this
mixed-methods approach uncovers recurring themes in public discourse, including
privacy, job displacement, misinformation, optimism, and ethical concerns. The
results reveal a dual sentiment: public fascination with AI's capabilities
coexists with anxiety and skepticism about its societal implications. The
discussion critiques the orthogonality thesis, which posits that intelligence
is separable from goal content, and instead argues that human intelligence
integrates goal-setting and goal-pursuit. It underscores the centrality of
embodied cognition in human reasoning and highlights how AI's limitations come
from its current reliance on computational processing. The study advocates for
enhancing emotional and digital literacy to foster responsible AI engagement.
It calls for reframing public discourse to recognize AI as a tool that augments
-- rather than replaces -- human intelligence. By positioning problem seeking
at the core of cognition and as a critical dimension of intelligence, this
research offers new perspectives on ethically aligned and human-centered AI
development.

</details>


### [347] [Evaluating Gemini in an arena for learning](https://arxiv.org/abs/2505.24477)
*LearnLM Team,Abhinit Modi,Aditya Srikanth Veerubhotla,Aliya Rysbek,Andrea Huber,Ankit Anand,Avishkar Bhoopchand,Brett Wiltshire,Daniel Gillick,Daniel Kasenberg,Eleni Sgouritsa,Gal Elidan,Hengrui Liu,Holger Winnemoeller,Irina Jurenka,James Cohan,Jennifer She,Julia Wilkowski,Kaiz Alarakyia,Kevin R. McKee,Komal Singh,Lisa Wang,Markus Kunesch,Miruna Pîslar,Niv Efron,Parsa Mahmoudieh,Pierre-Alexandre Kamienny,Sara Wiltberger,Shakir Mohamed,Shashank Agarwal,Shubham Milind Phal,Sun Jae Lee,Theofilos Strinopoulos,Wei-Jen Ko,Yael Gold-Zamir,Yael Haramaty,Yannis Assael*

Main category: cs.CY

TL;DR: An 'arena for learning' was conducted to evaluate AI models in education, where educators and experts compared leading AI models. Among them, Gemini 2.5 Pro ranked first overall.


<details>
  <summary>Details</summary>
Motivation: The research community lacks a robust benchmark to evaluate AI models for learning, so there is a need to assess state-of-the-art support for educational use cases.

Method: An 'arena for learning' was used where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. Participants role-played realistic learning use cases and interacted with two models sequentially.

Result: Experts preferred Gemini 2.5 Pro in 73.2% of match-ups, ranking it first overall in the arena. It also demonstrated higher performance across key principles of good pedagogy.

Conclusion: Gemini 2.5 Pro is positioned as a leading model for learning in educational use cases.

Abstract: Artificial intelligence (AI) is poised to transform education, but the
research community lacks a robust, general benchmark to evaluate AI models for
learning. To assess state-of-the-art support for educational use cases, we ran
an "arena for learning" where educators and pedagogy experts conduct blind,
head-to-head, multi-turn comparisons of leading AI models. In particular, $N =
189$ educators drew from their experience to role-play realistic learning use
cases, interacting with two models sequentially, after which $N = 206$ experts
judged which model better supported the user's learning goals. The arena
evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7
Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro
in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini
2.5 Pro also demonstrated markedly higher performance across key principles of
good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading
model for learning.

</details>


### [348] [Generative Knowledge Production Pipeline Driven by Academic Influencers](https://arxiv.org/abs/2505.24681)
*Katalin Feher,Marton Demeter*

Main category: cs.CY

TL;DR: Generative AI changes knowledge production and validation, causing academic integrity concerns. This study explores 53 academic influencer videos with 5.3 million viewers to develop a pipeline balancing originality, ethics, and human-AI collaboration. Findings show generative AI can automate workflows and democratize participation while challenging norms. Academic influencers are key intermediaries in this shift. The study proposes a generative publication pipeline and policy framework for responsible AI-powered research.


<details>
  <summary>Details</summary>
Motivation: To address the challenges and opportunities brought by generative AI in academic knowledge production, validation, and dissemination, particularly concerning academic integrity and credibility.

Method: Examining 53 academic influencer videos that reached 5.3 million viewers to identify a structured pipeline for balancing originality, ethical compliance, and human-AI collaboration.

Result: Generative AI has potential to automate publication workflows and democratize knowledge production while challenging traditional scientific norms. Academic influencers play a crucial role as intermediaries in this paradigm shift.

Conclusion: The study proposes a generative publication production pipeline and policy framework to support responsible and innovation-driven knowledge production, revealing pathways for automating best practices, optimizing workflows, and fostering creativity in academic research.

Abstract: Generative AI transforms knowledge production, validation, and dissemination,
raising academic integrity and credibility concerns. This study examines 53
academic influencer videos that reached 5.3 million viewers to identify an
emerging, structured, implementation-ready pipeline balancing originality,
ethical compliance, and human-AI collaboration despite the disruptive impacts.
Findings highlight generative AI's potential to automate publication workflows
and democratize participation in knowledge production while challenging
traditional scientific norms. Academic influencers emerge as key intermediaries
in this paradigm shift, connecting bottom-up practices with institutional
policies to improve adaptability. Accordingly, the study proposes a generative
publication production pipeline and a policy framework for co-intelligence
adaptation and reinforcing credibility-centered standards in AI-powered
research. These insights support scholars, educators, and policymakers in
understanding AI's transformative impact by advocating responsible and
innovation-driven knowledge production. Additionally, they reveal pathways for
automating best practices, optimizing scholarly workflows, and fostering
creativity in academic research and publication.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [349] [A survey of using EHR as real-world evidence for discovering and validating new drug indications](https://arxiv.org/abs/2505.24767)
*Nabasmita Talukdar,Xiaodan Zhang,Shreya Paithankar,Hui Wang,Bin Chen*

Main category: stat.AP

TL;DR: The paper explores the use of Electronic Health Records (EHRs) as real-world evidence for drug repurposing, discussing data sources, methodologies, representation techniques, study designs, and statistical frameworks. It highlights challenges in validation and the role of large language models and target trial emulation.


<details>
  <summary>Details</summary>
Motivation: There is a need to effectively utilize Electronic Health Records (EHRs) as real-world evidence to support the discovery and validation of new drug indications through drug repurposing.

Method: The paper surveys current approaches to EHR-based drug repurposing by covering data sources, processing methodologies, representation techniques, study designs, and statistical frameworks for evaluating drug efficacy.

Result: The work synthesizes recent developments and methodological advances in using EHRs for drug repurposing, providing insights into the key challenges in validation including the role of large language models and target trial emulation.

Conclusion: This paper serves as a foundational resource for researchers aiming to leverage real-world data from EHRs into actionable evidence for drug repurposing.

Abstract: Electronic Health Records (EHRs) have been increasingly used as real-world
evidence (RWE) to support the discovery and validation of new drug indications.
This paper surveys current approaches to EHR-based drug repurposing, covering
data sources, processing methodologies, and representation techniques. It
discusses study designs and statistical frameworks for evaluating drug
efficacy. Key challenges in validation are discussed, with emphasis on the role
of large language models (LLMs) and target trial emulation. By synthesizing
recent developments and methodological advances, this work provides a
foundational resource for researchers aiming to translate real-world data into
actionable drug-repurposing evidence.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [350] [Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data](https://arxiv.org/abs/2505.24852)
*Douwe den Blanken,Charlotte Frenkel*

Main category: cs.AR

TL;DR: The paper introduces Chameleon, a system designed for on-device learning that addresses challenges in scalable, low-power end-to-end on-chip learning from sequential data. It achieves high accuracy in few-shot learning and continual learning while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: On-device learning at the edge is crucial for low-latency, private personalization with long-term robustness and reduced maintenance costs. However, current accelerators either prioritize learning performance over inference efficiency or fail to achieve acceptable accuracy.

Method: Chameleon leverages three key contributions: (i) A unified learning and inference architecture supporting few-shot learning, continual learning, and inference with minimal area overhead. (ii) Temporal convolutional networks efficiently capture long temporal dependencies, enabling on-chip learning on sequential data and raw audio. (iii) A dual-mode, matrix-multiplication-free compute array that matches power consumption of state-of-the-art inference-only accelerators or provides higher peak GOPS.

Result: Chameleon sets new accuracy records on Omniglot for end-to-end on-chip few-shot learning (96.8%, 5-way 1-shot; 98.8%, 5-way 5-shot) and continual learning (82.2% final accuracy for learning 250 classes with 10 shots). It also maintains an inference accuracy of 93.3% on the Google Speech Commands dataset at an extreme-edge power budget of 3.1 μW.

Conclusion: Chameleon demonstrates significant advancements in on-device learning by achieving high accuracy in few-shot and continual learning scenarios while maintaining efficient power consumption, making it suitable for edge applications.

Abstract: On-device learning at the edge enables low-latency, private personalization
with improved long-term robustness and reduced maintenance costs. Yet,
achieving scalable, low-power end-to-end on-chip learning, especially from
real-world sequential data with a limited number of examples, is an open
challenge. Indeed, accelerators supporting error backpropagation optimize for
learning performance at the expense of inference efficiency, while simplified
learning algorithms often fail to reach acceptable accuracy targets. In this
work, we present Chameleon, leveraging three key contributions to solve these
challenges. (i) A unified learning and inference architecture supports few-shot
learning (FSL), continual learning (CL) and inference at only 0.5% area
overhead to the inference logic. (ii) Long temporal dependencies are
efficiently captured with temporal convolutional networks (TCNs), enabling the
first demonstration of end-to-end on-chip FSL and CL on sequential data and
inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free
compute array allows either matching the power consumption of state-of-the-art
inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$
higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records
on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way
5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),
while maintaining an inference accuracy of 93.3% on the 12-class Google Speech
Commands dataset at an extreme-edge power budget of 3.1 $\mu$W.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [351] [A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study](https://arxiv.org/abs/2505.24006)
*Agnideep Aich,Sameera Hewage,Md Monzur Murshed,Ashit Baran Aich,Amanda Mayeaux,Asim K. Dey,Kumer P. Das,Bruce Wade*

Main category: stat.ME

TL;DR: The paper introduces A2-SBNN, a spatial model using Archimedean copula in weight initialization for modeling complex spatial relationships and extreme dependencies. It is trained with Wasserstein loss, moment matching, and correlation penalties.


<details>
  <summary>Details</summary>
Motivation: To develop a predictive spatial model that can map coordinates to continuous fields while capturing both typical spatial patterns and extreme dependencies beyond traditional Gaussian-based approaches.

Method: Introduce A2 Copula Spatial Bayesian Neural Network (A2-SBNN) which embeds the dual-tail novel Archimedean copula directly into the network's weight initialization. The model is trained through a calibration-driven process combining Wasserstein loss, moment matching, and correlation penalties.

Result: Simulation results indicate that A2-SBNN consistently delivers high accuracy across a wide range of dependency strengths.

Conclusion: A2-SBNN offers a new, effective solution for spatial data modeling.

Abstract: In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network
(A2-SBNN), a predictive spatial model designed to map coordinates to continuous
fields while capturing both typical spatial patterns and extreme dependencies.
By embedding the dual-tail novel Archimedean copula viz. A2 directly into the
network's weight initialization, A2-SBNN naturally models complex spatial
relationships, including rare co-movements in the data. The model is trained
through a calibration-driven process combining Wasserstein loss, moment
matching, and correlation penalties to refine predictions and manage
uncertainty. Simulation results show that A2-SBNN consistently delivers high
accuracy across a wide range of dependency strengths, offering a new, effective
solution for spatial data modeling beyond traditional Gaussian-based
approaches.

</details>


### [352] [Data Fusion for Partial Identification of Causal Effects](https://arxiv.org/abs/2505.24296)
*Quinn Lanners,Cynthia Rudin,Alexander Volfovsky,Harsh Parikh*

Main category: stat.ME

TL;DR: 在因果推断中，提出了一个新的部分识别框架，通过可解释的敏感性参数量化假设违反情况，并得出因果效应界限，应用该框架于Project STAR研究中发现其结果对关键假设的同时违反具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据融合技术在因果推断中无法应对同时存在未观测混杂和反事实结果不可交换的情况，因此需要一种新方法来解决这一问题。

Method: 提出了一种新的部分识别框架，引入了可解释的敏感性参数以量化假设违反的程度，并推导出相应的因果效应界限。还开发了双重稳健估计器，并将断裂前沿分析操作化，以了解随着假设违反程度增加因果结论的变化。

Result: 将该框架应用于Project STAR研究中，发现其结果在平均值上以及在各个感兴趣的子群体中都对关键假设的同时违反具有鲁棒性。

Conclusion: 该研究增强了对Project STAR研究结论的信心，即使数据可能存在未测量的偏差。

Abstract: Data fusion techniques integrate information from heterogeneous data sources
to improve learning, generalization, and decision making across data sciences.
In causal inference, these methods leverage rich observational data to improve
causal effect estimation, while maintaining the trustworthiness of randomized
controlled trials. Existing approaches often relax the strong no unobserved
confounding assumption by instead assuming exchangeability of counterfactual
outcomes across data sources. However, when both assumptions simultaneously
fail - a common scenario in practice - current methods cannot identify or
estimate causal effects. We address this limitation by proposing a novel
partial identification framework that enables researchers to answer key
questions such as: Is the causal effect positive or negative? and How severe
must assumption violations be to overturn this conclusion? Our approach
introduces interpretable sensitivity parameters that quantify assumption
violations and derives corresponding causal effect bounds. We develop doubly
robust estimators for these bounds and operationalize breakdown frontier
analysis to understand how causal conclusions change as assumption violations
increase. We apply our framework to the Project STAR study, which investigates
the effect of classroom size on students' third-grade standardized test
performance. Our analysis reveals that the Project STAR results are robust to
simultaneous violations of key assumptions, both on average and across various
subgroups of interest. This strengthens confidence in the study's conclusions
despite potential unmeasured biases in the data.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [353] [Searching Clinical Data Using Generative AI](https://arxiv.org/abs/2505.24090)
*Karan Hanswadkar,Anika Kanchi,Shivani Tripathi,Shi Qiao,Rony Chatterjee,Alekh Jindal*

Main category: cs.DB

TL;DR: The paper introduces SearchAI, a generative AI approach that enhances clinical data search accuracy and efficiency through a hierarchical model. It outperforms traditional methods in metrics like accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Clinical data search in large datasets is still manual and error-prone, creating a need for automation to improve physicians' operational productivity.

Method: SearchAI uses a hierarchical model respecting the coding hierarchy to handle one-to-many search problems, ensuring all paths are reachable without losing relevant nodes.

Result: Experiments on public and production datasets show SearchAI outperforms default hierarchical traversals in accuracy, robustness, performance, and scalability.

Conclusion: SearchAI can make clinical data more accessible, streamlining workflows, reducing administrative burden, and enhancing coding and diagnostic accuracy.

Abstract: Artificial Intelligence (AI) is making a major impact on healthcare,
particularly through its application in natural language processing (NLP) and
predictive analytics. The healthcare sector has increasingly adopted AI for
tasks such as clinical data analysis and medical code assignment. However,
searching for clinical information in large and often unorganized datasets
remains a manual and error-prone process. Assisting this process with
automations can help physicians improve their operational productivity
significantly.
  In this paper, we present a generative AI approach, coined SearchAI, to
enhance the accuracy and efficiency of searching clinical data. Unlike
traditional code assignment, which is a one-to-one problem, clinical data
search is a one-to-many problem, i.e., a given search query can map to a family
of codes. Healthcare professionals typically search for groups of related
diseases, drugs, or conditions that map to many codes, and therefore, they need
search tools that can handle keyword synonyms, semantic variants, and broad
open-ended queries. SearchAI employs a hierarchical model that respects the
coding hierarchy and improves the traversal of relationships from parent to
child nodes. SearchAI navigates these hierarchies predictively and ensures that
all paths are reachable without losing any relevant nodes.
  To evaluate the effectiveness of SearchAI, we conducted a series of
experiments using both public and production datasets. Our results show that
SearchAI outperforms default hierarchical traversals across several metrics,
including accuracy, robustness, performance, and scalability. SearchAI can help
make clinical data more accessible, leading to streamlined workflows, reduced
administrative burden, and enhanced coding and diagnostic accuracy.

</details>


### [354] [Towards Scalable Schema Mapping using Large Language Models](https://arxiv.org/abs/2505.24716)
*Christopher Buss,Mahdis Safari,Arash Termehchy,Stefan Lee,David Maier*

Main category: cs.DB

TL;DR: The paper explores challenges and proposes solutions for using LLMs in schema mapping within data integration systems.


<details>
  <summary>Details</summary>
Motivation: To address scalability challenges in data integration systems caused by reliance on manually written schema mappings, which are complex, source-specific, and costly to maintain.

Method: Identifying three core issues with using LLMs for schema mapping and proposing methods such as sampling and aggregation techniques, more expressive mappings (e.g., GLaV), and strategies like data type prefiltering.

Result: Provides approaches to handle inconsistent outputs, limited context windows, and high computational costs when using LLMs for schema mapping.

Conclusion: Using LLMs for schema mapping in data integration systems is promising but requires addressing key challenges related to output consistency, mapping expressiveness, and computational efficiency.

Abstract: The growing need to integrate information from a large number of diverse
sources poses significant scalability challenges for data integration systems.
These systems often rely on manually written schema mappings, which are
complex, source-specific, and costly to maintain as sources evolve. While
recent advances suggest that large language models (LLMs) can assist in
automating schema matching by leveraging both structural and natural language
cues, key challenges remain. In this paper, we identify three core issues with
using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to
input phrasing and structure, which we propose methods to address through
sampling and aggregation techniques; (2) the need for more expressive mappings
(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the
computational cost of repeated LLM calls, which we propose to mitigate through
strategies like data type prefiltering.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [355] [Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System](https://arxiv.org/abs/2505.24429)
*Giovanny C-Londoño,Javier Sánchez,Ángel Rodríguez-Santana*

Main category: physics.ao-ph

TL;DR: 通过适应最初为全球天气预报开发的图神经网络，本研究提高了次区域海洋预测的准确性，特别是在加那利洋流涌升系统中。尽管在涌升区存在一些挑战，但该深度学习模型在降低RMSE误差方面优于ConvLSTM和GLORYS再分析，特别是在复杂海洋动力学区域（如Ghir海角、Bojador海角和Blanc海角）。该模型在5天预测中实现了高达26.5%的改进和高达76%的误差减少，证明了将气象数据驱动模型用于改进次区域中期海洋预报的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于全球循环模型的海洋预测方法计算成本高且速度慢，限制了快速预测的能力。虽然深度学习模型提供了更快、更准确的预测，但这些数据驱动模型通常使用来自数值模拟的全球数据进行训练，可能无法反映现实。此外，它们对细尺度海洋过程（如中尺度结构）的预测能力尚不清楚。因此，有必要探索并改进适用于次区域海洋预测的深度学习模型。

Method: 本研究采用了一种最初为全球天气预报开发的图神经网络，并将其应用于次区域海洋预测，特别是加那利洋流涌升系统。模型使用卫星数据进行训练，并与最先进的物理海洋模型进行比较，以评估其捕捉海洋动力学的能力。

Result: 结果显示，深度学习模型在精度上超过了传统方法，尽管在涌升区存在一些挑战。该模型在降低均方根误差（RMSE）方面表现出色，特别是在复杂海洋动力学区域（如Ghir海角、Bojador海角和Blanc海角），相对ConvLSTM的改进高达26.5%，相对于GLORYS再分析的误差减少高达76%（5天预测）。

Conclusion: 本研究证明了适应气象数据驱动模型以改善次区域中期海洋预报的可行性。这种图神经网络模型在复杂海洋动力学区域显示出增强的空间变异性捕捉能力和更高的预测准确性。

Abstract: Oceanographic forecasting impacts various sectors of society by supporting
environmental conservation and economic activities. Based on global circulation
models, traditional forecasting methods are computationally expensive and slow,
limiting their ability to provide rapid forecasts. Recent advances in deep
learning offer faster and more accurate predictions, although these data-driven
models are often trained with global data from numerical simulations, which may
not reflect reality. The emergence of such models presents great potential for
improving ocean prediction at a subregional domain. However, their ability to
predict fine-scale ocean processes, like mesoscale structures, remains largely
unknown. This work aims to adapt a graph neural network initially developed for
global weather forecasting to improve subregional ocean prediction,
specifically focusing on the Canary Current upwelling system. The model is
trained with satellite data and compared to state-of-the-art physical ocean
models to assess its performance in capturing ocean dynamics. Our results show
that the deep learning model surpasses traditional methods in precision despite
some challenges in upwelling areas. It demonstrated superior performance in
reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis,
particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape
Bojador, and Cape Blanc. The model achieved improvements of up to 26.5%
relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts
compared to the GLORYS reanalysis at these critical locations, highlighting its
enhanced capability to capture spatial variability and improve predictive
accuracy in complex areas. These findings suggest the viability of adapting
meteorological data-driven models for improving subregional medium-term ocean
forecasting.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [356] [Aligning Protein Conformation Ensemble Generation with Physical Feedback](https://arxiv.org/abs/2505.24203)
*Jiarui Lu,Xiaoyin Chen,Stephen Zhewen Lu,Aurélie Lozano,Vijil Chenthamarakshan,Payel Das,Jian Tang*

Main category: q-bio.BM

TL;DR: 本研究提出了一种名为Energy-based Alignment (EBA)的新方法，通过结合生成模型和物理模型的反馈，有效提高了生成高质量蛋白质结构集合的能力，在分子动力学集成基准测试中表现出色，为结构生物学和药物发现提供了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质动力学对其生物功能和特性至关重要，传统研究依赖耗时的分子动力学(MD)模拟。尽管生成模型（如去噪扩散模型）在预测蛋白质结构和构象采样方面取得了进展，但如何有效地将物理监督整合到这些数据驱动的方法中仍然是一个挑战。

Method: 提出了Energy-based Alignment (EBA)方法，该方法将生成模型与物理模型的反馈对齐，通过能量差异高效校准以平衡构象状态。

Result: 实验结果表明，EBA在MD集成基准上生成高质量蛋白质集合的表现达到了最先进的水平，提升了生成结构的物理合理性。

Conclusion: EBA方法通过改进生成结构的物理合理性，增强了模型预测能力，展示了在结构生物学和药物发现中的应用潜力。

Abstract: Protein dynamics play a crucial role in protein biological functions and
properties, and their traditional study typically relies on time-consuming
molecular dynamics (MD) simulations conducted in silico. Recent advances in
generative modeling, particularly denoising diffusion models, have enabled
efficient accurate protein structure prediction and conformation sampling by
learning distributions over crystallographic structures. However, effectively
integrating physical supervision into these data-driven approaches remains
challenging, as standard energy-based objectives often lead to intractable
optimization. In this paper, we introduce Energy-based Alignment (EBA), a
method that aligns generative models with feedback from physical models,
efficiently calibrating them to appropriately balance conformational states
based on their energy differences. Experimental results on the MD ensemble
benchmark demonstrate that EBA achieves state-of-the-art performance in
generating high-quality protein ensembles. By improving the physical
plausibility of generated structures, our approach enhances model predictions
and holds promise for applications in structural biology and drug discovery.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [357] [A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors](https://arxiv.org/abs/2505.23862)
*Zheng Gong,Ziyi Jiang,Weihao Gao,Deng Zhuo,Lan Ma*

Main category: q-bio.QM

TL;DR: The paper introduces RNop, a deep learning-based method for mRNA optimization that addresses current limitations in fidelity, computational efficiency, and multi-objective capability.


<details>
  <summary>Details</summary>
Motivation: Current mRNA optimization methods struggle to achieve fidelity, computational efficiency, and comprehensive consideration of optimization variables simultaneously. They also often lack incorporation of factors related to the mRNA lifecycle and translation process.

Method: The authors developed RNop using a large-scale dataset of over 3 million sequences and designed four specialized loss functions (GPLoss, CAILoss, tAILoss, MFELoss) to control sequence fidelity while optimizing codon adaptation, tRNA availability, and mRNA secondary structure features.

Result: RNop ensures high sequence fidelity, achieves significant computational throughput (47.32 sequences/s), and increases protein expression levels in functional proteins compared to controls. It surpasses existing methodologies in both quantitative metrics and experimental validation.

Conclusion: RNop represents a new approach for efficient and effective mRNA design, with potential for therapeutic and biotechnological applications.

Abstract: The mRNA optimization is critical for therapeutic and biotechnological
applications, since sequence features directly govern protein expression levels
and efficacy. However, current methods face significant challenges in
simultaneously achieving three key objectives: (1) fidelity (preventing
unintended amino acid changes), (2) computational efficiency (speed and
scalability), and (3) the scope of optimization variables considered
(multi-objective capability). Furthermore, existing methods often fall short of
comprehensively incorporating the factors related to the mRNA lifecycle and
translation process, including intrinsic mRNA sequence properties, secondary
structure, translation elongation kinetics, and tRNA availability. To address
these limitations, we introduce \textbf{RNop}, a novel deep learning-based
method for mRNA optimization. We collect a large-scale dataset containing over
3 million sequences and design four specialized loss functions, the GPLoss,
CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control
over sequence fidelity while optimizing species-specific codon adaptation, tRNA
availability, and desirable mRNA secondary structure features. Then, we
demonstrate RNop's effectiveness through extensive in silico and in vivo
experiments. RNop ensures high sequence fidelity, achieves significant
computational throughput up to 47.32 sequences/s, and yields optimized mRNA
sequences resulting in a significant increase in protein expression for
functional proteins compared to controls. RNop surpasses current methodologies
in both quantitative metrics and experimental validation, enlightening a new
dawn for efficient and effective mRNA design. Code and models will be available
at https://github.com/HudenJear/RPLoss.

</details>


### [358] [Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport](https://arxiv.org/abs/2505.24759)
*Mu Qiao*

Main category: q-bio.QM

TL;DR: The paper presents OT-MESH, an unsupervised computational framework using entropy-regularized optimal transport to identify cross-species cell type homologies. It accurately recovers known evolutionary relationships and uncovers novel correspondences.


<details>
  <summary>Details</summary>
Motivation: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches have limitations such as asymmetry or increased computational complexity.

Method: OT-MESH uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan. It selects genes with high Signal-to-Noise Ratio (SNR) to capture the most informative features, constructs a cost matrix using cosine distances between cell-type centroids, and iteratively refines the cost matrix for enhanced sparsity and interpretability.

Result: Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally.

Conclusion: OT-MESH offers a principled, scalable, symmetric, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.

Abstract: Identifying evolutionary correspondences between cell types across species is
a fundamental challenge in comparative genomics and evolutionary biology.
Existing approaches often rely on either reference-based matching, which
imposes asymmetry by designating one species as the reference, or
projection-based matching, which may increase computational complexity and
obscure biological interpretability at the cell-type level. Here, we present
OT-MESH, an unsupervised computational framework leveraging entropy-regularized
optimal transport (OT) to systematically determine cross-species cell type
homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn
(MESH) technique to refine the OT plan. It begins by selecting genes with high
Signal-to-Noise Ratio (SNR) to capture the most informative features, from
which a cost matrix is constructed using cosine distances between cell-type
centroids. Importantly, the MESH procedure iteratively refines the cost matrix,
leading to a transport plan with significantly enhanced sparsity and
interpretability of the resulting correspondence matrices. Applied to retinal
bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque,
OT-MESH accurately recovers known evolutionary relationships and uncovers novel
correspondences, one of which was independently validated experimentally. Thus,
our framework offers a principled, scalable, symmetric, and interpretable
solution for evolutionary cell type mapping, facilitating deeper insights into
cellular specialization and conservation across species.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [359] [Beyond the Black Box: Interpretability of LLMs in Finance](https://arxiv.org/abs/2505.24650)
*Hariom Tatsat,Ariye Shater*

Main category: cs.CE

TL;DR: 大型语言模型（LLMs）在金融服务领域表现出色，但透明度不足。本文首次通过机械可解释性研究LLMs在金融领域的应用，探讨其理论与实践意义，并强调了其在监管和合规方面的作用。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在金融领域有广泛的应用，但其复杂性和缺乏透明度成为挑战，特别是在高度监管的环境中。因此，需要一种方法来理解并控制这些模型的行为，以确保它们符合伦理、透明且与法规一致。

Method: 通过机械可解释性（Mechanistic Interpretability），即逆向工程LLMs的内部工作原理，分析激活和电路，从而了解特定特征或组件如何影响预测，并可能调整模型行为。

Result: 展示了机械可解释性在多个金融用例中的实际相关性，包括交易策略、情感分析、偏差和幻觉检测等，并强调其在未来AI系统中保持伦理和透明的重要性。

Conclusion: 随着LLMs的普及，机械可解释性将变得越来越重要，它可以帮助满足当前和未来的监管需求，确保AI系统与金融法规保持一致。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a
spectrum of tasks in financial services, including report generation, chatbots,
sentiment analysis, regulatory compliance, investment advisory, financial
knowledge retrieval, and summarization. However, their intrinsic complexity and
lack of transparency pose significant challenges, especially in the highly
regulated financial sector, where interpretability, fairness, and
accountability are critical. As far as we are aware, this paper presents the
first application in the finance domain of understanding and utilizing the
inner workings of LLMs through mechanistic interpretability, addressing the
pressing need for transparency and control in AI systems. Mechanistic
interpretability is the most intuitive and transparent way to understand LLM
behavior by reverse-engineering their internal workings. By dissecting the
activations and circuits within these models, it provides insights into how
specific features or components influence predictions - making it possible not
only to observe but also to modify model behavior. In this paper, we explore
the theoretical aspects of mechanistic interpretability and demonstrate its
practical relevance through a range of financial use cases and experiments,
including applications in trading strategies, sentiment analysis, bias, and
hallucination detection. While not yet widely adopted, mechanistic
interpretability is expected to become increasingly vital as adoption of LLMs
increases. Advanced interpretability tools can ensure AI systems remain
ethical, transparent, and aligned with evolving financial regulations. In this
paper, we have put special emphasis on how these techniques can help unlock
interpretability requirements for regulatory and compliance purposes -
addressing both current needs and anticipating future expectations from
financial regulators globally.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [360] [When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds](https://arxiv.org/abs/2505.24336)
*Minsu Kang,Seolhee Lee,Choonghyeon Lee,Namhyun Cho*

Main category: eess.AS

TL;DR: This paper introduces a preprocessing pipeline and an improved CVAE-based H2NH-VC model for transforming human speech into a broader range of non-speech sounds, including natural sounds and designed voice, with high-quality audio transformation at 44.1kHz.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of prior studies that focused on dog-sounds and lower quality audio transformation, and to accommodate generation of diverse non-speech sounds and high-quality audio transformation.

Method: Introduced a preprocessing pipeline and an improved CVAE-based H2NH-VC model optimized for both human and non-human voices.

Result: The proposed method outperformed baselines in quality, naturalness, and similarity MOS, achieving effective voice conversion across diverse non-human timbres.

Conclusion: The introduced preprocessing pipeline and improved CVAE-based H2NH-VC model successfully transform human speech into a broader range of non-speech sounds with high-quality audio.

Abstract: Human to non-human voice conversion (H2NH-VC) transforms human speech into
animal or designed vocalizations. Unlike prior studies focused on dog-sounds
and 16 or 22.05kHz audio transformation, this work addresses a broader range of
non-speech sounds, including natural sounds (lion-roars, birdsongs) and
designed voice (synthetic growls). To accomodate generation of diverse
non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a
preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized
for human and non-human voices. Experimental results showed that the proposed
method outperformed baselines in quality, naturalness, and similarity MOS,
achieving effective voice conversion across diverse non-human timbres. Demo
samples are available at
https://nc-ai.github.io/speech/publications/nonhuman-vc/

</details>


### [361] [Can Emotion Fool Anti-spoofing?](https://arxiv.org/abs/2505.23962)
*Aurosweta Mahapatra,Ismail Rasim Ulgen,Abinay Reddy Naini,Carlos Busso,Berrak Sisman*

Main category: eess.AS

TL;DR: The paper introduces EmoSpoof-TTS, a corpus of emotional text-to-speech samples, revealing existing anti-spoofing models' weakness against emotional synthetic speech and proposing GEM, an emotion-focused anti-spoofing model.


<details>
  <summary>Details</summary>
Motivation: Traditional anti-spoofing methods mainly focus on neutral synthetic speech and neglect diverse emotional variations, leading to uncertain robustness against high-quality, emotionally expressive synthetic speech.

Method: Introduced EmoSpoof-TTS corpus for analysis and proposed GEM, a gated ensemble of emotion-specialized models with a speech emotion recognition gating network.

Result: Existing anti-spoofing models struggle with emotional synthetic speech even when trained on emotional data, while GEM performs effectively across all emotions and neutral state.

Conclusion: There is a need for emotion-focused anti-spoofing paradigm in both dataset and methodology, and GEM improves defenses against spoofing attacks.

Abstract: Traditional anti-spoofing focuses on models and datasets built on synthetic
speech with mostly neutral state, neglecting diverse emotional variations. As a
result, their robustness against high-quality, emotionally expressive synthetic
speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of
emotional text-to-speech samples. Our analysis shows existing anti-spoofing
models struggle with emotional synthetic speech, exposing risks of
emotion-targeted attacks. Even trained on emotional data, the models
underperform due to limited focus on emotional aspect and show performance
disparities across emotions. This highlights the need for emotion-focused
anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated
ensemble of emotion-specialized models with a speech emotion recognition gating
network. GEM performs effectively across all emotions and neutral state,
improving defenses against spoofing attacks. We release the EmoSpoof-TTS
Dataset: https://emospoof-tts.github.io/Dataset/

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [362] [Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition](https://arxiv.org/abs/2505.24001)
*Wonjun Yi,Wonho Jung,Kangmin Jang,Yong-Hwa Park*

Main category: eess.SP

TL;DR: The paper proposes a novel multi-output classification (MOC) framework for domain adaptation in fault diagnosis of rotating machinery with partially labeled target datasets, focusing on compound faults. It introduces a cross-talk layer structure and frequency-layer normalization to enhance classification performance.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of rotating machinery and the diversity of operating conditions have made fault diagnosis more challenging, especially in scenarios requiring domain adaptation and involving compound faults.

Method: The study develops a MOC framework tailored for domain adaptation in partially labeled target datasets. It explores various single-task and multi-task architectures applicable to the MOC formulation, including shared trunk and cross-talk-based designs. A novel cross-talk layer structure is proposed to enable selective information sharing across diagnostic tasks. Frequency-layer normalization is incorporated to improve domain adaptation performance on motor vibration data.

Result: The experimental results demonstrate superior macro F1 performance compared to baseline models across six domain adaptation scenarios. The proposed model's structural advantage is more pronounced in compound fault settings compared to single-fault conditions. Frequency-layer normalization fits the fault diagnosis task better than conventional methods.

Conclusion: The improvement in fault diagnosis primarily stems from the model's structural ability to leverage inter-fault classification task interactions, rather than simply increasing model parameters.

Abstract: The increasing complexity of rotating machinery and the diversity of
operating conditions, such as rotating speed and varying torques, have
amplified the challenges in fault diagnosis in scenarios requiring domain
adaptation, particularly involving compound faults. This study addresses these
challenges by introducing a novel multi-output classification (MOC) framework
tailored for domain adaptation in partially labeled (PL) target datasets.
Unlike conventional multi-class classification (MCC) approaches, the proposed
MOC framework classifies the severity levels of compound faults simultaneously.
Furthermore, we explore various single-task and multi-task architectures
applicable to the MOC formulation-including shared trunk and cross-talk-based
designs-for compound fault diagnosis under PL conditions. Based on this
investigation, we propose a novel cross-talk layer structure that enables
selective information sharing across diagnostic tasks, effectively enhancing
classification performance in compound fault scenarios. In addition,
frequency-layer normalization was incorporated to improve domain adaptation
performance on motor vibration data. Compound fault conditions were implemented
using a motor-based test setup, and the proposed model was evaluated across six
domain adaptation scenarios. The experimental results demonstrate its superior
macro F1 performance compared to baseline models. We further showed that the
proposed mode's structural advantage is more pronounced in compound fault
settings through a single-fault comparison. We also found that frequency-layer
normalization fits the fault diagnosis task better than conventional methods.
Lastly, we discuss that this improvement primarily stems from the model's
structural ability to leverage inter-fault classification task interactions,
rather than from a simple increase in model parameters.

</details>


### [363] [Real-time Fall Prevention system for the Next-generation of Workers](https://arxiv.org/abs/2505.24487)
*Nicholas Cartocci,Antonios E. Gkikakis,Darwin G. Caldwell,Jesús Ortiz*

Main category: eess.SP

TL;DR: This paper proposes a hybrid approach for fall detection and prevention using inverted pendulum simulations and deep learning, aiming to reduce fall-related injuries in industrial settings.


<details>
  <summary>Details</summary>
Motivation: Developing an effective wearable real-time fall-detection system is challenging, particularly for healthy individuals in harsh environments. Traditional methods struggle to generate sufficient training data for diverse scenarios.

Method: A hybrid approach combining an inverted pendulum dynamic model to simulate falls and a deep learning framework to process these simulations. The system outputs signals to activate fall mitigation mechanisms when necessary.

Result: The method can efficiently generate training data for numerous subjects under various initial conditions, addressing the limitation of real-world experiments. It targets specific types of falls where initial configurations remain largely unchanged.

Conclusion: This hybrid approach represents a significant step towards creating a general-purpose wearable device for reducing fall-associated injuries in industrial environments, enhancing worker safety.

Abstract: Developing a general-purpose wearable real-time fall-detection system is
still a challenging task, especially for healthy and strong subjects, such as
industrial workers that work in harsh environments. In this work, we present a
hybrid approach for fall detection and prevention, which uses the dynamic model
of an inverted pendulum to generate simulations of falling that are then fed to
a deep learning framework. The output is a signal to activate a fall mitigation
mechanism when the subject is at risk of harm. The advantage of this approach
is that abstracted models can be used to efficiently generate training data for
thousands of different subjects with different falling initial conditions,
something that is practically impossible with real experiments. This approach
is suitable for a specific type of fall, where the subjects fall without
changing their initial configuration significantly, and it is the first step
toward a general-purpose wearable device, with the aim of reducing
fall-associated injuries in industrial environments, which can improve the
safety of workers.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [364] [Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM](https://arxiv.org/abs/2505.24062)
*Kamyar Barakati,Yu Liu,Hiroshi Funakubo,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: This paper uses ML-controlled automated Piezoresponse Force Microscopy to study the electric field driven dynamics of polar-strain domain structures in a large area epitaxial PbTiO3 film on cubic KTaO3. Analysis of 1500 switching events reveals that domain wall displacement depends not only on field parameters but also on the local ferroelectric-ferroelastic configuration.


<details>
  <summary>Details</summary>
Motivation: Domain-wall dynamics in ferroelectric materials are strongly position-dependent, necessitating spatially resolved studies of the wall-pinning using scanning-probe microscopy techniques.

Method: Investigation of a large area epitaxial PbTiO3 film on cubic KTaO3 using ML-controlled automated Piezoresponse Force Microscopy to quantify the electric field driven dynamics of the polar-strain domain structures.

Result: Analysis of 1500 switching events reveals that domain wall displacement depends not only on field parameters but also on the local ferroelectric-ferroelastic configuration.

Conclusion: The statistics on the possible ferroelectric and ferroelastic wall orientations, together with the automated, high-throughput AFM workflow, can be distilled into a predictive map that links domain configurations to pulse parameters, forming the foundation for designing ferroelectric memories.

Abstract: Domain-wall dynamics in ferroelectric materials are strongly
position-dependent since each polar interface is locked into a unique local
microstructure. This necessitates spatially resolved studies of the
wall-pinning using scanning-probe microscopy techniques. The pinning centers
and preexisting domain walls are usually sparse within image plane, precluding
the use of dense hyperspectral imaging modes and requiring time-consuming human
experimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$
were investigated to quantify the electric field driven dynamics of the
polar-strain domain structures using ML-controlled automated Piezoresponse
Force Microscopy. Analysis of 1500 switching events reveals that domain wall
displacement depends not only on field parameters but also on the local
ferroelectric-ferroelastic configuration. For example, twin boundaries in
polydomains regions like a$_1^-$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stay pinned up
to a certain level of bias magnitude and change only marginally as the bias
increases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$
$\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics
on the possible ferroelectric and ferroelastic wall orientations, together with
the automated, high-throughput AFM workflow, can be distilled into a predictive
map that links domain configurations to pulse parameters. This
microstructure-specific rule set forms the foundation for designing
ferroelectric memories.

</details>


### [365] [Cross-Modal Characterization of Thin Film MoS$_2$ Using Generative Models](https://arxiv.org/abs/2505.24065)
*Isaiah A. Moses,Chen Chen,Joan M. Redwing,Wesley F. Reinhart*

Main category: cond-mat.mtrl-sci

TL;DR: The paper explores using machine learning to project quantitative metrics from one characterization method to another, speeding up material growth and characterization. Generative models were trained to translate between Raman spectroscopy, photoluminescence spectra, and AFM images of MoS$_2$ thin films, with promising results.


<details>
  <summary>Details</summary>
Motivation: Material growth and characterization typically require significant expert time, experience, and resources. Conventional methods involve multiple characterization techniques to determine sample quality and properties. Machine learning could enhance these processes by leveraging historical data to provide speed and efficiency.

Method: Investigated projecting quantitative metric from microscopy measurements (AFM) using spectroscopy data (Raman). Trained generative models to generate features of Raman and photoluminescence spectra from each other and AFM images of MoS$_2$ thin films.

Result: Generative models successfully translated between different modalities (Raman, photoluminescence, AFM). The approach provided a foundational guide for using ML in cross-modal material characterization.

Conclusion: Machine learning, particularly generative models, shows promise in accelerating, enhancing efficiency, and reducing costs in material characterization through cross-modal data translation.

Abstract: The growth and characterization of materials using empirical optimization
typically requires a significant amount of expert time, experience, and
resources. Several complementary characterization methods are routinely
performed to determine the quality and properties of a grown sample. Machine
learning (ML) can support the conventional approaches by using historical data
to guide and provide speed and efficiency to the growth and characterization of
materials. Specifically, ML can provide quantitative information from
characterization data that is typically obtained from a different modality. In
this study, we have investigated the feasibility of projecting the quantitative
metric from microscopy measurements, such as atomic force microscopy (AFM),
using data obtained from spectroscopy measurements, like Raman spectroscopy.
Generative models were also trained to generate the full and specific features
of the Raman and photoluminescence spectra from each other and the AFM images
of the thin film MoS$_2$. The results are promising and have provided a
foundational guide for the use of ML for the cross-modal characterization of
materials for their accelerated, efficient, and cost-effective discovery.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [366] [Information-theoretic machine learning for time-varying mode decomposition of separated airfoil wakes](https://arxiv.org/abs/2505.24132)
*Kai Fukami,Ryo Araki*

Main category: physics.flu-dyn

TL;DR: An information-theoretic mode decomposition method using deep sigmoidal flow neural network is developed to extract causality-based insights from separated wakes around a wing for different flow scenarios.


<details>
  <summary>Details</summary>
Motivation: To uncover causality-based insights into unsteady aerodynamic problems by extracting informative components from flow field data with respect to future target variables.

Method: The method employs an information-theoretic mode decomposition based on a deep sigmoidal flow neural network. It processes flow field snapshots to extract components informative of a target variable at a future time, revealing time-varying modal structures.

Result: Informative vortical structures associated with time-varying lift responses are identified in three examples: laminar periodic wake, strong vortex-airfoil interactions, and turbulent wake. The model highlights structures near the wing and vortex cores without prior aerodynamic knowledge.

Conclusion: This study successfully reveals causality-based insights into various unsteady aerodynamic problems through the developed data-driven approach.

Abstract: We perform an information-theoretic mode decomposition for separated wakes
around a wing. The current data-driven approach based on a neural network
referred to as deep sigmoidal flow enables the extraction of an informative
component from a given flow field snapshot with respect to a target variable at
a future time stamp, thereby capturing the causality as a time-varying modal
structure. We consider three examples of separated flows around a NACA0012
airfoil, namely, 1. laminar periodic wake at post-stall angles of attack, 2.
strong vortex-airfoil interactions, and 3. a turbulent wake in a
spanwise-periodic domain. The present approach reveals informative vortical
structures associated with a time-varying lift response. For the periodic
shedding cases, the informative structures vary in time corresponding to the
fluctuation level from their mean values. With the second example of
vortex-airfoil interactions, how the effect of vortex gust on a wing emerges in
the lift response over time is identified in an interpretable manner.
Furthermore, for the case of turbulent wake, the present model highlights
structures near the wing and vortex cores as informative components based
solely on the information metric without any prior knowledge of aerodynamics
and length scales. This study provides causality-based insights into a range of
unsteady aerodynamic problems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [367] [Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach](https://arxiv.org/abs/2505.23953)
*Melika Sepidband,Hamed Taherkhani,Song Wang,Hadi Hemmati*

Main category: cs.SE

TL;DR: The paper investigates the relationship between code complexity and success of LLM-generated code, proposing an iterative feedback method based on complexity metrics to improve failed cases.


<details>
  <summary>Details</summary>
Motivation: While many studies focus on improving LLMs for code generation, there's limited work on understanding the characteristics of generated code and leveraging that knowledge to address failures.

Method: Using standard complexity metrics, the authors conduct an empirical analysis correlating these metrics with LLM performance in code generation. Logistic regression identifies predictive metrics, which inform an iterative feedback loop prompting LLMs to generate correct code based on prior failures.

Result: Experiments across multiple benchmarks (HumanEval, MBPP, LeetCode, BigCodeBench) and various LLMs show significant improvements, particularly with smaller models like GPT-3.5 Turbo, where Pass@1 increased by 35.71% compared to a baseline improvement of 12.5%. Integrating the method with Reflexion led to further enhancements.

Conclusion: Complexity-aware feedback improves both direct LLM prompting and agent-based workflows, offering notable performance boosts, especially for smaller models.

Abstract: Automatic code generation has gained significant momentum with the advent of
Large Language Models (LLMs) such as GPT-4. Although many studies focus on
improving the effectiveness of LLMs for code generation, very limited work
tries to understand the generated code's characteristics and leverage that to
improve failed cases. In this paper, as the most straightforward characteristic
of code, we investigate the relationship between code complexity and the
success of LLM generated code. Using a large set of standard complexity
metrics, we first conduct an empirical analysis to explore their correlation
with LLM's performance on code generation (i.e., Pass@1). Using logistic
regression models, we identify which complexity metrics are most predictive of
code correctness. Building on these findings, we propose an iterative feedback
method, where LLMs are prompted to generate correct code based on complexity
metrics from previous failed outputs. We validate our approach across multiple
benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs
(i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the
results with two baseline methods: (a) zero-shot generation, and (b) iterative
execution-based feedback without our code complexity insights. Experiment
results show that our approach makes notable improvements, particularly with a
smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to
the baseline's improvement of 12.5% on the HumanEval dataset. The study expands
experiments to BigCodeBench and integrates the method with the Reflexion code
generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07%
(GPT-o3 mini). The results highlight that complexity-aware feedback enhances
both direct LLM prompting and agent-based workflows.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [368] [Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning](https://arxiv.org/abs/2505.23783)
*Korel Gundem,Juncheng Dong,Dennis Zhang,Vahid Tarokh,Zhengling Qi*

Main category: stat.ML

TL;DR: Supervised Calibration (SC) is proposed to address the limitations of current calibration methods in In-Context Learning (ICL) by learning an optimal, per-class affine transformation of LLM's predictive probabilities.


<details>
  <summary>Details</summary>
Motivation: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new tasks with just a few examples. However, their predictions often suffer from systematic biases leading to unstable performances in classification.

Method: The authors propose Supervised Calibration (SC), a loss-minimization based framework which learns an optimal, per-class affine transformation of the LLM's predictive probabilities in the logit space without requiring external data beyond the context.

Result: SC delivers state-of-the-art performance over calibration baselines in the 4-shot, 8-shot, and 16-shot settings across all nine datasets for Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.

Conclusion: Supervised Calibration (SC) addresses the limitations of current calibration methods in ICL and provides a unifying framework that subsumes many existing calibration methods as special cases.

Abstract: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new
tasks with just a few examples, but their predictions often suffer from
systematic biases, leading to unstable performances in classification. While
calibration techniques are proposed to mitigate these biases, we show that, in
the logit space, many of these methods are equivalent to merely shifting the
LLM's decision boundary without having the ability to alter its orientation.
This proves inadequate when biases cause the LLM to be severely misdirected. To
address these limitations and provide a unifying framework, we propose
Supervised Calibration (SC), a loss-minimization based framework which learns
an optimal, per-class affine transformation of the LLM's predictive
probabilities in the logit space without requiring external data beyond the
context. By using a more expressive functional class, SC not only subsumes many
existing calibration methods in ICL as special cases, but also enables the
ability to alter and even completely reverse the orientation of the LLM's
decision boundary. Furthermore, SC's loss-based nature facilitates the seamless
integration of two purpose-built regularization techniques: context-invariance
and directional trust-region. The former is designed to tackle the instability
issue in ICL, while the latter controls the degree of calibration. Finally, SC
delivers state-of-the-art performance over calibration baselines in the 4-shot,
8-shot, and 16-shot settings across all nine datasets for
Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.

</details>


### [369] [Gibbs randomness-compression proposition: An efficient deep learning](https://arxiv.org/abs/2505.23869)
*M. Süzen*

Main category: stat.ML

TL;DR: This paper introduces the Gibbs randomness-compression proposition, which links randomness and compression through Gibbs entropy. The Dual Tomographic Compression (DTC) compress-train framework is proposed for neural network compression, enabling efficient training with state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between randomness and compression in the context of neural network compression and to develop an energy and resource-efficient deep learning training approach.

Method: The Dual Tomographic Compression (DTC) compress-train framework uses tomographic reconstruction of layer weight matrices via compressed sensed projections (weight rays), applied in a dual fashion to previous and next layers, triggering neuronal-level pruning.

Result: Experiments demonstrated state-of-the-art performance with efficient compression during training, supporting the lottery ticket hypothesis. Random compress-train iterations showed similar performance, indicating a statistical physics perspective on the randomness-compression relationship.

Conclusion: The DTC framework provides a promising approach for energy and resource-efficient deep learning training, underpinned by the Gibbs randomness-compression proposition.

Abstract: A proposition that connects randomness and compression put forward via Gibbs
entropy over set of measurement vectors associated with a compression process.
The proposition states that a lossy compression process is equivalent to {\it
directed randomness} that preserves information content. The proposition
originated from the observed behaviour in newly proposed {\it Dual Tomographic
Compression} (DTC) compress-train framework. This is akin to tomographic
reconstruction of layer weight matrices via building compressed sensed
projections, so called {\it weight rays}. This tomographic approach is applied
to previous and next layers in a dual fashion, that triggers neuronal-level
pruning. This novel model compress-train scheme appear in iterative fashion and
act as smart neural architecture search, Experiments demonstrated utility of
this dual-tomography producing state-of-the-art performance with efficient
compression during training, accelerating and supporting lottery ticket
hypothesis. However, random compress-train iterations having similar
performance demonstrated the connection between randomness and compression from
statistical physics perspective, we formulated so called {\it Gibbs
randomness-compression proposition}, signifying randomness-compression
relationship via Gibbs entropy. Practically, DTC framework provides a promising
approach for massively energy and resource efficient deep learning training
approach.

</details>


### [370] [Conformal Object Detection by Sequential Risk Control](https://arxiv.org/abs/2505.24038)
*Léo Andéol,Luca Mossina,Adrien Mazoyer,Sébastien Gerchinovitz*

Main category: stat.ML

TL;DR: Recent advances in object detectors have led to their adoption for industrial uses. However, their deployment in critical applications is hindered by the inherent lack of reliability of neural networks and the complex structure of object detection models. To address these challenges, we turn to Conformal Prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve the reliability of object detection models in critical applications, addressing the issues caused by the inherent lack of reliability of neural networks and the complex structure of object detection models.

Method: The authors formally define the problem of Conformal Object Detection (COD) and introduce a novel method, Sequential Conformal Risk Control (SeqCRC), that extends the statistical guarantees of Conformal Risk Control (CRC) to two sequential tasks with two parameters. They also propose loss functions and prediction sets suited to applying CRC to different applications and certification requirements.

Result: The authors present a conformal toolkit that enables replication and further exploration of their methods. Through extensive experiments, they yield a benchmark that validates the investigated methods and emphasizes trade-offs and other practical consequences.

Conclusion: This paper addresses the challenges in deploying object detectors in critical applications by introducing SeqCRC and providing a conformal toolkit for further research.

Abstract: Recent advances in object detectors have led to their adoption for industrial
uses. However, their deployment in critical applications is hindered by the
inherent lack of reliability of neural networks and the complex structure of
object detection models. To address these challenges, we turn to Conformal
Prediction, a post-hoc procedure which offers statistical guarantees that are
valid for any dataset size, without requiring prior knowledge on the model or
data distribution. Our contribution is manifold: first, we formally define the
problem of Conformal Object Detection (COD) and introduce a novel method,
Sequential Conformal Risk Control (SeqCRC), that extends the statistical
guarantees of Conformal Risk Control (CRC) to two sequential tasks with two
parameters, as required in the COD setting. Then, we propose loss functions and
prediction sets suited to applying CRC to different applications and
certification requirements. Finally, we present a conformal toolkit, enabling
replication and further exploration of our methods. Using this toolkit, we
perform extensive experiments, yielding a benchmark that validates the
investigated methods and emphasizes trade-offs and other practical
consequences.

</details>


### [371] [Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity](https://arxiv.org/abs/2505.24097)
*Victor Li,Baiting Chen,Yuzhen Mao,Qi Lei,Zhun Deng*

Main category: stat.ML

TL;DR: This paper introduces Performative Risk Control, a framework for calibrating machine learning models to achieve risk control under performativity with theoretical guarantees. It provides an iterative calibration process and demonstrates effectiveness in predicting credit default risk.


<details>
  <summary>Details</summary>
Motivation: Calibrating blackbox machine learning models is crucial for reliable decision-making, but existing methods do not account for the performativity of predictions where decisions influence outcomes.

Method: The paper proposes Performative Risk Control, an iteratively refined calibration process that ensures predictions are improved and risk-controlled. It studies different risk measures and tail bounds.

Result: The framework demonstrates effectiveness in numerical experiments on predicting credit default risk.

Conclusion: This work is the first to study statistically rigorous risk control under performativity, providing a safeguard against strategic manipulation in decision-making processes.

Abstract: Calibrating blackbox machine learning models to achieve risk control is
crucial to ensure reliable decision-making. A rich line of literature has been
studying how to calibrate a model so that its predictions satisfy explicit
finite-sample statistical guarantees under a fixed, static, and unknown
data-generating distribution. However, prediction-supported decisions may
influence the outcome they aim to predict, a phenomenon named performativity of
predictions, which is commonly seen in social science and economics. In this
paper, we introduce Performative Risk Control, a framework to calibrate models
to achieve risk control under performativity with provable theoretical
guarantees. Specifically, we provide an iteratively refined calibration
process, where we ensure the predictions are improved and risk-controlled
throughout the process. We also study different types of risk measures and
choices of tail bounds. Lastly, we demonstrate the effectiveness of our
framework by numerical experiments on the task of predicting credit default
risk. To the best of our knowledge, this work is the first one to study
statistically rigorous risk control under performativity, which will serve as
an important safeguard against a wide range of strategic manipulation in
decision-making processes.

</details>


### [372] [A Mathematical Perspective On Contrastive Learning](https://arxiv.org/abs/2505.24134)
*Ricardo Baptista,Andrew M. Stuart,Son Tran*

Main category: stat.ML

TL;DR: 本文提出了一种新的多模态对比学习框架，通过优化参数化编码器定义条件概率分布，并研究了其在多种任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对比学习方法主要关注于将不同模态的数据对齐到一个共同的潜在空间中，但尚未充分利用概率视角来改进损失函数和对齐度量。

Method: 作者将对比学习解释为优化（参数化）编码器，这些编码器定义了每个模态基于另一个模态的条件概率分布。并提出了两种自然的对比学习泛化：引入新的概率损失函数和使用替代度量来测量共同潜在空间中的对齐情况。

Result: 通过数值实验验证了该框架在多变量高斯分布、MNIST数据集以及海洋学中的数据同化应用的有效性，能够更好地近似自然统计量如条件均值和协方差。

Conclusion: 提出的概率视角下的对比学习框架不仅适用于跨模态检索和分类任务，还可以生成跨模态生成模型，为进一步改进对比学习算法提供了新方向。

Abstract: Multimodal contrastive learning is a methodology for linking different data
modalities; the canonical example is linking image and text data. The
methodology is typically framed as the identification of a set of encoders, one
for each modality, that align representations within a common latent space. In
this work, we focus on the bimodal setting and interpret contrastive learning
as the optimization of (parameterized) encoders that define conditional
probability distributions, for each modality conditioned on the other,
consistent with the available data. This provides a framework for multimodal
algorithms such as crossmodal retrieval, which identifies the mode of one of
these conditional distributions, and crossmodal classification, which is
similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This
probabilistic perspective suggests two natural generalizations of contrastive
learning: the introduction of novel probabilistic loss functions, and the use
of alternative metrics for measuring alignment in the common latent space. We
study these generalizations of the classical approach in the multivariate
Gaussian setting. In this context we view the latent space identification as a
low-rank matrix approximation problem. This allows us to characterize the
capabilities of loss functions and alignment metrics to approximate natural
statistics, such as conditional means and covariances; doing so yields novel
variants on contrastive learning algorithms for specific mode-seeking and for
generative tasks. The framework we introduce is also studied through numerical
experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data
assimilation application arising in oceanography.

</details>


### [373] [Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings](https://arxiv.org/abs/2505.24281)
*Yang Sui,Qi Xu,Yang Bai,Annie Qu*

Main category: stat.ML

TL;DR: 提出了一种双编码器框架的多任务学习方法，通过统一算法解决异质性问题，并在理论和实验上证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管多任务学习（MTL）被广泛应用，但现有方法难以在一个统一框架内处理各种形式的异质性，如分布和后验异质性。

Method: 提出了一个双编码器框架，包含任务共享编码器和任务特定编码器，用于捕捉共同信息和保留独特任务特征；探索了与学习到的潜在因素相对应的系数的内在相似性结构，允许跨任务自适应集成以管理后验异质性；引入了一个统一算法交替学习任务特定和任务共享编码器及系数。

Result: 理论上，研究了提出的MTL方法的过量风险界限，并将其应用于新但相关的任务；通过模拟研究证明该方法在不同设置下优于现有的数据集成方法；在PDX数据中，该方法对五种不同癌症类型的肿瘤倍增时间预测表现优异。

Conclusion: 所提出的双编码器框架能够有效解决多任务学习中的异质性问题，并在实际应用中表现出色。

Abstract: Multi-task learning (MTL) has become an essential machine learning tool for
addressing multiple learning tasks simultaneously and has been effectively
applied across fields such as healthcare, marketing, and biomedical research.
However, to enable efficient information sharing across tasks, it is crucial to
leverage both shared and heterogeneous information. Despite extensive research
on MTL, various forms of heterogeneity, including distribution and posterior
heterogeneity, present significant challenges. Existing methods often fail to
address these forms of heterogeneity within a unified framework. In this paper,
we propose a dual-encoder framework to construct a heterogeneous latent factor
space for each task, incorporating a task-shared encoder to capture common
information across tasks and a task-specific encoder to preserve unique task
characteristics. Additionally, we explore the intrinsic similarity structure of
the coefficients corresponding to learned latent factors, allowing for adaptive
integration across tasks to manage posterior heterogeneity. We introduce a
unified algorithm that alternately learns the task-specific and task-shared
encoders and coefficients. In theory, we investigate the excess risk bound for
the proposed MTL method using local Rademacher complexity and apply it to a new
but related task. Through simulation studies, we demonstrate that the proposed
method outperforms existing data integration methods across various settings.
Furthermore, the proposed method achieves superior predictive performance for
time to tumor doubling across five distinct cancer types in PDX data.

</details>


### [374] [Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels](https://arxiv.org/abs/2505.24311)
*Yi Gu*

Main category: stat.ML

TL;DR: 研究了带广义核的t-SNE算法的收敛性，并扩展了之前的研究成果。


<details>
  <summary>Details</summary>
Motivation: 尽管t-SNE是用于高维数据可视化的知名算法，但对于带广义核的t-SNE算法的收敛性研究尚不充分。

Method: 通过给出广义输入和输出核的具体公式，并在特定条件下分析t-SNE算法的行为。

Result: 证明了在数据点数量趋于无穷时，t-SNE算法收敛到一个平衡分布。

Conclusion: 这一结果表明t-SNE算法具有广泛的适用性，尤其是在处理不同类型的数据核时。

Abstract: T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm
for visualizing high-dimensional data by finding low-dimensional
representations. In this paper, we study the convergence of t-SNE with
generalized kernels and extend the results of Auffinger and Fletcher in 2023.
Our work starts by giving a concrete formulation of generalized input and
output kernels. Then we prove that under certain conditions, the t-SNE
algorithm converges to an equilibrium distribution for a wide range of input
and output kernels as the number of data points diverges.

</details>


### [375] [Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation](https://arxiv.org/abs/2505.24333)
*Alessio Giorlandino,Sebastian Goldt*

Main category: stat.ML

TL;DR: 在变压器块中，信号传播的分析理论揭示了自我关注层的两种初始状态模式：低差异模式下的秩崩溃和高差异模式下的熵崩溃。研究提供了训练图表以确定给定架构的正确初始超参数选择，并通过实验验证了这些预测。


<details>
  <summary>Details</summary>
Motivation: 找到神经网络的正确初始化对于确保平稳训练和良好性能至关重要。然而，在变压器中错误的初始化会导致自注意力层的失败模式，如秩崩溃或熵崩溃。虽然前馈网络的正确初始化已被广泛研究，但对完整变压器块的信号传播的确切描述尚不明确。

Method: 研究人员提供了一个关于通过具有自注意层、层归一化、跳过连接和ReLU MLP的普通变压器块的信号传播的分析理论。通过与统计物理学中的随机能量模型进行形式上的平行处理，识别并描述了由查询和键初始化的方差控制的两个制度：低方差制度（恢复已知的秩崩溃行为）和高方差制度（信号得以保存但发生熵崩溃）。

Result: 理论计算出了低方差制度下确保信号传播的残差连接的关键强度，并生成了可训练性图，以识别给定架构的正确初始化超参数选择。使用TinyStories训练的BERT风格模型的实验验证了这些预测。

Conclusion: 该理论框架为自注意力的两种失败模式提供了统一的观点，并给出了保证平稳训练的权重和残差连接规模的定量预测。

Abstract: Finding the right initialisation for neural networks is crucial to ensure
smooth training and good performance. In transformers, the wrong initialisation
can lead to one of two failure modes of self-attention layers: rank collapse,
where all tokens collapse into similar representations, and entropy collapse,
where highly concentrated attention scores lead to training instability. While
the right initialisation has been extensively studied in feed-forward networks,
an exact description of signal propagation through a full transformer block has
so far been lacking. Here, we provide an analytical theory of signal
propagation through vanilla transformer blocks with self-attention layers,
layer normalisation, skip connections and ReLU MLP. To treat the self-attention
layer, we draw on a formal parallel with the Random Energy Model from
statistical physics. We identify and characterise two regimes governed by the
variance of the query and key initialisations: a low-variance regime, where we
recover the known rank collapse behaviour; and a previously unexplored
high-variance regime, where signal is preserved but \textit{entropy collapse}
occurs. In the low-variance regime, we calculate the critical strength for the
residual connection to ensure signal propagation. Our theory yields
trainability diagrams that identify the correct choice of initialisation
hyper-parameters for a given architecture. Experiments with BERT-style models
trained on TinyStories validate our predictions. Our theoretical framework
gives a unified perspective on the two failure modes of self-attention and
gives quantitative predictions on the scale of both weights and residual
connections that guarantees smooth training.

</details>


### [376] [Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data](https://arxiv.org/abs/2505.24556)
*Gabriel V Cardoso,Mike Pereira*

Main category: stat.ML

TL;DR: The paper proposes a two-step approach using diffusion generative models (DGMs) to mimic predictive posterior distributions (PPDs) for non-stationary Gaussian process priors, enabling state-of-the-art predictions in environmental science inverse problems.


<details>
  <summary>Details</summary>
Motivation: Bayesian models with Gaussian processes provide a flexible framework for predicting spatially distributed variables with uncertainty. However, using nonstationary priors, which are often necessary for capturing complex spatial patterns, makes sampling from the predictive posterior distribution computationally intractable.

Method: A two-step approach based on diffusion generative models (DGMs) is proposed: replacing the GP prior by a DGM surrogate and leveraging training-free guidance algorithms for DGMs to sample from the desired posterior distribution.

Result: The approach was applied to a non-stationary GP prior where exact posterior sampling is untractable, validating that the issuing distributions are close to their GP counterpart using several statistical metrics. Additionally, trained DGMs can be fine-tuned to target specific parts of the GP prior. The method yields state-of-the-art predictions when applied to inverse problems in environmental sciences.

Conclusion: The proposed two-step approach based on DGMs successfully mimics PPDs associated with non-stationary GP priors, overcoming computational intractability and providing state-of-the-art predictions in environmental science inverse problems.

Abstract: Bayesian models based on Gaussian processes (GPs) offer a flexible framework
to predict spatially distributed variables with uncertainty. But the use of
nonstationary priors, often necessary for capturing complex spatial patterns,
makes sampling from the predictive posterior distribution (PPD) computationally
intractable. In this paper, we propose a two-step approach based on diffusion
generative models (DGMs) to mimic PPDs associated with non-stationary GP
priors: we replace the GP prior by a DGM surrogate, and leverage recent
advances on training-free guidance algorithms for DGMs to sample from the
desired posterior distribution. We apply our approach to a rich non-stationary
GP prior from which exact posterior sampling is untractable and validate that
the issuing distributions are close to their GP counterpart using several
statistical metrics. We also demonstrate how one can fine-tune the trained DGMs
to target specific parts of the GP prior. Finally we apply the proposed
approach to solve inverse problems arising in environmental sciences, thus
yielding state-of-the-art predictions.

</details>


### [377] [Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders](https://arxiv.org/abs/2505.24668)
*Jonghyun Ham,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: stat.ML

TL;DR: 现代深度学习架构中的两层线性去噪自动编码器在过参数化 regime 下的泛化行为研究，包括瓶颈层和跳跃连接对模型复杂度和测试风险的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已经对监督学习任务中的泛化现象有了显著理解，但对于无监督任务（如去噪）仍存在许多未解问题。本文试图通过分析两层线性去噪自动编码器来填补这一空白。

Method: 研究了在梯度流下训练的两层线性去噪自动编码器，包含瓶颈层和可能的跳跃连接。推导了该模型在乘积正则化下的所有临界点的闭式表达式，并描述了其全局最小值。进一步推导了过参数化 regime 下的测试风险公式，分析了瓶颈层和跳跃连接对模型复杂度和测试风险的影响。

Result: 发现了两个有趣的现象：1) 瓶颈层引入了一个类似于经典偏差-方差权衡的复杂度度量；2) 跳跃连接可以减轻去噪自动编码器中的方差，特别是在模型轻微过参数化时。使用随机矩阵理论进一步分析了跳跃连接的影响，并用数值证据支持了这些结论。

Conclusion: 本文通过分析两层线性去噪自动编码器，揭示了瓶颈层和跳跃连接对模型复杂度和泛化性能的影响，为理解现代深度学习架构在无监督任务中的泛化行为提供了新的视角。

Abstract: Modern deep neural networks exhibit strong generalization even in highly
overparameterized regimes. Significant progress has been made to understand
this phenomenon in the context of supervised learning, but for unsupervised
tasks such as denoising, several open questions remain. While some recent works
have successfully characterized the test error of the linear denoising problem,
they are limited to linear models (one-layer network). In this work, we focus
on two-layer linear denoising autoencoders trained under gradient flow,
incorporating two key ingredients of modern deep learning architectures: A
low-dimensional bottleneck layer that effectively enforces a rank constraint on
the learned solution, as well as the possibility of a skip connection that
bypasses the bottleneck. We derive closed-form expressions for all critical
points of this model under product regularization, and in particular describe
its global minimizer under the minimum-norm principle. From there, we derive
the test risk formula in the overparameterized regime, both for models with and
without skip connections. Our analysis reveals two interesting phenomena:
Firstly, the bottleneck layer introduces an additional complexity measure akin
to the classical bias-variance trade-off -- increasing the bottleneck width
reduces bias but introduces variance, and vice versa. Secondly, skip connection
can mitigate the variance in denoising autoencoders -- especially when the
model is mildly overparameterized. We further analyze the impact of skip
connections in denoising autoencoder using random matrix theory and support our
claims with numerical evidence.

</details>


### [378] [K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes](https://arxiv.org/abs/2505.24704)
*Hideaki Kim,Tomoharu Iwata,Akinori Fujino*

Main category: stat.ML

TL;DR: The paper introduces K$^2$IE, a novel regularized kernel method for Poisson processes that combines the least squares loss with RKHS theory. It shares dual coefficient properties with classical KIEs and demonstrates superior computational efficiency without sacrificing predictive performance in experiments on synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between classical kernel intensity estimators (KIEs) and kernel method-based intensity estimators within reproducing kernel Hilbert spaces (RKHSs), while enhancing computational efficiency.

Method: A new regularized kernel method is proposed for Poisson processes based on the least squares loss. This method utilizes RKHS theory and results in an intensity estimator that aligns with a specialized variant of the representer theorem, sharing properties with classical KIEs.

Result: K$^2$IE achieves comparable predictive performance to existing methods but significantly improves computational efficiency over state-of-the-art kernel method-based estimators.

Conclusion: The proposed K$^2$IE model not only provides new theoretical insights into the relationship between classical KIEs and kernel method-based intensity estimators but also offers practical advantages in terms of computational efficiency.

Abstract: Kernel method-based intensity estimators, formulated within reproducing
kernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs)
have been among the most easy-to-implement and feasible methods for estimating
the intensity functions of inhomogeneous Poisson processes. While both
approaches share the term "kernel", they are founded on distinct theoretical
principles, each with its own strengths and limitations. In this paper, we
propose a novel regularized kernel method for Poisson processes based on the
least squares loss and show that the resulting intensity estimator involves a
specialized variant of the representer theorem: it has the dual coefficient of
unity and coincides with classical KIEs. This result provides new theoretical
insights into the connection between classical KIEs and kernel method-based
intensity estimators, while enabling us to develop an efficient KIE by
leveraging advanced techniques from RKHS theory. We refer to the proposed model
as the kernel method-based kernel intensity estimator (K$^2$IE). Through
experiments on synthetic datasets, we show that K$^2$IE achieves comparable
predictive performance while significantly surpassing the state-of-the-art
kernel method-based estimator in computational efficiency.

</details>


### [379] [Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery](https://arxiv.org/abs/2505.24727)
*Xiaochen Zhang,Haoyi Xiong*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a novel Knockoff-guided compressive sensing framework,
referred to as \TheName{}, which enhances signal recovery by leveraging precise
false discovery rate (FDR) control during the support identification phase.
Unlike LASSO, which jointly performs support selection and signal estimation
without explicit error control, our method guarantees FDR control in finite
samples, enabling more reliable identification of the true signal support. By
separating and controlling the support recovery process through statistical
Knockoff filters, our framework achieves more accurate signal reconstruction,
especially in challenging scenarios where traditional methods fail. We
establish theoretical guarantees demonstrating how FDR control directly ensures
recovery performance under weaker conditions than traditional $\ell_1$-based
compressive sensing methods, while maintaining accurate signal reconstruction.
Extensive numerical experiments demonstrate that our proposed Knockoff-based
method consistently outperforms LASSO-based and other state-of-the-art
compressive sensing techniques. In simulation studies, our method improves
F1-score by up to 3.9x over baseline methods, attributed to principled false
discovery rate (FDR) control and enhanced support recovery. The method also
consistently yields lower reconstruction and relative errors. We further
validate the framework on real-world datasets, where it achieves top downstream
predictive performance across both regression and classification tasks, often
narrowing or even surpassing the performance gap relative to uncompressed
signals. These results establish \TheName{} as a robust and practical
alternative to existing approaches, offering both theoretical guarantees and
strong empirical performance through statistically grounded support selection.

</details>


### [380] [Generalization Dynamics of Linear Diffusion Models](https://arxiv.org/abs/2505.24769)
*Claudia Merger,Sebastian Goldt*

Main category: stat.ML

TL;DR: The paper explores the transition from memorization to generalization in diffusion models using linear denoisers, showing that this occurs when sample size $N$ is roughly equal to input dimension $d$, and examines how regularization and early stopping mitigate overfitting for $N < d$ while demonstrating optimal sampling distribution convergence for $N > d$.


<details>
  <summary>Details</summary>
Motivation: To understand the transition from memorization to generalization in diffusion models trained on finite datasets, which is crucial for characterizing sample efficiency and reliability of generative models.

Method: Analytical study of a simple model with linear denoisers allowing explicit computation of test errors, sampling distributions, and Kullback-Leibler divergences. Examines effects of regularization and early stopping, and studies behavior for both $N < d$ and $N > d$.

Result: Predicts the transition from memorization to generalization occurs at $N \asymp d$. For $N < d$, regularization and early stopping help prevent overfitting. For $N > d$, sampling distributions approach their optimum linearly with $d/N$.

Conclusion: Clarifies the role of sample complexity in governing generalisation in simple diffusion-based generative models and provides insights into training dynamics of linear denoisers.

Abstract: Diffusion models trained on finite datasets with $N$ samples from a target
distribution exhibit a transition from memorisation, where the model reproduces
training examples, to generalisation, where it produces novel samples that
reflect the underlying data distribution. Understanding this transition is key
to characterising the sample efficiency and reliability of generative models,
but our theoretical understanding of this transition is incomplete. Here, we
analytically study the memorisation-to-generalisation transition in a simple
model using linear denoisers, which allow explicit computation of test errors,
sampling distributions, and Kullback-Leibler divergences between samples and
target distribution. Using these measures, we predict that this transition
occurs roughly when $N \asymp d$, the dimension of the inputs. When $N$ is
smaller than the dimension of the inputs $d$, so that only a fraction of
relevant directions of variation are present in the training data, we
demonstrate how both regularization and early stopping help to prevent
overfitting. For $N > d$, we find that the sampling distributions of linear
diffusion models approach their optimum (measured by the Kullback-Leibler
divergence) linearly with $d/N$, independent of the specifics of the data
distribution. Our work clarifies how sample complexity governs generalisation
in a simple model of diffusion-based generative models and provides insight
into the training dynamics of linear denoisers.

</details>


### [381] [Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV](https://arxiv.org/abs/2505.24781)
*Karim Abou-Moustafa*

Main category: stat.ML

TL;DR: The paper proposes an efficient approximation method for estimating the shrinkage coefficient α in Regularized Tyler's M-estimator (RTME) by minimizing the leave-one-out cross-validated (LOOCV) log-likelihood loss, which significantly reduces computational complexity and demonstrates superior performance on both synthetic and real high-dimensional datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the computational challenge associated with estimating the shrinkage coefficient α using LOOCV in RTME, especially when dealing with moderate or large sample sizes.

Method: The authors propose a computationally efficient approximation for the LOOCV log-likelihood loss, eliminating the need to invoke the RTME procedure n times. This results in an O(n) reduction in running time complexity for the LOOCV procedure.

Result: Experiments conducted on synthetic high-dimensional data from heavy-tailed elliptical distributions and real-world datasets for object, face, and handwritten digit recognition show that the proposed approach is efficient and consistently more accurate than other methods in the literature.

Conclusion: The proposed method provides an efficient and accurate way to estimate the shrinkage coefficient α in RTME, offering significant speedup compared to traditional LOOCV approaches.

Abstract: We consider the problem of estimating a regularization parameter, or a
shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator
(RTME). In particular, we propose to estimate an optimal shrinkage coefficient
by setting $\alpha$ as the solution to a suitably chosen objective function;
namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since
LOOCV is computationally prohibitive even for moderate sample size $n$, we
propose a computationally efficient approximation for the LOOCV log-likelihood
loss that eliminates the need for invoking the RTME procedure $n$ times for
each sample left out during the LOOCV procedure. This approximation yields an
$O(n)$ reduction in the running time complexity for the LOOCV procedure, which
results in a significant speedup for computing the LOOCV estimate. We
demonstrate the efficiency and accuracy of the proposed approach on synthetic
high-dimensional data sampled from heavy-tailed elliptical distributions, as
well as on real high-dimensional datasets for object recognition, face
recognition, and handwritten digit's recognition. Our experiments show that the
proposed approach is efficient and consistently more accurate than other
methods in the literature for shrinkage coefficient estimation.

</details>


### [382] [Statistical mechanics of extensive-width Bayesian neural networks near interpolation](https://arxiv.org/abs/2505.24849)
*Jean Barbier,Francesco Camilli,Minh-Toan Nguyen,Mauro Pastore,Rudy Skerk*

Main category: stat.ML

TL;DR: 本文通过统计物理分析了具有通用权重分布和激活函数的两层全连接网络的监督学习，揭示了在数据增加时的各种学习转变现象。


<details>
  <summary>Details</summary>
Motivation: 尽管统计力学为神经网络提供了分析框架，但理论可处理的模型相对简单，与实际应用中的网络存在差距。因此，本文旨在缩小实践网络与其理论理解之间的差距。

Method: 作者研究了一个两层全连接网络的监督学习问题，该网络具有通用权重分布和激活函数，隐藏层较大但与输入维度成比例。重点在于教师-学生场景下的贝叶斯最优学习，并探讨插值区域内的特征学习现象。

Result: 随着数据量的增加，出现了各种学习转变。当特征对观察到的响应贡献越大时，所需的学习数据越少。此外，在数据稀缺的情况下，模型仅学习教师权重的非线性组合，而非通过对其权重进行对齐实现“专业化”。只有当有足够的数据时，才会发生专业化，但实际训练算法可能难以找到这种状态，可能是由于统计到计算的差距。

Conclusion: 本文揭示了两层全连接网络在监督学习中的丰富现象学，强调了数据量对特征学习的重要性，并指出了实际训练算法面临的挑战。

Abstract: For three decades statistical mechanics has been providing a framework to
analyse neural networks. However, the theoretically tractable models, e.g.,
perceptrons, random features models and kernel machines, or multi-index models
and committee machines with few neurons, remained simple compared to those used
in applications. In this paper we help reducing the gap between practical
networks and their theoretical understanding through a statistical physics
analysis of the supervised learning of a two-layer fully connected network with
generic weight distribution and activation function, whose hidden layer is
large but remains proportional to the inputs dimension. This makes it more
realistic than infinitely wide networks where no feature learning occurs, but
also more expressive than narrow ones or with fixed inner weights. We focus on
the Bayes-optimal learning in the teacher-student scenario, i.e., with a
dataset generated by another network with the same architecture. We operate
around interpolation, where the number of trainable parameters and of data are
comparable and feature learning emerges. Our analysis uncovers a rich
phenomenology with various learning transitions as the number of data
increases. In particular, the more strongly the features (i.e., hidden neurons
of the target) contribute to the observed responses, the less data is needed to
learn them. Moreover, when the data is scarce, the model only learns non-linear
combinations of the teacher weights, rather than "specialising" by aligning its
weights with the teacher's. Specialisation occurs only when enough data becomes
available, but it can be hard to find for practical training algorithms,
possibly due to statistical-to-computational~gaps.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [383] [Distributed gradient methods under heavy-tailed communication noise](https://arxiv.org/abs/2505.24464)
*Manojlo Vukovic,Dusan Jakovetic,Dragana Bajovic,Soummya Kar*

Main category: math.OC

TL;DR: The paper proposes a distributed gradient-type method for minimizing the sum of locally known convex costs in networked nodes with heavy-tailed noise.


<details>
  <summary>Details</summary>
Motivation: Existing distributed methods fail in the presence of infinite variance noise, so this paper aims to design and analyze distributed methods for solving optimization problems when inter-node communication is subject to heavy-tailed noise.

Method: A distributed gradient-type method is designed with carefully balanced mixed time-scale time-varying consensus and gradient contribution step sizes and a bounded nonlinear operator on the consensus update to limit the effect of heavy-tailed noise.

Result: The proposed method converges to a neighborhood of the network-wide problem solution in the mean squared error (MSE) sense, and the corresponding convergence rate is characterized. The asymptotic MSE can be made arbitrarily small through consensus step-size tuning, possibly at the cost of slowing down the transient error decay.

Conclusion: Numerical experiments demonstrate the resilience of the proposed method to heavy-tailed (and infinite variance) communication noise, while existing distributed methods fail in the presence of infinite variance noise.

Abstract: We consider a standard distributed optimization problem in which networked
nodes collaboratively minimize the sum of their locally known convex costs. For
this setting, we address for the first time the fundamental problem of design
and analysis of distributed methods to solve the above problem when inter-node
communication is subject to \emph{heavy-tailed} noise. Heavy-tailed noise is
highly relevant and frequently arises in densely deployed wireless sensor and
Internet of Things (IoT) networks. Specifically, we design a distributed
gradient-type method that features a carefully balanced mixed time-scale
time-varying consensus and gradient contribution step sizes and a bounded
nonlinear operator on the consensus update to limit the effect of heavy-tailed
noise. Assuming heterogeneous strongly convex local costs with mutually
different minimizers that are arbitrarily far apart, we show that the proposed
method converges to a neighborhood of the network-wide problem solution in the
mean squared error (MSE) sense, and we also characterize the corresponding
convergence rate. We further show that the asymptotic MSE can be made
arbitrarily small through consensus step-size tuning, possibly at the cost of
slowing down the transient error decay. Numerical experiments corroborate our
findings and demonstrate the resilience of the proposed method to heavy-tailed
(and infinite variance) communication noise. They also show that existing
distributed methods, designed for finite-communication-noise-variance settings,
fail in the presence of infinite variance noise.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [384] [More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI](https://arxiv.org/abs/2505.23780)
*Émilie Fabre,Katie Seaborn,Shuta Koiwai,Mizuki Watanabe,Paul Riesch*

Main category: cs.HC

TL;DR: The study explored longitudinal engagement with generative AI storytelling agents through a two-week diary study of an app called Dreamsmithy, revealing themes like 'oscillating ambivalence' and 'socio-chronological bonding', while highlighting both the potential and limitations of GenAI for storytelling.


<details>
  <summary>Details</summary>
Motivation: To understand the complex dynamics that emerge between individuals and generative AI (GenAI) storytelling agents over time, especially focusing on multi-generational experiences.

Method: A daily dream-crafting app named Dreamsmithy was used where participants co-created stories with an AI narrator called Makoto every day. A two-week diary study captured reflections and interactions.

Result: Participants appreciated personal notes, opportunities for reflection, and AI creativity but faced limitations in narrative coherence and control leading to frustration. Themes such as 'oscillating ambivalence' and 'socio-chronological bonding' emerged from reflexive thematic analysis.

Conclusion: GenAI has potential for longitudinal storytelling but raises critical questions about user agency and ethics, contributing empirical insights and design considerations for adaptive storytelling systems.

Abstract: Longitudinal engagement with generative AI (GenAI) storytelling agents is a
timely but less charted domain. We explored multi-generational experiences with
"Dreamsmithy," a daily dream-crafting app, where participants (N = 28)
co-created stories with AI narrator "Makoto" every day. Reflections and
interactions were captured through a two-week diary study. Reflexive thematic
analysis revealed themes likes "oscillating ambivalence" and
"socio-chronological bonding," highlighting the complex dynamics that emerged
between individuals and the AI narrator over time. Findings suggest that while
people appreciated the personal notes, opportunities for reflection, and AI
creativity, limitations in narrative coherence and control occasionally caused
frustration. The results underscore the potential of GenAI for longitudinal
storytelling, but also raise critical questions about user agency and ethics.
We contribute initial empirical insights and design considerations for
developing adaptive, more-than-human storytelling systems.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [385] [A Start To End Machine Learning Approach To Maximize Scientific Throughput From The LCLS-II-HE](https://arxiv.org/abs/2505.23858)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Fred Poitevin,Jana Thayer,Daniel Ratner,Auralee Edelen,Apurva Mehta*

Main category: physics.ins-det

TL;DR: 随着光源亮度的增加，包括APS的衍射极限亮度升级和LCLS的高重复频率升级，其中提出的实验变得越来越复杂。本文概述了我们在SLAC开发的策略，通过机器学习驱动的优化、自动化和实时知识提取，从电子加速器的电子注入器开始，到多维X射线光学系统，再到LCLS的实验端站和高读出率、多兆像素探测器，以向用户提供设计性能。


<details>
  <summary>Details</summary>
Motivation: 为了应对复杂实验需求的增长以及海量数据生成的挑战，需要实现实时主动反馈控制和优化的数据处理管道，从而避免研究人员被无用数据淹没，并成功提取最近亮度升级所承诺的高度复杂见解。

Method: 在SLAC开发并实施机器学习驱动的优化、自动化和实时知识提取策略，涵盖从电子加速器的电子注入器、多维X射线光学系统到实验端站和高读出率、多兆像素探测器的整个过程。

Result: 通过加速器、光学和终端用户应用的例子说明了该策略的有效性，能够实现设计性能并满足用户需求。

Conclusion: 机器学习驱动的方法可以有效应对未来光源实验中的复杂性和数据处理挑战，确保实验的成功和科学目标的实现。

Abstract: With the increasing brightness of Light sources, including the
Diffraction-Limited brightness upgrade of APS and the high-repetition-rate
upgrade of LCLS, the proposed experiments therein are becoming increasingly
complex. For instance, experiments at LCLS-II-HE will require the X-ray beam to
be within a fraction of a micron in diameter, with pointing stability of a few
nanoradians, at the end of a kilometer-long electron accelerator, a
hundred-meter-long undulator section, and tens of meters long X-ray optics.
This enhancement of brightness will increase the data production rate to rival
the largest data generators in the world. Without real-time active feedback
control and an optimized pipeline to transform measurements to scientific
information and insights, researchers will drown in a deluge of mostly useless
data, and fail to extract the highly sophisticated insights that the recent
brightness upgrades promise.
  In this article, we outline the strategy we are developing at SLAC to
implement Machine Learning driven optimization, automation and real-time
knowledge extraction from the electron-injector at the start of the electron
accelerator, to the multidimensional X-ray optical systems, and till the
experimental endstations and the high readout rate, multi-megapixel detectors
at LCLS to deliver the design performance to the users. This is illustrated via
examples from Accelerator, Optics and End User applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [386] [Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction](https://arxiv.org/abs/2505.24136)
*Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: In order to address the issue of artifacts in high acceleration rate MRI reconstructions when fully-sampled reference data is unavailable, this paper proposes a new training method for physics-driven deep learning (PD-DL) networks that uses carefully-designed perturbations. This approach enhances conventional self-supervised learning and leads to more reliable, artifact-free reconstructions.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning is crucial for training PD-DL models when fully-sampled reference data is not available. However, at high acceleration rates, it often introduces artifacts that compromise image fidelity. Thus, there is a need for an improved method to reduce these artifacts and improve reconstruction quality.

Method: The authors propose a novel training strategy for PD-DL networks that incorporates a consistency term assessing the model's ability to predict added perturbations in a sparse domain. This builds upon the k-space masking idea used in conventional self-supervised learning.

Result: The proposed method effectively reduces aliasing artifacts and mitigates noise amplification at high acceleration rates. It outperforms state-of-the-art self-supervised methods both visually and quantitatively on the fastMRI knee and brain datasets.

Conclusion: The novel training strategy for PD-DL networks using carefully-designed perturbations improves MRI reconstruction quality at high acceleration rates, surpassing existing self-supervised techniques.

Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful
approach for improved reconstruction of rapid MRI scans. In order to train
these models in scenarios where fully-sampled reference data is unavailable,
self-supervised learning has gained prominence. However, its application at
high acceleration rates frequently introduces artifacts, compromising image
fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL
networks via carefully-designed perturbations. In particular, we enhance the
k-space masking idea of conventional self-supervised learning with a novel
consistency term that assesses the model's ability to accurately predict the
added perturbations in a sparse domain, leading to more reliable and
artifact-free reconstructions. The results obtained from the fastMRI knee and
brain datasets show that the proposed training strategy effectively reduces
aliasing artifacts and mitigates noise amplification at high acceleration
rates, outperforming state-of-the-art self-supervised methods both visually and
quantitatively.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [387] [Online Fair Division with Additional Information](https://arxiv.org/abs/2505.24503)
*Tzeh Yuan Neoh,Jannik Peters,Nicholas Teh*

Main category: cs.GT

TL;DR: 研究了在在线环境下公平分配不可分割物品的问题，发现未来物品信息的可用性影响了公平分配的存在性和近似性。在没有任何此类信息的情况下，实现近似的公平保证是非常困难的；而拥有额外的信息（如归一化信息或频率预测）可以设计更公平的在线算法。


<details>
  <summary>Details</summary>
Motivation: 研究在线环境下公平分配不可分割物品的问题，探讨未来物品信息的可用性如何影响公平分配的存在性和近似性。

Method: 1. 建立强不可能性结果：在没有任何关于未来物品信息的情况下，展示实现近似公平保证的固有难度。2. 提出一个基于归一化信息的算法：该算法比之前已知的结果提供更强的公平保证。3. 引入一个元算法：利用频率预测匹配最佳已知的离线保证，适用于广泛的“基于份额”的公平概念。

Result: 1. 在没有任何关于未来物品信息的情况下，建立强不可能性结果，展示实现近似公平保证的固有难度。2. 给定归一化信息时，提出的算法实现了比之前更强的公平保证。3. 给定频率预测时，元算法能够匹配最佳已知的离线保证。

Conclusion: 未来物品信息的不确定性对公平分配构成了限制，但利用结构化信息可以实现更公平的结果。

Abstract: We study the problem of fairly allocating indivisible goods to agents in an
online setting, where goods arrive sequentially and must be allocated
irrevocably to agents. Focusing on the popular fairness notions of
envy-freeness, proportionality, and maximin share fairness (and their
approximate variants), we ask how the availability of information on future
goods influences the existence and approximability of fair allocations. In the
absence of any such information, we establish strong impossibility results,
demonstrating the inherent difficulty of achieving even approximate fairness
guarantees. In contrast, we demonstrate that knowledge of additional
information -- such as aggregate of each agent's total valuations
(equivalently, normalized valuations) or the multiset of future goods values
(frequency predictions) -- would enable the design of fairer online algorithms.
Given normalization information, we propose an algorithm that achieves stronger
fairness guarantees than previously known results. Given frequency predictions,
we introduce a meta-algorithm that leverages frequency predictions to match the
best-known offline guarantees for a broad class of ''share-based'' fairness
notions. Our complementary impossibility results in each setting underscore
both the limitations imposed by uncertainty about future goods and the
potential of leveraging structured information to achieve fairer outcomes in
online fair division.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [388] [Quantum computing and artificial intelligence: status and perspectives](https://arxiv.org/abs/2505.23860)
*Giovanni Acampora,Andris Ambainis,Natalia Ares,Leonardo Banchi,Pallavi Bhardwaj,Daniele Binosi,G. Andrew D. Briggs,Tommaso Calarco,Vedran Dunjko,Jens Eisert,Olivier Ezratty,Paul Erker,Federico Fedele,Elies Gil-Fuster,Martin Gärttner,Mats Granath,Markus Heyl,Iordanis Kerenidis,Matthias Klusch,Anton Frisk Kockum,Richard Kueng,Mario Krenn,Jörg Lässig,Antonio Macaluso,Sabrina Maniscalco,Florian Marquardt,Kristel Michielsen,Gorka Muñoz-Gil,Daniel Müssig,Hendrik Poulsen Nautrup,Evert van Nieuwenburg,Roman Orus,Jörg Schmiedmayer,Markus Schmitt,Philipp Slusallek,Filippo Vicentini,Christof Weitenberg,Frank K. Wilhelm*

Main category: quant-ph

TL;DR: This white paper explores the intersection of quantum computing and AI, discussing mutual benefits, use cases, and providing a long-term research agenda with recommendations.


<details>
  <summary>Details</summary>
Motivation: To understand the interaction and mutual benefits between quantum computing and AI, and to provide a long-term research agenda addressing foundational questions and challenges in this field.

Method: Discussion and exploration of various points of intersection between quantum computing and AI, examination of classical AI use cases empowering quantum technology research, and proposal of a research agenda.

Result: Identification of how quantum computing can support innovative AI solutions and how classical AI can empower quantum technologies, along with a set of recommendations for advancing the field.

Conclusion: The paper concludes with recommendations on orchestrating theoretical work, aligning developments with hardware roadmaps, estimating resources, optimizing energy consumption, advancing software engineering in the field, and enhancing industrial competitiveness while considering societal impacts.

Abstract: This white paper discusses and explores the various points of intersection
between quantum computing and artificial intelligence (AI). It describes how
quantum computing could support the development of innovative AI solutions. It
also examines use cases of classical AI that can empower research and
development in quantum technologies, with a focus on quantum computing and
quantum sensing. The purpose of this white paper is to provide a long-term
research agenda aimed at addressing foundational questions about how AI and
quantum computing interact and benefit one another. It concludes with a set of
recommendations and challenges, including how to orchestrate the proposed
theoretical work, align quantum AI developments with quantum hardware roadmaps,
estimate both classical and quantum resources - especially with the goal of
mitigating and optimizing energy consumption - advance this emerging hybrid
software engineering discipline, and enhance European industrial
competitiveness while considering societal implications.

</details>


### [389] [Leveraging machine learning features for linear optical interferometer control](https://arxiv.org/abs/2505.24032)
*Sergei S. Kuzmin,Ivan V. Dyakonov,Stanislav S. Straupe*

Main category: quant-ph

TL;DR: An algorithm for constructing a model of a reconfigurable optical interferometer is developed, which uses supervised learning and optimization to determine phase shifts for unitary transformations without needing precise analytical solutions.


<details>
  <summary>Details</summary>
Motivation: To create a model of a reconfigurable optical interferometer that is independent of specific architectural constraints and can effectively tune interferometers without requiring a precise analytical solution.

Method: The algorithm employs a supervised learning approach to align the interferometer model with a training set derived from the device being studied. Then, an optimization procedure is used to determine the phase shifts of the interferometer with a specific architecture to obtain the required unitary transformation.

Result: This approach allows for the effective tuning of interferometers without needing a precise analytical solution, opening up possibilities for exploring new interferometric circuit architectures.

Conclusion: The developed algorithm successfully constructs a model of a reconfigurable optical interferometer using supervised learning and optimization, allowing for exploration of new interferometric circuit architectures.

Abstract: We have developed an algorithm that constructs a model of a reconfigurable
optical interferometer, independent of specific architectural constraints. The
programming of unitary transformations on the interferometer's optical modes
relies on either an analytical method for deriving the unitary matrix from a
set of phase shifts or an optimization routine when such decomposition is not
available. Our algorithm employs a supervised learning approach, aligning the
interferometer model with a training set derived from the device being studied.
A straightforward optimization procedure leverages this trained model to
determine the phase shifts of the interferometer with a specific architecture,
obtaining the required unitary transformation. This approach enables the
effective tuning of interferometers without requiring a precise analytical
solution, paving the way for the exploration of new interferometric circuit
architectures.

</details>


### [390] [Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications](https://arxiv.org/abs/2505.24765)
*Srikanth Thudumu,Jason Fisher,Hung Du*

Main category: quant-ph

TL;DR: This paper reviews recent developments in supervised Quantum Machine Learning (QML), focusing on methods like variational quantum circuits, quantum neural networks, and quantum kernel methods. It examines experimental studies showing partial quantum advantage, describes current limitations, and provides a ten-year outlook for possible developments in supervised QML.


<details>
  <summary>Details</summary>
Motivation: To explore the intersection of quantum computing and classical machine learning by reviewing recent advancements in supervised QML and providing insights into its future potential.

Method: Reviewing recent developments in supervised QML methods such as variational quantum circuits, quantum neural networks, and quantum kernel methods along with hybrid quantum-classical workflows. Examining experimental studies that indicate quantum advantage and identifying current limitations including noise, barren plateaus, scalability issues, and lack of formal proofs.

Result: The review highlights partial indications of quantum advantage from recent experimental studies while outlining the challenges such as noise, barren plateaus, scalability issues, and the need for formal proofs of performance improvement over classical methods.

Conclusion: A ten-year outlook (2025-2035) is provided, detailing possible developments in supervised QML and presenting a roadmap for its use in applied research and enterprise systems under specific conditions.

Abstract: Supervised Quantum Machine Learning (QML) represents an intersection of
quantum computing and classical machine learning, aiming to use quantum
resources to support model training and inference. This paper reviews recent
developments in supervised QML, focusing on methods such as variational quantum
circuits, quantum neural networks, and quantum kernel methods, along with
hybrid quantum-classical workflows. We examine recent experimental studies that
show partial indications of quantum advantage and describe current limitations
including noise, barren plateaus, scalability issues, and the lack of formal
proofs of performance improvement over classical methods. The main contribution
is a ten-year outlook (2025-2035) that outlines possible developments in
supervised QML, including a roadmap describing conditions under which QML may
be used in applied research and enterprise systems over the next decade.

</details>
