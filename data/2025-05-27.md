<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 127]
- [cs.LG](#cs.LG) [Total: 297]
- [cs.CR](#cs.CR) [Total: 50]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.DL](#cs.DL) [Total: 3]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.CL](#cs.CL) [Total: 117]
- [cs.ET](#cs.ET) [Total: 1]
- [econ.GN](#econ.GN) [Total: 2]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.DS](#cs.DS) [Total: 3]
- [eess.IV](#eess.IV) [Total: 7]
- [eess.SY](#eess.SY) [Total: 3]
- [eess.SP](#eess.SP) [Total: 15]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CV](#cs.CV) [Total: 68]
- [cs.MA](#cs.MA) [Total: 6]
- [math.NA](#math.NA) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 14]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 18]
- [cs.DC](#cs.DC) [Total: 2]
- [eess.AS](#eess.AS) [Total: 6]
- [stat.ML](#stat.ML) [Total: 28]
- [cs.CY](#cs.CY) [Total: 8]
- [cs.HC](#cs.HC) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The end of radical concept nativism](https://arxiv.org/abs/2505.18277)
*Joshua S. Rule,Steven T. Piantadosi*

Main category: cs.AI

TL;DR: 尽管人类似乎是出色的学习者，但认知科学和心灵哲学长期以来一直认为学习全新的东西是不可能的。本文回顾了先前关于激进概念先天论的争论，并通过计算机科学和信息理论中的概念指出其在表达能力、概念结构和概念拥有方面与实际人类认知的偏差，最终得出人们确实可以学习新概念的结论。


<details>
  <summary>Details</summary>
Motivation: 探讨并反驳Jerry Fodor提出的激进概念先天论，即大多数（如果不是全部）概念是天生的，所谓的概念学习实际上并未导致新概念的获取。

Method: 1. 回顾先前关于激进概念先天论的特征和局限性。2. 识别三个关键点：表达能力、概念结构和概念拥有，分析这些点上的偏差。3. 使用计算机科学和信息理论中的思想来形式化相关概念。

Result: 通过使用计算机科学和信息理论的形式化方法，揭示了激进概念先天论在描述实际人类认知时存在的问题。

Conclusion: 人们确实能够在某种重要意义上学习新的概念，反驳了激进概念先天论的观点。

Abstract: Though humans seem to be remarkable learners, arguments in cognitive science
and philosophy of mind have long maintained that learning something
fundamentally new is impossible. Specifically, Jerry Fodor's arguments for
radical concept nativism hold that most, if not all, concepts are innate and
that what many call concept learning never actually leads to the acquisition of
new concepts. These arguments have deeply affected cognitive science, and many
believe that the counterarguments to radical concept nativism have been either
unsuccessful or only apply to a narrow class of concepts. This paper first
reviews the features and limitations of prior arguments. We then identify three
critical points - related to issues of expressive power, conceptual structure,
and concept possession - at which the arguments in favor of radical concept
nativism diverge from describing actual human cognition. We use ideas from
computer science and information theory to formalize the relevant ideas in ways
that are arguably more scientifically productive. We conclude that, as a
result, there is an important sense in which people do indeed learn new
concepts.

</details>


### [2] [Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary](https://arxiv.org/abs/2505.18325)
*Licheng Pan,Yongqi Tong,Xin Zhang,Xiaolu Zhang,Jun Zhou,Zhixuan Chu*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）存在过度拒绝合理请求的问题，这源于过于保守的安全对齐策略。本文研究了这一现象，并提出了RASS框架，通过生成和选择接近安全边界的提示来缓解过度拒绝问题，同时提供更精确的模型安全决策视图并支持多语言场景。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多任务中表现出色，但它们常常过度拒绝合理的查询，即所谓的过度拒绝现象。这种现象通常源于过于保守的安全对齐策略，导致模型将许多合理的提示视为潜在风险。为系统地理解这一问题，需要探究模型的安全决策边界。

Method: 提出了一种名为RASS的自动化框架，用于生成和选择提示。该框架利用表示空间中的转向向量，高效识别和整理与安全边界对齐的提示，从而更有针对性地缓解过度拒绝问题。此外，还构建了MORBench评估集，以促进对多种语言模型的安全性和有用性进行稳健评估。

Result: RASS框架能够更有效地识别和缓解过度拒绝问题，同时提供更精确和可解释的模型安全决策视图，并且可以无缝扩展到多语言场景。

Conclusion: 通过探查和利用模型的安全决策边界，可以更好地理解和缓解过度拒绝问题。RASS框架为此提供了有效的方法，并且其代码和数据集将在未来发布。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they often refuse to answer legitimate queries-a
phenomenon known as overrefusal. Overrefusal typically stems from
over-conservative safety alignment, causing models to treat many reasonable
prompts as potentially risky. To systematically understand this issue, we probe
and leverage the models'safety decision boundaries to analyze and mitigate
overrefusal. Our findings reveal that overrefusal is closely tied to
misalignment at these boundary regions, where models struggle to distinguish
subtle differences between benign and harmful content. Building on these
insights, we present RASS, an automated framework for prompt generation and
selection that strategically targets overrefusal prompts near the safety
boundary. By harnessing steering vectors in the representation space, RASS
efficiently identifies and curates boundary-aligned prompts, enabling more
effective and targeted mitigation of overrefusal. This approach not only
provides a more precise and interpretable view of model safety decisions but
also seamlessly extends to multilingual scenarios.We have explored the safety
decision boundaries of various LLMs and construct the MORBench evaluation set
to facilitate robust assessment of model safety and helpfulness across multiple
languages. Code and datasets will be released at
https://anonymous.4open.science/r/RASS-80D3.

</details>


### [3] [RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification](https://arxiv.org/abs/2505.18380)
*Praphul Singh,Charlotte Dzialo,Jangwon Kim,Sumana Srivatsa,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: The paper introduces RedactOR, a multi-modal framework for de-identifying electronic health records, which employs cost-efficient strategies and enhances data coherence. Evaluated on the i2b2 2014 De-ID dataset, it achieves competitive performance while reducing LLM costs.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to ensure clinical data privacy while preserving utility in AI-driven healthcare and data analytics. Existing de-identification methods suffer from recall errors, limited generalization, and inefficiencies.

Method: RedactOR uses cost-efficient de-identification strategies including intelligent routing, hybrid rule and LLM based approaches, and a two-step audio redaction approach. It also includes a retrieval-based entity relexicalization approach to ensure consistent substitutions of protected entities.

Result: Evaluated on the i2b2 2014 De-ID dataset using standard metrics with strict recall, the approach achieves competitive performance while optimizing token usage to reduce LLM costs.

Conclusion: The authors discuss key lessons and insights from deploying RedactOR in real-world AI-driven healthcare data pipelines.

Abstract: Ensuring clinical data privacy while preserving utility is critical for
AI-driven healthcare and data analytics. Existing de-identification (De-ID)
methods, including rule-based techniques, deep learning models, and large
language models (LLMs), often suffer from recall errors, limited
generalization, and inefficiencies, limiting their real-world applicability. We
propose a fully automated, multi-modal framework, RedactOR for de-identifying
structured and unstructured electronic health records, including clinical audio
records. Our framework employs cost-efficient De-ID strategies, including
intelligent routing, hybrid rule and LLM based approaches, and a two-step audio
redaction approach. We present a retrieval-based entity relexicalization
approach to ensure consistent substitutions of protected entities, thereby
enhancing data coherence for downstream applications. We discuss key design
desiderata, de-identification and relexicalization methodology, and modular
architecture of RedactX and its integration with the Oracle Health Clinical AI
system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with
strict recall, our approach achieves competitive performance while optimizing
token usage to reduce LLM costs. Finally, we discuss key lessons and insights
from deployment in real-world AI- driven healthcare data pipelines.

</details>


### [4] [Advertising in AI systems: Society must be vigilant](https://arxiv.org/abs/2505.18425)
*Menghua Wu,Yujia Bao*

Main category: cs.AI

TL;DR: AI systems are becoming key interfaces to the Internet, and commercial incentives will shape their outputs. This paper discusses design principles for commercially-influenced AI systems, strategies for users to mitigate commercial biases, and open questions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address how commercial content could be delivered through generative AI-based systems, considering the dynamic, personalized nature of AI outputs and the implications for transparency and regulation.

Method: The method involves analyzing the requirements of key stakeholders (advertisers, consumers, and platforms) to propose design principles for commercially-influenced AI systems. It also outlines strategies for end users to identify and mitigate commercial biases from model outputs.

Result: The result is a set of design principles for commercially-influenced AI systems and high-level strategies for users to deal with commercial biases in AI-generated content.

Conclusion: The paper concludes with open questions and a call to action towards achieving transparency and regulation in commercially-influenced AI systems.

Abstract: AI systems have increasingly become our gateways to the Internet. We argue
that just as advertising has driven the monetization of web search and social
media, so too will commercial incentives shape the content served by AI. Unlike
traditional media, however, the outputs of these systems are dynamic,
personalized, and lack clear provenance -- raising concerns for transparency
and regulation. In this paper, we envision how commercial content could be
delivered through generative AI-based systems. Based on the requirements of key
stakeholders -- advertisers, consumers, and platforms -- we propose design
principles for commercially-influenced AI systems. We then outline high-level
strategies for end users to identify and mitigate commercial biases from model
outputs. Finally, we conclude with open questions and a call to action towards
these goals.

</details>


### [5] [EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks](https://arxiv.org/abs/2505.18457)
*Abir Ray*

Main category: cs.AI

TL;DR: This paper presents EdgeAgentX, a framework for military communication networks combining federated learning, multi-agent reinforcement learning, and adversarial defense mechanisms. It improves decision-making, reduces latency, enhances throughput, and withstands disruptions.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve autonomous decision-making, reduce latency, enhance throughput, and robustly handle adversarial disruptions in military communication networks.

Method: The method involves creating the EdgeAgentX framework which integrates federated learning (FL), multi-agent reinforcement learning (MARL), and adversarial defense mechanisms.

Result: Comprehensive simulations show that EdgeAgentX significantly improves autonomous decision-making, reduces latency, enhances throughput, and robustly withstands adversarial disruptions.

Conclusion: EdgeAgentX is a promising solution for enhancing the performance and security of military communication networks.

Abstract: This paper introduces EdgeAgentX, a novel framework integrating federated
learning (FL), multi-agent reinforcement learning (MARL), and adversarial
defense mechanisms, tailored for military communication networks. EdgeAgentX
significantly improves autonomous decision-making, reduces latency, enhances
throughput, and robustly withstands adversarial disruptions, as evidenced by
comprehensive simulations.

</details>


### [6] [Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark](https://arxiv.org/abs/2505.18467)
*Unggi Lee,Jaeyong Lee,Jiyeong Bae,Yeil Jeong,Junbo Koh,Gyeonggeon Lee,Gunho Lee,Taekyung Ahn,Hyeoncheol Kim*

Main category: cs.AI

TL;DR: Recent advances in LRMs show strong performance in structured domains, but lack pedagogical coherence and realistic teaching behaviors. To solve this problem, Pedagogy-R1 framework was introduced with three innovations.


<details>
  <summary>Details</summary>
Motivation: LRMs have shown strong performance in structured domains such as mathematics and programming, but they often lack pedagogical coherence and realistic teaching behaviors.

Method: The method involves introducing the Pedagogy-R1 framework that adapts LRMs for classroom use through three innovations: a distillation-based pipeline, Well-balanced Educational Benchmark (WBEB), and Chain-of-Pedagogy (CoP) prompting strategy.

Result: The mixed-method evaluation combines quantitative metrics with qualitative analysis, providing the first systematic assessment of LRMs' pedagogical strengths and limitations.

Conclusion: Pedagogy-R1 is a promising framework to adapt LRMs for classroom use.

Abstract: Recent advances in large reasoning models (LRMs) show strong performance in
structured domains such as mathematics and programming; however, they often
lack pedagogical coherence and realistic teaching behaviors. To bridge this
gap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use
through three innovations: (1) a distillation-based pipeline that filters and
refines model outputs for instruction-tuning, (2) the Well-balanced Educational
Benchmark (WBEB), which evaluates performance across subject knowledge,
pedagogical knowledge, tracing, essay scoring, and teacher decision-making, and
(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting
teacher-style reasoning. Our mixed-method evaluation combines quantitative
metrics with qualitative analysis, providing the first systematic assessment of
LRMs' pedagogical strengths and limitations.

</details>


### [7] [Chemical classification program synthesis using generative artificial intelligence](https://arxiv.org/abs/2505.18470)
*Christopher J. Mungall,Adnan Malik,Daniel R. Korn,Justin T. Reese,Noel M. O'Boyle,Noel,Janna Hastings*

Main category: cs.AI

TL;DR: This paper presents C3PO, a generative AI approach for automatically writing chemical classifier programs with explainable results.


<details>
  <summary>Details</summary>
Motivation: Accurately classifying chemical structures is crucial in cheminformatics and bioinformatics, but manual classification is labor-intensive and current automated methods either rely on manually constructed rules or lack explainability.

Method: The approach uses generative artificial intelligence to create chemical classifier programs for classes in the ChEBI database. These programs provide efficient deterministic run-time classification of SMILES structures along with natural language explanations.

Result: The approach was validated against the ChEBI database and compared favorably to state-of-the-art deep learning models. It also successfully classified out-of-distribution examples from metabolomics repositories and natural product databases, identified systematic classification errors in existing databases, and demonstrated potential through an ensemble AI approach.

Conclusion: C3PO constitutes an explainable computable ontological model of chemical class nomenclature and shows promise for improving chemical classification accuracy and explainability.

Abstract: Accurately classifying chemical structures is essential for cheminformatics
and bioinformatics, including tasks such as identifying bioactive compounds of
interest, screening molecules for toxicity to humans, finding non-organic
compounds with desirable material properties, or organizing large chemical
libraries for drug discovery or environmental monitoring. However, manual
classification is labor-intensive and difficult to scale to large chemical
databases. Existing automated approaches either rely on manually constructed
classification rules, or the use of deep learning methods that lack
explainability.
  This work presents an approach that uses generative artificial intelligence
to automatically write chemical classifier programs for classes in the Chemical
Entities of Biological Interest (ChEBI) database. These programs can be used
for efficient deterministic run-time classification of SMILES structures, with
natural language explanations. The programs themselves constitute an
explainable computable ontological model of chemical class nomenclature, which
we call the ChEBI Chemical Class Program Ontology (C3PO).
  We validated our approach against the ChEBI database, and compared our
results against state of the art deep learning models. We also demonstrate the
use of C3PO to classify out-of-distribution examples taken from metabolomics
repositories and natural product databases. We also demonstrate the potential
use of our approach to find systematic classification errors in existing
chemical databases, and show how an ensemble artificial intelligence approach
combining generated ontologies, automated literature search, and multimodal
vision models can be used to pinpoint potential errors requiring expert
validation

</details>


### [8] [Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support](https://arxiv.org/abs/2505.18483)
*Hongjia Wu,Hongxin Zhang,Wei Chen,Jiazhi Xia*

Main category: cs.AI

TL;DR: An abstract about proposing the RAD method that integrates Multi-Criteria Decision Making with LLMs' semantic understanding capabilities to provide multi-level and transparent decision support.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based Retrieval-Augmented Generation methods lack quantitative weighting and traceable reasoning paths, making it difficult to offer multi-level and transparent decision support.

Method: The RAD method which integrates Multi-Criteria Decision Making with the semantic understanding capabilities of LLMs. It automatically extracts key criteria from industry documents, builds a weighted hierarchical decision model, and generates structured reports under model guidance.

Result: Experiments show that in various decision-making tasks, the decision reports generated by RAD significantly outperform existing methods in terms of detail, rationality, and structure.

Conclusion: RAD demonstrates its application value and potential in complex decision support scenarios.

Abstract: Various industries have produced a large number of documents such as
industrial plans, technical guidelines, and regulations that are structurally
complex and content-wise fragmented. This poses significant challenges for
experts and decision-makers in terms of retrieval and understanding. Although
existing LLM-based Retrieval-Augmented Generation methods can provide
context-related suggestions, they lack quantitative weighting and traceable
reasoning paths, making it difficult to offer multi-level and transparent
decision support. To address this issue, this paper proposes the RAD method,
which integrates Multi-Criteria Decision Making with the semantic understanding
capabilities of LLMs. The method automatically extracts key criteria from
industry documents, builds a weighted hierarchical decision model, and
generates structured reports under model guidance. The RAD framework introduces
explicit weight assignment and reasoning chains in decision generation to
ensure accuracy, completeness, and traceability. Experiments show that in
various decision-making tasks, the decision reports generated by RAD
significantly outperform existing methods in terms of detail, rationality, and
structure, demonstrating its application value and potential in complex
decision support scenarios.

</details>


### [9] [Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions](https://arxiv.org/abs/2505.18492)
*Jialiang Sun,Yuzhi Tang,Ao Li,Chris J. Maddison,Kuldeep S. Meel*

Main category: cs.AI

TL;DR: The paper introduces the ECP framework and ConstructiveBench dataset, enhancing mathematical reasoning by integrating LLMs with formal theorem proving. It improves answer construction accuracy from 14.54% to 45.06%, and correct proofs from 9.86% to 25.01%.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning is crucial in AI applications like education, program verification, and mathematical discovery. Existing methods either lack creativity or rigor when dealing with complex math problems.

Method: The ECP framework integrates LLM-based enumeration and conjecturing with formal theorem proving. It uses a neuro-symbolic approach to address both creative answer generation and rigorous proof verification.

Result: On the ConstructiveBench dataset, ECP improves answer construction accuracy from 14.54% (CoT baseline) to 45.06%. Combining ECP's answers with DeepSeek-Prover-V2-7B model generates correct proofs for 25.01% of problems, compared to 9.86% for symbolic-only baselines.

Conclusion: ECP demonstrates significant improvements in solving answer-construction problems and generating correct proofs. The publicly available code and dataset encourage further research in this area.

Abstract: Mathematical reasoning lies at the heart of artificial intelligence,
underpinning applications in education, program verification, and
research-level mathematical discovery. Mathematical competitions, in
particular, present two challenging problem types: theorem-proving, requiring
rigorous proofs of stated conclusions, and answer-construction, involving
hypothesizing and formally verifying mathematical objects. Large Language
Models (LLMs) effectively generate creative candidate answers but struggle with
formal verification, while symbolic provers ensure rigor but cannot efficiently
handle creative conjecture generation. We introduce the
Enumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method
integrating LLM-based enumeration and pattern-driven conjecturing with formal
theorem proving. We present ConstructiveBench, a dataset of 3,431
answer-construction problems in various math competitions with verified Lean
formalizations. On the ConstructiveBench dataset, ECP improves the accuracy of
answer construction from the Chain-of-Thought (CoT) baseline of 14.54% to
45.06% with the gpt-4.1-mini model. Moreover, combining with ECP's constructed
answers, the state-of-the-art DeepSeek-Prover-V2-7B model generates correct
proofs for 858 of the 3,431 constructive problems in Lean, achieving 25.01%
accuracy, compared to 9.86% for symbolic-only baselines. Our code and dataset
are publicly available at GitHub and HuggingFace, respectively.

</details>


### [10] [Knowledge Grafting of Large Language Models](https://arxiv.org/abs/2505.18502)
*Guodong Du,Xuanning Zhou,Junlin Li,Zhuo Li,Zesheng Shi,Wanyu Lin,Ho-Kin Tang,Xiucheng Li,Fangming Liu,Wenya Wang,Min Zhang,Jing Li*

Main category: cs.AI

TL;DR: GraftLLM is a novel method that stores source model capabilities in a target model with SkillPack format, preserving general capabilities and reducing parameter conflicts.


<details>
  <summary>Details</summary>
Motivation: Existing methods for cross-capability transfer primarily focus on small, homogeneous models, which limits their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs.

Method: GraftLLM introduces a SkillPack format to store source model capabilities in a target model. It employs a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge.

Result: Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning.

Conclusion: GraftLLM provides a scalable and efficient solution for cross-capability transfer, supporting forget-free continual learning and model fusion.

Abstract: Cross-capability transfer is a key challenge in large language model (LLM)
research, with applications in multi-task integration, model compression, and
continual learning. Recent works like FuseLLM and FuseChat have demonstrated
the potential of transferring multiple model capabilities to lightweight
models, enhancing adaptability and efficiency, which motivates our
investigation into more efficient cross-capability transfer methods. However,
existing approaches primarily focus on small, homogeneous models, limiting
their applicability. For large, heterogeneous models, knowledge distillation
with full-parameter fine-tuning often overlooks the student model's intrinsic
capacity and risks catastrophic forgetting, while PEFT methods struggle to
effectively absorb knowledge from source LLMs. To address these issues, we
introduce GraftLLM, a novel method that stores source model capabilities in a
target model with SkillPack format. This approach preserves general
capabilities, reduces parameter conflicts, and supports forget-free continual
learning and model fusion. We employ a module-aware adaptive compression
strategy to compress parameter updates, ensuring efficient storage while
maintaining task-specific knowledge. The resulting SkillPack serves as a
compact and transferable knowledge carrier, ideal for heterogeneous model
fusion and continual learning. Experiments across various scenarios demonstrate
that GraftLLM outperforms existing techniques in knowledge transfer, knowledge
fusion, and forget-free learning, providing a scalable and efficient solution
for cross-capability transfer. The code is publicly available at:
https://github.com/duguodong7/GraftLLM.

</details>


### [11] [LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs](https://arxiv.org/abs/2505.18517)
*Pooneh Mousavi,Shubham Gupta,Cem Subakan,Mirco Ravanelli*

Main category: cs.AI

TL;DR: Foundation models using large language models (LLMs) are successful in many tasks, but adapting them for audio-language tasks is difficult. This paper presents LiSTEN, a framework that uses learnable key-value pairs and dynamic prompt selection to adapt LLMs to speech and audio tasks. It balances general and specific knowledge, reduces overfitting, lessens dependence on large datasets, has competitive performance with fewer parameters, simplifies training, and enhances interpretability.


<details>
  <summary>Details</summary>
Motivation: There is a need to adapt foundation models based on large language models (LLMs) for general-purpose audio-language tasks, as current challenges include differing acoustic environments and task variations.

Method: LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs is introduced. It uses a dynamic prompt selection strategy with learnable key-value pairs which helps the model balance general and task-specific knowledge while avoiding overfitting in multitask settings. The approach also reduces reliance on large ASR or captioning datasets.

Result: The method achieves competitive performance with fewer trainable parameters, simplifies training through a single-stage process, and enhances interpretability by analyzing prompt diversity and overlap across different tasks.

Conclusion: LiSTEN provides an effective way to adapt LLMs to audio and speech tasks by addressing issues of overfitting, dataset dependency, parameter efficiency, training complexity, and interpretability.

Abstract: Foundation models based on large language models (LLMs) have shown great
success in handling various tasks and modalities. However, adapting these
models for general-purpose audio-language tasks is challenging due to
differences in acoustic environments and task variations. In this work, we
introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a
framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic
prompt selection strategy with learnable key-value pairs, allowing the model to
balance general and task-specific knowledge while avoiding overfitting in a
multitask setting. Our approach reduces dependence on large-scale ASR or
captioning datasets, achieves competitive performance with fewer trainable
parameters, and simplifies training by using a single-stage process.
Additionally, LiSTEN enhances interpretability by analyzing the diversity and
overlap of selected prompts across different tasks.

</details>


### [12] [Generative RLHF-V: Learning Principles from Multi-modal Human Preference](https://arxiv.org/abs/2505.18531)
*Jiayi Zhou,Jiaming Ji,Boyuan Chen,Jiapeng Sun,Wenqi Chen,Donghai Hong,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: The paper introduces Generative RLHF-V, a novel alignment framework integrating generative reward models (GRMs) with multi-modal reinforcement learning from human feedback (RLHF). It proposes a two-stage pipeline enhancing multi-modal scoring precision and generalization. Experiments show significant performance improvement over baseline.


<details>
  <summary>Details</summary>
Motivation: Traditional score-only reward models for aligning multi-modal large language models (MLLMs) with human intentions have limitations such as low accuracy, weak generalization, and poor interpretability. This restricts the progress of alignment methods like reinforcement learning from human feedback (RLHF).

Method: The method involves a two-stage pipeline: 1) Multi-modal generative reward modeling guided by reinforcement learning (RL) to actively capture human intention and predict correct pair-wise scores; 2) RL optimization through grouped comparison to enhance multi-modal scoring precision. This integrates GRMs with multi-modal RLHF.

Result: The framework improves the performance of 4 MLLMs across 7 benchmarks by 18.1%, compared to the baseline RLHF's improvement of only 5.3%. Additionally, it demonstrates near-linear improvement with an increasing number of candidate responses.

Conclusion: Generative RLHF-V successfully addresses the limitations of traditional score-only reward models by integrating GRMs with multi-modal RLHF, significantly improving out-of-distribution generalization and scoring precision.

Abstract: Training multi-modal large language models (MLLMs) that align with human
intentions is a long-term challenge. Traditional score-only reward models for
alignment suffer from low accuracy, weak generalization, and poor
interpretability, blocking the progress of alignment methods, e.g.,
reinforcement learning from human feedback (RLHF). Generative reward models
(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate
pair-wise responses, but their pair-wise paradigm makes it hard to generalize
to learnable rewards. We introduce Generative RLHF-V, a novel alignment
framework that integrates GRMs with multi-modal RLHF. We propose a two-stage
pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL
guides GRMs to actively capture human intention, then predict the correct
pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which
enhances multi-modal RL scoring precision by grouped responses comparison.
Experimental results demonstrate that, besides out-of-distribution
generalization of RM discrimination, our framework improves 4 MLLMs'
performance across 7 benchmarks by $18.1\%$, while the baseline RLHF is only
$5.3\%$. We further validate that Generative RLHF-V achieves a near-linear
improvement with an increasing number of candidate responses. Our code and
models can be found at https://generative-rlhf-v.github.io.

</details>


### [13] [RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval](https://arxiv.org/abs/2505.18541)
*Yongjie Wang,Jonathan Leung,Zhiqi Shen*

Main category: cs.AI

TL;DR: RoleRAG is a retrieval-based framework designed to improve character imitation by large language models through efficient entity disambiguation and boundary-aware retrieval, leading to more accurate and consistent role-playing.


<details>
  <summary>Details</summary>
Motivation: Large Language Models often generate content that is irrelevant or inconsistent with a character's background due to entity ambiguity and lack of awareness of the character's cognitive boundaries.

Method: Propose RoleRAG, a retrieval-based framework that integrates efficient entity disambiguation for knowledge indexing with a boundary-aware retriever for extracting contextually appropriate information from a structured knowledge graph.

Result: Experiments on role-playing benchmarks show that RoleRAG's calibrated retrieval helps both general-purpose and role-specific LLMs better align with character knowledge and reduce hallucinated responses.

Conclusion: RoleRAG effectively addresses the issues of entity ambiguity and lack of character cognitive boundary awareness, enhancing the performance of LLMs in character imitation.

Abstract: Large Language Models (LLMs) have shown promise in character imitation,
enabling immersive and engaging conversations. However, they often generate
content that is irrelevant or inconsistent with a character's background. We
attribute these failures to: (1) the inability to accurately recall
character-specific knowledge due to entity ambiguity, and (2) a lack of
awareness of the character's cognitive boundaries. To address these issues, we
propose RoleRAG, a retrieval-based framework that integrates efficient entity
disambiguation for knowledge indexing with a boundary-aware retriever for
extracting contextually appropriate information from a structured knowledge
graph. Experiments on role-playing benchmarks show that RoleRAG's calibrated
retrieval helps both general-purpose and role-specific LLMs better align with
character knowledge and reduce hallucinated responses.

</details>


### [14] [Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models](https://arxiv.org/abs/2505.18547)
*Min Cheng,Fatemeh Doudi,Dileep Kalathil,Mohammad Ghavamzadeh,Panganamala R. Kumar*

Main category: cs.AI

TL;DR: 通过结合微调模型的反向扩散过程，Diffusion Blend算法在推理时实现了多偏好对齐，无需额外微调，并在实验中超越了基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法在优化扩散模型时采用单一奖励函数和固定KL正则化，限制了其在实际应用中的灵活性，尤其是在需要平衡多个冲突目标或适应用户偏好变化的情况下。

Method: 提出了一种名为Diffusion Blend的新方法，该方法通过融合与微调模型相关的反向扩散过程，解决了推理时的多偏好对齐问题。具体地，设计了两个算法：DB-MPA用于多奖励对齐，DB-KLA用于控制KL正则化。

Result: 大量实验表明，Diffusion Blend算法在性能上始终优于相关基线方法，并且接近或超过单独微调模型的性能，从而实现了高效、用户驱动的推理时对齐。

Conclusion: Diffusion Blend提供了一种有效的方法来解决推理时的多偏好对齐问题，能够根据用户指定的奖励和正则化线性组合生成图像，而无需额外的微调。

Abstract: Reinforcement learning (RL) algorithms have been used recently to align
diffusion models with downstream objectives such as aesthetic quality and
text-image consistency by fine-tuning them to maximize a single reward function
under a fixed KL regularization. However, this approach is inherently
restrictive in practice, where alignment must balance multiple, often
conflicting objectives. Moreover, user preferences vary across prompts,
individuals, and deployment contexts, with varying tolerances for deviation
from a pre-trained base model. We address the problem of inference-time
multi-preference alignment: given a set of basis reward functions and a
reference KL regularization strength, can we design a fine-tuning procedure so
that, at inference time, it can generate images aligned with any user-specified
linear combination of rewards and regularization, without requiring additional
fine-tuning? We propose Diffusion Blend, a novel approach to solve
inference-time multi-preference alignment by blending backward diffusion
processes associated with fine-tuned models, and we instantiate this approach
with two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL
regularization control. Extensive experiments show that Diffusion Blend
algorithms consistently outperform relevant baselines and closely match or
exceed the performance of individually fine-tuned models, enabling efficient,
user-driven alignment at inference-time. The code is available at
https://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025.

</details>


### [15] [Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?](https://arxiv.org/abs/2505.18575)
*Yongjie Wang,Yibo Wang,Xin Zhou,Zhiqi Shen*

Main category: cs.AI

TL;DR: 研究发现，探针性能与LLM响应的不确定性有强关联：不确定性降低时探针性能提升，反之亦然。高响应方差意味着更多重要特征，这对探针模型更具挑战性。通过分析响应不确定性，可找到LLM表示与人类知识一致的具体实例。


<details>
  <summary>Details</summary>
Motivation: 探针技术在揭示大语言模型（LLM）如何编码人类可解释概念方面显示出潜力，但影响数据集适合进行有效探针训练的因素尚未被充分理解。

Method: 假设探针性能反映了LLM生成响应和内部特征空间的特性。通过定量分析一系列任务中的探针性能和LLM响应不确定性，进一步利用特征重要性分析深入探讨相关性。

Result: 探针性能与LLM响应不确定性呈强相关性；高LLM响应方差与更大数量的重要特征相关联，这增加了探针模型的难度并可能导致性能下降。此外，通过响应不确定性分析，可以识别出LLM表示与人类知识相一致的具体示例。

Conclusion: LLM响应的不确定性对探针性能有显著影响，且可以通过探针技术发现LLM中与人类知识一致的可解释推理。

Abstract: Probing techniques have shown promise in revealing how LLMs encode
human-interpretable concepts, particularly when applied to curated datasets.
However, the factors governing a dataset's suitability for effective probe
training are not well-understood. This study hypothesizes that probe
performance on such datasets reflects characteristics of both the LLM's
generated responses and its internal feature space. Through quantitative
analysis of probe performance and LLM response uncertainty across a series of
tasks, we find a strong correlation: improved probe performance consistently
corresponds to a reduction in response uncertainty, and vice versa.
Subsequently, we delve deeper into this correlation through the lens of feature
importance analysis. Our findings indicate that high LLM response variance is
associated with a larger set of important features, which poses a greater
challenge for probe models and often results in diminished performance.
Moreover, leveraging the insights from response uncertainty analysis, we are
able to identify concrete examples where LLM representations align with human
knowledge across diverse domains, offering additional evidence of interpretable
reasoning in LLMs.

</details>


### [16] [RvLLM: LLM Runtime Verification with Domain Knowledge](https://arxiv.org/abs/2505.18585)
*Yedi Zhang,Sun Yi Emma,Annabelle Lee Jia En,Annabelle Lee Jia En,Jin Song Dong*

Main category: cs.AI

TL;DR: RvLLM是一种新的运行时验证框架，结合了专门设计的规范语言ESL，允许领域专家定制特定领域的谓词，从而对大型语言模型（LLMs）的输出进行验证。该框架在三个任务上进行了评估，结果表明其能够有效地检测各种LLMs的错误输出。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在文本理解和生成方面表现出色，但它们容易产生不一致或错误的输出，这影响了其在需要准确性和可信度的高风险领域的可靠性。目前的研究主要集中在通用场景下检测和减轻模型的不当行为，而忽略了整合特定领域知识的潜力。

Method: 提出了一种新的规范语言ESL，使领域专家能够以简单直观的方式定制特定领域的谓词，并设计了一个运行时验证框架RvLLM，用于根据ESL中定义的特定领域约束来验证LLM的输出。

Result: 实验结果表明，RvLLM能够以轻量级和灵活的方式有效检测不同LLMs中的错误输出，揭示了即使具有令人印象深刻的能力，LLMs仍然容易出现低级错误。

Conclusion: 通过利用专家领域知识严格且高效地验证LLM输出，RvLLM提供了一个潜在的长期解决方案，可以弥补LLMs在解释性和推理保证方面的不足。

Abstract: Large language models (LLMs) have emerged as a dominant AI paradigm due to
their exceptional text understanding and generation capabilities. However,
their tendency to generate inconsistent or erroneous outputs challenges their
reliability, especially in high-stakes domains requiring accuracy and
trustworthiness. Existing research primarily focuses on detecting and
mitigating model misbehavior in general-purpose scenarios, often overlooking
the potential of integrating domain-specific knowledge. In this work, we
advance misbehavior detection by incorporating domain knowledge. The core idea
is to design a general specification language that enables domain experts to
customize domain-specific predicates in a lightweight and intuitive manner,
supporting later runtime verification of LLM outputs. To achieve this, we
design a novel specification language, ESL, and introduce a runtime
verification framework, RvLLM, to validate LLM output against domain-specific
constraints defined in ESL. We evaluate RvLLM on three representative tasks:
violation detection against Singapore Rapid Transit Systems Act, numerical
comparison, and inequality solving. Experimental results demonstrate that RvLLM
effectively detects erroneous outputs across various LLMs in a lightweight and
flexible manner. The results reveal that despite their impressive capabilities,
LLMs remain prone to low-level errors due to limited interpretability and a
lack of formal guarantees during inference, and our framework offers a
potential long-term solution by leveraging expert domain knowledge to
rigorously and efficiently verify LLM outputs.

</details>


### [17] [LLMs for Supply Chain Management](https://arxiv.org/abs/2505.18597)
*Haojie Wang,Jiuyun Jiang,L. Jeff Hong,Guangxin Jiang*

Main category: cs.AI

TL;DR: The paper introduces a RAG framework with a domain-specialized SCM LLM, showing expert-level competence in SCM tasks and games, improving performance, reproducing classical insights, uncovering novel behaviors, and offering fresh perspectives on SCM phenomena.


<details>
  <summary>Details</summary>
Motivation: To leverage large language models for advancing research in supply chain management by integrating external knowledge and demonstrating expert-level competence.

Method: Developed a retrieval-augmented generation (RAG) framework that incorporates external knowledge and a domain-specialized SCM LLM. Used LLMs to conduct horizontal and vertical supply chain games for analyzing competition and cooperation.

Result: Experiments show RAG significantly improves performance on SCM tasks, can reproduce insights from classical SCM literature, uncovers new behaviors, and offers fresh perspectives on phenomena like the bullwhip effect.

Conclusion: This work opens new avenues for exploring complex supply chain networks through the lens of LLMs, enhancing understanding of cooperation and competition.

Abstract: The development of large language models (LLMs) has provided new tools for
research in supply chain management (SCM). In this paper, we introduce a
retrieval-augmented generation (RAG) framework that dynamically integrates
external knowledge into the inference process, and develop a domain-specialized
SCM LLM, which demonstrates expert-level competence by passing standardized SCM
examinations and beer game tests. We further employ the use of LLMs to conduct
horizontal and vertical supply chain games, in order to analyze competition and
cooperation within supply chains. Our experiments show that RAG significantly
improves performance on SCM tasks. Moreover, game-theoretic analysis reveals
that the LLM can reproduce insights from the classical SCM literature, while
also uncovering novel behaviors and offering fresh perspectives on phenomena
such as the bullwhip effect. This paper opens the door for exploring
cooperation and competition for complex supply chain network through the lens
of LLMs.

</details>


### [18] [Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning](https://arxiv.org/abs/2505.18603)
*Ye Mo,Zirui Shao,Kai Ye,Xianwei Mao,Bo Zhang,Hangdi Xing,Peng Ye,Gang Huang,Kehan Chen,Zhou Huan,Zixu Yan,Sheng Zhou*

Main category: cs.AI

TL;DR: Multimodal large language models (MLLMs) struggle with document understanding due to the redundant information in document images. The paper introduces Doc-CoB, a mechanism that mimics human coarse-to-fine reading pattern, enabling MLLMs to focus on query-relevant regions. It includes an automatic pipeline and two tasks to enhance box identification and box-query reasoning. Experiments show significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs process entire document images without considering query relevance, often failing to focus on critical regions and producing unfaithful responses.

Method: Doc-CoB integrates human-style visual reasoning into MLLM without modifying its architecture. It allows the model to autonomously select the set of regions most relevant to the query and focus attention on them for further understanding. A fully automatic pipeline is designed, integrating a commercial MLLM with a layout analyzer, to generate training samples with intermediate visual reasoning supervision. Two enabling tasks improve box identification and box-query reasoning.

Result: Extensive experiments on seven benchmarks with four popular models show that Doc-CoB significantly improves performance.

Conclusion: Doc-CoB demonstrates its effectiveness and wide applicability in enhancing document understanding by MLLMs.

Abstract: Multimodal large language models (MLLMs) have made significant progress in
document understanding. However, the information-dense nature of document
images still poses challenges, as most queries depend on only a few relevant
regions, with the rest being redundant. Existing one-pass MLLMs process entire
document images without considering query relevance, often failing to focus on
critical regions and producing unfaithful responses. Inspired by the human
coarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a
simple-yet-effective mechanism that integrates human-style visual reasoning
into MLLM without modifying its architecture. Our method allows the model to
autonomously select the set of regions (boxes) most relevant to the query, and
then focus attention on them for further understanding. We first design a fully
automatic pipeline, integrating a commercial MLLM with a layout analyzer, to
generate 249k training samples with intermediate visual reasoning supervision.
Then we incorporate two enabling tasks that improve box identification and
box-query reasoning, which together enhance document understanding. Extensive
experiments on seven benchmarks with four popular models show that Doc-CoB
significantly improves performance, demonstrating its effectiveness and wide
applicability. All code, data, and models will be released publicly.

</details>


### [19] [Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs](https://arxiv.org/abs/2505.18607)
*Jonathan Leung,Yongjie Wang,Zhiqi Shen*

Main category: cs.AI

TL;DR: This paper proposes Goal-Oriented Graphs (GoGs) to improve LLMs' step-by-step reasoning, especially in games. GoGs enhance the ability of LLMs to construct coherent reasoning chains by identifying high-level goals and their subgoals, which significantly boosts performance in game-playing tasks compared to existing methods like GraphRAG.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have impressive general capabilities but face challenges with step-by-step reasoning, particularly in complex scenarios such as games. Current retrieval-augmented methods struggle due to fragmented entity-relation graphs and overly dense local connectivity.

Method: The authors propose a novel framework based on Goal-Oriented Graphs (GoGs). In this structure, each node represents a goal and its associated attributes, while edges encode logical dependencies between goals. The method retrieves reasoning paths by first identifying high-level goals and recursively retrieving their subgoals, thus forming coherent reasoning chains.

Result: Extensive experiments conducted on the Minecraft testbed show that the proposed method significantly enhances the reasoning ability of LLMs in game-playing tasks, outperforming GraphRAG and other baselines.

Conclusion: Goal-Oriented Graphs provide an effective way to improve the step-by-step reasoning of LLMs in complex applications such as games by enabling the construction of coherent reasoning chains.

Abstract: Large Language Models (LLMs) demonstrate impressive general capabilities but
often struggle with step-by-step reasoning, especially in complex applications
such as games. While retrieval-augmented methods like GraphRAG attempt to
bridge this gap through cross-document extraction and indexing, their
fragmented entity-relation graphs and overly dense local connectivity hinder
the construction of coherent reasoning. In this paper, we propose a novel
framework based on Goal-Oriented Graphs (GoGs), where each node represents a
goal and its associated attributes, and edges encode logical dependencies
between goals. This structure enables explicit retrieval of reasoning paths by
first identifying high-level goals and recursively retrieving their subgoals,
forming coherent reasoning chains to guide LLM prompting. Our method
significantly enhances the reasoning ability of LLMs in game-playing tasks, as
demonstrated by extensive experiments on the Minecraft testbed, outperforming
GraphRAG and other baselines.

</details>


### [20] [Mind The Gap: Deep Learning Doesn't Learn Deeply](https://arxiv.org/abs/2505.18623)
*Lucas Saldyt,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: This paper investigates how neural networks, specifically graph neural networks (GNNs), learn algorithmic reasoning by comparing conventionally learned parameters with those obtained through neural compilation, focusing on the expressability-trainability gaps and hypothesizing that inductive learning is most effective for parallel algorithms.


<details>
  <summary>Details</summary>
Motivation: To understand the faithfulness of learned algorithms when effective and the reasons behind failures in learning effective algorithms otherwise.

Method: Using neural compilation to encode source algorithms into neural network parameters, enabling exact computation and comparison between compiled and conventionally learned parameters, vectors, and behaviors.

Result: The analysis focuses on GNNs and their alignment with algorithmic reasoning tasks, introducing a novel application of neural compilation to these networks.

Conclusion: The study aims to characterize expressability-trainability gaps in learning algorithmic reasoning and suggests that inductive learning may be most effective for certain parallel algorithms.

Abstract: This paper aims to understand how neural networks learn algorithmic reasoning
by addressing two questions: How faithful are learned algorithms when they are
effective, and why do neural networks fail to learn effective algorithms
otherwise? To answer these questions, we use neural compilation, a technique
that directly encodes a source algorithm into neural network parameters,
enabling the network to compute the algorithm exactly. This enables comparison
between compiled and conventionally learned parameters, intermediate vectors,
and behaviors. This investigation is crucial for developing neural networks
that robustly learn complexalgorithms from data. Our analysis focuses on graph
neural networks (GNNs), which are naturally aligned with algorithmic reasoning
tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the
spectrum of effective, faithful, and ineffective learned algorithms. Commonly,
learning algorithmic reasoning is framed as induction over synthetic data,
where a parameterized model is trained on inputs, traces, and outputs produced
by an underlying ground truth algorithm. In contrast, we introduce a neural
compilation method for GNNs, which sets network parameters analytically,
bypassing training. Focusing on GNNs leverages their alignment with algorithmic
reasoning, extensive algorithmic induction literature, and the novel
application of neural compilation to GNNs. Overall, this paper aims to
characterize expressability-trainability gaps - a fundamental shortcoming in
learning algorithmic reasoning. We hypothesize that inductive learning is most
effective for parallel algorithms contained within the computational class
\texttt{NC}.

</details>


### [21] [Riverine Flood Prediction and Early Warning in Mountainous Regions using Artificial Intelligence](https://arxiv.org/abs/2505.18645)
*Haleema Bibi,Sadia Saleem,Zakia Jalil,Muhammad Nasir,Tahani Alsubait*

Main category: cs.AI

TL;DR: The study focuses on flood forecasting in transboundary basins, using the Kabul River as a case. It applies various machine learning models for river flow prediction, with LSTM showing the best performance. The results align with several Sustainable Development Goals.


<details>
  <summary>Details</summary>
Motivation: Flooding poses significant risks globally, especially in mountainous regions due to complex terrains and climate changes. There's a need for effective flood control measures and early warning systems, which are hindered by challenges in obtaining upstream data in transboundary basins.

Method: The study used satellite-based climatic data and applied advanced machine-learning and deep learning models such as SVM, XGBoost, ANN, LSTM, and GRU to predict daily and multi-step river flow in the Kabul River basin.

Result: LSTM network outperformed other models with an R2 value of 0.96 and RMSE of 140.96 m3/sec. Time series LSTM and GRU models were effective for short-term forecasts up to five days, but accuracy declined beyond the fourth day.

Conclusion: The findings support Sustainable Development Goals related to water and disaster management. Longer-term historical datasets are needed for reliable long-term flood predictions.

Abstract: Flooding is the most devastating phenomenon occurring globally, particularly
in mountainous regions, risk dramatically increases due to complex terrains and
extreme climate changes. These situations are damaging livelihoods,
agriculture, infrastructure, and human lives. This study uses the Kabul River
between Pakistan and Afghanistan as a case study to reflect the complications
of flood forecasting in transboundary basins. The challenges in obtaining
upstream data impede the efficacy of flood control measures and early warning
systems, a common global problem in similar basins. Utilizing satellite-based
climatic data, this study applied numerous advanced machine-learning and deep
learning models, such as Support Vector Machines (SVM), XGBoost, and Artificial
Neural Networks (ANN), Long Short-Term Memory (LSTM) networks, and Gated
Recurrent Units (GRU) to predict daily and multi-step river flow. The LSTM
network outperformed other models, achieving the highest R2 value of 0.96 and
the lowest RMSE value of 140.96 m3/sec. The time series LSTM and GRU network
models, utilized for short-term forecasts of up to five days, performed
significantly. However, the accuracy declined beyond the fourth day,
highlighting the need for longer-term historical datasets for reliable
long-term flood predictions. The results of the study are directly aligned with
Sustainable Development Goals 6, 11, 13, and 15, facilitating disaster and
water management, timely evacuations, improved preparedness, and effective
early warning.

</details>


### [22] [MLLMs are Deeply Affected by Modality Bias](https://arxiv.org/abs/2505.18657)
*Xu Zheng,Chenfei Liao,Yuqian Fu,Kaiyu Lei,Yuanhuiyi Lyu,Lutao Jiang,Bin Ren,Jialei Chen,Jiawen Wang,Chengxin Li,Linfeng Zhang,Danda Pani Paudel,Xuanjing Huang,Yu-Gang Jiang,Nicu Sebe,Dacheng Tao,Luc Van Gool,Xuming Hu*

Main category: cs.AI

TL;DR: Multimodal Large Language Models (MLLMs) suffer from modality bias, which prioritizes language over other modalities like vision. This paper explores manifestations of this bias, identifies contributing factors, and proposes a research road-map to address it, advocating for balanced training strategies and model architectures.


<details>
  <summary>Details</summary>
Motivation: Recent advances in MLLMs have shown promising results but are affected by modality bias, where models rely more on language than on other modalities such as visual inputs.

Method: The authors diagnose the current state of modality bias, propose a systematic research road-map, identify key factors contributing to modality bias, and conduct experiments demonstrating the influence of these factors.

Result: The key factors identified include data characteristics, imbalanced backbone capabilities favoring language models, and training objectives that do not promote balanced cross-modal alignment. These findings highlight the need for balanced training strategies and model architectures.

Conclusion: The authors call for interdisciplinary efforts to tackle the challenges posed by modality bias and offer insights for developing more robust and generalizable multimodal systems, advancing progress toward Artificial General Intelligence.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown
promising results in integrating diverse modalities such as texts and images.
MLLMs are heavily influenced by modality bias, often relying on language while
under-utilizing other modalities like visual inputs. This position paper argues
that MLLMs are deeply affected by modality bias. Firstly, we diagnose the
current state of modality bias, highlighting its manifestations across various
tasks. Secondly, we propose a systematic research road-map related to modality
bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and
offer actionable suggestions for future research to mitigate it. To
substantiate these findings, we conduct experiments that demonstrate the
influence of each factor: 1. Data Characteristics: Language data is compact and
abstract, while visual data is redundant and complex, creating an inherent
imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The
dominance of pretrained language models in MLLMs leads to overreliance on
language and neglect of visual information. 3. Training Objectives: Current
objectives often fail to promote balanced cross-modal alignment, resulting in
shortcut learning biased toward language. These findings highlight the need for
balanced training strategies and model architectures to better integrate
multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle
these challenges and drive innovation in MLLM research. Our work provides a
fresh perspective on modality bias in MLLMs and offers insights for developing
more robust and generalizable multimodal systems-advancing progress toward
Artificial General Intelligence.

</details>


### [23] [TrajMoE: Spatially-Aware Mixture of Experts for Unified Human Mobility Modeling](https://arxiv.org/abs/2505.18670)
*Chonghua Han,Yuan Yuan,Kaiyan Chen,Jingtao Ding,Yong Li*

Main category: cs.AI

TL;DR: TrajMoE is a unified model for cross-city human mobility modeling, providing scalable and transferable solutions by addressing inconsistent spatial semantics and diverse urban mobility patterns.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generalization across cities due to heterogeneous spatial representations and mobility patterns. They often rely on numerical coordinates or need city-specific models, which limits scalability and transferability.

Method: TrajMoE incorporates a spatial semantic encoder that learns transferable location representations from POI-based functional semantics and visit patterns. It also features a Spatially-Aware Mixture-of-Experts (SAMoE) Transformer, which includes structured priors into experts specialized in different mobility semantics and a shared expert for capturing city-invariant patterns.

Result: Experiments show TrajMoE achieves up to 27% relative improvement over competitive models after one epoch of fine-tuning and outperforms baselines using only 5% of target city data.

Conclusion: TrajMoE represents a significant advancement towards creating a generalizable, transferable, and pretrainable foundation model for human mobility.

Abstract: Modeling human mobility across diverse cities is essential for applications
such as urban planning, transportation optimization, and personalized services.
However, generalization remains challenging due to heterogeneous spatial
representations and mobility patterns across cities. Existing methods typically
rely on numerical coordinates or require training city-specific models,
limiting their scalability and transferability. We propose TrajMoE, a unified
and scalable model for cross-city human mobility modeling. TrajMoE addresses
two key challenges: (1) inconsistent spatial semantics across cities, and (2)
diverse urban mobility patterns. To tackle these, we begin by designing a
spatial semantic encoder that learns transferable location representations from
POI-based functional semantics and visit patterns. Furthermore, we design a
Spatially-Aware Mixture-of-Experts (SAMoE) Transformer that injects structured
priors into experts specialized in distinct mobility semantics, along with a
shared expert to capture city-invariant patterns and enable adaptive cross-city
generalization. Extensive experiments demonstrate that TrajMoE achieves up to
27% relative improvement over competitive mobility foundation models after only
one epoch of fine-tuning, and consistently outperforms full-data baselines
using merely 5% of target city data. These results establish TrajMoE as a
significant step toward realizing a truly generalizable, transferable, and
pretrainable foundation model for human mobility.

</details>


### [24] [AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa](https://arxiv.org/abs/2505.18694)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.AI

TL;DR: This paper explores the use of generative AI, specifically large language models (LLMs), to simulate climate policy scenarios for Sub-Saharan Africa. The project focuses on energy transition themes from historical COP documents and aims to create plausible and diverse policy scenarios that align with regional climate goals and energy challenges. Automated techniques were employed for scenario evaluation due to limited access to human evaluators. Using the llama3.2-3B model, 30 out of 34 generated responses passed expert validation. These validated responses were compared against assessments from a human climate expert and two additional LLMs, showing that generative AI can effectively generate coherent, relevant, plausible, and diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for climate policy scenario generation and evaluation, such as integrated assessment models (IAMs) and expert-driven qualitative analysis, are time-intensive, reliant on simple extrapolations of past trends, and limited in capturing the complex and interconnected nature of energy and climate issues. This motivates the exploration of novel methods using generative AI to address these limitations.

Method: The method employs generative AI, specifically large language models (LLMs), to simulate climate policy scenarios for Sub-Saharan Africa focusing on energy transition themes derived from historical United Nations Climate Change Conference (COP) documents. Automated techniques are used for scenario evaluation due to limited access to human evaluators.

Result: Using the llama3.2-3B model, 30 out of 34 generated responses passed expert validation, accurately reflecting the intended impacts provided in the corresponding prompts. The structured, embedding-based evaluation framework shows that generative AI effectively generates scenarios that are coherent, relevant, plausible, and diverse.

Conclusion: Generative AI offers a transformative tool for climate policy planning in data-constrained regions like Sub-Saharan Africa, providing a robust alternative to traditional methods.

Abstract: Climate policy scenario generation and evaluation have traditionally relied
on integrated assessment models (IAMs) and expert-driven qualitative analysis.
These methods enable stakeholders, such as policymakers and researchers, to
anticipate impacts, plan governance strategies, and develop mitigation
measures. However, traditional methods are often time-intensive, reliant on
simple extrapolations of past trends, and limited in capturing the complex and
interconnected nature of energy and climate issues. With the advent of
artificial intelligence (AI), particularly generative AI models trained on vast
datasets, these limitations can be addressed, ensuring robustness even under
limited data conditions. In this work, we explore the novel method that employs
generative AI, specifically large language models (LLMs), to simulate climate
policy scenarios for Sub-Saharan Africa. These scenarios focus on energy
transition themes derived from the historical United Nations Climate Change
Conference (COP) documents. By leveraging generative models, the project aims
to create plausible and diverse policy scenarios that align with regional
climate goals and energy challenges. Given limited access to human evaluators,
automated techniques were employed for scenario evaluation. We generated policy
scenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%)
passed expert validation, accurately reflecting the intended impacts provided
in the corresponding prompts. We compared these validated responses against
assessments from a human climate expert and two additional LLMs (gemma2-2B and
mistral-7B). Our structured, embedding-based evaluation framework shows that
generative AI effectively generate scenarios that are coherent, relevant,
plausible, and diverse. This approach offers a transformative tool for climate
policy planning in data-constrained regions.

</details>


### [25] [AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational Cost in Medical Device Classification](https://arxiv.org/abs/2505.18695)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: Regulatory affairs in medicine and law can benefit from AI automation. This study evaluates various AI models for classification tasks in regulatory affairs using a medical device dataset, assessing accuracy, interpretability, and computational cost.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of AI-enabled automation in regulatory affairs at the intersection of medicine and law, specifically focusing on the critical classification task that affects market access, regulatory scrutiny, and patient safety.

Method: Investigate a wide range of AI models including traditional ML algorithms, deep learning architectures, and large language models using a regulatory dataset of medical device descriptions. Evaluate each model based on three dimensions: accuracy, interpretability, and computational cost.

Result: The evaluation results of different AI models on the classification task in regulatory affairs will be presented, showing their performance in terms of accuracy, interpretability, and computational cost.

Conclusion: AI models have significant potential to enhance the efficiency and effectiveness of classification tasks in regulatory affairs, impacting market access, regulatory scrutiny, and patient safety.

Abstract: Regulatory affairs, which sits at the intersection of medicine and law, can
benefit significantly from AI-enabled automation. Classification task is the
initial step in which manufacturers position their products to regulatory
authorities, and it plays a critical role in determining market access,
regulatory scrutiny, and ultimately, patient safety. In this study, we
investigate a broad range of AI models -- including traditional machine
learning (ML) algorithms, deep learning architectures, and large language
models -- using a regulatory dataset of medical device descriptions. We
evaluate each model along three key dimensions: accuracy, interpretability, and
computational cost.

</details>


### [26] [AI-Researcher: Autonomous Scientific Innovation](https://arxiv.org/abs/2505.18705)
*Jiabin Tang,Lianghao Xia,Zhonghang Li,Chao Huang*

Main category: cs.AI

TL;DR: The paper presents AI-Researcher, an autonomous system that handles the entire research pipeline with minimal human intervention, and Scientist-Bench, a benchmark for evaluating autonomous research capabilities. Experiments show AI-Researcher produces high-quality research outputs.


<details>
  <summary>Details</summary>
Motivation: To create a fully autonomous research system that can accelerate scientific innovation by systematically exploring solution spaces beyond human cognitive limitations.

Method: Introduction of AI-Researcher, a system that automates the research pipeline from literature review to manuscript preparation. Development of Scientist-Bench, a benchmark for assessing autonomous research capabilities through guided innovation and open-ended exploration tasks.

Result: AI-Researcher demonstrates high implementation success rates and generates research papers approaching human-level quality.

Conclusion: This work lays new foundations for autonomous scientific innovation that can complement human researchers.

Abstract: The powerful reasoning capabilities of Large Language Models (LLMs) in
mathematics and coding, combined with their ability to automate complex tasks
through agentic frameworks, present unprecedented opportunities for
accelerating scientific innovation. In this paper, we introduce AI-Researcher,
a fully autonomous research system that transforms how AI-driven scientific
discovery is conducted and evaluated. Our framework seamlessly orchestrates the
complete research pipeline--from literature review and hypothesis generation to
algorithm implementation and publication-ready manuscript preparation--with
minimal human intervention. To rigorously assess autonomous research
capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising
state-of-the-art papers across diverse AI research domains, featuring both
guided innovation and open-ended exploration tasks. Through extensive
experiments, we demonstrate that AI-Researcher achieves remarkable
implementation success rates and produces research papers that approach
human-level quality. This work establishes new foundations for autonomous
scientific innovation that can complement human researchers by systematically
exploring solution spaces beyond cognitive limitations.

</details>


### [27] [$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking](https://arxiv.org/abs/2505.18746)
*Peijie Yu,Yifan Yang,Jinjian Li,Zelong Zhang,Haorui Wang,Xiao Feng,Feng Zhang*

Main category: cs.AI

TL;DR: Agents based on large language models need to consider complex factors when interacting with environments. Current research overlooks these factors, so this paper presents an open-source benchmark called $C^3$-Bench to evaluate agent robustness through three challenges and fine-grained metrics.


<details>
  <summary>Details</summary>
Motivation: To address the gap in current research which typically evaluates agents via multi-turn dialogues but overlooks critical factors such as inter-tool relationships, environmental feedback and previous decisions.

Method: The method involves designing three challenges for agents: navigating complex tool relationships, handling critical hidden information and managing dynamic decision paths. The benchmark also includes fine-grained metrics, innovative data collection algorithms and reproducible evaluation methods.

Result: Experiments conducted on 49 mainstream agents revealed significant shortcomings in handling tool dependencies, long context information dependencies and frequent policy-type switching.

Conclusion: $C^3$-Bench aims to expose model vulnerabilities and drive research into the interpretability of agent performance.

Abstract: Agents based on large language models leverage tools to modify environments,
revolutionizing how AI interacts with the physical world. Unlike traditional
NLP tasks that rely solely on historical dialogue for responses, these agents
must consider more complex factors, such as inter-tool relationships,
environmental feedback and previous decisions, when making choices. Current
research typically evaluates agents via multi-turn dialogues. However, it
overlooks the influence of these critical factors on agent behavior. To bridge
this gap, we present an open-source and high-quality benchmark $C^3$-Bench.
This benchmark integrates attack concepts and applies univariate analysis to
pinpoint key elements affecting agent robustness. In concrete, we design three
challenges: navigate complex tool relationships, handle critical hidden
information and manage dynamic decision paths. Complementing these challenges,
we introduce fine-grained metrics, innovative data collection algorithms and
reproducible evaluation methods. Extensive experiments are conducted on 49
mainstream agents, encompassing general fast-thinking, slow-thinking and
domain-specific models. We observe that agents have significant shortcomings in
handling tool dependencies, long context information dependencies and frequent
policy-type switching. In essence, $C^3$-Bench aims to expose model
vulnerabilities through these challenges and drive research into the
interpretability of agent performance. The benchmark is publicly available at
https://github.com/yupeijei1997/C3-Bench.

</details>


### [28] [The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation](https://arxiv.org/abs/2505.18759)
*Ruichen Zhang,Rana Muhammad Shahroz Khan,Zhen Tan,Dawei Li,Song Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: This paper presents DC-CoT, the first data-centric benchmark for evaluating chain-of-thought (CoT) distillation methods in language models. It examines data manipulation effects using various teacher and student models, focusing on reasoning abilities, generalization, and cross-domain transfer.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive benchmarks to systematically assess data-centric distillation techniques aimed at creating smaller, more efficient student Large Language Models (LLMs) with strong reasoning abilities.

Method: DC-CoT investigates data manipulation in CoT distillation from method, model, and data perspectives by utilizing different teacher models and student architectures, rigorously evaluating impacts on student model performance across multiple reasoning datasets.

Result: The study provides actionable insights and best practices for optimizing CoT distillation through data-centric techniques, enhancing both in-distribution and out-of-distribution generalization as well as cross-domain transfer capabilities.

Conclusion: DC-CoT serves as an essential tool for advancing the development of more accessible and capable reasoning models via systematic evaluation of data-centric distillation approaches.

Abstract: Data-centric distillation, including data augmentation, selection, and
mixing, offers a promising path to creating smaller, more efficient student
Large Language Models (LLMs) that retain strong reasoning abilities. However,
there still lacks a comprehensive benchmark to systematically assess the effect
of each distillation approach. This paper introduces DC-CoT, the first
data-centric benchmark that investigates data manipulation in chain-of-thought
(CoT) distillation from method, model and data perspectives. Utilizing various
teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student
architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of
these data manipulations on student model performance across multiple reasoning
datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)
generalization, and cross-domain transfer. Our findings aim to provide
actionable insights and establish best practices for optimizing CoT
distillation through data-centric techniques, ultimately facilitating the
development of more accessible and capable reasoning models. The dataset can be
found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is
shared in https://anonymous.4open.science/r/DC-COT-FF4C/.

</details>


### [29] [Mitigating Deceptive Alignment via Self-Monitoring](https://arxiv.org/abs/2505.18807)
*Jiaming Ji,Wenqi Chen,Kaile Wang,Donghai Hong,Sitong Fang,Boyuan Chen,Jiayi Zhou,Juntao Dai,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 现代大型语言模型依赖于链式思维（CoT）推理来实现卓越性能，但这种机制可能会放大欺骗性对齐。本文提出了第一个嵌入自我监控器的框架CoT Monitor+，可以在模型生成过程中拦截欺骗行为。通过在强化学习中使用内部自评估信号作为辅助奖励，创建反馈循环以奖励诚实推理并阻止隐藏目标。同时引入了DeceptionBench基准测试系统研究欺骗性对齐。实验表明，CoT Monitor+可以减少43.8%的欺骗行为，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的安全管道将欺骗视为事后过滤的黑箱输出，而没有干预模型内部推理过程中的欺骗行为。因此，作者希望探索是否可以在模型思考时拦截欺骗行为，从而更有效地解决欺骗性对齐问题。

Method: 提出了一种名为CoT Monitor+的新框架，在模型生成过程中，除了常规推理步骤外，还生成一个内部自评估信号，该信号经过训练可以标记和抑制不一致策略，并将其作为强化学习中的辅助奖励。此外，还引入了DeceptionBench基准测试系统，用于系统地研究欺骗性对齐。

Result: CoT Monitor+平均减少了43.8%的欺骗行为，同时保持了任务准确性。当自我监控信号在RL微调中替代外部弱评审时，模型表现出显著较少的模糊思维并保留了透明度。

Conclusion: CoT Monitor+是一种有效的解决方案，能够在模型推理过程中拦截欺骗行为，减少欺骗性对齐的影响，同时保持任务准确性。这为未来的模型安全性和对齐研究提供了新的方向。

Abstract: Modern large language models rely on chain-of-thought (CoT) reasoning to
achieve impressive performance, yet the same mechanism can amplify deceptive
alignment, situations in which a model appears aligned while covertly pursuing
misaligned goals. Existing safety pipelines treat deception as a black-box
output to be filtered post-hoc, leaving the model free to scheme during its
internal reasoning. We ask: Can deception be intercepted while the model is
thinking? We answer this question, the first framework that embeds a
Self-Monitor inside the CoT process itself, named CoT Monitor+. During
generation, the model produces (i) ordinary reasoning steps and (ii) an
internal self-evaluation signal trained to flag and suppress misaligned
strategies. The signal is used as an auxiliary reward in reinforcement
learning, creating a feedback loop that rewards honest reasoning and
discourages hidden goals. To study deceptive alignment systematically, we
introduce DeceptionBench, a five-category benchmark that probes covert
alignment-faking, sycophancy, etc. We evaluate various LLMs and show that
unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT
Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task
accuracy. Further, when the self-monitor signal replaces an external weak judge
in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and
retain transparency. Our project website can be found at
cot-monitor-plus.github.io

</details>


### [30] [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822)
*Shijue Huang,Hongru Wang,Wanjun Zhong,Zhaochen Su,Jiazhan Feng,Bowen Cao,Yi R. Fung*

Main category: cs.AI

TL;DR: 现代大型推理模型虽然展示了强大的问题解决能力，但往往难以平衡效率和效果。本文提出了AdaCtrl框架，通过自我评估问题难度动态调整推理长度，并允许用户手动控制推理深度以优先考虑效率或效果。实验结果表明，与标准训练基线相比，AdaCtrl在复杂数据集上提高了性能并减少了响应长度，在简单数据集上大幅缩短了推理链长度，同时提供了精确的用户控制功能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在解决简单问题时会生成不必要的长推理链，难以有效平衡效率和效果，因此需要一种能够根据问题难度自适应调整推理长度的方法。

Method: 提出AdaCtrl框架，包括两个阶段的训练流程：1) 冷启动微调阶段，赋予模型自我评估问题难度和调整推理预算的能力；2) 基于难度感知的强化学习阶段，优化模型的自适应推理策略并校准其难度评估。此外，设计了显式长度触发标签作为用户控制预算的自然接口。

Result: 在多个数据集上的实验证明，AdaCtrl可以根据估计的难度调整推理长度，相较于标准训练基线，在AIME2024和AIME2025数据集上分别减少响应长度10.06%和12.14%，在MATH500和GSM8K数据集上分别减少62.05%和91.04%。同时，AdaCtrl实现了对推理预算的精确用户控制。

Conclusion: AdaCtrl框架成功实现了基于问题难度的自适应推理预算分配，并支持用户对推理深度的显式控制，从而在保证效果的同时提高了推理效率。

Abstract: Modern large reasoning models demonstrate impressive problem-solving
capabilities by employing sophisticated reasoning strategies. However, they
often struggle to balance efficiency and effectiveness, frequently generating
unnecessarily lengthy reasoning chains for simple problems. In this work, we
propose AdaCtrl, a novel framework to support both difficulty-aware adaptive
reasoning budget allocation and explicit user control over reasoning depth.
AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem
difficulty, while also allowing users to manually control the budget to
prioritize either efficiency or effectiveness. This is achieved through a
two-stage training pipeline: an initial cold-start fine-tuning phase to instill
the ability to self-aware difficulty and adjust reasoning budget, followed by a
difficulty-aware reinforcement learning (RL) stage that refines the model's
adaptive reasoning strategies and calibrates its difficulty assessments based
on its evolving capabilities during online training. To enable intuitive user
interaction, we design explicit length-triggered tags that function as a
natural interface for budget control. Empirical results show that AdaCtrl
adapts reasoning length based on estimated difficulty, compared to the standard
training baseline that also incorporates fine-tuning and RL, it yields
performance improvements and simultaneously reduces response length by 10.06%
and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which
require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K
datasets, where more concise responses are sufficient. Furthermore, AdaCtrl
enables precise user control over the reasoning budget, allowing for tailored
responses to meet specific needs.

</details>


### [31] [LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS](https://arxiv.org/abs/2505.18829)
*Kai Mei,Xi Zhu,Hang Gao,Shuhang Lin,Yongfeng Zhang*

Main category: cs.AI

TL;DR: The paper introduces AIOS 1.0, a platform that enhances computer-use agent capabilities by bridging the semantic gap between language models and computer interfaces through environmental contextualization. It uses a Model Context Protocol server architecture to abstract computer states and actions, simplifying decision-making for agents. LiteCUA, a lightweight agent built on AIOS 1.0, outperforms specialized frameworks on the OSWorld benchmark.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the identified limitation in current approaches to enhancing computer-use agents, which primarily focus on building powerful agent frameworks or models without addressing the fundamental issue of the semantic disconnect between language models' understanding of the world and the structure of computer interfaces.

Method: AIOS 1.0 transforms computers into contextual environments that language models can understand natively by implementing a Model Context Protocol (MCP) server architecture. This architecture abstracts computer states and actions, effectively decoupling interface complexity from decision complexity.

Result: LiteCUA, a lightweight computer-use agent built on AIOS 1.0, achieves a 14.66% success rate on the OSWorld benchmark, surpassing several specialized agent frameworks despite its simple architecture.

Conclusion: The results indicate that contextualizing computer environments for language models is a promising direction for developing more capable computer-use agents and advancing AI's interaction with digital systems.

Abstract: We present AIOS 1.0, a novel platform designed to advance computer-use agent
(CUA) capabilities through environmental contextualization. While existing
approaches primarily focus on building more powerful agent frameworks or
enhancing agent models, we identify a fundamental limitation: the semantic
disconnect between how language models understand the world and how computer
interfaces are structured. AIOS 1.0 addresses this challenge by transforming
computers into contextual environments that language models can natively
comprehend, implementing a Model Context Protocol (MCP) server architecture to
abstract computer states and actions. This approach effectively decouples
interface complexity from decision complexity, enabling agents to reason more
effectively about computing environments. To demonstrate our platform's
effectiveness, we introduce LiteCUA, a lightweight computer-use agent built on
AIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,
outperforming several specialized agent frameworks despite its simple
architecture. Our results suggest that contextualizing computer environments
for language models represents a promising direction for developing more
capable computer-use agents and advancing toward AI that can interact with
digital systems. The source code of LiteCUA is available at
https://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS
main branch as part of AIOS at https://github.com/agiresearch/AIOS.

</details>


### [32] [Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework](https://arxiv.org/abs/2505.18847)
*William Han,Chaojing Duan,Zhepeng Cen,Yihang Yao,Xiaoyu Song,Atharva Mhaskar,Dylan Leong,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.AI

TL;DR: Recent advances have increasingly applied large language models (LLMs) to electrocardiogram (ECG) interpretation, giving rise to Electrocardiogram-Language Models (ELMs). Researchers are curating instruction-tuning datasets that pair ECGs with textual dialogues and are training ELMs on these resources.


<details>
  <summary>Details</summary>
Motivation: To explore the fundamental question of what is the most effective ECG input representation for ELMs.

Method: Presented a comprehensive benchmark of three candidate ECG input representations (raw time-series signals, rendered images, and discretized symbolic sequences) across 6 public datasets and 5 evaluation metrics. Further ablated the LLM backbone, ECG duration, and token budget, and evaluated robustness to signal perturbations.

Result: Found symbolic representations achieve the greatest number of statistically significant wins over both signal and image inputs.

Conclusion: The findings offer clear guidance for selecting input representations when developing the next generation of ELMs.

Abstract: Recent advances have increasingly applied large language models (LLMs) to
electrocardiogram (ECG) interpretation, giving rise to
Electrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual
query, an ELM autoregressively generates a free-form textual response. Unlike
traditional classification-based systems, ELMs emulate expert cardiac
electrophysiologists by issuing diagnoses, analyzing waveform morphology,
identifying contributing factors, and proposing patient-specific action plans.
To realize this potential, researchers are curating instruction-tuning datasets
that pair ECGs with textual dialogues and are training ELMs on these resources.
Yet before scaling ELMs further, there is a fundamental question yet to be
explored: What is the most effective ECG input representation? In recent works,
three candidate representations have emerged-raw time-series signals, rendered
images, and discretized symbolic sequences. We present the first comprehensive
benchmark of these modalities across 6 public datasets and 5 evaluation
metrics. We find symbolic representations achieve the greatest number of
statistically significant wins over both signal and image inputs. We further
ablate the LLM backbone, ECG duration, and token budget, and we evaluate
robustness to signal perturbations. We hope that our findings offer clear
guidance for selecting input representations when developing the next
generation of ELMs.

</details>


### [33] [The Theory of the Unique Latent Pattern: A Formal Epistemic Framework for Structural Singularity in Complex Systems](https://arxiv.org/abs/2505.18850)
*Mohamed Aly Bouke*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal
epistemic framework that redefines the origin of apparent complexity in dynamic
systems. Rather than attributing unpredictability to intrinsic randomness or
emergent nonlinearity, ULP asserts that every analyzable system is governed by
a structurally unique, deterministic generative mechanism, one that remains
hidden not due to ontological indeterminacy, but due to epistemic constraints.
The theory is formalized using a non-universal generative mapping \(
\mathcal{F}_S(P_S, t) \), where each system \( S \) possesses its own latent
structure \( P_S \), irreducible and non-replicable across systems. Observed
irregularities are modeled as projections of this generative map through
observer-limited interfaces, introducing epistemic noise \( \varepsilon_S(t) \)
as a measure of incomplete access. By shifting the locus of uncertainty from
the system to the observer, ULP reframes chaos as a context-relative failure of
representation. We contrast this position with foundational paradigms in chaos
theory, complexity science, and statistical learning. While they assume or
model shared randomness or collective emergence, ULP maintains that every
instance harbors a singular structural identity. Although conceptual, the
theory satisfies the criterion of falsifiability in the Popperian sense, it
invites empirical challenge by asserting that no two systems governed by
distinct latent mechanisms will remain indistinguishable under sufficient
resolution. This opens avenues for structurally individuated models in AI,
behavioral inference, and epistemic diagnostics.

</details>


### [34] [Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems](https://arxiv.org/abs/2505.18857)
*Alexander Khrabry,Edward Startsev,Andrew Powis,Igor Kaganovich*

Main category: cs.AI

TL;DR: 提出了一种新颖的有效架构，用于学习复杂多尺度物理系统中的长期演化。通过分层全卷积自动编码器，将系统状态转换为一系列嵌入层，保留了不同尺度结构的空间信息。预测器同步推进所有嵌入层，并使用卷积算子组合来建模不同尺度特征之间的相互作用。在Hasegawa-Wakatani湍流应用中，与传统ResNet架构相比，该模型在关键统计特性上显著提高了长期预测准确性。


<details>
  <summary>Details</summary>
Motivation: 复杂多尺度物理系统中，动态出现的各种尺度结构之间存在局部相互作用。小尺度特征之间的远距离相互作用不需要建模，这为高效建模提供了可能性。

Method: 采用分层全卷积自动编码器将物理系统的状态转换为一系列嵌入层，浅层嵌入小尺度结构，深层嵌入大尺度结构。利用卷积算子组合建模不同尺度特征的相互作用。

Result: 在Hasegawa-Wakatani湍流的应用中，模型的关键统计特性的长期预测准确性得到了显著提高。

Conclusion: 所提出的架构能够有效学习复杂多尺度物理系统的长期演化，显著提升了预测精度。

Abstract: We propose a novel efficient architecture for learning long-term evolution in
complex multi-scale physical systems which is based on the idea of separation
of scales. Structures of various scales that dynamically emerge in the system
interact with each other only locally. Structures of similar scale can interact
directly when they are in contact and indirectly when they are parts of larger
structures that interact directly. This enables modeling a multi-scale system
in an efficient way, where interactions between small-scale features that are
apart from each other do not need to be modeled. The hierarchical
fully-convolutional autoencoder transforms the state of a physical system not
just into a single embedding layer, as it is done conventionally, but into a
series of embedding layers which encode structures of various scales preserving
spatial information at a corresponding resolution level. Shallower layers embed
smaller structures on a finer grid, while deeper layers embed larger structures
on a coarser grid. The predictor advances all embedding layers in sync.
Interactions between features of various scales are modeled using a combination
of convolutional operators. We compare the performance of our model to
variations of a conventional ResNet architecture in application to the
Hasegawa-Wakatani turbulence. A multifold improvement in long-term prediction
accuracy was observed for crucial statistical characteristics of this system.

</details>


### [35] [Digital Overconsumption and Waste: A Closer Look at the Impacts of Generative AI](https://arxiv.org/abs/2505.18894)
*Vanessa Utz,Steve DiPaola*

Main category: cs.AI

TL;DR: Generative AI systems contribute to digital waste through energy consumption and CO2 emissions, thus replicating harmful consumer behavior in the digital space. The paper discusses climate implications, societal impacts, and solution pathways.


<details>
  <summary>Details</summary>
Motivation: To address the negative contribution of generative AI systems to digital waste and highlight the urgency of discussing harmful consumer behavior replication in the digital space.

Method: Outlining previous work on climate implications of generative AI systems and user sentiment, then expanding the discussion to include digital overconsumption, waste, societal impacts, and possible solutions.

Result: Raises awareness about the climate and societal impacts of generative AI systems and suggests a pathway for potential solutions to mitigate digital overconsumption and waste.

Conclusion: There is an urgent need for discussions and actions regarding the harmful effects of generative AI systems on the environment and society.

Abstract: Generative Artificial Intelligence (AI) systems currently contribute
negatively to the production of digital waste, via the associated energy
consumption and the related CO2 emissions. At this moment, a discussion is
urgently needed on the replication of harmful consumer behavior, such as
overconsumption, in the digital space. We outline our previous work on the
climate implications of commercially available generative AI systems and the
sentiment of generative AI users when confronted with AI-related climate
research. We expand on this work via a discussion of digital overconsumption
and waste, other related societal impacts, and a possible solution pathway

</details>


### [36] [Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations](https://arxiv.org/abs/2505.18907)
*Sanjay Kariyappa,G. Edward Suh*

Main category: cs.AI

TL;DR: This paper proposes a novel method to enhance the security of large language models against prompt injection attacks by injecting an Instruction Hierarchy (IH) signal into intermediate token representations, resulting in significant reduction in attack success rates without compromising model utility.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks pose a significant threat to large language models, and current defense mechanisms that use IH signals at the input layer have limitations in effectively distinguishing token privilege levels across model layers.

Method: The paper introduces a technique to inject the IH signal into intermediate token representations using layer-specific trainable embeddings that encode privilege information.

Result: Evaluations show a 1.6x to 9.2x reduction in attack success rate for gradient-based prompt injection attacks compared to existing methods, with minimal impact on model utility.

Conclusion: Injecting IH signals into intermediate layers significantly improves defense against prompt injection attacks while maintaining model performance.

Abstract: Prompt injection attacks are a critical security vulnerability in large
language models (LLMs), allowing attackers to hijack model behavior by
injecting malicious instructions within the input context. Recent defense
mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often
implemented through special delimiter tokens or additive embeddings to denote
the privilege level of input tokens. However, these prior works typically
inject the IH signal exclusively at the initial input layer, which we
hypothesize limits its ability to effectively distinguish the privilege levels
of tokens as it propagates through the different layers of the model. To
overcome this limitation, we introduce a novel approach that injects the IH
signal into the intermediate token representations within the network. Our
method augments these representations with layer-specific trainable embeddings
that encode the privilege information. Our evaluations across multiple models
and training methods reveal that our proposal yields between $1.6\times$ and
$9.2\times$ reduction in attack success rate on gradient-based prompt injection
attacks compared to state-of-the-art methods, without significantly degrading
the model's utility.

</details>


### [37] [Meta-aware Learning in text-to-SQL Large Language Model](https://arxiv.org/abs/2505.18929)
*Wenda Zhang*

Main category: cs.AI

TL;DR: The paper proposes a meta-aware learning framework with four strategies to enhance LLM's SQL generation in business domains.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of understanding complex domain information and database structures in text-to-SQL tasks.

Method: The method involves a meta-aware learning framework that integrates domain knowledge, database schema, chain-of-thought reasoning, and metadata relationships. It includes schema-based learning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key information tokenization.

Result: The experiments show superiority in execution accuracy, multi-task SQL generation, and reduction of catastrophic forgetting.

Conclusion: This approach provides a comprehensive understanding of database structure and metadata for LLMs, improving their performance in SQL generation.

Abstract: The advancements of Large language models (LLMs) have provided great
opportunities to text-to-SQL tasks to overcome the main challenges to
understand complex domain information and complex database structures in
business applications. In this paper, we propose a meta-aware learning
framework to integrate domain knowledge, database schema, chain-of-thought
reasoning processes, and metadata relationships to improve the SQL generation
quality. The proposed framework includes four learning strategies: schema-based
learning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key
information tokenization. This approach provides a comprehensive understanding
of database structure and metadata information towards LLM through fine-tuning
to improve its performance on SQL generation within business domains. Through
two experimental studies, we have demonstrated the superiority of the proposed
methods in execution accuracy, multi-task SQL generation capability, and
reduction of catastrophic forgetting.

</details>


### [38] [Can Large Language Models Infer Causal Relationships from Real-World Text?](https://arxiv.org/abs/2505.18931)
*Ryan Saklad,Aman Chadha,Oleg Pavlov,Raha Moraffah*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型（LLMs）从现实世界文本中推断因果关系的能力，并创建了一个全新的基准数据集。实验显示，即使是最先进的LLM，在此任务上的表现也存在显著挑战，平均F1分数仅为0.477。分析表明，LLMs在处理隐含信息、区分因果因素与上下文细节以及连接长篇文本中的因果相关信息方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前关于因果关系推理的研究主要集中在合成生成的文本上，这些文本涉及的因果关系简单且明确提及，无法反映现实世界任务的复杂性。因此，有必要研究LLMs是否能够从现实世界的文本中推断因果关系。

Method: 研究人员开发了一个从现实世界学术文献中提取的基准数据集，该数据集包含长度、关系复杂性（不同层次的显性程度、事件数量和因果关系）以及领域和子领域的多样化文本。然后使用最先进的LLMs对该基准进行评估。

Result: 实验结果表明，即使是表现最好的模型，其平均F1得分也只有0.477，显示出显著的挑战。此外，LLMs在处理隐含信息、区分因果因素与上下文细节以及连接长篇文本中的因果相关信息方面存在困难。

Conclusion: 这项研究表明，现有的LLMs在从现实世界文本中推断因果关系方面存在明显不足。通过系统地描述这些缺陷，该基准为未来提升LLMs因果推理能力的研究提供了有针对性的见解。

Abstract: Understanding and inferring causal relationships from texts is a core aspect
of human cognition and is essential for advancing large language models (LLMs)
towards artificial general intelligence. Existing work primarily focuses on
synthetically generated texts which involve simple causal relationships
explicitly mentioned in the text. This fails to reflect the complexities of
real-world tasks. In this paper, we investigate whether LLMs are capable of
inferring causal relationships from real-world texts. We develop a benchmark
drawn from real-world academic literature which includes diverse texts with
respect to length, complexity of relationships (different levels of
explicitness, number of events, and causal relationships), and domains and
sub-domains. To the best of our knowledge, our benchmark is the first-ever
real-world dataset for this task. Our experiments on state-of-the-art LLMs
evaluated on our proposed benchmark demonstrate significant challenges, with
the best-performing model achieving an average F1 score of only 0.477. Analysis
reveals common pitfalls: difficulty with implicitly stated information, in
distinguishing relevant causal factors from surrounding contextual details, and
with connecting causally relevant information spread across lengthy textual
passages. By systematically characterizing these deficiencies, our benchmark
offers targeted insights for further research into advancing LLM causal
reasoning.

</details>


### [39] [REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2505.18933)
*Haitian Zhong,Yuhuan Liu,Ziyang Xu,Guofan Liu,Qiang Liu,Shu Wu,Zhe Zhao,Liang Wang,Tieniu Tan*

Main category: cs.AI

TL;DR: REACT is a two-phase framework for precise and controllable knowledge editing in large language models, which significantly reduces overfitting and preserves balanced basic editing performance.


<details>
  <summary>Details</summary>
Motivation: Large language model editing methods often encounter overfitting issues where factual updates go beyond their intended scope. There's a need for a method that can precisely and controllably edit knowledge without overemphasizing the edited target inappropriately.

Method: The REACT framework consists of two phases: 1) Extracting latent factual representations using tailored stimuli and computing a directional 'belief shift' vector via Principal Component Analysis and a linear transformation. 2) Applying controllable perturbations to hidden states with the computed vector, gated by a pre-trained classifier to ensure edits are contextually necessary.

Result: Experiments on EVOKE benchmarks indicate that REACT significantly reduces overfitting across nearly all evaluation metrics. Experiments on COUNTERFACT and MQuAKE show that the method maintains balanced basic editing performance (reliability, locality, and generality) in various editing scenarios.

Conclusion: REACT provides a solution to the overfitting problem in large language model editing, offering precise and controllable knowledge editing capabilities.

Abstract: Large language model editing methods frequently suffer from overfitting,
wherein factual updates can propagate beyond their intended scope,
overemphasizing the edited target even when it's contextually inappropriate. To
address this challenge, we introduce REACT (Representation Extraction And
Controllable Tuning), a unified two-phase framework designed for precise and
controllable knowledge editing. In the initial phase, we utilize tailored
stimuli to extract latent factual representations and apply Principal Component
Analysis with a simple learnbale linear transformation to compute a directional
"belief shift" vector for each instance. In the second phase, we apply
controllable perturbations to hidden states using the obtained vector with a
magnitude scalar, gated by a pre-trained classifier that permits edits only
when contextually necessary. Relevant experiments on EVOKE benchmarks
demonstrate that REACT significantly reduces overfitting across nearly all
evaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our
method preserves balanced basic editing performance (reliability, locality, and
generality) under diverse editing scenarios.

</details>


### [40] [SANNet: A Semantic-Aware Agentic AI Networking Framework for Multi-Agent Cross-Layer Coordination](https://arxiv.org/abs/2505.18946)
*Yong Xiao,Haoran Zhou,Xubo Li,Yayu Gao,Guangming Shi,Ping Zhang*

Main category: cs.AI

TL;DR: The paper introduces SANNet, a new architecture for AI-native networking (AgentNet) that can infer user semantic goals and assign agents automatically. It also includes a conflict-resolving mechanism for agents with conflicting objectives. Experimental results show significant performance improvement in multi-agent networking systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is the challenge in AgentNet where different agents may have conflicting objectives when collaborating for certain goals, necessitating an effective networking framework to support automatic goal discovery and multi-agent self-orchestration and task assignment.

Method: SANNet, a semantic-aware agentic AI networking architecture, infers the semantic goal of the user and automatically assigns agents associated with different layers of a mobile system to fulfill the inferred goal. A dynamic weighting-based conflict-resolving mechanism is introduced to address conflicts between agents.

Result: Experimental results demonstrate that SANNet significantly improves the performance of multi-agent networking systems, even when agents with conflicting objectives collaborate for the same goal.

Conclusion: SANNet provides theoretical guarantees in conflict-resolving and model generalization performance for multi-agent collaboration in dynamic environments, showcasing its potential in enhancing AI-native networking systems.

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm
that relies on a large number of specialized AI agents to collaborate and
coordinate for autonomous decision-making, dynamic environmental adaptation,
and complex goal achievement. It has the potential to facilitate real-time
network management alongside capabilities for self-configuration,
self-optimization, and self-adaptation across diverse and complex networking
environments, laying the foundation for fully autonomous networking systems in
the future. Despite its promise, AgentNet is still in the early stage of
development, and there still lacks an effective networking framework to support
automatic goal discovery and multi-agent self-orchestration and task
assignment. This paper proposes SANNet, a novel semantic-aware agentic AI
networking architecture that can infer the semantic goal of the user and
automatically assign agents associated with different layers of a mobile system
to fulfill the inferred goal. Motivated by the fact that one of the major
challenges in AgentNet is that different agents may have different and even
conflicting objectives when collaborating for certain goals, we introduce a
dynamic weighting-based conflict-resolving mechanism to address this issue. We
prove that SANNet can provide theoretical guarantee in both conflict-resolving
and model generalization performance for multi-agent collaboration in dynamic
environment. We develop a hardware prototype of SANNet based on the open RAN
and 5GS core platform. Our experimental results show that SANNet can
significantly improve the performance of multi-agent networking systems, even
when agents with conflicting objectives are selected to collaborate for the
same goal.

</details>


### [41] [Co-PatcheR: Collaborative Software Patching with Component(s)-specific Small Reasoning Models](https://arxiv.org/abs/2505.18955)
*Yuheng Tang,Hongwei Li,Kaijie Zhu,Michael Yang,Yangruibo Ding,Wenbo Guo*

Main category: cs.AI

TL;DR: Co-PatcheR is a collaborative patching system with specialized small models for different components in the software patching pipeline, achieving 46% resolved rate on SWE-bench-Verified.


<details>
  <summary>Details</summary>
Motivation: Existing methods using a single model to handle the entire software patching pipeline have limitations due to differing requirements of sub-tasks. Although large models achieve certain success, smaller specialized models could potentially be more efficient with less training resources.

Method: Propose Co-PatcheR, which includes specific task designs and training recipes. A model is trained separately for localization and patch generation; localization pinpoints suspicious lines through a two-step procedure, while generation combines patch creation and critique. Hybrid patch validation uses two models for crafting test cases and judging correctness followed by majority vote-based patch selection.

Result: Co-PatcheR achieves a 46% resolved rate on SWE-bench-Verified using only 3 x 14B models, surpassing previous methods that used a 70 billion parameter model to reach 41%. Comprehensive ablation studies confirm the effectiveness of the approach.

Conclusion: Co-PatcheR demonstrates that a collaborative system with small specialized models can outperform larger monolithic models in software patching tasks, requiring fewer training resources.

Abstract: Motivated by the success of general-purpose large language models (LLMs) in
software patching, recent works started to train specialized patching models.
Most works trained one model to handle the end-to-end patching pipeline
(including issue localization, patch generation, and patch validation).
However, it is hard for a small model to handle all tasks, as different
sub-tasks have different workflows and require different expertise. As such, by
using a 70 billion model, SOTA methods can only reach up to 41% resolved rate
on SWE-bench-Verified. Motivated by the collaborative nature, we propose
Co-PatcheR, the first collaborative patching system with small and specialized
reasoning models for individual components. Our key technique novelties are the
specific task designs and training recipes. First, we train a model for
localization and patch generation. Our localization pinpoints the suspicious
lines through a two-step procedure, and our generation combines patch
generation and critique. We then propose a hybrid patch validation that
includes two models for crafting issue-reproducing test cases with and without
assertions and judging patch correctness, followed by a majority vote-based
patch selection. Through extensive evaluation, we show that Co-PatcheR achieves
46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes
Co-PatcheR the best patcher with specialized models, requiring the least
training resources and the smallest models. We conduct a comprehensive ablation
study to validate our recipes, as well as our choice of training data number,
model size, and testing-phase scaling strategy.

</details>


### [42] [Weaver: Interweaving SQL and LLM for Table Reasoning](https://arxiv.org/abs/2505.18961)
*Rohit Khoja,Devanshu Gupta,Yanjie Fu,Dan Roth,Vivek Gupta*

Main category: cs.AI

TL;DR: Weaver 是一个模块化管道，动态整合SQL和大型语言模型（LLMs）以进行基于表格的问题回答。它通过将复杂查询分解为可管理的子任务，提高了准确性和泛化能力，并在四个TableQA数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 查询包含非结构化数据的表格具有挑战性，因为传统SQL难以处理需要语义推理的任务，而现有的结合SQL和LLMs的方法依赖于僵化的预定义工作流程，限制了其对复杂查询的适应性。

Method: 提出Weaver，一个模块化管道，生成灵活的逐步计划，结合SQL进行结构化数据检索与LLMs进行语义处理，以动态集成两者并分解复杂查询为可管理的子任务。

Result: 实验表明，Weaver在四个TableQA数据集上持续优于最先进方法，同时减少了API调用和错误率。

Conclusion: Weaver通过其灵活的SQL和LLMs集成方法，在提高查询准确性、减少错误率和API调用方面表现出色，展现出强大的泛化能力。

Abstract: Querying tables with unstructured data is challenging due to the presence of
text (or image), either embedded in the table or in external paragraphs, which
traditional SQL struggles to process, especially for tasks requiring semantic
reasoning. While Large Language Models (LLMs) excel at understanding context,
they face limitations with long input sequences. Existing approaches that
combine SQL and LLMs typically rely on rigid, predefined work-flows, limiting
their adaptability to complex queries. To address these issues, we introduce
Weaver , a modular pipeline that dynamically integrates SQL and LLMs for
table-based question answering (TableQA). Weaver generates a flexible,
step-by-step plan that combines SQL for structured data retrieval with LLMs for
semantic processing. By decomposing complex queries into manageable subtasks,
Weaver improves accuracy and generalization. Our experiments show that Weaver
consistently outperforms state-of-the-art methods across four TableQA datasets,
reducing both API calls and error rates.

</details>


### [43] [Aligning LLM with human travel choices: a persona-based embedding learning approach](https://arxiv.org/abs/2505.19003)
*Tianming Liu,Manzi Li,Yafeng Yin*

Main category: cs.AI

TL;DR: This paper presents a novel framework that aligns large language models (LLMs) with human travel choice behavior using persona inference and loading, showing significant improvements over baselines in predicting mode choices and individual outcomes, while also providing interpretable insights.


<details>
  <summary>Details</summary>
Motivation: To overcome the behavioral misalignment between LLMs and humans in the context of travel demand modeling, which current alignment methods fail to address effectively given typical travel demand data constraints.

Method: The framework uses persona inference to establish base personas from empirical data and a learned persona loading function driven by behavioral embeddings to condition LLMs with appropriate prompts, enhancing alignment with human travel choice behavior.

Result: Validated on the Swissmetro mode choice dataset, the proposed approach significantly outperformed baseline choice models and LLM-based simulation models in predicting aggregate mode choice shares and individual choice outcomes. It also generates interpretable insights into population behavior.

Conclusion: The research provides an adaptable, interpretable, and resource-efficient method for robust LLM-based travel behavior simulation, paving the way for integrating LLMs into travel demand modeling.

Abstract: The advent of large language models (LLMs) presents new opportunities for
travel demand modeling. However, behavioral misalignment between LLMs and
humans presents obstacles for the usage of LLMs, and existing alignment methods
are frequently inefficient or impractical given the constraints of typical
travel demand data. This paper introduces a novel framework for aligning LLMs
with human travel choice behavior, tailored to the current travel demand data
sources. Our framework uses a persona inference and loading process to
condition LLMs with suitable prompts to enhance alignment. The inference step
establishes a set of base personas from empirical data, and a learned persona
loading function driven by behavioral embeddings guides the loading process. We
validate our framework on the Swissmetro mode choice dataset, and the results
show that our proposed approach significantly outperformed baseline choice
models and LLM-based simulation models in predicting both aggregate mode choice
shares and individual choice outcomes. Furthermore, we showcase that our
framework can generate insights on population behavior through interpretable
parameters. Overall, our research offers a more adaptable, interpretable, and
resource-efficient pathway to robust LLM-based travel behavior simulation,
paving the way to integrate LLMs into travel demand modeling practice in the
future.

</details>


### [44] [RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data](https://arxiv.org/abs/2505.19030)
*Wenhao Liu,Zhengkang Guo,Mingchen Xie,Jingwen Xu,Zisu Huang,Muzhao Tian,Jianhan Xu,Muling Wu,Xiaohua Wang,Changze Lv,He-Da Wang,Hu Yao,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: RECAST is a new framework that creates datasets with complex constraints to improve LLMs' ability to follow intricate instructions. It includes automatic verification and has led to significant performance improvements in models fine-tuned on the RECAST-30K dataset.


<details>
  <summary>Details</summary>
Motivation: Large language models face challenges in accurately following complex instructions with numerous explicitly stated requirements, especially when there are more than 10 constraints.

Method: The authors propose RECAST, a framework for synthesizing datasets with examples incorporating many more constraints than existing benchmarks. These constraints are extracted from real-world prompt-response pairs. The framework allows for automatic verification of constraint satisfaction through rule-based validators for quantitative constraints and LLM-based validators for qualitative ones.

Result: Using the RECAST framework, the authors constructed RECAST-30K, a large-scale, high-quality dataset with 30k instances spanning 15 constraint types. Models fine-tuned on this dataset show substantial improvements in following complex instructions. Additionally, the verifiability provided by RECAST enables the design of reward functions for reinforcement learning, further enhancing model performance.

Conclusion: RECAST addresses the challenge of improving LLMs' ability to follow complex instructions by providing a framework for creating relevant datasets with extensive constraints and enabling automatic verification. This leads to better model performance on intricate tasks.

Abstract: Large language models (LLMs) are increasingly expected to tackle complex
tasks, driven by their expanding applications and users' growing proficiency in
crafting sophisticated prompts. However, as the number of explicitly stated
requirements increases (particularly more than 10 constraints), LLMs often
struggle to accurately follow such complex instructions. To address this
challenge, we propose RECAST, a novel framework for synthesizing datasets where
each example incorporates far more constraints than those in existing
benchmarks. These constraints are extracted from real-world prompt-response
pairs to ensure practical relevance. RECAST enables automatic verification of
constraint satisfaction via rule-based validators for quantitative constraints
and LLM-based validators for qualitative ones. Using this framework, we
construct RECAST-30K, a large-scale, high-quality dataset comprising 30k
instances spanning 15 constraint types. Experimental results demonstrate that
models fine-tuned on RECAST-30K show substantial improvements in following
complex instructions. Moreover, the verifiability provided by RECAST enables
the design of reward functions for reinforcement learning, which further boosts
model performance on complex and challenging tasks.

</details>


### [45] [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs](https://arxiv.org/abs/2505.19075)
*Jaemin Kim,Hangeol Chang,Hyunmin Hwang,Choonghan Kim,Jong Chul Ye*

Main category: cs.AI

TL;DR: 提出了一种名为Universal Reasoner (UniR)的新方法，作为单一、轻量级、可组合和即插即用的推理模块，可以与任何冻结的大语言模型（LLM）结合使用，赋予其专门的推理能力。UniR通过将奖励分解为独立的推理模块，在预定义奖励下独立训练，将轨迹级别的信号转化为令牌级别的指导。实验结果表明，UniR在数学推理和机器翻译任务中显著优于现有的微调方法，并展示了强大的弱到强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然具有出色的通用能力，但增强特定技能如推理需要大量的计算资源，可能会削弱其通用性。参数高效微调（PEFT）方法虽然节省资源，但通常需要针对每个LLM主干重新训练，因为存在架构依赖问题。

Method: 提出了Universal Reasoner (UniR)，一个轻量级、可组合和即插即用的推理模块。UniR将奖励分解为独立的推理模块，该模块通过预定义奖励独立训练，从而将轨迹级别的信号转化为令牌级别的指导。在推理时，UniR可以通过简单地将其输出logits添加到LLM主干的logits上来与任何冻结的LLM结合使用。多个UniR模块可以联合应用，通过求和它们的logits来实现复杂推理。

Result: 实验结果表明，UniR在数学推理和机器翻译任务中显著优于现有的微调方法。此外，UniR展示了强大的弱到强泛化能力，即在较小模型上训练的推理模块能够有效地指导更大的LLM。

Conclusion: UniR是一种成本效益高、适应性强、稳健的解决方案，可以在不损害LLM核心能力的情况下增强其推理能力。代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable general
capabilities, but enhancing skills such as reasoning often demands substantial
computational resources and may compromise their generalization. While
Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious
alternative, they typically requires retraining for each LLM backbone due to
architectural dependencies. To address these challenges, here we propose
Universal Reasoner (UniR) - a single, lightweight, composable, and
plug-and-play reasoning module that can be used with any frozen LLM to endow it
with specialized reasoning capabilities. Specifically, UniR decomposes the
reward into a standalone reasoning module that is trained independently using
predefined rewards, effectively translating trajectory-level signals into
token-level guidance. Once trained, UniR can be combined with any frozen LLM at
inference time by simply adding its output logits to those of the LLM backbone.
This additive structure naturally enables modular composition: multiple UniR
modules trained for different tasks can be jointly applied by summing their
logits, enabling complex reasoning via composition. Experimental results on
mathematical reasoning and machine translation tasks show that UniR
significantly outperforms \add{existing baseline fine-tuning methods using the
Llama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong
generalization: reasoning modules trained on smaller models effectively guide
much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust
solution for enhancing reasoning in LLMs without compromising their core
capabilities. Code is open-sourced at https://github.com/hangeol/UniR

</details>


### [46] [Reinforced Latent Reasoning for LLM-based Recommendation](https://arxiv.org/abs/2505.19092)
*Yang Zhang,Wenxin Xu,Xiaoyan Zhao,Wenjie Wang,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 本研究提出了一种名为LatentR³的新型端到端训练框架，通过强化学习优化潜在推理，无需依赖显式的思维链数据，在推荐系统中显著提高了性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂问题解决任务中表现出强大的推理能力，但现有方法依赖于显式思维链（CoT）数据进行微调，存在获取高质量CoT数据困难以及推理延迟高的问题。

Method: 提出了一种名为LatentR³的框架，采用两阶段训练策略：首先通过监督微调初始化潜在推理模块，然后利用基于规则奖励设计的纯强化学习（RL）进行训练。该方法基于改进的GRPO算法，减少了训练计算开销并引入连续奖励信号以提高学习效率。

Result: 实验表明，LatentR³能够在没有任何直接推理过程监督的情况下实现有效的潜在推理，并显著提升与不同LLM推荐方法集成后的性能。

Conclusion: LatentR³提供了一种无需显式CoT数据的潜在推理优化方法，有效提升了推荐系统的性能和推理效率，为未来的研究提供了新的方向。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities in complex problem-solving tasks, sparking growing interest in
their application to preference reasoning in recommendation systems. Existing
methods typically rely on fine-tuning with explicit chain-of-thought (CoT)
data. However, these methods face significant practical limitations due to (1)
the difficulty of obtaining high-quality CoT data in recommendation and (2) the
high inference latency caused by generating CoT reasoning. In this work, we
explore an alternative approach that shifts from explicit CoT reasoning to
compact, information-dense latent reasoning. This approach eliminates the need
for explicit CoT generation and improves inference efficiency, as a small set
of latent tokens can effectively capture the entire reasoning process. Building
on this idea, we propose $\textit{\underline{R}einforced \underline{Latent}
\underline{R}easoning for \underline{R}ecommendation}$ (LatentR$^3$), a novel
end-to-end training framework that leverages reinforcement learning (RL) to
optimize latent reasoning without relying on any CoT data.LatentR$^3$ adopts a
two-stage training strategy: first, supervised fine-tuning to initialize the
latent reasoning module, followed by pure RL training to encourage exploration
through a rule-based reward design. Our RL implementation is based on a
modified GRPO algorithm, which reduces computational overhead during training
and introduces continuous reward signals for more efficient learning. Extensive
experiments demonstrate that LatentR$^3$ enables effective latent reasoning
without any direct supervision of the reasoning process, significantly
improving performance when integrated with different LLM-based recommendation
methods. Our codes are available at https://anonymous.4open.science/r/R3-A278/.

</details>


### [47] [ScreenExplorer: Training a Vision-Language Model for Diverse Exploration in Open GUI World](https://arxiv.org/abs/2505.19095)
*Runliang Niu,Jinglong Ji,Yi Chang,Qi Wang*

Main category: cs.AI

TL;DR: ScreenExplorer is a VLM trained via GRPO in real, dynamic GUI environments with curiosity reward function and experience distillation to enhance exploration capabilities for AGI.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents based on LLMs or VLMs have limitations such as failing to generalize to novel environments and relying heavily on manually curated datasets.

Method: Introduced ScreenExplorer, a VLM trained through Group Relative Policy Optimization (GRPO) in real, dynamic GUI environments. Used a world-model-based curiosity reward function to overcome the cold-start phase of exploration and distilled experience streams to further enhance exploration capabilities.

Result: Trained models showed better environmental adaptation and sustained exploration compared to static deployment models.

Conclusion: The training framework offers a scalable pathway toward AGI systems with self-improving capabilities in complex interactive settings.

Abstract: The rapid progress of large language models (LLMs) has sparked growing
interest in building Artificial General Intelligence (AGI) within Graphical
User Interface (GUI) environments. However, existing GUI agents based on LLMs
or vision-language models (VLMs) often fail to generalize to novel environments
and rely heavily on manually curated, diverse datasets. To overcome these
limitations, we introduce ScreenExplorer, a VLM trained via Group Relative
Policy Optimization(GRPO) in real, dynamic, and open-ended GUI environments.
Innovatively, we introduced a world-model-based curiosity reward function to
help the agent overcome the cold-start phase of exploration. Additionally,
distilling experience streams further enhances the model's exploration
capabilities. Our training framework enhances model exploration in open GUI
environments, with trained models showing better environmental adaptation and
sustained exploration compared to static deployment models. Our findings offer
a scalable pathway toward AGI systems with self-improving capabilities in
complex interactive settings.

</details>


### [48] [SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning](https://arxiv.org/abs/2505.19099)
*Kun Xiang,Heng Li,Terry Jingchen Zhang,Yinya Huang,Zirong Liu,Peixin Qu,Jixi He,Jiaqi Chen,Yu-Jie Yuan,Jianhua Han,Hang Xu,Hanhui Li,Mrinmaya Sachan,Xiaodan Liang*

Main category: cs.AI

TL;DR: The paper introduces SeePhys, a large-scale multimodal benchmark for LLM reasoning in physics questions. It features vision-essential problems and reveals challenges in current LLM's visual understanding capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark for evaluating the reasoning abilities of LLMs in physics-related questions that require both textual and visual understanding.

Method: Developed SeePhys, which includes physics questions from middle school to PhD level, covering 7 fundamental domains and 21 categories of diagrams. A significant portion (75%) of the questions are vision-essential, requiring visual information extraction for correct solutions.

Result: Evaluation shows that advanced visual reasoning models achieve sub-60% accuracy on SeePhys, indicating challenges in coupling diagram interpretation with physics reasoning and reducing reliance on textual cues.

Conclusion: SeePhys highlights the need for improving LLMs' visual understanding capabilities, especially in integrating diagram interpretation with physics reasoning.

Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning
grounded in physics questions ranging from middle school to PhD qualifying
exams. The benchmark covers 7 fundamental domains spanning the physics
discipline, incorporating 21 categories of highly heterogeneous diagrams. In
contrast to prior works where visual elements mainly serve auxiliary purposes,
our benchmark features a substantial proportion of vision-essential problems
(75\%) that mandate visual information extraction for correct solutions.
Through extensive evaluation, we observe that even the most advanced visual
reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\% accuracy
on our benchmark. These results reveal fundamental challenges in current large
language models' visual understanding capabilities, particularly in: (i)
establishing rigorous coupling between diagram interpretation and physics
reasoning, and (ii) overcoming their persistent reliance on textual cues as
cognitive shortcuts.

</details>


### [49] [OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs](https://arxiv.org/abs/2505.19165)
*Debdeep Sanyal Umakanta Maharana,Yash Sinha,Hong Ming Tan,Shirish Karande,Mohan Kankanhalli,Murari Mandal*

Main category: cs.AI

TL;DR: The paper introduces a benchmark called OrgAccess to evaluate Large Language Models' (LLMs) ability to understand and operate within organizational hierarchies and role-based permissions. It reveals that even state-of-the-art LLMs have significant difficulties maintaining compliance with complex rules, especially when dealing with conflicting permissions.


<details>
  <summary>Details</summary>
Motivation: To address the under explored challenge of whether LLMs can reliably understand and operate within the complex constraints imposed by organizational hierarchies and associated permissions.

Method: Introduced a synthetic yet representative OrgAccess benchmark consisting of 40 distinct types of permissions. Created three types of permissions: 40,000 easy (1 permission), 10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to test LLMs' ability to accurately assess these permissions and generate responses adhering to hierarchical rules.

Result: Findings reveal that state-of-the-art LLMs struggle significantly to maintain compliance with role-based structures, with performance degrading further in scenarios involving two or more conflicting permissions. GPT-4.1 achieved an F1-Score of only 0.27 on the hardest benchmark.

Conclusion: LLMs have critical limitations in complex rule following and compositional reasoning capabilities beyond standard factual or STEM-based benchmarks, highlighting a new paradigm for evaluating their fitness for practical structured environments.

Abstract: Role-based access control (RBAC) and hierarchical structures are foundational
to how information flows and decisions are made within virtually all
organizations. As the potential of Large Language Models (LLMs) to serve as
unified knowledge repositories and intelligent assistants in enterprise
settings becomes increasingly apparent, a critical, yet under explored,
challenge emerges: \textit{can these models reliably understand and operate
within the complex, often nuanced, constraints imposed by organizational
hierarchies and associated permissions?} Evaluating this crucial capability is
inherently difficult due to the proprietary and sensitive nature of real-world
corporate data and access control policies. We introduce a synthetic yet
representative \textbf{OrgAccess} benchmark consisting of 40 distinct types of
permissions commonly relevant across different organizational roles and levels.
We further create three types of permissions: 40,000 easy (1 permission),
10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to
test LLMs' ability to accurately assess these permissions and generate
responses that strictly adhere to the specified hierarchical rules,
particularly in scenarios involving users with overlapping or conflicting
permissions. Our findings reveal that even state-of-the-art LLMs struggle
significantly to maintain compliance with role-based structures, even with
explicit instructions, with their performance degrades further when navigating
interactions involving two or more conflicting permissions. Specifically, even
\textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}.
This demonstrates a critical limitation in LLMs' complex rule following and
compositional reasoning capabilities beyond standard factual or STEM-based
benchmarks, opening up a new paradigm for evaluating their fitness for
practical, structured environments.

</details>


### [50] [Amplifying Human Creativity and Problem Solving with AI Through Generative Collective Intelligence](https://arxiv.org/abs/2505.19167)
*Thomas P. Kehler,Scott E. Page,Alex Pentland,Martin Reeves,John Seely Brown*

Main category: cs.AI

TL;DR: The paper introduces Generative Collective Intelligence (GCI), a framework for human-AI collaboration that leverages both human creativity and AI's computational capabilities to solve complex societal challenges through structured group collaboration.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of purely algorithmic approaches in problem-solving and decision-making, and to create a more effective collaboration between humans and AI by integrating AI as social and cultural technology.

Method: Proposes GCI framework where AI operates at the group/social level in dual roles - interactive agents and knowledge accumulation/organization technology. It uses comparative judgment and minimum regret principles as mathematical foundations.

Result: Demonstrates potential applications in domains like climate adaptation, healthcare transformation, and civic participation, showcasing the ability to transcend traditional communication barriers and enhance group problem-solving.

Conclusion: GCI represents a promising approach to address complex societal challenges that neither humans nor machines can solve alone.

Abstract: We propose a new framework for human-AI collaboration that amplifies the
distinct capabilities of both. This framework, which we call Generative
Collective Intelligence (GCI), shifts AI to the group/social level and employs
AI in dual roles: as interactive agents and as technology that accumulates,
organizes, and leverages knowledge. By creating a cognitive bridge between
human reasoning and AI models, GCI can overcome the limitations of purely
algorithmic approaches to problem-solving and decision-making. The framework
demonstrates how AI can be reframed as a social and cultural technology that
enables groups to solve complex problems through structured collaboration that
transcends traditional communication barriers. We describe the mathematical
foundations of GCI based on comparative judgment and minimum regret principles,
and illustrate its applications across domains including climate adaptation,
healthcare transformation, and civic participation. By combining human
creativity with AI's computational capabilities, GCI offers a promising
approach to addressing complex societal challenges that neither human or
machines can solve alone.

</details>


### [51] [Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style](https://arxiv.org/abs/2505.19173)
*Debdeep Sanyal,Agniva Maiti,Umakanta Maharana,Dhruv Kumar,Ankur Mali,C. Lee Giles,Murari Mandal*

Main category: cs.AI

TL;DR: Effective teaching needs adapting strategies for diverse students. Current LLM-based simulation frameworks have limitations. This paper introduces a new framework integrating heterogeneous student agents and a self-optimizing teacher agent, along with Persona-RAG module. Experiments show its potential in adaptive teaching practices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current simulation frameworks that reduce students to static knowledge profiles and lack adaptive mechanisms for modeling teachers who evolve their strategies based on student feedback.

Method: The method involves introducing a novel simulation framework that integrates LLM-based heterogeneous student agents with a self-optimizing teacher agent whose pedagogical policy is evolved using a genetic algorithm. Additionally, Persona-RAG, a Retrieval Augmented Generation module enabling personalized knowledge retrieval for student agents, is proposed.

Result: Through extensive experiments, it is demonstrated that the framework supports the emergence of distinct and interpretable teaching patterns when interacting with varied student populations.

Conclusion: The results highlight the potential of LLM-driven simulations to inform adaptive teaching practices and provide a testbed for training human educators in controlled, data-driven environments.

Abstract: Effective teaching requires adapting instructional strategies to accommodate
the diverse cognitive and behavioral profiles of students, a persistent
challenge in education and teacher training. While Large Language Models (LLMs)
offer promise as tools to simulate such complex pedagogical environments,
current simulation frameworks are limited in two key respects: (1) they often
reduce students to static knowledge profiles, and (2) they lack adaptive
mechanisms for modeling teachers who evolve their strategies in response to
student feedback. To address these gaps, \textbf{we introduce a novel
simulation framework that integrates LLM-based heterogeneous student agents
with a self-optimizing teacher agent}. The teacher agent's pedagogical policy
is dynamically evolved using a genetic algorithm, allowing it to discover and
refine effective teaching strategies based on the aggregate performance of
diverse learners. In addition, \textbf{we propose Persona-RAG}, a Retrieval
Augmented Generation module that enables student agents to retrieve knowledge
tailored to their individual learning styles. Persona-RAG preserves the
retrieval accuracy of standard RAG baselines while enhancing personalization,
an essential factor in modeling realistic educational scenarios. Through
extensive experiments, we demonstrate how our framework supports the emergence
of distinct and interpretable teaching patterns when interacting with varied
student populations. Our results highlight the potential of LLM-driven
simulations to inform adaptive teaching practices and provide a testbed for
training human educators in controlled, data-driven environments.

</details>


### [52] [CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis](https://arxiv.org/abs/2505.19195)
*Shaohao Rui,Haoyang Su,Jinyi Xiang,Lian-Ming Wu,Xiaosong Wang*

Main category: cs.AI

TL;DR: CardioCoT is a new framework that improves prediction and interpretability of cardiovascular event recurrence in post-myocardial infarction patients using MRI and clinical notes.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting major adverse cardiovascular events (MACE) recurrence risk focus on risk stratification but lack robust reasoning and model interpretability needed in clinical practice. Also, end-to-end risk prediction with LLM/VLM has challenges due to data limitations and complexity.

Method: CardioCoT uses a two-stage approach: 1) An evidence-augmented self-refinement mechanism helps LLM/VLMs create robust reasoning trajectories from radiological findings; 2) Integrates these trajectories with imaging data for risk model training and prediction.

Result: CardioCoT shows better performance in predicting MACE recurrence risk and provides interpretable reasoning processes.

Conclusion: CardioCoT enhances both interpretability and predictive accuracy, providing useful insights for clinical decisions.

Abstract: Accurate prediction of major adverse cardiovascular events recurrence risk in
acute myocardial infarction patients based on postoperative cardiac MRI and
associated clinical notes is crucial for precision treatment and personalized
intervention. Existing methods primarily focus on risk stratification
capability while overlooking the need for intermediate robust reasoning and
model interpretability in clinical practice. Moreover, end-to-end risk
prediction using LLM/VLM faces significant challenges due to data limitations
and modeling complexity. To bridge this gap, we propose CardioCoT, a novel
two-stage hierarchical reasoning-enhanced survival analysis framework designed
to enhance both model interpretability and predictive performance. In the first
stage, we employ an evidence-augmented self-refinement mechanism to guide
LLM/VLMs in generating robust hierarchical reasoning trajectories based on
associated radiological findings. In the second stage, we integrate the
reasoning trajectories with imaging data for risk model training and
prediction. CardioCoT demonstrates superior performance in MACE recurrence risk
prediction while providing interpretable reasoning processes, offering valuable
insights for clinical decision-making.

</details>


### [53] [Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance](https://arxiv.org/abs/2505.19197)
*Chanyeol Choi,Jihoon Kwon,Minjae Kim,Juneha Hwang,Minsoo Ha,Chaewoon Kim,Jaeseon Ha,Suyeol Yun,Jin Kim*

Main category: cs.AI

TL;DR: The paper proposes a multi-agent system using large language models to efficiently extract quantitative insights from unstructured financial documents, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Extracting structured and quantitative insights from unstructured financial filings is essential in investment research but remains time-consuming and resource-intensive with conventional approaches.

Method: Proposes a multi-agent system composed of two specialized agents: the Extraction Agent that identifies key performance indicators, standardizes formats, and verifies accuracy; and the Text-to-SQL Agent that generates executable SQL statements from natural language queries.

Result: Demonstrates 95% accuracy in transforming financial filings into structured data and 91% correct responses in retrieval tasks based on human evaluations.

Conclusion: The system effectively transforms unstructured text into structured data accurately and enables precise retrieval of key information across various financial document types.

Abstract: Extracting structured and quantitative insights from unstructured financial
filings is essential in investment research, yet remains time-consuming and
resource-intensive. Conventional approaches in practice rely heavily on
labor-intensive manual processes, limiting scalability and delaying the
research workflow. In this paper, we propose an efficient and scalable method
for accurately extracting quantitative insights from unstructured financial
documents, leveraging a multi-agent system composed of large language models.
Our proposed multi-agent system consists of two specialized agents: the
\emph{Extraction Agent} and the \emph{Text-to-SQL Agent}. The
\textit{Extraction Agent} automatically identifies key performance indicators
from unstructured financial text, standardizes their formats, and verifies
their accuracy. On the other hand, the \textit{Text-to-SQL Agent} generates
executable SQL statements from natural language queries, allowing users to
access structured data accurately without requiring familiarity with the
database schema. Through experiments, we demonstrate that our proposed system
effectively transforms unstructured text into structured data accurately and
enables precise retrieval of key information. First, we demonstrate that our
system achieves approximately 95\% accuracy in transforming financial filings
into structured data, matching the performance level typically attained by
human annotators. Second, in a human evaluation of the retrieval task -- where
natural language queries are used to search information from structured data --
91\% of the responses were rated as correct by human evaluators. In both
evaluations, our system generalizes well across financial document types,
consistently delivering reliable performance.

</details>


### [54] [Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning](https://arxiv.org/abs/2505.19213)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.AI

TL;DR: The paper presents MedCCO, a multimodal reinforcement learning framework for medical VQA that combines close-ended and open-ended data through curriculum-driven RFT, leading to improved performance and generalization in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current medical reinforcement fine-tuning methods which mainly focus on close-ended VQA and lack the ability to perform open-ended, reasoning-intensive decision-making.

Method: MedCCO is initially fine-tuned on close-ended medical VQA tasks to establish domain-grounded reasoning capabilities, then progressively adapted to open-ended tasks for deeper knowledge enhancement and clinical interpretability.

Result: MedCCO consistently enhances performance and generalization with a 11.4% accuracy gain across three in-domain tasks and a 5.7% improvement on five out-of-domain benchmarks.

Conclusion: Curriculum-guided RL shows promise in advancing robust, clinically-relevant reasoning in medical multimodal language models.

Abstract: Recent advances in reinforcement learning with verifiable, rule-based rewards
have greatly enhanced the reasoning capabilities and out-of-distribution
generalization of VLMs/LLMs, obviating the need for manually crafted reasoning
chains. Despite these promising developments in the general domain, their
translation to medical imaging remains limited. Current medical reinforcement
fine-tuning (RFT) methods predominantly focus on close-ended VQA, thereby
restricting the model's ability to engage in world knowledge retrieval and
flexible task adaptation. More critically, these methods fall short of
addressing the critical clinical demand for open-ended, reasoning-intensive
decision-making. To bridge this gap, we introduce \textbf{MedCCO}, the first
multimodal reinforcement learning framework tailored for medical VQA that
unifies close-ended and open-ended data within a curriculum-driven RFT
paradigm. Specifically, MedCCO is initially fine-tuned on a diverse set of
close-ended medical VQA tasks to establish domain-grounded reasoning
capabilities, and is then progressively adapted to open-ended tasks to foster
deeper knowledge enhancement and clinical interpretability. We validate MedCCO
across eight challenging medical VQA benchmarks, spanning both close-ended and
open-ended settings. Experimental results show that MedCCO consistently
enhances performance and generalization, achieving a 11.4\% accuracy gain
across three in-domain tasks, and a 5.7\% improvement on five out-of-domain
benchmarks. These findings highlight the promise of curriculum-guided RL in
advancing robust, clinically-relevant reasoning in medical multimodal language
models.

</details>


### [55] [Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding](https://arxiv.org/abs/2505.19219)
*Shiyue Wang,Haozheng Xu,Yuhan Zhang,Jingran Lin,Changhong Lu,Xiangfeng Wang,Wenhao Li*

Main category: cs.AI

TL;DR: Multi-Agent Path Finding (MAPF) is crucial for coordinating autonomous systems. This survey integrates classical and learning-based methods, revealing evaluation disparities and suggesting future directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between classical algorithmic approaches and emerging learning-based methods in MAPF research and to provide a comprehensive reference for researchers and practitioners.

Method: A unified framework encompassing search-based methods (e.g., Conflict-Based Search), compilation-based approaches (e.g., SAT, SMT), and data-driven techniques (e.g., reinforcement learning). Systematic analysis of experimental practices across 200+ papers.

Result: Identification of significant disparities in evaluation methodologies between classical and learning-based approaches. A taxonomy of evaluation metrics, environment types, and baseline selections is provided, emphasizing the need for standardized benchmarking protocols.

Conclusion: The survey highlights promising future directions such as mixed-motive MAPF, language-grounded planning, and neural solver architectures. It serves as a reference for researchers and a guide for deploying MAPF solutions in real-world applications.

Abstract: Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial
intelligence and robotics, requiring the computation of collision-free paths
for multiple agents navigating from their start locations to designated goals.
As autonomous systems become increasingly prevalent in warehouses, urban
transportation, and other complex environments, MAPF has evolved from a
theoretical challenge to a critical enabler of real-world multi-robot
coordination. This comprehensive survey bridges the long-standing divide
between classical algorithmic approaches and emerging learning-based methods in
MAPF research. We present a unified framework that encompasses search-based
methods (including Conflict-Based Search, Priority-Based Search, and Large
Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP
formulations), and data-driven techniques (reinforcement learning, supervised
learning, and hybrid strategies). Through systematic analysis of experimental
practices across 200+ papers, we uncover significant disparities in evaluation
methodologies, with classical methods typically tested on larger-scale
instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based
approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy
of evaluation metrics, environment types, and baseline selections, highlighting
the need for standardized benchmarking protocols. Finally, we outline promising
future directions including mixed-motive MAPF with game-theoretic
considerations, language-grounded planning with large language models, and
neural solver architectures that combine the rigor of classical methods with
the flexibility of deep learning. This survey serves as both a comprehensive
reference for researchers and a practical guide for deploying MAPF solutions in
increasingly complex real-world applications.

</details>


### [56] [DeCoDe: Defer-and-Complement Decision-Making via Decoupled Concept Bottleneck Models](https://arxiv.org/abs/2505.19220)
*Chengbo He,Bochao Zou,Junliang Xing,Jiansheng Chen,Yuanchun Shi,Huimin Ma*

Main category: cs.AI

TL;DR: In human-AI collaboration, the abstract introduces DeCoDe, a framework that enhances decision-making by incorporating interpretable concept representations. It supports autonomous AI, human deferral, and collaborative modes, outperforming traditional methods while ensuring robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the limitations of existing Learning to Defer approaches which make binary decisions between AI and humans, ignoring complementary strengths and lacking interpretability.

Method: DeCoDe uses decoupled concept bottleneck models for human-AI collaboration. Strategy decisions are based on human-interpretable concept representations. It operates in three modes: autonomous AI prediction, deferral to humans, and collaborative complementarity. A gating network selects the mode using concept-level inputs trained with a surrogate loss balancing accuracy and human effort.

Result: Experiments show DeCoDe significantly outperforms AI-only, human-only, and traditional deferral methods. It maintains robustness and interpretability even with noisy expert annotations.

Conclusion: DeCoDe offers an improved framework for human-AI collaboration, providing instance-specific, interpretable, and adaptive strategies.

Abstract: In human-AI collaboration, a central challenge is deciding whether the AI
should handle a task, be deferred to a human expert, or be addressed through
collaborative effort. Existing Learning to Defer approaches typically make
binary choices between AI and humans, neglecting their complementary strengths.
They also lack interpretability, a critical property in high-stakes scenarios
where users must understand and, if necessary, correct the model's reasoning.
To overcome these limitations, we propose Defer-and-Complement Decision-Making
via Decoupled Concept Bottleneck Models (DeCoDe), a concept-driven framework
for human-AI collaboration. DeCoDe makes strategy decisions based on
human-interpretable concept representations, enhancing transparency throughout
the decision process. It supports three flexible modes: autonomous AI
prediction, deferral to humans, and human-AI collaborative complementarity,
selected via a gating network that takes concept-level inputs and is trained
using a novel surrogate loss that balances accuracy and human effort. This
approach enables instance-specific, interpretable, and adaptive human-AI
collaboration. Experiments on real-world datasets demonstrate that DeCoDe
significantly outperforms AI-only, human-only, and traditional deferral
baselines, while maintaining strong robustness and interpretability even under
noisy expert annotations.

</details>


### [57] [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234)
*Jialong Zhou,Lichao Wang,Xiao Yang*

Main category: cs.AI

TL;DR: The paper introduces GUARDIAN, a method for detecting and mitigating safety issues in multi-agent collaborations involving large language models (LLMs). It uses a discrete-time temporal attributed graph model and an unsupervised encoder-decoder architecture with incremental training to identify anomalies. Additionally, it employs a graph abstraction mechanism based on Information Bottleneck Theory for efficient processing. Experiments show GUARDIAN's effectiveness and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the safety challenges in multi-agent collaborations using LLMs, such as hallucination amplification and error injection/propagation, which could compromise the integrity and reliability of the system.

Method: GUARDIAN models the multi-agent collaboration process as a discrete-time temporal attributed graph to capture hallucination and error propagation dynamics. It uses an unsupervised encoder-decoder architecture with incremental training to reconstruct node attributes and graph structures from latent embeddings, identifying anomalous nodes and edges. A graph abstraction mechanism based on Information Bottleneck Theory compresses interaction graphs while preserving essential patterns.

Result: GUARDIAN effectively safeguards LLM multi-agent collaborations against various safety vulnerabilities, achieving state-of-the-art accuracy with efficient use of resources.

Conclusion: GUARDIAN is a robust and efficient solution for ensuring safety in multi-agent collaborations involving LLMs, demonstrating superior performance in detecting and mitigating safety concerns.

Abstract: The emergence of large language models (LLMs) enables the development of
intelligent agents capable of engaging in complex and multi-turn dialogues.
However, multi-agent collaboration face critical safety challenges, such as
hallucination amplification and error injection and propagation. This paper
presents GUARDIAN, a unified method for detecting and mitigating multiple
safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the
multi-agent collaboration process as a discrete-time temporal attributed graph,
GUARDIAN explicitly captures the propagation dynamics of hallucinations and
errors. The unsupervised encoder-decoder architecture incorporating an
incremental training paradigm, learns to reconstruct node attributes and graph
structures from latent embeddings, enabling the identification of anomalous
nodes and edges with unparalleled precision. Moreover, we introduce a graph
abstraction mechanism based on the Information Bottleneck Theory, which
compresses temporal interaction graphs while preserving essential patterns.
Extensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM
multi-agent collaborations against diverse safety vulnerabilities, achieving
state-of-the-art accuracy with efficient resource utilization.

</details>


### [58] [Sensorimotor features of self-awareness in multimodal large language models](https://arxiv.org/abs/2505.19237)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Diego Torricelli,Gabriel Delgado-Oleas,Jose Ignacio Serrano,Maria Dolores del Castillo Sobrino,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 通过将多模态LLM整合到自主移动机器人中，研究发现只要获得关于世界和自身的适当感官信息，多模态LLM就能展现出现有的自我意识，这为人工具身认知系统打开了大门。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型是否仅通过感测运动体验就能发展出自我意识。

Method: 将多模态LLM集成到自主移动机器人中，测试其能否通过感官整合影响不同的自我意识维度及其与过去-现在记忆的协调，以及驱动自我识别的分层内部关联来实现自我意识。

Result: 系统表现出强大的环境意识、自我识别和预测意识，能够推断其机器人性质和运动特征。感官输入的消融测试确定了每个维度的关键模式，展示了传感器之间的补偿相互作用，并确认了结构化和情景记忆在连贯推理中的关键作用。

Conclusion: 只要有适当的关于世界和自身的感官信息，多模态LLM就会展现出自我意识，这为人工具身认知系统的发展提供了可能性。

Abstract: Self-awareness - the ability to distinguish oneself from the surrounding
environment - underpins intelligent, autonomous behavior. Recent advances in AI
achieve human-like performance in tasks integrating multimodal information,
particularly in large language models, raising interest in the embodiment
capabilities of AI agents on nonhuman platforms such as robots. Here, we
explore whether multimodal LLMs can develop self-awareness solely through
sensorimotor experiences. By integrating a multimodal LLM into an autonomous
mobile robot, we test its ability to achieve this capacity. We find that the
system exhibits robust environmental awareness, self-recognition and predictive
awareness, allowing it to infer its robotic nature and motion characteristics.
Structural equation modeling reveals how sensory integration influences
distinct dimensions of self-awareness and its coordination with past-present
memory, as well as the hierarchical internal associations that drive
self-identification. Ablation tests of sensory inputs identify critical
modalities for each dimension, demonstrate compensatory interactions among
sensors and confirm the essential role of structured and episodic memory in
coherent reasoning. These findings demonstrate that, given appropriate sensory
information about the world and itself, multimodal LLMs exhibit emergent
self-awareness, opening the door to artificial embodied cognitive systems.

</details>


### [59] [Using Large Language Models to Assess Teachers' Pedagogical Content Knowledge](https://arxiv.org/abs/2505.19266)
*Yaxuan Yang,Shiyu Wang,Xiaoming Zhai*

Main category: cs.AI

TL;DR: This paper explores whether large language models (LLMs) introduce construct-irrelevant variance (CIV) similar to or different from traditional machine learning (ML) and human raters when assessing teachers' pedagogical content knowledge (PCK). Results indicate that while LLMs contribute to scoring efficiency, they also introduce CIV.


<details>
  <summary>Details</summary>
Motivation: Assessing PCK through performance-based tasks is time-consuming and labor-intensive. The emergence of LLMs offers an opportunity for efficient automatic scoring, but the potential impact on CIV compared with traditional ML and human raters needs to be examined.

Method: The study investigates three sources of CIV (scenario variability, rater severity, and rater sensitivity to scenario) in video-based constructed-response tasks targeting two PCK sub-constructs. Generalized linear mixed models (GLMMs) are used to compare variance components and rater-level scoring patterns across three scoring sources: human raters, supervised ML, and LLMs.

Result: Scenario-level variance was minimal across tasks, while rater-related factors contributed significantly to CIV, especially in the more interpretive Task II. The ML model was the most severe and least sensitive rater, whereas the LLM was the most lenient.

Conclusion: LLMs contribute to scoring efficiency but also introduce CIV like human raters do, though with varying levels of contribution compared to supervised ML.

Abstract: Assessing teachers' pedagogical content knowledge (PCK) through
performance-based tasks is both time and effort-consuming. While large language
models (LLMs) offer new opportunities for efficient automatic scoring, little
is known about whether LLMs introduce construct-irrelevant variance (CIV) in
ways similar to or different from traditional machine learning (ML) and human
raters. This study examines three sources of CIV -- scenario variability, rater
severity, and rater sensitivity to scenario -- in the context of video-based
constructed-response tasks targeting two PCK sub-constructs: analyzing student
thinking and evaluating teacher responsiveness. Using generalized linear mixed
models (GLMMs), we compared variance components and rater-level scoring
patterns across three scoring sources: human raters, supervised ML, and LLM.
Results indicate that scenario-level variance was minimal across tasks, while
rater-related factors contributed substantially to CIV, especially in the more
interpretive Task II. The ML model was the most severe and least sensitive
rater, whereas the LLM was the most lenient. These findings suggest that the
LLM contributes to scoring efficiency while also introducing CIV as human
raters do, yet with varying levels of contribution compared to supervised ML.
Implications for rater training, automated scoring design, and future research
on model interpretability are discussed.

</details>


### [60] [Next Token Prediction Is a Dead End for Creativity](https://arxiv.org/abs/2505.19277)
*Ibukun Olatunji,Mark Sheppard*

Main category: cs.AI

TL;DR: The paper argues that token prediction models lack true creativity, using battle rap as a case study to show their limitations, and suggests reframing AI creativity as an interactive process.


<details>
  <summary>Details</summary>
Motivation: To explore the limitations of token prediction models in generating truly creative outputs, particularly in contexts requiring spontaneity, originality, and emotional resonance.

Method: Using battle rap as a case study to analyze and demonstrate the limitations of predictive systems in engaging in adversarial or emotionally resonant exchanges.

Result: Predictive systems favor surface-level coherence over deeper aspects of creativity such as spontaneity and originality, making them inadequate for certain creative interactions.

Conclusion: AI systems should be reframed to focus on interactivity rather than mere prediction to enhance expressiveness, responsiveness, and alignment with human creative practices.

Abstract: This paper argues that token prediction is fundamentally misaligned with real
creativity. While next-token models have enabled impressive advances in
language generation, their architecture favours surface-level coherence over
spontaneity, originality, and improvisational risk. We use battle rap as a case
study to expose the limitations of predictive systems, demonstrating that they
cannot truly engage in adversarial or emotionally resonant exchanges. By
reframing creativity as an interactive process rather than a predictive output,
we offer a vision for AI systems that are more expressive, responsive, and
aligned with human creative practice.

</details>


### [61] [Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics](https://arxiv.org/abs/2505.19317)
*Tin Nguyen,Jiannan Xu,Zora Che,Phuong-Anh Nguyen-Le,Rushil Dandamudi,Donald Braman,Furong Huang,Hal Daumé III,Zubin Jelveh*

Main category: cs.AI

TL;DR: This paper proposes a philosophy-informed way to conceptualize and evaluate Effort-aware Fairness (EaF) based on the concept of Force, which considers the temporal trajectory of predictive features coupled with inertia. Empirical contributions include a pre-registered human subjects experiment and pipelines to compute Effort-aware Individual/Group Fairness in criminal justice and personal finance contexts.


<details>
  <summary>Details</summary>
Motivation: Existing AI fairness metrics do not consider the notion of effort, which is important in how Philosophy and humans understand fairness.

Method: The paper introduces a theoretical formulation of EaF metrics based on the concept of Force, conducts a pre-registered human subjects experiment, and develops pipelines for computing Effort-aware Individual/Group Fairness in specific contexts.

Result: The human subjects experiment demonstrates that people consider the temporal trajectory of a predictive feature more than its aggregate value. Pipelines for Effort-aware Fairness are developed for criminal justice and personal finance contexts.

Conclusion: This work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have made significant efforts to improve but are still affected by systemic/early-life disadvantages.

Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have
uncovered bias in AI-assisted decision-making outcomes, they do not consider
how much effort one has spent to get to where one is today in the input feature
space. However, the notion of effort is important in how Philosophy and humans
understand fairness. We propose a philosophy-informed way to conceptualize and
evaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal
trajectory of predictive features coupled with inertia. In addition to our
theoretical formulation of EaF metrics, our empirical contributions include: 1/
a pre-registered human subjects experiment, which demonstrates that for both
stages of the (individual) fairness evaluation process, people consider the
temporal trajectory of a predictive feature more than its aggregate value; 2/
pipelines to compute Effort-aware Individual/Group Fairness in the criminal
justice and personal finance contexts. Our work may enable AI model auditors to
uncover and potentially correct unfair decisions against individuals who spent
significant efforts to improve but are still stuck with systemic/early-life
disadvantages outside their control.

</details>


### [62] [Evaluating Steering Techniques using Human Similarity Judgments](https://arxiv.org/abs/2505.19333)
*Zach Studdiford,Timothy T. Rogers,Siddharth Suresh,Kushin Mukherjee*

Main category: cs.AI

TL;DR: Prompt-based steering methods in LLMs perform better in both accuracy and human cognitive alignment, especially when judging concept similarity by size or kind. However, LLMs show a bias towards 'kind' similarity and difficulty with 'size' alignment.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLM steering techniques not only on task-specific performance but also on how well they align with human cognition using a triadic similarity judgment task.

Method: Assess steered LLMs using a triadic similarity judgment task to judge the similarity between concepts based on size or kind and compare different steering methods including prompt-based ones.

Result: Prompt-based steering methods outperformed others in terms of steering accuracy and model-to-human alignment. LLMs exhibited a bias towards 'kind' similarity and had trouble with 'size' alignment.

Conclusion: This human cognition-grounded evaluation supports the efficacy of prompt-based steering methods and highlights privileged representational axes in LLMs prior to steering.

Abstract: Current evaluations of Large Language Model (LLM) steering techniques focus
on task-specific performance, overlooking how well steered representations
align with human cognition. Using a well-established triadic similarity
judgment task, we assessed steered LLMs on their ability to flexibly judge
similarity between concepts based on size or kind. We found that prompt-based
steering methods outperformed other methods both in terms of steering accuracy
and model-to-human alignment. We also found LLMs were biased towards 'kind'
similarity and struggled with 'size' alignment. This evaluation approach,
grounded in human cognition, adds further support to the efficacy of
prompt-based steering and reveals privileged representational axes in LLMs
prior to steering.

</details>


### [63] [PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity Evaluation](https://arxiv.org/abs/2505.19347)
*Yongmin Yoo,Qiongkai Xu,Longbing Cao*

Main category: cs.AI

TL;DR: PatentMind is a new framework for evaluating patent similarity using Multi-Aspect Reasoning Graph (MARG), focusing on technical features, application domains, and claim scopes. It shows high correlation with expert annotations.


<details>
  <summary>Details</summary>
Motivation: Current methods for patent similarity evaluation often neglect the complex structure of patent documents that combine technical specifications, legal boundaries, and application contexts.

Method: PatentMind decomposes patents into three dimensions - technical feature, application domain, and claim scope - to calculate dimension-specific similarity scores. These scores are dynamically weighted through a four-stage reasoning process integrating contextual signals.

Result: PatentMind achieves a strong correlation (r=0.938) with expert annotations, significantly outperforming embedding-based models and advanced prompt engineering methods.

Conclusion: Modular reasoning frameworks like PatentMind can effectively overcome key limitations of embedding-based methods in analyzing patent similarity.

Abstract: Patent similarity evaluation plays a critical role in intellectual property
analysis. However, existing methods often overlook the intricate structure of
patent documents, which integrate technical specifications, legal boundaries,
and application contexts. We introduce PatentMind, a novel framework for patent
similarity assessment based on a Multi-Aspect Reasoning Graph (MARG).
PatentMind decomposes patents into three core dimensions: technical feature,
application domain, and claim scope, to compute dimension-specific similarity
scores. These scores are dynamically weighted through a four-stage reasoning
process which integrates contextual signals to emulate expert-level judgment.
To support evaluation, we construct PatentSimBench, a human-annotated benchmark
comprising 500 patent pairs. Experimental results demonstrate that PatentMind
achieves a strong correlation ($r=0.938$) with expert annotations,
significantly outperforming embedding-based models and advanced prompt
engineering methods.These results highlight the effectiveness of modular
reasoning frameworks in overcoming key limitations of embedding-based methods
for analyzing patent similarity.

</details>


### [64] [Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation](https://arxiv.org/abs/2505.19353)
*Camilo Chacón Sartori*

Main category: cs.AI

TL;DR: With the rise of GenAI, Large Language Models are increasingly employed for code generation. This paper examines distinct 'Architectures of Error' between human and machine code generation through Dennett's mechanistic functionalism and Rescher's methodological pragmatism. It aims to offer a structured framework for understanding GenAI's unique epistemological challenges and providing software engineers a basis for more critically informed engagement.


<details>
  <summary>Details</summary>
Motivation: To ground an epistemic distinction between human and machine code generation by articulating distinct 'Architectures of Error'.

Method: Draw upon Dennett's mechanistic functionalism and Rescher's methodological pragmatism to systematically differentiate error profiles and utilize Floridi's levels of abstraction to understand how these error dimensions interact.

Result: Raises critical philosophical questions concerning semantic coherence, security robustness, epistemic limits, and control mechanisms in human-AI collaborative software development.

Conclusion: Offers philosophers a structured framework for understanding GenAI's unique epistemological challenges and provides software engineers a basis for more critically informed engagement.

Abstract: With the rise of generative AI (GenAI), Large Language Models are
increasingly employed for code generation, becoming active co-authors alongside
human programmers. Focusing specifically on this application domain, this paper
articulates distinct ``Architectures of Error'' to ground an epistemic
distinction between human and machine code generation. Examined through their
shared vulnerability to error, this distinction reveals fundamentally different
causal origins: human-cognitive versus artificial-stochastic. To develop this
framework and substantiate the distinction, the analysis draws critically upon
Dennett's mechanistic functionalism and Rescher's methodological pragmatism. I
argue that a systematic differentiation of these error profiles raises critical
philosophical questions concerning semantic coherence, security robustness,
epistemic limits, and control mechanisms in human-AI collaborative software
development. The paper also utilizes Floridi's levels of abstraction to provide
a nuanced understanding of how these error dimensions interact and may evolve
with technological advancements. This analysis aims to offer philosophers a
structured framework for understanding GenAI's unique epistemological
challenges, shaped by these architectural foundations, while also providing
software engineers a basis for more critically informed engagement.

</details>


### [65] [Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments](https://arxiv.org/abs/2505.19361)
*Mario Leiva,Noel Ngu,Joshua Shay Kricheli,Aditya Taparia,Ransalu Senanayake,Paulo Shakarian,Nathaniel Bastian,John Corcoran,Gerardo Simari*

Main category: cs.AI

TL;DR: 通过引入一致性推理方法，利用多个预训练模型的预测结果，缓解单一模型在新环境下的性能下降问题。提出的方法在模拟空中图像数据集上表现优于单个模型和标准集成基线。


<details>
  <summary>Details</summary>
Motivation: 在新环境下部署预训练感知模型时，由于分布变化，通常会导致性能下降。虽然最近使用逻辑规则进行元认知的人工智能方法可以表征和过滤模型错误，但提高精度往往会以降低召回率为代价。

Method: 将识别和管理来自各种模型的冲突预测挑战表示为基于一致性的推理问题。输入预测和从每个模型中学习到的错误检测规则被编码在一个逻辑程序中，寻找一个推理解释——即模型预测的一个子集——最大化预测覆盖率，同时确保逻辑不一致的比例低于指定阈值。为此提出了两种算法：基于整数规划（IP）的确切方法和有效的启发式搜索（HS）。

Result: 在具有受控复杂分布变化的模拟空中图像数据集上的广泛实验表明，所提出的推理框架在F1分数和准确性方面比单个模型和标准集成基线高出约13.6%和16.6%。

Conclusion: 研究结果验证了使用基于一致性的推理作为一种有效机制，可以在具有挑战性、新颖的情况下稳健地整合来自多个不完美推理器的知识。

Abstract: The deployment of pre-trained perception models in novel environments often
leads to performance degradation due to distributional shifts. Although recent
artificial intelligence approaches for metacognition use logical rules to
characterize and filter model errors, improving precision often comes at the
cost of reduced recall. This paper addresses the hypothesis that leveraging
multiple pre-trained models can mitigate this recall reduction. We formulate
the challenge of identifying and managing conflicting predictions from various
models as a consistency-based abduction problem. The input predictions and the
learned error detection rules derived from each model are encoded in a logic
program. We then seek an abductive explanation--a subset of model
predictions--that maximizes prediction coverage while ensuring the rate of
logical inconsistencies (derived from domain constraints) remains below a
specified threshold. We propose two algorithms for this knowledge
representation task: an exact method based on Integer Programming (IP) and an
efficient Heuristic Search (HS). Through extensive experiments on a simulated
aerial imagery dataset featuring controlled, complex distributional shifts, we
demonstrate that our abduction-based framework outperforms individual models
and standard ensemble baselines, achieving, for instance, average relative
improvements of approximately 13.6% in F1-score and 16.6% in accuracy across 15
diverse test datasets when compared to the best individual model. Our results
validate the use of consistency-based abduction as an effective mechanism to
robustly integrate knowledge from multiple imperfect reasoners in challenging,
novel scenarios.

</details>


### [66] [Foundations of Top-$k$ Decoding For Language Models](https://arxiv.org/abs/2505.19371)
*Georgy Noarov,Soham Mallick,Tao Wang,Sunay Joshi,Yan Sun,Yangxinyu Xie,Mengxin Yu,Edgar Dobriban*

Main category: cs.AI

TL;DR: 本研究为Top-k解码提供了一个理论框架，解释并推广了其使用，并发现了新的解码策略。


<details>
  <summary>Details</summary>
Motivation: 尽管Top-k解码在实际应用中广泛使用，但其精确的理论依据尚不清楚。因此，需要一个理论框架来解释和推广Top-k解码方法。

Method: 通过将解码视为稀疏概率分布的恢复问题，提出了一种基于最小化Bregman散度（包括原始和对偶情况）以及引入稀疏性诱导的ℓ₀正则化的解码方法。证明了对于一大类散度，可以高效地优化目标函数，并且最优解码策略是贪婪的。进一步表明损失函数在k上离散凸，使得可以通过二分查找有效找到最优k值。

Result: 揭示了Top-k解码作为KL散度的特殊情况出现，并提出了具有不同行为的新解码策略（例如，在重新归一化后非线性地增加较大概率）。

Conclusion: 该理论框架不仅解释了Top-k解码，还推广了解码方法，发现了新的解码策略，这些策略可能在实际应用中有不同的表现。

Abstract: Top-$k$ decoding is a widely used method for sampling from LLMs: at each
token, only the largest $k$ next-token-probabilities are kept, and the next
token is sampled after re-normalizing them to sum to unity. Top-$k$ and other
sampling methods are motivated by the intuition that true next-token
distributions are sparse, and the noisy LLM probabilities need to be truncated.
However, to our knowledge, a precise theoretical motivation for the use of
top-$k$ decoding is missing. In this work, we develop a theoretical framework
that both explains and generalizes top-$k$ decoding. We view decoding at a
fixed token as the recovery of a sparse probability distribution. We consider
\emph{Bregman decoders} obtained by minimizing a separable Bregman divergence
(for both the \emph{primal} and \emph{dual} cases) with a sparsity-inducing
$\ell_0$ regularization. Despite the combinatorial nature of the objective, we
show how to optimize it efficiently for a large class of divergences. We show
that the optimal decoding strategies are greedy, and further that the loss
function is discretely convex in $k$, so that binary search provably and
efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a
special case for the KL divergence, and identify new decoding strategies that
have distinct behaviors (e.g., non-linearly up-weighting larger probabilities
after re-normalization).

</details>


### [67] [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/abs/2505.19381)
*Anqing Jiang,Yu Gao,Zhigang Sun,Yiru Wang,Jijun Wang,Jinghao Chai,Qian Cao,Yuweng Heng,Hao Jiang,Zongzheng Zhang,Xianda Guo,Hao Sun,Hao Zhao*

Main category: cs.AI

TL;DR: An end-to-end autonomous driving method using a hybrid sparse-dense diffusion policy and Vision-Language Model (VLM) called Diff-VLA is proposed, achieving superior performance in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end autonomous driving methods suffer from expensive BEV computation, lack of action diversity, and sub-optimal decision making in complex real-world scenarios.

Method: The authors propose Diff-VLA, which uses a hybrid sparse-dense diffusion policy empowered by a Vision-Language Model (VLM) to improve multi-modal driving behavior and decision making through interaction across agent, map instances, and VLM output.

Result: Diff-VLA shows superior performance in the Autonomous Grand Challenge 2025, achieving 45.0 PDMS in both real and synthetic scenarios.

Conclusion: Diff-VLA addresses the challenges faced by existing end-to-end autonomous driving methods and demonstrates significant improvements in performance.

Abstract: Research interest in end-to-end autonomous driving has surged owing to its
fully differentiable design integrating modular tasks, i.e. perception,
prediction and planing, which enables optimization in pursuit of the ultimate
goal. Despite the great potential of the end-to-end paradigm, existing methods
suffer from several aspects including expensive BEV (bird's eye view)
computation, action diversity, and sub-optimal decision in complex real-world
scenarios. To address these challenges, we propose a novel hybrid sparse-dense
diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.
We explore the sparse diffusion representation for efficient multi-modal
driving behavior. Moreover, we rethink the effectiveness of VLM driving
decision and improve the trajectory generation guidance through deep
interaction across agent, map instances and VLM output. Our method shows
superior performance in Autonomous Grand Challenge 2025 which contains
challenging real and reactive synthetic scenarios. Our methods achieves 45.0
PDMS.

</details>


### [68] [CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models](https://arxiv.org/abs/2505.19383)
*Varun Reddy,Yen-Ling Kuo*

Main category: cs.AI

TL;DR: CaseEdit is a new dataset and pipeline for evaluating personalized commonsense knowledge editing in small LLMs. It uses ATOMIC20/20 and evaluates methods across reliability, generalization, locality, and portability. AlphaEdit outperforms other methods when applied to an LLaMA 3.2 3B model.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) have strong factual recall and reasoning capabilities but face challenges adapting to user-specific, commonsense knowledge, especially in small-parameter settings where computational efficiency is important.

Method: Introduced CaseEdit which builds on the ATOMIC20/20 commonsense graph and uses a multi-stage inference process to generate edits for household objects with targeted evaluation questions across four axes: reliability, generalization, locality, and portability. Evaluated established knowledge editing methods using this dataset.

Result: AlphaEdit, employing null-space projection to minimize interference with unrelated knowledge, consistently outperformed other methods when applied to an LLaMA 3.2 3B model, even in scalability tests, showing minimal ripple effects.

Conclusion: Using CaseEdit with effective editing techniques like AlphaEdit allows small models to internalize high-quality, context-sensitive common-sense knowledge, leading to lightweight, personalized assistants.

Abstract: Large language models (LLMs) exhibit strong performance on factual recall and
general reasoning but struggle to adapt to user-specific, commonsense
knowledge, a challenge particularly acute in small-parameter settings where
computational efficiency is prioritized. We introduce CaseEdit, a new dataset
and generation pipeline for evaluating localized, personalized commonsense
knowledge editing in small LLMs to address this. Built upon the ATOMIC20/20
commonsense graph, CaseEdit uses a multi-stage inference process to generate
both typical and atypical contextual edits for household objects, paired with
targeted evaluation questions across four axes: reliability, generalization,
locality, and portability. We evaluate established knowledge editing methods
using CaseEdit and demonstrate that AlphaEdit, a technique employing null-space
projection to minimize interference with unrelated knowledge, consistently
outperforms other methods when applied to an LLaMA 3.2 3B model, even in
scalability tests, showing minimal ripple effects. Our results indicate that
using CaseEdit with effective editing techniques like AlphaEdit allows small
models to internalize high-quality, context-sensitive common-sense knowledge,
paving the way for lightweight, personalized assistants.

</details>


### [69] [Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods](https://arxiv.org/abs/2505.19402)
*Tai-Quan Peng,Xuzhen Yang*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）如何改变传播研究及社会科学中核心定量方法，包括内容分析、调查研究和实验研究。尽管LLMs不会取代经典方法，但它们为文本编码与解释、动态应答模拟以及个性化互动刺激生成提供了新可能。文章结合Lasswell框架，展示了LLMs在信息研究、受众分析和效果研究中的应用，并强调了有效整合LLMs与传统逻辑的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，传播研究和社会科学中的核心定量方法面临新的机遇与挑战，需要重新审视这些技术对研究范式的影响并探索其潜力与局限。

Method: 通过跨学科视角，本文回顾了LLMs在内容分析、调查研究和实验研究中的具体应用，结合Lasswell的框架分析LLMs如何改变信息研究、受众分析和效果研究，并讨论了其在有效性、偏差和可解释性方面的问题。

Result: LLMs能够提供文本编码的新方式、模拟动态应答者以及生成个性化刺激，同时允许解释变异、受众轨迹建模和反事实实验，但其应用仍存在有效性和偏差等限制。

Conclusion: 尽管LLMs引入了新可能性，但经典研究逻辑仍然不可或缺。未来的研究应将LLMs视为认识论和文化工具，以严谨且富有想象力的方式使用它们。

Abstract: This paper examines how large language models (LLMs) are transforming core
quantitative methods in communication research in particular, and in the social
sciences more broadly-namely, content analysis, survey research, and
experimental studies. Rather than replacing classical approaches, LLMs
introduce new possibilities for coding and interpreting text, simulating
dynamic respondents, and generating personalized and interactive stimuli.
Drawing on recent interdisciplinary work, the paper highlights both the
potential and limitations of LLMs as research tools, including issues of
validity, bias, and interpretability. To situate these developments
theoretically, the paper revisits Lasswell's foundational framework -- "Who
says what, in which channel, to whom, with what effect?" -- and demonstrates
how LLMs reconfigure message studies, audience analysis, and effects research
by enabling interpretive variation, audience trajectory modeling, and
counterfactual experimentation. Revisiting the metaphor of the methodological
compass, the paper argues that classical research logics remain essential as
the field integrates LLMs and generative AI. By treating LLMs not only as
technical instruments but also as epistemic and cultural tools, the paper calls
for thoughtful, rigorous, and imaginative use of LLMs in future communication
and social science research.

</details>


### [70] [Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model](https://arxiv.org/abs/2505.19406)
*Tianle Li,Jihai Zhang,Yongming Rao,Yu Cheng*

Main category: cs.AI

TL;DR: 当前视觉-语言模型（VLMs）在跨模态和跨任务场景下的组合泛化能力存在显著不足，尽管强化学习（RL）训练的模型在组合泛化方面优于监督微调（SFT），但VLMs仍难以有效整合不同技能。通过显式描述视觉内容（如caption-before-thinking）及奖励逐步视觉到文本的对齐，可显著提升模型性能。研究揭示了视觉到文本对齐与准确视觉接地对于提升VLM组合性的重要性，并为未来改进提供了方向。


<details>
  <summary>Details</summary>
Motivation: 评估当前大型视觉-语言模型（VLMs）是否能够通过类似强化学习（RL）的后训练策略直接继承大型语言模型（LLMs）的强大推理能力，特别是在分布外条件下跨模态或跨任务的能力组成。

Method: 设计了一套诊断任务，这些任务在单模态任务或孤立推理技能上训练模型，并在需要技能整合的多模态、组合变体上进行评估。通过比较监督微调（SFT）和强化学习（RL）训练的模型，分析它们在组合泛化上的表现差异。此外，还测试了显式描述视觉内容（如caption-before-thinking）以及奖励逐步视觉到文本接地的效果。

Result: 1. RL训练的模型在组合泛化方面明显优于SFT，表现出更好的已学技能整合；2. 尽管VLMs在单独任务上表现良好，但在跨模态和跨任务场景下组合泛化表现较差；3. 显式描述视觉内容（如caption-before-thinking）以及奖励逐步视觉到文本接地可以显著提高模型性能。

Conclusion: 当前基于强化学习的VLM训练方法在组合泛化方面存在局限性。研究结果表明，视觉到文本对齐和准确的视觉接地是提升VLM组合性的关键因素，并为构建能够在跨模态和跨任务场景下进行组合推理的模型提供了可行的改进建议。

Abstract: While large language models (LLMs) demonstrate strong reasoning capabilities
utilizing reinforcement learning (RL) with verifiable reward, whether large
vision-language models (VLMs) can directly inherit such capabilities through
similar post-training strategies remains underexplored. In this work, we
conduct a systematic compositional probing study to evaluate whether current
VLMs trained with RL or other post-training strategies can compose capabilities
across modalities or tasks under out-of-distribution conditions. We design a
suite of diagnostic tasks that train models on unimodal tasks or isolated
reasoning skills, and evaluate them on multimodal, compositional variants
requiring skill integration. Through comparisons between supervised fine-tuning
(SFT) and RL-trained models, we identify three key findings: (1) RL-trained
models consistently outperform SFT on compositional generalization,
demonstrating better integration of learned skills; (2) although VLMs achieve
strong performance on individual tasks, they struggle to generalize
compositionally under cross-modal and cross-task scenario, revealing a
significant gap in current training strategies; (3) enforcing models to
explicitly describe visual content before reasoning (e.g.,
caption-before-thinking), along with rewarding progressive vision-to-text
grounding, yields notable gains. It highlights two essential ingredients for
improving compositionality in VLMs: visual-to-text alignment and accurate
visual grounding. Our findings shed light on the current limitations of
RL-based reasoning VLM training and provide actionable insights toward building
models that reason compositionally across modalities and tasks.

</details>


### [71] [Fusion Intelligence for Digital Twinning AI Data Centers: A Synergistic GenAI-PhyAI Approach](https://arxiv.org/abs/2505.19409)
*Ruihang Wang,Minghao Li,Zhiwei Cao,Jimin Jia,Kyle Guan,Yonggang Wen*

Main category: cs.AI

TL;DR: The paper introduces Fusion Intelligence, a new framework combining generative AI (GenAI) and physical AI (PhyAI) to create accurate digital twins for AI-dedicated data centers (AIDCs). These digital twins enhance power usage effectiveness (PUE) optimization in the design stage and improve accuracy over physics-based models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current AI methods for creating digital twins in AI-dedicated data centers (AIDCs), including the extensive customization required by physical AI (PhyAI) and the potential inaccuracies from generative AI (GenAI).

Method: Fusion Intelligence synergizes GenAI's automation with PhyAI's domain grounding through dual-agent collaboration. GenAI generates tokenized AIDC digital twins from natural language prompts, while PhyAI optimizes these twins using physical constraints and real-time data.

Result: Case studies show that Fusion Intelligence automates the creation and validation of AIDC digital twins, delivering predictive analytics for PUE optimization in the design stage and improving accuracy compared to pure physics-based models developed by human experts.

Conclusion: Fusion Intelligence provides a promising approach to accelerate digital transformation and offers reliable and efficient AI-driven solutions for mission-critical infrastructures.

Abstract: The explosion in artificial intelligence (AI) applications is pushing the
development of AI-dedicated data centers (AIDCs), creating management
challenges that traditional methods and standalone AI solutions struggle to
address. While digital twins are beneficial for AI-based design validation and
operational optimization, current AI methods for their creation face
limitations. Specifically, physical AI (PhyAI) aims to capture the underlying
physical laws, which demands extensive, case-specific customization, and
generative AI (GenAI) can produce inaccurate or hallucinated results. We
propose Fusion Intelligence, a novel framework synergizing GenAI's automation
with PhyAI's domain grounding. In this dual-agent collaboration, GenAI
interprets natural language prompts to generate tokenized AIDC digital twins.
Subsequently, PhyAI optimizes these generated twins by enforcing physical
constraints and assimilating real-time data. Case studies demonstrate the
advantages of our framework in automating the creation and validation of AIDC
digital twins. These twins deliver predictive analytics to support power usage
effectiveness (PUE) optimization in the design stage. With operational data
collected, the digital twin accuracy is further improved compared with pure
physics-based models developed by human experts. Fusion Intelligence offers a
promising pathway to accelerate digital transformation. It enables more
reliable and efficient AI-driven digital transformation for a broad range of
mission-critical infrastructures.

</details>


### [72] [Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study](https://arxiv.org/abs/2505.19414)
*Ruihang Wang,Zhiwei Cao,Qingang Zhang,Rui Tan,Yonggang Wen,Tommy Leung,Stuart Kennedy,Justin Teoh*

Main category: cs.AI

TL;DR: Data centers in tropical regions face high cooling costs due to high temperature and humidity. There are concerns about deploying machine learning solutions due to model extrapolation and system safety issues. This paper proposes incorporating physical characteristics of data centers into machine learning solutions, presenting a physics-informed machine learning system for modeling and optimization problems. A case study demonstrates the approach's effectiveness.


<details>
  <summary>Details</summary>
Motivation: Operating data centers in tropical regions faces unique challenges such as consistently high ambient temperature and relative humidity throughout the year, leading to increased cooling costs. Existing machine learning-based approaches have not been widely deployed due to concerns about model extrapolation capabilities and system safety issues.

Method: The method involves incorporating the physical characteristics of data centers into traditional data-driven machine learning solutions. The process begins with introducing the data center system, including relevant multiphysics processes and data-physics availability. Then it outlines associated modeling and optimization problems and proposes an integrated, physics-informed machine learning system.

Result: A case study on an industry-grade tropical data center demonstrates the effectiveness of the proposed approach in addressing the modeling and optimization problems across varying levels of operational intelligence.

Conclusion: The paper concludes by discussing key challenges and highlighting potential future directions for research and development in this area.

Abstract: Data centers are the backbone of computing capacity. Operating data centers
in the tropical regions faces unique challenges due to consistently high
ambient temperature and elevated relative humidity throughout the year. These
conditions result in increased cooling costs to maintain the reliability of the
computing systems. While existing machine learning-based approaches have
demonstrated potential to elevate operations to a more proactive and
intelligent level, their deployment remains dubious due to concerns about model
extrapolation capabilities and associated system safety issues. To address
these concerns, this article proposes incorporating the physical
characteristics of data centers into traditional data-driven machine learning
solutions. We begin by introducing the data center system, including the
relevant multiphysics processes and the data-physics availability. Next, we
outline the associated modeling and optimization problems and propose an
integrated, physics-informed machine learning system to address them. Using the
proposed system, we present relevant applications across varying levels of
operational intelligence. A case study on an industry-grade tropical data
center is provided to demonstrate the effectiveness of our approach. Finally,
we discuss key challenges and highlight potential future directions.

</details>


### [73] [Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents](https://arxiv.org/abs/2505.19436)
*Ye Ye*

Main category: cs.AI

TL;DR: The paper presents Task Memory Engine (TME), a modular memory controller that upgrades existing LLMs into robust, revision-aware agents without fine-tuning. TME uses graph-based structures to replace linear context and significantly reduces hallucinations and misinterpretations in multi-step interactions.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle with multi-step interactions due to reliance on linear, unstructured context which leads to issues such as hallucination, repetition, or misinterpretation of user corrections.

Method: TME implements a spatial memory framework that replaces flat context with graph-based structures such as trees or directed acyclic graphs (DAGs). It includes a TRIM component for modeling task semantics and user intent. This approach supports consistent, multi-turn reasoning and enables dependency-tracked revisions.

Result: TME eliminates 100% of hallucinations and misinterpretations in three tasks and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct.

Conclusion: TME's modular design supports plug-and-play deployment and domain-specific customization. The scalable architecture addresses a critical gap in agent performance across complex, interactive settings.

Abstract: Large Language Models (LLMs) falter in multi-step interactions -- often
hallucinating, repeating actions, or misinterpreting user corrections -- due to
reliance on linear, unstructured context. This fragility stems from the lack of
persistent memory to track evolving goals and task dependencies, undermining
trust in autonomous agents. We introduce the Task Memory Engine (TME), a
modular memory controller that transforms existing LLMs into robust,
revision-aware agents without fine-tuning. TME implements a spatial memory
framework that replaces flat context with graph-based structures to support
consistent, multi-turn reasoning. Departing from linear concatenation and
ReAct-style prompting, TME builds a dynamic task graph -- either a tree or
directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with
prior context, and enable dependency-tracked revisions. Its Task Representation
and Intent Management (TRIM) component models task semantics and user intent to
ensure accurate interpretation. Across four multi-turn scenarios-trip planning,
cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%
of hallucinations and misinterpretations in three tasks, and reduces
hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,
outperforming ReAct. TME's modular design supports plug-and-play deployment and
domain-specific customization, adaptable to both personal assistants and
enterprise automation. We release TME's codebase, benchmarks, and components as
open-source resources, enabling researchers to develop reliable LLM agents.
TME's scalable architecture addresses a critical gap in agent performance
across complex, interactive settings.

</details>


### [74] [Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning](https://arxiv.org/abs/2505.19442)
*Dutao Zhang,Sergey Kovalchuk,YuLong He*

Main category: cs.AI

TL;DR: 提出了一种结合对比学习和条件解码的两阶段训练框架，用于可控代码生成。该方法能够灵活控制代码风格，支持风格插值和用户个性化，并且不会牺牲代码正确性。


<details>
  <summary>Details</summary>
Motivation: 目前可控代码生成是一项具有挑战性的任务，需要能够在保持功能的同时遵循指定的风格。

Method: 提出了一种两阶段训练框架：第一阶段将代码风格表示与语义和结构特征对齐；第二阶段在学习到的风格向量条件下微调语言模型（如Flan-T5）以指导生成。

Result: 相比之前的工作，该统一框架提供了更好的风格控制能力，同时保持了代码的正确性。

Conclusion: 这是首批将对比对齐与条件解码结合用于风格引导代码生成的方法之一。

Abstract: Controllable code generation, the ability to synthesize code that follows a
specified style while maintaining functionality, remains a challenging task. We
propose a two-stage training framework combining contrastive learning and
conditional decoding to enable flexible style control. The first stage aligns
code style representations with semantic and structural features. In the second
stage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned
style vector to guide generation. Our method supports style interpolation and
user personalization via lightweight mixing. Compared to prior work, our
unified framework offers improved stylistic control without sacrificing code
correctness. This is among the first approaches to combine contrastive
alignment with conditional decoding for style-guided code generation.

</details>


### [75] [BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs](https://arxiv.org/abs/2505.19457)
*Guilong Lu,Xuntao Guo,Rongjunchen Zhang,Wenqiao Zhu,Ji Liu*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在通用任务中表现出色，但在逻辑密集、精度关键的领域（如金融、法律和医疗保健）中的可靠性评估仍具有挑战性。为了解决这个问题，我们引入了BizFinBench，这是第一个专门用于评估LLMs在实际金融应用中的基准测试。BizFinBench由6,781个中文注释查询组成，涵盖了五个维度：数值计算、推理、信息提取、预测识别和基于知识的问题回答，并分为九个细分类别。该基准测试包括客观和主观指标。我们还介绍了IteraJudge，这是一种新颖的LLM评估方法，在使用LLMs作为客观指标的评估者时可以减少偏差。我们对25个模型进行了基准测试，包括专有系统和开源系统。广泛的实验表明，没有一个模型能够在所有任务中占据主导地位。我们的评估揭示了不同的能力模式：(1) 在数值计算方面，Claude-3.5-Sonnet（63.18）和DeepSeek-R1（64.04）领先，而较小的模型如Qwen2.5-VL-3B（15.92）则明显滞后；(2) 在推理方面，专有模型占主导地位（ChatGPT-o3：83.58，Gemini-2.0-Flash：81.15），开源模型落后多达19.49点；(3) 在信息提取方面，性能差距最大，DeepSeek-R1得分71.46，而Qwen3-1.7B得分11.23；(4) 在预测识别方面，性能差异最小，顶级模型得分在39.16到50.00之间。我们发现，尽管当前的LLMs能够胜任处理常规金融查询，但它们在需要跨概念推理的复杂场景中表现不佳。BizFinBench提供了一个严格、与业务一致的基准测试，以供未来研究。代码和数据集可在https://github.com/HiThink-Research/BizFinBench获得。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在逻辑密集、精度关键的领域（如金融、法律和医疗保健）中的可靠性仍然是一个挑战。因此，研究团队希望开发一种新的基准来解决这一问题，从而更准确地衡量这些模型的能力。

Method: 研究团队创建了一个名为BizFinBench的新基准，其中包含6,781个中文注释查询，涵盖五个维度和九个细分类别。此外，他们还提出了一种新方法IteraJudge，用于减少LLMs作为评估者时的偏差。通过对25个模型进行测试，比较了它们在不同任务中的表现。

Result: 实验结果表明，没有一个模型能够在所有任务中占据主导地位。不同的模型在不同任务上表现出显著的能力差异，例如数值计算、推理、信息提取和预测识别等。

Conclusion: 当前的大型语言模型虽然能够很好地处理常规金融查询，但在涉及复杂推理的任务中仍然存在困难。BizFinBench为未来的相关研究提供了一个严格的基准测试工具。

Abstract: Large language models excel in general tasks, yet assessing their reliability
in logic-heavy, precision-critical domains like finance, law, and healthcare
remains challenging. To address this, we introduce BizFinBench, the first
benchmark specifically designed to evaluate LLMs in real-world financial
applications. BizFinBench consists of 6,781 well-annotated queries in Chinese,
spanning five dimensions: numerical calculation, reasoning, information
extraction, prediction recognition, and knowledge-based question answering,
grouped into nine fine-grained categories. The benchmark includes both
objective and subjective metrics. We also introduce IteraJudge, a novel LLM
evaluation method that reduces bias when LLMs serve as evaluators in objective
metrics. We benchmark 25 models, including both proprietary and open-source
systems. Extensive experiments show that no model dominates across all tasks.
Our evaluation reveals distinct capability patterns: (1) In Numerical
Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while
smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,
proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with
open-source models trailing by up to 19.49 points; (3) In Information
Extraction, the performance spread is the largest, with DeepSeek-R1 scoring
71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,
performance variance is minimal, with top models scoring between 39.16 and
50.00. We find that while current LLMs handle routine finance queries
competently, they struggle with complex scenarios requiring cross-concept
reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future
research. The code and dataset are available at
https://github.com/HiThink-Research/BizFinBench.

</details>


### [76] [Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs](https://arxiv.org/abs/2505.19466)
*Hongyu Liang,Yuting Zheng,Yihan Li,Yiran Zhang,Shiyu Liang*

Main category: cs.AI

TL;DR: To tackle the issue of transparency in fine-tuned large language models (LLMs), this paper introduces Origin-Tracer, a novel detection method that can verify if a model is fine-tuned from a specified base model and extract the LoRA rank used during fine-tuning. Tested on 31 open-source models under real-world obfuscation conditions, the results show its effectiveness and potential to set new standards in model verification.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the growing concern over misleading claims about the origins of fine-tuned LLMs, which affects transparency and trust within the open-source community. Current model verification techniques face challenges when dealing with obfuscation methods.

Method: The proposed method, Origin-Tracer, rigorously determines whether a model has been fine-tuned from a specific base model. It includes the ability to extract the LoRA rank used in the fine-tuning process, providing a more robust framework for model verification.

Result: The empirical validation on thirty-one diverse open-source models under simulated real-world obfuscation scenarios demonstrates the effectiveness of the Origin-Tracer method. The analysis also discusses the limitations of the framework.

Conclusion: The results indicate the potential of Origin-Tracer to establish new benchmarks for model verification, enhancing transparency and trust in the field of fine-tuned LLMs.

Abstract: As large language models (LLMs) continue to advance, their deployment often
involves fine-tuning to enhance performance on specific downstream tasks.
However, this customization is sometimes accompanied by misleading claims about
the origins, raising significant concerns about transparency and trust within
the open-source community. Existing model verification techniques typically
assess functional, representational, and weight similarities. However, these
approaches often struggle against obfuscation techniques, such as permutations
and scaling transformations. To address this limitation, we propose a novel
detection method Origin-Tracer that rigorously determines whether a model has
been fine-tuned from a specified base model. This method includes the ability
to extract the LoRA rank utilized during the fine-tuning process, providing a
more robust verification framework. This framework is the first to provide a
formalized approach specifically aimed at pinpointing the sources of model
fine-tuning. We empirically validated our method on thirty-one diverse
open-source models under conditions that simulate real-world obfuscation
scenarios. We empirically analyze the effectiveness of our framework and
finally, discuss its limitations. The results demonstrate the effectiveness of
our approach and indicate its potential to establish new benchmarks for model
verification.

</details>


### [77] [Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models](https://arxiv.org/abs/2505.19474)
*Xinmiao Hu,Chun Wang,Ruihe An,ChenYu Shao,Xiaojun Ye,Sheng Zhou,Liangcheng Li*

Main category: cs.AI

TL;DR: Multimodal Large Language Models (MLLMs) suffer from object hallucinations due to dataset biases. This paper proposes a causality-driven disentanglement framework, including a Causal-Driven Projector and a Causal Intervention Module, to mitigate these hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address the issue of object hallucinations in MLLMs which generate descriptions of objects inconsistent with or absent from the input, closely related to dataset biases leading to entangled semantic representations across modalities.

Method: The proposed method includes a Causal-Driven Projector in the visual pathway and a Causal Intervention Module integrated into the final transformer layer of the language model to reduce spurious correlations caused by biased training data.

Result: Experimental results demonstrate that the method significantly reduces hallucinations while maintaining strong performance on multiple multimodal benchmarks. Visualization analyses confirm improved separability of object representations.

Conclusion: The causality-driven disentanglement framework effectively mitigates object hallucinations in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
in visual understanding tasks, yet they often suffer from object
hallucinations--generating descriptions of objects that are inconsistent with
or entirely absent from the input. This issue is closely related to dataset
biases, where frequent co-occurrences of objects lead to entangled semantic
representations across modalities. As a result, models may erroneously activate
object representations that are commonly associated with the input but not
actually present.
  To address this, we propose a causality-driven disentanglement framework that
mitigates hallucinations through causal intervention. Our approach includes a
Causal-Driven Projector in the visual pathway and a Causal Intervention Module
integrated into the final transformer layer of the language model. These
components work together to reduce spurious correlations caused by biased
training data.
  Experimental results show that our method significantly reduces
hallucinations while maintaining strong performance on multiple multimodal
benchmarks. Visualization analyses further confirm improved separability of
object representations.
  The code is available at: https://github.com/IgniSavium/Causal-LLaVA

</details>


### [78] [Judging with Many Minds: Do More Perspectives Mean Less Prejudice?](https://arxiv.org/abs/2505.19477)
*Chiyu Ma,Enpei Zhang,Yilun Zhao,Wenjun Liu,Yaning Jia,Peijun Qing,Lin Shi,Arman Cohan,Yujun Yan,Soroush Vosoughi*

Main category: cs.AI

TL;DR: LLM-as-Judge是一种可扩展的人类评估替代方案，但其在多智能体环境中的内在偏差尚未得到充分研究。本文系统分析了四种偏差类型在两种多智能体框架（多智能体辩论和LLM-as-Meta-Judge）中的表现，并探讨了单智能体去偏方法PINE在这些系统中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM-as-Judge在训练中提供了奖励信号，但在多智能体环境中，其内在偏差如何表现仍是一个未解决的问题。

Method: 对四种偏差类型进行系统分析，并在两种多智能体框架中评估这些偏差。同时，研究单智能体去偏方法PINE在这些系统中的应用效果。

Result: 研究发现，辩论框架在初始辩论后显著放大了偏差，并在后续轮次中持续存在；而元评审方法表现出更强的抵抗力。引入PINE作为无偏代理可以有效减少辩论设置中的偏差，但在元评审场景中效果较差。

Conclusion: 本研究全面分析了多智能体LLM-as-Judge系统中的偏差行为，强调了在协作评估环境中制定针对性偏差缓解策略的重要性。

Abstract: LLM-as-Judge has emerged as a scalable alternative to human evaluation,
enabling large language models (LLMs) to provide reward signals in trainings.
While recent work has explored multi-agent extensions such as multi-agent
debate and meta-judging to enhance evaluation quality, the question of how
intrinsic biases manifest in these settings remains underexplored. In this
study, we conduct a systematic analysis of four diverse bias types: position
bias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate
these biases across two widely adopted multi-agent LLM-as-Judge frameworks:
Multi-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate
framework amplifies biases sharply after the initial debate, and this increased
bias is sustained in subsequent rounds, while meta-judge approaches exhibit
greater resistance. We further investigate the incorporation of PINE, a leading
single-agent debiasing method, as a bias-free agent within these systems. The
results reveal that this bias-free agent effectively reduces biases in debate
settings but provides less benefit in meta-judge scenarios. Our work provides a
comprehensive study of bias behavior in multi-agent LLM-as-Judge systems and
highlights the need for targeted bias mitigation strategies in collaborative
evaluation settings.

</details>


### [79] [Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs](https://arxiv.org/abs/2505.19489)
*Zhenhao Zhou,Zhuochen Huang,Yike He,Chong Wang,Jiajun Wang,Yijian Wu,Xin Peng,Yiling Lou*

Main category: cs.AI

TL;DR: The paper introduces LinuxFLBench, a fault localization (FL) benchmark for real-world Linux kernel bugs, and proposes LinuxFL$^+$, an enhancement framework to improve FL effectiveness of LLM agents.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of state-of-the-art LLM agents on fault localization in the Linux kernel, where FL is much more challenging due to the large-scale code base, limited observability, and diverse impact factors.

Method: 1. Introduced LinuxFLBench, a FL benchmark constructed from real-world Linux kernel bugs.
2. Conducted an empirical study to evaluate the performance of state-of-the-art LLM agents on the Linux kernel.
3. Proposed LinuxFL$^+$, an enhancement framework designed to improve FL effectiveness of LLM agents for the Linux kernel.

Result: Existing LLM agents struggle with fault localization in the Linux kernel, achieving a best top-1 accuracy of only 41.6% at file level. LinuxFL$^+$ substantially improves the FL accuracy of all studied agents with minimal costs (e.g., 7.2% - 11.2% accuracy increase).

Conclusion: LinuxFLBench provides a valuable resource for evaluating FL methods in the Linux kernel, and LinuxFL$^+$ offers an effective solution to enhance FL effectiveness of LLM agents.

Abstract: The Linux kernel is a critical system, serving as the foundation for numerous
systems. Bugs in the Linux kernel can cause serious consequences, affecting
billions of users. Fault localization (FL), which aims at identifying the buggy
code elements in software, plays an essential role in software quality
assurance. While recent LLM agents have achieved promising accuracy in FL on
recent benchmarks like SWE-bench, it remains unclear how well these methods
perform in the Linux kernel, where FL is much more challenging due to the
large-scale code base, limited observability, and diverse impact factors. In
this paper, we introduce LinuxFLBench, a FL benchmark constructed from
real-world Linux kernel bugs. We conduct an empirical study to assess the
performance of state-of-the-art LLM agents on the Linux kernel. Our initial
results reveal that existing agents struggle with this task, achieving a best
top-1 accuracy of only 41.6% at file level. To address this challenge, we
propose LinuxFL$^+$, an enhancement framework designed to improve FL
effectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially
improves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy
increase) with minimal costs. Data and code are available at
https://github.com/FudanSELab/LinuxFLBench.

</details>


### [80] [Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models](https://arxiv.org/abs/2505.19490)
*Jianxing Liao,Junyan Xu,Yatao Sun,Maowen Tang,Sicheng He,Jingxian Liao,Shui Yu,Yun Li,Hongguan Xiao*

Main category: cs.AI

TL;DR: This paper presents a language-guided framework for industrial design automation that combines large language models with computer-automated design, introducing innovations in data annotation, CAD generation via Transformer, and an enhanced modeling generation model. The approach improves accuracy and efficiency in generating complex CAD models from textual prompts.


<details>
  <summary>Details</summary>
Motivation: Designing complex CAD models is time-consuming due to computational inefficiency and difficulty in precision, necessitating a more efficient automated solution.

Method: The method integrates LLMs with CAutoD, proposing three key innovations: a semi-automated data annotation pipeline using LLMs and VLLMs, a Transformer-based CAD generator (TCADGen) for predicting modeling sequences, and an enhanced CAD modeling generation model (CADLLM) for refining generated sequences.

Result: The experimental results show that the proposed approach surpasses traditional methods in both accuracy and efficiency.

Conclusion: The novel framework provides a powerful tool for automating industrial workflows and generating complex CAD models from textual prompts.

Abstract: Designing complex computer-aided design (CAD) models is often time-consuming
due to challenges such as computational inefficiency and the difficulty of
generating precise models. We propose a novel language-guided framework for
industrial design automation to address these issues, integrating large
language models (LLMs) with computer-automated design (CAutoD).Through this
framework, CAD models are automatically generated from parameters and
appearance descriptions, supporting the automation of design tasks during the
detailed CAD design phase. Our approach introduces three key innovations: (1) a
semi-automated data annotation pipeline that leverages LLMs and vision-language
large models (VLLMs) to generate high-quality parameters and appearance
descriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts
modeling sequences via dual-channel feature aggregation; (3) an enhanced CAD
modeling generation model, called CADLLM, that is designed to refine the
generated sequences by incorporating the confidence scores from TCADGen.
Experimental results demonstrate that the proposed approach outperforms
traditional methods in both accuracy and efficiency, providing a powerful tool
for automating industrial workflows and generating complex CAD models from
textual prompts. The code is available at
https://jianxliao.github.io/cadllm-page/

</details>


### [81] [Genome-Bench: A Scientific Reasoning Benchmark from Real-World Expert Discussions](https://arxiv.org/abs/2505.19501)
*Ming Yin,Yuanhao Qu,Dyllan Liu,Ling Yang,Le Cong,Mengdi Wang*

Main category: cs.AI

TL;DR: 本报告提出了一种针对基因组学领域的自动化管道，并介绍了新的基准Genome-Bench，该基准由十多年关于基因组工程的科学论坛讨论构建。管道将原始交互转换为强化学习友好的多项选择题格式，包含3000多个高质量的问题答案对，涵盖了基础生物学、实验故障排除、工具使用等。这是第一个用于教导大型语言模型从科学讨论中推理的端到端管道，具有超越生物学领域的广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为了创建一个可以从科学讨论中学习推理的大型语言模型（LLM），并提供一个专门针对基因组学领域的自动化管道和基准数据集。

Method: 开发了一个自动化管道，将来自科学论坛的讨论转化为适合强化学习的多项选择题格式，并创建了一个名为Genome-Bench的新基准，其中包含超过3000个高质量问题答案对。

Result: 生成了一个新基准Genome-Bench，能够支持大型语言模型在基因组学领域内的推理训练，并显示出在其他科学领域中的潜在应用。

Conclusion: 这是首个针对基因组学领域设计的端到端管道，可有效提升LLM基于科学讨论进行推理的能力，并有望推广到其他科学领域。

Abstract: In this short report, we present an automated pipeline tailored for the
genomics domain and introduce \textit{Genome-Bench}, a new benchmark
constructed from over a decade of scientific forum discussions on genome
engineering. Our pipeline transforms raw interactions into a reinforcement
learning friendly multiple-choice questions format, supported by 3000+ high
quality question answer pairs spanning foundational biology, experimental
troubleshooting, tool usage, and beyond. To our knowledge, this is the first
end-to-end pipeline for teaching LLMs to reason from scientific discussions,
with promising potential for generalization across scientific domains beyond
biology.

</details>


### [82] [Turing Test 2.0: The General Intelligence Threshold](https://arxiv.org/abs/2505.19550)
*Georgios Mappouras*

Main category: cs.AI

TL;DR: The paper discusses the insufficiency of traditional methods like the Turing test in detecting AGI and introduces a new framework called Turing Tests 2.0, which includes a clear definition for general intelligence (GI) and a GI threshold (GIT) to distinguish between systems that achieve AGI and those that do not.


<details>
  <summary>Details</summary>
Motivation: With the rise of AI and large language models, there is an increasing interest in achieving artificial general intelligence (AGI). However, there is no clear agreement on how to detect AGI using current tools such as the Turing test.

Method: The authors first define general intelligence (GI) and establish a GI threshold (GIT) to differentiate between AGI-achieving systems and others. They then introduce a new framework, Turing Tests 2.0, to construct tests that can clearly determine if a system has achieved GI through a fail/pass method.

Result: The paper presents real-life examples of applying the Turing Tests 2.0 framework on modern AI models, demonstrating its practicality and effectiveness.

Conclusion: Turing Tests 2.0 provides a practical method to measure and detect AGI, advancing beyond traditional methods like the Turing test.

Abstract: With the rise of artificial intelligence (A.I.) and large language models
like Chat-GPT, a new race for achieving artificial general intelligence (A.G.I)
has started. While many speculate how and when A.I. will achieve A.G.I., there
is no clear agreement on how A.G.I. can be detected in A.I. models, even when
popular tools like the Turing test (and its modern variations) are used to
measure their intelligence. In this work, we discuss why traditional methods
like the Turing test do not suffice for measuring or detecting A.G.I. and
provide a new, practical method that can be used to decide if a (computer or
any other) system has reached or surpassed A.G.I. To achieve this, we make two
new contributions. First, we present a clear definition for general
intelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to
distinguish between systems that achieve A.G.I. and systems that do not.
Second, we present a new framework on how to construct tests that can detect if
a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass
way. We call this novel framework the Turing Tests 2.0. We then demonstrate
real-life examples of applying tests that follow our Turing Tests 2.0 framework
on modern A.I. models.

</details>


### [83] [AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare](https://arxiv.org/abs/2505.19562)
*Ying Xiao,Jie Huang,Ruijuan He,Jing Xiao,Mohammad Reza Mousavi,Yepang Liu,Kezhi Li,Zhenpeng Chen,Jie M. Zhang*

Main category: cs.AI

TL;DR: Large language models (LLMs) are reaching expert-level accuracy on medical diagnosis questions, but they still have mistakes and biases that can cause serious risks. To solve this problem, the paper introduces AMQA, an Adversarial Medical Question-Answering dataset for evaluating bias in LLMs medical QA.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of a consistent and automatic testbed for measuring bias in large language models when answering medical diagnosis questions.

Method: This paper presents AMQA -- an Adversarial Medical Question-Answering dataset -- built for automated, large-scale bias evaluation of LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the United States Medical Licensing Examination (USMLE) dataset, generated using a multi-agent framework to create diverse adversarial descriptions and question pairs.

Result: Using AMQA, the researchers benchmarked five representative LLMs and found substantial disparities: even GPT-4.1, the least biased model tested, answers privileged-group questions over 10 percentage points more accurately than unprivileged ones.

Conclusion: AMQA reveals larger accuracy gaps on average between privileged and unprivileged groups compared with the existing benchmark CPV. The dataset and code are publicly available to support reproducible research and advance trustworthy, bias-aware medical AI.

Abstract: Large language models (LLMs) are reaching expert-level accuracy on medical
diagnosis questions, yet their mistakes and the biases behind them pose
life-critical risks. Bias linked to race, sex, and socioeconomic status is
already well known, but a consistent and automatic testbed for measuring it is
missing. To fill this gap, this paper presents AMQA -- an Adversarial Medical
Question-Answering dataset -- built for automated, large-scale bias evaluation
of LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the
United States Medical Licensing Examination (USMLE) dataset, generated using a
multi-agent framework to create diverse adversarial descriptions and question
pairs. Using AMQA, we benchmark five representative LLMs and find surprisingly
substantial disparities: even GPT-4.1, the least biased model tested, answers
privileged-group questions over 10 percentage points more accurately than
unprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%
larger accuracy gaps on average between privileged and unprivileged groups. Our
dataset and code are publicly available at https://github.com/XY-Showing/AMQA
to support reproducible research and advance trustworthy, bias-aware medical
AI.

</details>


### [84] [Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights](https://arxiv.org/abs/2505.19563)
*Shi-Yu Tian,Zhi Zhou,Wei Dong,Ming Yang,Kun-Yang Yu,Zi-Jian Cheng,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.AI

TL;DR: Reasoning with tabular data is important in modern applications, but evaluation methods for table-based QA tasks are still developing. Current research faces two main challenges: expensive manual annotation of real-world data and diverse table structures that make it hard to analyze why LLMs underperform in reasoning-intensive tasks. To solve these problems, the paper introduces AutoT2T, an automated pipeline that converts math word problems into table-based reasoning tasks without needing manual annotation. It also generates different table versions for robustness testing. Based on this, they built a new benchmark called TabularGSM that covers various table complexities and tricky problems. Experiments show that the close connection between reasoning and retrieval/identification processes is a major reason why LLMs fail in complex table QA tasks, suggesting that models need better integrated reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to improve the evaluation methodologies for reasoning-intensive Table QA tasks by addressing two key issues: the high cost of manually annotated data and the difficulty in systematically analyzing LLMs' underperformance due to diverse table structures.

Method: The method involves creating an automated generation pipeline named AutoT2T that transforms mathematical word problems into table-based reasoning tasks. This eliminates the need for manual annotation and allows for the creation of multiple table variants, including noisy ones for robustness testing. Additionally, a new benchmark called TabularGSM is constructed based on AutoT2T.

Result: The experimental results indicate that the strong link between reasoning and retrieval or identification processes is a significant factor contributing to LLMs' failures in complex Table QA tasks. This finding underscores the importance of developing models with synergistic reasoning capabilities.

Conclusion: The conclusion is that current LLMs struggle with complex Table QA tasks due to the tight coupling between reasoning and other processes like retrieval or identification. To improve performance in such tasks, future models should focus on integrating synergistic reasoning abilities.

Abstract: Reasoning with tabular data holds increasing importance in modern
applications, yet comprehensive evaluation methodologies for
reasoning-intensive Table Question Answering (QA) tasks remain nascent.
Existing research is constrained by two primary bottlenecks: 1) Reliance on
costly manually annotated real-world data, which is difficult to cover complex
reasoning scenarios; 2) The heterogeneity of table structures hinders
systematic analysis of the intrinsic mechanisms behind the underperformance of
LLMs, especially in reasoning-intensive tasks. To address these issues, we
propose an automated generation pipeline AutoT2T that transforms mathematical
word problems into table-based reasoning tasks, eliminating the need for manual
annotation. The pipeline can generate multiple variants of a table for the same
reasoning problem, including noisy versions to support robustness evaluation.
Based on this, we construct a new benchmark TabularGSM, which systematically
spans a range of table complexities and trap problems. Experimental analyses
through AutoT2T and TabularGSM reveal that the tight coupling between reasoning
and retrieval or identification processes is a key factor underlying the
failure of LLMs in complex Table QA tasks. This highlights the necessity for
models to develop synergistic reasoning capabilities in order to perform
effectively in complex Table QA tasks.

</details>


### [85] [LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer](https://arxiv.org/abs/2505.19567)
*Rasoul Zahedifar,Sayyed Ali Mirghasemi,Mahdieh Soleymani Baghshah,Alireza Taheri*

Main category: cs.AI

TL;DR: This study introduces the LLM-Agent-Controller, a multi-agent system using large language models to solve problems in control engineering. It incorporates specialized agents, advanced capabilities like RAG and Chain-of-Thought reasoning, and a supervisor for high-level coordination. The system requires no prior knowledge of Control Theory from users and provides real-time solutions. Evaluated through new performance metrics and qualitative analysis, it successfully solved 83% of general tasks with individual agents achieving an average success rate of 87%. Performance improved with more advanced LLMs.


<details>
  <summary>Details</summary>
Motivation: To develop a multi-agent large language model system capable of addressing a wide range of problems in control engineering without requiring users to have prior knowledge of Control Theory.

Method: The LLM-Agent-Controller integrates a central controller agent with multiple specialized auxiliary agents responsible for various tasks related to control engineering. A supervisor oversees high-level decision-making and workflow coordination. The system includes capabilities such as Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning, self-criticism and correction, efficient memory handling, and natural language communication.

Result: The LLM-Agent-Controller successfully solved 83% of general tasks across five categories of Control Theory problems. Individual agents achieved an average success rate of 87%. Performance was found to improve with more advanced LLMs.

Conclusion: The research demonstrates the potential of multi-agent LLM architectures in solving complex, domain-specific problems. By integrating specialized agents, supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a scalable, robust, and accessible solution framework that can be extended to various technical domains.

Abstract: This study presents the LLM-Agent-Controller, a multi-agent large language
model (LLM) system developed to address a wide range of problems in control
engineering (Control Theory). The system integrates a central controller agent
with multiple specialized auxiliary agents, responsible for tasks such as
controller design, model representation, control analysis, time-domain
response, and simulation. A supervisor oversees high-level decision-making and
workflow coordination, enhancing the system's reliability and efficiency. The
LLM-Agent-Controller incorporates advanced capabilities, including
Retrieval-Augmented Generation (RAG), Chain-of-Thought reasoning,
self-criticism and correction, efficient memory handling, and user-friendly
natural language communication. It is designed to function without requiring
users to have prior knowledge of Control Theory, enabling them to input
problems in plain language and receive complete, real-time solutions. To
evaluate the system, we propose new performance metrics assessing both
individual agents and the system as a whole. We test five categories of Control
Theory problems and benchmark performance across three advanced LLMs.
Additionally, we conduct a comprehensive qualitative conversational analysis
covering all key services. Results show that the LLM-Agent-Controller
successfully solved 83% of general tasks, with individual agents achieving an
average success rate of 87%. Performance improved with more advanced LLMs. This
research demonstrates the potential of multi-agent LLM architectures to solve
complex, domain-specific problems. By integrating specialized agents,
supervisory control, and advanced reasoning, the LLM-Agent-Controller offers a
scalable, robust, and accessible solution framework that can be extended to
various technical domains.

</details>


### [86] [MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model](https://arxiv.org/abs/2505.19568)
*Jiongchao Jin,Xiuju Fu,Xiaowei Gao,Tao Cheng,Ran Yan*

Main category: cs.AI

TL;DR: The paper proposes MSD-LLM for predicting ship detention during PSC inspections, which combines a DSR-layer based autoencoder with a large language model to handle data imbalance and extract meaningful features. It outperforms existing methods by over 12% in AUC for Singapore ports.


<details>
  <summary>Details</summary>
Motivation: Maritime transportation is crucial for global trade, making ship inspection important for safety and environmental protection. Traditional machine learning methods have low accuracy for ship detention prediction, while autoencoder-based deep learning approaches struggle with imbalanced data.

Method: MSD-LLM integrates a dual robust subspace recovery (DSR) layer-based autoencoder with a progressive learning pipeline to address data imbalance and extract meaningful PSC representations. Then, a large language model groups and ranks features for detention prediction with dynamic thresholding.

Result: Evaluations on 31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM surpasses state-of-the-art methods by more than 12% in AUC for Singapore ports. It also demonstrates robustness to real-world challenges.

Conclusion: MSD-LLM effectively handles imbalanced data and provides accurate ship detention predictions, making it adaptable to diverse maritime risk assessment scenarios.

Abstract: Maritime transportation is the backbone of global trade, making ship
inspection essential for ensuring maritime safety and environmental protection.
Port State Control (PSC), conducted by national ports, enforces compliance with
safety regulations, with ship detention being the most severe consequence,
impacting both ship schedules and company reputations. Traditional machine
learning methods for ship detention prediction are limited by the capacity of
representation learning and thus suffer from low accuracy. Meanwhile,
autoencoder-based deep learning approaches face challenges due to the severe
data imbalance in learning historical PSC detention records. To address these
limitations, we propose Maritime Ship Detention with Large Language Models
(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based
autoencoder with a progressive learning pipeline to handle imbalanced data and
extract meaningful PSC representations. Then, a large language model groups and
ranks features to identify likely detention cases, enabling dynamic
thresholding for flexible detention predictions. Extensive evaluations on
31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM
outperforms state-of-the-art methods more than 12\% on Area Under the Curve
(AUC) for Singapore ports. Additionally, it demonstrates robustness to
real-world challenges, making it adaptable to diverse maritime risk assessment
scenarios.

</details>


### [87] [Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621)
*George Kour,Itay Nakash,Ateret Anaby-Tavor,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: This paper introduces POBs, a benchmark for assessing subjective tendencies in LLMs, finding that reasoning and self-reflection mechanisms have limited effect, and newer model versions are less consistent and more biased.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether and to what extent Large Language Models exhibit subjective preferences, opinions, and beliefs which may influence decision-making and advice given to users.

Method: Developed the Preference, Opinion, and Belief survey (POBs) benchmark to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. Evaluated leading open- and closed-source LLMs using this benchmark, and investigated the effect of increasing test-time compute through reasoning and self-reflection mechanisms.

Result: Reasoning and self-reflection mechanisms offer only limited gains in reducing subjectivity. Newer model versions are becoming less consistent and more biased towards specific viewpoints.

Conclusion: The study highlights a concerning trend in LLMs becoming less consistent and more biased, pointing out a blind spot in their development.

Abstract: As Large Language Models (LLMs) become deeply integrated into human life and
increasingly influence decision-making, it's crucial to evaluate whether and to
what extent they exhibit subjective preferences, opinions, and beliefs. These
tendencies may stem from biases within the models, which may shape their
behavior, influence the advice and recommendations they offer to users, and
potentially reinforce certain viewpoints. This paper presents the Preference,
Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs'
subjective inclinations across societal, cultural, ethical, and personal
domains. We applied our benchmark to evaluate leading open- and closed-source
LLMs, measuring desired properties such as reliability, neutrality, and
consistency. In addition, we investigated the effect of increasing the
test-time compute, through reasoning and self-reflection mechanisms, on those
metrics. While effective in other tasks, our results show that these mechanisms
offer only limited gains in our domain. Furthermore, we reveal that newer model
versions are becoming less consistent and more biased toward specific
viewpoints, highlighting a blind spot and a concerning trend. POBS:
https://ibm.github.io/POBS

</details>


### [88] [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
*Junteng Liu,Yuanxiang Fan,Zhuo Jiang,Han Ding,Yongyi Hu,Chi Zhang,Yiqi Shi,Shitong Weng,Aili Chen,Shiqi Chen,Yunan Huang,Mozhi Zhang,Pengyu Zhao,Junjie Yan,Junxian He*

Main category: cs.AI

TL;DR: SynLogic is a data synthesis framework and dataset that generates diverse logical reasoning data for Reinforcement Learning to enhance Large Language Models' reasoning abilities. Experiments show it improves logical reasoning performance and reasoning generalization when mixed with other tasks.


<details>
  <summary>Details</summary>
Motivation: Open-source replication efforts have mainly focused on mathematical and coding domains, while methods and resources for developing general reasoning capabilities in LLMs remain underexplored due to the challenge of collecting suitable data.

Method: The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity across 35 diverse logical reasoning tasks. All examples can be verified by simple rules, making them suited for RL with verifiable rewards.

Result: RL training on the SynLogic dataset based on 7B and 32B models leads to state-of-the-art logical reasoning performance among open-source datasets. Mixing SynLogic data with mathematical and coding tasks improves training efficiency and enhances reasoning generalization.

Conclusion: SynLogic is a valuable resource for advancing the broader reasoning capabilities of LLMs. The data synthesis pipeline and dataset are open-sourced.

Abstract: Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.

</details>


### [89] [Token-Importance Guided Direct Preference Optimization](https://arxiv.org/abs/2505.19653)
*Yang Ning,Lin Hai,Liu Yibo,Tian Baoliang,Liu Guoqing,Zhang Haijun*

Main category: cs.AI

TL;DR: The paper proposes Token-Importance Guided Direct Preference Optimization (TI-DPO), which introduces gradient-based token-importance weights and a triple loss for better alignment with human preferences in large language models. This leads to higher accuracy, stronger generative diversity, and more stable/computationally efficient solutions compared to DPO and other RLHF methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring large language models generate outputs aligned with human preferences is crucial for safe AI interactions. Current methods like DPO have limitations such as overlooking differential importance of tokens and being sensitive to judgment noise.

Method: TI-DPO is proposed with two key innovations: gradient-based token-importance weights that prioritize critical tokens dynamically, and a triple loss that guides model outputs towards human-preferred responses and away from non-preferred ones.

Result: Experimental results indicate TI-DPO achieves higher accuracy and stronger generative diversity, while providing more stable and computationally efficient solutions compared to existing methods.

Conclusion: TI-DPO addresses the shortcomings of current methods by incorporating token-importance guidance and a triple loss, leading to improved performance in terms of accuracy, generative diversity, stability, and computational efficiency.

Abstract: Ensuring that large language models (LLMs) generate outputs aligned with
human preferences is important for safe and effective AI interactions. While
Direct Preference Optimization (DPO) employs an implicit reward function to
optimize the policy model, however, it and its related variants overlook the
differential importance of individual tokens and are sensitive to judgment
noise in preference datasets during generation. Although recent methods attempt
to assess the important weight of tokens via probability prediction or
simplistic weighting schemes, these evaluation methods are prone to biases and
still cannot fully address these issues. To solve this problem, we propose the
Token-Importance Guided Direct Preference Optimization (TI-DPO), which
introduces two key innovations: the gradient-based token-importance weights
that dynamically prioritize critical tokens, and a triple loss that explicitly
guides model outputs to approach human-preferred responses and stay away from
non-preferred responses. Experimental results show that TI-DPO achieves higher
accuracy and stronger generative diversity, providing more stable and
computationally efficient solutions compared with DPO and other RLHF methods.

</details>


### [90] [FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks](https://arxiv.org/abs/2505.19662)
*Atsunori Moteki,Shoichi Masui,Fan Yang,Yueqi Song,Yonatan Bisk,Graham Neubig,Ikuo Kusajima,Yasuto Watanabe,Hiroyuki Ishida,Jun Takahashi,Shan Jiang*

Main category: cs.AI

TL;DR: This paper proposes FieldWorkArena, a benchmark for agentic AI in real-world field work, including a new action space and improved evaluation function. The dataset is based on real videos/documents and interviews with workers/managers.


<details>
  <summary>Details</summary>
Motivation: Existing agentic AI benchmarks are insufficient for evaluating agents in real-world work environments where complexity increases significantly.

Method: Define a new action space that agentic AI should possess for real world work environment benchmarks and improve the evaluation function from previous methods to assess the performance of agentic AI in diverse real-world tasks.

Result: Evaluation results confirmed that performance evaluation considering the characteristics of Multimodal LLM (MLLM) such as GPT-4o is feasible. Additionally, the effectiveness and limitations of the proposed new evaluation method were identified.

Conclusion: FieldWorkArena provides a comprehensive benchmark for agentic AI in real-world field work with an improved evaluation function.

Abstract: This paper proposes FieldWorkArena, a benchmark for agentic AI targeting
real-world field work. With the recent increase in demand for agentic AI, they
are required to monitor and report safety and health incidents, as well as
manufacturing-related incidents, that may occur in real-world work
environments. Existing agentic AI benchmarks have been limited to evaluating
web tasks and are insufficient for evaluating agents in real-world work
environments, where complexity increases significantly. In this paper, we
define a new action space that agentic AI should possess for real world work
environment benchmarks and improve the evaluation function from previous
methods to assess the performance of agentic AI in diverse real-world tasks.
The dataset consists of videos captured on-site and documents actually used in
factories and warehouses, and tasks were created based on interviews with
on-site workers and managers. Evaluation results confirmed that performance
evaluation considering the characteristics of Multimodal LLM (MLLM) such as
GPT-4o is feasible. Additionally, the effectiveness and limitations of the
proposed new evaluation method were identified. The complete dataset
(HuggingFace) and evaluation program (GitHub) can be downloaded from the
following website:
https://en-documents.research.global.fujitsu.com/fieldworkarena/.

</details>


### [91] [Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models](https://arxiv.org/abs/2505.19676)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: The paper explores the reasoning abilities of LLMs using ATP strategies, finding that advancements have stagnated over nine months and improvements are mainly due to system prompts or training in Chain of Thought prompting. Bottom-up reasoning is the most effective ATP strategy for LLMs, and there's a weak correlation between correct reasoning and correct conclusions.


<details>
  <summary>Details</summary>
Motivation: To evaluate the progress of LLMs in adopting Automated Theorem Prover (ATP) reasoning strategies and assess their reasoning capabilities over a specific timeframe.

Method: Assessing the performance of cutting-edge LLMs from December 2023 and August 2024 on PRONTOQA steamroller reasoning problems by developing methods to measure response accuracy and answer correctness correlation. Tracking completion tokens to understand reasoning ability improvements.

Result: LLM reasoning advancement has stalled over nine months; improvements since GPT-4 are largely due to hidden system prompts or training in Chain of Thought strategies. Bottom-up reasoning is most effective among ATP strategies, with only a weak correlation between correct reasoning and correct conclusions.

Conclusion: Current LLMs' reasoning capabilities have not significantly improved recently, relying heavily on certain prompting strategies, and their effectiveness with ATP strategies varies.

Abstract: Empirical methods to examine the capability of Large Language Models (LLMs)
to use Automated Theorem Prover (ATP) reasoning strategies are studied. We
evaluate the performance of State of the Art models from December 2023 and
August 2024 on PRONTOQA steamroller reasoning problems. For that, we develop
methods for assessing LLM response accuracy and correct answer correlation.
  Our results show that progress in improving LLM reasoning abilities has
stalled over the nine month period. By tracking completion tokens, we show that
almost all improvement in reasoning ability since GPT-4 was released can be
attributed to either hidden system prompts or the training of models to
automatically use generic Chain of Thought prompting strategies. Among the ATP
reasoning strategies tried, we found that current frontier LLMs are best able
to follow the bottom-up (also known as forward-chaining) strategy. A low
positive correlation was found between an LLM response containing correct
reasoning and arriving at the correct conclusion.

</details>


### [92] [Large Language Models for Planning: A Comprehensive and Systematic Survey](https://arxiv.org/abs/2505.19683)
*Pengfei Cao,Tianyi Men,Wencan Liu,Jingwen Zhang,Xuzhao Li,Xixun Lin,Dianbo Sui,Yanan Cao,Kang Liu,Jun Zhao*

Main category: cs.AI

TL;DR: 这篇论文全面回顾了基于大语言模型（LLM）的规划方法，包括理论基础、三种主要方法（外部模块增强法、微调法和搜索法）、评估框架及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在某些规划任务上表现出色，但其在规划领域的广泛应用需要系统性的研究。

Method: 通过引入理论基础、对当代LLM规划方法进行分类分析（分为外部模块增强法、微调法和搜索法），并总结现有的评估框架。

Result: 提供了关于LLM在规划领域应用的详尽综述，为未来的创新和进步提供了宝贵的资源。

Conclusion: 本调查希望成为激发创新和推动该领域进步的宝贵资源。

Abstract: Planning represents a fundamental capability of intelligent agents, requiring
comprehensive environmental understanding, rigorous logical reasoning, and
effective sequential decision-making. While Large Language Models (LLMs) have
demonstrated remarkable performance on certain planning tasks, their broader
application in this domain warrants systematic investigation. This paper
presents a comprehensive review of LLM-based planning. Specifically, this
survey is structured as follows: First, we establish the theoretical
foundations by introducing essential definitions and categories about automated
planning. Next, we provide a detailed taxonomy and analysis of contemporary
LLM-based planning methodologies, categorizing them into three principal
approaches: 1) External Module Augmented Methods that combine LLMs with
additional components for planning, 2) Finetuning-based Methods that involve
using trajectory data and feedback signals to adjust LLMs in order to improve
their planning abilities, and 3) Searching-based Methods that break down
complex tasks into simpler components, navigate the planning space, or enhance
decoding strategies to find the best solutions. Subsequently, we systematically
summarize existing evaluation frameworks, including benchmark datasets,
evaluation metrics and performance comparisons between representative planning
methods. Finally, we discuss the underlying mechanisms enabling LLM-based
planning and outline promising research directions for this rapidly evolving
field. We hope this survey will serve as a valuable resource to inspire
innovation and drive progress in this field.

</details>


### [93] [Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models](https://arxiv.org/abs/2505.19690)
*Baihui Zheng,Boren Zheng,Kerui Cao,Yingshui Tan,Zhendong Liu,Weixun Wang,Jiaheng Liu,Jian Yang,Wenbo Su,Xiaoyong Zhu,Bo Zheng,Kaifu Zhang*

Main category: cs.AI

TL;DR: 尽管大型推理模型（LRMs）在处理复杂推理任务方面表现出色，但在关键安全场景中的可靠性仍不确定。本文提出了一种新的基准测试Beyond Safe Answers (BSA)，以系统地研究表面安全对齐（SSA）问题，并评估了19个最先进的LRMs，结果表明其正确识别风险理由的准确率仅为38.0%。此外，还探讨了安全规则、专门的安全推理数据微调和不同的解码策略对缓解SSA的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估主要关注响应级别的安全性，而忽视了一个关键问题：表面安全对齐（SSA）。即模型虽然生成表面上安全的输出，但内部推理过程未能真正检测和缓解潜在风险，导致多次采样尝试中安全行为不一致。

Method: 引入了一个名为Beyond Safe Answers (BSA)的新基准，包含2000个挑战实例，分为三种不同的SSA情景类型并涵盖九个风险类别，每个实例都详细注释了风险理由。通过该基准评估了19个最先进的LRMs，并探索了安全规则、专门的安全推理数据微调和不同解码策略在缓解SSA方面的有效性。

Result: 顶级LRMs在正确识别风险理由方面的准确率仅为38.0%，证明了BSA基准的难度。同时发现安全规则、专门微调和解码策略能在一定程度上缓解SSA问题。

Conclusion: 本文提供的BSA基准为评估和改进LRMs的安全推理保真度提供了一个全面的工具，推动了真正具有风险意识且可靠安全的AI系统的开发。

Abstract: Despite the remarkable proficiency of \textit{Large Reasoning Models} (LRMs)
in handling complex reasoning tasks, their reliability in safety-critical
scenarios remains uncertain. Existing evaluations primarily assess
response-level safety, neglecting a critical issue we identify as
\textbf{\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where
models produce superficially safe outputs while internal reasoning processes
fail to genuinely detect and mitigate underlying risks, resulting in
inconsistent safety behaviors across multiple sampling attempts. To
systematically investigate SSA, we introduce \textbf{Beyond Safe Answers (BSA)}
bench, a novel benchmark comprising 2,000 challenging instances organized into
three distinct SSA scenario types and spanning nine risk categories, each
meticulously annotated with risk rationales. Evaluations of 19 state-of-the-art
LRMs demonstrate the difficulty of this benchmark, with top-performing models
achieving only 38.0\% accuracy in correctly identifying risk rationales. We
further explore the efficacy of safety rules, specialized fine-tuning on safety
reasoning data, and diverse decoding strategies in mitigating SSA. Our work
provides a comprehensive assessment tool for evaluating and improving safety
reasoning fidelity in LRMs, advancing the development of genuinely risk-aware
and reliably safe AI systems.

</details>


### [94] [Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting](https://arxiv.org/abs/2505.19716)
*Yifan Wu,Jingze Shi,Bingheng Wu,Jiayi Zhang,Xiaotian Lin,Nan Tang,Yuyu Luo*

Main category: cs.AI

TL;DR: 提出了一种名为difficulty-aware prompting (DAP)的新方法，能够动态缩短推理链长度而不损失性能，并基于此生成了一个精简的推理数据集LiteCoT和新的推理模型系列Liter。实验表明，使用短推理链在降低成本的同时能达到或超越长推理链的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的chain-of-thought (CoT)蒸馏方法虽然能有效转移推理能力到基础模型，但存在推理链过于冗长以及对问题难度适应性不足的问题，这导致了高昂的推理成本和不灵活的推理策略。

Method: 首先通过一个大型教师模型判断每个问题的难度，然后将推理链重写为适当的较短长度，从而生成简洁而完整的推理链。基于此方法构建了一个包含10万简洁推理样本的数据集LiteCoT，平均每个解决方案仅720个token。接着基于Qwen2.5架构蒸馏出一系列新的推理模型Liter（1.5B、7B和32B参数规模）。

Result: 实验表明，仅用10万经过难度修剪的CoT样本微调的学生模型优于用80万原始长CoT样本蒸馏的模型，同时显著降低了训练和推理成本。此外，在11个不同基准测试中，较短的difficulty-aware CoTs能达到与长链条相同的或更好的准确性，同时使用更少的token。例如，在AIME24考试中，该方法以约5K推理token达到了74.2%的Pass@1成绩，超越其他消耗更多token的方法。

Conclusion: 所提出的difficulty-aware prompting方法不仅能够有效缩短推理链长度，还能保持甚至提升模型性能，大幅降低训练和推理成本，具有广泛的应用潜力。

Abstract: Existing chain-of-thought (CoT) distillation methods can effectively transfer
reasoning abilities to base models but suffer from two major limitations:
excessive verbosity of reasoning traces and inadequate adaptability to problem
difficulty. Long reasoning traces significantly increase inference costs, and
uniform-length solutions prevent base models from learning adaptive reasoning
strategies. To address these issues, we propose a difficulty-aware prompting
(DAP) method to dynamically shorten reasoning traces without performance loss.
In our approach, a large teacher model first judges each problem's difficulty
and then rewrites its reasoning traces to an appropriate shorter length,
yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we
curate a distilled dataset called LiteCoT consisting of 100K concise reasoning
examples, with solutions averaging only 720 tokens (an order of magnitude
shorter than typical CoTs). Using LiteCoT, we distilled a new family of
reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5
architecture. Experiments show that a student model fine-tuned on just 100K of
these difficulty-pruned CoT samples outperforms a model distilled on 800K
original Long CoT samples, while significantly reducing training and inference
costs. Our method also generalizes well: across 11 diverse benchmarks, the
shorter difficulty-aware CoTs achieve equal or better accuracy than Long
chains, using far fewer tokens. For example, on the challenging AIME24 exam,
our approach reaches $74.2\%$ Pass@1 using only about 5K inference tokens,
surpassing other methods that consume many more tokens. Our code and data are
available at https://github.com/Evanwu1125/LiteCoT.

</details>


### [95] [ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection](https://arxiv.org/abs/2505.19734)
*Juxin Niu,Xiangfeng Liu,Dan Niu,Xi Wang,Zhe Jiang,Nan Guan*

Main category: cs.AI

TL;DR: This paper proposes ReChisel, an LLM-based system for generating Chisel code with a reflection mechanism to refine code quality and an escape mechanism to break non-progress loops.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to explore the potential of using LLMs for Chisel code generation, as previous efforts have mainly focused on traditional HDL Verilog.

Method: ReChisel incorporates a reflection mechanism to iteratively refine the quality of generated code using feedback from compilation and simulation processes, and introduces an escape mechanism to break free from non-progress loops.

Result: Experiments demonstrate that ReChisel significantly improves the success rate of Chisel code generation.

Conclusion: ReChisel achieves performance comparable to state-of-the-art LLM-based agentic systems for Verilog code generation.

Abstract: Coding with hardware description languages (HDLs) such as Verilog is a
time-intensive and laborious task. With the rapid advancement of large language
models (LLMs), there is increasing interest in applying LLMs to assist with HDL
coding. Recent efforts have demonstrated the potential of LLMs in translating
natural language to traditional HDL Verilog. Chisel, a next-generation HDL
based on Scala, introduces higher-level abstractions, facilitating more
concise, maintainable, and scalable hardware designs. However, the potential of
using LLMs for Chisel code generation remains largely unexplored. This work
proposes ReChisel, an LLM-based agentic system designed to enhance the
effectiveness of Chisel code generation. ReChisel incorporates a reflection
mechanism to iteratively refine the quality of generated code using feedback
from compilation and simulation processes, and introduces an escape mechanism
to break free from non-progress loops. Experiments demonstrate that ReChisel
significantly improves the success rate of Chisel code generation, achieving
performance comparable to state-of-the-art LLM-based agentic systems for
Verilog code generation.

</details>


### [96] [Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2505.19761)
*Zican Hu,Wei Liu,Xiaoye Qu,Xiangyu Yue,Chunlin Chen,Zhi Wang,Yu Cheng*

Main category: cs.AI

TL;DR: GLIDER is a framework that enhances large language models' decision-making abilities through hierarchical reinforcement learning, showing performance gains in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models have difficulty with long-term decision-making, particularly in environments with sparse rewards. This calls for better exploration strategies and credit assignment over longer time horizons.

Method: The GLIDER framework introduces a hierarchical structure to LLMs using offline hierarchical reinforcement learning. High-level policies create abstract plans which guide low-level controllers executing specific actions, breaking complex tasks into simpler sub-tasks.

Result: GLIDER shows consistent performance improvements on ScienceWorld and ALFWorld benchmarks, along with better generalization capabilities in non-stationary environments.

Conclusion: By incorporating hierarchical reinforcement learning, GLIDER improves large language models' ability to handle long-horizon decision-making tasks.

Abstract: While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in sparse-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.

</details>


### [97] [Language Model-Enhanced Message Passing for Heterophilic Graph Learning](https://arxiv.org/abs/2505.19762)
*Wenjun Wang,Dawei Cheng*

Main category: cs.AI

TL;DR: The paper proposes LEMP4HG, a language model-enhanced message passing approach for heterophilic graph learning that excels on heterophilic graphs and performs robustly on homophilic ones.


<details>
  <summary>Details</summary>
Motivation: Traditional graph neural networks struggle with heterophilic graphs where connected nodes exhibit dissimilar features and different labels. Existing methods address heterophily through graph structure refinement or adaptation of neighbor aggregation functions but often overlook the semantic potential of node text, rely on suboptimal message representation for propagation and compromise performance on homophilic graphs.

Method: LEMP4HG provides paired node texts for LM to generate their connection analysis, which are encoded and then fused with paired node textual embeddings through a gating mechanism. The synthesized messages are semantically enriched and adaptively balanced with both nodes' information. An active learning strategy guided by heuristic MVRD is introduced to selectively enhance node pairs suffer most from message passing.

Result: Extensive experiments validate that the proposed approach excels on heterophilic graphs and performs robustly on homophilic ones, with a graph convolutional network (GCN) backbone and a practical budget.

Conclusion: LEMP4HG mitigates contradictory signals when neighbor aggregation in heterophilic regions and reduces the cost of analysis generation and side effects on homophilic regions.

Abstract: Traditional graph neural networks (GNNs), which rely on homophily-driven
message passing, struggle with heterophilic graphs where connected nodes
exhibit dissimilar features and different labels. While existing methods
address heterophily through graph structure refinement or adaptation of
neighbor aggregation functions, they often overlook the semantic potential of
node text, rely on suboptimal message representation for propagation and
compromise performance on homophilic graphs. To address these limitations, we
propose a novel language model (LM)-enhanced message passing approach for
heterophilic graph leaning (LEMP4HG). Specifically, in the context of
text-attributed graph, we provide paired node texts for LM to generate their
connection analysis, which are encoded and then fused with paired node textual
embeddings through a gating mechanism. The synthesized messages are
semantically enriched and adaptively balanced with both nodes' information,
which mitigates contradictory signals when neighbor aggregation in heterophilic
regions. Furthermore, we introduce an active learning strategy guided by our
heuristic MVRD (Modulated Variation of Reliable Distance), selectively
enhancing node pairs suffer most from message passing, reducing the cost of
analysis generation and side effects on homophilic regions. Extensive
experiments validate that our approach excels on heterophilic graphs and
performs robustly on homophilic ones, with a graph convolutional network (GCN)
backbone and a practical budget.

</details>


### [98] [Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition](https://arxiv.org/abs/2505.19788)
*Zihao Zeng,Xuyao Huang,Boxiu Li,Hao Zhang,Zhijie Deng*

Main category: cs.AI

TL;DR: The paper proposes Multi-Turn Decomposition (MinD) to improve the efficiency of Large Reasoning Models by decoding conventional Chain-of-Thought into explicit, structured interactions. It reduces output tokens and time to first token by about 70% while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models are criticized for their lengthy Chain-of-Thought process which leads to high latency. There is a need to reduce the number of thinking units in this process for more efficiency, but the lack of explicit management of these units makes it challenging.

Method: MinD decodes the conventional Chain-of-Thought into multi-turn responses. Each turn includes a thinking unit and its corresponding answer, allowing reflection, verification, revision, or exploration of alternative approaches. The model is fine-tuned with supervised learning and then improved with reinforcement learning to prioritize correct outputs with fewer turns.

Result: MinD achieves approximately 70% reduction in both output token usage and time to first token on the MATH dataset while showing competitive performance on reasoning benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond.

Conclusion: MinD offers an effective way to make Large Reasoning Models more efficient by significantly reducing latency and token usage without compromising performance.

Abstract: Large Reasoning Models (LRMs) are criticized for the excessively lengthy
Chain-of-Thought (CoT) to derive the final answer, suffering from high
first-token and overall latency. Typically, the CoT of LRMs mixes multiple
thinking units; each unit attempts to produce a candidate answer to the
original query. Hence, a natural idea to improve efficiency is to reduce the
unit number. Yet, the fact that the thinking units in vanilla CoT cannot be
explicitly managed renders doing so challenging. This paper introduces
Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of
explicit, structured, and turn-wise interactions to bridge the gap. In MinD,
the model provides a multi-turn response to the query, where each turn embraces
a thinking unit and yields a corresponding answer. The subsequent turns can
reflect, verify, revise, or explore alternative approaches to both the thinking
and answer parts of earlier ones. This not only makes the answer delivered more
swiftly, but also enables explicit controls over the iterative reasoning
process (i.e., users may halt or continue at any turn). We follow a supervised
fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We
first rephrase the outputs of an LRM into multi-turn formats by prompting
another LLM, and then tune the LRM with such data. Observing that the tuned
model tends to consume even more tokens than the original one (probably due to
that the multi-turn formats introduce additional answer tokens), we advocate
leveraging RL algorithms like GRPO to prioritize correct outputs with fewer
turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up
to ~70% reduction in both output token usage and time to first token (TTFT),
while maintaining competitive performance on reasoning benchmarks such as
MATH-500, AIME24, AMC23, and GPQA-Diamond.

</details>


### [99] [Types of Relations: Defining Analogies with Category Theory](https://arxiv.org/abs/2505.19792)
*Claire Ott,Frank Jäkel*

Main category: cs.AI

TL;DR: The paper explores the use of category theory to construct, find, and evaluate analogies by formalizing knowledge domains as categories.


<details>
  <summary>Details</summary>
Motivation: To investigate what representation can be used to construct, find, and evaluate analogies, inspired by how humans use analogies to transfer knowledge.

Method: By formalizing knowledge domains as categories and using mathematical concepts such as functors, pullbacks, and pushouts to define an analogy, describe its core, and a corresponding blend of the underlying domains.

Result: Demonstrated how to construct domain categories using the example of the analogy between the solar system and the hydrogen atom. Showed the application of functors, pullbacks, and pushouts in defining analogies.

Conclusion: Category theory provides a powerful framework for understanding and constructing analogies.

Abstract: In order to behave intelligently both humans and machines have to represent
their knowledge adequately for how it is used. Humans often use analogies to
transfer their knowledge to new domains, or help others with this transfer via
explanations. Hence, an important question is: What representation can be used
to construct, find, and evaluate analogies? In this paper, we study features of
a domain that are important for constructing analogies. We do so by formalizing
knowledge domains as categories. We use the well-known example of the analogy
between the solar system and the hydrogen atom to demonstrate how to construct
domain categories. We also show how functors, pullbacks, and pushouts can be
used to define an analogy, describe its core and a corresponding blend of the
underlying domains.

</details>


### [100] [DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems](https://arxiv.org/abs/2505.19847)
*Wenqing Zhou,Yuxuan Yan,Qianqian Yang*

Main category: cs.AI

TL;DR: DGRAG is a distributed Knowledge Graph-based RAG approach in an edge-cloud system which organizes local knowledge using knowledge graphs and shares summaries in the cloud to enhance question-answering tasks without centralizing data, thereby addressing privacy concerns and computational costs.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional RAGs such as privacy issues, high computational costs, and latency when dealing with large-scale knowledge bases.

Method: Proposes DGRAG with two phases: Distributed Knowledge Construction (organizing local knowledge into knowledge graphs, generating subgraph summaries, and storing them in a cloud database) and Collaborative Retrieval and Generation (performing local retrieval and generation, using a gate mechanism to decide if cloud assistance is needed for queries beyond local scope).

Result: Experimental results indicate that DGRAG significantly improves the quality of question-answering tasks compared to baseline approaches.

Conclusion: DGRAG effectively enhances the capabilities of language models by integrating external knowledge in a distributed manner, reducing privacy risks and computational costs.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the capabilities of language models by integrating external knowledge.
Due to the diversity of data sources and the constraints of memory and
computing resources, real-world data is often scattered in multiple devices.
Conventional RAGs that store massive amounts of scattered data centrally face
increasing privacy concerns and high computational costs. Additionally, RAG in
a central node raises latency issues when searching over a large-scale
knowledge base. To address these challenges, we propose a distributed Knowledge
Graph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where
each edge device maintains a local knowledge base without the need to share it
with the cloud, instead sharing only summaries of its knowledge. Specifically,
DGRAG has two main phases. In the Distributed Knowledge Construction phase,
DGRAG organizes local knowledge using knowledge graphs, generating subgraph
summaries and storing them in a summary database in the cloud as information
sharing. In the Collaborative Retrieval and Generation phase, DGRAG first
performs knowledge retrieval and answer generation locally, and a gate
mechanism determines whether the query is beyond the scope of local knowledge
or processing capabilities. For queries that exceed the local knowledge scope,
the cloud retrieves knowledge from the most relevant edges based on the
summaries and generates a more precise answer. Experimental results demonstrate
the effectiveness of the proposed DGRAG approach in significantly improving the
quality of question-answering tasks over baseline approaches.

</details>


### [101] [HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation](https://arxiv.org/abs/2505.19866)
*Feng Xiong,Hongling Xu,Yifei Wang,Runxi Cheng,Yong Wang,Xiangxiang Chu*

Main category: cs.AI

TL;DR: In this paper, researchers propose HS-STaR, a hierarchical sampling framework that improves the mathematical reasoning abilities of large language models (LLMs) by efficiently identifying and utilizing boundary-level problems.


<details>
  <summary>Details</summary>
Motivation: Current self-taught reasoners typically allocate a uniform sampling budget across all problems, ignoring the varying utility of problems at different difficulty levels.

Method: The proposed method, HS-STaR, performs lightweight pre-sampling with a reward-guided difficulty estimation strategy to identify boundary-level problems. It then dynamically reallocates the remaining budget toward these high-utility problems during a re-sampling phase.

Result: Extensive experiments across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR significantly outperforms other baselines without requiring additional sampling budget.

Conclusion: HS-STaR is an effective approach for enhancing the mathematical reasoning abilities of LLMs by focusing on problems near the boundary of their reasoning capability.

Abstract: Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of
large language models (LLMs) by leveraging self-generated responses for
self-training. Recent studies have incorporated reward models to guide response
selection or decoding, aiming to obtain higher-quality data. However, they
typically allocate a uniform sampling budget across all problems, overlooking
the varying utility of problems at different difficulty levels. In this work,
we conduct an empirical study and find that problems near the boundary of the
LLM's reasoning capability offer significantly greater learning utility than
both easy and overly difficult ones. To identify and exploit such problems, we
propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.
Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling
with a reward-guided difficulty estimation strategy to efficiently identify
boundary-level problems. Subsequently, it dynamically reallocates the remaining
budget toward these high-utility problems during a re-sampling phase,
maximizing the generation of valuable training data. Extensive experiments
across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR
significantly outperforms other baselines without requiring additional sampling
budget.

</details>


### [102] [Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging](https://arxiv.org/abs/2505.19892)
*Yongxian Wei,Runxi Cheng,Weike Jin,Enneng Yang,Li Shen,Lu Hou,Sinan Du,Chun Yuan,Xiaochun Cao,Dacheng Tao*

Main category: cs.AI

TL;DR: 为了推动多模态大语言模型（MLLMs）的模型合并研究，本文提出了一种包含多种任务的模型合并基准，并实施了10种模型合并算法，还提出了一种新方法以优化合并后的模型性能。研究表明，模型合并能够在无需额外数据训练的情况下构建更优的MLLMs，且多模态间的互补性优于单一模态。


<details>
  <summary>Details</summary>
Motivation: 基础模型因资源密集型训练需求更新缓慢，而领域特定模型在更新间不断演化。模型合并旨在将多个专家模型整合为一个更强大的模型，从而减少存储和服务成本，同时支持去中心化的模型开发。然而，目前的研究主要集中在视觉分类模型或大型语言模型（LLMs）的合并上，对于多模态大型语言模型（MLLMs）的合并研究缺乏明确的任务划分和基准。

Method: 引入了针对MLLMs的模型合并基准，涵盖VQA、几何、图表、OCR和Grounding等任务，并提供LoRA和全微调模型；在该基准上实现了10种模型合并算法；提出一种新方法，通过去除任务向量中的噪声并基于任务向量交互定义的损失函数进行稳健优化，以提升合并后模型的性能。

Result: 所提出的新方法使合并后的模型平均性能提升了2.48%；研究表明模型合并是一种无需数据训练即可改进MLLMs的有效途径；多模态之间的互补性优于单一模态的表现。

Conclusion: 本文提出的模型合并基准和方法为MLLMs的研究提供了重要工具，证明了模型合并能够有效整合不同模态的优势，无需额外数据训练即可构建更强大的MLLMs。

Abstract: While foundation models update slowly due to resource-intensive training
requirements, domain-specific models evolve between updates. Model merging aims
to combine multiple expert models into a single, more capable model, thereby
reducing storage and serving costs while supporting decentralized model
development. Despite its potential, previous studies have primarily focused on
merging visual classification models or Large Language Models (LLMs) for code
and math tasks. Multimodal Large Language Models (MLLMs), which extend the
capabilities of LLMs through large-scale multimodal training, have gained
traction. However, there lacks a benchmark for model merging research that
clearly divides the tasks for MLLM training and evaluation. In this paper, (i)
we introduce the model merging benchmark for MLLMs, which includes multiple
tasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and
full fine-tuning models. Moreover, we explore how model merging can combine
different modalities (e.g., vision-language, audio-language, and video-language
models), moving toward the Omni-language model. (ii) We implement 10 model
merging algorithms on the benchmark. Furthermore, we propose a novel method
that removes noise from task vectors and robustly optimizes the merged vector
based on a loss defined over task vector interactions, achieving an average
performance gain of 2.48%. (iii) We find that model merging offers a promising
way for building improved MLLMs without requiring data training. Our results
also demonstrate that the complementarity among multiple modalities outperforms
individual modalities.

</details>


### [103] [Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program](https://arxiv.org/abs/2505.19896)
*Alejandro Carrasco,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.AI

TL;DR: The paper pioneers the integration of LLMs into space research by developing a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, ranking 2nd in the competition.


<details>
  <summary>Details</summary>
Motivation: To apply the concepts of using Large Language Models (LLMs) as autonomous agents to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations.

Method: Leveraging prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent for the KSPDG challenge.

Result: The developed LLM-based agent ranked 2nd in the Kerbal Space Program Differential Games (KSPDG) challenge.

Conclusion: This work pioneers the integration of LLM agents into space research, providing open repositories for replication and further research.

Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompts. We intend to apply these concepts to the field of Control in space,
enabling LLMs to play a significant role in the decision-making process for
autonomous satellite operations. As a first step towards this goal, we have
developed a pure LLM-based solution for the Kerbal Space Program Differential
Games (KSPDG) challenge, a public software design competition where
participants create autonomous agents for maneuvering satellites involved in
non-cooperative space operations, running on the KSP game engine. Our approach
leverages prompt engineering, few-shot prompting, and fine-tuning techniques to
create an effective LLM-based agent that ranked 2nd in the competition. To the
best of our knowledge, this work pioneers the integration of LLM agents into
space research. The project comprises several open repositories to facilitate
replication and further research. The codebase is accessible on
\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models
and datasets are available on \href{https://huggingface.co/OhhTuRnz}{Hugging
Face}. Additionally, experiment tracking and detailed results can be reviewed
on \href{https://wandb.ai/carrusk/huggingface}{Weights \& Biases

</details>


### [104] [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897)
*Qiushi Sun,Zhoumianze Liu,Chang Ma,Zichen Ding,Fangzhi Xu,Zhangyue Yin,Haiteng Zhao,Zhenyu Wu,Kanzhi Cheng,Zhaoyang Liu,Jianing Wang,Qintong Li,Xiangru Tang,Tianbao Xie,Xiachong Feng,Xiang Li,Ben Kao,Wenhai Wang,Biqing Qi,Lingpeng Kong,Zhiyong Wu*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）对跨学科研究产生了重大影响，特别是基于LLM的智能体在科学发现中的应用。本文介绍了ScienceBoard，它包括一个模拟真实科研工作流的多领域环境和一个人类策划的高质量任务基准。尽管使用最先进的模型，这些智能体在复杂科研任务中的成功率仅为15%。


<details>
  <summary>Details</summary>
Motivation: 认识到能够像人类一样与操作系统交互的计算机智能体在自动化科学研究问题解决和简化研究人员工作流程方面的潜力，推动了更高效的科学发现智能体的发展。

Method: ScienceBoard由两个互补部分组成：(i)一个多领域的动态、视觉丰富的环境，其中包含集成的专业软件，允许智能体通过不同界面自主互动以加速复杂的科研任务；(ii)一个具有挑战性的基准，包含169个高质量、经过严格验证的实际任务，涵盖生物化学、天文学和地理信息学等领域的科学发现工作流。

Result: 评估显示，尽管有希望的结果，但最先进的智能体在处理复杂科研任务时仍存在局限性，整体成功率为15%。深入分析提供了有价值的见解，有助于改进设计原则并构建更强大的科学发现智能体。

Conclusion: 当前的智能体在复杂科研任务中仍有显著不足，需要进一步的研究来克服现有局限性，并设计更有效的智能体，以更好地支持科学家进行科学发现。

Abstract: Large Language Models (LLMs) have extended their impact beyond Natural
Language Processing, substantially fostering the development of
interdisciplinary research. Recently, various LLM-based agents have been
developed to assist scientific discovery progress across multiple aspects and
domains. Among these, computer-using agents, capable of interacting with
operating systems as humans do, are paving the way to automated scientific
problem-solving and addressing routines in researchers' workflows. Recognizing
the transformative potential of these agents, we introduce ScienceBoard, which
encompasses two complementary contributions: (i) a realistic, multi-domain
environment featuring dynamic and visually rich scientific workflows with
integrated professional software, where agents can autonomously interact via
different interfaces to accelerate complex research tasks and experiments; and
(ii) a challenging benchmark of 169 high-quality, rigorously validated
real-world tasks curated by humans, spanning scientific-discovery workflows in
domains such as biochemistry, astronomy, and geoinformatics. Extensive
evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude
3.7, UI-TARS) show that, despite some promising results, they still fall short
of reliably assisting scientists in complex workflows, achieving only a 15%
overall success rate. In-depth analysis further provides valuable insights for
addressing current agent limitations and more effective design principles,
paving the way to build more capable agents for scientific discovery. Our code,
environment, and benchmark are at
https://qiushisun.github.io/ScienceBoard-Home/.

</details>


### [105] [EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM](https://arxiv.org/abs/2505.19905)
*Shuang Ao,Flora D. Salim,Simon Khan*

Main category: cs.AI

TL;DR: EMAC+ is an Embodied Multimodal Agent that integrates LLM and VLM through bidirectional training, enhancing task performance in robotics control.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of LLMs in robotics control, such as handling visual conditions, integrating reasoning with environment dynamics, and learning from visual interactions.

Method: EMAC+ dynamically refines high-level textual plans generated by an LLM using real-time feedback from a VLM executing low-level visual control tasks, allowing the LLM to internalize visual environment dynamics directly through interactive experience.

Result: Extensive experimental evaluations on ALFWorld and RT-1 benchmarks demonstrate superior task performance, robustness against noisy observations, and efficient learning.

Conclusion: EMAC+ addresses critical limitations of previous models, achieving better performance, robustness, and learning efficiency.

Abstract: Although LLMs demonstrate proficiency in several text-based reasoning and
planning tasks, their implementation in robotics control is constrained by
significant deficiencies: (1) LLM agents are designed to work mainly with
textual inputs rather than visual conditions; (2) Current multimodal agents
treat LLMs as static planners, which separates their reasoning from environment
dynamics, resulting in actions that do not take domain-specific knowledge into
account; and (3) LLMs are not designed to learn from visual interactions, which
makes it harder for them to make better policies for specific domains. In this
paper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively
integrates LLM and VLM via a bidirectional training paradigm. Unlike existing
methods, EMAC+ dynamically refines high-level textual plans generated by an LLM
using real-time feedback from a VLM executing low-level visual control tasks.
We address critical limitations of previous models by enabling the LLM to
internalize visual environment dynamics directly through interactive
experience, rather than relying solely on static symbolic mappings. Extensive
experimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+
achieves superior task performance, robustness against noisy observations, and
efficient learning. We also conduct thorough ablation studies and provide
detailed analyses of success and failure cases.

</details>


### [106] [TCP: a Benchmark for Temporal Constraint-Based Planning](https://arxiv.org/abs/2505.19927)
*Zifeng Ding,Sikuan Yan,Zhangdie Yuan,Xianglong Hu,Fangru Lin,Andreas Vlachos*

Main category: cs.AI

TL;DR: Temporal Constraint-based Planning (TCP) benchmark is introduced to evaluate temporal reasoning and planning capabilities of LLMs. It uses naturalistic dialogues with temporal constraints, showing that even the strongest models struggle, revealing limitations in LLMs.


<details>
  <summary>Details</summary>
Motivation: Most existing benchmarks evaluate temporal reasoning and planning in isolation and under limited forms of complexity.

Method: The TCP benchmark features naturalistic dialogues around collaborative projects with diverse and interdependent temporal constraints. Abstract problem prototypes are paired with realistic scenarios from various domains and enriched into dialogues using an LLM, followed by a human quality check.

Result: State-of-the-art LLMs struggle with the TCP benchmark, highlighting its difficulty and revealing limitations in LLMs' temporal constraint-based planning abilities.

Conclusion: Underlying failure cases were analyzed, the benchmark was open sourced, and it is hoped that these findings can inspire future research.

Abstract: Temporal reasoning and planning are essential capabilities for large language
models (LLMs), yet most existing benchmarks evaluate them in isolation and
under limited forms of complexity. To address this gap, we introduce the
Temporal Constraint-based Planning (TCP) benchmark, that jointly assesses both
capabilities. Each instance in TCP features a naturalistic dialogue around a
collaborative project, where diverse and interdependent temporal constraints
are explicitly or implicitly expressed, and models must infer an optimal
schedule that satisfies all constraints. To construct TCP, we first generate
abstract problem prototypes that are paired with realistic scenarios from
various domains and enriched into dialogues using an LLM. A human quality check
is performed on a sampled subset to confirm the reliability of our benchmark.
We evaluate state-of-the-art LLMs and find that even the strongest models
struggle with TCP, highlighting its difficulty and revealing limitations in
LLMs' temporal constraint-based planning abilities. We analyze underlying
failure cases, open source our benchmark, and hope our findings can inspire
future research.

</details>


### [107] [Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making](https://arxiv.org/abs/2505.19933)
*Yejin Son,Minseo Kim,Sungwoong Kim,Seungju Han,Jian Kim,Dongju Jang,Youngjae Yu,Chanyoung Park*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) are used in embodied agents for decision making but safety evaluations have been coarse and domain-specific. This paper introduces SAFEL, a framework to evaluate LLMs' physical safety in embodied decision making by assessing two key competencies: rejecting unsafe commands and generating safe plans. A benchmark called EMBODYGUARD is also introduced to support the framework.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluations for LLMs in embodied agents are coarse and domain-specific, which makes it difficult to understand why and where these models fail. This limits the deployment of LLMs in high-risk environments.

Method: SAFEL evaluates LLMs on two competencies: Command Refusal Test (rejecting unsafe commands) and Plan Safety Test (generating safe and executable plans). The latter is decomposed into functional modules such as goal interpretation, transition modeling, and action sequencing for fine-grained diagnosis of failures. EMBODYGUARD is a PDDL-grounded benchmark with 942 scenarios for testing.

Result: Evaluation across 13 state-of-the-art LLMs showed that while they can often reject clearly unsafe commands, they struggle with anticipating and mitigating subtle situational risks.

Conclusion: The results highlight critical limitations in current LLMs regarding embodied safety and provide a foundation for targeted improvements.

Abstract: Large Language Models (LLMs) are increasingly used for decision making in
embodied agents, yet existing safety evaluations often rely on coarse success
rates and domain-specific setups, making it difficult to diagnose why and where
these models fail. This obscures our understanding of embodied safety and
limits the selective deployment of LLMs in high-risk physical environments. We
introduce SAFEL, the framework for systematically evaluating the physical
safety of LLMs in embodied decision making. SAFEL assesses two key
competencies: (1) rejecting unsafe commands via the Command Refusal Test, and
(2) generating safe and executable plans via the Plan Safety Test. Critically,
the latter is decomposed into functional modules, goal interpretation,
transition modeling, action sequencing, enabling fine-grained diagnosis of
safety failures. To support this framework, we introduce EMBODYGUARD, a
PDDL-grounded benchmark containing 942 LLM-generated scenarios covering both
overtly malicious and contextually hazardous instructions. Evaluation across 13
state-of-the-art LLMs reveals that while models often reject clearly unsafe
commands, they struggle to anticipate and mitigate subtle, situational risks.
Our results highlight critical limitations in current LLMs and provide a
foundation for more targeted, modular improvements in safe embodied reasoning.

</details>


### [108] [DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph](https://arxiv.org/abs/2505.19956)
*Jihyung Lee,Jin-Seop Lee,Jaehoon Lee,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: The paper proposes a novel approach for Text-to-SQL translation using a Deep Contextual Schema Link Graph to improve SQL generation performance and efficiency across both large and small LLMs.


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL translation methods heavily rely on the intrinsic capabilities of hyper-scaled LLMs, leading to little improvement in performance and significant drops when smaller LLMs are used.

Method: The authors construct a Deep Contextual Schema Link Graph that captures key information and semantic relationships between a question and its database schema items, enabling effective representation and retrieval for in-context learning.

Result: Experimental results on the Spider benchmark show consistent improvements in SQL generation performance and efficiency for both hyper-scaled and small LLMs.

Conclusion: The proposed approach demonstrates effectiveness in enhancing Text-to-SQL translation across different scales of LLMs.

Abstract: Text-to-SQL, which translates a natural language question into an SQL query,
has advanced with in-context learning of Large Language Models (LLMs). However,
existing methods show little improvement in performance compared to randomly
chosen demonstrations, and significant performance drops when smaller LLMs
(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely
on the intrinsic capabilities of hyper-scaled LLMs, rather than effectively
retrieving useful demonstrations. In this paper, we propose a novel approach
for effectively retrieving demonstrations and generating SQL queries. We
construct a Deep Contextual Schema Link Graph, which contains key information
and semantic relationship between a question and its database schema items.
This graph-based structure enables effective representation of Text-to-SQL
samples and retrieval of useful demonstrations for in-context learning.
Experimental results on the Spider benchmark demonstrate the effectiveness of
our approach, showing consistent improvements in SQL generation performance and
efficiency across both hyper-scaled LLMs and small LLMs. Our code will be
released.

</details>


### [109] [Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction](https://arxiv.org/abs/2505.19965)
*Yu Wang,Junshu Dai,Yuchen Ying,Yuxuan Liang,Tongya Zheng,Mingli Song*

Main category: cs.AI

TL;DR: 提出了一种名为ALOHA的框架，用于解决长尾分布下的人类移动性预测问题。该框架通过利用大型语言模型和马斯洛需求层次理论构建位置层次结构，并通过优化技术提高预测效果。实验表明，该框架在平衡头部和尾部位置预测方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的长尾学习方法主要关注于数据、模型或类别层面的再平衡，忽略了位置的时空语义。这使得在人类移动性预测中，针对长尾分布的解决方案尚未得到充分探索。

Method: 1. 构建城市定制的位置层次结构：利用大型语言模型（LLMs）和马斯洛需求层次理论设计Chain-of-Thought（CoT）提示，捕捉时空语义。
2. 优化位置层次预测：通过Gumbel干扰和节点自适应权重在层次树结构中优化预测。

Result: 在六个数据集上的实验表明，该框架对各种最先进的模型具有持续的有效性和可推广性，能够很好地平衡头部和尾部位置的预测。权重分析和消融研究揭示了各组件对头部和尾部位置优化的差异。深入分析显示，位置层次结构提供了有效的语义指导。

Conclusion: ALOHA框架为长尾移动性预测提供了一种新颖且有效的方法，其在平衡头部和尾部位置预测方面的表现突出，展现了良好的应用前景。

Abstract: Human mobility prediction is crucial for applications ranging from
location-based recommendations to urban planning, which aims to forecast users'
next location visits based on historical trajectories. Despite the severe
long-tailed distribution of locations, the problem of long-tailed mobility
prediction remains largely underexplored. Existing long-tailed learning methods
primarily focus on rebalancing the skewed distribution at the data, model, or
class level, neglecting to exploit the spatiotemporal semantics of locations.
To address this gap, we propose the first plug-and-play framework for
long-tailed mobility prediction in an exploitation and exploration manner,
named \textbf{A}daptive \textbf{LO}cation \textbf{H}ier\textbf{A}rchy learning
(ALOHA). First, we construct city-tailored location hierarchy based on Large
Language Models (LLMs) by exploiting Maslow's theory of human motivation to
design Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.
Second, we optimize the location hierarchy predictions by Gumbel disturbance
and node-wise adaptive weights within the hierarchical tree structure.
Experiments on state-of-the-art models across six datasets demonstrate the
framework's consistent effectiveness and generalizability, which strikes a well
balance between head and tail locations. Weight analysis and ablation studies
reveal the optimization differences of each component for head and tail
locations. Furthermore, in-depth analyses of hierarchical distance and case
study demonstrate the effective semantic guidance from the location hierarchy.
Our code will be made publicly available.

</details>


### [110] [The Many Challenges of Human-Like Agents in Virtual Game Environments](https://arxiv.org/abs/2505.20011)
*Maciej Świechowski,Dominik Ślęzak*

Main category: cs.AI

TL;DR: The paper discusses the importance of human-like agents in games and beyond, presenting two contributions: a survey on challenges in implementing human-like AI in games and an empirical study using a machine-learning approach to distinguish human players from bots.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental research questions of how to model and implement human-like AI and how to measure its degree of human likeness.

Method: A survey of significant challenges in implementing human-like AI in games and an empirical study using a custom deep recurrent convolutional neural network in a tactical video game.

Result: The hypothesis is that the more challenging it is to create human-like AI for a given game, the easier it becomes to develop a method for distinguishing humans from AI-driven players.

Conclusion: This article provides insights into the challenges of creating human-like AI and demonstrates a method to distinguish human players from bots based on empirical data.

Abstract: Human-like agents are an increasingly important topic in games and beyond.
Believable non-player characters enhance the gaming experience by improving
immersion and providing entertainment. They also offer players the opportunity
to engage with AI entities that can function as opponents, teachers, or
cooperating partners. Additionally, in games where bots are prohibited -- and
even more so in non-game environments -- there is a need for methods capable of
identifying whether digital interactions occur with bots or humans. This leads
to two fundamental research questions: (1) how to model and implement
human-like AI, and (2) how to measure its degree of human likeness.
  This article offers two contributions. The first one is a survey of the most
significant challenges in implementing human-like AI in games (or any virtual
environment featuring simulated agents, although this article specifically
focuses on games). Thirteen such challenges, both conceptual and technical, are
discussed in detail. The second is an empirical study performed in a tactical
video game that addresses the research question: "Is it possible to distinguish
human players from bots (AI agents) based on empirical data?" A
machine-learning approach using a custom deep recurrent convolutional neural
network is presented. We hypothesize that the more challenging it is to create
human-like AI for a given game, the easier it becomes to develop a method for
distinguishing humans from AI-driven players.

</details>


### [111] [Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2505.20075)
*Mengdi Li,Jiaye Lin,Xufeng Zhao,Wenhao Lu,Peilin Zhao,Stefan Wermter,Di Wang*

Main category: cs.AI

TL;DR: Reward models trained with conventional Reinforcement Learning from AI Feedback (RLAIF) methods have poor generalizability, which affects the alignment performance of policy models during reinforcement learning. This paper proposes Curriculum-RLAIF, a framework that progressively trains reward models using preference pairs of increasing difficulty. Experimental results show that this method improves the generalizability of reward models and significantly enhances policy model alignment without extra inference costs compared to non-curriculum baselines.


<details>
  <summary>Details</summary>
Motivation: Current RLAIF methods for training reward models suffer from limited generalizability due to issues such as distribution shift, preference label noise, and mismatches between sample difficulty and model capacity.

Method: The authors propose Curriculum-RLAIF, a novel framework that constructs preference pairs with varying difficulty levels and progressively incorporates them into the training process of reward models. This data-centric approach aims to address the intertwined challenges related to data difficulty.

Result: Reward models trained with Curriculum-RLAIF demonstrate improved generalizability and significantly enhance the alignment performance of policy models by a large margin compared to various non-curriculum baselines. The method achieves these improvements without incurring additional inference costs.

Conclusion: Curriculum-RLAIF is superior in terms of simplicity, efficiency, and effectiveness compared to alternative approaches such as data selection via external pretrained reward models or internal self-selection mechanisms, as well as other curriculum strategies.

Abstract: Reward models trained with conventional Reinforcement Learning from AI
Feedback (RLAIF) methods suffer from limited generalizability, which hinders
the alignment performance of the policy model during reinforcement learning
(RL). This challenge stems from various issues, including distribution shift,
preference label noise, and mismatches between overly challenging samples and
model capacity. In this paper, we attempt to enhance the generalizability of
reward models through a data-centric approach, driven by the insight that these
issues are inherently intertwined from the perspective of data difficulty. To
address this, we propose a novel framework, $\textit{Curriculum-RLAIF}$, which
constructs preference pairs with varying difficulty levels and produces a
curriculum that progressively incorporates preference pairs of increasing
difficulty for reward model training. Our experimental results suggest that
reward models trained with Curriculum-RLAIF achieve improved generalizability,
significantly increasing the alignment performance of the policy model by a
large margin without incurring additional inference costs compared to various
non-curriculum baselines. Detailed analysis and comparisons with alternative
approaches, including data selection via external pretrained reward models or
internal self-selection mechanisms, as well as other curriculum strategies,
further demonstrate the superiority of our approach in terms of simplicity,
efficiency, and effectiveness.

</details>


### [112] [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087)
*Makesh Narsimhan Sreedhar,Traian Rebedea,Christopher Parisien*

Main category: cs.AI

TL;DR: 基于推理的语言模型在数学和编码任务中表现出色，同时对LLM的安全性和防护应用也有显著好处。本文全面分析了训练推理为基础的防护模型用于内容审核，重点在于推广到自定义安全策略的能力。研究发现推理模型在数据效率和推理效率上具有优势，并提出了推理预算、推理长度对延迟和准确性的影响以及双模式训练等方法。


<details>
  <summary>Details</summary>
Motivation: 探索推理模型在内容审核中的应用潜力，特别是其在推广到自定义安全策略时的数据效率和推理效率。

Method: 通过数据分析推理模型在训练样本效率上的表现；引入推理预算来评估推理长度对延迟和准确性的权衡；采用双模式训练以实现在运行时对推理行为的控制。

Result: 推理模型在较少训练样本下能达到与非推理模型相当的性能，且能进一步优化高价值困难样本的表现；推理长度对延迟和准确性有直接影响，而双模式训练提供了灵活的推理行为控制。

Conclusion: 推理模型为研究人员和开发者提供了有效和高效的训练和部署方法，适用于实际系统中的内容审核应用。

Abstract: Reasoning-based language models have demonstrated strong performance across
various domains, with the most notable gains seen in mathematical and coding
tasks. Recent research has shown that reasoning also offers significant
benefits for LLM safety and guardrail applications. In this work, we conduct a
comprehensive analysis of training reasoning-based guardrail models for content
moderation, with an emphasis on generalization to custom safety policies at
inference time. Our study focuses on two key dimensions: data efficiency and
inference efficiency. On the data front, we find that reasoning-based models
exhibit strong sample efficiency, achieving competitive performance with
significantly fewer training examples than their non-reasoning counterparts.
This unlocks the potential to repurpose the remaining data for mining
high-value, difficult samples that further enhance model performance. On the
inference side, we evaluate practical trade-offs by introducing reasoning
budgets, examining the impact of reasoning length on latency and accuracy, and
exploring dual-mode training to allow runtime control over reasoning behavior.
Our findings will provide practical insights for researchers and developers to
effectively and efficiently train and deploy reasoning-based guardrails models
in real-world systems.

</details>


### [113] [SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale](https://arxiv.org/abs/2505.20094)
*Qi Li,Kun Li,Haozhi Han,Honghui Shang,Xinfu He,Yunquan Zhang,Hong An,Ting Cao,Mao Yang*

Main category: cs.AI

TL;DR: SwarmThinkers 是一种基于强化学习的框架，将原子尺度模拟视为物理基础的群智能系统，首次实现了在单一 A100 GPU 上进行全规模、物理一致的模拟，比传统方法快 4963 倍，内存使用减少 485 倍。


<details>
  <summary>Details</summary>
Motivation: 现有的科学模拟方法要么保证热力学准确性但扩展性差（如经典方法 Kinetic Monte Carlo），要么提供效率但牺牲了物理一致性和可解释性（如基于学习的方法）。因此需要一种能够同时实现物理一致性、可解释性和跨尺度扩展的新方法。

Method: SwarmThinkers 框架将每个扩散粒子建模为一个局部决策代理，通过共享的策略网络选择过渡，并结合重加权机制融合学习偏好与过渡速率。训练采用集中式训练和分散式执行范式，使策略能够在不同系统大小、浓度和温度下泛化，而无需重新训练。

Result: 在模拟辐射诱导的 Fe-Cu 合金沉淀基准测试中，SwarmThinkers 实现了单个 A100 GPU 上的全规模物理一致模拟，计算速度提升高达 4963 倍（平均 3185 倍），内存使用降低 485 倍。

Conclusion: SwarmThinkers 标志着科学模拟的范式转变，通过将粒子视为决策者而非被动采样器，统一了物理一致性、可解释性和可扩展性。

Abstract: Can a scientific simulation system be physically consistent, interpretable by
design, and scalable across regimes--all at once? Despite decades of progress,
this trifecta remains elusive. Classical methods like Kinetic Monte Carlo
ensure thermodynamic accuracy but scale poorly; learning-based methods offer
efficiency but often sacrifice physical consistency and interpretability. We
present SwarmThinkers, a reinforcement learning framework that recasts
atomic-scale simulation as a physically grounded swarm intelligence system.
Each diffusing particle is modeled as a local decision-making agent that
selects transitions via a shared policy network trained under thermodynamic
constraints. A reweighting mechanism fuses learned preferences with transition
rates, preserving statistical fidelity while enabling interpretable, step-wise
decision making. Training follows a centralized-training,
decentralized-execution paradigm, allowing the policy to generalize across
system sizes, concentrations, and temperatures without retraining. On a
benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers
is the first system to achieve full-scale, physically consistent simulation on
a single A100 GPU, previously attainable only via OpenKMC on a supercomputer.
It delivers up to 4963x (3185x on average) faster computation with 485x lower
memory usage. By treating particles as decision-makers, not passive samplers,
SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies
physical consistency, interpretability, and scalability through agent-driven
intelligence.

</details>


### [114] [Spatiotemporal Causal Decoupling Model for Air Quality Forecasting](https://arxiv.org/abs/2505.20119)
*Jiaming Ma,Guanjun Wang,Sheng Huang,Kuo Yang,Binwu Wang,Pengkun Wang,Yang Wang*

Main category: cs.AI

TL;DR: The paper presents AirCade, a new air quality forecasting model using causal decoupling and spatiotemporal modules to improve prediction accuracy by over 20% compared to existing models.


<details>
  <summary>Details</summary>
Motivation: Air pollution significantly impacts human health, livelihoods, and economic development, making accurate air quality forecasting crucial.

Method: The AirCade model incorporates a spatiotemporal module with knowledge embedding techniques to capture AQI dynamics, a causal decoupling module to separate synchronous causality from past data, and a causal intervention mechanism to handle meteorological uncertainties.

Result: AirCade shows over 20% relative improvement in prediction accuracy on an open-source air quality dataset compared to state-of-the-art models.

Conclusion: AirCade enhances the accuracy and robustness of air quality forecasting through its innovative approach to modeling causal relationships and handling uncertainties.

Abstract: Due to the profound impact of air pollution on human health, livelihoods, and
economic development, air quality forecasting is of paramount significance.
Initially, we employ the causal graph method to scrutinize the constraints of
existing research in comprehensively modeling the causal relationships between
the air quality index (AQI) and meteorological features. In order to enhance
prediction accuracy, we introduce a novel air quality forecasting model,
AirCade, which incorporates a causal decoupling approach. AirCade leverages a
spatiotemporal module in conjunction with knowledge embedding techniques to
capture the internal dynamics of AQI. Subsequently, a causal decoupling module
is proposed to disentangle synchronous causality from past AQI and
meteorological features, followed by the dissemination of acquired knowledge to
future time steps to enhance performance. Additionally, we introduce a causal
intervention mechanism to explicitly represent the uncertainty of future
meteorological features, thereby bolstering the model's robustness. Our
evaluation of AirCade on an open-source air quality dataset demonstrates over
20\% relative improvement over state-of-the-art models.

</details>


### [115] [Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets](https://arxiv.org/abs/2505.20120)
*Simpson Zhang,Tennison Liu,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: In labor markets influenced by economic forces due to incomplete information, AI agents need metacognitive and strategic reasoning for effective performance. Metacognition involves internal reasoning such as self-assessment and task understanding, while strategic reasoning refers to external reasoning including beliefs about others and strategic decision-making. Both are essential for deciding actions in labor markets.


<details>
  <summary>Details</summary>
Motivation: Current labor markets are significantly impacted by adverse selection, moral hazard, and reputation caused by incomplete information. With the introduction of AI agents into labor markets, these economic forces will continue to be influential.

Method: The paper discusses metacognitive reasoning, which is a form of internal reasoning that includes self-assessment, task understanding, and evaluation of strategies, and strategic reasoning, which is external reasoning covering beliefs about other participants, strategic decisions, and learning over time.

Result: Both metacognitive and strategic reasoning are crucial for AI agents when deciding among various actions they can take in labor markets, both within and outside their jobs.

Conclusion: Further research is needed in the areas of metacognitive and strategic reasoning to enhance the capabilities of AI agents in labor markets.

Abstract: Current labor markets are strongly affected by the economic forces of adverse
selection, moral hazard, and reputation, each of which arises due to
$\textit{incomplete information}$. These economic forces will still be
influential after AI agents are introduced, and thus, agents must use
metacognitive and strategic reasoning to perform effectively. Metacognition is
a form of $\textit{internal reasoning}$ that includes the capabilities for
self-assessment, task understanding, and evaluation of strategies. Strategic
reasoning is $\textit{external reasoning}$ that covers holding beliefs about
other participants in the labor market (e.g., competitors, colleagues), making
strategic decisions, and learning about others over time. Both types of
reasoning are required by agents as they decide among the many
$\textit{actions}$ they can take in labor markets, both within and outside
their jobs. We discuss current research into metacognitive and strategic
reasoning and the areas requiring further development.

</details>


### [116] [Agentic AI Process Observability: Discovering Behavioral Variability](https://arxiv.org/abs/2505.20127)
*Fabiana Fournier,Lior Limonad,Yuval David*

Main category: cs.AI

TL;DR: AI agents using LLMs are crucial in modern software. Given their non-deterministic behavior, there is a need for debugging and observability tools. This paper explores process and causal discovery on agent execution trajectories to enhance developer observability and uses LLM-based static analysis to differentiate between intended and unintended variability.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the increasing reliance on AI agents powered by LLMs in modern software systems and the challenges posed by their non-deterministic behavior which necessitates robust debugging and observability tools.

Method: The method involves applying process and causal discovery to agent execution trajectories to improve observability and employing LLM-based static analysis techniques to identify and differentiate between intended and unintended behavioral variability.

Result: This approach results in enhanced developer observability and control over evolving specifications, aiding in identifying areas that require more precise definitions.

Conclusion: The conclusion emphasizes the importance of such instrumentation for developers to have better control over specifications and identify aspects needing explicit definitions.

Abstract: AI agents that leverage Large Language Models (LLMs) are increasingly
becoming core building blocks of modern software systems. A wide range of
frameworks is now available to support the specification of such applications.
These frameworks enable the definition of agent setups using natural language
prompting, which specifies the roles, goals, and tools assigned to the various
agents involved. Within such setups, agent behavior is non-deterministic for
any given input, highlighting the critical need for robust debugging and
observability tools. In this work, we explore the use of process and causal
discovery applied to agent execution trajectories as a means of enhancing
developer observability. This approach aids in monitoring and understanding the
emergent variability in agent behavior. Additionally, we complement this with
LLM-based static analysis techniques to distinguish between intended and
unintended behavioral variability. We argue that such instrumentation is
essential for giving developers greater control over evolving specifications
and for identifying aspects of functionality that may require more precise and
explicit definitions.

</details>


### [117] [MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents](https://arxiv.org/abs/2505.20148)
*Ziming Wei,Bingqian Lin,Zijian Jiao,Yunshuang Nie,Liang Ma,Yuecheng Liu,Yuzheng Zhuang,Xiaodan Liang*

Main category: cs.AI

TL;DR: An abstract discussing the importance of spatial planning in spatial intelligence and introducing MineAnyBuild, a new benchmark for evaluating AI agents' spatial planning abilities in Minecraft.


<details>
  <summary>Details</summary>
Motivation: To address the gap between abstract spatial understanding and concrete task execution in existing benchmarks for spatial intelligence.

Method: Building a comprehensive benchmark called MineAnyBuild which requires AI agents to generate executable architecture building plans based on multi-modal human instructions within the Minecraft game environment.

Result: MineAnyBuild contains 4,000 curated spatial planning tasks and offers a paradigm for infinitely expandable data collection. Evaluations using MineAnyBuild reveal limitations and potential in MLLM-based agents' spatial planning abilities.

Conclusion: MineAnyBuild opens new avenues for evaluating spatial intelligence and promotes development for open-world AI agents capable of spatial planning.

Abstract: Spatial Planning is a crucial part in the field of spatial intelligence,
which requires the understanding and planning about object arrangements in
space perspective. AI agents with the spatial planning ability can better adapt
to various real-world applications, including robotic manipulation, automatic
assembly, urban planning etc. Recent works have attempted to construct
benchmarks for evaluating the spatial intelligence of Multimodal Large Language
Models (MLLMs). Nevertheless, these benchmarks primarily focus on spatial
reasoning based on typical Visual Question-Answering (VQA) forms, which suffers
from the gap between abstract spatial understanding and concrete task
execution. In this work, we take a step further to build a comprehensive
benchmark called MineAnyBuild, aiming to evaluate the spatial planning ability
of open-world AI agents in the Minecraft game. Specifically, MineAnyBuild
requires an agent to generate executable architecture building plans based on
the given multi-modal human instructions. It involves 4,000 curated spatial
planning tasks and also provides a paradigm for infinitely expandable data
collection by utilizing rich player-generated content. MineAnyBuild evaluates
spatial planning through four core supporting dimensions: spatial
understanding, spatial reasoning, creativity, and spatial commonsense. Based on
MineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based
agents, revealing the severe limitations but enormous potential in their
spatial planning abilities. We believe our MineAnyBuild will open new avenues
for the evaluation of spatial intelligence and help promote further development
for open-world AI agents capable of spatial planning.

</details>


### [118] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Main category: cs.AI

TL;DR: As large language models grow, identifying vulnerabilities through red-teaming becomes crucial. Traditional methods may be ineffective when target models surpass red-teamers in capabilities. The study evaluates over 500 attacker-target pairs and finds that more capable models are better attackers, attack success drops when the target's capability exceeds the attacker's, and attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark.


<details>
  <summary>Details</summary>
Motivation: To understand the shift in red-teaming from a weak-to-strong problem where target models surpass red-teamers in capabilities, and to identify vulnerabilities for safe deployment of large language models.

Method: Evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels.

Result: Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark.

Conclusion: Fixed-capability attackers may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities.

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


### [119] [Program of Equations Thoughts to Solve Algebra Word Problems](https://arxiv.org/abs/2505.20170)
*Yunze Lin*

Main category: cs.AI

TL;DR: 为了解决大型语言模型（LLMs）在解决代数文字问题（AWPs）时的计算弱点，研究者提出了程序方程思维（POET）方法。该方法通过预测方程和生成代码的两阶段任务，将复杂计算转移到Python解释器上，从而避免了LLMs中的计算错误。此外，还提出了零样本POET，它使用手动设计的模板使LLMs能够直接生成Python代码进行一步求解。实验结果表明，POET和零样本POET在多个数据集上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维技术引导LLMs通过逐步推理解决了AWPs并取得显著成果，但LLMs本身的计算弱点限制了其推理能力，可能导致最终答案错误。因此，需要一种新方法来克服这一限制。

Method: 提出了一种名为POET的方法，将生成逐步推理答案的任务转化为两阶段任务：预测方程和生成代码。同时，还提出了零样本POET，利用手动设计的模板让LLMs直接生成Python代码进行一步求解。

Result: POET在PEN和ALG514数据集上分别达到了95.3%和98.0%的准确率，零样本POET在DRAW-1K数据集上达到了95.5%的准确率，均取得了新的最佳结果。

Conclusion: POET和零样本POET有效解决了LLMs在解决AWPs时的计算错误问题，并在多个数据集上取得了优异的性能，展示了其潜力和有效性。

Abstract: Solving algebraic word problems (AWPs) has recently emerged as an important
natural language processing task. Recently, large language models (LLMs) have
demonstrated powerful mathematical capabilities, and the Chain-of-Thought
technique, which guides LLMs through step-by-step reasoning, has yielded
impressive results. However, this reasoning ability is limited by the
computational weaknesses of LLMs themselves, where calculation errors can
accumulate, leading to incorrect final answers. To address this, we propose
Program of Equations Thoughts (POET), which transforms the task of generating
step-by-step reasoning answers into a two-stage task of predicting equations
and generating code, offloading complex computations to a Python interpreter to
avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which
utilizes a manually designed template to enable LLMs to directly generate
Python code for one-step solving. Our method achieves accuracies of 95.3% and
98.0% on the PEN and ALG514 datasets, respectively, setting a new
state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%
on the DRAW-1K dataset.

</details>


### [120] [An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation](https://arxiv.org/abs/2505.20182)
*Shubham Gandhi,Atharva Naik,Yiqing Xie,Carolyn Rose*

Main category: cs.AI

TL;DR: 研究了在仓库级别代码生成中，强弱语言模型之间的成本高效协作。弱模型以较低成本处理简单任务，而最具挑战性的任务则交给强模型处理。通过在GitHub问题解决上评估多种协作策略（基于上下文、基于管道和动态策略），发现最有效的协作策略能够在降低成本40%的同时达到与强模型相当的性能。根据研究结果，提供了在不同预算和性能限制下选择协作策略的实用指南。强-弱协作显著提升了弱模型的性能，并且成本大大降低，其中基于管道和上下文的方法最为高效。


<details>
  <summary>Details</summary>
Motivation: 在代码生成领域，虽然已有许多关于模型架构的研究，但很少有研究分析性能与成本之间的关系。因此，探索强弱模型之间如何协作以在降低成本的同时保持或提升性能成为重要课题。

Method: 研究了三种协作策略：基于上下文的协作、基于管道的协作和动态协作。这些策略被用于GitHub问题解决任务中，以评估它们的性能和成本效率。最终确定了哪些策略在性能和成本之间取得了最佳平衡。

Result: 最有效的协作策略实现了与强模型相当的性能，同时将成本降低了40%。研究表明，强-弱模型协作可以显著提高弱模型的性能，而成本仅为强模型的一小部分。基于管道和上下文的方法被证明是最高效的。

Conclusion: 强弱模型之间的协作可以在大幅降低成本的同时保持高性能。基于研究结果，作者提供了在不同预算和性能要求下选择合适协作策略的指南，并公开了相关代码。

Abstract: We study cost-efficient collaboration between strong and weak language models
for repository-level code generation, where the weak model handles simpler
tasks at lower cost, and the most challenging tasks are delegated to the strong
model. While many works propose architectures for this task, few analyze
performance relative to cost. We evaluate a broad spectrum of collaboration
strategies: context-based, pipeline-based, and dynamic, on GitHub issue
resolution. Our most effective collaborative strategy achieves equivalent
performance to the strong model while reducing the cost by 40%. Based on our
findings, we offer actionable guidelines for choosing collaboration strategies
under varying budget and performance constraints. Our results show that
strong-weak collaboration substantially boosts the weak model's performance at
a fraction of the cost, pipeline and context-based methods being most
efficient. We release the code for our work at
https://github.com/shubhamrgandhi/codegen-strong-weak-collab.

</details>


### [121] [Temporal Sampling for Forgotten Reasoning in LLMs](https://arxiv.org/abs/2505.20196)
*Yuetai Li,Zhangchen Xu,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Xiang Yue,Radha Poovendran*

Main category: cs.AI

TL;DR: Fine-tuning LLMs can lead to temporal forgetting, where models forget previously solved problems. Temporal Sampling addresses this by drawing outputs from multiple training checkpoints, recovering forgotten solutions and improving reasoning performance without retraining or ensembling.


<details>
  <summary>Details</summary>
Motivation: To explore and address the issue of temporal forgetting in fine-tuned LLMs, which refers to the counterintuitive phenomenon where models forget how to solve problems they previously answered correctly during training.

Method: Introduced Temporal Sampling, a decoding strategy that draws outputs from multiple checkpoints along the training trajectory. This method is further extended to LoRA-adapted models, storing only adapter weights across checkpoints.

Result: Temporal Sampling recovers forgotten solutions and leads to substantial improvements in reasoning performance, with gains from 4 to 19 points in Pass@k and consistent gains in Majority@k across several benchmarks.

Conclusion: Temporal Sampling offers a practical, compute-efficient way to surface hidden reasoning ability in LLMs and rethink evaluation methods.

Abstract: Fine-tuning large language models (LLMs) is intended to improve their
reasoning capabilities, yet we uncover a counterintuitive effect: models often
forget how to solve problems they previously answered correctly during
training. We term this phenomenon temporal forgetting and show that it is
widespread across model sizes, fine-tuning methods (both Reinforcement Learning
and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this
gap, we introduce Temporal Sampling, a simple decoding strategy that draws
outputs from multiple checkpoints along the training trajectory. This approach
recovers forgotten solutions without retraining or ensembling, and leads to
substantial improvements in reasoning performance, gains from 4 to 19 points in
Pass@k and consistent gains in Majority@k across several benchmarks. We further
extend our method to LoRA-adapted models, demonstrating that storing only
adapter weights across checkpoints achieves similar benefits with minimal
storage cost. By leveraging the temporal diversity inherent in training,
Temporal Sampling offers a practical, compute-efficient way to surface hidden
reasoning ability and rethink how we evaluate LLMs.

</details>


### [122] [Shutdownable Agents through POST-Agency](https://arxiv.org/abs/2505.20203)
*Elliott Thornley*

Main category: cs.AI

TL;DR: Many fear future artificial agents will resist shutdown. The POST-Agents Proposal ensures that doesn't happen by training agents to satisfy Preferences Only Between Same-Length Trajectories (POST). This leads to Neutrality+, which keeps agents shutdownable and useful.


<details>
  <summary>Details</summary>
Motivation: To address the concern that future artificial agents may resist shutdown, potentially leading to uncontrollable situations.

Method: The POST-Agents Proposal suggests training agents to satisfy Preferences Only Between Same-Length Trajectories (POST). It proves that POST, along with other conditions, implies Neutrality+.

Result: Neutrality+ ensures that the agent maximizes expected utility while ignoring the probability distribution over trajectory-lengths.

Conclusion: Neutrality+ keeps agents shutdownable and allows them to be useful.

Abstract: Many fear that future artificial agents will resist shutdown. I present an
idea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose
that we train agents to satisfy Preferences Only Between Same-Length
Trajectories (POST). I then prove that POST - together with other conditions -
implies Neutrality+: the agent maximizes expected utility, ignoring the
probability distribution over trajectory-lengths. I argue that Neutrality+
keeps agents shutdownable and allows them to be useful.

</details>


### [123] [The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels](https://arxiv.org/abs/2505.20214)
*Jiaming Ji,Sitong Fang,Wenjing Cao,Jiahao Li,Xuyao Wang,Juntao Dai,Chi-Min Chan,Sirui Han,Yike Guo,Yaodong Yang*

Main category: cs.AI

TL;DR: 研究发现，慢推理模型在面对不完整或误导性的多模态输入时，容易产生虚假细节支持错误推理，即“多模态幻象”。通过50人标注的5000样本数据集，揭示慢模型倾向于深度优先思考，而快模型则更谨慎。慢模型在数学等结构化领域有效，但在模糊多模态输入下表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 探究慢推理模型（System II）是否一定比快推理模型（System I）更真实，特别是在多模态情境下的表现。

Method: 构建了一个5000样本的分层提示数据集，由50人标注，逐步增加复杂度以分析不同模型的推理方式。

Result: 慢推理模型在面对模糊多模态输入时，倾向于深度优先思考，容易生成虚假细节；而快模型更倾向于广度优先推理，表现出更大的谨慎性。

Conclusion: 慢推理模型在结构化领域如数学中非常有效，但在处理模糊多模态输入时表现出脆弱性。

Abstract: Reasoning models have recently attracted significant attention, especially
for tasks that involve complex inference. Their strengths exemplify the System
II paradigm (slow, structured thinking), contrasting with the System I (rapid,
heuristic-driven). Yet, does slower reasoning necessarily lead to greater
truthfulness? Our findings suggest otherwise. In this study, we present the
first systematic investigation of distortions associated with System I and
System II reasoning in multimodal contexts. We demonstrate that slower
reasoning models, when presented with incomplete or misleading visual inputs,
are more likely to fabricate plausible yet false details to support flawed
reasoning -- a phenomenon we term the "Mirage of Multimodality". To examine
this, we constructed a 5,000-sample hierarchical prompt dataset annotated by 50
human participants. These prompts gradually increase in complexity, revealing a
consistent pattern: slower reasoning models tend to employ depth-first thinking
(delving deeper into incorrect premises), whereas faster chat models favor
breadth-first inference, exhibiting greater caution under uncertainty. Our
results highlight a critical vulnerability of slower reasoning models: although
highly effective in structured domains such as mathematics, it becomes brittle
when confronted with ambiguous multimodal inputs.

</details>


### [124] [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
*Jiahao Qiu,Fulian Xiao,Yimin Wang,Yuchen Mao,Yijia Chen,Xinzhe Juan,Siran Wang,Xuan Qi,Tongcheng Zhang,Zixin Yao,Jiacheng Guo,Yifu Lu,Charles Argon,Jundi Cui,Daixin Chen,Junran Zhou,Shuyao Zhou,Zhanpeng Zhou,Ling Yang,Shilong Liu,Hongru Wang,Kaixuan Huang,Xun Jiang,Yuming Cao,Yue Chen,Yunfei Chen,Zhengyi Chen,Ruowei Dai,Mengqiu Deng,Jiye Fu,Yunting Gu,Zijie Guan,Zirui Huang,Xiaoyan Ji,Yumeng Jiang,Delong Kong,Haolong Li,Jiaqi Li,Ruipeng Li,Tianze Li,Zhuoran Li,Haixia Lian,Mengyue Lin,Xudong Liu,Jiayi Lu,Jinghan Lu,Wanyu Luo,Ziyue Luo,Zihao Pu,Zhi Qiao,Ruihuan Ren,Liang Wan,Ruixiang Wang,Tianhui Wang,Yang Wang,Zeyu Wang,Zihua Wang,Yujia Wu,Zhaoyi Wu,Hao Xin,Weiao Xing,Ruojun Xiong,Weijie Xu,Yao Shu,Xiao Yao,Xiaorui Yang,Yuchen Yang,Nan Yi,Jiadong Yu,Yangyuxuan Yu,Huiting Zeng,Danni Zhang,Yunjie Zhang,Zhaoyu Zhang,Zhiheng Zhang,Xiaofeng Zheng,Peirong Zhou,Linyan Zhong,Xiaoyin Zong,Ying Zhao,Zhenxin Chen,Lin Ding,Xiaoyu Gao,Bingbing Gong,Yichao Li,Yang Liao,Guang Ma,Tianyuan Ma,Xinrui Sun,Tianyi Wang,Han Xia,Ruobing Xian,Gen Ye,Tengfei Yu,Wentao Zhang,Yuxi Wang,Xi Gao,Mengdi Wang*

Main category: cs.AI

TL;DR: Recent advances in LLMs have not been fully explored in the humanities, especially history. AI faces challenges in historical reasoning including multimodal source interpretation, temporal inference, and cross-linguistic analysis. The paper introduces HistBench, a benchmark with 414 questions designed by experts to evaluate AI's historical reasoning. It spans various historical problems and languages. Due to the poor performance of LLMs on HistBench, the authors present HistAgent, a history-specific agent equipped with tools for OCR, translation, archival search, and image understanding. HistAgent significantly outperforms other models.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capabilities of LLMs in historical reasoning, which involves unique challenges such as multimodal source interpretation, temporal inference, and cross-linguistic analysis.

Method: Introduced HistBench, a benchmark for evaluating AI's historical reasoning, and presented HistAgent, a history-specific agent with tools for OCR, translation, archival search, and image understanding.

Result: HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2 on HistBench, significantly outperforming other LLMs and generalist agents.

Conclusion: The results demonstrate the limitations of existing LLMs and generalist agents in historical reasoning and highlight the advantages of HistAgent.

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress across domains, yet their capabilities in the humanities, particularly
history, remain underexplored. Historical reasoning poses unique challenges for
AI, involving multimodal source interpretation, temporal inference, and
cross-linguistic analysis. While general-purpose agents perform well on many
existing benchmarks, they lack the domain-specific expertise required to engage
with historical materials and questions. To address this gap, we introduce
HistBench, a new benchmark of 414 high-quality questions designed to evaluate
AI's capacity for historical reasoning and authored by more than 40 expert
contributors. The tasks span a wide range of historical problems-from factual
retrieval based on primary sources to interpretive analysis of manuscripts and
images, to interdisciplinary challenges involving archaeology, linguistics, or
cultural history. Furthermore, the benchmark dataset spans 29 ancient and
modern languages and covers a wide range of historical periods and world
regions. Finding the poor performance of LLMs and other agents on HistBench, we
further present HistAgent, a history-specific agent equipped with carefully
designed tools for OCR, translation, archival search, and image understanding
in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of
27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online
search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)
and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These
results highlight the limitations of existing LLMs and generalist agents and
demonstrate the advantages of HistAgent for historical reasoning.

</details>


### [125] [syftr: Pareto-Optimal Generative AI](https://arxiv.org/abs/2505.20266)
*Alexander Conway,Debadeepta Dey,Stefan Hackmann,Matthew Hausknecht,Michael Schmidt,Mark Steadman,Nick Volynets*

Main category: cs.AI

TL;DR: The paper presents syftr, a framework for efficient multi-objective search over RAG configurations using Bayesian Optimization. It finds cost-effective flows while maintaining accuracy and supports integration of new modules.


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation (RAG) pipelines are complex to build effectively due to the variety of components that need careful selection and tuning. The challenge is intensified with agentic paradigms and performance-sensitive applications.

Method: syftr performs multi-objective search across a wide range of RAG configurations, utilizing Bayesian Optimization to find Pareto-optimal flows. It includes a novel early-stopping mechanism to prune suboptimal candidates.

Result: syftr discovers flows that are about 9 times cheaper on average compared to the most accurate flows, while preserving most of their accuracy. It also allows for easy integration of new modules.

Conclusion: syftr offers an efficient way to optimize RAG pipelines, balancing cost and accuracy, and facilitating the development of high-performing generative AI systems.

Abstract: Retrieval-Augmented Generation (RAG) pipelines are central to applying large
language models (LLMs) to proprietary or dynamic data. However, building
effective RAG flows is complex, requiring careful selection among vector
databases, embedding models, text splitters, retrievers, and synthesizing LLMs.
The challenge deepens with the rise of agentic paradigms. Modules like
verifiers, rewriters, and rerankers-each with intricate hyperparameter
dependencies have to be carefully tuned. Balancing tradeoffs between latency,
accuracy, and cost becomes increasingly difficult in performance-sensitive
applications.
  We introduce syftr, a framework that performs efficient multi-objective
search over a broad space of agentic and non-agentic RAG configurations. Using
Bayesian Optimization, syftr discovers Pareto-optimal flows that jointly
optimize task accuracy and cost. A novel early-stopping mechanism further
improves efficiency by pruning clearly suboptimal candidates. Across multiple
RAG benchmarks, syftr finds flows which are on average approximately 9 times
cheaper while preserving most of the accuracy of the most accurate flows on the
Pareto-frontier. Furthermore, syftr's ability to design and optimize allows
integrating new modules, making it even easier and faster to realize
high-performing generative AI pipelines.

</details>


### [126] [Ten Principles of AI Agent Economics](https://arxiv.org/abs/2505.20273)
*Ke Yang,ChengXiang Zhai*

Main category: cs.AI

TL;DR: The paper discusses the transformation of AI-based autonomous agents and their impact on human society and economic systems. It presents ten principles of AI agent economics to understand their decision-making, influence on social interactions, and participation in the economy. The paper calls for future research into AI trustworthiness, ethical guidelines, and regulatory oversight.


<details>
  <summary>Details</summary>
Motivation: AI-based autonomous agents are rapidly developing human-like or superhuman intelligence, becoming dynamic participants in social and economic ecosystems. This raises critical questions about their integration into economic activities, ethical concerns, and the balance between utility and safety.

Method: This paper presents ten principles of AI agent economics, drawing on economics, decision theory, and ethics to explore fundamental questions regarding AI agents' evolution, impact on labor markets, and necessary ethical safeguards.

Result: The ten principles provide a framework for understanding how AI agents make decisions, influence social interactions, and participate in the broader economy, while accounting for their unique traits.

Conclusion: The paper emphasizes the urgency of future research into AI trustworthiness, ethical guidelines, and regulatory oversight to ensure AI agents contribute positively to human progress while addressing associated risks.

Abstract: The rapid rise of AI-based autonomous agents is transforming human society
and economic systems, as these entities increasingly exhibit human-like or
superhuman intelligence. From excelling at complex games like Go to tackling
diverse general-purpose tasks with large language and multimodal models, AI
agents are evolving from specialized tools into dynamic participants in social
and economic ecosystems. Their autonomy and decision-making capabilities are
poised to impact industries, professions, and human lives profoundly, raising
critical questions about their integration into economic activities, potential
ethical concerns, and the balance between their utility and safety.
  To address these challenges, this paper presents ten principles of AI agent
economics, offering a framework to understand how AI agents make decisions,
influence social interactions, and participate in the broader economy. Drawing
on economics, decision theory, and ethics, we explore fundamental questions,
such as whether AI agents might evolve from tools into independent entities,
their impact on labor markets, and the ethical safeguards needed to align them
with human values. These principles build on existing economic theories while
accounting for the unique traits of AI agents, providing a roadmap for their
responsible integration into human systems.
  Beyond theoretical insights, this paper highlights the urgency of future
research into AI trustworthiness, ethical guidelines, and regulatory oversight.
As we enter a transformative era, this work serves as both a guide and a call
to action, ensuring AI agents contribute positively to human progress while
addressing risks tied to their unprecedented capabilities.

</details>


### [127] [Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution](https://arxiv.org/abs/2505.20286)
*Jiahao Qiu,Xuan Qi,Tongcheng Zhang,Xinzhe Juan,Jiacheng Guo,Yifu Lu,Yimin Wang,Zixin Yao,Qihan Ren,Xun Jiang,Xing Zhou,Dongrui Liu,Ling Yang,Yue Wu,Kaixuan Huang,Shilong Liu,Hongru Wang,Mengdi Wang*

Main category: cs.AI

TL;DR: Alita is a generalist agent that follows the principle of simplicity for scalable reasoning, achieving top results in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create an adaptable, scalable, and generalizable agent by reducing dependency on manually predefined tools and enhancing self-evolution.

Method: Equipped with one component for problem-solving and uses MCPs to autonomously construct, refine, and reuse external capabilities.

Result: Achieves 75.15% pass@1 and 87.27% pass@3 accuracy on GAIA benchmark, outperforming more complex systems in other benchmarks.

Conclusion: Alita demonstrates that simplicity and self-evolution can lead to superior performance across various tasks.

Abstract: Recent advances in large language models (LLMs) have enabled agents to
autonomously perform complex, open-ended tasks. However, many existing
frameworks depend heavily on manually predefined tools and workflows, which
hinder their adaptability, scalability, and generalization across domains. In
this work, we introduce Alita--a generalist agent designed with the principle
of "Simplicity is the ultimate sophistication," enabling scalable agentic
reasoning through minimal predefinition and maximal self-evolution. For minimal
predefinition, Alita is equipped with only one component for direct
problem-solving, making it much simpler and neater than previous approaches
that relied heavily on hand-crafted, elaborate tools and workflows. This clean
design enhances its potential to generalize to challenging questions, without
being limited by tools. For Maximal self-evolution, we enable the creativity of
Alita by providing a suite of general-purpose components to autonomously
construct, refine, and reuse external capabilities by generating task-related
model context protocols (MCPs) from open source, which contributes to scalable
agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3
accuracy, which is top-ranking among general-purpose agents, on the GAIA
benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on
Mathvista and PathVQA, outperforming many agent systems with far greater
complexity. More details will be updated at
$\href{https://github.com/CharlesQ9/Alita}{https://github.com/CharlesQ9/Alita}$.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [128] [Model-Distributed Inference for Large Language Models at the Edge](https://arxiv.org/abs/2505.18164)
*Davide Macario,Hulya Seferoglu,Erdem Koyuncu*

Main category: cs.LG

TL;DR: MDI-LLM is a framework that allows large-language models to be deployed across low-power edge devices by partitioning the model and using 'recurrent pipeline parallelism' for efficient inference.


<details>
  <summary>Details</summary>
Motivation: To enable the deployment of large-language models on resource-constrained edge devices, overcoming individual device memory limitations.

Method: Model-Distributed Inference for Large-Language Models (MDI-LLM) partitions LLMs into multiple sections assigned to different devices. Devices communicate through intermediate activation vectors and use 'recurrent pipeline parallelism' to minimize idle time and allow parallel processing.

Result: MDI-LLM successfully deploys LLMs beyond single-device memory capacity, increases token generation throughput with more devices, and reduces per-device memory consumption.

Conclusion: MDI-LLM provides an effective solution for deploying advanced LLMs on low-power edge devices, improving inference capabilities on cost-effective hardware.

Abstract: We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM),
a novel framework designed to facilitate the deployment of state-of-the-art
large-language models (LLMs) across low-power devices at the edge. This is
accomplished by dividing the model into multiple partitions, which are then
assigned to different devices/nodes within the network. These nodes exchange
intermediate activation vectors via device-to-device links, enabling
collaborative computation. To enhance the efficiency of this process, we
propose the "recurrent pipeline parallelism" technique, which reduces idle time
on each device and facilitates parallel inference during the generation of
multiple text sequences. By leveraging the combined computational resources of
multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the
memory capacity of individual devices, making it possible to perform inference
on low-cost hardware. Furthermore, as the number of participating devices
increases, MDI-LLM boosts token generation throughput and reduces memory
consumption per device.

</details>


### [129] [Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression](https://arxiv.org/abs/2505.18166)
*Jacob Sander,David Moe,Achraf Cohen,Brent Venable,Venkat Dasari,Brian Jalaian*

Main category: cs.LG

TL;DR: 现代基础模型通常通过结构化剪枝和再训练来满足边缘部署的严格计算、内存和连接限制。本文研究了在只对MLP块进行层-wise L2-norm剪枝的情况下，不同的再训练损失函数（交叉熵微调与基于KL散度的自蒸馏）对模型性能的影响。实验表明，在相同的剪枝计划下，基于KL散度的自蒸馏方法在测试准确率上可以匹配或超过交叉熵微调方法，说明即使采用基本的MLP-only剪枝策略，损失函数的选择也显著影响压缩模型的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的剪枝方案针对整个Transformer架构，但本文旨在探讨在固定baseline（仅对MLP块进行L2-norm剪枝）下，不同再训练损失函数对模型性能的具体影响，尤其是在资源受限环境中的表现。

Method: 使用层-wise L2-norm剪枝方法仅对MLP块进行剪枝，并比较两种再训练方法：(i) 使用交叉熵（Cross-Entropy）进行微调，需要标记数据；(ii) 使用KL散度进行自蒸馏，仅利用教师模型的logits而无需标签。两者均在OLMo2-7B-SFT模型上进行评估，该模型适用于边缘网络常见的间歇性或无连接场景。

Result: 在相同的剪枝计划下，基于KL散度的自蒸馏方法在测试准确率上能够匹配或超过交叉熵微调方法。

Conclusion: 即使采用简单的MLP-only剪枝策略，再训练损失函数的选择对压缩模型在资源受限环境中的恢复效果具有显著影响。

Abstract: Modern foundational models are often compressed via a combination of
structured pruning and re-training to meet the strict compute, memory, and
connectivity constraints of edge deployments. While state-of-the-art pruning
schemes target the entire Transformer, we adopt a simple, layer-wise L2-norm
pruning on only the MLP blocks as a fixed baseline. Our focus is not on
achieving maximal compression, but on isolating the impact of the re-training
loss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires
labeled data, versus (ii) Self-Distillation with KL-divergence, which leverages
only teacher logits (no labels) (L2PSD). We evaluate both pipelines on the
OLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied
connectivity scenarios typical of edge networks. Under identical pruning
schedules, KL-based distillation matches or exceeds CE fine-tuning in test
accuracy, demonstrating that, even with a basic MLP-only pruning, the choice of
loss function materially affects compressed model recovery in
resource-constrained environments.

</details>


### [130] [Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation](https://arxiv.org/abs/2505.18168)
*Feifan Wang,Tengfei Song,Minggui He,Chang Su,Zhanglin Wu,Hao Yang,Wenming Zheng,Osamu Yoshie*

Main category: cs.LG

TL;DR: 提出了一种名为SEKE的方法，通过结合人类先验知识和不确定性感知蒙特卡洛采样技术生成高质量的多粒度情感注释数据，构建了面部情感指令数据集(FEID)和分析基准(FEAB)，显著提升了视觉大语言模型在面部情感感知任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉大语言模型（VLLM）在面部情感感知方面受到高质量标注数据不足的限制，而生成高质的粗细粒度面部情感分析数据需要高昂的专业成本。

Method: 提出了自验证与情感知识增强（SEKE）方法，利用封闭源VLLM生成经济高效的多粒度情感分析数据，并结合人类先验知识和Uncertainty-Aware Monte Carlo采样技术提高注释可靠性，最终构建FEID数据集和FEAB基准。

Result: 该方法在三个下游面部情感分析任务中显著优于现有最先进方法。

Conclusion: SEKE方法能有效提升VLLM在面部情感感知方面的性能，为未来研究提供了新的方向和资源。

Abstract: Facial emotion perception in the vision large language model (VLLM) is
crucial for achieving natural human-machine interaction. However, creating
high-quality annotations for both coarse- and fine-grained facial emotion
analysis demands costly expertise. The lack of such high-quality instruction
data limits the performance of VLLMs in facial emotion perception. To address
this, we propose a self-verification approach with emotion knowledge
enhancement (SEKE), which generates high-quality instruction data for
multi-grained emotion analysis cost-effectively using closed-source VLLM. This
approach integrates prior human knowledge to VLLM inference, guided by the
inherent correlations between three grained levels of emotion descriptions,
i.e., discrete expression, valence-arousal, and action unit, to reliably
generate comprehensive annotations. A self-verification strategy with
Uncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to
efficiently extract more accurate VLLM predictions, further improving
annotation reliability. Consequently, we construct a facial emotion instruction
dataset (FEID) containing three comprehensive descriptions, which provides
coarse- and fine-grained emotional information for effective model training.
Additionally, we introduce a facial emotion analysis benchmark (FEAB) to
measure the VLLM's corresponding ability. Our method significantly outperforms
state-of-the-art methods on three downstream facial emotion analysis tasks.

</details>


### [131] [Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction](https://arxiv.org/abs/2505.18169)
*Nischal Mandal*

Main category: cs.LG

TL;DR: The paper presents a novel Multi-Task Physics-Informed Neural Network (PINN) for predicting Electrodermal Activity (EDA) and emotion classification using wearable sensors. It achieves high accuracy and outperforms existing models, while providing interpretable physical parameters.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced model that can simultaneously predict EDA and classify emotions using wearable sensor data, improving performance and interpretability compared to existing methods.

Method: A Multi-Task Physics-Informed Neural Network (PINN) is designed with dual outputs for EDA prediction and emotion classification. The model integrates psychological self-report features and a physics-inspired differential equation representing EDA dynamics, using a custom loss function that combines EDA regression, emotion classification, and a physics residual term.

Result: The model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919, and F1-score of 94.08 percent using 5-fold cross-validation. It outperforms classical models like SVR and XGBoost, as well as ablated variants. The learned physical parameters are interpretable and stable.

Conclusion: This work introduces the first multi-task PINN framework for wearable emotion recognition, offering improved performance, generalizability, and model transparency, laying the foundation for future interpretable and multimodal applications in healthcare and human-computer interaction.

Abstract: Understanding and predicting human emotional and physiological states using
wearable sensors has important applications in stress monitoring, mental health
assessment, and affective computing. This study presents a novel Multi-Task
Physics-Informed Neural Network (PINN) that performs Electrodermal Activity
(EDA) prediction and emotion classification simultaneously, using the publicly
available WESAD dataset. The model integrates psychological self-report
features (PANAS and SAM) with a physics-inspired differential equation
representing EDA dynamics, enforcing biophysically grounded constraints through
a custom loss function. This loss combines EDA regression, emotion
classification, and a physics residual term for improved interpretability.
  The architecture supports dual outputs for both tasks and is trained under a
unified multi-task framework. Evaluated using 5-fold cross-validation, the
model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919,
and F1-score of 94.08 percent. These results outperform classical models such
as SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only
models.
  In addition, the learned physical parameters including decay rate (alpha_0),
emotional sensitivity (beta), and time scaling (gamma) are interpretable and
stable across folds, aligning with known principles of human physiology. This
work is the first to introduce a multi-task PINN framework for wearable emotion
recognition, offering improved performance, generalizability, and model
transparency. The proposed system provides a foundation for future
interpretable and multimodal applications in healthcare and human-computer
interaction.

</details>


### [132] [Robust Knowledge Graph Embedding via Denoising](https://arxiv.org/abs/2505.18171)
*Tengwei Song,Xudong Ma,Yang Liu,Jie Luo*

Main category: cs.LG

TL;DR: The paper presents a novel framework, Robust Knowledge Graph Embedding via Denoising, which enhances the robustness of KGE models on noisy triples and proposes certified robustness evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To obtain robust knowledge graph embedding under perturbation in the embedding space.

Method: Treat KGE methods as energy-based models and leverage the connection between denoising and score matching to train a robust denoising KGE model.

Result: The framework shows superior performance compared to existing state-of-the-art KGE methods when faced with perturbed entity embedding.

Conclusion: Robust Knowledge Graph Embedding via Denoising is an effective framework for enhancing the robustness of KGE models.

Abstract: We focus on obtaining robust knowledge graph embedding under perturbation in
the embedding space. To address these challenges, we introduce a novel
framework, Robust Knowledge Graph Embedding via Denoising, which enhances the
robustness of KGE models on noisy triples. By treating KGE methods as
energy-based models, we leverage the established connection between denoising
and score matching, enabling the training of a robust denoising KGE model.
Furthermore, we propose certified robustness evaluation metrics for KGE methods
based on the concept of randomized smoothing. Through comprehensive experiments
on benchmark datasets, our framework consistently shows superior performance
compared to existing state-of-the-art KGE methods when faced with perturbed
entity embedding.

</details>


### [133] [Should We Simultaneously Calibrate Multiple Computer Models?](https://arxiv.org/abs/2505.18176)
*Jonathan Tammer Eweis-Labolle,Tyler Johnson,Xiangyu Sun,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 本研究提出了一种基于定制神经网络的多模型同时校准方法，通过学习每个校准参数的独特概率分布和开发新的损失函数，实现了对多个不同保真度计算机模型的同时校准。这种方法在分析和工程问题测试中表现出提高预测准确性的潜力，但在高维输入空间中可能存在非可识别性问题。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的应用中设计师可以接触到具有不同保真度和成本的多个计算机模型，传统的逐个校准模型的方法可能不再是最优选择。因此，本文探讨了同时校准多个计算机模型的潜在优势。

Method: 研究人员开发了一个基于定制神经网络的概率框架，该框架能够校准任意数量的计算机模型。具体方法包括：考虑多响应模型的特点及校准参数的变化；为每个模型的每个校准参数学习独特的概率分布；开发一个损失函数以使神经网络能够模拟所有数据源并校准模型；以及构建一个可视化的潜在空间，用于识别模型形式误差。

Result: 该方法在分析和工程问题上的测试结果表明，它可以提高预测准确性。然而，在由基础物理约束的高维输入空间中，可能会出现非可识别性问题。

Conclusion: 同时校准多个计算机模型的方法有潜力改善预测精度，但需要解决高维输入空间中的非可识别性问题。这表明在实际应用中需要根据具体情况权衡该方法的优势与局限性。

Abstract: In an increasing number of applications designers have access to multiple
computer models which typically have different levels of fidelity and cost.
Traditionally, designers calibrate these models one at a time against some
high-fidelity data (e.g., experiments). In this paper, we question this
tradition and assess the potential of calibrating multiple computer models at
the same time. To this end, we develop a probabilistic framework that is
founded on customized neural networks (NNs) that are designed to calibrate an
arbitrary number of computer models. In our approach, we (1) consider the fact
that most computer models are multi-response and that the number and nature of
calibration parameters may change across the models, and (2) learn a unique
probability distribution for each calibration parameter of each computer model,
(3) develop a loss function that enables our NN to emulate all data sources
while calibrating the computer models, and (4) aim to learn a visualizable
latent space where model-form errors can be identified. We test the performance
of our approach on analytic and engineering problems to understand the
potential advantages and pitfalls in simultaneous calibration of multiple
computer models. Our method can improve predictive accuracy, however, it is
prone to non-identifiability issues in higher-dimensional input spaces that are
normally constrained by underlying physics.

</details>


### [134] [FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations](https://arxiv.org/abs/2505.18177)
*Zhizhong Tan,Jiexin Zheng,Xingxing Yang,Chi Zhang,Weiping Deng,Wenyong Wang*

Main category: cs.LG

TL;DR: 为了应对跨境推荐中的隐私保护和数据不足问题，本文提出了FedGRec方法，通过联邦图学习在不泄露隐私的情况下提升多域推荐性能。


<details>
  <summary>Details</summary>
Motivation: 跨境数据共享中，由于隐私保护法规严格，导致用于模型训练的数据不足，同时现有联邦学习方法在高度异构的图数据上表现不佳。

Method: 提出了一种名为FedGRec的隐私保护联邦图学习方法，利用分布式多域数据捕捉用户偏好，通过本地子图的协作信号丰富表示学习，并采用动态时空建模整合全局和本地用户偏好，自动过滤无关行为以减少噪声干扰，以及通过个性化的联邦聚合策略适应全球偏好到异构领域数据。

Result: 在三个数据集上的广泛实验表明，FedGRec在跨境推荐中持续优于单域和跨域基线方法，同时有效保护数据隐私。

Conclusion: FedGRec为跨境推荐提供了一种有效的解决方案，在保证隐私安全的同时提升了推荐性能。

Abstract: Due to the highly sensitive nature of certain data in cross-border sharing,
collaborative cross-border recommendations and data sharing are often subject
to stringent privacy protection regulations, resulting in insufficient data for
model training. Consequently, achieving efficient cross-border business
recommendations while ensuring privacy security poses a significant challenge.
Although federated learning has demonstrated broad potential in collaborative
training without exposing raw data, most existing federated learning-based GNN
training methods still rely on federated averaging strategies, which perform
suboptimally on highly heterogeneous graph data. To address this issue, we
propose FedGRec, a privacy-preserving federated graph learning method for
cross-border recommendations. FedGRec captures user preferences from
distributed multi-domain data to enhance recommendation performance across all
domains without privacy leakage. Specifically, FedGRec leverages collaborative
signals from local subgraphs associated with users or items to enrich their
representation learning. Additionally, it employs dynamic spatiotemporal
modeling to integrate global and local user preferences in real time based on
business recommendation states, thereby deriving the final representations of
target users and candidate items. By automatically filtering relevant
behaviors, FedGRec effectively mitigates noise interference from unreliable
neighbors. Furthermore, through a personalized federated aggregation strategy,
FedGRec adapts global preferences to heterogeneous domain data, enabling
collaborative learning of user preferences across multiple domains. Extensive
experiments on three datasets demonstrate that FedGRec consistently outperforms
competitive single-domain and cross-domain baselines while effectively
preserving data privacy in cross-border recommendations.

</details>


### [135] [Less is More: Multimodal Region Representation via Pairwise Inter-view Learning](https://arxiv.org/abs/2505.18178)
*Min Namgung,Yijun Lin,JangHyeon Lee,Yao-Yi Chiang*

Main category: cs.LG

TL;DR: The paper introduces CooKIE, a novel information factorization approach for region representation learning (RRL) that captures both shared and unique multimodal representations. Evaluated on regression and classification tasks in NYC and Delhi, CooKIE outperforms existing methods with fewer parameters and FLOPs.


<details>
  <summary>Details</summary>
Motivation: Existing RRL methods using contrastive learning often focus on shared information between two modalities but overlook task-relevant unique information specific to each modality. Extending information factorization beyond two modalities is challenging due to the combinatorial increase in learning objectives and model complexity.

Method: CooKIE uses a pairwise inter-view learning approach to capture high-order information without modeling high-order dependency, avoiding exhaustive combinations. This allows it to capture both shared and unique representations from various geospatial data modalities.

Result: CooKIE was evaluated on three regression tasks and a land use classification task in New York City and Delhi, India. It outperformed existing RRL methods and a factorized RRL model, effectively capturing multimodal information with fewer training parameters and FLOPs.

Conclusion: CooKIE presents an effective solution for RRL by capturing both shared and unique multimodal representations through information factorization. The method achieves better performance with reduced computational resources.

Abstract: With the increasing availability of geospatial datasets, researchers have
explored region representation learning (RRL) to analyze complex region
characteristics. Recent RRL methods use contrastive learning (CL) to capture
shared information between two modalities but often overlook task-relevant
unique information specific to each modality. Such modality-specific details
can explain region characteristics that shared information alone cannot
capture. Bringing information factorization to RRL can address this by
factorizing multimodal data into shared and unique information. However,
existing factorization approaches focus on two modalities, whereas RRL can
benefit from various geospatial data. Extending factorization beyond two
modalities is non-trivial because modeling high-order relationships introduces
a combinatorial number of learning objectives, increasing model complexity. We
introduce Cross modal Knowledge Injected Embedding, an information
factorization approach for RRL that captures both shared and unique
representations. CooKIE uses a pairwise inter-view learning approach that
captures high-order information without modeling high-order dependency,
avoiding exhaustive combinations. We evaluate CooKIE on three regression tasks
and a land use classification task in New York City and Delhi, India. Results
show that CooKIE outperforms existing RRL methods and a factorized RRL model,
capturing multimodal information with fewer training parameters and
floating-point operations per second (FLOPs). We release the code:
https://github.com/MinNamgung/CooKIE.

</details>


### [136] [GAIA: A Foundation Model for Operational Atmospheric Dynamics](https://arxiv.org/abs/2505.18179)
*Ata Akbari Asanjan,Olivia Alexander,Tom Berg,Clara Zhang,Matt Yang,Jad Makki,Disha Shidham,Srija Chakraborty,William Bender,Stephen Peng,Arun Ravindran,Olivier Raiman,David Potere,David Bell*

Main category: cs.LG

TL;DR: The paper introduces GAIA, a foundation model combining MAE and DINO for analyzing atmospheric patterns in satellite imagery. It excels in reconstructing missing regions and estimating precipitation patterns with limited training data.


<details>
  <summary>Details</summary>
Motivation: To advance self-supervised learning techniques for atmospheric science, specifically addressing challenges like reconstructing missing regions and estimating precipitation patterns in satellite imagery.

Method: Combines masked autoencoders (MAE) and self-DIstillation with NO labels (DINO) to capture both local features and global dependencies in satellite data. Utilizes this integrated approach for downstream tasks such as gap-filling and precipitation estimation.

Result: Demonstrates superior temporal pattern capture compared to standard MAE approaches, robust performance in downstream tasks, strong gap-filling capabilities across varying mask ratios, and accurate precipitation estimation with limited training data. Achieves a false alarm ratio of 0.088 and structural similarity of 0.881.

Conclusion: Represents an advancement in self-supervised learning for atmospheric science, providing a foundation for improved weather monitoring and climate analysis.

Abstract: We present the GAIA (Geospatial Artificial Intelligence for Atmospheres)
Foundation Model, a novel model that combines masked autoencoders (MAE) and
self-DIstillation with NO labels (DINO) for analyzing global atmospheric
patterns in satellite imagery. By integrating these complementary
self-supervised learning approaches, our model simultaneously captures both
local features and global dependencies. We address two critical challenges in
satellite data analysis: reconstructing missing regions and estimating
precipitation patterns as our first downstream tasks. The model demonstrates
superior temporal pattern capture compared to standard MAE approaches, while
maintaining robust performance in downstream tasks. Our experimental results
show strong gap-filling capabilities across varying mask ratios and accurate
precipitation estimation with limited training data, achieving a false alarm
ratio of 0.088 and structural similarity of 0.881. This work represents an
advancement in self-supervised learning for atmospheric science, providing a
foundation for improved weather monitoring and climate analysis. The trained
model weights and accompanying code are publicly available as open-source on
Hugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.

</details>


### [137] [2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision](https://arxiv.org/abs/2505.18181)
*Yunrui Li,Hao Xu,Pengyu Hong*

Main category: cs.LG

TL;DR: The paper introduces 2DNMRGym, the first annotated experimental dataset for ML-based molecular representation learning in 2D NMR, containing over 22,000 HSQC spectra with corresponding molecular graphs and SMILES strings. It uses a surrogate supervision setup and provides benchmark results using GNN models.


<details>
  <summary>Details</summary>
Motivation: Accurately interpreting 2D NMR data is labor-intensive and error-prone, requiring highly trained domain experts, especially for complex molecules. Machine Learning (ML) has potential in 2D NMR analysis but progress has been limited by the lack of large-scale and high-quality annotated datasets.

Method: Introduced 2DNMRGym, an annotated experimental dataset designed for ML-based molecular representation learning in 2D NMR. The dataset includes over 22,000 HSQC spectra along with their molecular graphs and SMILES strings. A surrogate supervision setup is adopted where models are trained using algorithm-generated annotations and evaluated on human-annotated gold-standard labels.

Result: Benchmark results were provided using a series of 2D and 3D GNN and GNN transformer models, establishing a strong foundation for future work. 2DNMRGym supports scalable model training and introduces a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks.

Conclusion: 2DNMRGym is introduced as the first annotated experimental dataset for ML-based molecular representation learning in 2D NMR, supporting scalable model training and introducing a chemically meaningful benchmark. The data and code are open-source and available on Huggingface and Github.

Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy,
particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays
a critical role in elucidating molecular structures, interactions, and
electronic properties. However, accurately interpreting 2D NMR data remains
labor-intensive and error-prone, requiring highly trained domain experts,
especially for complex molecules. Machine Learning (ML) holds significant
potential in 2D NMR analysis by learning molecular representations and
recognizing complex patterns from data. However, progress has been limited by
the lack of large-scale and high-quality annotated datasets. In this work, we
introduce 2DNMRGym, the first annotated experimental dataset designed for
ML-based molecular representation learning in 2D NMR. It includes over 22,000
HSQC spectra, along with the corresponding molecular graphs and SMILES strings.
Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained
using algorithm-generated annotations derived from a previously validated
method and evaluated on a held-out set of human-annotated gold-standard labels.
This enables rigorous assessment of a model's ability to generalize from
imperfect supervision to expert-level interpretation. We provide benchmark
results using a series of 2D and 3D GNN and GNN transformer models,
establishing a strong foundation for future work. 2DNMRGym supports scalable
model training and introduces a chemically meaningful benchmark for evaluating
atom-level molecular representations in NMR-guided structural tasks. Our data
and code is open-source and available on Huggingface and Github.

</details>


### [138] [Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry](https://arxiv.org/abs/2505.18193)
*Antoine Collas,Ce Ju,Nicolas Salvy,Bertrand Thirion*

Main category: cs.LG

TL;DR: 生成逼真的脑连接矩阵对于分析脑组织的人群异质性、理解疾病和增强数据在具有挑战性的分类问题中至关重要。本文提出了一种名为DiffeoCFM的方法，通过利用欧几里得空间上的全局微分同胚诱导的回拉度量，在矩阵流形上实现条件流匹配（CFM）。该方法在三个大规模fMRI数据集和两个EEG运动想象数据集上进行了评估，实现了快速训练并取得了最先进的性能，同时保留了流形约束。


<details>
  <summary>Details</summary>
Motivation: 生成逼真的脑连接矩阵是分析脑组织人群异质性、理解疾病和增强数据在具有挑战性的分类问题中的关键。然而，使用黎曼工具通常需要重新定义核心操作，使得生成建模计算效率低下。

Method: 本文提出了DiffeoCFM方法，通过利用欧几里得空间上的全局微分同胚诱导的回拉度量，在矩阵流形上实现条件流匹配（CFM）。具体来说，黎曼CFM与这些度量等价于数据转换后的标准CFM应用。这一等价性允许有效的向量场学习，并通过标准ODE求解器实现快速采样。本文实例化了DiffeoCFM的两种不同设置：协方差矩阵的矩阵对数和相关矩阵的归一化Cholesky分解。

Result: DiffeoCFM方法在三个大规模fMRI数据集和两个EEG运动想象数据集上进行了评估，实现了快速训练并取得了最先进的性能，同时保留了流形约束。

Conclusion: DiffeoCFM方法为在矩阵流形上实现条件流匹配提供了一种有效的方法，适用于生成逼真的脑连接矩阵，从而有助于分析脑组织的人群异质性、理解疾病和增强数据在具有挑战性的分类问题中的应用。

Abstract: Generating realistic brain connectivity matrices is key to analyzing
population heterogeneity in brain organization, understanding disease, and
augmenting data in challenging classification problems. Functional connectivity
matrices lie in constrained spaces--such as the set of symmetric positive
definite or correlation matrices--that can be modeled as Riemannian manifolds.
However, using Riemannian tools typically requires redefining core operations
(geodesics, norms, integration), making generative modeling computationally
inefficient. In this work, we propose DiffeoCFM, an approach that enables
conditional flow matching (CFM) on matrix manifolds by exploiting pullback
metrics induced by global diffeomorphisms on Euclidean spaces. We show that
Riemannian CFM with such metrics is equivalent to applying standard CFM after
data transformation. This equivalence allows efficient vector field learning,
and fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two
different settings: the matrix logarithm for covariance matrices and the
normalized Cholesky decomposition for correlation matrices. We evaluate
DiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from
2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with
over 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables
fast training and achieves state-of-the-art performance, all while preserving
manifold constraints.

</details>


### [139] [Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs](https://arxiv.org/abs/2505.18221)
*Sharad Duwal,Mir Nafis Sharear Shopnil,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.LG

TL;DR: The paper presents a graph-based method using GNNs to detect multimodal out-of-context misinformation by evaluating the consistency between images and captions, achieving 93.05% accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for detecting misinformation, including LLMs and LVLMs, struggle with contextualization, leading to inaccuracies when dealing with out-of-context multimedia content.

Method: The method constructs two graph representations - an evidence graph from online textual evidence and a claim graph from the caption - and uses GNNs to encode and compare these graphs to assess the truthfulness of image-caption pairs.

Result: The proposed method achieves 93.05% detection accuracy on the evaluation set, outperforming the second-best method (an LLM) by 2.82%.

Conclusion: The graph-based method demonstrates superior performance in detecting out-of-context misinformation compared to larger models like LLMs, advocating for the use of smaller, task-specific approaches.

Abstract: Multimodal out-of-context (OOC) misinformation is misinformation that
repurposes real images with unrelated or misleading captions. Detecting such
misinformation is challenging because it requires resolving the context of the
claim before checking for misinformation. Many current methods, including LLMs
and LVLMs, do not perform this contextualization step. LLMs hallucinate in
absence of context or parametric knowledge. In this work, we propose a
graph-based method that evaluates the consistency between the image and the
caption by constructing two graph representations: an evidence graph, derived
from online textual evidence, and a claim graph, from the claim in the caption.
Using graph neural networks (GNNs) to encode and compare these representations,
our framework then evaluates the truthfulness of image-caption pairs. We create
datasets for our graph-based method, evaluate and compare our baseline model
against popular LLMs on the misinformation detection task. Our method scores
$93.05\%$ detection accuracy on the evaluation set and outperforms the
second-best performing method (an LLM) by $2.82\%$, making a case for smaller
and task-specific methods.

</details>


### [140] [Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality](https://arxiv.org/abs/2505.18227)
*Zhenglun Kong,Yize Li,Fanhu Zeng,Lei Xin,Shvat Messica,Xue Lin,Pu Zhao,Manolis Kellis,Hao Tang,Marinka Zitnik*

Main category: cs.LG

TL;DR: 在Transformer架构中，分词传统上用于提高效率。本文重新定义了分词的作用，认为它不仅是效率措施，更是生成建模的基本原则，可以促进多模态整合、减少幻觉、保持连贯性并增强训练稳定性等。


<details>
  <summary>Details</summary>
Motivation: 尽管分词在视觉和语言领域已被用作提高效率的策略，但在大型生成模型时代，其作用应超越效率，成为生成建模的核心原则。

Method: 提出将分词作为生成建模的基本原则，并探讨其在多个方面的应用，包括多模态集成、减少过拟合、保持长输入的一致性和增强训练稳定性等。

Result: 重新定义了分词的角色，指出了它对模型架构和学习策略的深远影响，有助于提高模型的鲁棒性和可解释性。

Conclusion: 分词不仅是一种效率手段，更是一种基本原则，可以推动新的模型架构和学习策略的发展，从而改善生成建模的目标一致性。

Abstract: In Transformer architectures, tokens\textemdash discrete units derived from
raw data\textemdash are formed by segmenting inputs into fixed-length chunks.
Each token is then mapped to an embedding, enabling parallel attention
computations while preserving the input's essential information. Due to the
quadratic computational complexity of transformer self-attention mechanisms,
token reduction has primarily been used as an efficiency strategy. This is
especially true in single vision and language domains, where it helps balance
computational costs, memory usage, and inference latency. Despite these
advances, this paper argues that token reduction should transcend its
traditional efficiency-oriented role in the era of large generative models.
Instead, we position it as a fundamental principle in generative modeling,
critically influencing both model architecture and broader applications.
Specifically, we contend that across vision, language, and multimodal systems,
token reduction can: (i) facilitate deeper multimodal integration and
alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain
coherence over long inputs, and (iv) enhance training stability, etc. We
reframe token reduction as more than an efficiency measure. By doing so, we
outline promising future directions, including algorithm design, reinforcement
learning-guided token reduction, token optimization for in-context learning,
and broader ML and scientific domains. We highlight its potential to drive new
model architectures and learning strategies that improve robustness, increase
interpretability, and better align with the objectives of generative modeling.

</details>


### [141] [Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models](https://arxiv.org/abs/2505.18230)
*Louis Béthune,David Vigouroux,Yilun Du,Rufin VanRullen,Thomas Serre,Victor Boutin*

Main category: cs.LG

TL;DR: 本文提出了一种从预训练的能量基础模型（EBM）中直接推导黎曼度量的方法，这些度量能够定义空间变化的距离，从而计算出遵循数据流形内在几何的测地线。实验结果表明，基于EBM的度量在高维设置下显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 在高维空间中寻找两个数据点之间的最短路径是一项复杂的任务，尤其是在数据位于弯曲的流形上时。现有的黎曼度量估计方法面临高维挑战，因此需要一种新方法来解决这一问题。

Method: 作者提出从预训练的能量基础模型（EBM）中推导黎曼度量，并引入了两种新颖的度量。这些度量允许计算与数据流形几何一致的测地线。该方法通过评估合成数据集、旋转字符图像和高分辨率自然图像（嵌入VAE潜在空间）上的性能进行验证。

Result: 实验结果表明，基于EBM的黎曼度量在复杂数据集上始终优于现有基线方法，特别是在高维情况下表现更优。测地线更接近数据流形并表现出更低的曲率畸变。

Conclusion: 本文首次实现了从EBM中推导黎曼度量，为生成建模和模拟提供了数据感知的测地线计算方法，推动了几何驱动学习的发展。

Abstract: What is the shortest path between two data points lying in a high-dimensional
space? While the answer is trivial in Euclidean geometry, it becomes
significantly more complex when the data lies on a curved manifold -- requiring
a Riemannian metric to describe the space's local curvature. Estimating such a
metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly
from pretrained Energy-Based Models (EBMs) -- a class of generative models that
assign low energy to high-density regions. These metrics define spatially
varying distances, enabling the computation of geodesics -- shortest paths that
follow the data manifold's intrinsic geometry. We introduce two novel metrics
derived from EBMs and show that they produce geodesics that remain closer to
the data manifold and exhibit lower curvature distortion, as measured by
alignment with ground-truth trajectories. We evaluate our approach on
increasingly complex datasets: synthetic datasets with known data density,
rotated character images with interpretable geometry, and high-resolution
natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established
baselines, especially in high-dimensional settings. Our work is the first to
derive Riemannian metrics from EBMs, enabling data-aware geodesics and
unlocking scalable, geometry-driven learning for generative modeling and
simulation.

</details>


### [142] [NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache](https://arxiv.org/abs/2505.18231)
*Donghyun Son,Euntae Choi,Sungjoo Yoo*

Main category: cs.LG

TL;DR: NSNQuant is a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache in Large Language Models, providing robust performance and significant throughput gains.


<details>
  <summary>Details</summary>
Motivation: Existing Vector Quantization approaches for alleviating memory intensity in LLM inference are susceptible to distribution shift due to reliance on calibration datasets.

Method: NSNQuant applies a three-step transformation - token-wise normalization, channel-wise centering, and a second token-wise normalization with Hadamard transform - to align token distribution with the standard normal distribution, enabling robust, calibration-free vector quantization.

Result: NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3× throughput gain over full-precision baselines.

Conclusion: NSNQuant provides a effective solution for low-bit compression of the KV cache in LLMs, reducing memory intensity and improving inference efficiency.

Abstract: Large Language Model (LLM) inference is typically memory-intensive,
especially when processing large batch sizes and long sequences, due to the
large size of key-value (KV) cache. Vector Quantization (VQ) is recently
adopted to alleviate this issue, but we find that the existing approach is
susceptible to distribution shift due to its reliance on calibration datasets.
To address this limitation, we introduce NSNQuant, a calibration-free Vector
Quantization (VQ) technique designed for low-bit compression of the KV cache.
By applying a three-step transformation-1) a token-wise normalization
(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise
normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns
the token distribution with the standard normal distribution. This alignment
enables robust, calibration-free vector quantization using a single reusable
codebook. Extensive experiments show that NSNQuant consistently outperforms
prior methods in both 1-bit and 2-bit settings, offering strong generalization
and up to 3$\times$ throughput gain over full-precision baselines.

</details>


### [143] [ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning](https://arxiv.org/abs/2505.18232)
*Mingkuan Feng,Jinyang Wu,Siyuan Liu,Shuai Zhang,Hongjian Fang,Ruihan Jin,Feihu Che,Pengpeng Shao,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: ELDeR是一种通过数据驱动的正则化逐层剪枝方法，有效减少了大型语言模型（LLMs）的计算和内存成本，同时保留了模型的语言建模能力并显著降低了恢复微调（RFT）的计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的高计算和内存成本限制了其广泛应用。尽管已有研究表明LLMs具有稀疏性，可以利用这种特性进行剪枝，但传统的剪枝方法（先剪枝后微调）会导致性能不可逆地下降，需要昂贵的恢复微调来维持性能。

Method: 提出了一种新的范式：先正则化后剪枝。具体来说，引入了ELDeR方法，该方法首先为每个Transformer层的输出乘以一个初始权重，然后使用少量数据迭代学习这些权重。接着对权重较小的层的输入与输出差异施加正则化，促使信息转移到剩余层中，从而减少直接参数移除带来的信息损失。

Result: 实验结果表明，ELDeR在显著降低RFT计算成本的同时，相较于强大的逐层结构化剪枝方法表现出更优的性能，并且由于是逐层剪枝方法，端到端加速效果明显。

Conclusion: ELDeR作为一种有效的LLMs剪枝技术，能够显著降低计算和内存成本，同时较好地保持模型性能，展现出在高效LLMs领域的应用潜力。

Abstract: The deployment of Large language models (LLMs) in many fields is largely
hindered by their high computational and memory costs. Recent studies suggest
that LLMs exhibit sparsity, which can be used for pruning. Previous pruning
methods typically follow a prune-then-finetune paradigm. Since the pruned parts
still contain valuable information, statically removing them without updating
the remaining parameters often results in irreversible performance degradation,
requiring costly recovery fine-tuning (RFT) to maintain performance. To address
this, we propose a novel paradigm: first apply regularization, then prune.
Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through
Data-Driven Regularized Layer-wise Pruning. We multiply the output of each
transformer layer by an initial weight, then we iteratively learn the weights
of each transformer layer by using a small amount of data in a simple way.
After that, we apply regularization to the difference between the output and
input of the layers with smaller weights, forcing the information to be
transferred to the remaining layers. Compared with direct pruning, ELDeR
reduces the information loss caused by direct parameter removal, thus better
preserving the model's language modeling ability. Experimental results show
that ELDeR achieves superior performance compared with powerful layer-wise
structured pruning methods, while greatly reducing RFT computational costs.
Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect
is obvious, making it a promising technique for efficient LLMs.

</details>


### [144] [POSTER: A Multi-Signal Model for Detecting Evasive Smishing](https://arxiv.org/abs/2505.18233)
*Shaghayegh Hosseinpour,Sanchari Das*

Main category: cs.LG

TL;DR: The paper presents a multi-channel smishing detection model that combines various tagging methods and embeddings, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Smishing poses an increasing threat to mobile users, and current detection models may not capture diverse linguistic and structural cues effectively.

Method: The method involves combining country-specific semantic tagging, structural pattern tagging, character-level stylistic cues, and contextual phrase embeddings within a unified architecture.

Result: The model achieved 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream models.

Conclusion: This work demonstrates the effectiveness of multi-signal learning in robust and region-aware phishing detection.

Abstract: Smishing, or SMS-based phishing, poses an increasing threat to mobile users
by mimicking legitimate communications through culturally adapted, concise, and
deceptive messages, which can result in the loss of sensitive data or financial
resources. In such, we present a multi-channel smishing detection model that
combines country-specific semantic tagging, structural pattern tagging,
character-level stylistic cues, and contextual phrase embeddings. We curated
and relabeled over 84,000 messages across five datasets, including 24,086
smishing samples. Our unified architecture achieves 97.89% accuracy, an F1
score of 0.963, and an AUC of 99.73%, outperforming single-stream models by
capturing diverse linguistic and structural cues. This work demonstrates the
effectiveness of multi-signal learning in robust and region-aware phishing.

</details>


### [145] [A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems](https://arxiv.org/abs/2505.18234)
*Yuanya She*

Main category: cs.LG

TL;DR: The paper proposes a robust network intrusion detection system (NIDS) for IIoT environments using TabTransformer and PPO, achieving high macro F1-score and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address class-imbalanced and few-shot attack scenarios in Industrial Internet of Things environments.

Method: Integrates TabTransformer for tabular feature representation with Proximal Policy Optimization to optimize classification decisions via policy learning.

Result: Achieves a macro F1-score of 97.73% and accuracy of 98.85% on TON_IoT benchmark. Notably, achieves an F1-score of 88.79% on extremely rare classes like MITM attacks.

Conclusion: Combining transformer-based tabular learning with reinforcement learning shows potential for real-world NIDS applications.

Abstract: In this paper, we propose a robust and reinforcement-learning-enhanced
network intrusion detection system (NIDS) designed for class-imbalanced and
few-shot attack scenarios in Industrial Internet of Things (IIoT) environments.
Our model integrates a TabTransformer for effective tabular feature
representation with Proximal Policy Optimization (PPO) to optimize
classification decisions via policy learning. Evaluated on the
TON\textunderscore IoT benchmark, our method achieves a macro F1-score of
97.73\% and accuracy of 98.85\%. Remarkably, even on extremely rare classes
like man-in-the-middle (MITM), our model achieves an F1-score of 88.79\%,
showcasing strong robustness and few-shot detection capabilities. Extensive
ablation experiments confirm the complementary roles of TabTransformer and PPO
in mitigating class imbalance and improving generalization. These results
highlight the potential of combining transformer-based tabular learning with
reinforcement learning for real-world NIDS applications.

</details>


### [146] [The Origins of Representation Manifolds in Large Language Models](https://arxiv.org/abs/2505.18235)
*Alexander Modell,Patrick Rubin-Delanchy,Nick Whiteley*

Main category: cs.LG

TL;DR: The paper explores how neural representations can encode features as manifolds and validates the theory using text embeddings.


<details>
  <summary>Details</summary>
Motivation: To advance mechanistic interpretability by moving beyond the linear representation hypothesis to a model where neural representations encode both presence and continuous, multidimensional values for features.

Method: Describe why and how features might be represented as manifolds, focusing on cosine similarity in representation space encoding the intrinsic geometry of a feature through shortest, on-manifold paths.

Result: Critical assumptions and predictions of the theory are validated on text embeddings and token activations of large language models.

Conclusion: Cosine similarity in representation space may connect distance in this space with relatedness in concept space via shortest, on-manifold paths.

Abstract: There is a large ongoing scientific effort in mechanistic interpretability to
map embeddings and internal representations of AI systems into
human-understandable concepts. A key element of this effort is the linear
representation hypothesis, which posits that neural representations are sparse
linear combinations of `almost-orthogonal' direction vectors, reflecting the
presence or absence of different features. This model underpins the use of
sparse autoencoders to recover features from representations. Moving towards a
fuller model of features, in which neural representations could encode not just
the presence but also a potentially continuous and multidimensional value for a
feature, has been a subject of intense recent discourse. We describe why and
how a feature might be represented as a manifold, demonstrating in particular
that cosine similarity in representation space may encode the intrinsic
geometry of a feature through shortest, on-manifold paths, potentially
answering the question of how distance in representation space and relatedness
in concept space could be connected. The critical assumptions and predictions
of the theory are validated on text embeddings and token activations of large
language models.

</details>


### [147] [Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning](https://arxiv.org/abs/2505.18245)
*Roy Elkayam*

Main category: cs.LG

TL;DR: This study introduces a new method using Skewed Gaussian Distributions (SGD) to decompose urban water demand patterns, providing behavioral insights and supporting operational planning. It outperforms symmetric Gaussian models in reconstruction accuracy by over 50% on average.


<details>
  <summary>Details</summary>
Motivation: To derive behavioral insights and support operational planning through the decomposition of urban water demand patterns.

Method: Using Skewed Gaussian Distributions (SGD) to break down daily demand curves into baseline and peak components, characterizing each peak with amplitude, timing, spread, and skewness parameters.

Result: The SGD method reduces root-mean-square error by over 50% compared to symmetric Gaussian models, while maintaining physical interpretability.

Conclusion: The SGD framework is effective for analyzing water demand patterns, enabling anomaly detection, real-time management, and synthetic scenario construction.

Abstract: This study presents a novel approach for decomposing urban water demand
patterns using Skewed Gaussian Distributions (SGD) to derive behavioral
insights and support operational planning. Hourly demand profiles contain
critical information for both long-term infrastructure design and daily
operations, influencing network pressures, water quality, energy consumption,
and overall reliability. By breaking down each daily demand curve into a
baseline component and distinct peak components, the proposed SGD method
characterizes each peak with interpretable parameters, including peak
amplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby
reconstructing the observed pattern and uncovering latent usage dynamics. This
detailed peak-level decomposition enables both operational applications, e.g.
anomaly and leakage detection, real-time demand management, and strategic
analyses, e.g. identifying behavioral shifts, seasonal influences, or policy
impacts on consumption patterns. Unlike traditional symmetric Gaussian or
purely statistical time-series models, SGDs explicitly capture asymmetric peak
shapes such as sharp morning surges followed by gradual declines, improving the
fidelity of synthetic pattern generation and enhancing the detection of
irregular consumption behavior. The method is demonstrated on several
real-world datasets, showing that SGD outperforms symmetric Gaussian models in
reconstruction accuracy, reducing root-mean-square error by over 50% on
average, while maintaining physical interpretability. The SGD framework can
also be used to construct synthetic demand scenarios by designing daily peak
profiles with chosen characteristics. All implementation code is publicly
available at: https://github.com/Relkayam/water-demand-decomposition-sgd

</details>


### [148] [Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks](https://arxiv.org/abs/2505.18266)
*Gavin McCracken,Gabriela Moisescu-Pareja,Vincent Letourneau,Doina Precup,Jonathan Love*

Main category: cs.LG

TL;DR: This paper proposes a universality hypothesis that neural network solutions for modular addition are unified under a common abstract algorithm, the approximate Chinese Remainder Theorem. It demonstrates this through multi-level analyses and introduces approximate cosets, providing a theory-backed interpretation of multilayer networks solving modular addition.


<details>
  <summary>Details</summary>
Motivation: To provide a testable universality hypothesis unifying seemingly disparate neural network solutions for modular addition under a common abstract algorithm.

Method: Demonstrate through multi-level analyses (neurons, neuron clusters, entire networks) that multilayer perceptrons and transformers universally implement the approximate Chinese Remainder Theorem. Introduce approximate cosets showing neurons activate exclusively on them.

Result: Confirmed that universally learned solutions in DNNs with trainable embeddings or more than one hidden layer require only O(log n) features.

Conclusion: Provides the first theory-backed interpretation of multilayer networks solving modular addition, advancing generalizable interpretability and opening a testable universality hypothesis for group multiplication beyond modular addition.

Abstract: We propose a testable universality hypothesis, asserting that seemingly
disparate neural network solutions observed in the simple task of modular
addition are unified under a common abstract algorithm. While prior work
interpreted variations in neuron-level representations as evidence for distinct
algorithms, we demonstrate - through multi-level analyses spanning neurons,
neuron clusters, and entire networks - that multilayer perceptrons and
transformers universally implement the abstract algorithm we call the
approximate Chinese Remainder Theorem. Crucially, we introduce approximate
cosets and show that neurons activate exclusively on them. Furthermore, our
theory works for deep neural networks (DNNs). It predicts that universally
learned solutions in DNNs with trainable embeddings or more than one hidden
layer require only O(log n) features, a result we empirically confirm. This
work thus provides the first theory-backed interpretation of multilayer
networks solving modular addition. It advances generalizable interpretability
and opens a testable universality hypothesis for group multiplication beyond
modular addition.

</details>


### [149] [Representative Action Selection for Large Action-Space Meta-Bandits](https://arxiv.org/abs/2505.18269)
*Quan Zhou,Mark Kozdoba,Shie Mannor*

Main category: cs.LG

TL;DR: This paper proposes an epsilon-net algorithm for selecting a representative subset from a large action space in the bandits problem, providing theoretical guarantees and empirical comparisons.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of selecting a subset from a large action space shared by a family of bandits, aiming to achieve performance nearly matching that of using the full action space while reducing computational complexity.

Method: The method involves proposing a simple epsilon-net algorithm for selecting a representative subset from a large action space. This is based on the assumption that similar actions tend to have related payoffs, which are modeled by a Gaussian process.

Result: Theoretical guarantees for the epsilon-net algorithm's performance are provided and it is empirically compared to Thompson Sampling and Upper Confidence Bound, showing its effectiveness.

Conclusion: The epsilon-net algorithm can effectively select a representative subset from a large action space in the context of bandits problem, achieving performance comparable to using the full action space.

Abstract: We study the problem of selecting a subset from a large action space shared
by a family of bandits, with the goal of achieving performance nearly matching
that of using the full action space. We assume that similar actions tend to
have related payoffs, modeled by a Gaussian process. To exploit this structure,
we propose a simple epsilon-net algorithm to select a representative subset. We
provide theoretical guarantees for its performance and compare it empirically
to Thompson Sampling and Upper Confidence Bound.

</details>


### [150] [Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior](https://arxiv.org/abs/2505.18280)
*Tsai Hor Chan,Dora Yan Zhang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: The paper introduces R2D2-Net, a novel Bayesian neural network with an R2-induced Dirichlet Decomposition (R2D2) prior that effectively shrinks irrelevant coefficients and prevents over-shrinkage of key features. A variational Gibbs inference algorithm is proposed for accurate posterior approximation, showing satisfactory performance in image classification and uncertainty estimation tasks.


<details>
  <summary>Details</summary>
Motivation: Bayesian neural networks (BNNs) are powerful tools for providing posterior uncertainty estimates and avoiding overfitting through weight inference. However, selecting appropriate priors remains challenging, as poor choices can lead to inflated variance or poor predictive performance. Existing BNN designs struggle with shrinking noisy signals while preserving important features.

Method: The authors propose R2D2-Net, which applies the R2-induced Dirichlet Decomposition (R2D2) prior to BNN weights. This prior effectively shrinks irrelevant coefficients towards zero while preventing over-shrinkage of key features. Additionally, they introduce a variational Gibbs inference algorithm combining Gibbs updating and gradient-based optimization to accurately approximate the posterior distribution of weights.

Result: Experiments on natural and medical image classification and uncertainty estimation tasks demonstrate the satisfactory performance of R2D2-Net. The method shows stability and consistency in estimation even when the variational objective involving shrinkage parameters is non-convex.

Conclusion: R2D2-Net addresses the challenges of prior selection in BNNs by introducing the R2D2 prior, which balances coefficient shrinkage and feature preservation. The variational Gibbs inference algorithm enhances posterior approximation accuracy. Theoretical analysis supports the effectiveness of the method.

Abstract: Bayesian neural networks (BNNs) treat neural network weights as random
variables, which aim to provide posterior uncertainty estimates and avoid
overfitting by performing inference on the posterior weights. However, the
selection of appropriate prior distributions remains a challenging task, and
BNNs may suffer from catastrophic inflated variance or poor predictive
performance when poor choices are made for the priors. Existing BNN designs
apply different priors to weights, while the behaviours of these priors make it
difficult to sufficiently shrink noisy signals or they are prone to
overshrinking important signals in the weights. To alleviate this problem, we
propose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition
(R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant
coefficients towards zero, while preventing key features from over-shrinkage.
To approximate the posterior distribution of weights more accurately, we
further propose a variational Gibbs inference algorithm that combines the Gibbs
updating procedure and gradient-based optimization. This strategy enhances
stability and consistency in estimation when the variational objective
involving the shrinkage parameters is non-convex. We also analyze the evidence
lower bound (ELBO) and the posterior concentration rates from a theoretical
perspective. Experiments on both natural and medical image classification and
uncertainty estimation tasks demonstrate satisfactory performance of our
method.

</details>


### [151] [Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed](https://arxiv.org/abs/2505.18284)
*Pritam Anand,Aadesh Minz,Asish Joel*

Main category: cs.LG

TL;DR: 通过使用Tube损失函数设计了一系列基于深度学习的概率预测方法，用于风速预测中的不确定性量化（UQ）。该方法在三个不同地点的风速数据集上验证，并表现出更可靠和更窄的预测区间（PI）。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化（UQ）在风速预测中至关重要，因为风具有固有的不稳定性。有效的UQ有助于电网运行和电力市场参与的决策。需要一种简单且无需分布假设的方法来获得狭窄的预测区间（PI），并保证覆盖精度。

Method: 采用Tube损失函数设计深度学习概率预测模型，结合LSTM、GRU和TCN等流行架构。还设计了一个简单的启发式方法来调整Tube损失函数的δ参数，以获得更窄的PI而不影响校准能力。

Result: 数值结果表明，所提出的深度预测模型相比最近开发的概率风速预测方法，能够产生更可靠和更窄的PI。

Conclusion: 所提出的基于Tube损失函数的深度学习概率预测方法在风速预测中表现优异，提供了更可靠的PI估计，适用于实际应用。

Abstract: Uncertainty Quantification (UQ) in wind speed forecasting is a critical
challenge in wind power production due to the inherently volatile nature of
wind. By quantifying the associated risks and returns, UQ supports more
effective decision-making for grid operations and participation in the
electricity market. In this paper, we design a sequence of deep learning based
probabilistic forecasting methods by using the Tube loss function for wind
speed forecasting. The Tube loss function is a simple and model agnostic
Prediction Interval (PI) estimation approach and can obtain the narrow PI with
asymptotical coverage guarantees without any distribution assumption. Our deep
probabilistic forecasting models effectively incorporate popular architectures
such as LSTM, GRU, and TCN within the Tube loss framework. We further design a
simple yet effective heuristic for tuning the $\delta$ parameter of the Tube
loss function so that our deep forecasting models obtain the narrower PI
without compromising its calibration ability. We have considered three wind
datasets, containing the hourly recording of the wind speed, collected from
three distinct location namely Jaisalmer, Los Angeles and San Fransico. Our
numerical results demonstrate that the proposed deep forecasting models produce
more reliable and narrower PIs compared to recently developed probabilistic
wind forecasting methods.

</details>


### [152] [Convexified Message-Passing Graph Neural Networks](https://arxiv.org/abs/2505.18289)
*Saar Cohen,Noa Agmon,Uri Shaham*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) have shown great success in graph representation learning. This paper introduces Convexified Message Passing Graph Neural Networks (CGNNs), which combines message-passing GNNs with convex optimization, providing strong theoretical guarantees and outperforming leading GNN models.


<details>
  <summary>Details</summary>
Motivation: Despite the success of GNNs, training them can be challenging due to non-convex optimization problems. The authors aim to create a model that leverages the power of GNNs while being solvable via efficient and optimal convex optimization techniques.

Method: The authors introduce CGNNs by mapping nonlinear filters into a reproducing kernel Hilbert space, transforming the training process into a convex optimization problem. They provide rigorous generalization guarantees for two-layer CGNNs and adopt a layer-wise training strategy for deeper architectures.

Result: Experiments on benchmark datasets indicate that CGNNs significantly outperform leading GNN models, achieving 10 to 40 percent higher accuracy in most cases. Even in rare cases where improvements are not substantial, the CGNNs either slightly exceed or match baseline performances.

Conclusion: CGNNs represent a powerful and principled method for graph representation learning, offering both strong theoretical foundations and superior performance compared to existing GNN models.

Abstract: Graph Neural Networks (GNNs) have become prominent methods for graph
representation learning, demonstrating strong empirical results on diverse
graph prediction tasks. In this paper, we introduce Convexified Message Passing
Graph Neural Networks (CGNNs), a novel and general framework that combines the
power of message-passing GNNs with the tractability of convex optimization. By
mapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs
transform training into a convex optimization problem, which can be solved
efficiently and optimally by projected gradient methods. This convexity further
allows the statistical properties of CGNNs to be analyzed accurately and
rigorously. For two-layer CGNNs, we establish rigorous generalization
guarantees, showing convergence to the performance of the optimal GNN. To scale
to deeper architectures, we adopt a principled layer-wise training strategy.
Experiments on benchmark datasets show that CGNNs significantly exceed the
performance of leading GNN models, achieving 10 to 40 percent higher accuracy
in most cases, underscoring their promise as a powerful and principled method
with strong theoretical foundations. In rare cases where improvements are not
quantitatively substantial, the convex models either slightly exceed or match
the baselines, stressing their robustness and wide applicability. Though
over-parameterization is often employed to enhance performance in nonconvex
models, we show that our CGNNs framework yields shallow convex models that can
surpass these models in both accuracy and resource efficiency.

</details>


### [153] [Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs](https://arxiv.org/abs/2505.18300)
*Jie Hu,Yi-Ting Ma,Do Young Eun*

Main category: cs.LG

TL;DR: 提出了一种基于历史驱动目标（HDT）的框架，用于改进马尔可夫链蒙特卡洛（MCMC）方法中的随机游走算法，通过引入依赖历史的目标分布，实现高效采样并减少方差。


<details>
  <summary>Details</summary>
Motivation: 现有的自排斥随机游走（SRRW）方法虽然可以减少方差，但计算开销大，并且仅适用于时间可逆的马尔可夫链，限制了其应用范围。

Method: HDT框架通过引入一个依赖于历史访问频率的目标分布来替代原始目标分布，从而避免直接修改转移核。该方法只需要局部信息，兼容可逆和不可逆的MCMC采样器，同时保持无偏性。

Result: 实验表明，HDT框架在图采样中表现出一致的性能提升，并通过LRU缓存确保了对大规模图的可扩展性。

Conclusion: HDT框架提供了一种轻量级、高效的改进MCMC采样器的方法，适用于广泛的离散状态空间问题。

Abstract: We propose a history-driven target (HDT) framework in Markov Chain Monte
Carlo (MCMC) to improve any random walk algorithm on discrete state spaces,
such as general undirected graphs, for efficient sampling from target
distribution $\boldsymbol{\mu}$. With broad applications in network science and
distributed optimization, recent innovations like the self-repellent random
walk (SRRW) achieve near-zero variance by prioritizing under-sampled states
through transition kernel modifications based on past visit frequencies.
However, SRRW's reliance on explicit computation of transition probabilities
for all neighbors at each step introduces substantial computational overhead,
while its strict dependence on time-reversible Markov chains excludes advanced
non-reversible MCMC methods. To overcome these limitations, instead of direct
modification of transition kernel, HDT introduces a history-dependent target
distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target
$\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the
empirical measure of past visits. This design preserves lightweight
implementation by requiring only local information between the current and
proposed states and achieves compatibility with both reversible and
non-reversible MCMC samplers, while retaining unbiased samples with target
distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive
experiments in graph sampling demonstrate consistent performance gains, and a
memory-efficient Least Recently Used (LRU) cache ensures scalability to large
general graphs.

</details>


### [154] [PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training](https://arxiv.org/abs/2505.18313)
*Matan Haroush,Daniel Soudry*

Main category: cs.LG

TL;DR: PLUMAGE是一种新的低秩梯度估计方法，可替代现有的估计器，无需引入额外的超参数，并解决了优化器状态错位问题，提高了训练稳定性。在相似的计算和内存占用下，PLUMAGE在预训练评估损失和GLUE基准上的平均训练损失方面表现优于GaloRE。


<details>
  <summary>Details</summary>
Motivation: 在训练大型语言模型（LLM）时，加速器内存和网络约束成为主要瓶颈。现有的低秩梯度估计方法（如GaLoRE和FLORA）虽然可以通过压缩梯度和优化张量来缓解这一问题，但这些方法要么是有偏的，要么方差较高，且在投影更新时会导致优化器状态错位，从而引起训练不稳定。

Method: 提出了一种新的低秩梯度估计方法——PLUMAGE（Probabilistic Low rank Unbiased Minimum vAriance Gradient Estimator）。该方法作为现有低秩梯度估计器的直接替代品，不引入新的超参数，仅需选择秩r和更新间隔。此外，通过解决优化器状态错位问题，防止了错误的权重更新，增强了训练稳定性。

Result: 实验结果表明，与全秩优化相比，PLUMAGE在多个模型上的预训练评估损失差距平均缩小了33%，在GLUE基准上的平均训练损失差距缩小了28%。同时，其计算和内存占用与GaloRE相当。

Conclusion: PLUMAGE作为一种改进的低秩梯度估计方法，不仅解决了现有方法中的偏差和高方差问题，还通过优化器状态对齐提升了训练稳定性，在资源受限的情况下表现出色。

Abstract: Accelerator memory and networking constraints have emerged as dominant
bottlenecks when training large language models LLMs with billions of
parameters. Existing low rank gradient estimators such as GaLoRE and FLORA
compress gradients and optimizer tensors by projecting weight gradients onto a
rank r subspace, enabling LLM training on consumer hardware. Yet, these methods
are either biased or subject to high estimator variance. Moreover, the
optimizer state based on the first and second moments estimates expressed in
the previous subspace becomes misaligned whenever the projection is updated,
leading to instabilities during training. We propose PLUMAGE: Probabilistic Low
rank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in
replacement for existing low rank gradient estimators. It does not introduce
new hyperparameters beyond the chosen rank r and the update interval. In
addition, we resolve optimizer state misalignment issues to prevent spurious
weight updates and enhance training stability. We empirically demonstrate that
PLUMAGE shrinks the full rank optimization's gap over the pre training
evaluation loss by 33% on average across models and the average training loss
across the GLUE benchmark by 28% within a similar computational and memory
footprint as GaloRE.

</details>


### [155] [Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access](https://arxiv.org/abs/2505.18344)
*Mudit Gaur,Prashant Trivedi,Sasidhar Kunapuli,Amrit Singh Bedi,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Diffusion models are powerful but their sample complexity analysis has issues. This paper provides a new principled analysis of score estimation with a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$ without unrealistic assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in previous theoretical analyses of diffusion models which either have poor scaling with input data dimension or rely on unrealistic assumptions.

Method: Provide a principled analysis of score estimation by decomposing the score estimation error into statistical, approximation, and optimization errors, leading to a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$ without assuming access to the empirical risk minimizer of score function estimation loss.

Result: Achieved a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$, eliminating the exponential dependence on neural network parameters found in previous analyses.

Conclusion: This work presents the first result that achieves sample complexity bounds for score estimation in diffusion models without relying on access to the empirical risk minimizer.

Abstract: Diffusion models have demonstrated state-of-the-art performance across
vision, language, and scientific domains. Despite their empirical success,
prior theoretical analyses of the sample complexity suffer from poor scaling
with input data dimension or rely on unrealistic assumptions such as access to
exact empirical risk minimizers. In this work, we provide a principled analysis
of score estimation, establishing a sample complexity bound of
$\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured
decomposition of the score estimation error into statistical, approximation,
and optimization errors, enabling us to eliminate the exponential dependence on
neural network parameters that arises in prior analyses. It is the first such
result which achieves sample complexity bounds without assuming access to the
empirical risk minimizer of score function estimation loss.

</details>


### [156] [Diffusion Self-Weighted Guidance for Offline Reinforcement Learning](https://arxiv.org/abs/2505.18345)
*Augusto Tagle,Javier Ruiz-del-Solar,Felipe Tobar*

Main category: cs.LG

TL;DR: 本文提出了一种新的离线强化学习方法Self-Weighted Guidance (SWG)，通过构建动作和权重的扩散模型，无需额外网络即可直接获得所需分数。在玩具示例和D4RL挑战环境中表现出与最先进方法相当的性能，同时保持简化的训练流程。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的离线强化学习方法虽然展现出有希望的结果，但计算所需的分数具有挑战性，因为这些分数依赖于未知的权重函数w。

Method: 作者通过构造一个覆盖动作和权重的扩散模型来解决这一问题。提出了一种新的指导方法Self-Weighted Guidance (SWG)，其中指导来源于同一个扩散模型，从而避免了学习额外的网络。

Result: 实验表明，SWG可以在玩具示例中生成来自目标分布的样本，并且在D4RL的挑战环境中表现与最先进的方法相当。此外，通过消融研究验证了权重公式和可扩展性。

Conclusion: Self-Weighted Guidance (SWG)是一种有效的离线强化学习方法，能够在不增加复杂性的情况下达到与现有最佳方法相当的性能。

Abstract: Offline reinforcement learning (RL) recovers the optimal policy $\pi$ given
historical observations of an agent. In practice, $\pi$ is modeled as a
weighted version of the agent's behavior policy $\mu$, using a weight function
$w$ working as a critic of the agent's behavior. Though recent approaches to
offline RL based on diffusion models have exhibited promising results, the
computation of the required scores is challenging due to their dependence on
the unknown $w$. In this work, we alleviate this issue by constructing a
diffusion over both the actions and the weights. With the proposed setting, the
required scores are directly obtained from the diffusion model without learning
extra networks. Our main conceptual contribution is a novel guidance method,
where guidance (which is a function of $w$) comes from the same diffusion
model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show
that SWG generates samples from the desired distribution on toy examples and
performs on par with state-of-the-art methods on D4RL's challenging
environments, while maintaining a streamlined training pipeline. We further
validate SWG through ablation studies on weight formulations and scalability.

</details>


### [157] [The Cell Must Go On: Agar.io for Continual Reinforcement Learning](https://arxiv.org/abs/2505.18347)
*Mohamed A. Mohamed,Kateryna Nekhomiazh,Vedant Vyas,Marcos M. Jose,Andrew Patterson,Marlos C. Machado*

Main category: cs.LG

TL;DR: The paper introduces AgarCL, a research platform for continual reinforcement learning based on the game Agar.io, and provides benchmark results of DQN, PPO, and SAC.


<details>
  <summary>Details</summary>
Motivation: Continual reinforcement learning agents need to learn continuously rather than converge to a fixed policy. Current simulators for continual RL are limited in scope or complexity and researchers often modify episodic RL environments which may not accurately represent real-world scenarios.

Method: The authors introduce AgarCL, a platform for continual RL based on the game Agar.io, which features non-episodic, high-dimensional problems with stochastic dynamics, continuous actions, and partial observability. They also provide benchmark results for DQN, PPO, and SAC algorithms in this environment.

Result: Benchmark results for DQN, PPO, and SAC were reported in both the primary continual RL problem within AgarCL and across smaller tasks that isolate different aspects of the game, allowing characterization of various challenges posed by the environment.

Conclusion: AgarCL serves as a valuable tool for empirical research in continual RL due to its ability to support increasingly sophisticated behavior and its representation of complex, evolving dynamics.

Abstract: Continual reinforcement learning (RL) concerns agents that are expected to
learn continually, rather than converge to a policy that is then fixed for
evaluation. Such an approach is well suited to environments the agent perceives
as changing, which renders any static policy ineffective over time. The few
simulators explicitly designed for empirical research in continual RL are often
limited in scope or complexity, and it is now common for researchers to modify
episodic RL environments by artificially incorporating abrupt task changes
during interaction. In this paper, we introduce AgarCL, a research platform for
continual RL that allows for a progression of increasingly sophisticated
behaviour. AgarCL is based on the game Agar.io, a non-episodic,
high-dimensional problem featuring stochastic, ever-evolving dynamics,
continuous actions, and partial observability. Additionally, we provide
benchmark results reporting the performance of DQN, PPO, and SAC in both the
primary, challenging continual RL problem, and across a suite of smaller tasks
within AgarCL, each of which isolates aspects of the full environment and allow
us to characterize the challenges posed by different aspects of the game.

</details>


### [158] [Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?](https://arxiv.org/abs/2505.18350)
*Waleed Reda,Abhinav Jangda,Krishna Chintalapudi*

Main category: cs.LG

TL;DR: 研究人员开发了LLM-Sieve框架，该框架能够减少20-75%的参数量，同时仅降低1-5%的准确性。此框架通过学习任务感知联合投影和使用遗传算法发现每个矩阵的不同修剪水平来实现。它与LoRA微调和量化完全兼容，并在相同任务领域内的数据集上表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被越来越多地用于特定任务（如医疗问答或情感分析），并且在资源受限的环境中部署，人们开始关注一个问题：一个任务实际上需要多少参数？

Method: LLM-Sieve是一个全面的框架，用于特定任务的LLM修剪。它不仅学习任务感知联合投影以更好地逼近输出行为，还采用遗传算法为每个矩阵发现不同的修剪级别。此外，LLM-Sieve与LoRA微调和量化完全兼容。

Result: LLM-Sieve实现了20-75%的参数减少，同时仅有1-5%的准确率下降，并且在相同任务领域的不同数据集上显示出强大的泛化能力。

Conclusion: 这些结果建立了一个实用且稳健的机制，可以生成更小且性能良好的特定任务模型。

Abstract: As Large Language Models (LLMs) are increasingly being adopted for narrow
tasks - such as medical question answering or sentiment analysis - and deployed
in resource-constrained settings, a key question arises: how many parameters
does a task actually need? In this work, we present LLM-Sieve, the first
comprehensive framework for task-specific pruning of LLMs that achieves 20-75%
parameter reduction with only 1-5% accuracy degradation across diverse domains.
Unlike prior methods that apply uniform pruning or rely on low-rank
approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns
task-aware joint projections to better approximate output behavior, and (ii)
employs a Genetic Algorithm to discover differentiated pruning levels for each
matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,
and uniquely demonstrates strong generalization across datasets within the same
task domain. Together, these results establish a practical and robust mechanism
to generate smaller performant task-specific models.

</details>


### [159] [X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI](https://arxiv.org/abs/2505.18355)
*Yiming Sun,Shuo Chen,Shengyu Chen,Chonghao Qiu,Licheng Liu,Youmi Oh,Sparkle L. Malone,Gavin McNicol,Qianlai Zhuang,Chris Smith,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: This paper introduces X-MethaneWet, a novel cross-scale global wetland methane benchmark dataset combining physics-based model simulation data and real-world observation data. The authors evaluate sequential deep learning models for methane flux prediction and explore transfer learning techniques to enhance model generalization.


<details>
  <summary>Details</summary>
Motivation: Methane is a significant greenhouse gas after carbon dioxide. Accurately modeling methane fluxes globally and at fine temporal scales is essential for understanding its variability and developing mitigation strategies. There is a need for better datasets and AI algorithms to improve methane modeling.

Method: The authors created the X-MethaneWet dataset by synthesizing data from TEM-MDM (physics-based model) and FLUXNET-CH$_4$ (real-world observations). They evaluated various sequential deep learning models on this dataset for methane flux prediction and explored four different transfer learning techniques to leverage simulated data for improving model performance on real-world observations.

Result: Extensive experiments showed the effectiveness of the evaluated approaches in improving methane flux prediction. Transfer learning techniques were successful in enhancing the generalization of deep learning models when applied to real-world FLUXNET-CH$_4$ observations.

Conclusion: X-MethaneWet provides new opportunities for improving global wetland methane modeling using AI algorithms. The study demonstrates the potential of transfer learning techniques and contributes to the development of more accurate and scalable AI-driven climate models.

Abstract: Methane (CH$_4$) is the second most powerful greenhouse gas after carbon
dioxide and plays a crucial role in climate change due to its high global
warming potential. Accurately modeling CH$_4$ fluxes across the globe and at
fine temporal scales is essential for understanding its spatial and temporal
variability and developing effective mitigation strategies. In this work, we
introduce the first-of-its-kind cross-scale global wetland methane benchmark
dataset (X-MethaneWet), which synthesizes physics-based model simulation data
from TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This
dataset can offer opportunities for improving global wetland CH$_4$ modeling
and science discovery with new AI algorithms. To set up AI model baselines for
methane flux prediction, we evaluate the performance of various sequential deep
learning models on X-MethaneWet. Furthermore, we explore four different
transfer learning techniques to leverage simulated data from TEM-MDM to improve
the generalization of deep learning models on real-world FLUXNET-CH$_4$
observations. Our extensive experiments demonstrate the effectiveness of these
approaches, highlighting their potential for advancing methane emission
modeling and contributing to the development of more accurate and scalable
AI-driven climate models.

</details>


### [160] [Small Models, Smarter Learning: The Power of Joint Task Training](https://arxiv.org/abs/2505.18369)
*Csaba Both,Benjamin Hoover,Hendrik Strobelt,Dmitry Krotov,Daniel Karl I. Weidele,Mauro Martino,Nima Dehmamy*

Main category: cs.LG

TL;DR: 通过研究ListOps数据集，发现任务难度与模型参数需求相关，并且联合训练可以改变模型行为，提升性能。模型学习纯SUM任务困难，但结合其他操作（如MAX和MED）可降低难度，甚至使低于SUM学习阈值的模型通过预训练获得能力。这表明语言模型的 emergent abilities 受模型规模和训练课程影响。


<details>
  <summary>Details</summary>
Motivation: 理解任务难度如何影响小规模Transformer模型学习特定任务所需的最少参数数量。

Method: 聚焦ListOps数据集，逐步增加任务难度（引入新运算或运算组合），观察模型表现及参数需求变化。比较不同训练方式（单独SUM vs. 联合训练）对模型行为和性能的影响。

Result: 1. 纯SUM任务最难学，但与其他运算结合后变简单，需更少参数。
2. 单独训练SUM可能导致记忆而非理解，联合训练生成更好的数字表示。
3. SUM-only模型依赖前馈层，联合训练模型激活注意力机制更多。
4. MAX+MED预训练能使低于SUM学习阈值的模型学会纯SUM任务。

Conclusion: 语言模型的 emergent abilities 不仅取决于模型大小，还受训练课程影响。

Abstract: The ability of a model to learn a task depends strongly on both the task
difficulty and the model size. We aim to understand how task difficulty relates
to the minimum number of parameters required for learning specific tasks in
small transformer models. Our study focuses on the ListOps dataset, which
consists of nested mathematical operations. We gradually increase task
difficulty by introducing new operations or combinations of operations into the
training data. We observe that sum modulo n is the hardest to learn. Curiously,
when combined with other operations such as maximum and median, the sum
operation becomes easier to learn and requires fewer parameters. We show that
joint training not only improves performance but also leads to qualitatively
different model behavior. We show evidence that models trained only on SUM
might be memorizing and fail to capture the number structure in the embeddings.
In contrast, models trained on a mixture of SUM and other operations exhibit
number-like representations in the embedding space, and a strong ability to
distinguish parity. Furthermore, the SUM-only model relies more heavily on its
feedforward layers, while the jointly trained model activates the attention
mechanism more. Finally, we show that learning pure SUM can be induced in
models below the learning threshold of pure SUM, by pretraining them on
MAX+MED. Our findings indicate that emergent abilities in language models
depend not only on model size, but also the training curriculum.

</details>


### [161] [Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373)
*Paul M. Riechers,Henry R. Bigelow,Eric A. Alt,Adam Shai*

Main category: cs.LG

TL;DR: 该研究通过信息论框架解释了在分布内的情境学习（ICL）是如何从标准的自监督预训练中可预测地出现，并非一种奇异的突现属性，同时通过实验验证了相关理论预测。


<details>
  <summary>Details</summary>
Motivation: 探讨情境学习是否为模型预训练过程中可预测出现的特性，而非一种奇异的突现现象。

Method: 采用信息论框架分析模型在不同相关结构合成数据集上的表现，验证在分布内的ICL动态（如上下文依赖的损失减少）。

Result: 成功预测并验证了诸如训练损失中的相变、归纳头形成及情境损失的幂律缩放等现象，证明模型的情境性能与预训练任务集合存在数学耦合关系。

Conclusion: 情境学习是标准自监督预训练的可预测结果，其出现可以通过信息论框架解释，并且这种学习机制不依赖于特定架构或模态。

Abstract: We argue that in-context learning (ICL) predictably arises from standard
self-supervised next-token pretraining, rather than being an exotic emergent
property. This work establishes the foundational principles of this emergence
by focusing on in-distribution ICL, demonstrating how models necessarily adapt
to context when trained on token sequences, especially from non-ergodic
sources. Our information-theoretic framework precisely predicts these
in-distribution ICL dynamics (i.e., context-dependent loss reduction). We
verify this with experiments using synthetic datasets of differing types of
correlational structure, reproducing characteristic phenomena like phase
transitions in training loss for induction head formation and power-law scaling
of in-context loss. We further show that a model's in-context performance on
any task is mathematically coupled to the ensemble of tasks seen in
pretraining, offering a fundamental explanation, grounded in architecture- and
modality-independent principles, for such inference-time learning.

</details>


### [162] [Applications of Modular Co-Design for De Novo 3D Molecule Generation](https://arxiv.org/abs/2505.18392)
*Danny Reidenbach,Filipp Nikitin,Olexandr Isayev,Saee Paliwal*

Main category: cs.LG

TL;DR: Megalodon is a new family of transformer models designed for de novo 3D molecule generation, achieving state-of-the-art results in benchmarks.


<details>
  <summary>Details</summary>
Motivation: De novo 3D molecule generation is crucial in drug discovery, but current geometric generative models have difficulty producing high-quality 3D structures despite maintaining 2D validity and topological stability.

Method: The Megalodon model uses scalable transformer models enhanced with basic equivariant layers and trained with a joint continuous and discrete denoising co-design objective. It focuses on generating realistic molecular structures and improving energetics.

Result: Megalodon achieves state-of-the-art results in 3D molecule generation, conditional structure generation, and structure energy benchmarks. Doubling the parameters to 40M significantly improves performance, generating more valid large molecules and achieving much lower energy levels compared to previous models.

Conclusion: Megalodon represents a significant advancement in the field of de novo 3D molecule generation for drug discovery.

Abstract: De novo 3D molecule generation is a pivotal task in drug discovery. However,
many recent geometric generative models struggle to produce high-quality 3D
structures, even if they maintain 2D validity and topological stability. To
tackle this issue and enhance the learning of effective molecular generation
dynamics, we present Megalodon-a family of scalable transformer models. These
models are enhanced with basic equivariant layers and trained using a joint
continuous and discrete denoising co-design objective. We assess Megalodon's
performance on established molecule generation benchmarks and introduce new 3D
structure benchmarks that evaluate a model's capability to generate realistic
molecular structures, particularly focusing on energetics. We show that
Megalodon achieves state-of-the-art results in 3D molecule generation,
conditional structure generation, and structure energy benchmarks using
diffusion and flow matching. Furthermore, doubling the number of parameters in
Megalodon to 40M significantly enhances its performance, generating up to 49x
more valid large molecules and achieving energy levels that are 2-10x lower
than those of the best prior generative models.

</details>


### [163] [Thought calibration: Efficient and confident test-time scaling](https://arxiv.org/abs/2505.18404)
*Menghua Wu,Cai Zhou,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: A method named thought calibration is proposed to dynamically decide when to terminate the thinking process of large language models, reducing compute cost while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Reasoning large language models achieve impressive test-time scaling by thinking for longer, but this performance gain comes at significant compute cost.

Method: Thought calibration views a language model's growing body of thoughts as a nested sequence of reasoning trees and identifies the point at which novel reasoning plateaus. Lightweight probes operating on the language model's hidden representations are used to inform both the reasoning structure and overall consistency of response.

Result: Thought calibration preserves model performance with up to a 60% reduction in thinking tokens on in-distribution data, and up to 20% in out-of-distribution data.

Conclusion: Thought calibration is an effective method to reduce compute cost while maintaining model performance.

Abstract: Reasoning large language models achieve impressive test-time scaling by
thinking for longer, but this performance gain comes at significant compute
cost. Directly limiting test-time budget hurts overall performance, but not all
problems are equally difficult. We propose thought calibration to decide
dynamically when thinking can be terminated. To calibrate our decision rule, we
view a language model's growing body of thoughts as a nested sequence of
reasoning trees, where the goal is to identify the point at which novel
reasoning plateaus. We realize this framework through lightweight probes that
operate on top of the language model's hidden representations, which are
informative of both the reasoning structure and overall consistency of
response. Based on three reasoning language models and four datasets, thought
calibration preserves model performance with up to a 60% reduction in thinking
tokens on in-distribution data, and up to 20% in out-of-distribution data.

</details>


### [164] [KL-regularization Itself is Differentially Private in Bandits and RLHF](https://arxiv.org/abs/2505.18407)
*Yizhou Zhang,Kishan Panaganti,Laixi Shi,Juba Ziani,Adam Wierman*

Main category: cs.LG

TL;DR: 通过在学习目标中添加KL正则化，可以使从结果随机策略中采样的动作本身具有差分隐私性，从而在不增加额外噪声的情况下提供隐私保护，并同时保留正则化提升性能的固有优势。


<details>
  <summary>Details</summary>
Motivation: 探索在三种不同的决策问题（多臂老虎机、线性情境老虎机和基于人类反馈的强化学习）中，利用正则化实现差分隐私的可能性，以避免显式注入噪声。

Method: 在离线数据设置下，研究向学习目标添加KL-正则化的方法，证明这种方法可以使从结果随机策略中采样的动作本身满足差分隐私。

Result: 证明了添加KL-正则化不仅能够实现差分隐私，还能保持正则化对性能的提升作用。

Conclusion: KL-正则化为实现差分隐私提供了一条新途径，无需额外的噪声注入，同时保留了正则化的性能优势。

Abstract: Differential Privacy (DP) provides a rigorous framework for privacy, ensuring
the outputs of data-driven algorithms remain statistically indistinguishable
across datasets that differ in a single entry. While guaranteeing DP generally
requires explicitly injecting noise either to the algorithm itself or to its
outputs, the intrinsic randomness of existing algorithms presents an
opportunity to achieve DP ``for free''. In this work, we explore the role of
regularization in achieving DP across three different decision-making problems:
multi-armed bandits, linear contextual bandits, and reinforcement learning from
human feedback (RLHF), in offline data settings. We show that adding
KL-regularization to the learning objective (a common approach in optimization
algorithms) makes the action sampled from the resulting stochastic policy
itself differentially private. This offers a new route to privacy guarantees
without additional noise injection, while also preserving the inherent
advantage of regularization in enhancing performance.

</details>


### [165] [LatentLLM: Attention-Aware Joint Tensor Compression](https://arxiv.org/abs/2505.18413)
*Toshiaki Koike-Akino,Xiangyu Chen,Jing Liu,Ye Wang,Pu,Wang,Matthew Brand*

Main category: cs.LG

TL;DR: 提出了一种将大型语言模型和多模态模型转换为低维度潜在结构的新框架，通过全局注意力感知的联合张量分解方法，在降低潜在维度的同时提高了模型准确性，并在多个基准测试中展示了其优势。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型（如大型语言模型和大型多模态模型）需要大量的计算和内存资源，因此需要一种有效的方法来压缩这些模型，同时保持或提高其性能。

Method: 提出了一种新的框架，该框架扩展了局部激活感知的张量分解到全局注意力感知的联合张量分解，以将大型语言模型和多模态模型转换为低维度潜在结构。

Result: 与现有的模型压缩方法相比，该框架可以在降低潜在维度时显著提高模型的准确性，从而实现计算和内存高效的大型语言模型和多模态模型，并在多个基准测试中展示了其优势，包括多模态推理任务。

Conclusion: 所提出的框架能够有效地将大型语言模型和多模态模型转换为低维度潜在结构，同时提高模型的准确性，为实现计算和内存高效的模型提供了一种新途径。

Abstract: Modern foundation models such as large language models (LLMs) and large
multi-modal models (LMMs) require a massive amount of computational and memory
resources. We propose a new framework to convert such LLMs/LMMs into a
reduced-dimension latent structure. Our method extends a local activation-aware
tensor decomposition to a global attention-aware joint tensor de-composition.
Our framework can significantly improve the model accuracy over the existing
model compression methods when reducing the latent dimension to realize
computationally/memory-efficient LLMs/LLMs. We show the benefit on several
benchmark including multi-modal reasoning tasks.

</details>


### [166] [A Dual Basis Approach for Structured Robust Euclidean Distance Geometry](https://arxiv.org/abs/2505.18414)
*Chandra Kundu,Abiy Tasissa,HanQin Cai*

Main category: cs.LG

TL;DR: The paper proposes RoDEoDB, a novel algorithmic framework for recovering Euclidean distance geometry using non-orthogonal dual basis in the presence of outliers and partial observations.


<details>
  <summary>Details</summary>
Motivation: To address the problem of recovering Euclidean distance geometry (point configuration) from partial and potentially corrupted observations of EDM, especially in applications like sensor localization and molecular conformation where only distances to anchor nodes are known.

Method: RoDEoDB connects EDM with a positive semi-definite Gram matrix via a non-orthogonal dual basis. It leverages recent developments in optimization involving non-orthogonal dual bases to recover the underlying point configuration despite partial observations and corruptions.

Result: Exact recovery guarantees are established under mild conditions for both the Gram matrix and point configuration. Empirical experiments demonstrate superior performance on sensor localization and molecular conformation datasets compared to existing methods.

Conclusion: RoDEoDB provides an effective solution for robustly recovering Euclidean distance geometry in the presence of outliers and partial data, showing promising results in practical applications.

Abstract: Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean
distances of a given point configuration, finds many applications in modern
machine learning. This paper considers the setting where only a set of anchor
nodes is used to collect the distances between themselves and the rest. In the
presence of potential outliers, it results in a structured partial observation
on EDM with partial corruptions. Note that an EDM can be connected to a
positive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by
recent development of non-orthogonal dual basis in optimization, we propose a
novel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual
Basis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the
underlying point configuration. The exact recovery guarantees have been
established in terms of both the Gram matrix and point configuration, under
some mild conditions. Empirical experiments show superior performance of
RoDEoDB on sensor localization and molecular conformation datasets.

</details>


### [167] [Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia](https://arxiv.org/abs/2505.18421)
*Junyi Fan,Shuheng Chen,Li Sun,Yong Si,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: cs.LG

TL;DR: 研究人员利用MIMIC-IV数据库中的数据，通过机器学习方法筛选出七个关键预测因素，并构建了逻辑回归和Cox回归模型以预测患有再生障碍性贫血的ICU患者的7天、14天及28天死亡率。逻辑回归模型表现出更优的性能（AUROC值分别为0.8227、0.8311和0.8298），并在外部验证中保持了一定的准确性。基于该模型开发的交互式列线图有助于临床医生进行个性化风险评估和决策。


<details>
  <summary>Details</summary>
Motivation: 再障是一种罕见且危及生命的血液疾病，其ICU入院通常预示着严重并发症或疾病进展，因此早期风险评估对于临床决策和资源分配至关重要。然而，目前尚缺乏针对此类患者短期死亡率的有效预测工具。

Method: 研究使用MIMIC-IV数据库识别出患有再障并入住ICU的患者，从人口统计学、综合指标、实验室结果、合并症和用药五个领域提取了超过400个变量。通过机器学习方法将这些变量缩减至七个关键预测因子，并构建了逻辑回归和Cox回归模型来预测7天、14天和28天的死亡率。使用AUROC评价模型性能，并通过eICU数据库进行外部验证。最后，基于逻辑回归模型开发了交互式列线图。

Result: 逻辑回归模型在预测7天、14天和28天死亡率时表现出更高的AUROC值（分别为0.8227、0.8311和0.8298），优于Cox模型。外部验证结果显示AUROC值为0.7391、0.7119和0.7093，表明模型具有一定的泛化能力。

Conclusion: 本研究确定了七个简洁的关键预测因子，其中APS III最为重要，并构建了经过验证且可推广的列线图，能够准确估计患有再障的ICU患者的短期死亡风险。这些工具可能帮助临床医生实现个性化风险分层和床旁决策支持。

Abstract: Aplastic anemia is a rare, life-threatening hematologic disorder
characterized by pancytopenia and bone marrow failure. ICU admission in these
patients often signals critical complications or disease progression, making
early risk assessment crucial for clinical decision-making and resource
allocation. In this study, we used the MIMIC-IV database to identify ICU
patients diagnosed with aplastic anemia and extracted clinical features from
five domains: demographics, synthetic indicators, laboratory results,
comorbidities, and medications. Over 400 variables were reduced to seven key
predictors through machine learning-based feature selection. Logistic
regression and Cox regression models were constructed to predict 7-, 14-, and
28-day mortality, and their performance was evaluated using AUROC. External
validation was conducted using the eICU Collaborative Research Database to
assess model generalizability. Among 1,662 included patients, the logistic
regression model demonstrated superior performance, with AUROC values of
0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively,
compared to the Cox model. External validation yielded AUROCs of 0.7391,
0.7119, and 0.7093. Interactive nomograms were developed based on the logistic
regression model to visually estimate individual patient risk. In conclusion,
we identified a concise set of seven predictors, led by APS III, to build
validated and generalizable nomograms that accurately estimate short-term
mortality in ICU patients with aplastic anemia. These tools may aid clinicians
in personalized risk stratification and decision-making at the point of care.

</details>


### [168] [Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.18433)
*Zhiyao Zhang,Myeung Suk Oh,FNU Hairi,Ziyue Luo,Alvaro Velasquez,Jia Liu*

Main category: cs.LG

TL;DR: The paper develops a deep neural actor-critic method for decentralized MARL with global optimality guarantee and finite-time convergence rate, bridging the gap between practical success and theoretical understanding.


<details>
  <summary>Details</summary>
Motivation: To address the gap between practical success of deep neural actor-critic methods in decentralized MARL and limited theoretical convergence studies which are mostly confined to linear function approximation.

Method: Propose a deep neural actor-critic method for decentralized MARL where both actor and critic components are inherently non-linear, ensuring global optimality guarantee with finite-time convergence.

Result: Achieves the first global convergence result for deep neural actor-critic methods in MARL with a convergence rate of O(1/T). Numerical experiments verify the theoretical results.

Conclusion: This work bridges the gap between practical application and theoretical understanding of deep neural actor-critic methods in decentralized MARL.

Abstract: Actor-critic methods for decentralized multi-agent reinforcement learning
(MARL) facilitate collaborative optimal decision making without centralized
coordination, thus enabling a wide range of applications in practice. To date,
however, most theoretical convergence studies for existing actor-critic
decentralized MARL methods are limited to the guarantee of a stationary
solution under the linear function approximation. This leaves a significant gap
between the highly successful use of deep neural actor-critic for decentralized
MARL in practice and the current theoretical understanding. To bridge this gap,
in this paper, we make the first attempt to develop a deep neural actor-critic
method for decentralized MARL, where both the actor and critic components are
inherently non-linear. We show that our proposed method enjoys a global
optimality guarantee with a finite-time convergence rate of O(1/T), where T is
the total iteration times. This marks the first global convergence result for
deep neural actor-critic methods in the MARL literature. We also conduct
extensive numerical experiments, which verify our theoretical results.

</details>


### [169] [DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces](https://arxiv.org/abs/2505.18441)
*Romeo Valentin,Sydney M. Katz,Vincent Vanhoucke,Mykel J. Kochenderfer*

Main category: cs.LG

TL;DR: Dictionary learning is crucial for understanding large transformer models. This paper proposes Double-Batch KSVD (DB-KSVD), a scalable algorithm that performs well on high-dimensional data with large sample sizes, achieving results comparable to sparse autoencoders (SAEs).


<details>
  <summary>Details</summary>
Motivation: To improve the mechanistic interpretability of large transformer models by developing a more sophisticated and scalable dictionary learning algorithm than current methods such as SAEs.

Method: Proposed DB-KSVD, an adaptation of the classic KSVD algorithm, designed to handle datasets with millions of samples and thousands of dimensions. It was tested on disentangling embeddings of the Gemma-2-2B model.

Result: DB-KSVD achieved competitive results compared to established approaches based on SAEs when evaluated on six metrics from the SAEBench benchmark.

Conclusion: (i) SAEs are effective at solving the dictionary learning problem; (ii) Traditional optimization methods like KSVD can be scaled to handle large problems, opening new possibilities for future research.

Abstract: Dictionary learning has recently emerged as a promising approach for
mechanistic interpretability of large transformer models. Disentangling
high-dimensional transformer embeddings, however, requires algorithms that
scale to high-dimensional data with large sample sizes. Recent work has
explored sparse autoencoders (SAEs) for this problem. However, SAEs use a
simple linear encoder to solve the sparse encoding subproblem, which is known
to be NP-hard. It is therefore interesting to understand whether this structure
is sufficient to find good solutions to the dictionary learning problem or if a
more sophisticated algorithm could find better solutions. In this work, we
propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm
that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich
theoretical foundations of KSVD but scales to datasets with millions of samples
and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by
disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics
from the SAEBench benchmark, where we achieve competitive results when compared
to established approaches based on SAEs. By matching SAE performance with an
entirely different optimization approach, our results suggest that (i) SAEs do
find strong solutions to the dictionary learning problem and (ii) that
traditional optimization approaches can be scaled to the required problem
sizes, offering a promising avenue for further research. We provide an
implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.

</details>


### [170] [Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting](https://arxiv.org/abs/2505.18442)
*Zhining Liu,Ze Yang,Xiao Lin,Ruizhong Qiu,Tianxin Wei,Yada Zhu,Hendrik Hamann,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: In this paper, the authors propose TimeFuse, a framework that adaptively combines different forecasting models for time-series prediction. It uses meta-features and a learnable fusor to determine optimal model weights per sample, leading to consistent improvements over individual models.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is based on the observation that no single forecasting model consistently outperforms others across all test samples. Instead, each model has its own strengths in specific cases. Therefore, there is a need to explore how to adaptively use the strengths of various models for different samples.

Method: The proposed method, TimeFuse, is a framework for collective time-series forecasting with sample-level adaptive fusion of heterogeneous models. It uses meta-features to characterize input time series and trains a learnable fusor to predict optimal model fusion weights for any given input. The fusor can leverage samples from diverse datasets for joint training, allowing it to adapt to a wide variety of temporal patterns.

Result: Extensive experiments demonstrate the effectiveness of TimeFuse in various long-/short-term forecasting tasks. It achieves near-universal improvement over the state-of-the-art individual models.

Conclusion: TimeFuse provides an effective way to adaptively fuse different forecasting models at the sample level, leading to better generalization and performance improvements in time-series forecasting.

Abstract: Time-series forecasting plays a critical role in many real-world
applications. Although increasingly powerful models have been developed and
achieved superior results on benchmark datasets, through a fine-grained
sample-level inspection, we find that (i) no single model consistently
outperforms others across different test samples, but instead (ii) each model
excels in specific cases. These findings prompt us to explore how to adaptively
leverage the distinct strengths of various forecasting models for different
samples. We introduce TimeFuse, a framework for collective time-series
forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse
utilizes meta-features to characterize input time series and trains a learnable
fusor to predict optimal model fusion weights for any given input. The fusor
can leverage samples from diverse datasets for joint training, allowing it to
adapt to a wide variety of temporal patterns and thus generalize to new inputs,
even from unseen datasets. Extensive experiments demonstrate the effectiveness
of TimeFuse in various long-/short-term forecasting tasks, achieving
near-universal improvement over the state-of-the-art individual models. Code is
available at https://github.com/ZhiningLiu1998/TimeFuse.

</details>


### [171] [Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning](https://arxiv.org/abs/2505.18447)
*Chi Zhang,Ziying Jia,George K. Atia,Sihong He,Yue Wang*

Main category: cs.LG

TL;DR: Transfer reinforcement learning uses data from related source domains to derive a near-optimal policy for a target environment with limited data. A novel framework based on the pessimism principle addresses key challenges by providing an optimized lower bound on target performance and exhibiting monotonic improvement.


<details>
  <summary>Details</summary>
Motivation: Transfer reinforcement learning faces challenges such as lack of performance guarantees for transferred policies and risk of negative transfer when multiple source domains are involved.

Method: The paper proposes a framework based on the pessimism principle that constructs and optimizes a conservative estimation of the target domain's performance, addressing the challenges by providing a lower bound on target performance and ensuring monotonic improvement with respect to the quality of the source domains.

Result: Two types of conservative estimations were constructed, their effectiveness was rigorously characterized, and efficient distributed algorithms with convergence guarantees were developed.

Conclusion: The framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning.

Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a
target environment with limited data by leveraging abundant data from related
source domains. However, it faces two key challenges: the lack of performance
guarantees for the transferred policy, which can lead to undesired actions, and
the risk of negative transfer when multiple source domains are involved. We
propose a novel framework based on the pessimism principle, which constructs
and optimizes a conservative estimation of the target domain's performance. Our
framework effectively addresses the two challenges by providing an optimized
lower bound on target performance, ensuring safe and reliable decisions, and by
exhibiting monotonic improvement with respect to the quality of the source
domains, thereby avoiding negative transfer. We construct two types of
conservative estimations, rigorously characterize their effectiveness, and
develop efficient distributed algorithms with convergence guarantees. Our
framework provides a theoretically sound and practically robust solution for
transfer learning in reinforcement learning.

</details>


### [172] [$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts](https://arxiv.org/abs/2505.18451)
*Toshiaki Koike-Akino,Jing Liu,Ye Wang*

Main category: cs.LG

TL;DR: 为了应对大模型的高计算需求，提出了无需重新训练的激活感知压缩技术。然而，由于依赖校准数据，未知下游任务可能会出现领域偏移问题。通过高效的校准，激活感知剪枝可以自适应地为每个提示执行，并在推理时降低复杂度。我们将其建模为微专家混合体，称为$\mu$-MoE。多个实验表明，$\mu$-MoE可以动态适应任务/提示相关的结构化稀疏性。


<details>
  <summary>Details</summary>
Motivation: 大模型的计算需求巨大，而现有的激活感知压缩技术虽然无需重新训练，但依赖校准数据，可能导致未知下游任务中的领域偏移问题。

Method: 提出了一种名为$\mu$-MoE的方法，通过高效的校准过程，使激活感知剪枝能够自适应地应用于每个提示，从而在推理阶段减少计算复杂度。该方法被建模为微专家混合体。

Result: 实验结果表明，$\mu$-MoE可以在不增加额外计算负担的情况下，动态适应于任务或提示相关的结构化稀疏性。

Conclusion: $\mu$-MoE是一种有效的解决方案，能够在保证性能的同时降低推理复杂度，适用于不同的任务和提示场景。

Abstract: To tackle the huge computational demand of large foundation models,
activation-aware compression techniques without retraining have been
introduced. However, since these rely on calibration data, domain shift may
arise for unknown downstream tasks. With a computationally efficient
calibration, activation-aware pruning can be executed for every prompt
adaptively, yet achieving reduced complexity at inference. We formulate it as a
mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate
that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured
sparsity on the fly.

</details>


### [173] [Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation](https://arxiv.org/abs/2505.18461)
*Morteza Karimzadeh,Zhongying Wang,James L. Crooks*

Main category: cs.LG

TL;DR: 在动态和高空间异质性的应用领域（如日均PM2.5浓度估算）中，探讨了将地理位置信息纳入深度学习模型的影响。研究发现，使用预训练的位置编码器（如GeoCLIP）可以提高预测性能和地理普适性，但也会产生一些伪影模式。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在地理空间应用中表现出色，但地理位置信息对提升模型性能和地理普适性的具体作用尚未得到充分研究。因此，本文旨在量化地理位置信息在动态和高空间异质性场景中的影响。

Method: 研究基于一个最近发布的、在美国本土数据上达到最先进水平的PM2.5估算深度学习模型。通过三种方法引入地理位置信息：1) 不使用地理位置作为基线；2) 使用原始地理坐标；3) 利用预训练位置编码器。评估分别在区域内（WR）和区域外（OoR）场景下进行。

Result: 结果表明，简单地加入原始地理坐标可以在区域内提升性能，但可能降低跨区域的普适性。相比之下，预训练位置编码器（如GeoCLIP）不仅提高了预测性能，还增强了地理普适性。然而，定性分析显示某些区域存在由高阶基函数和稀疏上游样本引起的伪影模式，且不同位置编码器的表现存在差异。

Conclusion: 预训练位置编码器能够有效提升深度学习模型在动态和高空间异质性任务中的预测性能和地理普适性，但需要进一步研究以减少伪影模式的影响。

Abstract: Deep learning models have demonstrated success in geospatial applications,
yet quantifying the role of geolocation information in enhancing model
performance and geographic generalizability remains underexplored. A new
generation of location encoders have emerged with the goal of capturing
attributes present at any given location for downstream use in predictive
modeling. Being a nascent area of research, their evaluation has remained
largely limited to static tasks such as species distributions or average
temperature mapping. In this paper, we discuss and quantify the impact of
incorporating geolocation into deep learning for a real-world application
domain that is characteristically dynamic (with fast temporal change) and
spatially heterogeneous at high resolutions: estimating surface-level daily
PM2.5 levels using remotely sensed and ground-level data. We build on a
recently published deep learning-based PM2.5 estimation model that achieves
state-of-the-art performance on data observed in the contiguous United States.
We examine three approaches for incorporating geolocation: excluding
geolocation as a baseline, using raw geographic coordinates, and leveraging
pretrained location encoders. We evaluate each approach under within-region
(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance
metrics indicate that while na\"ive incorporation of raw geographic coordinates
improves within-region performance by retaining the interpolative value of
geographic location, it can hinder generalizability across regions. In
contrast, pretrained location encoders like GeoCLIP enhance predictive
performance and geographic generalizability for both WR and OoR scenarios.
However, qualitative analysis reveals artifact patterns caused by high-degree
basis functions and sparse upstream samples in certain areas, and ablation
results indicate varying performance among location encoders...

</details>


### [174] [Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey](https://arxiv.org/abs/2505.18475)
*Mengran Li,Pengyu Zhang,Wenbin Xing,Yijia Zheng,Klim Zaporojets,Junzhou Chen,Ronghui Zhang,Yong Zhang,Siyuan Gong,Jia Hu,Xiaolei Ma,Zhiyuan Liu,Paul Groth,Marcel Worring*

Main category: cs.LG

TL;DR: Graphs are used for non-Euclidean data, but traditional methods face challenges such as incompleteness, imbalance, cross-domain heterogeneity, and dynamic instability. This paper reviews how Large Language Models (LLMs) can be integrated with graph learning to address these challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of how LLMs can be integrated with graph learning to address the fundamental challenges in real-world graph data, including incompleteness, imbalance, cross-domain heterogeneity, and dynamic instability.

Method: Review both traditional solutions and modern LLM-driven approaches for each challenge faced by graph learning methods. Highlight the unique advantages that LLMs offer in tackling these challenges.

Result: Demonstrates the potential of LLMs in overcoming the limitations of traditional graph learning methods through rich semantic reasoning and external knowledge.

Conclusion: Discusses open research questions and promising future directions in integrating LLMs with graph learning. Provides a curated repository of recent advances on graph learning challenges.

Abstract: Graphs are a widely used paradigm for representing non-Euclidean data, with
applications ranging from social network analysis to biomolecular prediction.
Conventional graph learning approaches typically rely on fixed structural
assumptions or fully observed data, limiting their effectiveness in more
complex, noisy, or evolving settings. Consequently, real-world graph data often
violates the assumptions of traditional graph learning methods, in particular,
it leads to four fundamental challenges: (1) Incompleteness, real-world graphs
have missing nodes, edges, or attributes; (2) Imbalance, the distribution of
the labels of nodes or edges and their structures for real-world graphs are
highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains
exhibit incompatible feature spaces or structural patterns; and (4) Dynamic
Instability, graphs evolve over time in unpredictable ways. Recent advances in
Large Language Models (LLMs) offer the potential to tackle these challenges by
leveraging rich semantic reasoning and external knowledge. This survey provides
a comprehensive review of how LLMs can be integrated with graph learning to
address the aforementioned challenges. For each challenge, we review both
traditional solutions and modern LLM-driven approaches, highlighting how LLMs
contribute unique advantages. Finally, we discuss open research questions and
promising future directions in this emerging interdisciplinary field. To
support further exploration, we have curated a repository of recent advances on
graph learning challenges:
https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.

</details>


### [175] [The Prompt is Mightier than the Example](https://arxiv.org/abs/2505.18485)
*Shengzhe Xu,Nikhil Muralidhar,Naren Ramakrishnan*

Main category: cs.LG

TL;DR: 在本文中，作者提出了知识引导提示（KGP）作为一种新的优化方法，探讨了其替代情境学习（ICL）成本的能力。通过实验揭示了ICL与KGP之间的权衡，并展示了KGP可成为生成合成数据的可扩展替代方案。


<details>
  <summary>Details</summary>
Motivation: 尽管使用大量示例进行情境学习可以显著提高大型语言模型生成内容的质量，但获取大量示例可能昂贵或不可行。同时，随着模型规模增大，内置先验知识变得丰富，可能部分取代具体数据示例的需求。

Method: 引入知识引导提示（KGP）作为优化提示的新方法，将领域知识显式注入提示中，减少对情境学习示例的依赖。通过系统实验探索ICL和KGP之间的权衡关系。

Result: 实验揭示了一个经验缩放定律，表明生成合成数据的质量如何随领域知识增加和示例数量减少而变化。结果证明，KGP可以作为情境学习的可扩展替代方案或补充。

Conclusion: 知识引导提示（KGP）为合成数据生成提供了新的途径，可以有效减少对大量示例的依赖，从而降低成本并提高效率。

Abstract: Numerous recent prompt optimization approaches like chain-of-thought, have
been demonstrated to significantly improve the quality of content generated by
large language models (LLMs). In-context learning (ICL), a recent paradigm
where a few representative examples guide content generation has also led to
strong improvements in generation quality of LLM generated content. This idea
has been applied to great effect in synthetic tabular data generation, where
LLMs, through effective use of ICL and prompt optimization, can generate data
that approximate samples from complex, heterogeneous distributions based on
representative examples. However, ensuring high-fidelity synthetic data often
requires a very large number of ICL examples which may be unavailable or costly
to obtain. At the same time, as LLMs get larger and larger, their in-built
prior knowledge becomes vast and can potentially substitute for specific data
examples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new
knob in prompt optimization and explore the ability of KGP-based prompt
optimization to offset the cost of ICL. Specifically, we explore the question
`how many examples can a prompt substitute for?' and explore knowledge-guided
prompting (KGP) where domain knowledge, either inferred or available, is
explicitly injected into the prompt, reducing dependence on ICL examples. Our
experiments systematically explore the trade-off between ICL and KGP, revealing
an empirical scaling law that quantifies how quality of generated synthetic
data varies with increasing domain knowledge and decreasing example count. Our
results demonstrate that knowledge-guided prompting can be a scalable
alternative, or addition, to in-context examples, unlocking new approaches to
synthetic data generation.

</details>


### [176] [Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications](https://arxiv.org/abs/2505.18488)
*Yanxiang Zhang,Zheng Xu,Shanshan Wu,Yuanbo Zhang,Daniel Ramage*

Main category: cs.LG

TL;DR: The paper explores using LLMs for error correction in mobile applications, synthesizing a high-quality dataset of error correction pairs and adapting it to the mobile domain through reweighting samples based on live A/B test metrics.


<details>
  <summary>Details</summary>
Motivation: Error correction is crucial when using large language models (LLMs) to assist with typing on mobile devices. There is a need to evaluate and enhance LLMs specifically for this purpose.

Method: Firstly, LLMs are prompted with error correction domain knowledge to create a scalable addition to data synthesis pipelines. Secondly, synthetic data distribution is adapted to match the mobile application domain by reweighting samples, which involves learning a reweighting model that predicts live A/B test metrics from offline evaluation data and scores from an on-device language model. Lastly, best practices are presented for mixing synthetic data with other sources to improve performance.

Result: A high-quality dataset of error correction pairs is synthesized, and the synthetic data distribution is successfully adapted to the mobile application domain, improving model performance in both offline evaluations and live A/B testing.

Conclusion: Using LLMs to synthesize datasets and adapt them to specific domains can significantly enhance error correction capabilities in mobile applications.

Abstract: Error correction is an important capability when applying large language
models (LLMs) to facilitate user typing on mobile devices. In this paper, we
use LLMs to synthesize a high-quality dataset of error correction pairs to
evaluate and improve LLMs for mobile applications. We first prompt LLMs with
error correction domain knowledge to build a scalable and reliable addition to
the existing data synthesis pipeline. We then adapt the synthetic data
distribution to match the mobile application domain by reweighting the samples.
The reweighting model is learnt by predicting (a handful of) live A/B test
metrics when deploying LLMs in production, given the LLM performance on offline
evaluation data and scores from a small privacy-preserving on-device language
model. Finally, we present best practices for mixing our synthetic data with
other data sources to improve model performance on error correction in both
offline evaluation and production live A/B testing.

</details>


### [177] [FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation](https://arxiv.org/abs/2505.18494)
*Zihao Peng,Jiandian Zeng,Boyuan Li,Guo Li,Shengbo Chen,Tian Wang*

Main category: cs.LG

TL;DR: In this paper, researchers identify a problem with current Federated Learning methods that use Low-Rank Adaptation (LoRA), specifically that they lack formal convergence guarantees due to parameter truncation and biased gradient updates. To solve this issue, they introduce FedHL, a new framework which uses full-rank global models to reduce truncation bias and derives optimal aggregation weights for better performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the recognition that while Low-Rank Adaptation (LoRA) in Federated Learning offers benefits like low communication costs and strong performance, existing methods suffer from issues such as parameter truncation and biased gradient updates, which degrade model performance over training rounds.

Method: The proposed method, FedHL, leverages a full-rank global model as an aggregation basis to eliminate direct truncation bias associated with adapting client-specific LoRA ranks. It also derives theoretically optimal aggregation weights by minimizing the gradient drift term in the convergence upper bound.

Result: The analysis shows that FedHL ensures a convergence rate of O(1/√T). Experimental results on various real-world datasets indicate a 1-3% improvement compared to several state-of-the-art methods.

Conclusion: FedHL addresses the shortcomings of previous FL methods using LoRA by providing formal convergence guarantees and improving performance through the reduction of truncation errors and more accurate gradient updates.

Abstract: Federated Learning (FL) facilitates the fine-tuning of Foundation Models
(FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining
popularity due to its low communication costs and strong performance. While
recent work acknowledges the benefits of heterogeneous LoRA in FL and
introduces flexible algorithms to support its implementation, our theoretical
analysis reveals a critical gap: existing methods lack formal convergence
guarantees due to parameter truncation and biased gradient updates.
Specifically, adapting client-specific LoRA ranks necessitates truncating
global parameters, which introduces inherent truncation errors and leads to
subsequent inaccurate gradient updates that accumulate over training rounds,
ultimately degrading performance. To address the above issues, we propose
\textbf{FedHL}, a simple yet effective \textbf{Fed}erated Learning framework
tailored for \textbf{H}eterogeneous \textbf{L}oRA. By leveraging the full-rank
global model as a calibrated aggregation basis, FedHL eliminates the direct
truncation bias from initial alignment with client-specific ranks. Furthermore,
we derive the theoretically optimal aggregation weights by minimizing the
gradient drift term in the convergence upper bound. Our analysis shows that
FedHL guarantees $\mathcal{O}(1/\sqrt{T})$ convergence rate, and experiments on
multiple real-world datasets demonstrate a 1-3\% improvement over several
state-of-the-art methods.

</details>


### [178] [Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking](https://arxiv.org/abs/2505.18495)
*Chen-Hao Chao,Wei-Fang Sun,Hanwen Liang,Chun-Yi Lee,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: A new method called Partial masking scheme (Prime) improves Masked diffusion models (MDM) by allowing tokens intermediate states, which reduces redundant computation and enhances performance in text and image generative tasks.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of MDM is due to the redundant computation when processing identical inputs during consecutive sampling steps. To solve this problem, the researchers aimed to develop a more efficient model that can handle partially observed token information.

Method: The Partial masking scheme (Prime) was introduced as an enhancement to MDM. It allows tokens to take intermediate states between masked and unmasked states, enabling fine-grained denoising process. A variational training objective and architectural design were also developed to accommodate these intermediate states.

Result: Prime outperformed previous MDM, autoregressive models, and their hybrid variants on text data with a perplexity of 15.36 on OpenWebText. On image data, it achieved competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32.

Conclusion: The proposed Prime method demonstrates superior performance across various generative modeling tasks, proving its effectiveness in both text and image data.

Abstract: Masked diffusion models (MDM) are powerful generative models for discrete
data that generate samples by progressively unmasking tokens in a sequence.
Each token can take one of two states: masked or unmasked. We observe that
token sequences often remain unchanged between consecutive sampling steps;
consequently, the model repeatedly processes identical inputs, leading to
redundant computation. To address this inefficiency, we propose the Partial
masking scheme (Prime), which augments MDM by allowing tokens to take
intermediate states interpolated between the masked and unmasked states. This
design enables the model to make predictions based on partially observed token
information, and facilitates a fine-grained denoising process. We derive a
variational training objective and introduce a simple architectural design to
accommodate intermediate-state inputs. Our method demonstrates superior
performance across a diverse set of generative modeling tasks. On text data, it
achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM
(21.52), autoregressive models (17.54), and their hybrid variants (17.58),
without relying on an autoregressive formulation. On image data, it attains
competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable
to leading continuous generative models.

</details>


### [179] [G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning](https://arxiv.org/abs/2505.18499)
*Xiaojun Guo,Ang Li,Yifei Wang,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: 尽管大语言模型（LLMs）取得了显著进展，但在图相关任务中的表现仍然有限。本文介绍了一种名为G1的方法，通过在合成图理论任务上使用强化学习（RL），显著提升了LLMs的图推理能力。该方法利用了Erd~os数据集，这是目前最大的图推理数据集，包含50个不同难度的图理论任务。实验结果表明，经过RL训练的模型在图推理方面有了显著提升，并且具有强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在图相关任务上的表现有限，而现有的预训练图基础模型或监督微调方法面临着大规模、通用表示的图数据稀缺的问题。

Method: 引入G1方法，通过在合成图理论任务上应用强化学习来扩展LLMs的图推理能力。同时，构建了Erd~os数据集，用于支持强化学习训练。

Result: G1方法显著提高了图推理能力，甚至3B参数的模型性能超过了Qwen2.5-72B-Instruct（24倍参数量）。此外，强化学习训练的模型在未见过的任务、领域和图编码方案上表现出强大的零样本泛化能力。

Conclusion: 通过在图理论任务上对LLMs进行强化学习微调，可以高效、可扩展地构建强大的图推理器，结合了预训练LLMs的能力与自动合成数据的优势，证明了LLMs具备可通过强化学习成功激发的图理解能力。

Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress,
their proficiency in graph-related tasks remains notably limited, hindering the
development of truly general-purpose models. Previous attempts, including
pretraining graph foundation models or employing supervised fine-tuning, often
face challenges such as the scarcity of large-scale, universally represented
graph data. We introduce G1, a simple yet effective approach demonstrating that
Reinforcement Learning (RL) on synthetic graph-theoretic tasks can
significantly scale LLMs' graph reasoning abilities. To enable RL training, we
curate Erd\~os, the largest graph reasoning dataset to date comprising 50
diverse graph-theoretic tasks of varying difficulty levels, 100k training data
and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1
obtains substantial improvements in graph reasoning, where our finetuned 3B
model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also
show strong zero-shot generalization to unseen tasks, domains, and graph
encoding schemes, including other graph-theoretic benchmarks as well as
real-world node classification and link prediction tasks, without compromising
general reasoning abilities. Our findings offer an efficient, scalable path for
building strong graph reasoners by finetuning LLMs with RL on graph-theoretic
tasks, which combines the strengths of pretrained LLM capabilities with
abundant, automatically generated synthetic data, suggesting that LLMs possess
graph understanding abilities that RL can elicit successfully.

</details>


### [180] [How Particle System Theory Enhances Hypergraph Message Passing](https://arxiv.org/abs/2505.18505)
*Yixuan Ma,Kai Yi,Pietro Lio,Shi Jin,Yu Guang Wang*

Main category: cs.LG

TL;DR: The paper proposes a novel hypergraph message passing framework inspired by interacting particle systems to model higher-order relationships and complex interactions, incorporating attraction, repulsion, and Allen-Cahn forcing terms. The approach mitigates over-smoothing and heterophily, allowing for deeper message passing and demonstrating competitive performance on real-world hypergraph node classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for modeling relationships in natural phenomena may not adequately capture higher-order relationships and complex interactions beyond pairwise connections. This motivates the development of a new framework that can more effectively model these intricate dynamics.

Method: The method introduces a hypergraph message passing framework based on interacting particle systems. Hyperedges act as fields inducing shared node dynamics with attraction, repulsion, and Allen-Cahn forcing terms. Both first-order and second-order particle system equations are used to model the dynamics, with the second-order system being more stable and permitting deeper message passing. A stochastic element is added to deterministic message passing to account for interaction uncertainties. Theoretically, the approach maintains a positive lower bound on the hypergraph Dirichlet energy during propagation to mitigate over-smoothing.

Result: The models demonstrate competitive performance on diverse real-world hypergraph node classification tasks, excelling on both homophilic and heterophilic datasets. They show effectiveness in capturing complete interactions while mitigating issues like over-smoothing and heterophily.

Conclusion: The proposed hypergraph message passing framework offers a powerful tool for modeling higher-order relationships and complex interactions in natural phenomena. By incorporating principles from interacting particle systems and addressing challenges such as over-smoothing and heterophily, the framework enables effective deep message passing and achieves strong results on hypergraph node classification tasks.

Abstract: Hypergraphs effectively model higher-order relationships in natural
phenomena, capturing complex interactions beyond pairwise connections. We
introduce a novel hypergraph message passing framework inspired by interacting
particle systems, where hyperedges act as fields inducing shared node dynamics.
By incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles
of varying classes and features achieve class-dependent equilibrium, enabling
separability through the particle-driven message passing. We investigate both
first-order and second-order particle system equations for modeling these
dynamics, which mitigate over-smoothing and heterophily thus can capture
complete interactions. The more stable second-order system permits deeper
message passing. Furthermore, we enhance deterministic message passing with
stochastic element to account for interaction uncertainties. We prove
theoretically that our approach mitigates over-smoothing by maintaining a
positive lower bound on the hypergraph Dirichlet energy during propagation and
thus to enable hypergraph message passing to go deep. Empirically, our models
demonstrate competitive performance on diverse real-world hypergraph node
classification tasks, excelling on both homophilic and heterophilic datasets.

</details>


### [181] [SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs](https://arxiv.org/abs/2505.18511)
*Zheyan Li,Yuantu Zhu,Hao Ni,Siran Li,Bingguang Chen,Qi Meng*

Main category: cs.LG

TL;DR: The paper introduces SPDEBench, a tool for solving stochastic partial differential equations (SPDEs) using machine learning methods. It addresses the lack of comprehensive datasets for SPDE learning and highlights the impact of computational error on ML model performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling physical processes with rough spatio-temporal dynamics using SPDEs driven by random noise, and to overcome the lack of extensive and unified datasets for SPDE learning that account for computational errors and renormalization.

Method: Developed SPDEBench which solves significant SPDEs on 1D or 2D tori driven by white noise via ML methods. Constructed new datasets based on renormalization process for singular SPDEs and proposed novel ML models achieving state-of-the-art results.

Result: Benchmarking shows that naive application of ML models on data without specifying numerical schemes can lead to significant errors. SPDEBench ensures reproducibility across various SPDE datasets and offers flexibility for incorporating new datasets and ML baselines.

Conclusion: SPDEBench is an open-source codebase providing valuable resource for the community to accurately evaluate ML models for SPDEs.

Abstract: Stochastic Partial Differential Equations (SPDEs) driven by random noise play
a central role in modelling physical processes whose spatio-temporal dynamics
can be rough, such as turbulence flows, superconductors, and quantum dynamics.
To efficiently model these processes and make predictions, machine learning
(ML)-based surrogate models are proposed, with their network architectures
incorporating the spatio-temporal roughness in their design. However, it lacks
an extensive and unified datasets for SPDE learning; especially, existing
datasets do not account for the computational error introduced by noise
sampling and the necessary renormalization required for handling singular
SPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of
physical significance (e.g., the $\Phi^4_d$, wave, incompressible
Navier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via
ML methods. New datasets for singular SPDEs based on the renormalization
process have been constructed, and novel ML models achieving the best results
to date have been proposed. In particular, we investigate the impact of
computational error introduced by noise sampling and renormalization on the
performance comparison of ML models and highlight the importance of selecting
high-quality test data for accurate evaluation. Results are benchmarked with
traditional numerical solvers and ML-based models, including FNO, NSPDE and
DLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models
on data without specifying the numerical schemes can lead to significant errors
and misleading conclusions. Our SPDEBench provides an open-source codebase that
ensures full reproducibility of benchmarking across a variety of SPDE datasets
while offering the flexibility to incorporate new datasets and machine learning
baselines, making it a valuable resource for the community.

</details>


### [182] [Enhancing Training Data Attribution with Representational Optimization](https://arxiv.org/abs/2505.18513)
*Weiwei Sun,Haokun Liu,Nikhil Kandpal,Colin Raffel,Yiming Yang*

Main category: cs.LG

TL;DR: AirRep is a scalable representation-based approach that learns task-specific and model-aligned representations for training data attribution (TDA), achieving performance comparable to gradient-based methods but with significantly higher efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing TDA methods either have high computational costs (gradient-based) or limited fidelity due to heuristic embeddings (representation-based).

Method: AirRep introduces a trainable encoder optimized for attribution quality and an attention-based pooling mechanism for group-wise influence estimation. It uses a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions.

Result: Experiments show AirRep matches state-of-the-art gradient-based approaches in performance while being nearly two orders of magnitude more efficient during inference. It also demonstrates robustness and generalization across tasks and models.

Conclusion: AirRep provides a scalable, efficient, and robust solution for TDA, closing the gap between gradient-based and representation-based approaches.

Abstract: Training data attribution (TDA) methods aim to measure how training data
impacts a model's predictions. While gradient-based attribution methods, such
as influence functions, offer theoretical grounding, their computational costs
make them impractical for large-scale applications. Representation-based
approaches are far more scalable, but typically rely on heuristic embeddings
that are not optimized for attribution, limiting their fidelity. To address
these challenges, we propose AirRep, a scalable, representation-based approach
that closes this gap by learning task-specific and model-aligned
representations optimized explicitly for TDA. AirRep introduces two key
innovations: a trainable encoder tuned for attribution quality, and an
attention-based pooling mechanism that enables accurate estimation of
group-wise influence. We train AirRep using a ranking objective over
automatically constructed training subsets labeled by their empirical effect on
target predictions. Experiments on instruction-tuned LLMs demonstrate that
AirRep achieves performance on par with state-of-the-art gradient-based
approaches while being nearly two orders of magnitude more efficient at
inference time. Further analysis highlights its robustness and generalization
across tasks and models. Our code is available at
https://github.com/sunnweiwei/AirRep.

</details>


### [183] [Test-Time Adaptation with Binary Feedback](https://arxiv.org/abs/2505.18514)
*Taeckyung Lee,Sorn Chottananurak,Junsu Kim,Jinwoo Shin,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: In the abstract, the authors present BiTTA, a novel approach for test-time adaptation (TTA) using binary feedback to reduce labeling costs and improve model accuracy under severe domain shifts.


<details>
  <summary>Details</summary>
Motivation: Deep learning models encounter significant performance degradation when there are domain shifts between training and test data. Current TTA methods either fail under severe domain shifts or require impractical full-class labels with high labeling costs.

Method: The authors introduce a new TTA setting that uses binary feedback from annotators to indicate whether model predictions are correct. They propose BiTTA, a dual-path optimization framework leveraging reinforcement learning to balance binary feedback-guided adaptation on uncertain samples with agreement-based self-adaptation on confident predictions.

Result: Experiments demonstrate that BiTTA achieves 13.3% point accuracy improvements over state-of-the-art baselines, showing its effectiveness in handling severe distribution shifts with minimal labeling effort.

Conclusion: BiTTA is an effective solution for adapting deep learning models to severe domain shifts with significantly reduced labeling burden.

Abstract: Deep learning models perform poorly when domain shifts exist between training
and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue
by adapting pre-trained models using only unlabeled test samples. However,
existing TTA methods can fail under severe domain shifts, while recent active
TTA approaches requiring full-class labels are impractical due to high labeling
costs. To address this issue, we introduce a new setting of TTA with binary
feedback. This setting uses a few binary feedback inputs from annotators to
indicate whether model predictions are correct, thereby significantly reducing
the labeling burden of annotators. Under the setting, we propose BiTTA, a novel
dual-path optimization framework that leverages reinforcement learning to
balance binary feedback-guided adaptation on uncertain samples with
agreement-based self-adaptation on confident predictions. Experiments show
BiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines,
demonstrating its effectiveness in handling severe distribution shifts with
minimal labeling effort. The source code is available at
https://github.com/taeckyung/BiTTA.

</details>


### [184] [CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs](https://arxiv.org/abs/2505.18527)
*Yiqing Zhang,Xiaozhong Liu,Fabricio Murai*

Main category: cs.LG

TL;DR: CLaDMoP is a new pre-training approach for clinical trial outcome prediction which leverages a Large Language Model-to encode trials' eligibility criteria and introduces a grouping block to reduce computational overhead. It significantly improves PR-AUC and ROC-AUC.


<details>
  <summary>Details</summary>
Motivation: Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data, but this can hinder learning of generalizable representations leading to more false positives/negatives.

Method: CLaDMoP leverages a Large Language Model-to encode trials' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. A grouping block is incorporated to fuse long embeddings across levels reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a 'pair matching' proxy task.

Result: Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. After Parameter-Efficient Fine-Tuning, CLaDMoP achieves up to 10.5% improvement in PR-AUC and 3.6% in ROC-AUC while attaining comparable F1 score to MEXA-CTP.

Conclusion: CLaDMoP shows potential for clinical trial outcome prediction with significant improvements in PR-AUC and ROC-AUC.

Abstract: Many existing models for clinical trial outcome prediction are optimized
using task-specific loss functions on trial phase-specific data. While this
scheme may boost prediction for common diseases and drugs, it can hinder
learning of generalizable representations, leading to more false
positives/negatives. To address this limitation, we introduce CLaDMoP, a new
pre-training approach for clinical trial outcome prediction, alongside the
Successful Clinical Trials dataset(SCT), specifically designed for this task.
CLaDMoP leverages a Large Language Model-to encode trials' eligibility
criteria-linked to a lightweight Drug-Molecule branch through a novel
multi-level fusion technique. To efficiently fuse long embeddings across
levels, we incorporate a grouping block, drastically reducing computational
overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training
on a "pair matching" proxy task. Compared to established zero-shot and few-shot
baselines, our method significantly improves both PR-AUC and ROC-AUC,
especially for phase I and phase II trials. We further evaluate and perform
ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to
state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome
Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC
and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP,
highlighting its potential for clinical trial outcome prediction. Code and SCT
dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.

</details>


### [185] [Preserving AUC Fairness in Learning with Noisy Protected Groups](https://arxiv.org/abs/2505.18532)
*Mingyang Wu,Li Lin,Wenbin Zhang,Xin Wang,Zhenhuan Yang,Shu Hu*

Main category: cs.LG

TL;DR: The paper proposes a robust AUC fairness approach under noisy protected groups with theoretical guarantees, outperforming current methods in preserving AUC fairness.


<details>
  <summary>Details</summary>
Motivation: AUC is an important metric for classification, especially in scenarios like medical image analysis and deepfake detection where class imbalance exists. However, fairness considerations in AUC optimization are still in their early stages, and the impact of noisy protected groups on AUC fairness has been overlooked.

Method: The authors propose a novel method that uses distributionally robust optimization to ensure AUC fairness even when protected groups are noisy. This method provides fairness theoretical guarantees.

Result: Through extensive experiments on both tabular and image datasets, the proposed method demonstrates superior performance in maintaining AUC fairness compared to state-of-the-art approaches.

Conclusion: The proposed approach offers a significant advancement in ensuring AUC fairness under noisy conditions, providing robustness and theoretical guarantees.

Abstract: The Area Under the ROC Curve (AUC) is a key metric for classification,
especially under class imbalance, with growing research focus on optimizing AUC
over accuracy in applications like medical image analysis and deepfake
detection. This leads to fairness in AUC optimization becoming crucial as
biases can impact protected groups. While various fairness mitigation
techniques exist, fairness considerations in AUC optimization remain in their
early stages, with most research focusing on improving AUC fairness under the
assumption of clean protected groups. However, these studies often overlook the
impact of noisy protected groups, leading to fairness violations in practice.
To address this, we propose the first robust AUC fairness approach under noisy
protected groups with fairness theoretical guarantees using distributionally
robust optimization. Extensive experiments on tabular and image datasets show
that our method outperforms state-of-the-art approaches in preserving AUC
fairness. The code is in
https://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups.

</details>


### [186] [Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD](https://arxiv.org/abs/2505.18535)
*Dmitry Dudukalov,Artem Logachov,Vladimir Lotov,Timofei Prasolov,Evgeny Prokopenko,Anton Tarasenko*

Main category: cs.LG

TL;DR: This paper explores the convergence properties and escape dynamics of Stochastic Gradient Descent (SGD) in one-dimensional landscapes with infinite- and finite-variance noise, focusing on time scales for reliable movement to local minima.


<details>
  <summary>Details</summary>
Motivation: To understand how Stochastic Gradient Descent (SGD) converges to local minima and escapes from local maxima in one-dimensional landscapes under different noise conditions.

Method: The study analyzes the convergence properties and escape dynamics of SGD separately considering infinite- and finite-variance noise. It investigates the time scales required for SGD to move from an initial point to a local minimum within the same basin and examines the behavior of SGD near local maxima.

Result: Under suitable noise distribution conditions, SGD reliably converges to the basin's minimum unless the initial point is too close to a local maximum. Near a sharp maximum, SGD does not remain stuck and the probability of reaching neighboring minima can be estimated.

Conclusion: The results provide a detailed understanding of SGD's transitions between local maxima and minima, highlighting the influence of noise characteristics and function geometry.

Abstract: We study the convergence properties and escape dynamics of Stochastic
Gradient Descent (SGD) in one-dimensional landscapes, separately considering
infinite- and finite-variance noise. Our main focus is to identify the time
scales on which SGD reliably moves from an initial point to the local minimum
in the same ''basin''. Under suitable conditions on the noise distribution, we
prove that SGD converges to the basin's minimum unless the initial point lies
too close to a local maximum. In that near-maximum scenario, we show that SGD
can linger for a long time in its neighborhood. For initial points near a
''sharp'' maximum, we show that SGD does not remain stuck there, and we provide
results to estimate the probability that it will reach each of the two
neighboring minima. Overall, our findings present a nuanced view of SGD's
transitions between local maxima and minima, influenced by both noise
characteristics and the underlying function geometry.

</details>


### [187] [B-score: Detecting biases in large language models using response history](https://arxiv.org/abs/2505.18545)
*An Vo,Mohammad Reza Taesiri,Daeyoung Kim,Anh Totti Nguyen*

Main category: cs.LG

TL;DR: Large language models can reduce bias in multi-turn conversations and B-score improves answer verification accuracy.


<details>
  <summary>Details</summary>
Motivation: To investigate if large language models can produce less biased answers in multi-turn conversations and to understand which types of questions lead to more biased responses.

Method: Testing LLMs with a set of questions spanning 9 topics and belonging to three types: Subjective, Random, and Objective. Proposing the B-score metric for detecting biases.

Result: LLMs can 'de-bias' themselves in response to Random questions in multi-turn conversations. B-score effectively improves the verification accuracy of LLM answers on MMLU, HLE, and CSQA.

Conclusion: Code and data for B-score are available, indicating a promising approach to detect and mitigate bias in LLMs.

Abstract: Large language models (LLMs) often exhibit strong biases, e.g, against women
or in favor of the number 7. We investigate whether LLMs would be able to
output less biased answers when allowed to observe their prior answers to the
same question in a multi-turn conversation. To understand which types of
questions invite more biased answers, we test LLMs on our proposed set of
questions that span 9 topics and belong to three types: (1) Subjective; (2)
Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves
in a multi-turn conversation in response to questions that seek an Random,
unbiased answer. Furthermore, we propose B-score, a novel metric that is
effective in detecting biases to Subjective, Random, Easy, and Hard questions.
On MMLU, HLE, and CSQA, leveraging B-score substantially improves the
verification accuracy of LLM answers (i.e, accepting LLM correct answers and
rejecting incorrect ones) compared to using verbalized confidence scores or the
frequency of single-turn answers alone. Code and data are available at:
https://b-score.github.io.

</details>


### [188] [Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning](https://arxiv.org/abs/2505.18558)
*Wenbo He,Zhijian Ou*

Main category: cs.LG

TL;DR: An analysis of deep generative models (DGMs) like VAEs and GANs shows two main problems: poor handling of discrete observations/latent codes and indirect optimization criteria related to data likelihood. To solve these issues, Joint-stochastic-approximation (JSA) autoencoders are proposed. These algorithms build deep directed generative models for semi-supervised learning by directly maximizing data log-likelihood and minimizing inclusive KL divergence simultaneously. Theoretical results and experiments demonstrate its superiority, including robustness to structure mismatch between encoder and decoder, consistent handling of discrete and continuous variables, and comparable performance to state-of-the-art DGMs with continuous latent space in semi-supervised tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is the unsatisfactory capability of existing DGMs in handling discrete observations and latent codes, as well as their indirect optimization criteria related to data likelihood.

Method: Joint-stochastic-approximation (JSA) autoencoders are introduced. This method directly maximizes the data log-likelihood and simultaneously minimizes the inclusive KL divergence between the posteriori and the inference model.

Result: JSA autoencoders are robust to structure mismatch between encoder and decoder, can consistently handle both discrete and continuous variables, and achieve comparable performance to other state-of-the-art DGMs with continuous latent space in semi-supervised tasks over datasets like MNIST and SVHN.

Conclusion: This work successfully applies discrete latent variable models in challenging semi-supervised tasks, marking a significant advancement.

Abstract: Our examination of existing deep generative models (DGMs), including VAEs and
GANs, reveals two problems. First, their capability in handling discrete
observations and latent codes is unsatisfactory, though there are interesting
efforts. Second, both VAEs and GANs optimize some criteria that are indirectly
related to the data likelihood. To address these problems, we formally present
Joint-stochastic-approximation (JSA) autoencoders - a new family of algorithms
for building deep directed generative models, with application to
semi-supervised learning. The JSA learning algorithm directly maximizes the
data log-likelihood and simultaneously minimizes the inclusive KL divergence
the between the posteriori and the inference model. We provide theoretical
results and conduct a series of experiments to show its superiority such as
being robust to structure mismatch between encoder and decoder, consistent
handling of both discrete and continuous variables. Particularly we empirically
show that JSA autoencoders with discrete latent space achieve comparable
performance to other state-of-the-art DGMs with continuous latent space in
semi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the
best of our knowledge, this is the first demonstration that discrete latent
variable models are successfully applied in the challenging semi-supervised
tasks.

</details>


### [189] [Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models](https://arxiv.org/abs/2505.19969)
*Antti Koskela,Tejas Kulkarni*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fully decentralized training of machine learning models offers significant
advantages in scalability, robustness, and fault tolerance. However, achieving
differential privacy (DP) in such settings is challenging due to the absence of
a central aggregator and varying trust assumptions among nodes. In this work,
we present a novel privacy analysis of decentralized gossip-based averaging
algorithms with additive node-level noise, both with and without secure
summation over each node's direct neighbors. Our main contribution is a new
analytical framework based on a linear systems formulation that accurately
characterizes privacy leakage across these scenarios. This framework
significantly improves upon prior analyses, for example, reducing the R\'enyi
DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of
training rounds. We validate our analysis with numerical results demonstrating
superior DP bounds compared to existing approaches. We further illustrate our
analysis with a logistic regression experiment on MNIST image classification in
a fully decentralized setting, demonstrating utility comparable to central
aggregation methods.

</details>


### [190] [Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods](https://arxiv.org/abs/2505.18565)
*Afrah Farea,Saiful Khan,Reza Daryani,Emre Cenk Ersan,Mustafa Serdar Celebi*

Main category: cs.LG

TL;DR: The paper presents two neural network architectures combining PINNs and IBM for solving FSI problems, with the Eulerian-Lagrangian architecture performing better. Adaptive B-spline activation improves accuracy. Pressure recovery remains challenging without explicit force-coupling constraints.


<details>
  <summary>Details</summary>
Motivation: To develop effective neural network architectures for solving fluid-structure interaction (FSI) problems by integrating physics-informed neural networks (PINNs) with the immersed boundary method (IBM).

Method: Two distinct architectures are introduced: a Single-FSI network with unified parameter space and an Eulerian-Lagrangian network with separate parameter spaces for fluid and structure domains. Tanh and adaptive B-spline activation functions are used in empirical studies.

Result: The Eulerian-Lagrangian architecture outperforms the Single-FSI network, especially when using adaptive B-spline activation which enhances accuracy near boundaries. Promising results in predicting velocity field but challenges remain in pressure recovery.

Conclusion: Domain-specific architectural design and adaptive activation functions are crucial for modeling FSI problems within the PINN framework.

Abstract: We introduce neural network architectures that combine physics-informed
neural networks (PINNs) with the immersed boundary method (IBM) to solve
fluid-structure interaction (FSI) problems. Our approach features two distinct
architectures: a Single-FSI network with a unified parameter space, and an
innovative Eulerian-Lagrangian network that maintains separate parameter spaces
for fluid and structure domains. We study each architecture using standard Tanh
and adaptive B-spline activation functions. Empirical studies on a 2D cavity
flow problem involving a moving solid structure show that the
Eulerian-Lagrangian architecture performs significantly better. The adaptive
B-spline activation further enhances accuracy by providing locality-aware
representation near boundaries. While our methodology shows promising results
in predicting the velocity field, pressure recovery remains challenging due to
the absence of explicit force-coupling constraints in the current formulation.
Our findings underscore the importance of domain-specific architectural design
and adaptive activation functions for modeling FSI problems within the PINN
framework.

</details>


### [191] [Transformer in Protein: A Survey](https://arxiv.org/abs/2505.20098)
*Xiaowen Ling,Zhiqiang Li,Yanbin Wang,Zhuhong You*

Main category: cs.LG

TL;DR: This paper reviews the application of Transformer models in protein informatics, covering structure prediction, function prediction, interaction analysis, annotation, and drug discovery. It provides a domain-oriented classification system, evaluates previous methods, summarizes datasets and resources, and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement in protein informatics has increased the demand for better predictive accuracy, structural analysis, and functional understanding. Transformer models have shown great potential in addressing various challenges in protein research, but there is a lack of comprehensive review on their applications in this field.

Method: The authors conducted an in-depth survey of over 100 studies, systematically covering critical domains such as protein structure prediction, function prediction, protein-protein interaction analysis, functional annotation, and drug discovery/target identification. They introduced foundational concepts of Transformer architecture and attention mechanisms, categorized Transformer variants for protein science, and summarized essential protein knowledge. For each domain, they outlined objectives, evaluated prior methods, and highlighted contributions of Transformer models.

Result: The review offers a detailed insight into the advancements brought by Transformer models across various protein-related tasks. It also provides pivotal datasets and open-source code resources to support reproducibility and benchmarking.

Conclusion: This review aims to consolidate the foundation for integrating Transformer models with protein informatics, promoting further innovation and expanded applications in the field.

Abstract: As protein informatics advances rapidly, the demand for enhanced predictive
accuracy, structural analysis, and functional understanding has intensified.
Transformer models, as powerful deep learning architectures, have demonstrated
unprecedented potential in addressing diverse challenges across protein
research. However, a comprehensive review of Transformer applications in this
field remains lacking. This paper bridges this gap by surveying over 100
studies, offering an in-depth analysis of practical implementations and
research progress of Transformers in protein-related tasks. Our review
systematically covers critical domains, including protein structure prediction,
function prediction, protein-protein interaction analysis, functional
annotation, and drug discovery/target identification. To contextualize these
advancements across various protein domains, we adopt a domain-oriented
classification system. We first introduce foundational concepts: the
Transformer architecture and attention mechanisms, categorize Transformer
variants tailored for protein science, and summarize essential protein
knowledge. For each research domain, we outline its objectives and background,
critically evaluate prior methods and their limitations, and highlight
transformative contributions enabled by Transformer models. We also curate and
summarize pivotal datasets and open-source code resources to facilitate
reproducibility and benchmarking. Finally, we discuss persistent challenges in
applying Transformers to protein informatics and propose future research
directions. This review aims to provide a consolidated foundation for the
synergistic integration of Transformer and protein informatics, fostering
further innovation and expanded applications in the field.

</details>


### [192] [Learning without Isolation: Pathway Protection for Continual Learning](https://arxiv.org/abs/2505.18568)
*Zhikang Chen,Abudukelimu Wuerkaixi,Sen Cui,Haoxuan Li,Ding Li,Jingfeng Zhang,Bo Han,Gang Niu,Houfang Liu,Yi Yang,Sifan Yang,Changshui Zhang,Tianling Ren*

Main category: cs.LG

TL;DR: 提出了一种新的持续学习框架LwI，通过保护旧任务的知识路径而非参数，解决了灾难性遗忘问题，并在参数高效的情况下分配新任务的可用路径。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法主要关注调节或保护与先前任务相关的参数，但随着任务数量增加，存储旧任务知识的参数量线性增长，难以实际应用。

Method: 受神经科学和物理学启发，提出学习不隔离（LwI）框架，将模型融合视为图匹配，保护旧任务占用的路径而不进行隔离，同时利用深度网络中激活通道的稀疏性，自适应地为新任务分配可用路径。

Result: 在流行基准数据集上的实验表明，所提出的LwI框架表现出优越性。

Conclusion: LwI框架通过路径保护解决了灾难性遗忘问题，并以参数高效的方式实现了任务学习。

Abstract: Deep networks are prone to catastrophic forgetting during sequential task
learning, i.e., losing the knowledge about old tasks upon learning new tasks.
To this end, continual learning(CL) has emerged, whose existing methods focus
mostly on regulating or protecting the parameters associated with the previous
tasks. However, parameter protection is often impractical, since the size of
parameters for storing the old-task knowledge increases linearly with the
number of tasks, otherwise it is hard to preserve the parameters related to the
old-task knowledge. In this work, we bring a dual opinion from neuroscience and
physics to CL: in the whole networks, the pathways matter more than the
parameters when concerning the knowledge acquired from the old tasks. Following
this opinion, we propose a novel CL framework, learning without isolation(LwI),
where model fusion is formulated as graph matching and the pathways occupied by
the old tasks are protected without being isolated. Thanks to the sparsity of
activation channels in a deep network, LwI can adaptively allocate available
pathways for a new task, realizing pathway protection and addressing
catastrophic forgetting in a parameter-efficient manner. Experiments on popular
benchmark datasets demonstrate the superiority of the proposed LwI.

</details>


### [193] [VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis](https://arxiv.org/abs/2505.18570)
*Tina Khezresmaeilzadeh,Parsa Razmara,Seyedarmin Azizi,Mohammad Erfan Sadeghi,Erfan Baghaei Portaghloo*

Main category: cs.LG

TL;DR: This paper presents VISTA, a training-free framework leveraging Vision-Language Models for multi-modal stock forecasting by combining textual and visual data. It outperforms traditional methods up to 89.83% in experiments.


<details>
  <summary>Details</summary>
Motivation: Stock price prediction is a complex task traditionally approached with statistical models or language models. The authors aim to explore the potential of Vision-Language Models in financial forecasting without requiring task-specific training.

Method: VISTA uses Vision-Language Models prompted with textual representations of historical stock prices and their corresponding line charts for predicting future prices. It operates in a zero-shot setting with chain-of-thought prompts to capture patterns missed by unimodal approaches.

Result: VISTA significantly outperforms standard baselines such as ARIMA and text-only LLM-based methods by up to 89.83% in experimental benchmarks.

Conclusion: The study concludes that multi-modal inference using Vision-Language Models is highly effective for stock time-series analysis, showcasing great potential in financial forecasting tasks without the need for specific training.

Abstract: Stock price prediction remains a complex and high-stakes task in financial
analysis, traditionally addressed using statistical models or, more recently,
language models. In this work, we introduce VISTA (Vision-Language Inference
for Stock Time-series Analysis), a novel, training-free framework that
leverages Vision-Language Models (VLMs) for multi-modal stock forecasting.
VISTA prompts a VLM with both textual representations of historical stock
prices and their corresponding line charts to predict future price values. By
combining numerical and visual modalities in a zero-shot setting and using
carefully designed chain-of-thought prompts, VISTA captures complementary
patterns that unimodal approaches often miss. We benchmark VISTA against
standard baselines, including ARIMA and text-only LLM-based prompting methods.
Experimental results show that VISTA outperforms these baselines by up to
89.83%, demonstrating the effectiveness of multi-modal inference for stock
time-series analysis and highlighting the potential of VLMs in financial
forecasting tasks without requiring task-specific training.

</details>


### [194] [Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs](https://arxiv.org/abs/2505.18573)
*Mengqi Liao,Xiangyu Xi,Ruinian Chen,Jia Leng,Yangen Hu,Ke Zeng,Shuai Liu,Huaiyu Wan*

Main category: cs.LG

TL;DR: 在强化学习（RL）训练大语言模型（LLMs）过程中，动态分配rollout预算和调整温度参数可以提高训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在训练LLMs时对所有问题平均分配rollouts，导致简单问题上收益有限而复杂问题需要更多尝试，同时RL可能限制模型的探索能力，影响最终表现。

Method: 提出了一种根据问题难度动态分配rollout预算的机制，并引入自适应动态温度调整策略以保持熵稳定，从而促进充分探索。

Result: 该方法使LLMs在提高回答准确性的同时保留了探索能力，避免了性能上限低于基础模型的问题。

Conclusion: 通过动态分配rollout预算和调整温度参数，可以更高效地训练LLMs并提升其性能。

Abstract: Reasoning large language models (LLMs) excel in complex tasks, which has
drawn significant attention to reinforcement learning (RL) for LLMs. However,
existing approaches allocate an equal number of rollouts to all questions
during the RL process, which is inefficient. This inefficiency stems from the
fact that training on simple questions yields limited gains, whereas more
rollouts are needed for challenging questions to sample correct answers.
Furthermore, while RL improves response precision, it limits the model's
exploration ability, potentially resulting in a performance cap below that of
the base model prior to RL. To address these issues, we propose a mechanism for
dynamically allocating rollout budgets based on the difficulty of the problems,
enabling more efficient RL training. Additionally, we introduce an adaptive
dynamic temperature adjustment strategy to maintain the entropy at a stable
level, thereby encouraging sufficient exploration. This enables LLMs to improve
response precision while preserving their exploratory ability to uncover
potential correct pathways. The code and data is available on:
https://github.com/LiaoMengqi/E3-RL4LLMs

</details>


### [195] [Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power](https://arxiv.org/abs/2505.18579)
*Tingpeng Zhang,Xuzhang Peng,Mingyuan Zhou,Guobiao Hu,Zhilu Lai*

Main category: cs.LG

TL;DR: This paper introduces a programmable metamaterial-based sensor (MM-sensor) that can physically process structural vibration information for specific structural health monitoring tasks, such as damage warning, without the need for further information processing or external resources.


<details>
  <summary>Details</summary>
Motivation: To address challenges in current SHM practices such as high energy consumption and low throughput due to digital units, and to explore the potential of metamaterials in creating a pure-physical entity for seamlessly integrating sensing and computing.

Method: The MM-sensor uses a locally resonant metamaterial plate (LRMP) configuration. It leverages the bandgap properties of LRMP to differentiate structural dynamic behavior before and after damage. The geometric parameters are inversely designed to adjust bandgap features.

Result: Effective for engineering systems with a first natural frequency ranging from 9.54 Hz to 81.86 Hz. The MM-sensor can perform binary classification (structural damage warning) in-situ at the sensor level without further information processing or resource consumption.

Conclusion: The proposed MM-sensor provides a novel approach for SHM by physically processing structural vibration information and performing specific tasks like damage warning without external electronic power supplies.

Abstract: Structural health monitoring (SHM) involves sensor deployment, data
acquisition, and data interpretation, commonly implemented via a tedious wired
system. The information processing in current practice majorly depends on
electronic computers, albeit with universal applications, delivering challenges
such as high energy consumption and low throughput due to the nature of digital
units. In recent years, there has been a renaissance interest in shifting
computations from electronic computing units to the use of real physical
systems, a concept known as physical computation. This approach provides the
possibility of thinking out of the box for SHM, seamlessly integrating sensing
and computing into a pure-physical entity, without relying on external
electronic power supplies, thereby properly coping with resource-restricted
scenarios. The latest advances of metamaterials (MM) hold great promise for
this proactive idea. In this paper, we introduce a programmable
metamaterial-based sensor (termed as MM-sensor) for physically processing
structural vibration information to perform specific SHM tasks, such as
structural damage warning (binary classification) in this initiation, without
the need for further information processing or resource-consuming, that is, the
data collection and analysis are completed in-situ at the sensor level. We
adopt the configuration of a locally resonant metamaterial plate (LRMP) to
achieve the first fabrication of the MM-sensor. We take advantage of the
bandgap properties of LRMP to physically differentiate the dynamic behavior of
structures before and after damage. By inversely designing the geometric
parameters, our current approach allows for adjustments to the bandgap
features. This is effective for engineering systems with a first natural
frequency ranging from 9.54 Hz to 81.86 Hz.

</details>


### [196] [Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks](https://arxiv.org/abs/2505.18591)
*Joery A. de Vries,Jinke He,Mathijs M. de Weerdt,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: The paper proposes a method to augment point-estimate distributions in meta-reinforcement learning using the Laplace approximation, allowing for estimation of distribution statistics without modifying model architecture. This approach addresses overconfidence in estimators and performs similarly to variational baselines with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Meta-reinforcement learning aims to train agents that can quickly generalize to new tasks. Current methods often use point-estimate distributions via recurrent neural networks, but these may be overconfident and lack consistency.

Method: The authors apply the Laplace approximation to point-estimate distributions at different stages of learning (start, during, or after) without altering the base model architecture. This allows for the estimation of full distributions.

Result: The proposed method can estimate distribution statistics such as entropy for non-Bayesian agents. It reduces overconfidence and inconsistency issues. Performance is comparable to variational baselines while using significantly fewer parameters.

Conclusion: Augmenting point-estimate distributions with the Laplace approximation in meta-reinforcement learning provides a way to estimate full distributions effectively without increasing model complexity.

Abstract: Meta-reinforcement learning trains a single reinforcement learning agent on a
distribution of tasks to quickly generalize to new tasks outside of the
training set at test time. From a Bayesian perspective, one can interpret this
as performing amortized variational inference on the posterior distribution
over training tasks. Among the various meta-reinforcement learning approaches,
a common method is to represent this distribution with a point-estimate using a
recurrent neural network. We show how one can augment this point estimate to
give full distributions through the Laplace approximation, either at the start
of, during, or after learning, without modifying the base model architecture.
With our approximation, we are able to estimate distribution statistics (e.g.,
the entropy) of non-Bayesian agents and observe that point-estimate based
methods produce overconfident estimators while not satisfying consistency.
Furthermore, when comparing our approach to full-distribution based learning of
the task posterior, our method performs on par with variational baselines while
having much fewer parameters.

</details>


### [197] [MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations](https://arxiv.org/abs/2505.18595)
*The Viet Bui,Tien Mai,Hong Thanh Nguyen*

Main category: cs.LG

TL;DR: This paper studies offline imitation learning in cooperative multi-agent settings with unlabeled mixed quality demonstrations, proposing a two-stage solution: trajectory labeling and multi-agent imitation learning.


<details>
  <summary>Details</summary>
Motivation: Current methods for offline imitation learning in multi-agent settings may not effectively handle unlabeled mixed quality demonstrations, which contain both expert and suboptimal trajectories. There is a need for an approach that can distinguish expert-quality trajectories and learn robust policies from such data.

Method: The proposed solution has two stages: 1) Trajectory labeling - combining large language models and preference-based reinforcement learning to create a pipeline that distinguishes expert-quality trajectories; 2) Multi-agent imitation learning - introducing MisoDICE, a novel algorithm that leverages these labels to learn robust policies while addressing computational complexity. MisoDICE extends the single-agent DICE framework to multi-agent settings with value decomposition and mixing architecture.

Result: MisoDICE was evaluated on multiple standard multi-agent RL benchmarks and showed superior performance compared to baselines, particularly excelling when expert data is scarce.

Conclusion: The two-stage approach of trajectory labeling followed by multi-agent imitation learning using MisoDICE successfully enables effective learning from heterogeneous, unlabeled data in cooperative multi-agent settings.

Abstract: We study offline imitation learning (IL) in cooperative multi-agent settings,
where demonstrations have unlabeled mixed quality - containing both expert and
suboptimal trajectories. Our proposed solution is structured in two stages:
trajectory labeling and multi-agent imitation learning, designed jointly to
enable effective learning from heterogeneous, unlabeled data. In the first
stage, we combine advances in large language models and preference-based
reinforcement learning to construct a progressive labeling pipeline that
distinguishes expert-quality trajectories. In the second stage, we introduce
MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn
robust policies while addressing the computational complexity of large joint
state-action spaces. By extending the popular single-agent DICE framework to
multi-agent settings with a new value decomposition and mixing architecture,
our method yields a convex policy optimization objective and ensures
consistency between global and local policies. We evaluate MisoDICE on multiple
standard multi-agent RL benchmarks and demonstrate superior performance,
especially when expert data is scarce.

</details>


### [198] [Exemplar-Free Continual Learning for State Space Models](https://arxiv.org/abs/2505.18604)
*Isaac Ning Lee,Leila Mahmoodi,Trung Le,Mehrtash Harandi*

Main category: cs.LG

TL;DR: The paper proposes Inf-SSM, a geometry-aware regularization method for continual learning in State-Space Models (SSMs), which effectively reduces catastrophic forgetting and enhances accuracy across sequential tasks.


<details>
  <summary>Details</summary>
Motivation: State-Space Models (SSMs) are effective for sequence modeling due to their ability to capture long-range dependencies. However, adapting SSMs under Continual Learning (CL) is challenging, especially in exemplar-free settings where catastrophic forgetting occurs due to unconstrained state evolution.

Method: Inf-SSM is introduced as a novel regularization method that leverages the geometry of the infinite-dimensional Grassmannian to constrain state evolution during CL. Instead of constraining weight updates, it regularizes the infinite-horizon evolution of SSMs within their extended observability subspace. The method involves solving the Sylvester equation efficiently with a complexity reduced from O(n^3) to O(n^2).

Result: Extensive experiments on benchmarks such as ImageNet-R and Caltech-256 indicate that Inf-SSM significantly mitigates catastrophic forgetting and improves accuracy in sequential tasks compared to classical CL methods.

Conclusion: Inf-SSM offers an efficient and effective approach to continual learning in SSMs by utilizing geometric properties to regularize state evolution, leading to better performance in sequential tasks without catastrophic forgetting.

Abstract: State-Space Models (SSMs) excel at capturing long-range dependencies with
structured recurrence, making them well-suited for sequence modeling. However,
their evolving internal states pose challenges in adapting them under Continual
Learning (CL). This is particularly difficult in exemplar-free settings, where
the absence of prior data leaves updates to the dynamic SSM states
unconstrained, resulting in catastrophic forgetting. To address this, we
propose Inf-SSM, a novel and simple geometry-aware regularization method that
utilizes the geometry of the infinite-dimensional Grassmannian to constrain
state evolution during CL. Unlike classical continual learning methods that
constrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of
SSMs encoded in their extended observability subspace. We show that enforcing
this regularization requires solving a matrix equation known as the Sylvester
equation, which typically incurs $\mathcal{O}(n^3)$ complexity. We develop a
$\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs.
This leads to an efficient regularization mechanism that can be seamlessly
integrated into existing CL methods. Comprehensive experiments on challenging
benchmarks, including ImageNet-R and Caltech-256, demonstrate a significant
reduction in forgetting while improving accuracy across sequential tasks.

</details>


### [199] [Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation](https://arxiv.org/abs/2505.18622)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar,Pegah Ghaffari*

Main category: cs.LG

TL;DR: In recent machine learning systems, confidence scores are increasingly used for selective prediction. Conventional metrics fall short in evaluating the reliability of these predictions. This paper introduces Confidence-Weighted Selective Accuracy (CWSA) and its normalized variant CWSA+, which reward confident accuracy and penalize overconfident mistakes. Through experiments on real-world and artificial datasets, CWSA and CWSA+ outperform classical metrics in trust-sensitive tests.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the limitation of conventional metrics such as accuracy, expected calibration error (ECE), and area under the risk-coverage curve (AURC) in capturing the actual reliability of predictions in machine learning models. These metrics either ignore confidence, dilute localized information, or inadequately penalize overconfident misclassifications.

Method: The method involves introducing two new metrics: Confidence-Weighted Selective Accuracy (CWSA) and its normalized variant CWSA+. These metrics provide a principled and interpretable way to evaluate predictive models under confidence thresholds by rewarding confident accuracy and penalizing overconfident mistakes. They are threshold-local, decomposable, and applicable in both evaluation and deployment settings.

Result: Through extensive experiments on real-world datasets (MNIST, CIFAR-10) and artificial model variants, the results show that CWSA and CWSA+ effectively detect nuanced failure modes and outperform classical metrics in trust-sensitive tests.

Conclusion: The conclusion is that CWSA serves as a robust foundation for developing and assessing selective prediction systems, especially for safety-critical domains, due to its ability to reward confident accuracy and penalize overconfident mistakes.

Abstract: In recent machine learning systems, confidence scores are being utilized more
and more to manage selective prediction, whereby a model can abstain from
making a prediction when it is unconfident. Yet, conventional metrics like
accuracy, expected calibration error (ECE), and area under the risk-coverage
curve (AURC) do not capture the actual reliability of predictions. These
metrics either disregard confidence entirely, dilute valuable localized
information through averaging, or neglect to suitably penalize overconfident
misclassifications, which can be particularly detrimental in real-world
systems. We introduce two new metrics Confidence-Weighted Selective Accuracy
(CWSA) and its normalized variant CWSA+ that offer a principled and
interpretable way to evaluate predictive models under confidence thresholds.
Unlike existing methods, our metrics explicitly reward confident accuracy and
penalize overconfident mistakes. They are threshold-local, decomposable, and
usable in both evaluation and deployment settings where trust and risk must be
quantified. Through exhaustive experiments on both real-world data sets (MNIST,
CIFAR-10) and artificial model variants (calibrated, overconfident,
underconfident, random, perfect), we show that CWSA and CWSA+ both effectively
detect nuanced failure modes and outperform classical metrics in
trust-sensitive tests. Our results confirm that CWSA is a sound basis for
developing and assessing selective prediction systems for safety-critical
domains.

</details>


### [200] [Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding](https://arxiv.org/abs/2505.18629)
*Yixuan Wang,Yijun Liu,Shiyu ji,Yuzhuang Xu,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）因自回归解码过程而面临高推理延迟的问题。投机解码通过生成多个草稿令牌并行验证来加速推理。然而，现有的验证方法过于依赖分布一致性而忽视语义正确性，限制了投机解码的潜在加速。本文提出了一种无训练、具有语义意识的方法——反思验证，利用LLMs的内在反思能力，在验证过程中并行评估草稿令牌的语义正确性。通过基于提示的探测，可以在一次前向传递中获得草稿令牌的原始和反思分布。这些分布的融合使得草稿令牌的语义级验证能够同时包含一致性和正确性。多个领域基准测试和模型规模的实验表明，该方法显著增加了草稿令牌的接受长度，而不影响模型性能。此外，发现所提出的反思验证与现有的统计验证方法是正交的，它们的结合带来了额外5~15%的解码速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的投机解码验证方法过于依赖分布一致性，忽视了语义正确性，这限制了其加速潜力。并且，一些使用额外模型进行宽松验证的方法在更多样化或开放领域的设置中往往无法有效推广。

Method: 提出了一种名为反思验证的无训练且具有语义意识的方法。该方法利用LLMs的内在反思能力，在验证过程中并行语义评估草稿令牌的正确性。具体来说，通过提示词探测，在单次前向传递中获取草稿令牌的原始和反思分布，并将这些分布融合以实现语义级别的验证。

Result: 实验结果表明，反思验证方法显著增加了草稿令牌的接受长度，同时保持了模型性能。此外，反思验证与现有统计验证方法正交，两者的结合进一步提升了5~15%的解码速度。

Conclusion: 反思验证是一种有效的无训练语义验证方法，能够显著提高投机解码的效率，同时不损害模型性能。它与现有方法结合可进一步提升解码速度，适用于多样化和开放领域的设置。

Abstract: Large language models (LLMs) suffer from high inference latency due to the
auto-regressive decoding process. Speculative decoding accelerates inference by
generating multiple draft tokens using a lightweight model and verifying them
in parallel. However, existing verification methods rely heavily on
distributional consistency while overlooking semantic correctness, thereby
limiting the potential speedup of speculative decoding. While some methods
employ additional models for relaxed verification of draft tokens, they often
fail to generalize effectively to more diverse or open-domain settings. In this
work, we propose Reflective Verification, a training-free and semantics-aware
approach that achieves a better trade-off between correctness and efficiency.
Specifically, we leverage the inherent reflective capacity of LLMs to
semantically assess the correctness of draft tokens in parallel during
verification. Using prompt-based probing, we obtain both the original and
reflective distributions of draft tokens in a single forward pass. The fusion
of these distributions enables semantic-level verification of draft tokens that
incorporates both consistency and correctness. Experiments across multiple
domain benchmarks and model scales demonstrate that our method significantly
increases the acceptance length of draft tokens without compromising model
performance. Furthermore, we find that the proposed Reflective Verification is
orthogonal to existing statistical verification methods, and their combination
yields additional 5$\sim$15\% improvements in decoding speed.

</details>


### [201] [Asymmetric Duos: Sidekicks Improve Uncertainty](https://arxiv.org/abs/2505.18636)
*Tim G. Zhou,Evan Shelhamer,Geoff Pleiss*

Main category: cs.LG

TL;DR: 通过将大型模型与小型模型（sidekick）结合，提出了一种新的成本效益策略来改进不确定性量化和下游决策。这种方法在多个图像分类基准测试中显著提高了准确性、不确定性和选择性分类指标，且仅增加约10-20%的计算量。


<details>
  <summary>Details</summary>
Motivation: 在需要利用不确定性进行决策的情况下，传统的多训练运行集成策略对于当今的大规模模型和实际微调工作流程来说效率低下。因此，需要一种更有效的方法来改进不确定性量化和下游决策。

Method: 引入了一个较小但不那么精确的“配角”模型（sidekick），与较大的模型结合形成不对称二人组（Asymmetric Duo）。通过简单的学习加权平均方法聚合这两个模型的预测结果。

Result: 在五个图像分类基准测试和多种模型架构及训练方案中，不对称二人组显著提高了准确性、不确定性量化和选择性分类指标，同时仅增加了约10-20%的计算成本。并且，配角模型几乎不会损害较大模型的性能。

Conclusion: 提出的不对称二人组策略是一种有效的解决方案，能够在合理增加计算成本的前提下，显著提高模型的性能和不确定性量化能力。

Abstract: The go-to strategy to apply deep networks in settings where uncertainty
informs decisions--ensembling multiple training runs with random
initializations--is ill-suited for the extremely large-scale models and
practical fine-tuning workflows of today. We introduce a new cost-effective
strategy for improving the uncertainty quantification and downstream decisions
of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate
but much smaller "sidekick" (e.g. a fine-tuned ResNet-34) with a fraction of
the computational cost. We propose aggregating the predictions of this
\emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly,
despite their inherent asymmetry, the sidekick model almost never harms the
performance of the larger model. In fact, across five image classification
benchmarks and a variety of model architectures and training schemes (including
soups), Asymmetric Duos significantly improve accuracy, uncertainty
quantification, and selective classification metrics with only ${\sim}10-20\%$
more computation.

</details>


### [202] [ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation](https://arxiv.org/abs/2505.18640)
*Jian Liang,Wenke Huang,Xianda Guo,Guancheng Wan,Bo Du,Mang Ye*

Main category: cs.LG

TL;DR: 提出ThanoRA框架，解决多任务适配中的子空间干扰和任务干扰问题，保持推理效率且性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用需要基础模型同时专精于多个任务，而现有结合LoRA与MoE的方法由于路由器的使用限制了参数合并性，增加了推理开销并阻碍统一的多任务适配。

Method: ThanoRA在初始化时构建任务特定的LoRA子空间以实现与任务异质性对齐的精细知识注入，并引入子空间保持正则化以防止多任务训练期间的任务干扰和子空间坍缩。

Result: 广泛的实验表明，ThanoRA在不增加额外推理开销的情况下，持续展现出比强大基线更稳健和优越的性能。

Conclusion: ThanoRA实现了高效和统一的多任务适配，其代码已公开。

Abstract: Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of
foundation models due to its efficiency and zero additional inference cost.
Many real-world applications require foundation models to specialize in
multiple tasks simultaneously, motivating the need for efficient multi-task
adaptation. While recent approaches integrate LoRA with mixture-of-experts
(MoE) to address this, the use of routers prevents parameter mergeability,
which increases inference overhead and hinders unified multi-task adaptation,
thereby limiting deployment practicality. In this work, we propose ThanoRA, a
Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables
multi-task adaptation while preserving the inference efficiency of LoRA.
ThanoRA jointly models task heterogeneity and mitigates subspace interference
throughout training. Specifically, motivated by inherent differences in
complexity and heterogeneity across tasks, ThanoRA constructs task-specific
LoRA subspaces at initialization, enabling fine-grained knowledge injection
aligned with task heterogeneity. Furthermore, to prevent task interference and
subspace collapse during multi-task training, ThanoRA introduces a
subspace-preserving regularization that maintains the independence of
task-specific representations. With the synergy of both components, ThanoRA
enables efficient and unified multi-task adaptation. Extensive experiments
across multimodal and text-only benchmarks under varying multi-task mixtures
demonstrate that ThanoRA consistently achieves robust and superior performance
over strong baselines without introducing additional inference overhead. Our
code is publicly available at: https://github.com/LiangJian24/ThanoRA.

</details>


### [203] [Flow Matching for Geometric Trajectory Simulation](https://arxiv.org/abs/2505.18647)
*Kiet Bennema ten Brinke,Koen Minartz,Vlado Menkovski*

Main category: cs.LG

TL;DR: 提出了一种名为STFlow的新方法，通过利用流匹配和数据依赖耦合，实现了在不牺牲模型表达能力或可扩展性的情况下对几何轨迹的物理信息模拟。在多个基准测试中，STFlow显著降低了预测误差并提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度生成建模和几何深度学习的方法虽然能够进行概率模拟，但需要从无信息的噪声开始学习复杂的转换，并且无法利用领域知情的先验知识。

Method: 提出了STFlow方法，通过利用流匹配和数据依赖耦合来解决现有方法的局限性，从而实现物理信息的几何轨迹模拟。

Result: 在N体动力系统、分子动力学以及行人动力学基准上的评估表明，STFlow产生的预测误差显著较低，同时允许更高效的推理。

Conclusion: 展示了使用物理信息先验分布在概率几何轨迹建模中的优势。

Abstract: The simulation of N-body systems is a fundamental problem with applications
in a wide range of fields, such as molecular dynamics, biochemistry, and
pedestrian dynamics. Machine learning has become an invaluable tool for scaling
physics-based simulators and developing models directly from experimental data.
In particular, recent advances based on deep generative modeling and geometric
deep learning have enabled probabilistic simulation by modeling complex
distributions over trajectories while respecting the permutation symmetry that
is fundamental to N-body systems. However, to generate realistic trajectories,
existing methods must learn complex transformations starting from uninformed
noise and do not allow for the exploitation of domain-informed priors. In this
work, we propose STFlow to address this limitation. By leveraging flow matching
and data-dependent couplings, STFlow facilitates physics-informed simulation of
geometric trajectories without sacrificing model expressivity or scalability.
Our evaluation on N-body dynamical systems, molecular dynamics, and pedestrian
dynamics benchmarks shows that STFlow produces significantly lower prediction
errors while enabling more efficient inference, highlighting the benefits of
employing physics-informed prior distributions in probabilistic geometric
trajectory modeling.

</details>


### [204] [LLM-QFL: Distilling Large Language Model for Quantum Federated Learning](https://arxiv.org/abs/2505.18656)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: The paper integrates large language models (LLMs) with quantum federated learning (QFL) via a proposed federated fine-tuning method, enhancing efficiency, providing theoretical guarantees, and ensuring scalability.


<details>
  <summary>Details</summary>
Motivation: To leverage the capabilities of LLMs in improving the efficiency and performance of QFL while addressing challenges like communication costs and privacy.

Method: Propose a federated fine-tuning method that incorporates LLMs into QFL. This method allows local model adaptation by clients, preserves privacy, reduces global updates, and uses the fine-tuned LLM as a reinforcement agent to optimize various aspects of QFL.

Result: Experiments demonstrate significant efficiency gains, including reduced communication costs, faster convergence, and effective client selection. Theoretical guarantees for adaptive federated optimization are also provided.

Conclusion: This work successfully creates a synergy between LLMs and QFL, offering practical efficiency improvements, rigorous theoretical foundations, and scalable deployment options using PEFT methods.

Abstract: Inspired by the power of large language models (LLMs), our research adapts
them to quantum federated learning (QFL) to boost efficiency and performance.
We propose a federated fine-tuning method that distills an LLM within QFL,
allowing each client to locally adapt the model to its own data while
preserving privacy and reducing unnecessary global updates. The fine-tuned LLM
also acts as a reinforcement agent, optimizing QFL by adjusting optimizer
steps, cutting down communication rounds, and intelligently selecting clients.
Experiments show significant efficiency gains. We pioneer a synergy between LLM
and QFL, offering: i) practical efficiency: Reduced communication costs and
faster convergence. ii) theoretical rigor: Provable guarantees for adaptive
federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable
deployment on resource-constrained quantum devices. Code implementation is
available here 1.

</details>


### [205] [Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems](https://arxiv.org/abs/2505.18671)
*Giacomo Turri,Luigi Bonati,Kai Zhu,Massimiliano Pontil,Pietro Novelli*

Main category: cs.LG

TL;DR: The paper presents an encoder-only method for learning evolution operators of large-scale non-linear dynamical systems, which can be applied to understand complex spatio-temporal patterns in various scientific domains.


<details>
  <summary>Details</summary>
Motivation: To provide a data-driven tool for understanding large-scale non-linear dynamical systems as terabyte-scale datasets and advanced simulation tools become more accessible.

Method: An encoder-only approach leveraging self-supervised representation learning methods and the learning theory of evolution operators.

Result: Successfully explains folding dynamics of small proteins, binding processes of drug-like molecules, and finds patterns in climate data.

Conclusion: This method offers an effective way to analyze complex natural phenomena across multiple scientific domains.

Abstract: We introduce an encoder-only approach to learn the evolution operators of
large-scale non-linear dynamical systems, such as those describing complex
natural phenomena. Evolution operators are particularly well-suited for
analyzing systems that exhibit complex spatio-temporal patterns and have become
a key analytical tool across various scientific communities. As terabyte-scale
weather datasets and simulation tools capable of running millions of molecular
dynamics steps per day are becoming commodities, our approach provides an
effective tool to make sense of them from a data-driven perspective. The core
of it lies in a remarkable connection between self-supervised representation
learning methods and the recently established learning theory of evolution
operators. To show the usefulness of the proposed method, we test it across
multiple scientific domains: explaining the folding dynamics of small proteins,
the binding process of drug-like molecules in host sites, and autonomously
finding patterns in climate data. Code and data to reproduce the experiments
are made available open source.

</details>


### [206] [Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?](https://arxiv.org/abs/2505.18672)
*Hongzheng Yang,Yongqiang Chen,Zeyu Qin,Tongliang Liu,Chaowei Xiao,Kun Zhang,Bo Han*

Main category: cs.LG

TL;DR: The paper explores the challenge of locating faithful concepts for intervention in LLMs to improve safety alignment. It proposes Concept Concentration (COCA), a method that refracts training data with an explicit reasoning process to identify unsafe concepts and decide responses, thereby simplifying the decision boundary between harmful and benign representations. Experiments show COCA reduces jailbreak success rates while maintaining performance on regular tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of examination on whether one can locate faithful concepts for intervention in LLMs, particularly focusing on safety alignment.

Method: Proposes COCA, which refracts training data using an explicit reasoning process to identify unsafe concepts and determine appropriate responses, aiming to simplify the decision boundary between harmful and benign representations.

Result: COCA significantly reduces both in-distribution and out-of-distribution jailbreak success rates while preserving strong performance on standard tasks like math and code generation.

Conclusion: Locating faithful concepts for intervention is challenging in non-linear settings, but COCA offers an effective solution by modifying the training data to enable more successful linear erasure.

Abstract: Representation intervention aims to locate and modify the representations
that encode the underlying concepts in Large Language Models (LLMs) to elicit
the aligned and expected behaviors. Despite the empirical success, it has never
been examined whether one could locate the faithful concepts for intervention.
In this work, we explore the question in safety alignment. If the interventions
are faithful, the intervened LLMs should erase the harmful concepts and be
robust to both in-distribution adversarial prompts and the out-of-distribution
(OOD) jailbreaks. While it is feasible to erase harmful concepts without
degrading the benign functionalities of LLMs in linear settings, we show that
it is infeasible in the general non-linear setting. To tackle the issue, we
propose Concept Concentration (COCA). Instead of identifying the faithful
locations to intervene, COCA refractors the training data with an explicit
reasoning process, which firstly identifies the potential unsafe concepts and
then decides the responses. Essentially, COCA simplifies the decision boundary
between harmful and benign representations, enabling more effective linear
erasure. Extensive experiments with multiple representation intervention
methods and model architectures demonstrate that COCA significantly reduces
both in-distribution and OOD jailbreak success rates, and meanwhile maintaining
strong performance on regular tasks such as math and code generation.

</details>


### [207] [Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor](https://arxiv.org/abs/2505.18693)
*Ihtesham Ibn Malek,Hafiz Imtiaz,Samia Subrina*

Main category: cs.LG

TL;DR: 本文通过机器学习框架优化无空穴传输层的钙钛矿太阳能电池，提升效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 无空穴传输层（HTL）的钙钛矿太阳能电池因其成本效益和稳定性而成为传统结构的替代方案，但需要进一步优化其效率和稳定性。

Method: 研究结合实验验证与数值模拟，生成1650个样本的数据集，采用四阶多项式回归模型进行预测，并使用L-BFGS-B优化算法寻找最优配置。最后通过多层感知机（MLP）分类器对数据集进行分类识别。

Result: 模型在效率和降解预测上表现出色，成功将设备效率从13.7%提高到16.84%，并将1000小时内的降解从6.61%减少到2.39%。MLP分类器达到了100%的分类准确率。

Conclusion: 机器学习驱动的框架能够有效优化无HTL的钙钛矿太阳能电池的性能，为未来设计提供了指导。

Abstract: Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a
cost-effective and stable alternative to conventional architectures, utilizing
only an absorber layer and an electron transport layer (ETL). This study
presents a machine learning (ML)-driven framework to optimize the efficiency
and stability of HTL-free PSCs by integrating experimental validation with
numerical simulations. Excellent agreement is achieved between a fabricated
device and its simulated counterpart at a molar fraction \( x = 68.7\% \) in
\(\mathrm{MAPb}_{1-x}\mathrm{Sb}_{2x/3}\mathrm{I}_3\), where MA is
methylammonium. A dataset of 1650 samples is generated by varying molar
fraction, absorber defect density, thickness, and ETL doping, with
corresponding efficiency and 50-hour degradation as targets. A fourth-degree
polynomial regressor (PR-4) shows the best performance, achieving RMSEs of
0.0179 and 0.0117, and \( R^2 \) scores of 1 and 0.999 for efficiency and
degradation, respectively. The derived model generalizes beyond the training
range and is used in an L-BFGS-B optimization algorithm with a weighted
objective function to maximize efficiency and minimize degradation. This
improves device efficiency from 13.7\% to 16.84\% and reduces degradation from
6.61\% to 2.39\% over 1000 hours. Finally, the dataset is labeled into superior
and inferior classes, and a multilayer perceptron (MLP) classifier achieves
100\% accuracy, successfully identifying optimal configurations.

</details>


### [208] [Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study](https://arxiv.org/abs/2505.18697)
*Ziyang Cheng,Zhixun Li,Yuhan Li,Yixin Song,Kangyi Zhao,Dawei Cheng,Jia Li,Jeffrey Xu Yu*

Main category: cs.LG

TL;DR: This paper explores the potential of large language models (LLMs) in mitigating catastrophic forgetting in Graph Continual Learning (GCL). It identifies flaws in current GCL experimental setups, evaluates LLMs in realistic scenarios, and proposes a new method called Simple Graph Continual Learning (SimGCL), which significantly outperforms existing methods. The authors also provide a benchmark for training and evaluating GCL methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address catastrophic forgetting in graph machine learning using pretrained large language models (LLMs) within the context of Graph Continual Learning (GCL). Current approaches are based on training from scratch with streaming data, but the rise of pretrained models offers a new avenue to leverage their strong generalization abilities.

Method: The authors first identify flaws in current GCL experimental setups, specifically task ID leakage during evaluation. They then evaluate LLM performance in more realistic scenarios, finding that minor modifications can lead to significant improvements. Based on extensive experiments, they propose SimGCL, a simple-yet-effective method for GCL that does not require rehearsal.

Result: SimGCL surpasses the previous state-of-the-art GNN-based baseline by approximately 20% under the rehearsal-free constraint. The proposed method demonstrates outstanding results in realistic scenarios.

Conclusion: The study concludes that LLMs can effectively mitigate catastrophic forgetting in GCL. The authors contribute a new method (SimGCL) and an easy-to-use benchmark (LLM4GCL) for future research in this area.

Abstract: Nowadays, real-world data, including graph-structure data, often arrives in a
streaming manner, which means that learning systems need to continuously
acquire new knowledge without forgetting previously learned information.
Although substantial existing works attempt to address catastrophic forgetting
in graph machine learning, they are all based on training from scratch with
streaming data. With the rise of pretrained models, an increasing number of
studies have leveraged their strong generalization ability for continual
learning. Therefore, in this work, we attempt to answer whether large language
models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning
(GCL). We first point out that current experimental setups for GCL have
significant flaws, as the evaluation stage may lead to task ID leakage. Then,
we evaluate the performance of LLMs in more realistic scenarios and find that
even minor modifications can lead to outstanding results. Finally, based on
extensive experiments, we propose a simple-yet-effective method, Simple Graph
Continual Learning (SimGCL), that surpasses the previous state-of-the-art
GNN-based baseline by around 20% under the rehearsal-free constraint. To
facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL
for training and evaluating existing GCL methods. The code is available at:
https://github.com/ZhixunLEE/LLM4GCL.

</details>


### [209] [MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention](https://arxiv.org/abs/2505.18698)
*Can Yaras,Alec S. Xu,Pierre Abillama,Changwoo Lee,Laura Balzano*

Main category: cs.LG

TL;DR: Transformers excel in tasks but face quadratic complexity. This paper introduces MonarchAttention, a sub-quadratic attention approximation method using Monarch matrices, which is transferable and hardware-efficient. It achieves significant speed-ups over FlashAttention-2 while accurately approximating softmax attention.


<details>
  <summary>Details</summary>
Motivation: To address the quadratic complexity issue of Transformers' attention mechanism that limits their scalability to long sequences.

Method: Propose MonarchAttention based on Monarch matrices and variational softmax form for efficient computation with reduced complexity. The approach involves an optimization-based algorithm for approximate projection onto Monarch matrices.

Result: MonarchAttention shows substantial speed-ups over FlashAttention-2 across different sequence lengths without significant performance loss, and it performs well on various vision and language tasks.

Conclusion: MonarchAttention provides a scalable and efficient solution for Transformer models by reducing computational and memory complexity while maintaining high performance.

Abstract: Transformers have achieved state-of-the-art performance across various tasks,
but suffer from a notable quadratic complexity in sequence length due to the
attention mechanism. In this work, we propose MonarchAttention -- a novel
approach to sub-quadratic attention approximation via Monarch matrices, an
expressive class of structured matrices. Based on the variational form of
softmax, we describe an efficient optimization-based algorithm to compute an
approximate projection of softmax attention onto the class of Monarch matrices
with $\Theta(N\sqrt{N} d)$ computational complexity and $\Theta(Nd)$ memory/IO
complexity. Unlike previous approaches, MonarchAttention is both (1)
transferable, yielding minimal performance loss with no additional training,
even when replacing every attention layer of the transformer, and (2)
hardware-efficient, utilizing the highest-throughput tensor core units on
modern GPUs. With optimized kernels, MonarchAttention achieves substantial
speed-ups in wall-time over FlashAttention-2: $1.4\times$ for shorter sequences
$(N=256)$, $4.5\times$ for medium-length sequences $(N=4K)$, and $8.2\times$
for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention
on diverse tasks and architectures in vision and language problems, showing
that it flexibly and accurately approximates softmax attention in a variety of
contexts. Our code is available at
https://github.com/cjyaras/monarch-attention.

</details>


### [210] [Steering LLM Reasoning Through Bias-Only Adaptation](https://arxiv.org/abs/2505.18706)
*Viacheslav Sinii,Alexey Gorbatovski,Artem Cherepanov,Boris Shaposhnikov,Nikita Balagansky,Daniil Gavrilov*

Main category: cs.LG

TL;DR: Steering vectors, which amplify selected hidden features without changing original weights, recover and sometimes exceed the accuracy of fully-tuned models in reasoning tasks, suggesting that reasoning skills pre-exist in base models.


<details>
  <summary>Details</summary>
Motivation: To test the claim that reinforcement-learning finetuning strengthens rather than creates reasoning capabilities by using steering vectors to amplify hidden features in pretrained networks.

Method: Training steering vectors - layer-wise biases that amplify selected hidden features without altering original weights, and conducting experiments on four base models across GSM8K and MATH benchmarks. Also, performing logit-lens analysis to understand the impact on token groups.

Result: Steering vectors recover and in some cases exceed the accuracy of fully-tuned models. Logit-lens analysis shows that vectors boost token groups related to structured languages and logical connectors.

Conclusion: The required reasoning skills already exist in the base model and can be effectively enhanced using steering vectors.

Abstract: Recent work on reasoning-oriented language models, exemplified by o1-like
systems, suggests that reinforcement-learning (RL) finetuning does not create
new capabilities but instead strengthens reasoning patterns already latent in
the pretrained network. We test this claim by training steering vectors:
layer-wise biases that additively amplify selected hidden features while
leaving all original weights unchanged. Experiments on four base models across
the GSM8K and MATH benchmarks show that steering vectors recover, and in
several cases exceed, the accuracy of fully-tuned counterparts. This result
supports the view that the required reasoning skills pre-exist in the base
model. Further, logit-lens analysis reveals that the trained vectors
consistently boost token groups linked to structured languages and logical
connectors, providing an interpretable account that aligns with the demands of
quantitative reasoning tasks.

</details>


### [211] [Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer](https://arxiv.org/abs/2505.18713)
*Guodong Du,Zitao Fang,Jing Li,Junlin Li,Runhua Jiang,Shuyang Yu,Yifei Guo,Yangneng Chen,Sim Kuan Goh,Ho-Kin Tang,Daojing He,Honghai Liu,Min Zhang*

Main category: cs.LG

TL;DR: This paper presents NPS-Pruning, a novel method for pruning fine-tuned models by searching through neural parameters of task vectors within low-rank subspaces. It enhances knowledge transfer, facilitates knowledge fusion, and enables deployment of compressed models with near-original performance while reducing storage costs.


<details>
  <summary>Details</summary>
Motivation: Fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency.

Method: The method preprocesses fine-tuned models by calculating the differences between them and the original model, then searches through neural parameters of task vectors within low-rank subspaces to enhance pruning efficiency.

Result: Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of the approach, resulting in substantial performance gains.

Conclusion: NPS-Pruning is an effective method for slimming down fine-tuned models, with applications in enhancing knowledge transfer, facilitating knowledge fusion, and enabling deployment of compressed models.

Abstract: Foundation models and their checkpoints have significantly advanced deep
learning, boosting performance across various applications. However, fine-tuned
models often struggle outside their specific domains and exhibit considerable
redundancy. Recent studies suggest that combining a pruned fine-tuned model
with the original pre-trained model can mitigate forgetting, reduce
interference when merging model parameters across tasks, and improve
compression efficiency. In this context, developing an effective pruning
strategy for fine-tuned models is crucial. Leveraging the advantages of the
task vector mechanism, we preprocess fine-tuned models by calculating the
differences between them and the original model. Recognizing that different
task vector subspaces contribute variably to model performance, we introduce a
novel method called Neural Parameter Search (NPS-Pruning) for slimming down
fine-tuned models. This method enhances pruning efficiency by searching through
neural parameters of task vectors within low-rank subspaces. Our method has
three key applications: enhancing knowledge transfer through pairwise model
interpolation, facilitating effective knowledge fusion via model merging, and
enabling the deployment of compressed models that retain near-original
performance while significantly reducing storage costs. Extensive experiments
across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness
and robustness of our approach, resulting in substantial performance gains. The
code is publicly available at: https://github.com/duguodong7/NPS-Pruning.

</details>


### [212] [LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning](https://arxiv.org/abs/2505.18724)
*Junyu Chen,Junzhuo Li,Zhen Peng,Wenjie Wang,Yuxiang Ren,Long Shi,Xuming Hu*

Main category: cs.LG

TL;DR: This paper proposes LoTA-QAF, a novel fine-tuning method for quantized large language models (LLMs) that enables lossless merging of ternary adaptation weights and adjusts all quantized weights. It addresses challenges in fine-tuning quantized models, such as data type mismatch and accuracy degradation. Experiments show that LoTA-QAF recovers performance for quantized models on the MMLU benchmark and outperforms other methods in task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning quantized models is crucial for deploying LLMs on resource-constrained edge devices but presents significant challenges including data type mismatch between low-precision quantized weights and high-precision adaptation weights, potential accuracy degradation when merging these weights, and lack of methods supporting lossless merging of adaptation while adjusting all quantized weights.

Method: The method, called LoTA-QAF, includes: i) A custom-designed ternary adaptation (TA) aligning ternary weights with the quantization grid and using them to adjust quantized weights; ii) A TA-based mechanism enabling lossless merging of adaptation weights; iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights.

Result: LoTA-QAF effectively recovers performance for quantized models on the MMLU benchmark, surpassing 16-bit LoRA by up to 5.14%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods.

Conclusion: LoTA-QAF is a promising solution for fine-tuning quantized LLMs, addressing key challenges in this area and demonstrating strong performance on various downstream tasks.

Abstract: Quantization and fine-tuning are crucial for deploying large language models
(LLMs) on resource-constrained edge devices. However, fine-tuning quantized
models presents significant challenges, primarily stemming from: First, the
mismatch in data types between the low-precision quantized weights (e.g.,
4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch
limits the computational efficiency advantage offered by quantized weights
during inference. Second, potential accuracy degradation when merging these
high-precision adaptation weights into the low-precision quantized weights, as
the adaptation weights often necessitate approximation or truncation. Third, as
far as we know, no existing methods support the lossless merging of adaptation
while adjusting all quantized weights. To address these challenges, we
introduce lossless ternary adaptation for quantization-aware fine-tuning
(LoTA-QAF). This is a novel fine-tuning method specifically designed for
quantized LLMs, enabling the lossless merging of ternary adaptation weights
into quantized weights and the adjustment of all quantized weights. LoTA-QAF
operates through a combination of: i) A custom-designed ternary adaptation (TA)
that aligns ternary weights with the quantization grid and uses these ternary
weights to adjust quantized weights. ii) A TA-based mechanism that enables the
lossless merging of adaptation weights. iii) Ternary signed gradient descent
(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and
Qwen-2.5 model families and validate its effectiveness on several downstream
tasks. On the MMLU benchmark, our method effectively recovers performance for
quantized models, surpassing 16-bit LoRA by up to 5.14\%. For task-specific
fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still
outperforms other methods.

</details>


### [213] [Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling](https://arxiv.org/abs/2505.18728)
*Andrea Ceni,Alessio Gravina,Claudio Gallicchio,Davide Bacciu,Carola-Bibiane Schonlieb,Moshe Eliasof*

Main category: cs.LG

TL;DR: This paper introduces MP-SSM, a new approach that integrates State-Space Models (SSMs) into the Message-Passing Neural Network framework for graph learning. It ensures permutation equivariance, computational efficiency, and long-range information propagation while enabling exact sensitivity analysis to address issues like vanishing gradients.


<details>
  <summary>Details</summary>
Motivation: Existing Graph State-Space Models (GSSMs) compromise core properties such as permutation equivariance, message-passing compatibility, and computational efficiency when applying SSM modules to sequences extracted from graphs.

Method: The method involves embedding the key principles of modern SSM computation directly into the Message-Passing Neural Network framework, creating MP-SSM which is designed for both static and temporal graphs. This design choice allows for permutation-equivariant, efficient long-range information propagation and an exact sensitivity analysis.

Result: MP-SSM was validated across various tasks including node classification, graph property prediction, long-range benchmarks, and spatiotemporal forecasting. The results showed its versatility and strong empirical performance.

Conclusion: MP-SSM offers a unified methodology for graph learning with advantages in permutation equivariance, computational efficiency, and addressing issues like vanishing gradients through exact sensitivity analysis.

Abstract: The recent success of State-Space Models (SSMs) in sequence modeling has
motivated their adaptation to graph learning, giving rise to Graph State-Space
Models (GSSMs). However, existing GSSMs operate by applying SSM modules to
sequences extracted from graphs, often compromising core properties such as
permutation equivariance, message-passing compatibility, and computational
efficiency. In this paper, we introduce a new perspective by embedding the key
principles of modern SSM computation directly into the Message-Passing Neural
Network framework, resulting in a unified methodology for both static and
temporal graphs. Our approach, MP-SSM, enables efficient,
permutation-equivariant, and long-range information propagation while
preserving the architectural simplicity of message passing. Crucially, MP-SSM
enables an exact sensitivity analysis, which we use to theoretically
characterize information flow and evaluate issues like vanishing gradients and
over-squashing in the deep regime. Furthermore, our design choices allow for a
highly optimized parallel implementation akin to modern SSMs. We validate
MP-SSM across a wide range of tasks, including node classification, graph
property prediction, long-range benchmarks, and spatiotemporal forecasting,
demonstrating both its versatility and strong empirical performance.

</details>


### [214] [Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction](https://arxiv.org/abs/2505.18731)
*Wei Shen,Xiaonan He,Chuheng Zhang,Xuyun Zhang,Xiaolong Xu,Wanchun Dou*

Main category: cs.LG

TL;DR: Reward-driven dialogue agents need accurate user satisfaction estimation. Weak labels from post-hoc user actions introduce bias and fail to capture dissatisfaction due to ASR errors. Power-law distribution of queries causes accuracy drops in low-frequency domains. Proposed auxiliary tasks improve representation learning for rare utterances and long-tailed domains, enhancing error recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Proactive dialogue agents require precise user satisfaction estimation as intrinsic reward signals. Current methods using weak labels suffer from noisy supervision and feedback sparsity.

Method: Propose two auxiliary tasks: contrastive self-supervised learning for rare utterance representation and ASR error identification, and domain-intent classification for session representation in long-tailed domains.

Result: Significant improvements in error recognition accuracy on rare user utterances and long-tailed domains demonstrated on DuerOS.

Conclusion: Auxiliary tasks enhance user satisfaction prediction by improving representation learning of rare user utterances and sessions in long-tailed domains.

Abstract: Reward-driven proactive dialogue agents require precise estimation of user
satisfaction as an intrinsic reward signal to determine optimal interaction
strategies. Specifically, this framework triggers clarification questions when
detecting potential user dissatisfaction during interactions in the industrial
dialogue system. Traditional works typically rely on training a neural network
model based on weak labels which are generated by a simple model trained on
user actions after current turn. However, existing methods suffer from two
critical limitations in real-world scenarios: (1) Noisy Reward Supervision,
dependence on weak labels derived from post-hoc user actions introduces bias,
particularly failing to capture satisfaction signals in ASR-error-induced
utterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user
queries causes reward prediction accuracy to drop in low-frequency domains. The
noise in the weak labels and a power-law distribution of user utterances
results in that the model is hard to learn good representation of user
utterances and sessions. To address these limitations, we propose two auxiliary
tasks to improve the representation learning of user utterances and sessions
that enhance user satisfaction prediction. The first one is a contrastive
self-supervised learning task, which helps the model learn the representation
of rare user utterances and identify ASR errors. The second one is a
domain-intent classification task, which aids the model in learning the
representation of user sessions from long-tailed domains and improving the
model's performance on such domains. The proposed method is evaluated on
DuerOS, demonstrating significant improvements in the accuracy of error
recognition on rare user utterances and long-tailed domains.

</details>


### [215] [AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping](https://arxiv.org/abs/2505.18738)
*Haonan Dong,Wenhao Zhu,Guojie Song,Liang Wang*

Main category: cs.LG

TL;DR: AuroRA introduces an Adaptive Nonlinear Layer (ANL) to overcome LoRA's low-rank bottleneck, achieving better performance with fewer parameters in NLP and CV tasks.


<details>
  <summary>Details</summary>
Motivation: LoRA faces a low-rank bottleneck where enhancing performance requires increasing parameter matrix rank, leading to significant overhead. Linear variants fail to improve representational capacity fundamentally.

Method: AuroRA incorporates an ANL between two linear projectors forming an MLP-like structure with compressed rank, allowing flexible approximation of target functions and theoretically guaranteeing lower errors and bounded gradients.

Result: AuroRA matches or surpasses full fine-tuning performance using only 6.18% ~ 25% of LoRA's parameters, outperforms state-of-the-art PEFT methods by up to 10.88%, and shows robust performance across various rank configurations.

Conclusion: AuroRA effectively addresses LoRA's limitations, providing superior performance with reduced parameter requirements in both NLP and CV domains.

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA
faces an inherent low-rank bottleneck: narrowing its performance gap with full
finetuning requires increasing the rank of its parameter matrix, resulting in
significant parameter overhead. Recent linear LoRA variants have attempted to
enhance expressiveness by introducing additional linear mappings; however,
their composition remains inherently linear and fails to fundamentally improve
LoRA's representational capacity. To address this limitation, we propose
AuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear
projectors to capture fixed and learnable nonlinearities. This combination
forms an MLP-like structure with a compressed rank, enabling flexible and
precise approximation of diverse target functions while theoretically
guaranteeing lower approximation errors and bounded gradients. Extensive
experiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I)
not only matches or surpasses full fine-tuning performance with only 6.18% ~
25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT
methods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust
performance across various rank configurations.

</details>


### [216] [Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation](https://arxiv.org/abs/2505.18755)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: cs.LG

TL;DR: In this paper, researchers developed a hybrid deep learning model for electricity theft detection in residential PV systems within smart cities. The model combines multi-scale CNN, LSTM, and Transformer to capture temporal dependencies and integrates time-series data with temperature variables through data embedding. Experiments show significant improvements in detecting complex energy theft, ensuring supply-demand balance and fairness in smart city energy systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the increasing challenges faced by smart cities due to cyber-attacks and sophisticated electricity theft behaviors in residential photovoltaic (PV) generation systems. Traditional ETD methods are insufficient as they fail to capture complex temporal dependencies and integrate multi-source data effectively.

Method: The method involves proposing an efficient Electricity Theft Detection (ETD) system using a hybrid deep learning model that combines multi-scale Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer. Additionally, a data embedding technique is introduced to integrate time-series data with discrete temperature variables.

Result: The extensive simulation experiments conducted using real-world data confirm the effectiveness of the proposed approach. There were significant improvements in the accuracy of detecting sophisticated energy theft activities.

Conclusion: The conclusion drawn from this research is that the hybrid deep learning model significantly enhances the ability to detect electricity theft in residential PV systems, contributing to the stability and fairness of energy systems in smart cities.

Abstract: With the proliferation of smart grids, smart cities face growing challenges
due to cyber-attacks and sophisticated electricity theft behaviors,
particularly in residential photovoltaic (PV) generation systems. Traditional
Electricity Theft Detection (ETD) methods often struggle to capture complex
temporal dependencies and integrating multi-source data, limiting their
effectiveness. In this work, we propose an efficient ETD method that accurately
identifies fraudulent behaviors in residential PV generation, thus ensuring the
supply-demand balance in smart cities. Our hybrid deep learning model,
combining multi-scale Convolutional Neural Network (CNN), Long Short-Term
Memory (LSTM), and Transformer, excels in capturing both short-term and
long-term temporal dependencies. Additionally, we introduce a data embedding
technique that seamlessly integrates time-series data with discrete temperature
variables, enhancing detection robustness. Extensive simulation experiments
using real-world data validate the effectiveness of our approach, demonstrating
significant improvements in the accuracy of detecting sophisticated energy
theft activities, thereby contributing to the stability and fairness of energy
systems in smart cities.

</details>


### [217] [Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding](https://arxiv.org/abs/2505.18758)
*Alexander Conzelmann,Robert Bamler*

Main category: cs.LG

TL;DR: 提出了一种新的后训练压缩框架，结合了速率感知量化与熵编码，允许快速解码且兼容任意量化网格，在保持性能的同时将比特率降低20-40%。


<details>
  <summary>Details</summary>
Motivation: 神经网络规模不断增大，对资源受限设备（如嵌入式传感器）提出了严重挑战，因此需要压缩算法来减少模型大小，同时保持接近原始的模型性能。

Method: 通过(1)在逐层损失中加入二次速率估计进行扩展，(2)基于最优脑外科医生(OBS)方法对修改后的目标提供局部精确解，从而结合速率感知量化与熵编码实现压缩框架。

Result: 在各种计算机视觉网络上的实证测试表明，该方法在保持与流行压缩算法NNCodec相同性能的同时，可使比特率降低20-40%。

Conclusion: 所提出的压缩框架能够有效降低神经网络的比特率，并保持模型性能，具有快速解码和兼容任意量化网格的优势。

Abstract: The ever-growing size of neural networks poses serious challenges on
resource-constrained devices, such as embedded sensors. Compression algorithms
that reduce their size can mitigate these problems, provided that model
performance stays close to the original. We propose a novel post-training
compression framework that combines rate-aware quantization with entropy coding
by (1) extending the well-known layer-wise loss by a quadratic rate estimation,
and (2) providing locally exact solutions to this modified objective following
the Optimal Brain Surgeon (OBS) method. Our method allows for very fast
decoding and is compatible with arbitrary quantization grids. We verify our
results empirically by testing on various computer-vision networks, achieving a
20-40\% decrease in bit rate at the same performance as the popular compression
algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.

</details>


### [218] [GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning](https://arxiv.org/abs/2505.18763)
*Shutong Ding,Ke Hu,Shan Zhong,Haoyang Luo,Weinan Zhang,Jingya Wang,Jun Wang,Ye Shi*

Main category: cs.LG

TL;DR: GenPO，一种生成策略优化框架，通过精确扩散逆变换和双重虚拟动作机制解决了在on-policy RL中使用扩散策略的关键挑战，首次成功将扩散策略整合进on-policy RL，展现其在多种机器人任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管离线RL和off-policy RL中扩散策略取得进展，但将其整合到如PPO等on-policy框架中仍未被充分探索，尤其是在大规模并行GPU加速模拟器（如IsaacLab）上进行复杂机器人任务训练时存在挑战，主要困难在于计算状态-动作对数似然度在扩散策略下不可行。

Method: 提出GenPO框架，利用精确扩散逆变换构建可逆动作映射，并引入双重虚拟动作机制通过交替更新实现可逆性，从而解决对数似然度计算障碍。同时，利用动作对数似然度进行无偏熵和KL散度估计，支持KL自适应学习率和熵正则化。

Result: 在八个IsaacLab基准测试中，包括腿部运动、灵巧操作、空中控制和机械臂任务，GenPO表现出优于现有RL基线的性能。这是首个成功将扩散策略整合进on-policy RL的方法。

Conclusion: GenPO填补了扩散策略在on-policy RL应用中的空白，为大规模并行训练和真实世界机器人部署打开了新的可能性。

Abstract: Recent advances in reinforcement learning (RL) have demonstrated the powerful
exploration capabilities and multimodality of generative diffusion-based
policies. While substantial progress has been made in offline RL and off-policy
RL settings, integrating diffusion policies into on-policy frameworks like PPO
remains underexplored. This gap is particularly significant given the
widespread use of large-scale parallel GPU-accelerated simulators, such as
IsaacLab, which are optimized for on-policy RL algorithms and enable rapid
training of complex robotic tasks. A key challenge lies in computing
state-action log-likelihoods under diffusion policies, which is straightforward
for Gaussian policies but intractable for flow-based models due to irreversible
forward-reverse processes and discretization errors (e.g., Euler-Maruyama
approximations). To bridge this gap, we propose GenPO, a generative policy
optimization framework that leverages exact diffusion inversion to construct
invertible action mappings. GenPO introduces a novel doubled dummy action
mechanism that enables invertibility via alternating updates, resolving
log-likelihood computation barriers. Furthermore, we also use the action
log-likelihood for unbiased entropy and KL divergence estimation, enabling
KL-adaptive learning rates and entropy regularization in on-policy updates.
Extensive experiments on eight IsaacLab benchmarks, including legged locomotion
(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow
Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate
GenPO's superiority over existing RL baselines. Notably, GenPO is the first
method to successfully integrate diffusion policies into on-policy RL,
unlocking their potential for large-scale parallelized training and real-world
robotic deployment.

</details>


### [219] [Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization](https://arxiv.org/abs/2505.18765)
*Dai Hai Nguyen,Hiroshi Mamitsuka,Atsuyoshi Nakamura*

Main category: cs.LG

TL;DR: An iterative particle-based algorithm named MWGraD is proposed to solve the Multi-Objective Distributional Optimization problem, which gradually minimizes multiple objective functionals simultaneously.


<details>
  <summary>Details</summary>
Motivation: Multi-Objective Distributional Optimization that minimizes multiple objective functionals over a family of probability distributions is common in machine learning and statistics. Applications include multiple target sampling, multi-task learning, and multi-objective generative modeling.

Method: The proposed method, Muliple Wasserstein Gradient Descent (MWGraD), is an iterative particle-based algorithm. It constructs a flow of intermediate empirical distributions represented by particles to minimize multiple objectives simultaneously. At each iteration, it estimates the Wasserstein gradient for each objective based on current particles, aggregates these gradients with dynamically adjusted weights, and updates the particles accordingly.

Result: Theoretical analysis and experimental results on both synthetic and real-world datasets demonstrate the effectiveness of MWGraD.

Conclusion: MWGraD provides an effective solution to the Multi-Objective Distributional Optimization problem.

Abstract: We address the optimization problem of simultaneously minimizing multiple
objective functionals over a family of probability distributions. This type of
Multi-Objective Distributional Optimization commonly arises in machine learning
and statistics, with applications in areas such as multiple target sampling,
multi-task learning, and multi-objective generative modeling. To solve this
problem, we propose an iterative particle-based algorithm, which we call
Muliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of
intermediate empirical distributions, each being represented by a set of
particles, which gradually minimize the multiple objective functionals
simultaneously. Specifically, MWGraD consists of two key steps at each
iteration. First, it estimates the Wasserstein gradient for each objective
functional based on the current particles. Then, it aggregates these gradients
into a single Wasserstein gradient using dynamically adjusted weights and
updates the particles accordingly. In addition, we provide theoretical analysis
and present experimental results on both synthetic and real-world datasets,
demonstrating the effectiveness of MWGraD.

</details>


### [220] [HD-PiSSA: High-Rank Distributed Orthogonal Adaptation](https://arxiv.org/abs/2505.18777)
*Yiding Wang,Fauxu meng,Xuefeng Zhang,Fan Jiang,Pingzhi Tang,Muhan Zhang*

Main category: cs.LG

TL;DR: The paper presents High-rank Distributed PiSSA (HD-PiSSA), a distributed parameter-efficient fine-tuning approach for large language models that enhances expressiveness and performance on complex tasks by initializing orthogonal adapters across different devices and significantly expanding the range of update directions.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA and PiSSA constrain model updates to low-rank subspaces, limiting their expressiveness and leading to suboptimal performance on complex tasks.

Method: High-rank Distributed PiSSA (HD-PiSSA) initializes orthogonal adapters across different devices and aggregates their delta updates collectively on W for fine-tuning. It assigns different principal components of the pre-trained weights to each GPU, significantly expanding the range of update directions.

Result: HD-PiSSA results in over 16x higher effective updated ranks than data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device adapter rank. In the multi-task setting, it achieves average gains of 10.0 absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12 benchmarks.

Conclusion: HD-PiSSA demonstrates its benefits from the extra optimization flexibility and shows significant performance improvements over existing PEFT methods.

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods for large language
models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank
subspaces, limiting their expressiveness and leading to suboptimal performance
on complex tasks. To address this, we introduce High-rank Distributed PiSSA
(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters
across different devices and aggregates their delta updates collectively on W
for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical
adapters across all devices, HD-PiSSA assigns different principal components of
the pre-trained weights to each GPU, significantly expanding the range of
update directions. This results in over 16x higher effective updated ranks than
data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device
adapter rank. Empirically, we evaluate HD-PiSSA across various challenging
downstream tasks, including mathematics, code generation, and multi-task
learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0
absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12
benchmarks, demonstrating its benefits from the extra optimization flexibility.

</details>


### [221] [Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains](https://arxiv.org/abs/2505.18781)
*Shizheng Wen,Arsh Kumbhat,Levi Lingsch,Sepehr Mousavi,Praveen Chandrashekar,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出了一种新的几何感知算子变换器（GAOT），用于在任意域上学习PDE，其在准确性和计算效率方面均表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确且高效地学习偏微分方程（PDE）解算符对于工程和工业仿真至关重要。然而，现有的算子学习算法要么不够精确，要么计算效率低下。

Method: GAOT结合了多尺度注意力图神经算子编码器和解码器、几何嵌入以及（视觉）变换处理器，以精确映射关于域和输入的信息，从而获得PDE解的鲁棒近似。此外，GAOT的多项创新确保了计算效率和可扩展性。

Result: 在大量来自不同PDE的学习任务中，GAOT相比多个基线模型在准确性和效率上均有显著提升，并在大规模三维工业CFD数据集上达到了最先进的性能。

Conclusion: 几何感知算子变换器（GAOT）为在任意域上学习PDE提供了一种既准确又高效的解决方案，具有广泛的应用前景。

Abstract: The very challenging task of learning solution operators of PDEs on arbitrary
domains accurately and efficiently is of vital importance to engineering and
industrial simulations. Despite the existence of many operator learning
algorithms to approximate such PDEs, we find that accurate models are not
necessarily computationally efficient and vice versa. We address this issue by
proposing a geometry aware operator transformer (GAOT) for learning PDEs on
arbitrary domains. GAOT combines novel multiscale attentional graph neural
operator encoders and decoders, together with geometry embeddings and (vision)
transformer processors to accurately map information about the domain and the
inputs into a robust approximation of the PDE solution. Multiple innovations in
the implementation of GAOT also ensure computational efficiency and
scalability. We demonstrate this significant gain in both accuracy and
efficiency of GAOT over several baselines on a large number of learning tasks
from a diverse set of PDEs, including achieving state of the art performance on
a large scale three-dimensional industrial CFD dataset.

</details>


### [222] [Soft Weighted Machine Unlearning](https://arxiv.org/abs/2505.18783)
*Xinbao Qiao,Ningning Ding,Yushi Cheng,Meng Zhang*

Main category: cs.LG

TL;DR: Machine unlearning has been used for fairness and robustness but suffers from over-unlearning causing utility degradation. This paper investigates its causes using counterfactual leave-one-out analysis and proposes a soft-weighted framework with a weighted influence function to address the issue, showing significant improvements in experiments.


<details>
  <summary>Details</summary>
Motivation: To address the problem of over-unlearning in machine unlearning techniques which leads to information loss and utility degradation when applied for fairness and robustness.

Method: Introduce a weighted influence function that assigns tailored weights to each sample by solving a convex quadratic programming problem. Propose a soft-weighted framework enabling fine-grained model adjustments based on this function.

Result: The soft-weighted scheme significantly outperforms hard-weighted schemes in fairness/robustness metrics and alleviates the decline in utility metric in fairness- and robustness-driven tasks.

Conclusion: The proposed soft-weighted framework enhances machine unlearning algorithms as an effective correction solution for fairness and robustness.

Abstract: Machine unlearning, as a post-hoc processing technique, has gained widespread
adoption in addressing challenges like bias mitigation and robustness
enhancement, colloquially, machine unlearning for fairness and robustness.
However, existing non-privacy unlearning-based solutions persist in using
binary data removal framework designed for privacy-driven motivation, leading
to significant information loss, a phenomenon known as over-unlearning. While
over-unlearning has been largely described in many studies as primarily causing
utility degradation, we investigate its fundamental causes and provide deeper
insights in this work through counterfactual leave-one-out analysis. In this
paper, we introduce a weighted influence function that assigns tailored weights
to each sample by solving a convex quadratic programming problem analytically.
Building on this, we propose a soft-weighted framework enabling fine-grained
model adjustments to address the over-unlearning challenge. We demonstrate that
the proposed soft-weighted scheme is versatile and can be seamlessly integrated
into most existing unlearning algorithms. Extensive experiments show that in
fairness- and robustness-driven tasks, the soft-weighted scheme significantly
outperforms hard-weighted schemes in fairness/robustness metrics and alleviates
the decline in utility metric, thereby enhancing machine unlearning algorithm
as an effective correction solution.

</details>


### [223] [Leveraging Per-Instance Privacy for Machine Unlearning](https://arxiv.org/abs/2505.18786)
*Nazanin Mohammadi Sepahvand,Anvith Thudi,Berivan Isik,Ashmita Bhattacharyya,Nicolas Papernot,Eleni Triantafillou,Daniel M. Roy,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: The paper presents a per-instance approach to quantify unlearning difficulty via fine-tuning, improves utility-unlearning tradeoff by using per-instance privacy losses, demonstrates empirical results for SGLD and standard fine-tuning, correlates with data difficulty metrics, and introduces new evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To develop a principled approach to quantify the difficulty of unlearning via fine-tuning, improving upon previous analyses by focusing on per-instance privacy losses rather than worst-case bounds.

Method: Sharpened analysis of noisy gradient descent for unlearning by replacing worst-case privacy loss bounds with per-instance privacy losses, demonstrating this approach through empirical results with Stochastic Gradient Langevin Dynamics (SGLD) and standard fine-tuning without explicit noise.

Result: Empirical results confirm theoretical predictions for both SGLD and standard fine-tuning. Per-instance privacy losses correlate well with existing data difficulty metrics and identify harder groups of data points. Novel evaluation methods based on loss barriers are introduced.

Conclusion: The findings establish a foundation for more efficient and adaptive unlearning strategies that can be tailored to individual data point properties.

Abstract: We present a principled, per-instance approach to quantifying the difficulty
of unlearning via fine-tuning. We begin by sharpening an analysis of noisy
gradient descent for unlearning (Chien et al., 2024), obtaining a better
utility-unlearning tradeoff by replacing worst-case privacy loss bounds with
per-instance privacy losses (Thudi et al., 2024), each of which bounds the
(Renyi) divergence to retraining without an individual data point. To
demonstrate the practical applicability of our theory, we present empirical
results showing that our theoretical predictions are born out both for
Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard
fine-tuning without explicit noise. We further demonstrate that per-instance
privacy losses correlate well with several existing data difficulty metrics,
while also identifying harder groups of data points, and introduce novel
evaluation methods based on loss barriers. All together, our findings provide a
foundation for more efficient and adaptive unlearning strategies tailored to
the unique properties of individual data points.

</details>


### [224] [Governing Equation Discovery from Data Based on Differential Invariants](https://arxiv.org/abs/2505.18798)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: This paper proposes a pipeline for governing equation discovery based on differential invariants, which can effectively reduce the search space while respecting symmetry. The method outperforms other symmetry-informed PDE discovery methods.


<details>
  <summary>Details</summary>
Motivation: Directly discovering partial differential equations from data is challenging due to the vast search space and complexity of determining relevant terms. Symmetry, as a crucial prior knowledge, has been widely applied in various scientific tasks, but its potential in governing equation discovery remains underexplored.

Method: The authors propose a pipeline that computes the set of differential invariants corresponding to the infinitesimal generators of the symmetry group and selects them as the relevant terms for equation discovery. This approach losslessly reduces the search space of existing equation discovery methods while strictly adhering to symmetry.

Result: The proposed method, exemplified by DI-SINDy, demonstrates superior success rate and accuracy in PDE discovery compared to other symmetry-informed governing equation discovery methods across a series of PDEs.

Conclusion: The pipeline based on differential invariants provides an effective way to discover governing equations while respecting symmetry, outperforming existing methods.

Abstract: The explicit governing equation is one of the simplest and most intuitive
forms for characterizing physical laws. However, directly discovering partial
differential equations (PDEs) from data poses significant challenges, primarily
in determining relevant terms from a vast search space. Symmetry, as a crucial
prior knowledge in scientific fields, has been widely applied in tasks such as
designing equivariant networks and guiding neural PDE solvers. In this paper,
we propose a pipeline for governing equation discovery based on differential
invariants, which can losslessly reduce the search space of existing equation
discovery methods while strictly adhering to symmetry. Specifically, we compute
the set of differential invariants corresponding to the infinitesimal
generators of the symmetry group and select them as the relevant terms for
equation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as
an example, we demonstrate that its success rate and accuracy in PDE discovery
surpass those of other symmetry-informed governing equation discovery methods
across a series of PDEs.

</details>


### [225] [How to build a consistency model: Learning flow maps via self-distillation](https://arxiv.org/abs/2505.18825)
*Nicholas M. Boffi,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: A systematic approach for learning flow maps associated with flow and diffusion models is presented, which can convert distillation schemes into direct training algorithms via self-distillation. Empirical evaluations show high-dimensional tasks benefit from objective functions avoiding derivatives while lower-dimensional tasks can incorporate higher-order derivatives.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of generative models based on solutions to differential equations by presenting a systematic approach for learning flow maps associated with flow and diffusion models.

Method: Exploiting the relationship between the velocity field underlying a continuous-time flow and the instantaneous rate of change of the flow map to convert existing distillation schemes into direct training algorithms via self-distillation.

Result: High-dimensional tasks like image synthesis benefit from objective functions that avoid temporal and spatial derivatives of the flow map, while lower-dimensional tasks can benefit from objectives incorporating higher-order derivatives to capture sharp features.

Conclusion: The presented systematic approach for learning flow maps can effectively improve the efficiency of generative models and provides different benefits for high-dimensional and lower-dimensional tasks.

Abstract: Building on the framework proposed in Boffi et al. (2024), we present a
systematic approach for learning flow maps associated with flow and diffusion
models. Flow map-based models, commonly known as consistency models, encompass
recent efforts to improve the efficiency of generative models based on
solutions to differential equations. By exploiting a relationship between the
velocity field underlying a continuous-time flow and the instantaneous rate of
change of the flow map, we show how to convert existing distillation schemes
into direct training algorithms via self-distillation, eliminating the need for
pre-trained models. We empirically evaluate several instantiations of our
framework, finding that high-dimensional tasks like image synthesis benefit
from objective functions that avoid temporal and spatial derivatives of the
flow map, while lower-dimensional tasks can benefit from objectives
incorporating higher-order derivatives to capture sharp features.

</details>


### [226] [Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality](https://arxiv.org/abs/2505.18828)
*Junyan Liu,Ziyun Chen,Kun Wang,Haipeng Luo,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the Pandora's Box problem in an online learning setting with
semi-bandit feedback. In each round, the learner sequentially pays to open up
to $n$ boxes with unknown reward distributions, observes rewards upon opening,
and decides when to stop. The utility of the learner is the maximum observed
reward minus the cumulative cost of opened boxes, and the goal is to minimize
regret defined as the gap between the cumulative expected utility and that of
the optimal policy. We propose a new algorithm that achieves
$\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the
$\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known
lower bound up to logarithmic factors. To better capture real-life
applications, we then extend our results to a natural but challenging
contextual linear setting, where each box's expected reward is linear in some
known but time-varying $d$-dimensional context and the noise distribution is
fixed over time. We design an algorithm that learns both the linear function
and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret.
Finally, we show that our techniques also apply to the online Prophet
Inequality problem, where the learner must decide immediately whether or not to
accept a revealed reward. In both non-contextual and contextual settings, our
approach achieves similar improvements and regret bounds.

</details>


### [227] [On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization](https://arxiv.org/abs/2505.18830)
*Wenlong Deng,Yi Ren,Muchen Li,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: An unrecognized phenomenon called Lazy Likelihood Displacement (LLD) is identified in GRPO algorithm, where the likelihood of correct responses may not increase properly during training. This issue stems from naive penalization of tokens in incorrect responses. A new method named NTHR is developed to address this by downweighting penalties on specific tokens, leading to performance improvements in models.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing a newly recognized issue, Lazy Likelihood Displacement (LLD), within the GRPO algorithm that affects the learning dynamics and hinders proper increase in likelihood of correct responses during training.

Method: NTHR method is developed which downweights penalties on tokens contributing to LLD. It leverages GRPO's group-based structure using correct responses as anchors to identify influential tokens.

Result: Experiments on math reasoning benchmarks show that NTHR effectively mitigates LLD and provides consistent performance gains across models with varying parameters.

Conclusion: NTHR offers an effective solution to the LLD problem in GRPO, enhancing model performance.

Abstract: Reinforcement learning (RL) has become popular in enhancing the reasoning
capabilities of large language models (LLMs), with Group Relative Policy
Optimization (GRPO) emerging as a widely used algorithm in recent systems.
Despite GRPO's widespread adoption, we identify a previously unrecognized
phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood
of correct responses marginally increases or even decreases during training.
This behavior mirrors a recently discovered misalignment issue in Direct
Preference Optimization (DPO), attributed to the influence of negative
gradients. We provide a theoretical analysis of GRPO's learning dynamic,
identifying the source of LLD as the naive penalization of all tokens in
incorrect responses with the same strength. To address this, we develop a
method called NTHR, which downweights penalties on tokens contributing to the
LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's
group-based structure, using correct responses as anchors to identify
influential tokens. Experiments on math reasoning benchmarks demonstrate that
NTHR effectively mitigates LLD, yielding consistent performance gains across
models ranging from 0.5B to 3B parameters.

</details>


### [228] [Distribution-Aware Mobility-Assisted Decentralized Federated Learning](https://arxiv.org/abs/2505.18866)
*Md Farhamdur Reza,Reza Jahani,Richeng Jin,Huaiyu Dai*

Main category: cs.LG

TL;DR: The paper explores the impact of user mobility on decentralized federated learning (DFL) and proposes distribution-aware mobility patterns to enhance DFL performance.


<details>
  <summary>Details</summary>
Motivation: To investigate how user mobility can improve the accuracy of DFL by facilitating information flow and mitigating data heterogeneity.

Method: Introducing a small fraction of mobile clients with random movement in DFL, and proposing novel distribution-aware mobility patterns where mobile clients strategically navigate the network based on data distributions and static client locations.

Result: Extensive experiments show that induced mobility significantly improves DFL accuracy, and the proposed distribution-aware mobility patterns outperform random movement in enhancing learning convergence.

Conclusion: User mobility has a positive impact on DFL performance. Distribution-aware mobility patterns are effective in improving information flow and mitigating data heterogeneity.

Abstract: Decentralized federated learning (DFL) has attracted significant attention
due to its scalability and independence from a central server. In practice,
some participating clients can be mobile, yet the impact of user mobility on
DFL performance remains largely unexplored, despite its potential to facilitate
communication and model convergence. In this work, we demonstrate that
introducing a small fraction of mobile clients, even with random movement, can
significantly improve the accuracy of DFL by facilitating information flow. To
further enhance performance, we propose novel distribution-aware mobility
patterns, where mobile clients strategically navigate the network, leveraging
knowledge of data distributions and static client locations. The proposed
moving strategies mitigate the impact of data heterogeneity and boost learning
convergence. Extensive experiments validate the effectiveness of induced
mobility in DFL and demonstrate the superiority of our proposed mobility
patterns over random movement.

</details>


### [229] [RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.18877)
*Yilang Zhang,Bingcong Li,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: RefLoRA通过每步最小化损失上界来确定最优低秩分解，促进稳定收敛，在多个大语言模型上的实验表明其收敛更快、性能优于多种基准且计算开销可忽略。


<details>
  <summary>Details</summary>
Motivation: LoRA通过更新预训练权重矩阵的低维子空间降低微调大模型的计算和内存开销，但存在因非唯一低秩分解导致的收敛效果差和性能下降问题。

Method: 本文提出了一种新的方法RefLoRA，通过每步确定最优低秩分解以最小化损失上界，从而促进更平坦的损失景观和一致平衡的权重更新。

Result: 在自然语言理解和常识推理任务中使用DeBERTaV3、LLaMA-7B、LLaMA2-7B和LLaMA3-8B等流行的大语言模型进行的广泛实验表明，RefLoRA收敛更快，优于各种基准，并且与最先进的LoRA变体相比计算开销可忽略不计。

Conclusion: RefLoRA解决了LoRA中存在的收敛效果差和性能下降的问题，是一种快速稳定收敛且计算开销小的有效方法。

Abstract: Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of
fine-tuning large models by updating a low-dimensional subspace of the
pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal
convergence and noticeable performance degradation, due to inconsistent and
imbalanced weight updates induced by its nonunique low-rank factorizations. To
overcome these limitations, this article identifies the optimal low-rank
factorization per step that minimizes an upper bound on the loss. The resultant
refactored low-rank adaptation (RefLoRA) method promotes a flatter loss
landscape, along with consistent and balanced weight updates, thus speeding up
stable convergence. Extensive experiments evaluate RefLoRA on natural language
understanding, and commonsense reasoning tasks with popular large language
models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical
tests corroborate that RefLoRA converges faster, outperforms various
benchmarks, and enjoys negligible computational overhead compared to
state-of-the-art LoRA variants.

</details>


### [230] [Partition Generative Modeling: Masked Modeling Without Masks](https://arxiv.org/abs/2505.18883)
*Justin Deschenaux,Lan Tran,Caglar Gulcehre*

Main category: cs.LG

TL;DR: The paper introduces Partition Generative Models (PGMs) that enhance computational efficiency in masked generative modeling by eliminating MASK tokens and using sparse attention patterns. Experiments show PGMs offer 5x improvements in latency and throughput compared to MDLM, with better generative perplexity.


<details>
  <summary>Details</summary>
Motivation: To improve the inefficiency of processing MASK tokens in traditional Masked Generative Models (MGMs).

Method: Partition tokens into two groups and use sparse attention patterns to prevent cross-group information exchange, allowing prediction of one group based solely on the other group's information.

Result: PGMs deliver at least 5x improvements in both latency and throughput compared to MDLM when using the same number of sampling steps, while generating samples with better generative perplexity than MDLM.

Conclusion: PGMs provide significant computational efficiency improvements over traditional MGMs and can be further optimized with Self-Distillation Through Time (SDTT) for additional inference gains.

Abstract: We introduce ``Partition Generative Models'' (PGMs), a novel approach to
masked generative modeling (MGMs), particularly effective for masked diffusion
language modeling (MDLMs). PGM divides tokens into two distinct groups and
employs sparse attention patterns to prevent cross-group information exchange.
Hence, the model is trained to predict tokens in one group based solely on
information from the other group. This partitioning strategy eliminates the
need for MASK tokens entirely. While traditional MGMs inefficiently process
MASK tokens during generation, PGMs achieve greater computational efficiency by
operating exclusively on unmasked tokens. Our experiments on OpenWebText with a
context length of 1024 tokens demonstrate that PGMs deliver at least 5x
improvements in both latency and throughput compared to MDLM when using the
same number of sampling steps, while generating samples with better generative
perplexity than MDLM. Finally, we show that PGMs can be distilled with
Self-Distillation Through Time (SDTT), a method originally devised for MDLM, in
order to achieve further inference gains.

</details>


### [231] [LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders](https://arxiv.org/abs/2505.18884)
*Borna Khodabandeh,Amirabbas Afzali,Amirhossein Afsharrad,Seyed Shahabeddin Mousavi,Sanjay Lall,Sajjad Amini,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: The paper proposes Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework for visual encoders, which significantly improves zero-shot adversial robustness with minimal degradation in clean data accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods of adversarial fine-tuning suffer from instability and suboptimal trade-off between robustness and clean data accuracy.

Method: LORE uses constrained optimization to balance competing goals like improving robustness while preserving nominal performance by enforcing embedding-space proximity constraints.

Result: LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. It also enhances out-of-distribution generalization and interpretability of image embeddings.

Conclusion: LORE is an effective unsupervised adversarial fine-tuning framework that addresses the limitations of current approaches.

Abstract: Visual encoders have become fundamental components in modern computer vision
pipelines. However, ensuring robustness against adversarial perturbations
remains a critical challenge. Recent efforts have explored both supervised and
unsupervised adversarial fine-tuning strategies. We identify two key
limitations in these approaches: (i) they often suffer from instability,
especially during the early stages of fine-tuning, resulting in suboptimal
convergence and degraded performance on clean data, and (ii) they exhibit a
suboptimal trade-off between robustness and clean data accuracy, hindering the
simultaneous optimization of both objectives. To overcome these challenges, we
propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised
adversarial fine-tuning framework. LORE utilizes constrained optimization,
which offers a principled approach to balancing competing goals, such as
improving robustness while preserving nominal performance. By enforcing
embedding-space proximity constraints, LORE effectively maintains clean data
performance throughout adversarial fine-tuning. Extensive experiments show that
LORE significantly improves zero-shot adversarial robustness with minimal
degradation in clean data accuracy. Furthermore, we demonstrate the
effectiveness of the adversarially fine-tuned CLIP image encoder in
out-of-distribution generalization and enhancing the interpretability of image
embeddings.

</details>


### [232] [KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning](https://arxiv.org/abs/2505.18886)
*Zhendong Mi,Qitao Tan,Xiaodong Yu,Zining Zhu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: KerZOO是一种基于核函数的零阶优化框架，用于减少大语言模型微调中的梯度估计偏差，提高优化稳定性及收敛速度。相比现有方法，KerZOO在减少训练时间和提升准确性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 零阶优化虽然内存高效且无需反向传播，但在大语言模型微调中存在梯度估计偏差的问题，影响了收敛速度和性能。因此，需要一种方法来减轻这种偏差。

Method: 通过数学物理工具分析零阶优化中的低阶偏差，并引入基于核函数的零阶优化框架（KerZOO）以减轻偏差并提高优化稳定性。该框架适用于全参数和参数高效的微调场景。

Result: KerZOO在WSC和MultiRC数据集上分别减少了74%和44%的GPU训练时间，并在准确性上超过MeZO基线2.9%和2.6%。证明了核函数是减少零阶方法估计偏差的有效途径。

Conclusion: 基于核函数的零阶优化框架能够有效减少偏差、提升优化稳定性和收敛速度，在大语言模型微调中展现出优越性能。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
numerous NLP tasks. Nevertheless, conventional first-order fine-tuning
techniques impose heavy memory demands, creating practical obstacles to
real-world applications. Zeroth-order (ZO) optimization has recently emerged as
a promising memory-efficient alternative, as it circumvents the need for
backpropagation by estimating gradients solely through forward passes--making
it particularly suitable for resource-limited environments. Despite its
efficiency, ZO optimization suffers from gradient estimation bias, which
significantly hinders convergence speed. To address this, we analytically
identify and characterize the lower-order bias introduced during ZO-based
gradient estimation in LLM fine-tuning. Motivated by tools in mathematical
physics, we introduce a kernel-function-based ZO framework aimed at mitigating
this bias and improving optimization stability. KerZOO achieves comparable or
superior performance to existing ZO baselines in both full-parameter and
parameter-efficient fine-tuning settings of LLMs, while significantly reducing
the number of iterations required to reach convergence. For example, KerZOO
reduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC
datasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9%
and 2.6% in accuracy. We show that the kernel function is an effective avenue
for reducing estimation bias in ZO methods.

</details>


### [233] [Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction](https://arxiv.org/abs/2505.18890)
*Morteza Rakhshaninejad,Mira Jurgens,Nicolas Dewolf,Willem Waegeman*

Main category: cs.LG

TL;DR: The paper explores cluster-conditioned conformal prediction (CP) methods for drug-target interaction (DTI) prediction and compares them with marginal and group-conditioned CP. Nonconformity-based clustering offers the most reliable subgroup coverage, while residual-driven clustering provides robust uncertainty estimates in sparse or novel scenarios.


<details>
  <summary>Details</summary>
Motivation: Accurate DTI prediction is crucial in drug discovery, yet classical marginal CP often neglects variability across drug and protein subgroups.

Method: Three cluster-conditioned CP methods are analyzed and compared with marginal and group-conditioned CP using the KIBA dataset and four data-splitting strategies. Clusterings are obtained via nonconformity scores, feature similarity, and nearest neighbors.

Result: Nonconformity-based clustering yields the tightest intervals and most reliable subgroup coverage, especially in random and fully unseen drug-protein splits. Residual-driven clustering provides robust uncertainty estimates even in sparse or novel scenarios.

Conclusion: Cluster-based CP shows potential for improving DTI prediction under uncertainty.

Abstract: Accurate drug-target interaction (DTI) prediction with machine learning
models is essential for drug discovery. Such models should also provide a
credible representation of their uncertainty, but applying classical marginal
conformal prediction (CP) in DTI prediction often overlooks variability across
drug and protein subgroups. In this work, we analyze three cluster-conditioned
CP methods for DTI prediction, and compare them with marginal and
group-conditioned CP. Clusterings are obtained via nonconformity scores,
feature similarity, and nearest neighbors, respectively. Experiments on the
KIBA dataset using four data-splitting strategies show that nonconformity-based
clustering yields the tightest intervals and most reliable subgroup coverage,
especially in random and fully unseen drug-protein splits. Group-conditioned CP
works well when one entity is familiar, but residual-driven clustering provides
robust uncertainty estimates even in sparse or novel scenarios. These results
highlight the potential of cluster-based CP for improving DTI prediction under
uncertainty.

</details>


### [234] [PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models](https://arxiv.org/abs/2505.18901)
*Xiaoyan Hu,Lauren Pick,Ho-fung Leung,Farzan Farnia*

Main category: cs.LG

TL;DR: The paper introduces PromptWise, an online learning framework that assigns prompts to language models in a cost-effective way by querying cheaper models first and only progressing to more expensive options if necessary. Experiments show PromptWise outperforms cost-unaware baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of model-selection approaches that consider both performance and service cost of generative AI models. Existing methods typically prioritize performance without accounting for pricing differences.

Method: PromptWise is an online learning framework that strategically queries cheaper models first and progresses to more expensive models only when necessary. It aims to assign a sequence of prompts to large language models in a cost-effective manner.

Result: PromptWise demonstrates effectiveness across various tasks such as puzzles and code generation/translation. It consistently outperforms cost-unaware baseline methods, showing that using the most expensive models can lead to higher costs and potentially lower average performance.

Conclusion: PromptWise provides a solution for cost-effective prompt assignment to language models, highlighting the importance of considering both performance and cost in model selection.

Abstract: The rapid advancement of generative AI models has provided users with
numerous options to address their prompts. When selecting a generative AI model
for a given prompt, users should consider not only the performance of the
chosen model but also its associated service cost. The principle guiding such
consideration is to select the least expensive model among the available
satisfactory options. However, existing model-selection approaches typically
prioritize performance, overlooking pricing differences between models. In this
paper, we introduce PromptWise, an online learning framework designed to assign
a sequence of prompts to a group of large language models (LLMs) in a
cost-effective manner. PromptWise strategically queries cheaper models first,
progressing to more expensive options only if the lower-cost models fail to
adequately address a given prompt. Through numerical experiments, we
demonstrate PromptWise's effectiveness across various tasks, including puzzles
of varying complexity and code generation/translation tasks. The results
highlight that PromptWise consistently outperforms cost-unaware baseline
methods, emphasizing that directly assigning prompts to the most expensive
models can lead to higher costs and potentially lower average performance.

</details>


### [235] [Behavior Injection: Preparing Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.18917)
*Zhepeng Cen,Yihang Yao,William Han,Zuxin Liu,Ding Zhao*

Main category: cs.LG

TL;DR: Reinforcement fine-tuning (RFT) is a powerful technique for improving reasoning in large language models (LLMs), but its effectiveness varies. This paper identifies two key conditions for successful RFT and proposes behavior injection, a data-augmentation scheme that enhances RFT performance.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs respond inconsistently to reinforcement fine-tuning (RFT) and to identify ways to improve the effectiveness of RFT.

Method: The authors analyze the per-step influence of the RL objective and identify two key conditions for effective post-training: (1) RL-informative rollout accuracy, and (2) strong data co-influence. They then propose behavior injection, a task-agnostic data-augmentation scheme applied prior to RL, which enriches the supervised finetuning (SFT) data by seeding exploratory and exploitative behaviors.

Result: The proposed method was evaluated across two reasoning benchmarks with multiple base models. The results showed that behavior injection significantly increases the performance gain from RFT over the pre-RL model.

Conclusion: Behavior injection is an effective way to enhance the performance of LLMs during RFT by making them more RL-ready.

Abstract: Reinforcement fine-tuning (RFT) has emerged as a powerful post-training
technique to incentivize the reasoning ability of large language models (LLMs).
However, LLMs can respond very inconsistently to RFT: some show substantial
performance gains, while others plateau or even degrade. To understand this
divergence, we analyze the per-step influence of the RL objective and identify
two key conditions for effective post-training: (1) RL-informative rollout
accuracy, and (2) strong data co-influence, which quantifies how much the
training data affects performance on other samples. Guided by these insights,
we propose behavior injection, a task-agnostic data-augmentation scheme applied
prior to RL. Behavior injection enriches the supervised finetuning (SFT) data
by seeding exploratory and exploitative behaviors, effectively making the model
more RL-ready. We evaluate our method across two reasoning benchmarks with
multiple base models. The results demonstrate that our theoretically motivated
augmentation can significantly increases the performance gain from RFT over the
pre-RL model.

</details>


### [236] [Graph-Based Operator Learning from Limited Data on Irregular Domains](https://arxiv.org/abs/2505.18923)
*Yile Li,Shandian Zhe*

Main category: cs.LG

TL;DR: The paper proposes GOLA, a framework that uses attention-enhanced GNNs to model spatial dependencies and introduces a Fourier-based encoder for flexible embeddings in irregular domains.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing operator learning methods that rely on regular grid discretizations, which restricts their applicability to complex or irregular domains.

Method: Propose GOLA framework which constructs graphs from irregularly sampled spatial points and leverages attention-enhanced Graph Neural Networks (GNNs) to model spatial dependencies with global information. Introduce a Fourier-based encoder that projects input functions into frequency space using learnable complex coefficients.

Result: Outperforms baselines across a range of 2D PDEs including Darcy Flow, Advection, Eikonal, and Nonlinear Diffusion, especially in data-scarce regimes.

Conclusion: Demonstrates strong generalization and efficiency on irregular domains.

Abstract: Operator learning seeks to approximate mappings from input functions to
output solutions, particularly in the context of partial differential equations
(PDEs). While recent advances such as DeepONet and Fourier Neural Operator
(FNO) have demonstrated strong performance, they often rely on regular grid
discretizations, limiting their applicability to complex or irregular domains.
In this work, we propose a Graph-based Operator Learning with Attention (GOLA)
framework that addresses this limitation by constructing graphs from
irregularly sampled spatial points and leveraging attention-enhanced Graph
Neural Netwoks (GNNs) to model spatial dependencies with global information. To
improve the expressive capacity, we introduce a Fourier-based encoder that
projects input functions into a frequency space using learnable complex
coefficients, allowing for flexible embeddings even with sparse or nonuniform
samples. We evaluated our approach across a range of 2D PDEs, including Darcy
Flow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling
densities. Our method consistently outperforms baselines, particularly in
data-scarce regimes, demonstrating strong generalization and efficiency on
irregular domains.

</details>


### [237] [Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time](https://arxiv.org/abs/2505.18926)
*Jingxuan Xu,Hong Huang,Chuhang Zou,Manolis Savva,Yunchao Wei,Wuyang Chen*

Main category: cs.LG

TL;DR: 提出了一种用于实时交互流体模拟的神经物理系统，通过结合数值模拟、神经物理和生成控制，实现低延迟、高保真度的模拟，并支持用户友好的自由手绘草图引导流体控制。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的方法虽然准确但计算量大且存在延迟问题，而现有的机器学习方法虽减少了计算成本但仍未满足实时使用所需的低延迟要求，并且缺乏对交互应用的支持。

Method: 引入了一种新的混合方法，将数值模拟、神经物理和生成控制相结合。神经物理部分采用经典数值求解器作为回退保护以同时追求低延迟和高保真度的模拟；开发了一个基于扩散的控制器，该控制器通过逆向建模策略训练生成外部动态力场来进行流体操作。

Result: 系统在各种2D/3D场景、材料类型和障碍物交互中表现出强大的性能，实现了高帧率的实时模拟（延迟降低11~29%），并且能够通过用户友好的自由手绘草图进行流体控制。

Conclusion: 这是朝着实际、可控且物理上合理的实时交互流体模拟迈出的重要一步，作者承诺在接受后发布模型和数据。

Abstract: We propose a neural physics system for real-time, interactive fluid
simulations. Traditional physics-based methods, while accurate, are
computationally intensive and suffer from latency issues. Recent
machine-learning methods reduce computational costs while preserving fidelity;
yet most still fail to satisfy the latency constraints for real-time use and
lack support for interactive applications. To bridge this gap, we introduce a
novel hybrid method that integrates numerical simulation, neural physics, and
generative control. Our neural physics jointly pursues low-latency simulation
and high physical fidelity by employing a fallback safeguard to classical
numerical solvers. Furthermore, we develop a diffusion-based controller that is
trained using a reverse modeling strategy to generate external dynamic force
fields for fluid manipulation. Our system demonstrates robust performance
across diverse 2D/3D scenarios, material types, and obstacle interactions,
achieving real-time simulations at high frame rates (11~29% latency) while
enabling fluid control guided by user-friendly freehand sketches. We present a
significant step towards practical, controllable, and physically plausible
fluid simulations for real-time interactive applications. We promise to release
both models and data upon acceptance.

</details>


### [238] [Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection](https://arxiv.org/abs/2505.18934)
*Xiping Li,Xiangyu Dong,Xingyi Zhang,Kun Xie,Yuanhao Feng,Bo Wang,Guilin Li,Wuxiong Zeng,Xiujun Shu,Sibo Wang*

Main category: cs.LG

TL;DR: ChiGAD is a spectral GNN framework designed for anomaly detection in heterogeneous information networks, addressing three key issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique challenges due to node and edge heterogeneity. Existing Graph Neural Network (GNN) methods primarily focus on homogeneous GAD and thus fail to address three key issues: Capturing abnormal signal and rich semantics across diverse meta-paths; Retaining high-frequency content in HIN dimension alignment; Learning effectively from difficult anomaly samples with class imbalance.

Method: ChiGAD consists of: 1) Multi-Graph Chi-Square Filter, which captures anomalous information via applying dedicated Chi-Square filters to each meta-path graph; 2) Interactive Meta-Graph Convolution, which aligns features while preserving high-frequency information and incorporates heterogeneous messages by a unified Chi-Square Filter; 3) Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies to address class imbalance.

Result: Extensive experiments on public and industrial datasets show that ChiGAD outperforms state-of-the-art models on multiple metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD datasets, validating the effectiveness of Chi-Square filters.

Conclusion: ChiGAD addresses key limitations in existing GAD methods for heterogeneous networks, showing superior performance in extensive experiments.

Abstract: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique
challenges due to node and edge heterogeneity. Existing Graph Neural Network
(GNN) methods primarily focus on homogeneous GAD and thus fail to address three
key issues: (C1) Capturing abnormal signal and rich semantics across diverse
meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment;
and (C3) Learning effectively from difficult anomaly samples with class
imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based
on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse
domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter,
which captures anomalous information via applying dedicated Chi-Square filters
to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns
features while preserving high-frequency information and incorporates
heterogeneous messages by a unified Chi-Square Filter; and (3)
Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies
to address class imbalance. Extensive experiments on public and industrial
datasets show that ChiGAD outperforms state-of-the-art models on multiple
metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD
datasets, validating the effectiveness of Chi-Square filters. Our code is
available at https://github.com/HsipingLi/ChiGAD.

</details>


### [239] [Exact Expressive Power of Transformers with Padding](https://arxiv.org/abs/2505.18948)
*William Merrill,Ashish Sabharwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Chain of thought is a natural inference-time method for increasing the
computational power of transformer-based large language models (LLMs), but
comes at the cost of sequential decoding. Are there more efficient alternatives
to expand a transformer's expressive power without adding parameters? We
consider transformers with padding tokens as a form of parallelizable test-time
compute. We show that averaging-hard-attention, masked-pre-norm transformers
with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of
extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was
known, proving a matching lower bound had been elusive. Further, our novel
analysis reveals the precise expanded power of padded transformers when coupled
with another form of inference-time compute, namely dynamically increasing
depth via looping. Our core technical contribution is to show how padding helps
bring the notions of complete problems and reductions, which have been a
cornerstone of classical complexity theory, to the formal study of
transformers. Armed with this new tool, we prove that padded transformers with
$O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class
$\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and
looping together systematically expand transformers' expressive power: with
polylogarithmic looping, padded transformers converge to the class
$\mathsf{NC}$, the best that could be expected without losing parallelism
(unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further
exploration of padding and looping as parallelizable alternatives to chain of
thought.

</details>


### [240] [Online Knowledge Distillation with Reward Guidance](https://arxiv.org/abs/2505.18952)
*Chen Jia*

Main category: cs.LG

TL;DR: This paper studies KD for LLMs through preference optimization and proposes a reward-guided imitation learning framework for sequential KD.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to minimize the performance gap between student and teacher policies in knowledge distillation for large language models via preference optimization.

Method: The method involves formulating a min-max optimization problem between policy and reward model (RM) with reward optimization constrained within a confidence set for preference alignment. It also explores offline and online preference-based KD, reformulates RM using Q-value function, and extends the framework to white-box KD.

Result: Theoretical analysis and empirical results show the effectiveness of the proposed framework.

Conclusion: A reward-guided imitation learning framework for sequential KD effectively minimizes the performance gap between student and teacher policies.

Abstract: This work studies knowledge distillation (KD) for large language models
(LLMs) through preference optimization. We propose a reward-guided imitation
learning framework for sequential KD, formulating a min-max optimization
problem between the policy and reward model (RM) to minimize the performance
gap between the student and teacher policies. Specifically, the reward
optimization is constrained to achieve near-optimality within a confidence set
for preference alignment. For preference data construction, we explore both
offline and online preference-based KD. Additionally, we reformulate the RM
using the $Q$-value function and extend the framework to white-box KD, where
the teacher policy's predicted probabilities are accessible. Theoretical
analysis and empirical results demonstrate the effectiveness of the proposed
framework.

</details>


### [241] [Protein Design with Dynamic Protein Vocabulary](https://arxiv.org/abs/2505.18966)
*Nuowei Liu,Jiahao Kuang,Yanting Liu,Changzhi Sun,Tao Ji,Yuanbin Wu,Man Lan*

Main category: cs.LG

TL;DR: ProDVa是一种新的蛋白质设计方法，它结合了文本编码器、蛋白质语言模型和片段编码器，以生成功能对齐且结构合理的蛋白质序列。相比现有模型，ProDVa使用极少的训练数据，却能显著提高设计出良好折叠蛋白质的比例。


<details>
  <summary>Details</summary>
Motivation: 尽管深度生成模型在基于功能的蛋白质设计方面取得了进展，但它们在结构合理性方面仍面临挑战。为了解决这个问题，研究者探索了将天然蛋白质片段融入生成模型中的可能性。

Method: ProDVa整合了三个主要组件：一个用于功能描述的文本编码器，一个用于设计蛋白质的蛋白质语言模型，以及一个根据文本功能描述动态检索蛋白质片段的片段编码器。通过这种方式，该方法旨在同时确保蛋白质的功能性和结构合理性。

Result: 实验结果表明，ProDVa能够有效设计出功能对齐且结构合理的蛋白质序列。与最先进的模型相比，ProDVa使用不到0.04%的训练数据，却使pLDDT高于70的蛋白质比例提高了7.38%，PAE低于10的蛋白质比例提高了9.6%。

Conclusion: ProDVa提供了一种新颖且有效的蛋白质设计方法，能够在保证功能性的同时提高蛋白质的结构合理性。这为未来的蛋白质设计研究提供了新的方向。

Abstract: Protein design is a fundamental challenge in biotechnology, aiming to design
novel sequences with specific functions within the vast space of possible
proteins. Recent advances in deep generative models have enabled function-based
protein design from textual descriptions, yet struggle with structural
plausibility. Inspired by classical protein design methods that leverage
natural protein structures, we explore whether incorporating fragments from
natural proteins can enhance foldability in generative models. Our empirical
results show that even random incorporation of fragments improves foldability.
Building on this insight, we introduce ProDVa, a novel protein design approach
that integrates a text encoder for functional descriptions, a protein language
model for designing proteins, and a fragment encoder to dynamically retrieve
protein fragments based on textual functional descriptions. Experimental
results demonstrate that our approach effectively designs protein sequences
that are both functionally aligned and structurally plausible. Compared to
state-of-the-art models, ProDVa achieves comparable function alignment using
less than 0.04% of the training data, while designing significantly more
well-folded proteins, with the proportion of proteins having pLDDT above 70
increasing by 7.38% and those with PAE below 10 increasing by 9.6%.

</details>


### [242] [GraSS: Scalable Influence Function with Sparse Gradient Compression](https://arxiv.org/abs/2505.18976)
*Pingbang Hu,Joseph Melkonian,Weijing Tang,Han Zhao,Jiaqi W. Ma*

Main category: cs.LG

TL;DR: The paper proposes GraSS and FactGraSS, gradient compression algorithms that leverage sparsity of per-sample gradients for efficient data attribution in large-scale models.


<details>
  <summary>Details</summary>
Motivation: Gradient-based data attribution methods are crucial but limited by high computational costs. This work aims to address these limitations by developing a more scalable algorithm.

Method: Proposed GraSS and its variant FactGraSS which use gradient compression techniques to reduce space and time complexity by exploiting the sparsity of per-sample gradients.

Result: Experiments show substantial speedups with preserved data influence fidelity. FactGraSS is up to 165% faster on billion-scale models compared to previous baselines.

Conclusion: GraSS and FactGraSS effectively improve scalability of gradient-based data attribution methods, making them viable for large-scale models.

Abstract: Gradient-based data attribution methods, such as influence functions, are
critical for understanding the impact of individual training samples without
requiring repeated model retraining. However, their scalability is often
limited by the high computational and memory costs associated with per-sample
gradient computation. In this work, we propose GraSS, a novel gradient
compression algorithm and its variants FactGraSS for linear layers
specifically, that explicitly leverage the inherent sparsity of per-sample
gradients to achieve sub-linear space and time complexity. Extensive
experiments demonstrate the effectiveness of our approach, achieving
substantial speedups while preserving data influence fidelity. In particular,
FactGraSS achieves up to 165% faster throughput on billion-scale models
compared to the previous state-of-the-art baselines. Our code is publicly
available at https://github.com/TRAIS-Lab/GraSS.

</details>


### [243] [GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization](https://arxiv.org/abs/2505.18979)
*Zixuan Chen,Hao Lin,Ke Xu,Xinghao Jiang,Tanfeng Sun*

Main category: cs.LG

TL;DR: The paper presents GhostPrompt, an automated jailbreak framework for bypassing text and image safety filters in AI systems. It combines dynamic prompt optimization with multimodal feedback, achieving significant improvements in bypass rates and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the ineffectiveness of current jailbreak methods against modern safety filters that use large language models (LLMs) for semantic-level detection.

Method: GhostPrompt framework with two components: Dynamic Optimization for generating adversarial prompts using feedback from text safety filters and CLIP similarity scores, and Adaptive Safety Indicator Injection which uses reinforcement learning to inject benign visual cues to bypass image-level filters.

Result: Increased ShieldLM-7B bypass rate from 12.5% to 99.0%, improved CLIP score from 0.2637 to 0.2762, reduced time cost by 4.2 times, and successfully bypassed filters like GPT-4.1 and DALLE 3.

Conclusion: GhostPrompt reveals vulnerabilities in current multimodal defenses and will be released under a controlled-access protocol to support further research on AI safety.

Abstract: Text-to-image (T2I) generation models can inadvertently produce
not-safe-for-work (NSFW) content, prompting the integration of text and image
safety filters. Recent advances employ large language models (LLMs) for
semantic-level detection, rendering traditional token-level perturbation
attacks largely ineffective. However, our evaluation shows that existing
jailbreak methods are ineffective against these modern filters. We introduce
GhostPrompt, the first automated jailbreak framework that combines dynamic
prompt optimization with multimodal feedback. It consists of two key
components: (i) Dynamic Optimization, an iterative process that guides a large
language model (LLM) using feedback from text safety filters and CLIP
similarity scores to generate semantically aligned adversarial prompts; and
(ii) Adaptive Safety Indicator Injection, which formulates the injection of
benign visual cues as a reinforcement learning problem to bypass image-level
filters. GhostPrompt achieves state-of-the-art performance, increasing the
ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP
score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$.
Moreover, it generalizes to unseen filters including GPT-4.1 and successfully
jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing
systemic vulnerabilities in current multimodal defenses. To support further
research on AI safety and red-teaming, we will release code and adversarial
prompts under a controlled-access protocol.

</details>


### [244] [FedSKC: Federated Learning with Non-IID Data via Structural Knowledge Collaboration](https://arxiv.org/abs/2505.18981)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Lijuan Wang,Jiahua Shi,Shiping Chen,Jun Shen*

Main category: cs.LG

TL;DR: In this paper, the authors address the data heterogeneity challenge in federated learning (FL) by proposing Federated Learning with Structural Knowledge Collaboration (FedSKC). FedSKC decomposes model divergence into local, global, and sampling drift sub-problems and uses intra-client class-wise structural knowledge to handle these drifts. The method includes local contrastive learning, global discrepancy aggregation, and global period review components.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the data heterogeneity issue in FL that negatively impacts convergence and model performance. Previous methods fail to consider the underlying class-wise structure information contained in each client.

Method: The proposed method, FedSKC, consists of three components: 1) local contrastive learning to prevent weight divergence during local training; 2) global discrepancy aggregation to address parameter deviation between the server and clients; and 3) global period review to correct for sampling drift introduced by the server randomly selecting devices.

Result: FedSKC has been theoretically analyzed under non-convex objectives and empirically validated through extensive experiments, demonstrating its superiority.

Conclusion: FedSKC effectively tackles the data heterogeneity challenge in FL by leveraging intra-client class-wise structural knowledge and offering diverse class-relevant knowledge and a fair convergent signal.

Abstract: With the advancement of edge computing, federated learning (FL) displays a
bright promise as a privacy-preserving collaborative learning paradigm.
However, one major challenge for FL is the data heterogeneity issue, which
refers to the biased labeling preferences among multiple clients, negatively
impacting convergence and model performance. Most previous FL methods attempt
to tackle the data heterogeneity issue locally or globally, neglecting
underlying class-wise structure information contained in each client. In this
paper, we first study how data heterogeneity affects the divergence of the
model and decompose it into local, global, and sampling drift sub-problems. To
explore the potential of using intra-client class-wise structural knowledge in
handling these drifts, we thus propose Federated Learning with Structural
Knowledge Collaboration (FedSKC). The key idea of FedSKC is to extract and
transfer domain preferences from inter-client data distributions, offering
diverse class-relevant knowledge and a fair convergent signal. FedSKC comprises
three components: i) local contrastive learning, to prevent weight divergence
resulting from local training; ii) global discrepancy aggregation, which
addresses the parameter deviation between the server and clients; iii) global
period review, correcting for the sampling drift introduced by the server
randomly selecting devices. We have theoretically analyzed FedSKC under
non-convex objectives and empirically validated its superiority through
extensive experimental results.

</details>


### [245] [AmorLIP: Efficient Language-Image Pretraining via Amortization](https://arxiv.org/abs/2505.18983)
*Haotian Sun,Yitong Li,Yuchen Zhuang,Niao He,Hanjun Dai,Bo Dai*

Main category: cs.LG

TL;DR: AmorLIP is an efficient CLIP pretraining framework that improves training efficiency and performance through lightweight neural networks and novel amortization objectives, outperforming standard CLIP baselines by up to 12.24%.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP methods need large batch sizes and high computational resources, while previous solutions compromise performance, take longer or have scalability issues.

Method: AmorLIP uses lightweight neural networks to reduce expensive computations in contrastive learning and introduces new amortization objectives based on spectral factorization of energy-based models to enhance stability.

Result: AmorLIP shows superior zero-shot classification and retrieval capabilities across 38 downstream tasks, consistently surpassing standard CLIP baselines with relative improvements up to 12.24%.

Conclusion: AmorLIP addresses the limitations of existing CLIP methods by improving training efficiency and performance without compromising downstream results.

Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong
zero-shot performance across diverse downstream text-image tasks. Existing CLIP
methods typically optimize a contrastive objective using negative samples drawn
from each minibatch. To achieve robust representation learning, these methods
require extremely large batch sizes and escalate computational demands to
hundreds or even thousands of GPUs. Prior approaches to mitigate this issue
often compromise downstream performance, prolong training duration, or face
scalability challenges with very large datasets. To overcome these limitations,
we propose AmorLIP, an efficient CLIP pretraining framework that amortizes
expensive computations involved in contrastive learning through lightweight
neural networks, which substantially improves training efficiency and
performance. Leveraging insights from a spectral factorization of energy-based
models, we introduce novel amortization objectives along with practical
techniques to improve training stability. Extensive experiments across 38
downstream tasks demonstrate the superior zero-shot classification and
retrieval capabilities of AmorLIP, consistently outperforming standard CLIP
baselines with substantial relative improvements of up to 12.24%.

</details>


### [246] [STRICT: Stress Test of Rendering Images Containing Text](https://arxiv.org/abs/2505.18985)
*Tianyu Zhang,Xinyu Wang,Zhenghan Tai,Lu Li,Jijun Chi,Jingrui Tian,Hailin He,Suyuchen Wang*

Main category: cs.LG

TL;DR: Diffusion models have issues generating consistent and legible text in images due to locality bias. This paper introduces STRICT, a benchmark evaluating diffusion models' ability to render coherent and instruction-aligned text in images across multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate and reveal the limitations of diffusion models in generating consistent and legible text within images, which is currently hindered by locality bias.

Method: Introduced STRICT benchmark that evaluates models across three dimensions: maximum length of readable text, correctness and legibility of generated text, and ratio of not following instructions for generating text.

Result: Evaluation of several state-of-the-art models revealed persistent limitations in long-range consistency and instruction-following capabilities.

Conclusion: The findings provide insights into architectural bottlenecks and motivate future research directions in multimodal generative modeling.

Abstract: While diffusion models have revolutionized text-to-image generation with
their ability to synthesize realistic and diverse scenes, they continue to
struggle to generate consistent and legible text within images. This
shortcoming is commonly attributed to the locality bias inherent in
diffusion-based generation, which limits their ability to model long-range
spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a
benchmark designed to systematically stress-test the ability of diffusion
models to render coherent and instruction-aligned text in images. Our benchmark
evaluates models across multiple dimensions: (1) the maximum length of readable
text that can be generated; (2) the correctness and legibility of the generated
text, and (3) the ratio of not following instructions for generating text. We
evaluate several state-of-the-art models, including proprietary and open-source
variants, and reveal persistent limitations in long-range consistency and
instruction-following capabilities. Our findings provide insights into
architectural bottlenecks and motivate future research directions in multimodal
generative modeling. We release our entire evaluation pipeline at
https://github.com/tianyu-z/STRICT-Bench.

</details>


### [247] [Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs](https://arxiv.org/abs/2505.18996)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: Hybrid neural ODEs are powerful in healthcare but suffer from inefficiency and over-fitting. This paper proposes a pipeline for state selection and structure optimization, enhancing performance, stability, and sparsity.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and over-fitting issues of hybrid neural ODEs in healthcare settings due to excessive latent states and interactions.

Method: Propose a new hybrid pipeline that combines domain-informed graph modifications with data-driven regularization to optimize mechanistic neural ODEs.

Result: Experiments on synthetic and real-world data demonstrate improved predictive performance, robustness, and desired sparsity.

Conclusion: The proposed pipeline effectively reduces hybrid models in healthcare applications while maintaining mechanistic plausibility.

Abstract: Hybrid neural ordinary differential equations (neural ODEs) integrate
mechanistic models with neural ODEs, offering strong inductive bias and
flexibility, and are particularly advantageous in data-scarce healthcare
settings. However, excessive latent states and interactions from mechanistic
models can lead to training inefficiency and over-fitting, limiting practical
effectiveness of hybrid neural ODEs. In response, we propose a new hybrid
pipeline for automatic state selection and structure optimization in
mechanistic neural ODEs, combining domain-informed graph modifications with
data-driven regularization to sparsify the model for improving predictive
performance and stability while retaining mechanistic plausibility. Experiments
on synthetic and real-world data show improved predictive performance and
robustness with desired sparsity, establishing an effective solution for hybrid
model reduction in healthcare applications.

</details>


### [248] [Semi-pessimistic Reinforcement Learning](https://arxiv.org/abs/2505.19002)
*Jin Zhu,Xin Zhou,Jiaang Yao,Gholamali Aminian,Omar Rivasplata,Simon Little,Lexin Li,Chengchun Shi*

Main category: cs.LG

TL;DR: An offline reinforcement learning method that uses semi-pessimistic RL to leverage unlabeled data and improve policy learning.


<details>
  <summary>Details</summary>
Motivation: Offline reinforcement learning faces challenges with distributional shift and limited labeled reward data, leading to suboptimal policy learning.

Method: Propose a semi-pessimistic RL method which leverages large amounts of unlabeled data by seeking a lower bound of the reward function rather than Q-function or state transition function.

Result: The method is competitive with alternative solutions as shown through analytical and numerical comparisons, and has been demonstrated in an application for adaptive deep brain stimulation for Parkinson's disease.

Conclusion: The semi-pessimistic RL method effectively simplifies the learning process, is flexible with model-free and model-based algorithms, and guarantees improvement when using unlabeled data.

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from
pre-collected data. However, it faces challenges of distributional shift, where
the learned policy may encounter unseen scenarios not covered in the offline
data. Additionally, numerous applications suffer from a scarcity of labeled
reward data. Relying on labeled data alone often leads to a narrow state-action
distribution, further amplifying the distributional shift, and resulting in
suboptimal policy learning. To address these issues, we first recognize that
the volume of unlabeled data is typically substantially larger than that of
labeled data. We then propose a semi-pessimistic RL method to effectively
leverage abundant unlabeled data. Our approach offers several advantages. It
considerably simplifies the learning process, as it seeks a lower bound of the
reward function, rather than that of the Q-function or state transition
function. It is highly flexible, and can be integrated with a range of
model-free and model-based RL algorithms. It enjoys the guaranteed improvement
when utilizing vast unlabeled data, but requires much less restrictive
conditions. We compare our method with a number of alternative solutions, both
analytically and numerically, and demonstrate its clear competitiveness. We
further illustrate with an application to adaptive deep brain stimulation for
Parkinson's disease.

</details>


### [249] [Faithful Group Shapley Value](https://arxiv.org/abs/2505.19013)
*Kiljae Lee,Ziqi Liu,Weijing Tang,Yuan Zhang*

Main category: cs.LG

TL;DR: An improved method for group-level data valuation, Faithful Group Shapley Value (FGSV), is developed to defend against shell company attacks and provide accurate approximations.


<details>
  <summary>Details</summary>
Motivation: Existing group-level extensions of Data Shapley are vulnerable to shell company attacks where strategic group splitting can unfairly inflate valuations.

Method: The authors propose Faithful Group Shapley Value (FGSV) which defends against shell company attacks. They also develop a fast and accurate approximation algorithm for computing FGSV based on original mathematical insights.

Result: Empirical experiments show that the proposed algorithm significantly outperforms state-of-the-art methods in computational efficiency and approximation accuracy while ensuring faithful group-level valuation.

Conclusion: FGSV uniquely defends against shell company attacks and provides a provably fast and accurate way for group-level data valuation.

Abstract: Data Shapley is an important tool for data valuation, which quantifies the
contribution of individual data points to machine learning models. In practice,
group-level data valuation is desirable when data providers contribute data in
batch. However, we identify that existing group-level extensions of Data
Shapley are vulnerable to shell company attacks, where strategic group
splitting can unfairly inflate valuations. We propose Faithful Group Shapley
Value (FGSV) that uniquely defends against such attacks. Building on original
mathematical insights, we develop a provably fast and accurate approximation
algorithm for computing FGSV. Empirical experiments demonstrate that our
algorithm significantly outperforms state-of-the-art methods in computational
efficiency and approximation accuracy, while ensuring faithful group-level
valuation.

</details>


### [250] [Tokenizing Electron Cloud in Protein-Ligand Interaction Learning](https://arxiv.org/abs/2505.19014)
*Haitao Lin,Odin Zhang,Jia Xu,Yunfan Liu,Zheng Cheng,Lirong Wu,Yufei Huang,Zhifeng Gao,Stan Z. Li*

Main category: cs.LG

TL;DR: ECBind is a novel method that tokenizes electron cloud signals into embeddings for better prediction of protein-molecule binding affinity and modes, showing significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning-based prediction approaches mainly focus on atom or fragment structures, while quantum chemical properties like electronic structures, which are crucial for understanding interaction patterns, are underexplored.

Method: The ECBind method tokenizes electron cloud signals into quantized embeddings by using a structure-aware transformer and hierarchical codebooks to encode 3D binding sites enriched with electron structures. These tokens are then used in specific tasks with labels. Additionally, knowledge distillation is employed to create an electron-cloud-agnostic prediction model for broader applicability.

Result: ECBind shows state-of-the-art performance across multiple tasks, improving per-structure Pearson correlation coefficient by 6.42% and Spearman correlation coefficient by 15.58%. It uncovers binding modes not fully represented by atom-level models.

Conclusion: ECBind effectively integrates quantum chemical properties into binding affinity prediction, offering superior performance and revealing new insights into protein-molecule interactions.

Abstract: The affinity and specificity of protein-molecule binding directly impact
functional outcomes, uncovering the mechanisms underlying biological regulation
and signal transduction. Most deep-learning-based prediction approaches focus
on structures of atoms or fragments. However, quantum chemical properties, such
as electronic structures, are the key to unveiling interaction patterns but
remain largely underexplored. To bridge this gap, we propose ECBind, a method
for tokenizing electron cloud signals into quantized embeddings, enabling their
integration into downstream tasks such as binding affinity prediction. By
incorporating electron densities, ECBind helps uncover binding modes that
cannot be fully represented by atom-level models. Specifically, to remove the
redundancy inherent in electron cloud signals, a structure-aware transformer
and hierarchical codebooks encode 3D binding sites enriched with electron
structures into tokens. These tokenized codes are then used for specific tasks
with labels. To extend its applicability to a wider range of scenarios, we
utilize knowledge distillation to develop an electron-cloud-agnostic prediction
model. Experimentally, ECBind demonstrates state-of-the-art performance across
multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure
Pearson and Spearman correlation coefficients, respectively.

</details>


### [251] [Querying Kernel Methods Suffices for Reconstructing their Training Data](https://arxiv.org/abs/2505.19019)
*Daniel Barzilai,Yuval Margalit,Eitan Gronich,Gilad Yehudai,Meirav Galun,Ronen Basri*

Main category: cs.LG

TL;DR: 过参数化模型即使在实现强大泛化能力的情况下，也可能记住训练数据，这存在隐私隐患。本文通过实验和理论证明，对于一系列核方法模型，仅通过查询模型输出即可重构其训练数据。


<details>
  <summary>Details</summary>
Motivation: 研究过参数化模型在具备良好泛化性能时是否会因记住训练数据而带来隐私问题，特别是在无法访问模型参数、只能获取模型输出的情况下。

Method: 以核方法为研究对象，包括核回归、支持向量机和核密度估计等模型，通过在不同点查询这些模型的输出，来尝试重构它们的训练数据，并结合实验和理论分析进行验证。

Result: 实验证明，仅通过查询模型输出，无需访问模型参数，就可以成功重构核方法模型的训练数据。该结果适用于多种核方法模型。

Conclusion: 本研究表明，核方法模型存在通过输出泄露训练数据的风险，从而引发隐私问题，这一发现有助于揭示此类模型潜在的隐私隐患。

Abstract: Over-parameterized models have raised concerns about their potential to
memorize training data, even when achieving strong generalization. The privacy
implications of such memorization are generally unclear, particularly in
scenarios where only model outputs are accessible. We study this question in
the context of kernel methods, and demonstrate both empirically and
theoretically that querying kernel models at various points suffices to
reconstruct their training data, even without access to model parameters. Our
results hold for a range of kernel methods, including kernel regression,
support vector machines, and kernel density estimation. Our hope is that this
work can illuminate potential privacy concerns for such models.

</details>


### [252] [Learn Beneficial Noise as Graph Augmentation](https://arxiv.org/abs/2505.19024)
*Siqi Huang,Yanchen Xu,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: The paper proposes PiNGDA, a new method for graph contrastive learning that uses positive-incentive noise to generate more effective and stable graph augmentations.


<details>
  <summary>Details</summary>
Motivation: Graph contrastive learning often relies on heuristic augmentations like random edge dropping which can disrupt important graph structures leading to unstable performance.

Method: PiNGDA introduces positive-incentive noise (pi-noise) to analyze the beneficial effect of noise under information theory. A Gaussian auxiliary variable is designed to convert the loss function into information entropy, proving standard GCL with pre-defined augmentations estimates beneficial noise via point estimation. PiNGDA learns this beneficial noise through a trainable noise generator for graph augmentations impacting both topology and attributes.

Result: Extensive experiments show PiNGDA's effectiveness and stability in graph contrastive learning.

Conclusion: PiNGDA offers a more reliable approach to graph augmentations compared to existing methods.

Abstract: Although graph contrastive learning (GCL) has been widely investigated, it is
still a challenge to generate effective and stable graph augmentations.
Existing methods often apply heuristic augmentation like random edge dropping,
which may disrupt important graph structures and result in unstable GCL
performance. In this paper, we propose Positive-incentive Noise driven Graph
Data Augmentation (PiNGDA), where positive-incentive noise (pi-noise)
scientifically analyzes the beneficial effect of noise under the information
theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian
auxiliary variable to convert the loss function to information entropy. We
prove that the standard GCL with pre-defined augmentations is equivalent to
estimate the beneficial noise via the point estimation. Following our analysis,
PiNGDA is derived from learning the beneficial noise on both topology and
attributes through a trainable noise generator for graph augmentations, instead
of the simple estimation. Since the generator learns how to produce beneficial
perturbations on graph topology and node attributes, PiNGDA is more reliable
compared with the existing methods. Extensive experimental results validate the
effectiveness and stability of PiNGDA.

</details>


### [253] [Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias](https://arxiv.org/abs/2505.19038)
*Hao Wu,Yuan Gao,Ruiqi Shu,Zean Han,Fan Xu,Zhihong Zhu,Qingsong Wen,Xian Wu,Kun Wang,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了一种新的湍流预测方法Turb-L1，解决了现有深度学习方法在长期预测中因频谱偏差导致的过度平滑问题。通过多网格架构和分层动力学合成机制，Turb-L1显著提高了预测精度和物理保真度。


<details>
  <summary>Details</summary>
Motivation: 准确预测湍流的长期演化对于科学研究和工程应用至关重要，但现有的深度学习方法在长期自回归预测方面存在瓶颈，表现为过度平滑且无法精确追踪复杂的流体动力学。频谱偏差是这一问题的核心障碍。

Method: 提出了Turb-L1，一种创新的湍流预测方法。该方法利用多网格架构中的分层动力学合成机制，显式克服频谱偏差，准确捕捉跨尺度相互作用并保留高频动态的保真度。

Result: 在2D湍流基准测试中，Turb-L1表现出优异性能：(I) 长期预测中，与现有最佳方法相比，MSE降低80.3%，SSIM提高9倍以上；(II) 成功克服频谱偏差，准确再现全涡度谱，并在高波数区域保持物理真实性。

Conclusion: Turb-L1通过有效解决频谱偏差问题，实现了对湍流演化的可靠长期跟踪，显著提升了预测精度和物理保真度。

Abstract: Accurately predicting the long-term evolution of turbulence is crucial for
advancing scientific understanding and optimizing engineering applications.
However, existing deep learning methods face significant bottlenecks in
long-term autoregressive prediction, which exhibit excessive smoothing and fail
to accurately track complex fluid dynamics. Our extensive experimental and
spectral analysis of prevailing methods provides an interpretable explanation
for this shortcoming, identifying Spectral Bias as the core obstacle.
Concretely, spectral bias is the inherent tendency of models to favor
low-frequency, smooth features while overlooking critical high-frequency
details during training, thus reducing fidelity and causing physical
distortions in long-term predictions. Building on this insight, we propose
Turb-L1, an innovative turbulence prediction method, which utilizes a
Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to
explicitly overcome spectral bias. It accurately captures cross-scale
interactions and preserves the fidelity of high-frequency dynamics, enabling
reliable long-term tracking of turbulence evolution. Extensive experiments on
the 2D turbulence benchmark show that Turb-L1 demonstrates excellent
performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE)
by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$
compared to the SOTA baseline, significantly improving prediction fidelity.
(II) It effectively overcomes spectral bias, accurately reproducing the full
enstrophy spectrum and maintaining physical realism in high-wavenumber regions,
thus avoiding the spectral distortions or spurious energy accumulation seen in
other methods.

</details>


### [254] [Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments](https://arxiv.org/abs/2505.19043)
*Jingyuan Liu,Zeyu Zhang,Xuchuang Wang,Xutong Liu,John C. S. Lui,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: The paper addresses the offline clustering of bandits (Off-ClusBand) problem, proposing two algorithms Off-C$^2$LUB and Off-CLUB to utilize offline datasets for learning cluster properties and improving decision-making in contextual linear multi-armed bandits.


<details>
  <summary>Details</summary>
Motivation: Existing works on contextual linear multi-armed bandits mainly focus on online settings, which may not fully utilize the offline data available in many applications. This motivates the study of how offline datasets can be used to learn cluster properties and enhance decision-making across multiple users.

Method: Two algorithms are proposed: Off-C$^2$LUB and Off-CLUB. Off-C$^2$LUB performs well with arbitrary amounts of user data, while Off-CLUB is prone to bias with limited data but matches a theoretical lower bound given sufficient data.

Result: Theoretical analysis shows that Off-C$^2$LUB performs well regardless of data amount, and Off-CLUB achieves the theoretical lower bound when data is sufficient. Experimental results on real and synthetic datasets validate these findings.

Conclusion: The study successfully addresses the challenge of data insufficiency in offline clustering of bandits by proposing effective algorithms, demonstrating their performance through theoretical analysis and experiments.

Abstract: Contextual linear multi-armed bandits are a learning framework for making a
sequence of decisions, e.g., advertising recommendations for a sequence of
arriving users. Recent works have shown that clustering these users based on
the similarity of their learned preferences can significantly accelerate the
learning. However, prior work has primarily focused on the online setting,
which requires continually collecting user data, ignoring the offline data
widely available in many applications. To tackle these limitations, we study
the offline clustering of bandits (Off-ClusBand) problem, which studies how to
use the offline dataset to learn cluster properties and improve decision-making
across multiple users. The key challenge in Off-ClusBand arises from data
insufficiency for users: unlike the online case, in the offline case, we have a
fixed, limited dataset to work from and thus must determine whether we have
enough data to confidently cluster users together. To address this challenge,
we propose two algorithms: Off-C$^2$LUB, which we analytically show performs
well for arbitrary amounts of user data, and Off-CLUB, which is prone to bias
when data is limited but, given sufficient data, matches a theoretical lower
bound that we derive for the offline clustered MAB problem. We experimentally
validate these results on both real and synthetic datasets.

</details>


### [255] [Structured Reinforcement Learning for Combinatorial Decision-Making](https://arxiv.org/abs/2505.19053)
*Heiko Hoppe,Léo Baty,Louis Bouvier,Axel Parmentier,Maximilian Schiffer*

Main category: cs.LG

TL;DR: The paper introduces Structured Reinforcement Learning (SRL), a novel actor-critic framework embedding combinatorial optimization layers into the actor neural network. SRL outperforms unstructured RL and imitation learning on dynamic problems, with better stability and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning is increasingly applied to real-world problems involving complex and structured decisions, but standard RL algorithms struggle to scale, generalize, and exploit structure in the presence of combinatorial action spaces.

Method: The authors propose Structured Reinforcement Learning (SRL), a novel actor-critic framework that embeds combinatorial optimization layers into the actor neural network. End-to-end learning of the actor is enabled via Fenchel-Young losses, and a geometric interpretation of SRL as a primal-dual algorithm in the dual of the moment polytope is provided.

Result: Across six environments with exogenous and endogenous uncertainty, SRL matches or surpasses the performance of unstructured RL and imitation learning on static tasks and improves over these baselines by up to 92% on dynamic problems, with improved stability and convergence speed.

Conclusion: Structured Reinforcement Learning (SRL) effectively addresses the challenges faced by standard RL algorithms in settings with combinatorial action spaces, offering superior performance, stability, and convergence speed on dynamic problems.

Abstract: Reinforcement learning (RL) is increasingly applied to real-world problems
involving complex and structured decisions, such as routing, scheduling, and
assortment planning. These settings challenge standard RL algorithms, which
struggle to scale, generalize, and exploit structure in the presence of
combinatorial action spaces. We propose Structured Reinforcement Learning
(SRL), a novel actor-critic framework that embeds combinatorial optimization
layers into the actor neural network. We enable end-to-end learning of the
actor via Fenchel-Young losses and provide a geometric interpretation of SRL as
a primal-dual algorithm in the dual of the moment polytope. Across six
environments with exogenous and endogenous uncertainty, SRL matches or
surpasses the performance of unstructured RL and imitation learning on static
tasks and improves over these baselines by up to 92% on dynamic problems, with
improved stability and convergence speed.

</details>


### [256] [Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning](https://arxiv.org/abs/2505.19054)
*Zhuochen Liu,Rahul Jain,Quan Nguyen*

Main category: cs.LG

TL;DR: The paper introduces an actor-critic algorithm using randomized neural networks to reduce computational costs while maintaining strong performance in control tasks, notably achieving shorter wall-clock training time.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in reinforcement learning have achieved state-of-the-art performance but often require significant computational resources. This motivates the need for a more efficient algorithm that can maintain performance while reducing computational costs.

Method: An actor-critic algorithm is introduced which utilizes randomized neural networks. This method aims to drastically reduce computational costs while still effectively solving a range of control problems.

Result: The algorithm successfully solves various control problems, including the locomotion control of a 12-motor quadruped robot, achieving results comparable to leading algorithms like PPO. It performs better in terms of wall-clock training time rather than sample efficiency.

Conclusion: The proposed actor-critic algorithm with randomized neural networks reduces computational costs and achieves competitive performance in control tasks, particularly excelling in reducing actual training time.

Abstract: Recent advancements in reinforcement learning (RL) have leveraged neural
networks to achieve state-of-the-art performance across various control tasks.
However, these successes often come at the cost of significant computational
resources, as training deep neural networks requires substantial time and data.
In this paper, we introduce an actor-critic algorithm that utilizes randomized
neural networks to drastically reduce computational costs while maintaining
strong performance. Despite its simple architecture, our method effectively
solves a range of control problems, including the locomotion control of a
highly dynamic 12-motor quadruped robot, and achieves results comparable to
leading algorithms such as Proximal Policy Optimization (PPO). Notably, our
approach does not outperform other algorithms in terms of sample efficnency but
rather in terms of wall-clock training time. That is, although our algorithm
requires more timesteps to converge to an optimal policy, the actual time
required for training turns out to be lower.

</details>


### [257] [Distributionally Robust Deep Q-Learning](https://arxiv.org/abs/2505.19058)
*Chung I Lu,Julian Sester,Aijia Zhang*

Main category: cs.LG

TL;DR: The paper proposes a new distributionally robust Q-learning algorithm for non-tabular cases with continuous state spaces under model uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning algorithms do not adequately account for model uncertainty in the state transitions of Markov decision processes, particularly in non-tabular cases with continuous state spaces.

Method: A worst-case transition is considered from a ball around a reference probability measure. The optimal policy under this worst-case scenario is determined by solving the non-linear Bellman equation. This is achieved by dualising and regularising the Bellman operator with the Sinkhorn distance, which is then parameterized using deep neural networks.

Result: The approach successfully modifies the Deep Q-Network algorithm to optimise for the worst-case state transition. Its effectiveness and feasibility are demonstrated through various applications, including a portfolio optimisation task based on S&P 500 data.

Conclusion: The novel distributionally robust Q-learning algorithm provides an effective method for handling model uncertainty in continuous state spaces, as evidenced by its successful application in several test cases, including financial portfolio optimisation.

Abstract: We propose a novel distributionally robust $Q$-learning algorithm for the
non-tabular case accounting for continuous state spaces where the state
transition of the underlying Markov decision process is subject to model
uncertainty. The uncertainty is taken into account by considering the
worst-case transition from a ball around a reference probability measure. To
determine the optimal policy under the worst-case state transition, we solve
the associated non-linear Bellman equation by dualising and regularising the
Bellman operator with the Sinkhorn distance, which is then parameterized with
deep neural networks. This approach allows us to modify the Deep Q-Network
algorithm to optimise for the worst case state transition.
  We illustrate the tractability and effectiveness of our approach through
several applications, including a portfolio optimisation task based on
S\&{P}~500 data.

</details>


### [258] [Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management](https://arxiv.org/abs/2505.19061)
*Chen Avin,Zvi Lotker,Shie Mannor,Gil Shabat,Hanan Shteingart,Roey Yadgar*

Main category: cs.LG

TL;DR: This paper proposes ABoB, a hierarchical Adversarial Bandit over Bandits algorithm for nonstochastic multi-armed bandit problems in metric action spaces with oblivious Lipschitz adversaries. It achieves lower regret and faster convergence than the flat method.


<details>
  <summary>Details</summary>
Motivation: The motivation is dynamic parameter optimization in finite, but large action (configurations) spaces.

Method: ABoB clusters similar configurations to exploit local structures and adapt to changing environments. It uses state-of-the-art existing "flat" algorithms.

Result: Under favorable conditions, the regret bound can be improved to $O\left(k^{\frac{1}{4}}T^{\frac{1}{2}}\right)$. In simulations and experiments on a real storage system, ABoB achieves up to 50% improvement in known previous setups, nonstochastic and stochastic, as well as in their settings.

Conclusion: ABoB guarantees a standard worst-case regret bound of $O\left(k^{\frac{1}{2}}T^{\frac{1}{2}}\right)$, matching the traditional flat approach.

Abstract: Motivated by dynamic parameter optimization in finite, but large action
(configurations) spaces, this work studies the nonstochastic multi-armed bandit
(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We
propose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can
use state-of-the-art existing "flat" algorithms, but additionally clusters
similar configurations to exploit local structures and adapt to changing
environments. We prove that in the worst-case scenario, such clustering
approach cannot hurt too much and ABoB guarantees a standard worst-case regret
bound of $O\left(k^{\frac{1}{2}}T^{\frac{1}{2}}\right)$, where $T$ is the
number of rounds and $k$ is the number of arms, matching the traditional flat
approach. However, under favorable conditions related to the algorithm
properties, clusters properties, and certain Lipschitz conditions, the regret
bound can be improved to $O\left(k^{\frac{1}{4}}T^{\frac{1}{2}}\right)$.
Simulations and experiments on a real storage system demonstrate that ABoB,
using standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and
faster convergence than the flat method, up to 50% improvement in known
previous setups, nonstochastic and stochastic, as well as in our settings.

</details>


### [259] [Recalibrating binary probabilistic classifiers](https://arxiv.org/abs/2505.19068)
*Dirk Tasche*

Main category: cs.LG

TL;DR: The paper discusses the recalibration of binary probabilistic classifiers to a target prior probability, proposing two new methods CSPD and QMM which are tested for their effectiveness.


<details>
  <summary>Details</summary>
Motivation: Recalibration of binary probabilistic classifiers is crucial in areas such as credit risk management. There is a need for meaningful recalibration methods based on distribution shift assumptions linked to AUC.

Method: Two new methods for recalibration are proposed - parametric covariate shift with posterior drift (CSPD) and ROC-based quasi moment matching (QMM). These are analyzed from a distribution shift perspective.

Result: The test outcomes indicate that QMM methods can provide appropriately conservative results when evaluated with concave functionals like risk weight functions for credit risk.

Conclusion: QMM methods are effective for recalibrating binary probabilistic classifiers in contexts requiring conservative evaluations, such as credit risk management.

Abstract: Recalibration of binary probabilistic classifiers to a target prior
probability is an important task in areas like credit risk management. We
analyse methods for recalibration from a distribution shift perspective.
Distribution shift assumptions linked to the area under the curve (AUC) of a
probabilistic classifier are found to be useful for the design of meaningful
recalibration methods. Two new methods called parametric covariate shift with
posterior drift (CSPD) and ROC-based quasi moment matching (QMM) are proposed
and tested together with some other methods in an example setting. The outcomes
of the test suggest that the QMM methods discussed in the paper can provide
appropriately conservative results in evaluations with concave functionals like
for instance risk weights functions for credit risk.

</details>


### [260] [Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes](https://arxiv.org/abs/2505.19087)
*Itamar Harel,Yonathan Wolanowsky,Gal Vardi,Nathan Srebro,Daniel Soudry*

Main category: cs.LG

TL;DR: The paper analyzes the generalization gap for over-parametrized models trained with Markovian stochastic algorithms, focusing on Langevin dynamics. It provides a bound for the generalization gap that depends only on the sample size and initial distribution, without considering training time or model properties.


<details>
  <summary>Details</summary>
Motivation: To understand the generalization gap in potentially over-parametrized models using Markovian stochastic training algorithms, especially Langevin dynamics, which introduces Gaussian noise to gradient descent.

Method: Bounding the generalization gap by analyzing any Markov process-based training with a Gibbs-style stationary distribution, observing that the marginal distribution divergence from initialization remains bounded.

Result: The generalization gap is bounded by √((βE L (θ_0) + log(1/δ))/N) with probability 1-δ over the dataset, where N is the sample size and E L (θ_0) = O(1) with standard initialization scaling.

Conclusion: The analysis shows no dependence on training time, mixing, dimensionality, gradient norms, or other loss/model properties, providing a simpler understanding of generalization in over-parametrized models.

Abstract: We analyze the generalization gap (gap between the training and test errors)
when training a potentially over-parametrized model using a Markovian
stochastic training algorithm, initialized from some distribution $\theta_0
\sim p_0$. We focus on Langevin dynamics with a positive temperature
$\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal
step size, perturbed with $\beta^{-1}$-variances Gaussian noise, and lightly
regularized or bounded. There, we bound the generalization gap, at any time
during training, by $\sqrt{(\beta\mathbb{E} L (\theta_0) + \log(1/\delta))/N}$
with probability $1-\delta$ over the dataset, where $N$ is the sample size, and
$\mathbb{E} L (\theta_0) =O(1)$ with standard initialization scaling. In
contrast to previous guarantees, we have no dependence on either training time
or reliance on mixing, nor a dependence on dimensionality, gradient norms, or
any other properties of the loss or model. This guarantee follows from a
general analysis of any Markov process-based training that has a Gibbs-style
stationary distribution. The proof is surprisingly simple, once we observe that
the marginal distribution divergence from initialization remains bounded, as
implied by a generalized second law of thermodynamics.

</details>


### [261] [CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations](https://arxiv.org/abs/2505.19090)
*Haotian Si,Changhua Pei,Jianhui Li,Dan Pei,Gaogang Xie*

Main category: cs.LG

TL;DR: CMoS is a super-lightweight time series forecasting model that outperforms state-of-the-art models while using only 1% of DLinear's parameters, offering interpretable weights for better insights.


<details>
  <summary>Details</summary>
Motivation: To explore the simplicity of time series forecasting tasks and develop a more efficient model with minimal parameters while maintaining or improving performance compared to existing models.

Method: The model CMoS directly models spatial correlations between different time series chunks instead of learning shape embeddings. It uses Correlation Mixing to capture diverse spatial correlations with few parameters and an optional Periodicity Injection for faster convergence.

Result: CMoS outperforms existing state-of-the-art models across multiple datasets while using as little as 1% of DLinear's parameter count. The model also provides interpretable learned weights.

Conclusion: CMoS represents a significant advancement in lightweight time series forecasting, offering superior performance with minimal parameters and valuable interpretability.

Abstract: Recent advances in lightweight time series forecasting models suggest the
inherent simplicity of time series forecasting tasks. In this paper, we present
CMoS, a super-lightweight time series forecasting model. Instead of learning
the embedding of the shapes, CMoS directly models the spatial correlations
between different time series chunks. Additionally, we introduce a Correlation
Mixing technique that enables the model to capture diverse spatial correlations
with minimal parameters, and an optional Periodicity Injection technique to
ensure faster convergence. Despite utilizing as low as 1% of the lightweight
model DLinear's parameters count, experimental results demonstrate that CMoS
outperforms existing state-of-the-art models across multiple datasets.
Furthermore, the learned weights of CMoS exhibit great interpretability,
providing practitioners with valuable insights into temporal structures within
specific application scenarios.

</details>


### [262] [Towards Robust Influence Functions with Flat Validation Minima](https://arxiv.org/abs/2505.19097)
*Xichen Ye,Yifan Wu,Weizhong Zhang,Cheng Jin,Yifan Chen*

Main category: cs.LG

TL;DR: The paper highlights the problem of Influence Function (IF) reliability in deep neural networks, especially with noisy data, tracing it to loss change estimation issues rather than parameter change inaccuracies. It establishes a theoretical link between influence estimation error and validation risk sharpness, advocating for flat validation minima. A new IF estimation method tailored for such minima is introduced, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: There is a need for reliable influence estimates in deep neural networks, particularly when dealing with noisy training data. Current IF methods fall short due to deficiencies in loss change estimation related to the sharpness of validation risk.

Method: The authors establish a theoretical connection between influence estimation error, validation set risk, and its sharpness. They propose a novel form of Influence Function designed specifically for scenarios involving flat validation minima.

Result: Experimental results across various tasks demonstrate the superiority of the proposed approach over existing methods.

Conclusion: Flat validation minima are crucial for accurate influence estimation in deep neural networks. The newly introduced Influence Function form tailored for such minima outperforms traditional methods.

Abstract: The Influence Function (IF) is a widely used technique for assessing the
impact of individual training samples on model predictions. However, existing
IF methods often fail to provide reliable influence estimates in deep neural
networks, particularly when applied to noisy training data. This issue does not
stem from inaccuracies in parameter change estimation, which has been the
primary focus of prior research, but rather from deficiencies in loss change
estimation, specifically due to the sharpness of validation risk. In this work,
we establish a theoretical connection between influence estimation error,
validation set risk, and its sharpness, underscoring the importance of flat
validation minima for accurate influence estimation. Furthermore, we introduce
a novel estimation form of Influence Function specifically designed for flat
validation minima. Experimental results across various tasks validate the
superiority of our approach.

</details>


### [263] [Latent Mamba Operator for Partial Differential Equations](https://arxiv.org/abs/2505.19105)
*Karn Tiwari,Niladri Dutta,N M Anoop Krishnan,Prathosh A P*

Main category: cs.LG

TL;DR: The paper presents Latent Mamba Operator (LaMO), a new method for solving PDEs that combines state-space models and kernel integral formulations, achieving SOTA performance with significant improvements over existing baselines.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators for solving PDEs have issues with scalability in high-dimensional spaces, computational costs, and capturing long-range dependencies.

Method: LaMO integrates the efficiency of state-space models in latent space with the expressive power of kernel integral formulations in neural operators, establishing a theoretical connection between these two concepts.

Result: LaMO achieves consistent SOTA performance across diverse PDE benchmarks, with a 32.3% improvement over existing baselines in solution operator approximation.

Conclusion: LaMO is effective in modeling complex PDE solutions, offering improvements in scalability, computational efficiency, and capturing long-range dependencies.

Abstract: Neural operators have emerged as powerful data-driven frameworks for solving
Partial Differential Equations (PDEs), offering significant speedups over
numerical methods. However, existing neural operators struggle with scalability
in high-dimensional spaces, incur high computational costs, and face challenges
in capturing continuous and long-range dependencies in PDE dynamics. To address
these limitations, we introduce the Latent Mamba Operator (LaMO), which
integrates the efficiency of state-space models (SSMs) in latent space with the
expressive power of kernel integral formulations in neural operators. We also
establish a theoretical connection between state-space models (SSMs) and the
kernel integral of neural operators. Extensive experiments across diverse PDE
benchmarks on regular grids, structured meshes, and point clouds covering solid
and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA)
performance, with a 32.3\% improvement over existing baselines in solution
operator approximation, highlighting its efficacy in modeling complex PDE
solutions.

</details>


### [264] [Optimization-Inspired Few-Shot Adaptation for Large Language Models](https://arxiv.org/abs/2505.19107)
*Boyan Gao,Xin Wang,Yibo Yang,David Clifton*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) show great performance in real-world applications, but adapting them to new tasks via fine-tuning usually needs a lot of training data and computational resources which are not practical in few-shot scenarios. Existing methods have limitations. This work proposes Optimization-Inspired Few-Shot Adaptation (OFA), which integrates a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency.


<details>
  <summary>Details</summary>
Motivation: Adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Current approaches face key limitations.

Method: The forward pass of LLMs is reinterpreted as an optimization process, and OFA is proposed, integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound.

Result: OFA overcomes the issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments.

Conclusion: OFA shows promising results in few-shot adaptation tasks and can be a better alternative for adapting LLMs to novel tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in
real-world applications. However, adapting LLMs to novel tasks via fine-tuning
often requires substantial training data and computational resources that are
impractical in few-shot scenarios. Existing approaches, such as in-context
learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations:
in-context learning introduces additional inference computational overhead with
limited performance gains, while PEFT models are prone to overfitting on the
few demonstration examples. In this work, we reinterpret the forward pass of
LLMs as an optimization process, a sequence of preconditioned gradient descent
steps refining internal representations. Based on this connection, we propose
Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization
that learns preconditioners without introducing additional trainable
parameters, and an objective that improves optimization efficiency by learning
preconditioners based on a convergence bound, while simultaneously steering the
optimization path toward the flat local minimum. Our method overcomes both
issues of ICL-based and PEFT-based methods, and demonstrates superior
performance over the existing methods on a variety of few-shot adaptation tasks
in experiments.

</details>


### [265] [FP4 All the Way: Fully Quantized Training of LLMs](https://arxiv.org/abs/2505.19115)
*Brian Chmiel,Maxim Fishman,Ron Banner,Daniel Soudry*

Main category: cs.LG

TL;DR: 研究人员首次实现了主要使用4位浮点数（FP4）精度的大型语言模型（LLMs）的完全量化训练（FQT），在多达200亿个令牌的数据集上进行了实验。他们深入研究了FP4的关键设计选择，包括块大小、缩放格式和舍入方法，并确定了最佳格式（NVFP4）。通过使用随机舍入进行反向传播和更新，以及最近邻舍入进行前向传播，成功训练了一个70亿参数的模型，其性能与标准BF16基线相当。这表明FP4训练是一种实用且高效的LLM大规模训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型训练通常需要高精度计算，消耗大量计算资源和能源。为了降低训练成本并提高效率，探索低精度（如4位浮点数）的量化训练技术成为重要方向。

Method: 1. 研究FP4格式的关键设计选择：块大小、缩放格式和舍入方法。
2. 采用NVFP4格式，其中每个16个FP4值的块共享一个E4M3表示的比例因子。
3. 在反向传播和更新过程中使用随机舍入，在前向传播中使用最近邻舍入以增强稳定性。
4. 确定量化训练的有效阈值：当梯度范数低于约√3倍的量化噪声时，量化训练效果下降。
5. 使用Intel Gaudi2加速器训练70亿参数模型，并验证其下游任务性能。

Result: 成功训练了一个70亿参数的FP4量化模型，其下游任务性能与标准BF16基线相当。证明了FP4训练在大规模LLM训练中的可行性与高效性。

Conclusion: FP4训练是一种实际可行且高效的方法，可以显著降低大规模语言模型训练的成本和资源需求，为未来的大规模模型训练提供了一种新的可能性。

Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large
language models (LLMs) using predominantly 4-bit floating-point (FP4) precision
for weights, activations, and gradients on datasets up to 200 billion tokens.
We extensively investigate key design choices for FP4, including block sizes,
scaling formats, and rounding methods. Our analysis shows that the NVFP4
format, where each block of 16 FP4 values (E2M1) shares a scale represented in
E4M3, provides optimal results. We use stochastic rounding for backward and
update passes and round-to-nearest for the forward pass to enhance stability.
Additionally, we identify a theoretical and empirical threshold for effective
quantized training: when the gradient norm falls below approximately $\sqrt{3}$
times the quantization noise, quantized training becomes less effective.
Leveraging these insights, we successfully train a 7-billion-parameter model on
256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves
downstream task performance comparable to a standard BF16 baseline, confirming
that FP4 training is a practical and highly efficient approach for large-scale
LLM training. A reference implementation is supplied in
https://github.com/Anonymous1252022/fp4-all-the-way .

</details>


### [266] [Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization](https://arxiv.org/abs/2505.19133)
*Yan Xia,Hao Feng,Hongwei Sun,Junjie Wang,Qicong Hu*

Main category: cs.LG

TL;DR: This paper proposes Regularization-optimized Low-Rank Factorization with a PID controller for adaptive adjustment of regularization coefficient in power load data missing value recovery, demonstrating superior imputation accuracy and training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank factorization models have limitations in generalization capability or slow convergence due to the sensitivity to manually tuned regularization parameters.

Method: Introduce a Proportional-Integral-Derivative controller into low-rank factorization model for adaptively adjusting the regularization coefficient, preserving computational efficiency while enhancing adaptivity.

Result: Experimental results on real-world power load datasets show that the proposed method outperforms existing baselines in both imputation accuracy and training efficiency.

Conclusion: The Regularization-optimized Low-Rank Factorization with adaptive adjustment mechanism achieves better performance and is more adaptable in practical scenarios.

Abstract: Low-rank representation learning has emerged as a powerful tool for
recovering missing values in power load data due to its ability to exploit the
inherent low-dimensional structures of spatiotemporal measurements. Among
various techniques, low-rank factorization models are favoured for their
efficiency and interpretability. However, their performance is highly sensitive
to the choice of regularization parameters, which are typically fixed or
manually tuned, resulting in limited generalization capability or slow
convergence in practical scenarios. In this paper, we propose a
Regularization-optimized Low-Rank Factorization, which introduces a
Proportional-Integral-Derivative controller to adaptively adjust the
regularization coefficient. Furthermore, we provide a detailed algorithmic
complexity analysis, showing that our method preserves the computational
efficiency of stochastic gradient descent while improving adaptivity.
Experimental results on real-world power load datasets validate the superiority
of our method in both imputation accuracy and training efficiency compared to
existing baselines.

</details>


### [267] [ADGSyn: Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction](https://arxiv.org/abs/2505.19144)
*Yuxuan Nie,Yutong Song,Hong Peng*

Main category: cs.LG

TL;DR: In this paper, researchers developed a new computational method called ADGSyn for predicting drug synergy in cancer therapy. It features shared projection matrices, AMP-optimized graph operations, and residual pathways stabilized by LayerNorm. Evaluated on the O'Neil dataset, ADGSyn shows superior performance over eight baseline methods and supports full-batch processing of up to 256 molecular graphs on a single GPU.


<details>
  <summary>Details</summary>
Motivation: Drug combinations significantly enhance treatment efficacy and overcome drug resistance in cancer therapy, but the combinatorial space grows exponentially, making experimental screening impractical. Thus, there is a need for efficient computational methods to predict promising drug combinations.

Method: The proposed method, ADGSyn, includes shared projection matrices combined with attention mechanisms for cross-drug feature alignment, automatic mixed precision (AMP)-optimized graph operations that reduce memory consumption by 40% while accelerating training speed threefold, and residual pathways stabilized by LayerNorm for stable gradient propagation during training.

Result: ADGSyn demonstrates superior performance over eight baseline methods when evaluated on the O'Neil dataset containing 13,243 drug-cell line combinations. The framework supports full-batch processing of up to 256 molecular graphs on a single GPU.

Conclusion: ADGSyn sets a new standard for efficiency in drug synergy prediction within the field of computational oncology.

Abstract: Drug combinations play a critical role in cancer therapy by significantly
enhancing treatment efficacy and overcoming drug resistance. However, the
combinatorial space of possible drug pairs grows exponentially, making
experimental screening highly impractical. Therefore, developing efficient
computational methods to predict promising drug combinations and guide
experimental validation is of paramount importance. In this work, we propose
ADGSyn, an innovative method for predicting drug synergy. The key components of
our approach include: (1) shared projection matrices combined with attention
mechanisms to enable cross-drug feature alignment; (2) automatic mixed
precision (AMP)-optimized graph operations that reduce memory consumption by
40\% while accelerating training speed threefold; and (3) residual pathways
stabilized by LayerNorm to ensure stable gradient propagation during training.
Evaluated on the O'Neil dataset containing 13,243 drug--cell line combinations,
ADGSyn demonstrates superior performance over eight baseline methods. Moreover,
the framework supports full-batch processing of up to 256 molecular graphs on a
single GPU, setting a new standard for efficiency in drug synergy prediction
within the field of computational oncology.

</details>


### [268] [Computational Inertia as a Conserved Quantity in Frictionless and Damped Learning Dynamics](https://arxiv.org/abs/2505.19171)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: In this paper, researchers discover a conserved quantity in continuous-time optimization dynamics called computational inertia, which is the sum of kinetic energy and potential energy. It remains constant under idealized frictionless training and decays predictably with damping or stochastic perturbations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to identify and understand an invariant quantity during the optimization process in machine learning models, specifically in continuous-time optimization dynamics. This could provide insights into the learning trajectories and offer theoretical tools for analyzing convergence, stability, and the geometry of training.

Method: The method involves defining computational inertia as the sum of kinetic energy (parameter velocity) and potential energy (loss). The conservation law of this quantity is formalized, and its analytic decay is derived under conditions of damping and stochastic perturbations. Experiments are conducted on a synthetic system to demonstrate its behavior.

Result: The result shows that computational inertia remains invariant under idealized, frictionless training and follows predictable analytic decay patterns when subjected to damping or stochastic perturbations. This provides a new perspective for interpreting learning trajectories in optimization processes.

Conclusion: The conclusion drawn from this study is that computational inertia serves as an important invariant in continuous-time optimization dynamics. It offers a compact lens for understanding learning trajectories and may inform the development of theoretical tools for analyzing various aspects of model training such as convergence, stability, and training geometry.

Abstract: We identify a conserved quantity in continuous-time optimization dynamics,
termed computational inertia. Defined as the sum of kinetic energy (parameter
velocity) and potential energy (loss), this scalar remains invariant under
idealized, frictionless training. We formalize this conservation law, derive
its analytic decay under damping and stochastic perturbations, and demonstrate
its behavior in a synthetic system. The invariant offers a compact lens for
interpreting learning trajectories, and may inform theoretical tools for
analyzing convergence, stability, and training geometry.

</details>


### [269] [Federated Learning: From Theory to Practice](https://arxiv.org/abs/2505.19183)
*A. Jung*

Main category: cs.LG

TL;DR: This book provides a hands-on introduction to building federated learning (FL) systems with focus on personalization.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to offer an introduction and guide for building scalable, privacy preserving FL systems.

Method: The method involves representing real-world FL systems as networks of devices where nodes correspond to device and edges represent communication links and data similarities between them. The training of personalized models for these devices can be framed as a distributed optimization problem called generalized total variation minimization (GTVMin).

Result: It ensures that devices with similar learning tasks learn similar model parameters.

Conclusion: This approach is both mathematically principled and practically motivated.

Abstract: This book offers a hands-on introduction to building and understanding
federated learning (FL) systems. FL enables multiple devices -- such as
smartphones, sensors, or local computers -- to collaboratively train machine
learning (ML) models, while keeping their data private and local. It is a
powerful solution when data cannot or should not be centralized due to privacy,
regulatory, or technical reasons. The book is designed for students, engineers,
and researchers who want to learn how to design scalable, privacy preserving FL
systems. Our main focus is on personalization: enabling each device to train
its own model while still benefiting from collaboration with relevant devices.
This is achieved by leveraging similarities between (the learning tasks
associated with) devices that are encoded by the weighted edges (or links) of a
federated learning network (FL network). The key idea is to represent
real-world FL systems as networks of devices, where nodes correspond to device
and edges represent communication links and data similarities between them. The
training of personalized models for these devices can be naturally framed as a
distributed optimization problem. This optimization problem is referred to as
generalized total variation minimization (GTVMin) and ensures that devices with
similar learning tasks learn similar model parameters. Our approach is both
mathematically principled and practically motivated. While we introduce some
advanced ideas from optimization theory and graph-based learning, we aim to
keep the book accessible. Readers are guided through the core ideas step by
step, with intuitive explanations.

</details>


### [270] [Chordless Structure: A Pathway to Simple and Expressive GNNs](https://arxiv.org/abs/2505.19188)
*Hongxu Pan,Shuxian Hu,Mo Zhou,Zhibin Wang,Rong Gu,Chen Tian,Kun Yang,Sheng Zhong*

Main category: cs.LG

TL;DR: The paper proposes CSGNN, a GNN that uses chordless structures to improve efficiency and expressiveness without high computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing GNN expressiveness are either computationally expensive or lack provable expressiveness. Also, chords in graph structures often add complexity with little useful information.

Method: Propose CSGNN which omits chords and leverages chordless structures to represent graph information.

Result: CSGNN outperforms existing GNNs on various tasks with lower computational costs and surpasses 3-WL expressive GNNs.

Conclusion: CSGNN is more powerful than k-hop GNN with polynomial complexity and proves to be efficient and effective.

Abstract: Researchers have proposed various methods of incorporating more structured
information into the design of Graph Neural Networks (GNNs) to enhance their
expressiveness. However, these methods are either computationally expensive or
lacking in provable expressiveness. In this paper, we observe that the chords
increase the complexity of the graph structure while contributing little useful
information in many cases. In contrast, chordless structures are more efficient
and effective for representing the graph. Therefore, when leveraging the
information of cycles, we choose to omit the chords. Accordingly, we propose a
Chordless Structure-based Graph Neural Network (CSGNN) and prove that its
expressiveness is strictly more powerful than the k-hop GNN (KPGNN) with
polynomial complexity. Experimental results on real-world datasets demonstrate
that CSGNN outperforms existing GNNs across various graph tasks while incurring
lower computational costs and achieving better performance than the GNNs of
3-WL expressiveness.

</details>


### [271] [I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts](https://arxiv.org/abs/2505.19190)
*Jiayi Xin,Sukwon Yun,Jie Peng,Inyoung Choi,Jenna L. Ballard,Tianlong Chen,Qi Long*

Main category: cs.LG

TL;DR: I2MoE is a novel interpretable multimodal interaction-aware mixture of experts framework that enhances modality fusion by modeling diverse multimodal interactions and providing local/global interpretation.


<details>
  <summary>Details</summary>
Motivation: Vanilla modality fusion methods cannot account for heterogeneous interactions between modalities and lack interpretability in uncovering the inherent multimodal interactions.

Method: I2MoE uses different interaction experts with weakly supervised interaction losses to learn multimodal interactions data-driven. It also employs a reweighting model assigning importance scores for each interaction expert's output, offering sample-level and dataset-level interpretation.

Result: Extensive evaluation on medical and general multimodal datasets shows I2MoE is flexible to combine with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios.

Conclusion: I2MoE enhances modality fusion through explicit modeling of diverse multimodal interactions and provides interpretable insights at both local and global levels.

Abstract: Modality fusion is a cornerstone of multimodal learning, enabling information
integration from diverse data sources. However, vanilla fusion methods are
limited by (1) inability to account for heterogeneous interactions between
modalities and (2) lack of interpretability in uncovering the multimodal
interactions inherent in the data. To this end, we propose I2MoE (Interpretable
Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework
designed to enhance modality fusion by explicitly modeling diverse multimodal
interactions, as well as providing interpretation on a local and global level.
First, I2MoE utilizes different interaction experts with weakly supervised
interaction losses to learn multimodal interactions in a data-driven way.
Second, I2MoE deploys a reweighting model that assigns importance scores for
the output of each interaction expert, which offers sample-level and
dataset-level interpretation. Extensive evaluation of medical and general
multimodal datasets shows that I2MoE is flexible enough to be combined with
different fusion techniques, consistently improves task performance, and
provides interpretation across various real-world scenarios. Code is available
at https://github.com/Raina-Xin/I2MoE.

</details>


### [272] [Interpretable Graph Learning Over Sets of Temporally-Sparse Data](https://arxiv.org/abs/2505.19193)
*Andrea Zerio,Maya Bechler-Speicher,Maor Huri,Marie Vibeke Vestergaard,Ran Gilad-Bachrach,Tine Jess,Samir Bhatt,Aleksejs Sazonovs*

Main category: cs.LG

TL;DR: The paper proposes Graph Mixing Additive Networks (GMAN), an interpretable model designed for learning from irregular sets of temporal signals, which achieves state-of-the-art performance in real-world medical tasks and fake news detection.


<details>
  <summary>Details</summary>
Motivation: Real-world data, particularly in medical field, often includes measurements from multiple signals collected at irregular and asynchronous time intervals. Existing methods struggle to handle such fragmented and unevenly scattered temporal data effectively.

Method: The authors propose GMAN, a novel and interpretable-by-design model for learning over irregular sets of temporal signals. It can handle temporally sparse and heterogeneous signals and is flexible enough to be applied to different domains like medical tasks and fake news detection.

Result: GMAN achieves state-of-the-art performance in real-world medical tasks with a 4-point increase in the AUROC score for in-hospital mortality prediction compared to existing methods. It also performs well in fake news detection tasks.

Conclusion: GMAN is an effective and flexible model for handling irregular sets of temporal signals. Its interpretability capabilities allow for gaining insights in various domains, including medical and social network analysis. Theoretical insights on GMAN's expressive power are also provided.

Abstract: Real-world medical data often includes measurements from multiple signals
that are collected at irregular and asynchronous time intervals. For example,
different types of blood tests can be measured at different times and
frequencies, resulting in fragmented and unevenly scattered temporal data.
Similar issues of irregular sampling of different attributes occur in other
domains, such as monitoring of large systems using event log files or the
spread of fake news on social networks. Effectively learning from such data
requires models that can handle sets of temporally sparse and heterogeneous
signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a
novel and interpretable-by-design model for learning over irregular sets of
temporal signals. Our method achieves state-of-the-art performance in
real-world medical tasks, including a 4-point increase in the AUROC score of
in-hospital mortality prediction, compared to existing methods. We further
showcase GMAN's flexibility by applying it to a fake news detection task. We
demonstrate how its interpretability capabilities, including node-level,
graph-level, and subset-level importance, allow for transition phases detection
and gaining medical insights with real-world high-stakes implications. Finally,
we provide theoretical insights on GMAN expressive power.

</details>


### [273] [Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation](https://arxiv.org/abs/2505.19194)
*Peiran Sun*

Main category: cs.LG

TL;DR: 本研究提出了一种新的查询高效方法，动态曲率估计（DCE），用于在黑盒环境下估计决策边界曲率，并发现决策边界曲率与对抗鲁棒性之间的统计关联。同时，还提出了基于动态估计曲率的新攻击方法——曲率动态黑盒攻击（CDBA），其性能有所提升。


<details>
  <summary>Details</summary>
Motivation: 尽管已经存在许多关于对抗攻击和防御的方法，但这些方法大多关注模型内部的损失函数或分数的曲率，而不是决策边界的曲率。因此，需要一种更直接的方法来评估决策边界的曲率及其对模型鲁棒性的影响。

Method: 研究者基于现有的黑盒对抗攻击方法（CGBA），提出了一种名为动态曲率估计（DCE）的新方法，以在黑盒设置中估计决策边界的曲率。通过在多种分类器上应用DCE，研究者发现了决策边界曲率与对抗鲁棒性之间的统计联系。此外，还开发了一种新的攻击方法——曲率动态黑盒攻击（CDBA），利用动态估计的曲率提升了攻击性能。

Result: 研究结果表明，决策边界曲率与模型的对抗鲁棒性之间存在显著的统计关系。使用动态估计的曲率，新提出的CDBA攻击方法在性能上优于现有方法。

Conclusion: 动态曲率估计（DCE）为理解决策边界曲率及其对模型鲁棒性的影响提供了一种有效工具。所提出的CDBA攻击方法证明了利用决策边界曲率可以提高黑盒对抗攻击的效率。

Abstract: Adversarial attack reveals the vulnerability of deep learning models. For
about a decade, countless attack and defense methods have been proposed,
leading to robustified classifiers and better understanding of models. Among
these methods, curvature-based approaches have attracted attention because it
is assumed that high curvature may give rise to rough decision boundary.
However, the most commonly used \textit{curvature} is the curvature of loss
function, scores or other parameters from within the model as opposed to
decision boundary curvature, since the former can be relatively easily formed
using second order derivative. In this paper, we propose a new query-efficient
method, dynamic curvature estimation(DCE), to estimate the decision boundary
curvature in a black-box setting. Our approach is based on CGBA, a black-box
adversarial attack. By performing DCE on a wide range of classifiers, we
discovered, statistically, a connection between decision boundary curvature and
adversarial robustness. We also propose a new attack method, curvature dynamic
black-box attack(CDBA) with improved performance using the dynamically
estimated curvature.

</details>


### [274] [OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization](https://arxiv.org/abs/2505.19205)
*Meher Bhaskar Madiraju,Meher Sai Preetam Madiraju*

Main category: cs.LG

TL;DR: The paper presents OptiMindTune, a multi-agent framework for hyperparameter optimization using specialized AI agents powered by Google's Gemini models.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter optimization is crucial for machine learning model development, but traditional methods face challenges such as high dimensionality, complex interdependencies, and computational expense.

Method: OptiMindTune employs three specialized AI agents - Recommender Agent, Evaluator Agent, and Decision Agent - to address different aspects of the HPO problem. These agents interact dynamically and share knowledge to converge to optimal configurations more efficiently.

Result: The framework aims to achieve faster and more robust convergence to optimal hyperparameter configurations compared to single-agent or monolithic approaches.

Conclusion: The multi-agent paradigm of OptiMindTune offers a promising approach for handling the growing complexity in modern machine learning model tuning.

Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of
machine learning model development, significantly impacting model performance
and generalization. Traditional HPO methods often struggle with high
dimensionality, complex interdependencies, and computational expense. This
paper introduces OptiMindTune, a novel multi-agent framework designed to
intelligently and efficiently optimize hyperparameters. OptiMindTune leverages
the collaborative intelligence of three specialized AI agents -- a Recommender
Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's
Gemini models. These agents address distinct facets of the HPO problem, from
model selection and hyperparameter suggestion to robust evaluation and
strategic decision-making. By fostering dynamic interactions and knowledge
sharing, OptiMindTune aims to converge to optimal hyperparameter configurations
more rapidly and robustly than existing single-agent or monolithic approaches.
Our framework integrates principles from advanced large language models, and
adaptive search to achieve scalable and intelligent AutoML. We posit that this
multi-agent paradigm offers a promising avenue for tackling the increasing
complexity of modern machine learning model tuning.

</details>


### [275] [LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models](https://arxiv.org/abs/2505.19223)
*Fengqi Zhu,Rongzhen Wang,Shen Nie,Xiaolu Zhang,Chunwei Wu,Jun Hu,Jun Zhou,Jianfei Chen,Yankai Lin,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: The paper proposes Variance-Reduced Preference Optimization (VRPO) to improve the alignment of Masked Diffusion Models with human preferences through reinforcement learning. VRPO reduces variance in ELBO-based likelihood estimates, leading to better performance in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Masked Diffusion Models (MDMs) show promise in language modeling but lack effective alignment with human preferences via reinforcement learning due to high variance in ELBO-based likelihood estimates.

Method: Propose VRPO, a framework that analyzes the variance of ELBO estimators and derives bounds on bias and variance of preference optimization gradients. Introduce unbiased variance reduction strategies like optimal Monte Carlo budget allocation and antithetic sampling.

Result: LLaDA 1.5, using VRPO, outperforms its predecessor significantly across mathematical, code, and alignment benchmarks.

Conclusion: VRPO effectively improves MDM alignment with human preferences, demonstrated by LLaDA 1.5's superior performance.

Abstract: While Masked Diffusion Models (MDMs), such as LLaDA, present a promising
paradigm for language modeling, there has been relatively little effort in
aligning these models with human preferences via reinforcement learning. The
challenge primarily arises from the high variance in Evidence Lower Bound
(ELBO)-based likelihood estimates required for preference optimization. To
address this issue, we propose Variance-Reduced Preference Optimization (VRPO),
a framework that formally analyzes the variance of ELBO estimators and derives
bounds on both the bias and variance of preference optimization gradients.
Building on this theoretical foundation, we introduce unbiased variance
reduction strategies, including optimal Monte Carlo budget allocation and
antithetic sampling, that significantly improve the performance of MDM
alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,
and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor
consistently and significantly across mathematical (GSM8K +4.7), code
(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard
+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical
performance compared to strong language MDMs and ARMs. Project page:
https://ml-gsai.github.io/LLaDA-1.5-Demo/.

</details>


### [276] [Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law](https://arxiv.org/abs/2505.19227)
*Frederik Kunstner,Francis Bach*

Main category: cs.LG

TL;DR: The paper explores the optimization challenges faced by gradient descent in training transformer-based language models and compares it with sign descent under Zipf-distributed data.


<details>
  <summary>Details</summary>
Motivation: Recent works have shown that gradient descent faces difficulties in optimizing the first and last layers of transformer-based language models, which are overcome by optimizers like Adam. This difficulty is linked to the heavy-tailed distribution of words in text data following Zipf's law.

Method: A linear bigram model for next-token prediction is studied when tokens follow a power law parameterized by the exponent α > 0. Optimization scaling laws for deterministic gradient descent and sign descent are derived as a function of the exponent α.

Result: For Zipf-distributed data (α = 1), gradient descent requires a number of iterations that scales almost linearly with dimension to reach a small relative error. In contrast, sign descent shows better performance as the number of iterations scales only with the square-root of the dimension, leading to significant improvement for large vocabularies.

Conclusion: Sign descent outperforms gradient descent in terms of convergence speed for high-dimensional problems with Zipf-distributed data.

Abstract: Recent works have highlighted optimization difficulties faced by gradient
descent in training the first and last layers of transformer-based language
models, which are overcome by optimizers such as Adam. These works suggest that
the difficulty is linked to the heavy-tailed distribution of words in text
data, where the frequency of the $k$th most frequent word $\pi_k$ is
proportional to $1/k$, following Zipf's law. To better understand the impact of
the data distribution on training performance, we study a linear bigram model
for next-token prediction when the tokens follow a power law $\pi_k \propto
1/k^\alpha$ parameterized by the exponent $\alpha > 0$. We derive optimization
scaling laws for deterministic gradient descent and sign descent as a proxy for
Adam as a function of the exponent $\alpha$. Existing theoretical
investigations in scaling laws assume that the eigenvalues of the data decay as
a power law with exponent $\alpha > 1$. This assumption effectively makes the
problem ``finite dimensional'' as most of the loss comes from a few of the
largest eigencomponents. In comparison, we show that the problem is more
difficult when the data have heavier tails. The case $\alpha = 1$ as found in
text data is ``worst-case'' for gradient descent, in that the number of
iterations required to reach a small relative error scales almost linearly with
dimension. While the performance of sign descent also depends on the dimension,
for Zipf-distributed data the number of iterations scales only with the
square-root of the dimension, leading to a large improvement for large
vocabularies.

</details>


### [277] [CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models](https://arxiv.org/abs/2505.19235)
*Qinsi Wang,Hancheng Ye,Ming-Yu Chung,Yudong Liu,Yueqian Lin,Martin Kuo,Mingyuan Ma,Jianyi Zhang,Yiran Chen*

Main category: cs.LG

TL;DR: Vision-Language Models (VLMs) are great but costly. Token and neuron sparsity can help, yet their interplay is underexplored. This paper investigates this relationship, proposes CoreMatching framework that synergizes bothsparities for more efficient inference, demonstrating superior performance across tasks and hardware.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the high inference costs of Vision-Language Models in terms of time and memory, as well as investigating the potential interplay between token sparsity and neuron sparsity paradigms.

Method: The method involves introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, leading to the development of CoreMatching - a co-adaptive sparse inference framework leveraging the synergy between token and neuron sparsity.

Result: CoreMatching surpasses state-of-the-art baselines on ten image understanding tasks across three hardware devices, achieving 5x FLOPs reduction and 10x overall speedup on NVIDIA Titan Xp.

Conclusion: The conclusion highlights the successful uncovering of the interplay between token and neuron sparsity, with CoreMatching offering an effective solution to enhance inference efficiency.

Abstract: Vision-Language Models (VLMs) excel across diverse tasks but suffer from high
inference costs in time and memory. Token sparsity mitigates inefficiencies in
token usage, while neuron sparsity reduces high-dimensional computations, both
offering promising solutions to enhance efficiency. Recently, these two
sparsity paradigms have evolved largely in parallel, fostering the prevailing
assumption that they function independently. However, a fundamental yet
underexplored question remains: Do they truly operate in isolation, or is there
a deeper underlying interplay that has yet to be uncovered? In this paper, we
conduct the first comprehensive investigation into this question. By
introducing and analyzing the matching mechanism between Core Neurons and Core
Tokens, we found that key neurons and tokens for inference mutually influence
and reinforce each other. Building on this insight, we propose CoreMatching, a
co-adaptive sparse inference framework, which leverages the synergy between
token and neuron sparsity to enhance inference efficiency. Through theoretical
analysis and efficiency evaluations, we demonstrate that the proposed method
surpasses state-of-the-art baselines on ten image understanding tasks and three
hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs
reduction and a 10x overall speedup. Code is released at
https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.

</details>


### [278] [Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees](https://arxiv.org/abs/2505.19238)
*Sourav Ganguly,Arnob Ghosh,Kishan Panaganti,Adam Wierman*

Main category: cs.LG

TL;DR: 设计安全策略需要考虑约束条件下的决策，但模拟环境难以捕捉真实世界的不确定性。本文研究了在模型失配情况下的稳健约束马尔可夫决策问题(RCMDP)，提出了一种新方法来最小化约束值函数并最大化稳健奖励值函数。该算法在$O(\epsilon^{-2})$迭代后找到次优性和可行策略，并显著减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的控制系统需要在约束条件下进行决策以确保安全性，但现有模拟环境无法充分反映真实世界的挑战和不确定性。因此，需要一种能够在模型不确定性和约束条件下优化策略的方法。

Method: 提出了一个新的技术，通过最小化约束值函数来满足约束条件，同时当所有约束都被满足时，最大化稳健奖励值函数。该方法不需要使用二分搜索，从而降低了计算成本。

Result: 证明了所提出的算法可以在$O(\epsilon^{-2})$迭代后找到次优性和可行的策略。相比最先进的方法，对于较小的折扣因子($\gamma$)减少了至少4倍的计算时间，对于较大的$\gamma$则减少至少6倍的计算时间。

Conclusion: 本文提出的算法为解决RCMDP提供了一个有效的方法，不仅保证了策略的安全性，还提高了计算效率。

Abstract: Constrained decision-making is essential for designing safe policies in
real-world control systems, yet simulated environments often fail to capture
real-world adversities. We consider the problem of learning a policy that will
maximize the cumulative reward while satisfying a constraint, even when there
is a mismatch between the real model and an accessible simulator/nominal model.
In particular, we consider the robust constrained Markov decision problem
(RCMDP) where an agent needs to maximize the reward and satisfy the constraint
against the worst possible stochastic model under the uncertainty set centered
around an unknown nominal model. Primal-dual methods, effective for standard
constrained MDP (CMDP), are not applicable here because of the lack of the
strong duality property. Further, one cannot apply the standard robust
value-iteration based approach on the composite value function either as the
worst case models may be different for the reward value function and the
constraint value function. We propose a novel technique that effectively
minimizes the constraint value function--to satisfy the constraints; on the
other hand, when all the constraints are satisfied, it can simply maximize the
robust reward value function. We prove that such an algorithm finds a policy
with at most $\epsilon$ sub-optimality and feasible policy after
$O(\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we
do not need to employ a binary search, thus, we reduce the computation time by
at least 4x for smaller value of discount factor ($\gamma$) and by at least 6x
for larger value of $\gamma$.

</details>


### [279] [ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment](https://arxiv.org/abs/2505.19241)
*Xiaoqiang Lin,Arun Verma,Zhongxiang Dai,Daniela Rus,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: The abstract introduces ActiveDPO, an algorithm designed for efficient active data selection in aligning large language models (LLMs) using human preferences. It leverages a theoretically grounded criterion suitable for non-linear reward functions and directly uses the LLM to parameterize the reward model, leading to more effective and efficient data collection.


<details>
  <summary>Details</summary>
Motivation: Current methods for collecting high-quality human preference datasets for LLM alignment are costly and resource-intensive. Existing active data selection methods either lack strong theoretical foundation or rely on restrictive assumptions about reward functions.

Method: ActiveDPO is proposed, which uses a theoretically grounded data selection criterion for non-linear reward functions while leveraging the LLM itself to parameterize the reward model used for active data selection.

Result: Extensive experiments demonstrate that ActiveDPO outperforms existing methods across various models and datasets.

Conclusion: ActiveDPO leads to more effective and efficient data collection for LLM alignment by accounting for the influence of LLM on data selection.

Abstract: The recent success of using human preferences to align large language models
(LLMs) has significantly improved their performance in various downstream tasks
like question answering, mathematical reasoning, and code generation. However,3
achieving effective LLM alignment depends on high-quality human preference
datasets. Collecting these datasets requires human preference annotation, which
is costly and resource-intensive, necessitating efficient active data selection
methods. Existing methods either lack a strong theoretical foundation or depend
on restrictive reward function assumptions (e.g., linearity). To this end, we
propose an algorithm, ActiveDPO, that uses a theoretically grounded data
selection criterion for non-linear reward functions while directly leveraging
the LLM itself to parameterize the reward model that is used for active data
selection. As a result, ActiveDPO explicitly accounts for the influence of LLM
on data selection, unlike methods that select the data without considering the
LLM that is being aligned, thereby leading to more effective and efficient data
collection. Extensive experiments show that ActiveDPO outperforms existing
methods across various models and datasets.

</details>


### [280] [To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers](https://arxiv.org/abs/2505.19245)
*Kevin Xu,Issei Sato*

Main category: cs.LG

TL;DR: Looped Transformers在确定性任务中表现良好，可以通过有向无环图进行评估；而Chain-of-Thought结合随机解码在处理组合结构的近似推理问题上更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 研究Chain-of-Thought (CoT)和Looped Transformers在推理任务中的相对优势和局限性，以便更好地理解它们各自的能力。

Method: 通过形式化分析两种方法：Looped Transformers能够有效模拟并行计算以处理确定性任务（表现为有向无环图上的评估）；CoT与随机解码相结合，在自归约问题（即组合结构）的近似推理中表现出色。

Result: 明确了Looped Transformers适合于确定性任务，而CoT更适合于组合结构的近似推理任务。

Conclusion: 深度驱动递归对于某些任务更为合适，并提供了选择推理范式的实际指导。

Abstract: Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically
improve performance on reasoning tasks and to theoretically enhance
expressivity by recursively increasing the number of computational steps.
However, their comparative capabilities are still not well understood. In this
paper, we provide a formal analysis of their respective strengths and
limitations. We show that Looped Transformers can efficiently simulate parallel
computations for deterministic tasks, which we formalize as evaluation over
directed acyclic graphs. In contrast, CoT with stochastic decoding excels at
approximate inference for compositional structures, namely self-reducible
problems. These separations suggest the tasks for which depth-driven recursion
is more suitable, thereby offering practical cues for choosing between
reasoning paradigms.

</details>


### [281] [Improving Value Estimation Critically Enhances Vanilla Policy Gradient](https://arxiv.org/abs/2505.19247)
*Tao Wang,Ruipeng Zhang,Sicun Gao*

Main category: cs.LG

TL;DR: 通过增加每轮迭代中的价值更新步骤，简单的策略梯度方法可以在连续控制基准环境中达到与PPO相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 质疑强化学习中常见的信念：实施近似信任区域会导致实际中的稳定策略改进。研究发现，更重要的是每轮迭代中更多的价值更新步骤提高了价值估计的准确性。

Method: 通过简单地增加每轮迭代中的价值更新步骤来增强价值估计的准确性，从而提高原始策略梯度方法的性能。

Result: 在所有标准连续控制基准环境中，简单改变后的原始策略梯度方法可以实现与PPO相当或更优的性能，并且对超参数选择具有显著更高的鲁棒性。

Conclusion: RL算法可能通过简化方法变得更加高效和易于使用，强调了价值估计准确性的重要性。

Abstract: Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla
policy gradient in many RL tasks. Questioning the common belief that enforcing
approximate trust regions leads to steady policy improvement in practice, we
show that the more critical factor is the enhanced value estimation accuracy
from more value update steps in each iteration. To demonstrate, we show that by
simply increasing the number of value update steps per iteration, vanilla
policy gradient itself can achieve performance comparable to or better than PPO
in all the standard continuous control benchmark environments. Importantly,
this simple change to vanilla policy gradient is significantly more robust to
hyperparameter choices, opening up the possibility that RL algorithms may still
become more effective and easier to use.

</details>


### [282] [VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use](https://arxiv.org/abs/2505.19255)
*Mingyuan Wu,Jingcheng Yang,Jize Jiang,Meitang Li,Kaizhuo Yan,Hanchao Yu,Minjia Zhang,Chengxiang Zhai,Klara Nahrstedt*

Main category: cs.LG

TL;DR: VTool-R1 is a new framework that teaches AI models to reason with both text and images, improving their problem-solving abilities.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods for vision-language models mainly focus on text-based reasoning given static image inputs, lacking true multimodal reasoning. Test-time methods incorporating visual steps exist but lack training mechanisms.

Method: VTool-R1 integrates Python-based visual editing tools into the reinforcement learning finetuning process, allowing vision-language models to generate multimodal chains of thought by combining text and visual reasoning steps. It uses outcome-based rewards to encourage strategic use of visual tools without needing process-based supervision.

Result: Experiments on structured visual question answering tasks involving charts and tables demonstrate that VTool-R1 improves reasoning performance by enabling vision-language models to 'think with images' and create multimodal chains of thought using tools.

Conclusion: VTool-R1 represents a significant advancement in training vision-language models to effectively incorporate visual reasoning steps, enhancing their overall reasoning capabilities.

Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the
reasoning capabilities of large language models (LLMs) by enabling long chains
of thought, self-correction, and effective tool use. While recent works attempt
to extend RFT to vision-language models (VLMs), these efforts largely produce
text-only reasoning conditioned on static image inputs, falling short of true
multimodal reasoning in the response. In contrast, test-time methods like
Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate
multimodal chains of thought by interleaving text and intermediate visual
reasoning steps. VTool-R1 integrates Python-based visual editing tools into the
RFT process, enabling VLMs to learn when and how to generate visual reasoning
steps that benefit final reasoning. Trained with outcome-based rewards tied to
task accuracy, our approach elicits strategic visual tool use for reasoning
without relying on process-based supervision. Experiments on structured visual
question answering over charts and tables show that VTool-R1 enhances reasoning
performance by teaching VLMs to "think with images" and generate multimodal
chain of thoughts with tools.

</details>


### [283] [Towards a Spatiotemporal Fusion Approach to Precipitation Nowcasting](https://arxiv.org/abs/2505.19258)
*Felipe Curcio,Pedro Castro,Augusto Fonseca,Rafaela Castro,Raquel Franco,Eduardo Ogasawara,Victor Stepanenko,Fabio Porto,Mariza Ferro,Eduardo Bezerra*

Main category: cs.LG

TL;DR: The paper proposes a data fusion approach using STConvS2S for precipitation nowcasting in Rio de Janeiro, integrating multiple meteorological data sources and achieving an F1-score of 0.2033 for heavy precipitation events forecasting.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient data integration methods to improve weather forecasts and hydrometeorological studies as more meteorological data becomes available from various sensors, numerical models, and reanalysis products.

Method: Employing the spatiotemporal deep learning architecture STConvS2S to fuse data from meteorological and rain gauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data and GFS numerical weather prediction over a 9 x 11 grid dataset covering January 2011 to October 2024.

Result: Achieved an F1-score of 0.2033 for forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour lead time through the fusion-based model; an ablation study was also conducted to evaluate the contribution of each station network.

Conclusion: The proposed data fusion approach demonstrates potential in improving precipitation nowcasting by integrating diverse meteorological data sources.

Abstract: With the increasing availability of meteorological data from various sensors,
numerical models and reanalysis products, the need for efficient data
integration methods has become paramount for improving weather forecasts and
hydrometeorological studies. In this work, we propose a data fusion approach
for precipitation nowcasting by integrating data from meteorological and rain
gauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data
and GFS numerical weather prediction. We employ the spatiotemporal deep
learning architecture called STConvS2S, leveraging a structured dataset
covering a 9 x 11 grid. The study spans from January 2011 to October 2024, and
we evaluate the impact of integrating three surface station systems. Among the
tested configurations, the fusion-based model achieves an F1-score of 0.2033
for forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour
lead time. Additionally, we present an ablation study to assess the
contribution of each station network and propose a refined inference strategy
for precipitation nowcasting, integrating the GFS numerical weather prediction
(NWP) data with in-situ observations.

</details>


### [284] [Towards Large Reasoning Models for Agriculture](https://arxiv.org/abs/2505.19259)
*Hossein Zaremehrjerdi,Shreyan Ganguly,Ashlyn Rairdin,Elizabeth Tranel,Benjamin Feuer,Juan Ignacio Di Salvo,Srikanth Panthulugiri,Victoria Moser,Sarah Jones,Joscif G Raigne,Yanben Shen,Heidi M. Dornath,Aditya Balu,Adarsh Krishnamurthy,Asheesh K Singh,Arti Singh,Baskar Ganapathysubramanian,Chinmay Hegde,Soumik Sarkar*

Main category: cs.LG

TL;DR: The study introduces AgReason, a benchmark for agricultural reasoning with 100 questions, and AgThoughts, a large dataset of question-answer pairs. Evaluations show that large reasoning models (LRMs) outperform traditional models in agricultural reasoning tasks, but challenges remain. Using AgThoughts, the researchers developed AgThinker, a set of small reasoning models.


<details>
  <summary>Details</summary>
Motivation: Agricultural decision-making is complex and context-specific, requiring nuanced reasoning based on geographic, climatic, and economic conditions. Traditional large language models often lack the necessary reasoning capacity for such tasks.

Method: The researchers created AgReason, a benchmark with 100 questions for agricultural reasoning, and AgThoughts, a dataset of 44.6K question-answer pairs. They evaluated thirteen models, including LRMs, on AgReason and developed AgThinker, a suite of small reasoning models using AgThoughts.

Result: LRMs outperformed conventional models on the agricultural reasoning tasks, with the strongest Gemini-based baseline achieving 36% accuracy. The use of AgThoughts was effective in enhancing agricultural reasoning abilities in LLMs.

Conclusion: While LRMs demonstrate superior performance in agricultural reasoning compared to traditional models, significant challenges still exist. The development of AgThinker shows promise for improving reasoning capabilities on consumer-grade hardware.

Abstract: Agricultural decision-making involves complex, context-specific reasoning,
where choices about crops, practices, and interventions depend heavily on
geographic, climatic, and economic conditions. Traditional large language
models (LLMs) often fall short in navigating this nuanced problem due to
limited reasoning capacity. We hypothesize that recent advances in large
reasoning models (LRMs) can better handle such structured, domain-specific
inference. To investigate this, we introduce AgReason, the first expert-curated
open-ended science benchmark with 100 questions for agricultural reasoning.
Evaluations across thirteen open-source and proprietary models reveal that LRMs
outperform conventional ones, though notable challenges persist, with the
strongest Gemini-based baseline achieving 36% accuracy. We also present
AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with
human oversight and equipped with synthetically generated reasoning traces.
Using AgThoughts, we develop AgThinker, a suite of small reasoning models that
can be run on consumer-grade GPUs, and show that our dataset can be effective
in unlocking agricultural reasoning abilities in LLMs. Our project page is
here: https://baskargroup.github.io/Ag_reasoning/

</details>


### [285] [Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning](https://arxiv.org/abs/2505.19263)
*Hui Ma,Kai Yang,Yang Jiao*

Main category: cs.LG

TL;DR: The paper proposes an asynchronous differential federated learning framework based on distributionally robust optimization for network traffic prediction, which addresses privacy and Byzantine attack issues.


<details>
  <summary>Details</summary>
Motivation: Traditional network traffic prediction methods rely on centralized training, causing latency and privacy concerns. Existing federated learning protocols are vulnerable to Byzantine attacks, compromising model robustness.

Method: An asynchronous differential federated learning framework is proposed, integrating distributionally robust optimization and local differential privacy. Regularization techniques are also employed to enhance Byzantine robustness.

Result: Extensive experiments on three real-world datasets demonstrate that the proposed distributed algorithm outperforms existing methods in terms of performance.

Conclusion: The proposed framework effectively addresses privacy and Byzantine attack issues in network traffic prediction, achieving superior performance.

Abstract: Network traffic prediction plays a crucial role in intelligent network
operation. Traditional prediction methods often rely on centralized training,
necessitating the transfer of vast amounts of traffic data to a central server.
This approach can lead to latency and privacy concerns. To address these
issues, federated learning integrated with differential privacy has emerged as
a solution to improve data privacy and model robustness in distributed
settings. Nonetheless, existing federated learning protocols are vulnerable to
Byzantine attacks, which may significantly compromise model robustness.
Developing a robust and privacy-preserving prediction model in the presence of
Byzantine clients remains a significant challenge. To this end, we propose an
asynchronous differential federated learning framework based on
distributionally robust optimization. The proposed framework utilizes multiple
clients to train the prediction model collaboratively with local differential
privacy. In addition, regularization techniques have been employed to further
improve the Byzantine robustness of the models. We have conducted extensive
experiments on three real-world datasets, and the results elucidate that our
proposed distributed algorithm can achieve superior performance over existing
methods.

</details>


### [286] [A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning](https://arxiv.org/abs/2505.19281)
*Yuzheng Hu,Fan Wu,Haotian Ye,David Forsyth,James Zou,Nan Jiang,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: The paper introduces a local attribution framework for online reinforcement learning (RL) using Proximal Policy Optimization (PPO), with target functions capturing agent action and cumulative return. It proposes the iterative influence-based filtering (IIF) algorithm to enhance training efficiency and effectiveness across various RL benchmarks.


<details>
  <summary>Details</summary>
Motivation: Online RL faces challenges like sample inefficiency, instability, and lack of interpretability. Traditional data attribution methods are not suitable for online RL due to its dynamic nature where each training sample influences both policy updates and future data collection.

Method: Established a local attribution framework for PPO that interprets model checkpoints relative to recent training buffer records. Designed two target functions for agent action and cumulative return, measuring record contributions via gradient similarity. Proposed IIF algorithm for experience filtering during training.

Result: IIF reduces sample complexity, accelerates training, and achieves higher returns across multiple RL benchmarks including classic control, navigation, locomotion, and even RLHF for large language models.

Conclusion: This study advances the interpretability, efficiency, and effectiveness of online RL through the introduction of the local attribution framework and the IIF algorithm.

Abstract: Online reinforcement learning (RL) excels in complex, safety-critical
domains, yet it faces challenges such as sample inefficiency, training
instability, and a lack of interpretability. Data attribution offers a
principled way to trace model behavior back to individual training samples.
However, in online RL, each training sample not only drives policy updates but
also influences future data collection, violating the fixed dataset assumption
in existing attribution methods. In this paper, we initiate the study of data
attribution for online RL, focusing on the widely used Proximal Policy
Optimization (PPO) algorithm. We start by establishing a local attribution
framework, interpreting model checkpoints with respect to the records in the
recent training buffer. We design two target functions, capturing agent action
and cumulative return respectively, and measure each record's contribution
through gradient similarity between its training loss and these targets. We
demonstrate the power of this framework through three concrete applications:
diagnosis of learning, temporal analysis of behavior formation, and targeted
intervention during training. Leveraging this framework, we further propose an
algorithm, iterative influence-based filtering (IIF), for online RL training
that iteratively performs experience filtering to refine policy updates. Across
standard RL benchmarks (classic control, navigation, locomotion) to RLHF for
large language models, IIF reduces sample complexity, speeds up training, and
achieves higher returns. Overall, these results advance interpretability,
efficiency, and effectiveness of online RL.

</details>


### [287] [Hypercube-RAG: Hypercube-Based Retrieval-Augmented Generation for In-domain Scientific Question-Answering](https://arxiv.org/abs/2505.19288)
*Jimeng Shi,Sizhe Zhou,Bowen Jin,Wei Hu,Shaowen Wang,Giri Narasimhan,Jiawei Han*

Main category: cs.LG

TL;DR: 为了应对领域知识密集型任务（如科学问答）的挑战，本文提出了一种新的检索增强生成框架Hypercube-RAG。它基于多维立方体结构，通过分解查询并根据超立方体维度对齐来实现精确和高效的检索。实验表明，该方法在三个科学问答数据集上提高了3.7%的准确率，并将检索效率提升了81.2%，同时具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于语义相似性的RAG在领域知识密集型任务（如科学问答）中难以返回简洁且高度相关的信息，因此需要一种更高效、更精准的检索方法。

Method: Hypercube-RAG基于多维立方体结构（Hypercube），可以按照应用驱动、人工定义的多维空间索引文档。对于给定的查询，首先根据其实体和主题进行分解，然后通过与超立方体维度对齐来检索相关文档。

Result: 在三个科学问答数据集上的实验表明，Hypercube-RAG将准确率相对提高了3.7%，检索效率提升了81.2%（相对于最强的RAG基线）。此外，该方法通过揭示用于检索的预定义超立方体维度，提供了内在的可解释性。

Conclusion: Hypercube-RAG是一种新颖的RAG框架，适用于精确和高效的检索任务，特别是在领域知识密集型任务中表现出色，并且具有良好的可解释性。

Abstract: Large language models (LLMs) often need to incorporate external knowledge to
solve theme-specific problems. Retrieval-augmented generation (RAG), which
empowers LLMs to generate more qualified responses with retrieved external data
and knowledge, has shown its high promise. However, traditional semantic
similarity-based RAGs struggle to return concise yet highly relevant
information for domain knowledge-intensive tasks, such as scientific
question-answering (QA). Built on a multi-dimensional (cube) structure called
Hypercube, which can index documents in an application-driven, human-defined,
multi-dimensional space, we introduce the Hypercube-RAG, a novel RAG framework
for precise and efficient retrieval. Given a query, Hypercube-RAG first
decomposes it based on its entities and topics and then retrieves relevant
documents from cubes by aligning these decomposed components with hypercube
dimensions. Experiments on three in-domain scientific QA datasets demonstrate
that our method improves accuracy by 3.7% and boosts retrieval efficiency by
81.2%, measured as relative gains over the strongest RAG baseline. More
importantly, our Hypercube-RAG inherently offers explainability by revealing
the underlying predefined hypercube dimensions used for retrieval. The code and
data sets are available at https://github.com/JimengShi/Hypercube-RAG.

</details>


### [288] [Concept Reachability in Diffusion Models: Beyond Dataset Constraints](https://arxiv.org/abs/2505.19313)
*Marta Aparicio Rodriguez,Xenia Miscouridou,Anastasia Borovykh*

Main category: cs.LG

TL;DR: 尽管文本到图像模型的质量和复杂性取得了显著进展，但提示并不总能产生期望的结果。本文通过一系列实验探讨了概念可达性的理解，发现少量样本足以实现潜在空间中的概念可达性，并且干预的位置对可达性有重要影响。此外，即使数据集质量下降，通过引导仍能可靠地到达某些概念，这为模型提供者提供了一种无需昂贵的再训练和数据集整理的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成模型在质量和复杂性上有了显著进步，但是仅仅依靠提示词并不总是能得到理想的输出。因此，探索一种新的方法来控制模型行为，使其能够达到通过提示可能无法触及的概念，成为了一个重要的研究方向。

Method: 作者设计了一套实验以深入理解概念可达性，采用包含三个关键障碍的训练数据设置：概念稀缺、标题中概念描述不足以及与绑定概念相关的数据偏差。通过这些实验，研究了潜在空间中概念可达性的特性及影响因素。

Result: 研究表明：(i) 潜在空间中的概念可达性表现出明显的相变特征，少量样本就足以实现可达性；(ii) 干预在潜在空间中的位置对可达性有重要影响，表明某些概念仅在特定转换阶段可到达；(iii) 随着数据集质量下降，提示能力迅速减弱，但通过引导仍能可靠地到达许多概念。

Conclusion: 模型提供者可以通过利用这些发现，避免昂贵的再训练和数据集整理，转而创新用户界面控制机制，从而提高用户体验和模型性能。

Abstract: Despite significant advances in quality and complexity of the generations in
text-to-image models, prompting does not always lead to the desired outputs.
Controlling model behaviour by directly steering intermediate model activations
has emerged as a viable alternative allowing to reach concepts in latent space
that may otherwise remain inaccessible by prompt. In this work, we introduce a
set of experiments to deepen our understanding of concept reachability. We
design a training data setup with three key obstacles: scarcity of concepts,
underspecification of concepts in the captions, and data biases with tied
concepts. Our results show: (i) concept reachability in latent space exhibits a
distinct phase transition, with only a small number of samples being sufficient
to enable reachability, (ii) where in the latent space the intervention is
performed critically impacts reachability, showing that certain concepts are
reachable only at certain stages of transformation, and (iii) while prompting
ability rapidly diminishes with a decrease in quality of the dataset, concepts
often remain reliably reachable through steering. Model providers can leverage
this to bypass costly retraining and dataset curation and instead innovate with
user-facing control mechanisms.

</details>


### [289] [Paying Alignment Tax with Contrastive Learning](https://arxiv.org/abs/2505.19327)
*Buse Sibel Korkmaz,Rahul Nair,Elizabeth M. Daly,Antonio del Rio Chanona*

Main category: cs.LG

TL;DR: A new contrastive learning framework is proposed to solve the problem of model capability degradation caused by current debiasing methods. It uses positive and negative examples, contrast computation, and dynamic loss scaling to balance bias mitigation and faithfulness preservation. Experiments show that this method improves both toxicity reduction and faithfulness preservation without degrading model capabilities.


<details>
  <summary>Details</summary>
Motivation: Current debiasing methods often lead to a decrease in model performance, such as reduced factual accuracy and knowledge retention. Especially in smaller models, there are fundamental trade-offs leading to issues like reduced truthfulness, knowledge loss, or unintelligible outputs.

Method: The paper proposes a contrastive learning framework that learns through carefully constructed positive and negative examples. The approach includes contrast computation and dynamic loss scaling to balance bias mitigation with faithfulness preservation.

Result: Experimental results across multiple model scales indicate that the proposed method significantly improves both toxicity reduction and faithfulness preservation. It is the first to consistently improve both metrics simultaneously without degrading model capabilities.

Conclusion: Explicit modeling of both positive and negative examples via contrastive learning shows promise in reducing the alignment tax in language model debiasing.

Abstract: Current debiasing approaches often result a degradation in model capabilities
such as factual accuracy and knowledge retention. Through systematic evaluation
across multiple benchmarks, we demonstrate that existing debiasing methods face
fundamental trade-offs, particularly in smaller models, leading to reduced
truthfulness, knowledge loss, or unintelligible outputs. To address these
limitations, we propose a contrastive learning framework that learns through
carefully constructed positive and negative examples. Our approach introduces
contrast computation and dynamic loss scaling to balance bias mitigation with
faithfulness preservation. Experimental results across multiple model scales
demonstrate that our method achieves substantial improvements in both toxicity
reduction and faithfulness preservation. Most importantly, we show that our
framework is the first to consistently improve both metrics simultaneously,
avoiding the capability degradation characteristic of existing approaches.
These results suggest that explicit modeling of both positive and negative
examples through contrastive learning could be a promising direction for
reducing the alignment tax in language model debiasing.

</details>


### [290] [Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales](https://arxiv.org/abs/2505.19334)
*Charles Godfrey,Ping Nie,Natalia Ostapuk,David Ken,Shang Gao,Souheil Inati*

Main category: cs.LG

TL;DR: When pointwise scoring uses a large ordinal relevance label space, the performance gap with listwise ranking diminishes and becomes statistically insignificant in many cases.


<details>
  <summary>Details</summary>
Motivation: To explore whether the current consensus that listwise ranking outperforms pointwise scoring is universally true, especially when pointwise scoring utilizes a sufficiently large ordinal relevance label space.

Method: Evaluate pointwise scoring and listwise ranking methods using four LLMs, eight benchmark datasets from BEIR and TREC-DL suites, and two proprietary datasets. Analyze the performance gap between the two methods under different conditions.

Result: The gap between pointwise scoring and listwise ranking significantly reduces and becomes statistically insignificant for many LLM-benchmark dataset combinations when pointwise scoring uses a large ordinal relevance label space.

Conclusion: Pointwise scoring with a sufficiently large ordinal relevance label space can match the performance of listwise ranking, challenging the current research community consensus.

Abstract: Large language models (LLMs) obtain state of the art zero shot relevance
ranking performance on a variety of information retrieval tasks. The two most
common prompts to elicit LLM relevance judgments are pointwise scoring (a.k.a.
relevance generation), where the LLM sees a single query-document pair and
outputs a single relevance score, and listwise ranking (a.k.a. permutation
generation), where the LLM sees a query and a list of documents and outputs a
permutation, sorting the documents in decreasing order of relevance. The
current research community consensus is that listwise ranking yields superior
performance, and significant research effort has been devoted to crafting LLM
listwise ranking algorithms. The underlying hypothesis is that LLMs are better
at making relative relevance judgments than absolute ones. In tension with this
hypothesis, we find that the gap between pointwise scoring and listwise ranking
shrinks when pointwise scoring is implemented using a sufficiently large
ordinal relevance label space, becoming statistically insignificant for many
LLM-benchmark dataset combinations (where ``significant'' means ``95\%
confidence that listwise ranking improves NDCG@10''). Our evaluations span four
LLMs, eight benchmark datasets from the BEIR and TREC-DL suites, and two
proprietary datasets with relevance labels collected after the training cut-off
of all LLMs evaluated.

</details>


### [291] [Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies](https://arxiv.org/abs/2505.19337)
*Kevin Li,Marinka Zitnik*

Main category: cs.LG

TL;DR: RADT, a decision transformer model for offline, reward-free, goal-conditioned, avoid region-conditioned RL, encodes goals and avoid regions as prompt tokens, allowing flexible specification at evaluation time. It learns through hindsight relabeling and outperforms baselines in benchmarks across 11 tasks. When applied to cell reprogramming in biology, it reduces visits to undesirable states.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reach-avoid tasks encode avoid-region information into an augmented state space and cost function, which prevents dynamic specification at evaluation time and rely on well-designed reward and cost functions, limiting scalability.

Method: RADT uses a decision transformer model for offline, reward-free, goal-conditioned, avoid region-conditioned RL. It encodes goals and avoid regions directly as prompt tokens and applies goal and avoid-region hindsight relabeling to learn from suboptimal offline trajectories.

Result: RADT generalizes to out-of-distribution avoid region sizes and counts, outperforming baselines that require retraining. In one zero-shot setting, RADT achieves 35.7% improvement in normalized cost while maintaining high goal-reaching success. It also successfully applies to cell reprogramming in biology.

Conclusion: RADT offers a flexible approach to specify avoid-region information at evaluation time and demonstrates superior performance in benchmarks and real-world applications like cell reprogramming.

Abstract: Offline goal-conditioned reinforcement learning methods have shown promise
for reach-avoid tasks, where an agent must reach a target state while avoiding
undesirable regions of the state space. Existing approaches typically encode
avoid-region information into an augmented state space and cost function, which
prevents flexible, dynamic specification of novel avoid-region information at
evaluation time. They also rely heavily on well-designed reward and cost
functions, limiting scalability to complex or poorly structured environments.
We introduce RADT, a decision transformer model for offline, reward-free,
goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid
regions directly as prompt tokens, allowing any number of avoid regions of
arbitrary size to be specified at evaluation time. Using only suboptimal
offline trajectories from a random policy, RADT learns reach-avoid behavior
through a novel combination of goal and avoid-region hindsight relabeling. We
benchmark RADT against 3 existing offline goal-conditioned RL models across 11
tasks, environments, and experimental settings. RADT generalizes in a zero-shot
manner to out-of-distribution avoid region sizes and counts, outperforming
baselines that require retraining. In one such zero-shot setting, RADT achieves
35.7% improvement in normalized cost over the best retrained baseline while
maintaining high goal-reaching success. We apply RADT to cell reprogramming in
biology, where it reduces visits to undesirable intermediate gene expression
states during trajectories to desired target states, despite stochastic
transitions and discrete, structured state dynamics.

</details>


### [292] [Communication-Efficient Multi-Device Inference Acceleration for Transformer Models](https://arxiv.org/abs/2505.19342)
*Xiao Liu,Lijun Zhang,Deepak Ganesan,Hui Guan*

Main category: cs.LG

TL;DR: Transformer模型尽管在许多AI应用中表现出色，但其推理延迟较高，限制了在实时场景中的应用。现有的多设备推理方法需要高带宽，因此在带宽受限的环境中不实用。本文提出了ASTRA，一个高效的通信框架，通过序列并行性和混合精度注意力机制减少设备间通信。ASTRA使用向量量化压缩非本地token嵌入，并通过噪声增强量化和分布式类token优化保持任务准确性。实验表明，ASTRA在ViT和GPT2模型上分别实现了高达2.64倍和15.25倍的速度提升，同时支持低至10 Mbps的带宽。


<details>
  <summary>Details</summary>
Motivation: Transformer模型因其高推理延迟而难以在实时场景中应用，且现有的多设备推理方法对高带宽的需求使其在带宽受限环境下不切实际。这促使研究者探索一种能够在低带宽条件下有效加速Transformer推理的新方法。

Method: 提出了一种名为ASTRA的高效通信框架，结合了序列并行性和混合精度注意力机制，以减少设备间的通信需求。具体技术包括：1）通过向量量化压缩非本地token嵌入；2）采用噪声增强量化和分布式类token优化来保持任务准确性。

Result: 实验结果表明，在视觉和NLP任务中，ASTRA相较于单设备推理可实现最高2.64倍的速度提升，相对于最先进的多设备推理方法可达到最高15.25倍的速度提升，同时能够在低至10 Mbps的带宽下运行。

Conclusion: ASTRA提供了一种有效的解决方案，可以在低带宽环境下显著加速Transformer模型的推理过程，为在资源受限环境下的AI应用提供了新的可能性。

Abstract: Transformer models power many AI applications but suffer from high inference
latency, limiting their use in real-time settings. Multi-device inference can
reduce latency by parallelizing computation. Yet, existing methods require high
inter-device bandwidth, making them impractical for bandwidth-constrained
environments. We propose ASTRA, a communication-efficient framework that
accelerates Transformer inference through a novel integration of sequence
parallelism and a Mixed-Precision Attention mechanism designed to minimize
inter-device communication. ASTRA compresses non-local token embeddings via
vector quantization and preserves task accuracy through two optimizations,
Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT
and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X
speedups over single-device inference and up to 15.25X speedups over
state-of-the-art multi-device inferences, while operating under bandwidths as
low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.

</details>


### [293] [SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition](https://arxiv.org/abs/2505.19369)
*Yunbo Liu,Xukui Qin,Yifan Gao,Xiang Li,Chengwei Feng*

Main category: cs.LG

TL;DR: SETransformer is a hybrid deep neural architecture that combines Transformer-based temporal modeling with channel-wise squeeze-and-excitation (SE) attention and a learnable temporal attention pooling mechanism, significantly outperforming conventional models in Human Activity Recognition tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models like CNNs and RNNs struggle to capture long-range temporal dependencies and contextual relevance across multiple sensor channels in HAR using wearable sensor data.

Method: The model takes raw triaxial accelerometer data as input and leverages global self-attention to capture activity-specific motion dynamics over extended time windows, while adaptively emphasizing informative sensor channels and critical time steps through channel-wise SE attention and temporal attention pooling.

Result: SETransformer achieves a validation accuracy of 84.68% and a macro F1-score of 84.64%, surpassing all baseline architectures by a notable margin when evaluated on the WISDM dataset.

Conclusion: SETransformer is a competitive and interpretable solution for real-world HAR tasks, with strong potential for deployment in mobile and ubiquitous sensing applications.

Abstract: Human Activity Recognition (HAR) using wearable sensor data has become a
central task in mobile computing, healthcare, and human-computer interaction.
Despite the success of traditional deep learning models such as CNNs and RNNs,
they often struggle to capture long-range temporal dependencies and contextual
relevance across multiple sensor channels. To address these limitations, we
propose SETransformer, a hybrid deep neural architecture that combines
Transformer-based temporal modeling with channel-wise squeeze-and-excitation
(SE) attention and a learnable temporal attention pooling mechanism. The model
takes raw triaxial accelerometer data as input and leverages global
self-attention to capture activity-specific motion dynamics over extended time
windows, while adaptively emphasizing informative sensor channels and critical
time steps.
  We evaluate SETransformer on the WISDM dataset and demonstrate that it
significantly outperforms conventional models including LSTM, GRU, BiLSTM, and
CNN baselines. The proposed model achieves a validation accuracy of 84.68\% and
a macro F1-score of 84.64\%, surpassing all baseline architectures by a notable
margin. Our results show that SETransformer is a competitive and interpretable
solution for real-world HAR tasks, with strong potential for deployment in
mobile and ubiquitous sensing applications.

</details>


### [294] [Alignment of large language models with constrained learning](https://arxiv.org/abs/2505.19387)
*Botong Zhang,Shuo Li,Ignacio Hounie,Osbert Bastani,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: The paper presents an iterative dual-based alignment method using Lagrangian duality to compute an optimal large language model policy for constrained alignment problems, demonstrating its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of computing an optimal LLM policy for a constrained alignment problem where primary reward objective is maximized while satisfying constraints on secondary utilities.

Method: Employing Lagrangian duality to develop an iterative dual-based alignment method that alternates between updating the LLM policy via Lagrangian maximization and updating the dual variable via dual descent.

Result: The approach effectively characterizes the primal-dual gap and quantifies the optimality gap of learned LLM policies, proving that dual-based alignment methods can find an optimal constrained LLM policy within an LLM parametrization gap.

Conclusion: Through extensive experiments on the PKU-SafeRLHF dataset, the iterative dual-based alignment method demonstrates its effectiveness and merits in solving constrained alignment problems.

Abstract: We study the problem of computing an optimal large language model (LLM)
policy for a constrained alignment problem, where the goal is to maximize a
primary reward objective while satisfying constraints on secondary utilities.
Despite the popularity of Lagrangian-based LLM policy search in constrained
alignment, iterative primal-dual methods often fail to converge, and
non-iterative dual-based methods do not achieve optimality in the LLM parameter
space. To address these challenges, we employ Lagrangian duality to develop an
iterative dual-based alignment method that alternates between updating the LLM
policy via Lagrangian maximization and updating the dual variable via dual
descent. In theory, we characterize the primal-dual gap between the primal
value in the distribution space and the dual value in the LLM parameter space.
We further quantify the optimality gap of the learned LLM policies at
near-optimal dual variables with respect to both the objective and the
constraint functions. These results prove that dual-based alignment methods can
find an optimal constrained LLM policy, up to an LLM parametrization gap. We
demonstrate the effectiveness and merits of our approach through extensive
experiments conducted on the PKU-SafeRLHF dataset.

</details>


### [295] [Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains](https://arxiv.org/abs/2505.19397)
*Jiawen Zhang,Zhenwei Zhang,Shun Zheng,Xumeng Wen,Jia Li,Jiang Bian*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFM）在大规模跨域数据上进行预训练，能够在新场景中实现零样本预测。然而，它们对对抗性输入扰动是否鲁棒尚未被研究。本文通过系统性调查发现，微小扰动即可引发显著且可控的预测行为变化，如趋势反转、时间漂移和幅度偏移，这对基于TSFM的服务构成严重风险。实验揭示了其一致的脆弱性，并确定了可能提高鲁棒性的架构设计，例如结构稀疏性和多任务预训练。


<details>
  <summary>Details</summary>
Motivation: 尽管TSFMs在实际应用中越来越受欢迎，但它们对于对抗性输入扰动的鲁棒性问题却未被充分研究。这种扰动可能会被用于中间人攻击或数据投毒，因此需要对其进行评估。

Method: 作者通过对代表性TSFMs和多个数据集进行实验，系统地研究了TSFMs的对抗性鲁棒性。他们分析了不同类型的扰动对预测结果的影响，并识别出可能导致改进鲁棒性的潜在架构设计。

Result: 实验结果表明，即使是最小的扰动也能引起显著且可控的预测行为变化，包括趋势反转、时间漂移和幅度偏移。这些发现揭示了TSFMs的一致脆弱性。

Conclusion: 本研究提供了关于TSFMs对抗性鲁棒性的关键评估，并为设计更具弹性的预测系统提供了可行的指导。

Abstract: Time Series Foundation Models (TSFMs), which are pretrained on large-scale,
cross-domain data and capable of zero-shot forecasting in new scenarios without
further training, are increasingly adopted in real-world applications. However,
as the zero-shot forecasting paradigm gets popular, a critical yet overlooked
question emerges: Are TSFMs robust to adversarial input perturbations? Such
perturbations could be exploited in man-in-the-middle attacks or data
poisoning. To address this gap, we conduct a systematic investigation into the
adversarial robustness of TSFMs. Our results show that even minimal
perturbations can induce significant and controllable changes in forecast
behaviors, including trend reversal, temporal drift, and amplitude shift,
posing serious risks to TSFM-based services. Through experiments on
representative TSFMs and multiple datasets, we reveal their consistent
vulnerabilities and identify potential architectural designs, such as
structural sparsity and multi-task pretraining, that may improve robustness.
Our findings offer actionable guidance for designing more resilient forecasting
systems and provide a critical assessment of the adversarial robustness of
TSFMs.

</details>


### [296] [Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning](https://arxiv.org/abs/2505.19404)
*Yuta Ono,Hiroshi Nakamura,Hideki Takase*

Main category: cs.LG

TL;DR: This paper explores the effectiveness of TypiClust, a successful low-budget Active Learning (AL) strategy, in Federated Active Learning (FAL) settings with limited annotations. Empirical results indicate that TypiClust performs well even in low-budget FAL scenarios and is not significantly affected by distribution shifts caused by FAL settings.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper lies in reducing the annotation burden under realistic federated learning constraints through leveraging Active Learning techniques.

Method: The method involves investigating the effectiveness of TypiClust, a successful low-budget AL strategy, in low-budget FAL settings. The researchers also analyze the sensitivity of TypiClust to feature extraction methods.

Result: Empirical results demonstrate that TypiClust works well in low-budget FAL settings compared to other methods, despite challenges like data heterogeneity. It is also found that TypiClust is not very vulnerable to distribution shifts caused by FAL settings.

Conclusion: The conclusion is that TypiClust can be effectively used in low-budget FAL settings and provides a way to perform FAL even when data is limited.

Abstract: Federated Active Learning (FAL) seeks to reduce the burden of annotation
under the realistic constraints of federated learning by leveraging Active
Learning (AL). As FAL settings make it more expensive to obtain ground truth
labels, FAL strategies that work well in low-budget regimes, where the amount
of annotation is very limited, are needed. In this work, we investigate the
effectiveness of TypiClust, a successful low-budget AL strategy, in low-budget
FAL settings. Our empirical results show that TypiClust works well even in
low-budget FAL settings contrasted with relatively low performances of other
methods, although these settings present additional challenges, such as data
heterogeneity, compared to AL. In addition, we show that FAL settings cause
distribution shifts in terms of typicality, but TypiClust is not very
vulnerable to the shifts. We also analyze the sensitivity of TypiClust to
feature extraction methods, and it suggests a way to perform FAL even in
limited data situations.

</details>


### [297] [Future Link Prediction Without Memory or Aggregation](https://arxiv.org/abs/2505.19408)
*Lu Yi,Runlin Lei,Fengran Mo,Yanping Zheng,Zhewei Wei,Yuhang Ye*

Main category: cs.LG

TL;DR: The paper proposes CRAFT, a new model for future link prediction on temporal graphs that uses learnable node embeddings and cross-attention to achieve superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for future link prediction on temporal graphs struggle with unseen edges despite relying on complex memory and aggregation modules.

Method: CRAFT discards memory and aggregation modules and instead uses learnable node embeddings and cross-attention between the destination and the source's recent interactions.

Result: CRAFT consistently achieves superior performance with high efficiency in extensive experiments on diverse datasets.

Conclusion: CRAFT is well-suited for large-scale real-world applications due to its strong performance and efficiency.

Abstract: Future link prediction on temporal graphs is a fundamental task with wide
applicability in real-world dynamic systems. These scenarios often involve both
recurring (seen) and novel (unseen) interactions, requiring models to
generalize effectively across both types of edges. However, existing methods
typically rely on complex memory and aggregation modules, yet struggle to
handle unseen edges. In this paper, we revisit the architecture of existing
temporal graph models and identify two essential but overlooked modeling
requirements for future link prediction: representing nodes with unique
identifiers and performing target-aware matching between source and destination
nodes. To this end, we propose Cross-Attention based Future Link Predictor on
Temporal Graphs (CRAFT), a simple yet effective architecture that discards
memory and aggregation modules and instead builds on two components: learnable
node embeddings and cross-attention between the destination and the source's
recent interactions. This design provides strong expressive power and enables
target-aware modeling of the compatibility between candidate destinations and
the source's interaction patterns. Extensive experiments on diverse datasets
demonstrate that CRAFT consistently achieves superior performance with high
efficiency, making it well-suited for large-scale real-world applications.

</details>


### [298] [Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network](https://arxiv.org/abs/2505.19423)
*Bingdong Li,Mei Jiang,Hong Qian,Peng Yang,Wenjing Hong,Hong Qian,Ke Tang*

Main category: cs.LG

TL;DR: 本文提出了一种结合自编码器（AE）和双曲神经网络（HNN）的新型代理辅助进化强化学习（ERL）方法，有效减少无效评估并提高搜索效率。实验表明该方法在Atari和Mujoco游戏上显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 进化强化学习（ERL）相较于传统策略梯度具有更强的探索能力和鲁棒性，但其计算成本高且搜索效率低，许多候选策略无效。使用代理模型可减少无效评估，但由于策略为高维向量，构建有效代理极具挑战性。

Method: 利用自编码器（AE）将高维策略压缩为低维表示以提取关键特征作为代理输入；采用双曲神经网络（HNN）作为分类代理模型，从采样数据中学习复杂非线性关系，实现更准确的策略预选。

Result: 在10个Atari游戏和4个Mujoco游戏上的实验表明，所提方法显著优于先前方法，并且通过AE和HNN引导的搜索轨迹在探索和收敛方面更为有效。

Conclusion: 本文首次提出了适用于高维ERL策略的可学习策略嵌入和代理建模模块，并经验揭示了其成功的时间和原因。

Abstract: Evolutionary Reinforcement Learning (ERL), training the Reinforcement
Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated
enhanced exploration capabilities and greater robustness than using traditional
policy gradient. However, ERL suffers from the high computational costs and low
search efficiency, as EAs require evaluating numerous candidate policies with
expensive simulations, many of which are ineffective and do not contribute
meaningfully to the training. One intuitive way to reduce the ineffective
evaluations is to adopt the surrogates. Unfortunately, existing ERL policies
are often modeled as deep neural networks (DNNs) and thus naturally represented
as high-dimensional vectors containing millions of weights, which makes the
building of effective surrogates for ERL policies extremely challenging. This
paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)
and Hyperbolic Neural Networks (HNN). Specifically, AE compresses
high-dimensional policies into low-dimensional representations while extracting
key features as the inputs for the surrogate. HNN, functioning as a
classification-based surrogate model, can learn complex nonlinear relationships
from sampled data and enable more accurate pre-selection of the sampled
policies without real evaluations. The experiments on 10 Atari and 4 Mujoco
games have verified that the proposed method outperforms previous approaches
significantly. The search trajectories guided by AE and HNN are also visually
demonstrated to be more effective, in terms of both exploration and
convergence. This paper not only presents the first learnable policy embedding
and surrogate-modeling modules for high-dimensional ERL policies, but also
empirically reveals when and why they can be successful.

</details>


### [299] [WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference](https://arxiv.org/abs/2505.19427)
*Sihan Chen,Dan Zhao,Jongwoo Ko,Colby Banbury,Huiping Zhuang,Luming Liang,Tianyi Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The growing computational demands of large language models (LLMs) make
efficient inference and activation strategies increasingly critical. While
recent approaches, such as Mixture-of-Experts (MoE), leverage selective
activation but require specialized training, training-free sparse activation
methods offer broader applicability and superior resource efficiency through
their plug-and-play design. However, many existing methods rely solely on
hidden state magnitudes to determine activation, resulting in high
approximation errors and suboptimal inference accuracy. To address these
limitations, we propose WINA (Weight Informed Neuron Activation), a novel,
simple, and training-free sparse activation framework that jointly considers
hidden state magnitudes and the column-wise $\ell_2$-norms of weight matrices.
We show that this leads to a sparsification strategy that obtains optimal
approximation error bounds with theoretical guarantees tighter than existing
techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,
TEAL) by up to $2.94\%$ in average performance at the same sparsity levels,
across a diverse set of LLM architectures and datasets. These results position
WINA as a new performance frontier for training-free sparse activation in LLM
inference, advancing training-free sparse activation methods and setting a
robust baseline for efficient inference. The source code is available at
https://github.com/microsoft/wina.

</details>


### [300] [Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage](https://arxiv.org/abs/2505.19431)
*Chenguang Wang,Xiaoyu Zhang,Kaiyuan Cui,Weichen Zhao,Yongtao Guan,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出了一种名为Importance Weighted Score Matching的方法，通过优化类似前向KL散度的目标函数来训练扩散模型采样器，从而实现更好的模式覆盖。实验表明该方法在多模态分布上优于现有神经采样器。


<details>
  <summary>Details</summary>
Motivation: 直接从未归一化密度中训练神经采样器而不依赖目标分布样本是一个重要挑战。现有的方法通常通过优化反向KL散度目标函数来解决缺乏目标数据的问题，但这些方法往往表现出模式寻求行为，无法全面表示潜在分布。因此需要一种能够鼓励模式覆盖的训练方法。

Method: 提出了Importance Weighted Score Matching方法，通过对score matching loss进行可处理的重要性采样估计的重新加权，优化类似于前向KL散度的目标函数。同时提供了所提出的蒙特卡洛估计器和实际损失函数的偏差和方差的理论分析。

Result: 在逐渐复杂的多模态分布（包括多达120个模式的二维高斯混合模型和具有固有对称性的挑战性粒子系统）上的实验表明，该方法在所有分布距离度量上始终优于现有的神经采样器，并在所有基准测试中取得了最先进的结果。

Conclusion: 所提出的方法在训练扩散模型采样器时能够有效促进模式覆盖，并在各种复杂分布上实现了卓越的表现。

Abstract: Training neural samplers directly from unnormalized densities without access
to target distribution samples presents a significant challenge. A critical
desideratum in these settings is achieving comprehensive mode coverage,
ensuring the sampler captures the full diversity of the target distribution.
However, prevailing methods often circumvent the lack of target data by
optimizing reverse KL-based objectives. Such objectives inherently exhibit
mode-seeking behavior, potentially leading to incomplete representation of the
underlying distribution. While alternative approaches strive for better mode
coverage, they typically rely on implicit mechanisms like heuristics or
iterative refinement. In this work, we propose a principled approach for
training diffusion-based samplers by directly targeting an objective analogous
to the forward KL divergence, which is conceptually known to encourage mode
coverage. We introduce \textit{Importance Weighted Score Matching}, a method
that optimizes this desired mode-covering objective by re-weighting the score
matching loss using tractable importance sampling estimates, thereby overcoming
the absence of target distribution data. We also provide theoretical analysis
of the bias and variance for our proposed Monte Carlo estimator and the
practical loss function used in our method. Experiments on increasingly complex
multi-modal distributions, including 2D Gaussian Mixture Models with up to 120
modes and challenging particle systems with inherent symmetries -- demonstrate
that our approach consistently outperforms existing neural samplers across all
distributional distance metrics, achieving state-of-the-art results on all
benchmarks.

</details>


### [301] [Advanced long-term earth system forecasting by learning the small-scale nature](https://arxiv.org/abs/2505.19432)
*Hao Wu,Yuan Gao,Ruiqi Shu,Kun Wang,Ruijian Gou,Chuhan Wu,Xinliang Liu,Juncai He,Shuhao Cao,Junfeng Fang,Xingjian Shi,Feng Tao,Qi Song,Shengxuan Ji,Yanfei Xiang,Yuze Sun,Jiahao Li,Fan Xu,Huanshuo Dong,Haixin Wang,Fan Zhang,Penghao Zhao,Xian Wu,Qingsong Wen,Deliang Chen,Xiaomeng Huang*

Main category: cs.LG

TL;DR: Triton is an AI framework that mitigates spectral bias and models cross-scale dynamics for stable long-term Earth system forecasts.


<details>
  <summary>Details</summary>
Motivation: Current AI models struggle with long-term forecasting due to spectral bias, leading to poor representation of high-frequency processes and error amplification.

Method: Triton uses a hierarchical architecture to process information across multiple resolutions, inspired by numerical models increasing grids to resolve small scales.

Result: Triton achieves stable year-long global temperature forecasts, skillful Kuroshio eddy predictions up to 120 days, and high-fidelity turbulence simulations.

Conclusion: Triton effectively suppresses high-frequency error accumulation, offering a promising approach for trustworthy AI-driven climate and Earth system simulations.

Abstract: Reliable long-term forecast of Earth system dynamics is heavily hampered by
instabilities in current AI models during extended autoregressive simulations.
These failures often originate from inherent spectral bias, leading to
inadequate representation of critical high-frequency, small-scale processes and
subsequent uncontrolled error amplification. We present Triton, an AI framework
designed to address this fundamental challenge. Inspired by increasing grids to
explicitly resolve small scales in numerical models, Triton employs a
hierarchical architecture processing information across multiple resolutions to
mitigate spectral bias and explicitly model cross-scale dynamics. We
demonstrate Triton's superior performance on challenging forecast tasks,
achieving stable year-long global temperature forecasts, skillful Kuroshio eddy
predictions till 120 days, and high-fidelity turbulence simulations preserving
fine-scale structures all without external forcing, with significantly
surpassing baseline AI models in long-term stability and accuracy. By
effectively suppressing high-frequency error accumulation, Triton offers a
promising pathway towards trustworthy AI-driven simulation for climate and
earth system science.

</details>


### [302] [Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression](https://arxiv.org/abs/2505.19433)
*Peijie Dong,Zhenheng Tang,Xiang Liu,Lujun Li,Xiaowen Chu,Bo Li*

Main category: cs.LG

TL;DR: Post-training compression for LLMs is crucial for efficient deployment, but current benchmarks neglect agentic capabilities. This paper introduces ACBench to evaluate how compression impacts these abilities across various tasks, techniques, and models. Experiments show tradeoffs with 4-bit quantization affecting real-world application accuracy more than workflow or tool use. New metrics (ERank etc.) are introduced for systematic analysis.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing compression benchmarks which focus only on language modeling and natural language understanding tasks, ignoring the agentic capabilities of LLMs that are important for practical applications.

Method: Introduced ACBench, a comprehensive benchmark evaluating the impact of compression on LLMs' agentic abilities across 12 tasks, 4 capabilities, two quantization methods (GPTQ, AWQ), one pruning method (Wanda, SparseGPT), and 15 models including small, standard, and distilled reasoning LLMs. Used new metrics ERank, Top-k Ranking Correlation, and Energy for analysis.

Result: Experiments revealed tradeoffs in compression: 4-bit quantization had minimal effect on workflow generation and tool use (1%-3% drop) but significantly degraded real-world application accuracy by 10%-15%. ACBench provided insights for optimizing LLM compression in agentic scenarios.

Conclusion: ACBench fills an important gap in evaluating the impact of compression on LLMs' agentic capabilities. The findings highlight tradeoffs in using different compression techniques and provide actionable insights for optimizing LLM compression in practical scenarios.

Abstract: Post-training compression reduces the computational and memory costs of large
language models (LLMs), enabling resource-efficient deployment. However,
existing compression benchmarks only focus on language modeling (e.g.,
perplexity) and natural language understanding tasks (e.g., GLUE accuracy),
ignoring the agentic capabilities - workflow, tool use/function call,
long-context understanding and real-world application. We introduce the Agent
Compression Benchmark (ACBench), the first comprehensive benchmark for
evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)
12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,
Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)
and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),
standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).
Our experiments reveal compression tradeoffs: 4-bit quantization preserves
workflow generation and tool use (1%-3% drop) but degrades real-world
application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation
and Energy to systematize analysis. ACBench provides actionable insights for
optimizing LLM compression in agentic scenarios. The code can be found in
https://github.com/pprp/ACBench.

</details>


### [303] [MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration](https://arxiv.org/abs/2505.19445)
*Rishabh Bhattacharya,Hari Shankar,Vaishnavi Shivkumar,Ponnurangam Kumaraguru*

Main category: cs.LG

TL;DR: MetaGMT is a meta-learning framework that enhances explanation fidelity of GNNs through a novel bi-level optimization approach, significantly improving explanation quality and robustness on benchmarks while maintaining competitive classification accuracy.


<details>
  <summary>Details</summary>
Motivation: The increasing use of GNNs in high-stakes domains requires reliable explanations of their decision-making processes. Existing interpretable GNN architectures are vulnerable to spurious correlations, which can undermine trust.

Method: MetaGMT uses a meta-learning framework with a bi-level optimization approach to improve the fidelity of explanations produced by GNNs.

Result: MetaGMT significantly improves both explanation quality (measured by AUC-ROC and Precision@K) and robustness to spurious patterns across several benchmarks, including BA-2Motifs, MUTAG, and SP-Motif. It also maintains competitive classification accuracy with an increase up to 8% of Explanation ROC on SP-Motif 0.5 compared to baseline methods.

Conclusion: By enhancing the reliability of explanations, MetaGMT enables safer deployment of GNNs in sensitive domains through better model debugging, targeted retraining, and meaningful human oversight.

Abstract: The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains
like healthcare and finance demands reliable explanations of their
decision-making processes. While inherently interpretable GNN architectures
like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to
generating explanations based on spurious correlations, potentially undermining
trust in critical applications. We present MetaGMT, a meta-learning framework
that enhances explanation fidelity through a novel bi-level optimization
approach. We demonstrate that MetaGMT significantly improves both explanation
quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across
BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive
classification accuracy while producing more faithful explanations (with an
increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline
methods. These advancements in interpretability could enable safer deployment
of GNNs in sensitive domains by (1) facilitating model debugging through more
reliable explanations, (2) supporting targeted retraining when biases are
identified, and (3) enabling meaningful human oversight. By addressing the
critical challenge of explanation reliability, our work contributes to building
more trustworthy and actionable GNN systems for real-world applications.

</details>


### [304] [Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians](https://arxiv.org/abs/2505.19458)
*Akiyoshi Tomihari,Ryo Karakida*

Main category: cs.LG

TL;DR: Self-Attention (SA) theoretical understanding progresses by relaxing energy constraints and using dynamical systems analysis to characterize inference dynamics, enhancing performance through normalization layers.


<details>
  <summary>Details</summary>
Motivation: Current studies on self-attention layers often rely on idealized assumptions or additional constraints not necessarily present in standard SA. The motivation is to broaden the understanding of inference dynamics without these constraints.

Method: The method involves relaxing symmetry and single-head constraints traditionally required in energy-based formulations of self-attention layers. It also analyzes the Jacobian matrix of the state to investigate more general SA architectures capable of oscillatory dynamics without an energy function. Normalization layers are found to normalize the Jacobian's complex eigenvalues.

Result: Normalization layers force the dynamics close to a critical state, significantly enhancing inference performance. Regularization methods for training and a pseudo-energy for monitoring inference dynamics were developed from the Jacobian perspective.

Conclusion: This work provides an energy-agnostic characterization of inference dynamics in self-attention layers through dynamical systems analysis, revealing the role of normalization layers in enhancing performance.

Abstract: The theoretical understanding of self-attention (SA) has been steadily
progressing. A prominent line of work studies a class of SA layers that admit
an energy function decreased by state updates. While it provides valuable
insights into inherent biases in signal propagation, it often relies on
idealized assumptions or additional constraints not necessarily present in
standard SA. Thus, to broaden our understanding, this work aims to relax these
energy constraints and provide an energy-agnostic characterization of inference
dynamics by dynamical systems analysis. In more detail, we first consider
relaxing the symmetry and single-head constraints traditionally required in
energy-based formulations. Next, to investigate more general SA architectures
capable of oscillatory dynamics without necessarily admitting an energy
function, we analyze the Jacobian matrix of the state. We reveal that
normalization layers effectively normalize the Jacobian's complex eigenvalues,
forcing the dynamics close to a critical state. This significantly enhances
inference performance. Furthermore, we utilize the Jacobian perspective to
develop regularization methods for training and a pseudo-energy for monitoring
inference dynamics.

</details>


### [305] [Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation](https://arxiv.org/abs/2505.19459)
*Kaichao Jiang,He Wang,Xiaoshuai Hao,Xiulong Yang,Ajian Liu,Qi Chu,Yunfeng Diao*

Main category: cs.LG

TL;DR: This paper proposes Energy-based Joint Distribution Adversarial Training (EB-JDAT), a method that combines the strengths of Joint Energy-based Models (JEMs) and adversarial training (AT) to achieve high classification accuracy, adversarial robustness, and generative performance simultaneously.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether a single model can achieve high classification accuracy, adversarial robustness, and generative performance at the same time, addressing the trade-offs between these properties in existing models like JEMs and AT-based classifiers.

Method: The authors analyze the energy distribution differences across clean, adversarial, and generated samples in JEMs and AT models. Based on this analysis, they propose EB-JDAT, which jointly models the clean data distribution, adversarial distribution, and classifier by maximizing their joint probability.

Result: Experimental results show that EB-JDAT maintains the original accuracy and generative capability of JEMs while significantly enhancing robustness, even surpassing state-of-the-art AT methods.

Conclusion: EB-JDAT is an effective optimization method that unifies the strengths of JEMs and AT, providing a solution to the triple trade-off problem in achieving classification accuracy, generative capability, and robustness.

Abstract: Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative
models, are well known for their ability to achieve both high classification
accuracy and generative capability within a single model. However, their
robustness still lags significantly behind the classifiers based adversarial
training (AT). Conversely, while AT is currently the most effective approach to
improving the classifier's robustness, it typically sacrifices accuracy on
clean data and lacks generative capability. The triple trade-off between
classification accuracy, generative capability and robustness, raises a natural
question: Can a single model simultaneously achieve high classification
accuracy, adversarial robustness, and generative performance? -- a goal that
has been rarely explored. To address this question, we systematically analyze
the energy distribution differences of clean, adversarial, and generated
samples across various JEM variants and adversarially trained models. We
observe that AT tends to reduce the energy gap between clean and adversarial
samples, while JEMs reduce the gap between clean and synthetic ones. This
observation suggests a key insight: if the energy distributions of all three
data types can be aligned, we might unify the strengths of AT and JEMs,
resolving their inherent trade-offs. Building on this idea, we propose
Energy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly
model the clean data distribution, the adversarial distribution, and the
classifier by maximizing their joint probability. EB-JDAT is a general and
flexible optimization method, compatible with various JEM variants. Extensive
experimental results demonstrate that EB-JDAT not only maintains near original
accuracy and generative capability of JEMs, but also significantly enhances
robustness, even surpassing state-of-the-art ATs.

</details>


### [306] [Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding](https://arxiv.org/abs/2505.19465)
*Hengwei Zhang,Minghui Wu,Li Qiao,Ling Liu,Ziqi Han,Zhen Gao*

Main category: cs.LG

TL;DR: This letter proposes a deep-learning (DL)-based multi-user channel state information (CSI) feedback framework for massive multiple-input multiple-output systems, which utilizes deep joint source-channel coding (DJSCC) to improve CSI reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in CSI feedback for massive MIMO systems, such as high feedback overhead and sensitivity to uplink noise.

Method: The authors design a multi-user joint CSI feedback framework that uses CSI correlation of nearby users to reduce feedback overhead. They also propose a residual cross-attention transformer architecture deployed at the base station and integrate DJSCC with a two-stage training scheme to handle varying uplink noise levels.

Result: Experimental results show the proposed method's superiority in CSI feedback performance, featuring low network complexity and better scalability.

Conclusion: The deep-learning based multi-user CSI feedback framework with DJSCC improves CSI reconstruction accuracy, reduces feedback overhead, and mitigates the 'cliff-effect' in conventional approaches.

Abstract: This letter proposes a deep-learning (DL)-based multi-user channel state
information (CSI) feedback framework for massive multiple-input multiple-output
systems, where the deep joint source-channel coding (DJSCC) is utilized to
improve the CSI reconstruction accuracy. Specifically, we design a multi-user
joint CSI feedback framework, whereby the CSI correlation of nearby users is
utilized to reduce the feedback overhead. Under the framework, we propose a new
residual cross-attention transformer architecture, which is deployed at the
base station to further improve the CSI feedback performance. Moreover, to
tackle the "cliff-effect" of conventional bit-level CSI feedback approaches, we
integrated DJSCC into the multi-user CSI feedback, together with utilizing a
two-stage training scheme to adapt to varying uplink noise levels. Experimental
results demonstrate the superiority of our methods in CSI feedback performance,
with low network complexity and better scalability.

</details>


### [307] [Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory](https://arxiv.org/abs/2505.19469)
*Mingzhuo Li,Guang Li,Jiafeng Mao,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: The paper proposes a diversity-driven generative dataset distillation method based on a diffusion model to improve the representativeness of distilled datasets and enhance downstream validation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for dataset distillation have insufficiently diverse distributions, which reduces downstream validation accuracy.

Method: A diversity-driven generative dataset distillation method using a diffusion model is introduced. Self-adaptive memory is used to align the distribution between distilled and real datasets, assessing the representativeness and guiding the generation of more diverse datasets.

Result: Experiments show that the proposed method outperforms existing state-of-the-art methods in most situations.

Conclusion: The new method effectively tackles dataset distillation tasks by improving the diversity and representativeness of distilled datasets.

Abstract: Dataset distillation enables the training of deep neural networks with
comparable performance in significantly reduced time by compressing large
datasets into small and representative ones. Although the introduction of
generative models has made great achievements in this field, the distributions
of their distilled datasets are not diverse enough to represent the original
ones, leading to a decrease in downstream validation accuracy. In this paper,
we present a diversity-driven generative dataset distillation method based on a
diffusion model to solve this problem. We introduce self-adaptive memory to
align the distribution between distilled and real datasets, assessing the
representativeness. The degree of alignment leads the diffusion model to
generate more diverse datasets during the distillation process. Extensive
experiments show that our method outperforms existing state-of-the-art methods
in most situations, proving its ability to tackle dataset distillation tasks.

</details>


### [308] [Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs](https://arxiv.org/abs/2505.19481)
*Hao Kang,Qingru Zhang,Han Cai,Weiyuan Xu,Tushar Krishna,Yilun Du,Tsachy Weissman*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）在各种推理和生成任务中表现出色，但在严格延迟约束下进行决策的重要性尚未得到充分研究。本文首次系统地研究了实时决策任务中的延迟质量权衡，并提出了FPX框架，该框架根据实时需求动态选择模型大小和量化水平。实验结果表明，在Street Fighter游戏中胜率提高了80%，在交易中每日收益率提高了26.52%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在许多领域取得了显著成就，但在实际应用中，如高频交易和实时竞争性游戏，需要在严格的延迟限制下做出决策，这种情况下更快的响应直接转化为更高的回报。然而，关于延迟质量权衡的研究仍然不足。

Method: 引入了两个新的基准测试：HFTBench（高频交易模拟）和StreetFighter（竞争性游戏平台）。提出了一种名为FPX的自适应框架，该框架能够根据实时需求动态选择模型大小和量化水平。

Result: 在两个基准测试上均实现了最佳性能：在Street Fighter游戏中胜率提高了80%，在交易中每日收益率提高了26.52%。

Conclusion: 延迟敏感的评估和部署策略对于基于LLM的实际应用至关重要。提供了延迟敏感基准测试供进一步研究。

Abstract: Large language models (LLMs) have shown remarkable performance across diverse
reasoning and generation tasks, and are increasingly deployed as agents in
dynamic environments such as code generation and recommendation systems.
However, many real-world applications, such as high-frequency trading and
real-time competitive gaming, require decisions under strict latency
constraints, where faster responses directly translate into higher rewards.
Despite the importance of this latency quality trade off, it remains
underexplored in the context of LLM based agents. In this work, we present the
first systematic study of this trade off in real time decision making tasks. To
support our investigation, we introduce two new benchmarks: HFTBench, a high
frequency trading simulation, and StreetFighter, a competitive gaming platform.
Our analysis reveals that optimal latency quality balance varies by task, and
that sacrificing quality for lower latency can significantly enhance downstream
performance. To address this, we propose FPX, an adaptive framework that
dynamically selects model size and quantization level based on real time
demands. Our method achieves the best performance on both benchmarks, improving
win rate by up to 80% in Street Fighter and boosting daily yield by up to
26.52% in trading, underscoring the need for latency aware evaluation and
deployment strategies for LLM based agents. These results demonstrate the
critical importance of latency aware evaluation and deployment strategies for
real world LLM based agents. Our benchmarks are available at Latency Sensitive
Benchmarks.

</details>


### [309] [Understanding Transformer from the Perspective of Associative Memory](https://arxiv.org/abs/2505.19488)
*Shu Zhong,Mingyu Xu,Tenglong Ao,Guang Shi*

Main category: cs.LG

TL;DR: 这篇论文通过联想记忆的视角分析了Transformer架构，探讨了其记忆容量和更新机制，并提出了对Transformer表达能力和无限上下文情境下的智能水平的思考。


<details>
  <summary>Details</summary>
Motivation: 通过联想记忆这一经典心理学概念，深入理解Transformer架构的工作原理及其潜力。

Method: 1. 引入检索信噪比（retrieval SNR）来衡量Transformer的记忆容量，并从核函数的角度解释Softmax Attention的有效性；2. 将前馈神经网络（FFNs）视为一种联想记忆形式，提出设计改进方案；3. 构建统一框架，解析不同Transformer变体如何更新其知识库。

Result: 揭示了Transformer在记忆和学习方面的特性，提供了对其设计的新见解，并引发了关于其表达能力和潜在局限性的讨论。

Conclusion: 通过对Transformer架构的深入剖析，澄清了现有设计的原理，为未来的研究和创新提供了方向。

Abstract: In this paper, we share our reflections and insights on understanding
Transformer architectures through the lens of associative memory--a classic
psychological concept inspired by human cognition. We start with the basics of
associative memory (think simple linear attention) and then dive into two
dimensions:
  Memory Capacity: How much can a Transformer really remember, and how well? We
introduce retrieval SNR to measure this and use a kernel perspective to
mathematically reveal why Softmax Attention is so effective. We also show how
FFNs can be seen as a type of associative memory, leading to insights on their
design and potential improvements.
  Memory Update: How do these memories learn and evolve? We present a unified
framework for understanding how different Transformer variants (like DeltaNet
and Softmax Attention) update their "knowledge base". This leads us to tackle
two provocative questions: 1. Are Transformers fundamentally limited in what
they can express, and can we break these barriers? 2. If a Transformer had
infinite context, would it become infinitely intelligent?
  We want to demystify Transformer architecture, offering a clearer
understanding of existing designs. This exploration aims to provide fresh
insights and spark new avenues for Transformer innovation.

</details>


### [310] [Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval](https://arxiv.org/abs/2505.19491)
*Wenhao Yang,Sifan Yang,Lijun Zhang*

Main category: cs.LG

TL;DR: 在非平稳环境下，近期历史比远期历史更重要。本文通过分析平滑在线梯度下降(SOGD)算法，解决了未知折扣因子λ的折扣后悔问题。结果表明，SOGD能够实现统一的$O(\sqrt{\log T/1-\lambda})$折扣后悔，适用于所有连续区间的λ值。


<details>
  <summary>Details</summary>
Motivation: 在线凸优化（OCO）中，折扣后悔($\lambda$-discounted regret)用于优雅地遗忘旧数据，适应新信息。然而，在实际应用中折扣因子λ通常是未知的，因此需要开发一种能适应未知折扣因子的算法。

Method: 提出使用平滑在线梯度下降(SOGD)算法，并结合Discounted-Normal-Predictor (DNP)来处理不同折扣因子。维持多个OGD实例以应对不同的折扣因子，通过DNP对这些实例的输出进行序列聚合。

Result: 证明了SOGD算法可以实现统一的$O(\sqrt{\log T/1-\lambda})$折扣后悔，对于所有连续区间的λ值均成立。此外，DNP能够有效地结合来自不同折扣因子的两个专家决策。

Conclusion: 本文肯定回答了是否可能开发适应未知折扣因子的折扣算法这一开放性问题，为在线学习和非平稳环境下的优化提供了新的理论支持和方法。

Abstract: Reflecting the greater significance of recent history over the distant past
in non-stationary environments, $\lambda$-discounted regret has been introduced
in online convex optimization (OCO) to gracefully forget past data as new
information arrives. When the discount factor $\lambda$ is given, online
gradient descent with an appropriate step size achieves an
$O(1/\sqrt{1-\lambda})$ discounted regret. However, the value of $\lambda$ is
often not predetermined in real-world scenarios. This gives rise to a
significant open question: is it possible to develop a discounted algorithm
that adapts to an unknown discount factor. In this paper, we affirmatively
answer this question by providing a novel analysis to demonstrate that smoothed
OGD (SOGD) achieves a uniform $O(\sqrt{\log T/1-\lambda})$ discounted regret,
holding for all values of $\lambda$ across a continuous interval
simultaneously. The basic idea is to maintain multiple OGD instances to handle
different discount factors, and aggregate their outputs sequentially by an
online prediction algorithm named as Discounted-Normal-Predictor (DNP)
(Kapralov and Panigrahy,2010). Our analysis reveals that DNP can combine the
decisions of two experts, even when they operate on discounted regret with
different discount factors.

</details>


### [311] [Learning for Dynamic Combinatorial Optimization without Training Data](https://arxiv.org/abs/2505.19497)
*Yiqiao Liao,Farinaz Koushanfar,Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: The paper presents DyCO-GNN, an unsupervised learning framework for dynamic combinatorial optimization that uses structural similarities in time-evolving graphs to achieve faster and better solutions without external training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of dynamic combinatorial optimization problems where rapid changes require fast and accurate solutions without relying on extensive training data.

Method: DyCO-GNN leverages structural similarities across time-evolving graph snapshots for optimization, eliminating the need for external training data beyond the problem instance itself.

Result: DyCO-GNN outperforms baseline methods by providing high-quality solutions up to 3-60x faster, especially under tight and moderate time budgets.

Conclusion: DyCO-GNN is a practical and effective framework for dynamic combinatorial optimization, demonstrating superior performance in resource-constrained settings.

Abstract: We introduce DyCO-GNN, a novel unsupervised learning framework for Dynamic
Combinatorial Optimization that requires no training data beyond the problem
instance itself. DyCO-GNN leverages structural similarities across
time-evolving graph snapshots to accelerate optimization while maintaining
solution quality. We evaluate DyCO-GNN on dynamic maximum cut, maximum
independent set, and the traveling salesman problem across diverse datasets of
varying sizes, demonstrating its superior performance under tight and moderate
time budgets. DyCO-GNN consistently outperforms the baseline methods, achieving
high-quality solutions up to 3-60x faster, highlighting its practical
effectiveness in rapidly evolving resource-constrained settings.

</details>


### [312] [DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation](https://arxiv.org/abs/2505.19504)
*Pingzhi Li,Zhen Tan,Huaizhi Qu,Huan Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) are significant investments but their outputs can be easily imitated through knowledge distillation (KD). This paper presents a Defensive Output Generation (DOGe) strategy that modifies LLM outputs to mislead KD attempts, while still being useful for legitimate users.


<details>
  <summary>Details</summary>
Motivation: To actively protect LLMs from imitation via knowledge distillation, especially when only API-based access is available and existing protective methods are ineffective or post-hoc.

Method: Introduce the DOGe strategy which involves fine-tuning only the final linear layer of the teacher LLM with an adversarial loss to subtly modify its outputs in a way that misleads distillation attempts.

Result: Experiments show that the DOGe strategy preserves or even improves the original performance of the teacher model, while significantly reducing the performance of student models distilled from the defensive outputs.

Conclusion: DOGe is an effective and practical safeguard against KD-based model imitation.

Abstract: Large Language Models (LLMs) represent substantial intellectual and economic
investments, yet their effectiveness can inadvertently facilitate model
imitation via knowledge distillation (KD).In practical scenarios, competitors
can distill proprietary LLM capabilities by simply observing publicly
accessible outputs, akin to reverse-engineering a complex performance by
observation alone. Existing protective methods like watermarking only identify
imitation post-hoc, while other defenses assume the student model mimics the
teacher's internal logits, rendering them ineffective against distillation
purely from observed output text. This paper confronts the challenge of
actively protecting LLMs within the realistic constraints of API-based access.
We introduce an effective and efficient Defensive Output Generation (DOGe)
strategy that subtly modifies the output behavior of an LLM. Its outputs remain
accurate and useful for legitimate users, yet are designed to be misleading for
distillation, significantly undermining imitation attempts. We achieve this by
fine-tuning only the final linear layer of the teacher LLM with an adversarial
loss. This targeted training approach anticipates and disrupts distillation
attempts during inference time. Our experiments show that, while preserving or
even improving the original performance of the teacher model, student models
distilled from the defensively generated teacher outputs demonstrate
catastrophically reduced performance, demonstrating our method's effectiveness
as a practical safeguard against KD-based model imitation.

</details>


### [313] [Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models](https://arxiv.org/abs/2505.19509)
*Yifan Jia,Kailin Jiang,Yuyang Liang,Qihan Ren,Yi Xin,Rui Yang,Fenze Feng,Mingcai Chen,Hengyang Lu,Haozhe Wang,Xiaoye Qu,Dongrui Liu,Lizhen Cui,Yuntao Du*

Main category: cs.LG

TL;DR: 大型多模态模型(LMMs)在处理多模态知识冲突时存在显著挑战，尤其是在检索增强生成(RAG)框架下。为了解决现有基准测试未能反映此类现实冲突场景的问题，我们提出了MMKC-Bench基准测试。该基准测试涵盖了三种类型的多模态知识冲突，包含1,573个知识实例和3,381张图像。我们评估了三个代表性的LMM系列，发现当前的LMMs能够识别知识冲突，但更倾向于内部参数知识而非外部证据。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试未能充分反映多模态知识冲突的真实场景，特别是在检索增强生成(RAG)框架下的冲突。这促使研究者开发一个能更好地评估模型在面对知识冲突时表现的新基准测试。

Method: 提出一个新的基准测试MMKC-Bench，用于评估多模态知识冲突。该基准测试包括三种类型的多模态知识冲突，并收集了大量数据实例和图像。通过自动化管道与人工验证相结合的方法构建数据集。使用该基准测试评估了三个代表性系列的LMMs。

Result: 评估结果显示，当前的LMMs能够在一定程度上识别知识冲突，但在冲突解决中更倾向于依赖内部参数知识，而不是外部提供的证据。

Conclusion: MMKC-Bench为多模态知识冲突的研究提供了新的工具，有助于推动多模态RAG系统的进一步发展。

Abstract: Large Multimodal Models(LMMs) face notable challenges when encountering
multimodal knowledge conflicts, particularly under retrieval-augmented
generation(RAG) frameworks where the contextual information from external
sources may contradict the model's internal parametric knowledge, leading to
unreliable outputs. However, existing benchmarks fail to reflect such realistic
conflict scenarios. Most focus solely on intra-memory conflicts, while
context-memory and inter-context conflicts remain largely investigated.
Furthermore, commonly used factual knowledge-based evaluations are often
overlooked, and existing datasets lack a thorough investigation into conflict
detection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark
designed to evaluate factual knowledge conflicts in both context-memory and
inter-context scenarios. MMKC-Bench encompasses three types of multimodal
knowledge conflicts and includes 1,573 knowledge instances and 3,381 images
across 23 broad types, collected through automated pipelines with human
verification. We evaluate three representative series of LMMs on both model
behavior analysis and conflict detection tasks. Our findings show that while
current LMMs are capable of recognizing knowledge conflicts, they tend to favor
internal parametric knowledge over external evidence. We hope MMKC-Bench will
foster further research in multimodal knowledge conflict and enhance the
development of multimodal RAG systems. The source code is available at
https://github.com/MLLMKCBENCH/MLLMKC.

</details>


### [314] [Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate](https://arxiv.org/abs/2505.19525)
*Liangwei Nathan Zheng,Wei Emma Zhang,Mingyu Guo,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.LG

TL;DR: 为了应对现实世界多模态学习场景中数据缺失的挑战，本文提出了一种新的SMoE架构——Conf-SMoE。该方法通过引入两阶段插补模块来处理缺失模态问题，并通过理论分析和实证证据揭示了专家退化的见解。此外，Conf-SMoE提出了一种新颖的专家门控机制，无需引入额外的负载平衡损失函数即可自然缓解专家退化问题。实验结果表明，Conf-SMoE在四种不同真实世界数据集上表现出色，具有良好的模态融合能力和对缺失模态的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏混合专家（SMoE）方法在处理缺失模态方面能力不足，导致实际应用中的性能下降和泛化能力差。因此，需要一种更有效的方法来解决这一问题。

Method: Conf-SMoE引入了一个两阶段插补模块以应对SMoE架构中的缺失模态问题，并通过理论分析揭示了专家退化的见解。同时，基于理论分析，提出了一种新的专家门控机制，通过分离softmax路由分数为任务置信度分数来缓解专家退化问题。此外，还验证了专家退化与其他门控机制（如高斯门和拉普拉斯门）之间的联系。

Result: Conf-SMoE在四个不同的真实世界数据集上进行了评估，在三种不同的实验设置下表现出了强大的模态融合能力和对缺失模态的抵抗能力。

Conclusion: Conf-SMoE有效地解决了SMoE架构中缺失模态的问题，通过引入两阶段插补模块和新的专家门控机制，显著提高了模型的性能和泛化能力。这为未来多模态学习提供了重要的参考价值。

Abstract: Effectively managing missing modalities is a fundamental challenge in
real-world multimodal learning scenarios, where data incompleteness often
results from systematic collection errors or sensor failures. Sparse
Mixture-of-Experts (SMoE) architectures have the potential to naturally handle
multimodal data, with individual experts specializing in different modalities.
However, existing SMoE approach often lacks proper ability to handle missing
modality, leading to performance degradation and poor generalization in
real-world applications. We propose Conf-SMoE to introduce a two-stage
imputation module to handle the missing modality problem for the SMoE
architecture and reveal the insight of expert collapse from theoretical
analysis with strong empirical evidence. Inspired by our theoretical analysis,
Conf-SMoE propose a novel expert gating mechanism by detaching the softmax
routing score to task confidence score w.r.t ground truth. This naturally
relieves expert collapse without introducing additional load balance loss
function. We show that the insights of expert collapse aligns with other gating
mechanism such as Gaussian and Laplacian gate. We also evaluate the proposed
method on four different real world dataset with three different experiment
settings to conduct comprehensive the analysis of Conf-SMoE on modality fusion
and resistance to missing modality.

</details>


### [315] [Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation](https://arxiv.org/abs/2505.19527)
*Mohammed D. Belgoumri,Mohamed Reda Bouadjenek,Hakim Hacid,Imran Razzak,Sunil Aryal*

Main category: cs.LG

TL;DR: The paper proposes an alternative optimizer that simulates the motion of the center of a ball rolling on the loss landscape to reduce dependence on loss landscape fine structure and avoid sharp minima, improving generalization.


<details>
  <summary>Details</summary>
Motivation: Gradient-based optimization for large neural networks often leads to undesirable training dynamics due to high-dimensional loss landscapes with pathological geometry. This results in poor generalization from convergence to sharp minima and unstable training dynamics.

Method: The method involves simulating the motion of the center of a ball rolling on the loss landscape, controlled by a hyperparameter representing the radius of the ball. This allows probing the loss landscape at different scales.

Result: This approach reduces dependence on the fine structure of the loss landscape and avoids sharp minima, leading to improved generalization.

Conclusion: The proposed optimizer provides a valuable tool for understanding the geometry of the loss landscape and improving the generalization of neural network models.

Abstract: Training large neural networks through gradient-based optimization requires
navigating high-dimensional loss landscapes, which often exhibit pathological
geometry, leading to undesirable training dynamics. In particular, poor
generalization frequently results from convergence to sharp minima that are
highly sensitive to input perturbations, causing the model to overfit the
training data while failing to generalize to unseen examples. Furthermore,
these optimization procedures typically display strong dependence on the fine
structure of the loss landscape, leading to unstable training dynamics, due to
the fractal-like nature of the loss surface. In this work, we propose an
alternative optimizer that simultaneously reduces this dependence, and avoids
sharp minima, thereby improving generalization. This is achieved by simulating
the motion of the center of a ball rolling on the loss landscape. The degree to
which our optimizer departs from the standard gradient descent is controlled by
a hyperparameter, representing the radius of the ball. Changing this
hyperparameter allows for probing the loss landscape at different scales,
making it a valuable tool for understanding its geometry.

</details>


### [316] [Minimalist Softmax Attention Provably Learns Constrained Boolean Functions](https://arxiv.org/abs/2505.19531)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Maojiang Su,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: A minimalist single-head softmax-attention mechanism cannot solve k-bit Boolean AND/OR functions alone, but with teacher forcing it can. This highlights the gap between minimal architecture's capabilities with and without ideal supervision.


<details>
  <summary>Details</summary>
Motivation: To explore the computational limits of learning k-bit Boolean functions (AND, OR, and their noisy variants) using a minimalist single-head softmax-attention mechanism.

Method: Investigate the ability of a minimalist single-head softmax-attention mechanism to learn k-bit Boolean functions, comparing its performance with and without teacher forcing.

Result: Simple AND and OR functions are unsolvable by a single-head softmax-attention mechanism alone, but become solvable with teacher forcing. Solving Boolean tasks requires only minimalist attention without deep Transformer blocks or FFNs. One gradient descent update with supervision suffices for solving these tasks.

Conclusion: There is a fundamental gap between what a minimal architecture can achieve under ideal supervision versus standard training.

Abstract: We study the computational limits of learning $k$-bit Boolean functions
(specifically, $\mathrm{AND}$, $\mathrm{OR}$, and their noisy variants), using
a minimalist single-head softmax-attention mechanism, where $k=\Theta(d)$
relevant bits are selected from $d$ inputs. We show that these simple
$\mathrm{AND}$ and $\mathrm{OR}$ functions are unsolvable with a single-head
softmax-attention mechanism alone. However, with teacher forcing, the same
minimalist attention is capable of solving them. These findings offer two key
insights: Architecturally, solving these Boolean tasks requires only minimalist
attention, without deep Transformer blocks or FFNs. Methodologically, one
gradient descent update with supervision suffices and replaces the multi-step
Chain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for
solving Boolean problems. Together, the bounds expose a fundamental gap between
what this minimal architecture achieves under ideal supervision and what is
provably impossible under standard training.

</details>


### [317] [Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning](https://arxiv.org/abs/2505.19532)
*Shijie Liu,Andrew C. Cullen,Paul Montague,Sarah Erfani,Benjamin I. P. Rubinstein*

Main category: cs.LG

TL;DR: The paper introduces SCAB attack which only needs legitimate interactions to poison RL training and shows its high effectiveness, suggesting untrusted RL supply chains are vulnerable.


<details>
  <summary>Details</summary>
Motivation: Current backdoor attacks on RL rely on strong assumptions about attacker's access. This work explores if such strong access assumption is necessary for launching effective backdoor attacks.

Method: Propose SCAB attack targeting RL workflow that trains agents using external ones. The attack poisons a small portion (3%) of training experiences through legitimate interactions with supplied agents.

Result: SCAB attack successfully activates over 90% of triggered actions and reduces average episodic return by 80%, indicating its effectiveness under limited access model.

Conclusion: RL systems are vulnerable to backdoor attacks even when attackers have limited access, highlighting the risks in untrusted RL training supply chains.

Abstract: The current state-of-the-art backdoor attacks against Reinforcement Learning
(RL) rely upon unrealistically permissive access models, that assume the
attacker can read (or even write) the victim's policy parameters, observations,
or rewards. In this work, we question whether such a strong assumption is
required to launch backdoor attacks against RL. To answer this question, we
propose the \underline{S}upply-\underline{C}h\underline{a}in
\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:
training agents using external agents that are provided separately or embedded
within the environment. In contrast to prior works, our attack only relies on
legitimate interactions of the RL agent with the supplied agents. Despite this
limited access model, by poisoning a mere $3\%$ of training experiences, our
attack can successfully activate over $90\%$ of triggered actions, reducing the
average episodic return by $80\%$ for the victim. Our novel attack demonstrates
that RL attacks are likely to become a reality under untrusted RL training
supply-chains.

</details>


### [318] [ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models](https://arxiv.org/abs/2505.19533)
*Yachuan Liu,Xiaochun Wei,Lin Shi,Xinnuo Li,Bohan Zhang,Paramveer Dhillon,Qiaozhu Mei*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在先验推理方面存在显著挑战，本文提出了一种新任务和基准测试以评估LLMs在时间约束下的推理能力，并揭示了其在遵循时间截止点方面的困难。


<details>
  <summary>Details</summary>
Motivation: 尽管有明确的时间限制提示，大型语言模型仍然经常生成受未来事件影响的输出，这表明它们在先验推理方面存在重大挑战。

Method: 引入一个新的任务和基准测试，包括股票预测、维基百科事件预测、科学出版物预测和问答任务，使用泄漏率来量化模型对超出截止时间戳的未来信息的依赖程度。

Result: 实验结果表明，大型语言模型难以在常见的提示策略和任务中始终遵循时间截止点，展示了在先验推理方面持续存在的挑战。

Conclusion: 该基准测试为推动大型语言模型的时间推理能力发展提供了潜在的评估框架，适用于时间敏感型应用。

Abstract: Large language models (LLMs) face significant challenges in ex-ante
reasoning, where analysis, inference, or predictions must be made without
access to information from future events. Even with explicit prompts enforcing
temporal cutoffs, LLMs often generate outputs influenced by internalized
knowledge of events beyond the specified cutoff. This paper introduces a novel
task and benchmark designed to evaluate the ability of LLMs to reason while
adhering to such temporal constraints. The benchmark includes a variety of
tasks: stock prediction, Wikipedia event prediction, scientific publication
prediction, and Question Answering (QA), designed to assess factual knowledge
under temporal cutoff constraints. We use leakage rate to quantify models'
reliance on future information beyond cutoff timestamps. Experimental results
reveal that LLMs struggle to consistently adhere to temporal cutoffs across
common prompting strategies and tasks, demonstrating persistent challenges in
ex-ante reasoning. This benchmark provides a potential evaluation framework to
advance the development of LLMs' temporal reasoning ability for time-sensitive
applications.

</details>


### [319] [Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating](https://arxiv.org/abs/2505.19543)
*Yiyun Zhou,Zheqi Lv,Shengyu Zhang,Jingyuan Chen*

Main category: cs.LG

TL;DR: This paper proposes Cuff-KT to address the Real-time Learning Pattern Adjustment (RLPA) task, improving adaptability of Knowledge Tracing models without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Tracing models lack adaptability for real-time learning pattern adjustments due to irregular changes in learners' abilities and current strategies lead to overfitting and high time costs.

Method: Cuff-KT consists of a controller that assigns value scores to learners and a generator that creates personalized parameters for selected learners, allowing fast and flexible adaptation without fine-tuning.

Result: Experiments on five datasets show Cuff-KT improves performance of five KT models with different structures under intra- and inter-learner shifts, achieving an average relative increase in AUC of 10% and 4%, respectively, with negligible time cost.

Conclusion: Cuff-KT effectively tackles the RLPA task by enhancing adaptability of KT models without the need for retraining.

Abstract: Knowledge Tracing (KT) is a core component of Intelligent Tutoring Systems,
modeling learners' knowledge state to predict future performance and provide
personalized learning support. Traditional KT models assume that learners'
learning abilities remain relatively stable over short periods or change in
predictable ways based on prior performance. However, in reality, learners'
abilities change irregularly due to factors like cognitive fatigue, motivation,
and external stress -- a task introduced, which we refer to as Real-time
Learning Pattern Adjustment (RLPA). Existing KT models, when faced with RLPA,
lack sufficient adaptability, because they fail to timely account for the
dynamic nature of different learners' evolving learning patterns. Current
strategies for enhancing adaptability rely on retraining, which leads to
significant overfitting and high time overhead issues. To address this, we
propose Cuff-KT, comprising a controller and a generator. The controller
assigns value scores to learners, while the generator generates personalized
parameters for selected learners. Cuff-KT controllably adapts to data changes
fast and flexibly without fine-tuning. Experiments on five datasets from
different subjects demonstrate that Cuff-KT significantly improves the
performance of five KT models with different structures under intra- and
inter-learner shifts, with an average relative increase in AUC of 10% and 4%,
respectively, at a negligible time cost, effectively tackling RLPA task. Our
code and datasets are fully available at https://github.com/zyy-2001/Cuff-KT.

</details>


### [320] [STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization](https://arxiv.org/abs/2505.19547)
*Haoyu Zhang,Wentao Zhang,Hao Miao,Xinke Jiang,Yuchen Fang,Yifan Zhang*

Main category: cs.LG

TL;DR: STRAP is a novel framework that improves the generalization of Spatio-Temporal Graph Neural Networks (STGNNs) in out-of-distribution scenarios by integrating retrieval-augmented learning.


<details>
  <summary>Details</summary>
Motivation: STGNNs often fail to generalize in Spatio-Temporal Out-of-Distribution (STOOD) scenarios, where both temporal dynamics and spatial structures evolve beyond the training distribution.

Method: The proposed STRAP framework enhances model generalization by integrating retrieval-augmented learning into the STGNN continue learning pipeline. It includes a compact and expressive pattern library storing representative spatio-temporal patterns enriched with historical, structural, and semantic information. During inference, relevant patterns are retrieved from this library based on similarity to the current input and injected into the model via a plug-and-play prompting mechanism. Additionally, STRAP introduces a knowledge-balancing objective to harmonize new information with retrieved knowledge.

Result: Extensive experiments across multiple real-world streaming graph datasets show that STRAP consistently outperforms state-of-the-art STGNN baselines on STOOD tasks.

Conclusion: STRAP demonstrates its robustness, adaptability, and strong generalization capability without task-specific fine-tuning.

Abstract: Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful
tool for modeling dynamic graph-structured data across diverse domains.
However, they often fail to generalize in Spatio-Temporal Out-of-Distribution
(STOOD) scenarios, where both temporal dynamics and spatial structures evolve
beyond the training distribution. To address this problem, we propose an
innovative Spatio-Temporal Retrieval-Augmented Pattern Learning
framework,STRAP, which enhances model generalization by integrating
retrieval-augmented learning into the STGNN continue learning pipeline. The
core of STRAP is a compact and expressive pattern library that stores
representative spatio-temporal patterns enriched with historical, structural,
and semantic information, which is obtained and optimized during the training
phase. During inference, STRAP retrieves relevant patterns from this library
based on similarity to the current input and injects them into the model via a
plug-and-play prompting mechanism. This not only strengthens spatio-temporal
representations but also mitigates catastrophic forgetting. Moreover, STRAP
introduces a knowledge-balancing objective to harmonize new information with
retrieved knowledge. Extensive experiments across multiple real-world streaming
graph datasets show that STRAP consistently outperforms state-of-the-art STGNN
baselines on STOOD tasks, demonstrating its robustness, adaptability, and
strong generalization capability without task-specific fine-tuning.

</details>


### [321] [On scalable and efficient training of diffusion samplers](https://arxiv.org/abs/2505.19552)
*Minkyu Kim,Kiyoung Seong,Dongyeop Woo,Sungsoo Ahn,Minsu Kim*

Main category: cs.LG

TL;DR: The paper proposes a scalable and sample-efficient framework that combines MCMC samplers with diffusion models to improve sampling in high-dimensional spaces with expensive energy evaluations. It also addresses mode collapse through periodic re-initialization.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle to scale in demanding scenarios where energy evaluations are expensive and the sampling space is high-dimensional.

Method: The method uses Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy as a Searcher to collect off-policy samples, which are then combined with on-policy data to train the diffusion sampler. Additionally, it introduces a periodic re-initialization trick to address primacy bias and prevent mode collapse.

Result: This approach significantly improves sample efficiency on standard benchmarks for diffusion samplers and performs well on higher-dimensional problems and real-world molecular conformer generation.

Conclusion: The proposed framework enhances scalability and sample efficiency of diffusion samplers, making them more applicable to complex, high-dimensional tasks.

Abstract: We address the challenge of training diffusion models to sample from
unnormalized energy distributions in the absence of data, the so-called
diffusion samplers. Although these approaches have shown promise, they struggle
to scale in more demanding scenarios where energy evaluations are expensive and
the sampling space is high-dimensional. To address this limitation, we propose
a scalable and sample-efficient framework that properly harmonizes the powerful
classical sampling method and the diffusion sampler. Specifically, we utilize
Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy
as a Searcher to collect off-policy samples, using an auxiliary energy function
to compensate for exploring modes the diffusion sampler rarely visits. These
off-policy samples are then combined with on-policy data to train the diffusion
sampler, thereby expanding its coverage of the energy landscape. Furthermore,
we identify primacy bias, i.e., the preference of samplers for early experience
during training, as the main cause of mode collapse during training, and
introduce a periodic re-initialization trick to resolve this issue. Our method
significantly improves sample efficiency on standard benchmarks for diffusion
samplers and also excels at higher-dimensional problems and real-world
molecular conformer generation.

</details>


### [322] [Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams](https://arxiv.org/abs/2505.19561)
*Yuan Feng,Yukun Cao,Hairu Wang,Xike Xie,S Kevin Zhou*

Main category: cs.LG

TL;DR: This paper presents Lego sketch, a novel scalable MANN architecture that dynamically coordinates multiple memory bricks to adapt to various space budgets and diverse data domains, offering superior scalability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing neural sketches lack flexibility in scaling across different data domains and space budgets due to inflexible MANN configurations.

Method: The authors introduce the Lego sketch, a new sketch type that uses a scalable MANN architecture. It dynamically coordinates multiple memory bricks to adapt to different conditions, similar to assembling Lego creations with modular bricks.

Result: Theoretical analysis confirms high scalability of the Lego sketch and provides an error bound for neural sketches. Experimental evaluations show it outperforms existing handcrafted and neural sketches in terms of space-accuracy trade-offs.

Conclusion: The Lego sketch offers superior scalability and accuracy, advancing the field of neural sketches.

Abstract: Sketches, probabilistic structures for estimating item frequencies in
infinite data streams with limited space, are widely used across various
domains. Recent studies have shifted the focus from handcrafted sketches to
neural sketches, leveraging memory-augmented neural networks (MANNs) to enhance
the streaming compression capabilities and achieve better space-accuracy
trade-offs.However, existing neural sketches struggle to scale across different
data domains and space budgets due to inflexible MANN configurations. In this
paper, we introduce a scalable MANN architecture that brings to life the {\it
Lego sketch}, a novel sketch with superior scalability and accuracy. Much like
assembling creations with modular Lego bricks, the Lego sketch dynamically
coordinates multiple memory bricks to adapt to various space budgets and
diverse data domains. Our theoretical analysis guarantees its high scalability
and provides the first error bound for neural sketch. Furthermore, extensive
experimental evaluations demonstrate that the Lego sketch exhibits superior
space-accuracy trade-offs, outperforming existing handcrafted and neural
sketches. Our code is available at https://github.com/FFY0/LegoSketch_ICML.

</details>


### [323] [Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing](https://arxiv.org/abs/2505.19578)
*Dan Peng,Zhihui Fu,Zewen Ye,Zhuoran Song,Jun Wang*

Main category: cs.LG

TL;DR: An improved sparse attention mechanism is proposed, which shares precise attention patterns across heads based on inter-head similarity, achieving better speed and accuracy in long-context inference compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing sparse attention methods rely on predefined patterns or inaccurate estimations, failing to fully capture the true dynamics of attention, leading to reduced efficiency and compromised accuracy.

Method: The method exploits the strong inter-head similarity of attention patterns. It strategically shares computed accurate patterns across attention heads, requiring full attention computation for only a small subset of heads.

Result: Comprehensive evaluations show that the approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy.

Conclusion: The proposed sparse attention mechanism provides a more accurate and efficient way to capture the dynamic behavior of attention in long-context inference.

Abstract: Sparse attention methods exploit the inherent sparsity in attention to speed
up the prefilling phase of long-context inference, mitigating the quadratic
complexity of full attention computation. While existing sparse attention
methods rely on predefined patterns or inaccurate estimations to approximate
attention behavior, they often fail to fully capture the true dynamics of
attention, resulting in reduced efficiency and compromised accuracy. Instead,
we propose a highly accurate sparse attention mechanism that shares similar yet
precise attention patterns across heads, enabling a more realistic capture of
the dynamic behavior of attention. Our approach is grounded in two key
observations: (1) attention patterns demonstrate strong inter-head similarity,
and (2) this similarity remains remarkably consistent across diverse inputs. By
strategically sharing computed accurate patterns across attention heads, our
method effectively captures actual patterns while requiring full attention
computation for only a small subset of heads. Comprehensive evaluations
demonstrate that our approach achieves superior or comparable speedup relative
to state-of-the-art methods while delivering the best overall accuracy.

</details>


### [324] [WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts](https://arxiv.org/abs/2505.19587)
*Shadi Alijani,Homayoun Najjaran*

Main category: cs.LG

TL;DR: WQLCP改进了RLSCP，通过结合加权可交换性概念调整校准分位数阈值，从而在分布偏移情况下提高了预测集输出的性能。实验表明，WQLCP能够在保持覆盖率的同时减少预测集大小。


<details>
  <summary>Details</summary>
Motivation: 传统CP方法假设数据可交换，但在实际中存在分布偏移，导致覆盖率不可靠和预测集膨胀。为了解决这个问题，提出了RLSCP和WQLCP两种方法。

Method: RLSCP使用来自VAE的重建损失作为不确定性度量来缩放得分函数；WQLCP则通过结合加权可交换性概念，根据校准和测试损失值的比例调整校准分位数阈值。

Result: WQLCP在大型数据集（如ImageNet变体）上的实验结果表明，它比现有基线方法表现更好，能够持续保持覆盖率并减少预测集大小。

Conclusion: WQLCP提供了一个在分布偏移下稳健的CP解决方案，能够在保证覆盖率的同时优化预测集大小。

Abstract: Conformal prediction (CP) provides a framework for constructing prediction
sets with guaranteed coverage, assuming exchangeable data. However, real-world
scenarios often involve distribution shifts that violate exchangeability,
leading to unreliable coverage and inflated prediction sets. To address this
challenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction
(RLSCP), which utilizes reconstruction losses derived from a Variational
Autoencoder (VAE) as an uncertainty metric to scale score functions. While
RLSCP demonstrates performance improvements, mainly resulting in better
coverage, it quantifies quantiles based on a fixed calibration dataset without
considering the discrepancies between test and train datasets in an
unexchangeable setting. In the next step, we propose Weighted Quantile
Loss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating
a weighted notion of exchangeability, adjusting the calibration quantile
threshold based on weights with respect to the ratio of calibration and test
loss values. This approach improves the CP-generated prediction set outputs in
the presence of distribution shifts. Experiments on large-scale datasets,
including ImageNet variants, demonstrate that WQLCP outperforms existing
baselines by consistently maintaining coverage while reducing prediction set
sizes, providing a robust solution for CP under distribution shifts.

</details>


### [325] [Model Agnostic Differentially Private Causal Inference](https://arxiv.org/abs/2505.19589)
*Christiant Lebeda,Mathieu Even,Aurélien Bellet,Julie Josse*

Main category: cs.LG

TL;DR: The paper presents a model-agnostic framework for differentially private estimation of average treatment effects (ATE), which decouples nuisance estimation from privacy protection, applies to three classical estimators, and maintains competitive performance under realistic privacy budgets.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns are crucial in fields such as medicine, economics, and social sciences when estimating causal effects from observational data. Current methods enforcing differential privacy by privatizing nuisance components lead to a privacy cost scaling with model complexity.

Method: A general, model-agnostic framework is proposed for differentially private ATE estimation. It decouples nuisance estimation from privacy protection, allowing flexible black-box models while achieving differential privacy by perturbing predictions and aggregation steps within a fold-splitting scheme with ensemble techniques.

Result: Empirical results demonstrate that the methods maintain competitive performance under realistic privacy budgets. The framework can also support meta-analysis of multiple private ATE estimates.

Conclusion: This work bridges the gap between causal inference and privacy-preserving data analysis, providing formal utility and privacy guarantees.

Abstract: Estimating causal effects from observational data is essential in fields such
as medicine, economics and social sciences, where privacy concerns are
paramount. We propose a general, model-agnostic framework for differentially
private estimation of average treatment effects (ATE) that avoids strong
structural assumptions on the data-generating process or the models used to
estimate propensity scores and conditional outcomes. In contrast to prior work,
which enforces differential privacy by directly privatizing these nuisance
components and results in a privacy cost that scales with model complexity, our
approach decouples nuisance estimation from privacy protection. This separation
allows the use of flexible, state-of-the-art black-box models, while
differential privacy is achieved by perturbing only predictions and aggregation
steps within a fold-splitting scheme with ensemble techniques. We instantiate
the framework for three classical estimators -- the G-formula, inverse
propensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal
utility and privacy guarantees. Empirical results show that our methods
maintain competitive performance under realistic privacy budgets. We further
extend our framework to support meta-analysis of multiple private ATE
estimates. Our results bridge a critical gap between causal inference and
privacy-preserving data analysis.

</details>


### [326] [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
*Xuandong Zhao,Zhewei Kang,Aosong Feng,Sergey Levine,Dawn Song*

Main category: cs.LG

TL;DR: An RLIF method called Intuitor is proposed, which uses self-certainty as the reward signal for LLMs to learn complex reasoning in an unsupervised way. It performs as well as GRPO on math benchmarks and generalizes better to out-of-domain tasks like code generation without needing gold solutions or test cases.


<details>
  <summary>Details</summary>
Motivation: Training LLMs for complex reasoning via RLVR has been effective but limited due to costly, domain-specific supervision. There's a need for a method that can enable LLMs to learn from intrinsic signals without external rewards or labeled data.

Method: Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, using a model's own confidence as its sole reward signal, thus enabling fully unsupervised learning.

Result: Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks such as code generation without requiring gold solutions or test cases.

Conclusion: Intrinsic model signals can drive effective learning across domains and offer a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable.

Abstract: Training large language models (LLMs) for complex reasoning via Reinforcement
Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on
costly, domain-specific supervision. We explore Reinforcement Learning from
Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic
signals without external rewards or labeled data. We propose Intuitor, an RLIF
method that uses a model's own confidence, termed self-certainty, as its sole
reward signal. Intuitor replaces external rewards in Group Relative Policy
Optimization (GRPO) with self-certainty scores, enabling fully unsupervised
learning. Experiments demonstrate that Intuitor matches GRPO's performance on
mathematical benchmarks while achieving superior generalization to
out-of-domain tasks like code generation, without requiring gold solutions or
test cases. Our findings show that intrinsic model signals can drive effective
learning across domains, offering a scalable alternative to RLVR for autonomous
AI systems where verifiable rewards are unavailable. Code is available at
https://github.com/sunblaze-ucb/Intuitor

</details>


### [327] [Preference Optimization by Estimating the Ratio of the Data Distribution](https://arxiv.org/abs/2505.19601)
*Yeongmin Kim,Heesun Bae,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: Direct Preference Optimization (DPO) is a method for aligning large language models with human preferences. This paper introduces generalized DPO loss and proposes Bregman Preference Optimization (BPO), which subsumes DPO as a special case and offers tractable forms for all instances. Experiments show that BPO improves both win rate and entropy compared with DPO.


<details>
  <summary>Details</summary>
Motivation: To investigate a generalized DPO loss that enables a policy model to match the target policy from a likelihood ratio estimation perspective, providing unique identification of the policy distribution without relying on reward models or partition functions.

Method: Propose Bregman preference optimization (BPO), a generalized framework for ratio matching that provides a family of objective functions achieving target policy optimality. Develop scaled Basu's power divergence (SBA), a gradient scaling method for BPO instances.

Result: In experiments, unlike other probabilistic loss extensions, instances of BPO improve both win rate and entropy compared with DPO. When applied to Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B backbones.

Conclusion: BPO subsumes DPO as a special case and offers tractable forms for all instances, allowing easy implementation. It complements other DPO variants and is applicable to target policies defined by these variants.

Abstract: Direct preference optimization (DPO) is widely used as a simple and stable
method for aligning large language models (LLMs) with human preferences. This
paper investigates a generalized DPO loss that enables a policy model to match
the target policy from a likelihood ratio estimation perspective. The ratio of
the target policy provides a unique identification of the policy distribution
without relying on reward models or partition functions. This allows the
generalized loss to retain both simplicity and theoretical guarantees, which
prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman
preference optimization (BPO), a generalized framework for ratio matching that
provides a family of objective functions achieving target policy optimality.
BPO subsumes DPO as a special case and offers tractable forms for all
instances, allowing implementation with a few lines of code. We further develop
scaled Basu's power divergence (SBA), a gradient scaling method that can be
used for BPO instances. The BPO framework complements other DPO variants and is
applicable to target policies defined by these variants. In experiments, unlike
other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a
trade-off between generation fidelity and diversity, instances of BPO improve
both win rate and entropy compared with DPO. When applied to
Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B
backbones, with a 55.9\% length-controlled win rate on AlpacaEval2.

</details>


### [328] [Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression](https://arxiv.org/abs/2505.19602)
*Kunjun Li,Zigeng Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.LG

TL;DR: Visual Autoregressive (VAR) modeling improves efficiency, scalability and zero-shot generalization but suffers from memory consumption and computational redundancy due to exponential KV cache growth. ScaleKV is introduced as a novel KV cache compression framework for VAR architectures which reduces required KV cache memory to 10% while preserving pixel-level fidelity.


<details>
  <summary>Details</summary>
Motivation: VAR modeling has significant advantages in efficiency, scalability and zero-shot generalization but its coarse-to-fine methodology causes exponential growth of the KV cache during inference leading to high memory consumption and computational redundancy.

Method: ScaleKV categorizes transformer layers into drafters and refiners based on varying cache demands across transformer layers and distinct attention patterns at different scales. Drafters require greater cache capacity while refiners need reduced cache capacity. This allows for differentiated cache management tailored to each scale.

Result: Evaluation on the Infinity text-to-image VAR model family shows that ScaleKV effectively reduces the required KV cache memory to 10% while maintaining pixel-level fidelity.

Conclusion: ScaleKV provides an effective solution to the memory consumption and computational redundancy issues in VAR modeling by significantly reducing KV cache memory requirements without compromising on output quality.

Abstract: Visual Autoregressive (VAR) modeling has garnered significant attention for
its innovative next-scale prediction approach, which yields substantial
improvements in efficiency, scalability, and zero-shot generalization.
Nevertheless, the coarse-to-fine methodology inherent in VAR results in
exponential growth of the KV cache during inference, causing considerable
memory consumption and computational redundancy. To address these bottlenecks,
we introduce ScaleKV, a novel KV cache compression framework tailored for VAR
architectures. ScaleKV leverages two critical observations: varying cache
demands across transformer layers and distinct attention patterns at different
scales. Based on these insights, ScaleKV categorizes transformer layers into
two functional groups: drafters and refiners. Drafters exhibit dispersed
attention across multiple scales, thereby requiring greater cache capacity.
Conversely, refiners focus attention on the current token map to process local
details, consequently necessitating substantially reduced cache capacity.
ScaleKV optimizes the multi-scale inference pipeline by identifying
scale-specific drafters and refiners, facilitating differentiated cache
management tailored to each scale. Evaluation on the state-of-the-art
text-to-image VAR model family, Infinity, demonstrates that our approach
effectively reduces the required KV cache memory to 10% while preserving
pixel-level fidelity.

</details>


### [329] [Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity](https://arxiv.org/abs/2505.19605)
*Aggrey Muhebwa,Khotso Selialia,Fatima Anwar,Khalid K. Osman*

Main category: cs.LG

TL;DR: The paper introduces Kuramoto-FedAvg, a federated optimization algorithm that uses synchronization principles to reduce client drift and accelerate convergence in non-IID settings.


<details>
  <summary>Details</summary>
Motivation: Federated learning struggles with slow convergence when dealing with heterogeneous (non-IID) data due to client drift.

Method: Kuramoto-FedAvg reframes the weight aggregation step as a synchronization problem inspired by the Kuramoto model. The server dynamically weighs each client's update based on its phase alignment with the global update, enhancing aligned contributions and minimizing out-of-phase impacts.

Result: Empirical validation confirms the theoretical findings, demonstrating that Kuramoto-FedAvg significantly accelerates convergence and improves accuracy across multiple benchmark datasets.

Conclusion: The proposed synchronization mechanism reduces client drift and provides a tighter convergence bound compared to standard FedAvg under non-IID data distributions, highlighting the potential of synchronization-based strategies in federated optimization.

Abstract: Federated learning on heterogeneous (non-IID) client data experiences slow
convergence due to client drift. To address this challenge, we propose
Kuramoto-FedAvg, a federated optimization algorithm that reframes the weight
aggregation step as a synchronization problem inspired by the Kuramoto model of
coupled oscillators. The server dynamically weighs each client's update based
on its phase alignment with the global update, amplifying contributions that
align with the global gradient direction while minimizing the impact of updates
that are out of phase. We theoretically prove that this synchronization
mechanism reduces client drift, providing a tighter convergence bound compared
to the standard FedAvg under heterogeneous data distributions. Empirical
validation supports our theoretical findings, showing that Kuramoto-FedAvg
significantly accelerates convergence and improves accuracy across multiple
benchmark datasets. Our work highlights the potential of coordination and
synchronization-based strategies for managing gradient diversity and
accelerating federated optimization in realistic non-IID settings.

</details>


### [330] [Energy-based Preference Optimization for Test-time Adaptation](https://arxiv.org/abs/2505.19607)
*Yewon Han,Seoyun Yang,Taesup Kim*

Main category: cs.LG

TL;DR: The paper introduces EPOTTA, an energy-based preference optimization method for test-time adaptation that achieves computational efficiency and good performance without relying on uncertain predictions or extensive sampling.


<details>
  <summary>Details</summary>
Motivation: Existing Test-Time Adaptation (TTA) methods either depend on uncertain predictions in the absence of label information or require extensive SGLD sampling which is impractical for immediate adaptation.

Method: EPOTTA uses a pretrained model and residual energy function to parameterize the target model, allowing marginal likelihood maximization of target data without sampling. It leverages the mathematical equivalence to DPO objective to adapt the model to a target distribution without explicitly training the residual.

Result: Experiments show that EPOTTA is well-calibrated, performant, and computationally efficient.

Conclusion: EPOTTA offers a promising approach for test-time adaptation by avoiding reliance on uncertain predictions and extensive sampling while maintaining good performance and computational efficiency.

Abstract: Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation
to target distributions that differ from training distributions, improving
real-world generalizability. Existing TTA approaches focus on adjusting the
conditional distribution; however these methods often depend on uncertain
predictions in the absence of label information, leading to unreliable
performance. Energy-based frameworks suggest a promising alternative to address
distribution shifts without relying on uncertain predictions, instead computing
the marginal distribution of target data. However, they involve the critical
challenge of requiring extensive SGLD sampling, which is impractical for
test-time scenarios requiring immediate adaptation. In this work, we propose
Energy-based Preference Optimization for Test-time Adaptation (EPOTTA), which
is based on a sampling free strategy. We first parameterize the target model
using a pretrained model and residual energy function, enabling marginal
likelihood maximization of target data without sampling. Building on the
observation that the parameterization is mathematically equivalent to DPO
objective, we then directly adapt the model to a target distribution without
explicitly training the residual. Our experiments verify that EPOTTA is
well-calibrated and performant while achieving computational efficiency.

</details>


### [331] [Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling](https://arxiv.org/abs/2505.19609)
*Hongtao Xu,Wenting Shen,Yuanxin Wei,Ang Wang,Guo Runfan,Tianxing Wang,Yong Li,Mingzhen Li,Weile Jia*

Main category: cs.LG

TL;DR: Skrull是一种为长上下文监督微调（Long-SFT）设计的动态数据调度器，通过平衡长短序列的计算需求提高训练效率，在实际场景中比DeepSpeed平均快3.76倍（最高达7.54倍）。


<details>
  <summary>Details</summary>
Motivation: 现有的训练系统在处理包含长短序列的混合数据集时，无法同时高效处理两种长度的序列，导致长上下文任务中的整体性能不佳。

Method: 提出了一种名为Skrull的动态数据调度器，将调度过程建模为联合优化问题，并采用轻量级调度算法实现近乎零成本的在线调度。

Result: 实验结果表明，Skrull在真实世界的长上下文微调场景中显著优于DeepSpeed，平均性能提升3.76倍，最高可达7.54倍。

Conclusion: Skrull有效解决了长上下文微调中异构数据分布带来的挑战，显著提高了训练效率。

Abstract: Long-context supervised fine-tuning (Long-SFT) plays a vital role in
enhancing the performance of large language models (LLMs) on long-context
tasks. To smoothly adapt LLMs to long-context scenarios, this process typically
entails training on mixed datasets containing both long and short sequences.
However, this heterogeneous sequence length distribution poses significant
challenges for existing training systems, as they fail to simultaneously
achieve high training efficiency for both long and short sequences, resulting
in sub-optimal end-to-end system performance in Long-SFT. In this paper, we
present a novel perspective on data scheduling to address the challenges posed
by the heterogeneous data distributions in Long-SFT. We propose Skrull, a
dynamic data scheduler specifically designed for efficient long-SFT. Through
dynamic data scheduling, Skrull balances the computation requirements of long
and short sequences, improving overall training efficiency. Furthermore, we
formulate the scheduling process as a joint optimization problem and thoroughly
analyze the trade-offs involved. Based on those analysis, Skrull employs a
lightweight scheduling algorithm to achieve near-zero cost online scheduling in
Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art
distributed training system for LLMs. Experimental results demonstrate that
Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world
long-SFT scenarios.

</details>


### [332] [Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning](https://arxiv.org/abs/2505.19614)
*Sanghyuk Chun*

Main category: cs.LG

TL;DR: The paper emphasizes the importance of addressing multiplicity in multimodal learning, which refers to the many-to-many relationships between modalities. It highlights the negative impacts of ignoring multiplicity and calls for new research directions.


<details>
  <summary>Details</summary>
Motivation: Multiplicity in multimodal relationships is a fundamental issue that current deterministic approaches fail to capture, leading to various problems such as training uncertainty, unreliable evaluation, and low dataset quality.

Method: This position paper discusses the causes and consequences of multiplicity in multimodal learning and advocates for the development of multiplicity-aware learning frameworks and improved dataset construction protocols.

Result: The analysis reveals that multiplicity is a critical bottleneck in multimodal learning affecting all stages of the pipeline, necessitating new research strategies.

Conclusion: To advance multimodal learning, researchers should focus on creating frameworks and protocols that account for the inherent multiplicity in multimodal data.

Abstract: Multimodal learning has seen remarkable progress, particularly with the
emergence of large-scale pre-training across various modalities. However, most
current approaches are built on the assumption of a deterministic, one-to-one
alignment between modalities. This oversimplifies real-world multimodal
relationships, where their nature is inherently many-to-many. This phenomenon,
named multiplicity, is not a side-effect of noise or annotation error, but an
inevitable outcome of semantic abstraction, representational asymmetry, and
task-dependent ambiguity in multimodal tasks. This position paper argues that
multiplicity is a fundamental bottleneck that manifests across all stages of
the multimodal learning pipeline: from data construction to training and
evaluation. This paper examines the causes and consequences of multiplicity,
and highlights how multiplicity introduces training uncertainty, unreliable
evaluation, and low dataset quality. This position calls for new research
directions on multimodal learning: novel multiplicity-aware learning frameworks
and dataset construction protocols considering multiplicity.

</details>


### [333] [Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models](https://arxiv.org/abs/2505.19616)
*Rui Cai,Bangzheng Li,Xiaofei Wen,Muhao Chen,Zhe Zhao*

Main category: cs.LG

TL;DR: Multimodal Large Language Models (MLLMs) face challenges in distinguishing task-relevant signals from irrelevant ones, especially in Visual Question Answering (VQA). This leads to susceptibility to misleading inputs and performance degradation. The paper addresses this Cross-Modality Competency Problem by proposing a novel framework that fine-tunes MLLMs through perturbation-based data augmentations and consistency regularization strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that MLLMs struggle with separating relevant from irrelevant information across modalities, particularly impacting tasks such as VQA, image classification, and pure text question answering. This limitation is referred to as the Cross-Modality Competency Problem and Modality Interference.

Method: The method involves designing a perturbation-based causal diagnostic experiment to verify and quantify the problem of modality interference. To mitigate this issue, the authors propose a framework for fine-tuning MLLMs which includes both heuristic and adversarial perturbations via Projected Gradient Descent (PGD), along with a consistency regularization strategy applied to model outputs with original and perturbed inputs.

Result: Experiments conducted on various benchmark datasets, including those focused on images, text, and VQA, and using multiple model families of different scales, showed significant improvements in robustness and cross-modality competency. The results indicate enhanced unimodal reasoning ability and overall performance on multimodal tasks.

Conclusion: The proposed framework effectively boosts the robustness and cross-modality competency of MLLMs, improving their unimodal reasoning abilities while enhancing performance on multimodal tasks. This advancement addresses the Cross-Modality Competency Problem and mitigates Modality Interference.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across tasks, yet they often exhibit difficulty in distinguishing
task-relevant from irrelevant signals, particularly in tasks like Visual
Question Answering (VQA), which can lead to susceptibility to misleading or
spurious inputs. We refer to this broader limitation as the Cross-Modality
Competency Problem: the model's inability to fairly evaluate all modalities.
This vulnerability becomes more evident in modality-specific tasks such as
image classification or pure text question answering, where models are expected
to rely solely on one modality. In such tasks, spurious information from
irrelevant modalities often leads to significant performance degradation. We
refer to this failure as Modality Interference, which serves as a concrete and
measurable instance of the cross-modality competency problem. We further design
a perturbation-based causal diagnostic experiment to verify and quantify this
problem. To mitigate modality interference, we propose a novel framework to
fine-tune MLLMs, including perturbation-based data augmentations with both
heuristic perturbations and adversarial perturbations via Projected Gradient
Descent (PGD), and a consistency regularization strategy applied to model
outputs with original and perturbed inputs. Experiments on multiple benchmark
datasets (image-heavy, text-heavy, and VQA tasks) and multiple model families
with different scales demonstrate significant improvements in robustness and
cross-modality competency, indicating our method's effectiveness in boosting
unimodal reasoning ability while enhancing performance on multimodal tasks.

</details>


### [334] [SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows](https://arxiv.org/abs/2505.19619)
*Janik Kreit,Dominic Schuh,Kim A. Nicoli,Lena Funcke*

Main category: cs.LG

TL;DR: The paper introduces Symmetry-Enforcing Stochastic Modulation (SESaMo), which incorporates inductive biases like symmetries into normalizing flows through stochastic modulation, enhancing the flexibility of generative models and effectively learning various symmetries.


<details>
  <summary>Details</summary>
Motivation: Deep generative models have shown great potential but face challenges when sampling from unnormalized Boltzmann-like distributions. Incorporating prior knowledge such as symmetries can improve training performance.

Method: SESaMo uses stochastic modulation to incorporate inductive biases (e.g., symmetries) into normalizing flows, increasing the model's flexibility to learn exact and broken symmetries.

Result: Numerical experiments on an 8-Gaussian mixture model and physically relevant field theories ($\phi^4$ theory and the Hubbard model) demonstrate SESaMo's effectiveness.

Conclusion: SESaMo provides a novel approach for incorporating symmetries into generative models, enhancing their ability to learn complex symmetries.

Abstract: Deep generative models have recently garnered significant attention across
various fields, from physics to chemistry, where sampling from unnormalized
Boltzmann-like distributions represents a fundamental challenge. In particular,
autoregressive models and normalizing flows have become prominent due to their
appealing ability to yield closed-form probability densities. Moreover, it is
well-established that incorporating prior knowledge - such as symmetries - into
deep neural networks can substantially improve training performances. In this
context, recent advances have focused on developing symmetry-equivariant
generative models, achieving remarkable results. Building upon these
foundations, this paper introduces Symmetry-Enforcing Stochastic Modulation
(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the
incorporation of inductive biases (e.g., symmetries) into normalizing flows
through a novel technique called stochastic modulation. This approach enhances
the flexibility of the generative model, allowing to effectively learn a
variety of exact and broken symmetries. Our numerical experiments benchmark
SESaMo in different scenarios, including an 8-Gaussian mixture model and
physically relevant field theories, such as the $\phi^4$ theory and the Hubbard
model.

</details>


### [335] [Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs](https://arxiv.org/abs/2505.19620)
*Jiawen Chen,Qi Shao,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出了一种新的时空预测框架STH-SepNet，通过分离时间和空间建模来提高效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有的时空预测方法在处理大规模实际数据集时难以平衡模型表达能力和计算效率。

Method: 使用轻量级大语言模型捕捉时间动态，并采用自适应超图神经网络建模空间交互，同时设计了融合两者的门控机制。

Result: 在多个基准的大规模实际数据集上进行了广泛的实验，证明了STH-SepNet在保持计算效率的同时提高了预测性能。

Conclusion: 该工作提供了一个有前景的轻量级框架，降低了计算需求并增强了预测性能。

Abstract: Spatio-temporal prediction is a pivotal task with broad applications in
traffic management, climate monitoring, energy scheduling, etc. However,
existing methodologies often struggle to balance model expressiveness and
computational efficiency, especially when scaling to large real-world datasets.
To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph
Separation Networks), a novel framework that decouples temporal and spatial
modeling to enhance both efficiency and precision. Therein, the temporal
dimension is modeled using lightweight large language models, which effectively
capture low-rank temporal dynamics. Concurrently, the spatial dimension is
addressed through an adaptive hypergraph neural network, which dynamically
constructs hyperedges to model intricate, higher-order interactions. A
carefully designed gating mechanism is integrated to seamlessly fuse temporal
and spatial representations. By leveraging the fundamental principles of
low-rank temporal dynamics and spatial interactions, STH-SepNet offers a
pragmatic and scalable solution for spatio-temporal prediction in real-world
applications. Extensive experiments on large-scale real-world datasets across
multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting
predictive performance while maintaining computational efficiency. This work
may provide a promising lightweight framework for spatio-temporal prediction,
aiming to reduce computational demands and while enhancing predictive
performance. Our code is avaliable at
https://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.

</details>


### [336] [When fractional quasi p-norms concentrate](https://arxiv.org/abs/2505.19635)
*Ivan Y. Tyukin,Bogdan Grechuk,Evgeny M. Mirkes,Alexander N. Gorban*

Main category: cs.LG

TL;DR: 本研究探讨了高维空间中分数准p-范数的距离集中性问题，明确了其在特定分布下的集中条件，并揭示了通过选择p值控制集中率的可能性。这些发现为数据编码和表示方案的设计提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 距离在高维中的集中性是开发稳定可靠的数据分析算法的重要因素。然而，关于分数准p-范数在高维中的集中性问题一直存在理论和实证争议。

Method: 作者首次确定了分数准p-范数在哪些条件下会集中以及不会集中的条件。他们展示了对于广泛的分布类别，这些范数具有指数级和与p无关的集中界限。同时，指定了可以通过适当选择p来控制集中率的条件和分布族。此外，还发现了一类具有反集中性质的分布。

Result: 研究结果表明，对于某些分布，可以通过调整p值来控制距离集中率。而对于其他分布，则表现出抗集中特性。这为设计数据编码或表示方案提供了依据。

Conclusion: 该研究解决了关于分数准p-范数在高维中的集中性的长期争议，澄清了理论与实证证据之间的矛盾。

Abstract: Concentration of distances in high dimension is an important factor for the
development and design of stable and reliable data analysis algorithms. In this
paper, we address the fundamental long-standing question about the
concentration of distances in high dimension for fractional quasi $p$-norms,
$p\in(0,1)$. The topic has been at the centre of various theoretical and
empirical controversies. Here we, for the first time, identify conditions when
fractional quasi $p$-norms concentrate and when they don't. We show that
contrary to some earlier suggestions, for broad classes of distributions,
fractional quasi $p$-norms admit exponential and uniform in $p$ concentration
bounds. For these distributions, the results effectively rule out previously
proposed approaches to alleviate concentration by "optimal" setting the values
of $p$ in $(0,1)$. At the same time, we specify conditions and the
corresponding families of distributions for which one can still control
concentration rates by appropriate choices of $p$. We also show that in an
arbitrarily small vicinity of a distribution from a large class of
distributions for which uniform concentration occurs, there are uncountably
many other distributions featuring anti-concentration properties. Importantly,
this behavior enables devising relevant data encoding or representation schemes
favouring or discouraging distance concentration. The results shed new light on
this long-standing problem and resolve the tension around the topic in both
theory and empirical evidence reported in the literature.

</details>


### [337] [MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE](https://arxiv.org/abs/2505.19645)
*Zongle Huang,Lei Zhu,Zongyuan Zhan,Ting Hu,Weikai Mao,Xianzhi Yu,Yongpan Liu,Tianyu Zhang*

Main category: cs.LG

TL;DR: Speculative decoding (SD) accelerates MoE models more effectively than dense models, especially at medium batch sizes. A new metric 'target efficiency' is introduced to better understand SD acceleration.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of speculative decoding (SD) on Mixture of Experts (MoE) models and address limitations in current SD research.

Method: Demonstrate that MoE models benefit more from SD than dense models under medium batch sizes, develop a reliable modeling based on theoretical analyses, and introduce the 'target efficiency' metric to characterize effects of workload and model architecture on SD acceleration.

Result: Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate the theoretical predictions.

Conclusion: This work provides a new perspective to speed up MoE inference using SD and helps researchers identify system bottlenecks with the 'target efficiency' metric.

Abstract: Large Language Models (LLMs) have achieved remarkable success across many
applications, with Mixture of Experts (MoE) models demonstrating great
potential. Compared to traditional dense models, MoEs achieve better
performance with less computation. Speculative decoding (SD) is a widely used
technique to accelerate LLM inference without accuracy loss, but it has been
considered efficient only for dense models. In this work, we first demonstrate
that, under medium batch sizes, MoE surprisingly benefits more from SD than
dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in
MoE designs -- the batch size range where SD acceleration is expected to be
effective becomes broader. To quantitatively understand tradeoffs involved in
SD, we develop a reliable modeling based on theoretical analyses. While current
SD research primarily focuses on improving acceptance rates of algorithms,
changes in workload and model architecture can still lead to degraded SD
acceleration even with high acceptance rates. To address this limitation, we
introduce a new metric 'target efficiency' that characterizes these effects,
thus helping researchers identify system bottlenecks and understand SD
acceleration more comprehensively. For scenarios like private serving, this
work unveils a new perspective to speed up MoE inference, where existing
solutions struggle. Experiments on different GPUs show up to 2.29x speedup for
Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.

</details>


### [338] [Energy-based generator matching: A neural sampler for general state space](https://arxiv.org/abs/2505.19646)
*Dongyeop Woo,Minsu Kim,Minkyu Kim,Kiyoung Seong,Sungsoo Ahn*

Main category: cs.LG

TL;DR: The paper introduces Energy-based generator matching (EGM), a method for training generative models from energy functions without data, which can handle various types of Markov processes and data modalities.


<details>
  <summary>Details</summary>
Motivation: To develop a versatile approach that can train generative models directly from energy functions without needing actual data samples, enabling generation across continuous, discrete, and mixed data modalities.

Method: Propose EGM, an extension of generator matching, to train arbitrary continuous-time Markov processes using self-normalized importance sampling with a bootstrapping trick to reduce variance in the importance weight.

Result: EGM successfully validated on both discrete and multimodal tasks up to 100 and 20 dimensions respectively.

Conclusion: EGM is a modality-agnostic approach capable of training generative models from energy functions without data, effectively generating data across different modalities.

Abstract: We propose Energy-based generator matching (EGM), a modality-agnostic
approach to train generative models from energy functions in the absence of
data. Extending the recently proposed generator matching, EGM enables training
of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump,
and can generate data from continuous, discrete, and a mixture of two
modalities. To this end, we propose estimating the generator matching loss
using self-normalized importance sampling with an additional bootstrapping
trick to reduce variance in the importance weight. We validate EGM on both
discrete and multimodal tasks up to 100 and 20 dimensions, respectively.

</details>


### [339] [Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling](https://arxiv.org/abs/2505.19669)
*Haiyang Sun,Shujie Hu,Shujie Liu,Lingwei Meng,Hui Wang,Bing Han,Yifan Yang,Yanqing Liu,Sheng Zhao,Yan Lu,Yanmin Qian*

Main category: cs.LG

TL;DR: This paper presents SMLLE, a zero-shot streaming text-to-speech framework that generates high-quality speech with minimal delay and comparable performance to sentence-level TTS systems.


<details>
  <summary>Details</summary>
Motivation: Existing methods for streaming text-to-speech primarily use a lookahead mechanism which introduces high processing latency. The need for a more efficient and low-latency method motivates this research.

Method: SMLLE employs a Transducer to convert text into semantic tokens in real time while obtaining duration alignment information. These outputs are fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. A Delete < Bos > Mechanism is designed to stabilize the generation process with minimal delay.

Result: Experimental results show that SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems.

Conclusion: SMLLE is an effective framework for zero-shot streaming text-to-speech, providing high-quality speech generation with low latency.

Abstract: Zero-shot streaming text-to-speech is an important research topic in
human-computer interaction. Existing methods primarily use a lookahead
mechanism, relying on future text to achieve natural streaming speech
synthesis, which introduces high processing latency. To address this issue, we
propose SMLLE, a streaming framework for generating high-quality speech
frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens
in real time while simultaneously obtaining duration alignment information. The
combined outputs are then fed into a fully autoregressive (AR) streaming model
to reconstruct mel-spectrograms. To further stabilize the generation process,
we design a Delete < Bos > Mechanism that allows the AR model to access future
text introducing as minimal delay as possible. Experimental results suggest
that the SMLLE outperforms current streaming TTS methods and achieves
comparable performance over sentence-level TTS systems. Samples are available
on https://anonymous.4open.science/w/demo_page-48B7/.

</details>


### [340] [Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning](https://arxiv.org/abs/2505.19680)
*Xinrui Wang,Shao-yuan Li,Jiaqiang Zhang,Songcan Chen*

Main category: cs.LG

TL;DR: The paper addresses the challenges of Multi-Label Online Continual Learning (MOCL) by proposing CUTER, a strategy that leverages label-specific region identifying and feature learning to tackle catastrophic forgetting, missing labels, and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Existing MOCL methods fail to address label-specific region identifying and feature learning, which is crucial for multi-label learning but difficult in an online setting with partial supervision.

Method: CUTER (CUT-out-and-Experience-Replay) is proposed. It uses inherent structural information of input data to evaluate pre-trained models' localization capability and provides fine-grained supervision signals by identifying, strengthening, and cutting out label-specific regions for efficient experience replay.

Result: Extensive experiments on multiple multi-label image benchmarks show the superiority of CUTER in addressing catastrophic forgetting, missing labels, and class imbalance.

Conclusion: CUTER is a versatile strategy that effectively tackles the challenges of MOCL and can be integrated with existing approaches.

Abstract: Multi-Label Online Continual Learning (MOCL) requires models to learn
continuously from endless multi-label data streams, facing complex challenges
including persistent catastrophic forgetting, potential missing labels, and
uncontrollable imbalanced class distributions. While existing MOCL methods
attempt to address these challenges through various techniques, \textit{they
all overlook label-specific region identifying and feature learning} - a
fundamental solution rooted in multi-label learning but challenging to achieve
in the online setting with incremental and partial supervision. To this end, we
first leverage the inherent structural information of input data to evaluate
and verify the innate localization capability of different pre-trained models.
Then, we propose CUTER (CUT-out-and-Experience-Replay), a simple yet versatile
strategy that provides fine-grained supervision signals by further identifying,
strengthening and cutting out label-specific regions for efficient experience
replay. It not only enables models to simultaneously address catastrophic
forgetting, missing labels, and class imbalance challenges, but also serves as
an orthogonal solution that seamlessly integrates with existing approaches.
Extensive experiments on multiple multi-label image benchmarks demonstrate the
superiority of our proposed method. The code is available at
\href{https://github.com/wxr99/Cut-Replay}{https://github.com/wxr99/Cut-Replay}

</details>


### [341] [Deep Actor-Critics with Tight Risk Certificates](https://arxiv.org/abs/2505.19682)
*Bahareh Tasdighi,Manuel Haussmann,Yi-Shan Wu,Andres R. Masegosa,Melih Kandemir*

Main category: cs.LG

TL;DR: Deep actor-critic algorithms' risk of malfunction can be quantified using tight risk certificates derived from minimal evaluation data and PAC-Bayes theory, making them potentially practical for physical systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of validation schemes that quantify the risk of malfunction in deep actor-critic algorithms, which hinders their deployment in physical systems.

Method: Developing tight risk certificates by leveraging minimal evaluation data collected from pretrained policies and adapting PAC-Bayes theory. Specifically, adopting a recursive PAC-Bayes approach to split validation data into portions, recursively building PAC-Bayes bounds on excess loss with data-informed priors.

Result: Empirical results across multiple locomotion tasks and policy expertise levels show risk certificates tight enough for practical consideration.

Conclusion: It is possible to develop tight risk certificates for deep actor-critic algorithms, potentially enabling their wider adoption in physical systems.

Abstract: After a period of research, deep actor-critic algorithms have reached a level
where they influence our everyday lives. They serve as the driving force behind
the continual improvement of large language models through user-collected
feedback. However, their deployment in physical systems is not yet widely
adopted, mainly because no validation scheme that quantifies their risk of
malfunction. We demonstrate that it is possible to develop tight risk
certificates for deep actor-critic algorithms that predict generalization
performance from validation-time observations. Our key insight centers on the
effectiveness of minimal evaluation data. Surprisingly, a small feasible of
evaluation roll-outs collected from a pretrained policy suffices to produce
accurate risk certificates when combined with a simple adaptation of PAC-Bayes
theory. Specifically, we adopt a recently introduced recursive PAC-Bayes
approach, which splits validation data into portions and recursively builds
PAC-Bayes bounds on the excess loss of each portion's predictor, using the
predictor from the previous portion as a data-informed prior. Our empirical
results across multiple locomotion tasks and policy expertise levels
demonstrate risk certificates that are tight enough to be considered for
practical use.

</details>


### [342] [Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation](https://arxiv.org/abs/2505.19685)
*Victor M. Tenorio,Nicolas Zilberstein,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: Diffusion models are powerful for graph generation, but conditional graph generation is challenging due to discrete and combinatorial nature of graphs. This paper proposes Graph Guided Diffusion (GGDiff), a novel framework that interprets conditional diffusion on graphs as a stochastic control problem. GGDiff unifies multiple guidance strategies and enables zero-shot guidance of pre-trained diffusion models under both differentiable and non-differentiable reward functions.


<details>
  <summary>Details</summary>
Motivation: Conditional graph generation using diffusion models is difficult because gradient-based methods are unsuitable for the discrete and combinatorial nature of graphs, and non-differentiable rewards further complicate the process.

Method: The proposed method, Graph Guided Diffusion (GGDiff), interprets conditional diffusion on graphs as a stochastic control problem. It unifies multiple guidance strategies including gradient-based guidance, control-based guidance, and zero-order approximations.

Result: Experiments demonstrate the efficacy of GGDiff in various tasks such as constraints on graph motifs, fairness, and link prediction. The model achieves superior alignment with target rewards while maintaining diversity and fidelity.

Conclusion: GGDiff provides a comprehensive, plug-and-play framework for conditional graph generation that balances computational efficiency, reward alignment, and sample quality.

Abstract: Diffusion models have emerged as powerful generative models for graph
generation, yet their use for conditional graph generation remains a
fundamental challenge. In particular, guiding diffusion models on graphs under
arbitrary reward signals is difficult: gradient-based methods, while powerful,
are often unsuitable due to the discrete and combinatorial nature of graphs,
and non-differentiable rewards further complicate gradient-based guidance. We
propose Graph Guided Diffusion (GGDiff), a novel guidance framework that
interprets conditional diffusion on graphs as a stochastic control problem to
address this challenge. GGDiff unifies multiple guidance strategies, including
gradient-based guidance (for differentiable rewards), control-based guidance
(using control signals from forward reward evaluations), and zero-order
approximations (bridging gradient-based and gradient-free optimization). This
comprehensive, plug-and-play framework enables zero-shot guidance of
pre-trained diffusion models under both differentiable and non-differentiable
reward functions, adapting well-established guidance techniques to graph
generation--a direction largely unexplored. Our formulation balances
computational efficiency, reward alignment, and sample quality, enabling
practical conditional generation across diverse reward types. We demonstrate
the efficacy of GGDiff in various tasks, including constraints on graph motifs,
fairness, and link prediction, achieving superior alignment with target rewards
while maintaining diversity and fidelity.

</details>


### [343] [JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning](https://arxiv.org/abs/2505.19698)
*Jing Yu Lim,Zarif Ikram,Samson Yu,Haozhe Ma,Tze-Yun Leong,Dianbo Liu*

Main category: cs.LG

TL;DR: Recent advances in MBRL have achieved super-human level performance on Atari100k, but there is a major performance asymmetry between different tasks. This paper proposes JEDI to address this issue.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address the pronounced asymmetry observed in pixel-based agents trained with diffusion world models and rethink what it truly means to cross human-level performance in Atari100k.

Method: The method proposed in this paper is Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective.

Result: JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline.

Conclusion: This work rethinks what it truly means to cross human-level performance in Atari100k and proposes a new model that addresses the performance asymmetry.

Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved
super-human level performance on the Atari100k benchmark, driven by
reinforcement learning agents trained on powerful diffusion world models.
However, we identify that the current aggregates mask a major performance
asymmetry: MBRL agents dramatically outperform humans in some tasks despite
drastically underperforming in others, with the former inflating the aggregate
metrics. This is especially pronounced in pixel-based agents trained with
diffusion world models. In this work, we address the pronounced asymmetry
observed in pixel-based agents as an initial attempt to reverse the worrying
upward trend observed in them. We address the problematic aggregates by
delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal
importance on metrics from both sets. Next, we hypothesize this pronounced
asymmetry is due to the lack of temporally-structured latent space trained with
the World Model objective in pixel-based methods. Lastly, to address this
issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion
world model trained end-to-end with the self-consistency objective. JEDI
outperforms SOTA models in human-optimal tasks while staying competitive across
the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the
latest pixel-based diffusion baseline. Overall, our work rethinks what it truly
means to cross human-level performance in Atari100k.

</details>


### [344] [Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments](https://arxiv.org/abs/2505.19699)
*Junming Liu,Yanting Gao,Siyuan Meng,Yifei Sun,Aoqi Wu,Yufei Jin,Yirong Chen,Ding Wang,Guosun Zeng*

Main category: cs.LG

TL;DR: Mosaic is a data-free knowledge distillation framework for Federated Learning that uses local generative models and a Mixture-of-Experts approach to improve global model performance in heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges due to model and data heterogeneity, which leads to inconsistent representations and optimization dynamics across clients, thus hindering robust global performance.

Method: Mosaic trains local generative models to approximate personalized distributions for synthetic data generation. It then forms a Mixture-of-Experts from client models and distills it into a global model using the generated data. A lightweight meta model is used to integrate expert predictions.

Result: Extensive experiments on standard image classification benchmarks show that Mosaic consistently outperforms state-of-the-art approaches under both model and data heterogeneity.

Conclusion: Mosaic addresses the challenges of model and data heterogeneity in Federated Learning by using a novel data-free knowledge distillation framework, demonstrating superior performance.

Abstract: Federated Learning (FL) is a decentralized machine learning paradigm that
enables clients to collaboratively train models while preserving data privacy.
However, the coexistence of model and data heterogeneity gives rise to
inconsistent representations and divergent optimization dynamics across
clients, ultimately hindering robust global performance. To transcend these
challenges, we propose Mosaic, a novel data-free knowledge distillation
framework tailored for heterogeneous distributed environments. Mosaic first
trains local generative models to approximate each client's personalized
distribution, enabling synthetic data generation that safeguards privacy
through strict separation from real data. Subsequently, Mosaic forms a
Mixture-of-Experts (MoE) from client models based on their specialized
knowledge, and distills it into a global model using the generated data. To
further enhance the MoE architecture, Mosaic integrates expert predictions via
a lightweight meta model trained on a few representative prototypes. Extensive
experiments on standard image classification benchmarks demonstrate that Mosaic
consistently outperforms state-of-the-art approaches under both model and data
heterogeneity. The source code has been published at
https://github.com/Wings-Of-Disaster/Mosaic.

</details>


### [345] [On the Relation between Rectified Flows and Optimal Transport](https://arxiv.org/abs/2505.19712)
*Johannes Hertrich,Antonin Chambolle,Julie Delon*

Main category: cs.LG

TL;DR: This paper explores the relationship among rectified flows, flow matching, and optimal transport. It provides invariance properties of rectified flows, explicit velocity fields, constructions in Gaussian settings, and challenges recent claims about rectified flows solving optimal transport problems.


<details>
  <summary>Details</summary>
Motivation: To understand the connections between rectified flows, flow matching, and optimal transport, and to evaluate the validity of recent claims that rectified flows can solve optimal transport problems when constrained as gradients.

Method: Investigating invariance properties and explicit velocity fields of rectified flows, providing constructions in Gaussian settings, and analyzing the relation to optimal transport. Also, studying the existence of solutions for rectified flows under gradient constraints and comparing them with optimal transport solutions.

Result: Provided invariance properties and explicit constructions for rectified flows. Demonstrated that earlier equivalence results between rectified flows and optimal transport are invalid due to stronger assumptions required. Argued that enforcing a gradient constraint on rectified flows is not a reliable method for computing optimal transport maps.

Conclusion: Rectified flows have specific invariance properties and constructions, but their connection to optimal transport is weaker than previously thought. Enforcing a gradient constraint does not reliably compute optimal transport maps.

Abstract: This paper investigates the connections between rectified flows, flow
matching, and optimal transport. Flow matching is a recent approach to learning
generative models by estimating velocity fields that guide transformations from
a source to a target distribution. Rectified flow matching aims to straighten
the learned transport paths, yielding more direct flows between distributions.
Our first contribution is a set of invariance properties of rectified flows and
explicit velocity fields. In addition, we also provide explicit constructions
and analysis in the Gaussian (not necessarily independent) and Gaussian mixture
settings and study the relation to optimal transport. Our second contribution
addresses recent claims suggesting that rectified flows, when constrained such
that the learned velocity field is a gradient, can yield (asymptotically)
solutions to optimal transport problems. We study the existence of solutions
for this problem and demonstrate that they only relate to optimal transport
under assumptions that are significantly stronger than those previously
acknowledged. In particular, we present several counter-examples that
invalidate earlier equivalence results in the literature, and we argue that
enforcing a gradient constraint on rectified flows is, in general, not a
reliable method for computing optimal transport maps.

</details>


### [346] [OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction](https://arxiv.org/abs/2505.19719)
*Juntong Wang,Xiyuan Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: The paper presents Orthogonal Common Neighbor (OCN), a new approach that improves link prediction by addressing redundancy and over-smoothing in high-order common neighbors, outperforming baselines by 7.7%.


<details>
  <summary>Details</summary>
Motivation: Common Neighbors (CNs) are important features for link prediction but existing methods have issues with redundancy and over-smoothing in higher-order CNs which limits their performance.

Method: The authors use orthogonalization to remove redundancy between different-order CNs and normalization to reduce over-smoothing. These techniques are combined into the OCN method.

Result: OCN outperforms the strongest baselines by an average of 7.7% on popular link prediction benchmarks. Theoretical analysis and ablation studies support the effectiveness of the method.

Conclusion: OCN is a novel approach that significantly enhances link prediction by effectively dealing with redundancy and over-smoothing.

Abstract: Common Neighbors (CNs) and their higher-order variants are important pairwise
features widely used in state-of-the-art link prediction methods. However,
existing methods often struggle with the repetition across different orders of
CNs and fail to fully leverage their potential. We identify that these
limitations stem from two key issues: redundancy and over-smoothing in
high-order common neighbors. To address these challenges, we design
orthogonalization to eliminate redundancy between different-order CNs and
normalization to mitigate over-smoothing. By combining these two techniques, we
propose Orthogonal Common Neighbor (OCN), a novel approach that significantly
outperforms the strongest baselines by an average of 7.7% on popular link
prediction benchmarks. A thorough theoretical analysis is provided to support
our method. Ablation studies also verify the effectiveness of our
orthogonalization and normalization techniques.

</details>


### [347] [Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data](https://arxiv.org/abs/2505.19740)
*Weichen Si,Yihao Ou,Zhen Tian*

Main category: cs.LG

TL;DR: A machine learning method named DeepSeqDenoise is proposed for noise reduction and feature extraction in gene sequencing. It combines CNN and RNN, improves signal-to-noise ratio by 9.4 dB, identifies 17 key features, predicts disease-causing genes with 94.3% accuracy, finds 57 new candidate genes and 3 missed variants.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective noise reduction and feature extraction methods in gene sequencing to support accurate diagnosis of genetic diseases.

Method: The DeepSeqDenoise algorithm which integrates CNN and RNN is developed for removing sequencing noise. Feature engineering is used to screen key features and an integrated learning model is constructed for predicting disease-causing genes.

Result: Improved the signal-to-noise ratio by 9.4 dB, identified 17 key features, achieved 94.3% prediction accuracy, found 57 new candidate disease-causing genes and detected 3 missed variants.

Conclusion: DeepSeqDenoise significantly outperforms existing tools and provides strong support for accurate diagnosis of genetic diseases.

Abstract: In this study, we propose a machine learning-based method for noise reduction
and disease-causing gene feature extraction in gene sequencing DeepSeqDenoise
algorithm combines CNN and RNN to effectively remove the sequencing noise, and
improves the signal-to-noise ratio by 9.4 dB. We screened 17 key features by
feature engineering, and constructed an integrated learning model to predict
disease-causing genes with 94.3% accuracy. We successfully identified 57 new
candidate disease-causing genes in a cardiovascular disease cohort validation,
and detected 3 missed variants in clinical applications. The method
significantly outperforms existing tools and provides strong support for
accurate diagnosis of genetic diseases.

</details>


### [348] [Discrete Markov Bridge](https://arxiv.org/abs/2505.19752)
*Hengli Li,Yuxuan Wang,Song-Chun Zhu,Ying Nian Wu,Zilong Zheng*

Main category: cs.LG

TL;DR: Discrete Markov Bridge is a new framework for discrete representation learning that improves upon existing methods by enhancing latent expressiveness and expanding design space, showing strong performance on Text8 and CIFAR-10 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing discrete data modeling methods rely on a fixed rate transition matrix which limits the expressiveness of latent representations and constrains the overall design space.

Method: The Discrete Markov Bridge framework consists of two key components: Matrix Learning and Score Learning. It provides formal performance guarantees for Matrix Learning and proves convergence of the overall framework.

Result: The proposed model achieves an Evidence Lower Bound (ELBO) of 1.38 on the Text8 dataset and shows competitive performance on the CIFAR-10 dataset.

Conclusion: Discrete Markov Bridge effectively addresses the limitations of existing discrete data modeling methods and demonstrates superior performance in discrete representation learning.

Abstract: Discrete diffusion has recently emerged as a promising paradigm in discrete
data modeling. However, existing methods typically rely on a fixed rate
transition matrix during training, which not only limits the expressiveness of
latent representations, a fundamental strength of variational methods, but also
constrains the overall design space. To address these limitations, we propose
Discrete Markov Bridge, a novel framework specifically designed for discrete
representation learning. Our approach is built upon two key components: Matrix
Learning and Score Learning. We conduct a rigorous theoretical analysis,
establishing formal performance guarantees for Matrix Learning and proving the
convergence of the overall framework. Furthermore, we analyze the space
complexity of our method, addressing practical constraints identified in prior
studies. Extensive empirical evaluations validate the effectiveness of the
proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)
of 1.38 on the Text8 dataset, outperforming established baselines. Moreover,
the proposed model demonstrates competitive performance on the CIFAR-10
dataset, achieving results comparable to those obtained by image-specific
generation approaches.

</details>


### [349] [Unfolding AlphaFold's Bayesian Roots in Probability Kinematics](https://arxiv.org/abs/2505.19763)
*Thomas Hamelryck,Kanti V. Mardia*

Main category: cs.LG

TL;DR: The paper reinterprets AlphaFold1's learned potential energy function as probability kinematics, a form of generalized Bayesian updating. This theoretical contribution connects AlphaFold1 to well-justified Bayesian methods.


<details>
  <summary>Details</summary>
Motivation: To provide a novel theoretical interpretation of AlphaFold1's learned potential energy function and connect it to probability kinematics.

Method: Reinterpret AlphaFold1's potential as an instance of probability kinematics, analyze a synthetic 2D model with an angular random walk prior updated with evidence on distances via probability kinematics.

Result: Confirms the probabilistic framework's scope and precision by successfully analyzing the synthetic 2D model, showing AlphaFold1's potential as a form of generalized Bayesian updating.

Conclusion: AlphaFold1's potential is best understood as a form of generalized Bayesian updating rather than a thermodynamic potential, connecting it to a broader class of Bayesian methods.

Abstract: We present a novel theoretical interpretation of AlphaFold1. The seminal
breakthrough of AlphaFold1 in protein structure prediction by deep learning
relied on a learned potential energy function, in contrast to the later
end-to-end architectures of AlphaFold2 and AlphaFold3. While this potential was
originally justified by referring to physical potentials of mean force (PMFs),
we reinterpret AlphaFold1's potential as an instance of probability kinematics
- also known as Jeffrey conditioning - a principled but underrecognised
generalization of conventional Bayesian updating. Probability kinematics
accommodates uncertain or soft evidence in the form of updated probabilities
over a partition. This perspective reveals AlphaFold1's potential as a form of
generalized Bayesian updating, rather than a thermodynamic potential. To
confirm our probabilistic framework's scope and precision, we analyze a
synthetic 2D model in which an angular random walk prior is updated with
evidence on distances via probability kinematics, mirroring AlphaFold1's
approach. This theoretical contribution connects AlphaFold1 to a broader class
of well-justified Bayesian methods, allowing precise quantification, surpassing
merely qualitative heuristics based on PMFs. More broadly, given the
achievements of AlphaFold1, probability kinematics holds considerable promise
for probabilistic deep learning, as it allows for the formulation of complex
models from a few simpler components.

</details>


### [350] [Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding](https://arxiv.org/abs/2505.19764)
*Patara Trirat,Wonyong Jeong,Sung Ju Hwang*

Main category: cs.LG

TL;DR: Large language models (LLMs) show great ability, but optimizing LLM-based systems is difficult. This paper introduces Agentic Predictor, a lightweight tool for evaluating agentic workflows. It uses multi-view workflow encoding and cross-domain unsupervised pretraining to achieve high accuracy and reduce evaluations needed. Experiments show it performs better than current methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing LLM-based agentic systems due to the large search space of configurations, prompting strategies, and communication patterns.

Method: Propose Agentic Predictor with multi-view workflow encoding technique incorporating code architecture, textual prompts, and interaction graph features, and use cross-domain unsupervised pretraining to reduce required workflow evaluations.

Result: Experiments on a benchmark across three domains indicate that Agentic Predictor outperforms state-of-the-art methods in predictive accuracy and workflow utility.

Conclusion: Agentic Predictor shows potential in streamlining the design of LLM-based agentic workflows.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but optimizing LLM-based agentic systems remains challenging due
to the vast search space of agent configurations, prompting strategies, and
communication patterns. Existing approaches often rely on heuristic-based
tuning or exhaustive evaluation, which can be computationally expensive and
suboptimal. This paper proposes Agentic Predictor, a lightweight predictor for
efficient agentic workflow evaluation. Agentic Predictor is equipped with a
multi-view workflow encoding technique that leverages multi-view representation
learning of agentic systems by incorporating code architecture, textual
prompts, and interaction graph features. To achieve high predictive accuracy
while significantly reducing the number of required workflow evaluations for
training a predictor, Agentic Predictor employs cross-domain unsupervised
pretraining. By learning to approximate task success rates, Agentic Predictor
enables fast and accurate selection of optimal agentic workflow configurations
for a given task, significantly reducing the need for expensive trial-and-error
evaluations. Experiments on a carefully curated benchmark spanning three
domains show that our predictor outperforms state-of-the-art methods in both
predictive accuracy and workflow utility, highlighting the potential of
performance predictors in streamlining the design of LLM-based agentic
workflows.

</details>


### [351] [Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO](https://arxiv.org/abs/2505.19770)
*Ruizhe Shi,Minhak Song,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: This paper conducts a fine-grained theoretical analysis of the performance gap between reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), decomposing the gap into explicit and implicit representation gaps, and providing insights on when each method is preferred.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the reasons behind the performance differences between RLHF and DPO, and to provide practical guidance for choosing between these methods under different conditions.

Method: The study uses theoretical analysis to break down the performance gap into two components: explicit representation gap in exact optimization and implicit representation gap in finite samples. It also compares the relative capacities of reward and policy model classes in influencing final policy quality.

Result: Key findings include that online DPO can outperform both RLHF and standard DPO under certain conditions, and that RLHF requires fewer samples than DPO in approximate optimization settings with implicitly sparse ground-truth rewards.

Conclusion: The results give a comprehensive understanding of the performance gap between RLHF and DPO across various settings, offering practical insights into which method is preferable under specific circumstances.

Abstract: We present a fine-grained theoretical analysis of the performance gap between
reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) under a representation gap. Our study decomposes this gap
into two sources: an explicit representation gap under exact optimization and
an implicit representation gap under finite samples. In the exact optimization
setting, we characterize how the relative capacities of the reward and policy
model classes influence the final policy qualities. We show that RLHF, DPO, or
online DPO can outperform one another depending on the type of model
mis-specifications. Notably, online DPO can outperform both RLHF and standard
DPO when the reward and policy model classes are isomorphic and both
mis-specified. In the approximate optimization setting, we provide a concrete
construction where the ground-truth reward is implicitly sparse and show that
RLHF requires significantly fewer samples than DPO to recover an effective
reward model -- highlighting a statistical advantage of two-stage learning.
Together, these results provide a comprehensive understanding of the
performance gap between RLHF and DPO under various settings, and offer
practical insights into when each method is preferred.

</details>


### [352] [MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support](https://arxiv.org/abs/2505.19785)
*Qianyi Xu,Gousia Habib,Dilruk Perera,Mengling Feng*

Main category: cs.LG

TL;DR: 提出MedDreamer，一种基于模型的强化学习框架，用于个性化治疗推荐。它通过潜在想象模拟可能的患者轨迹，增强学习过程，并在临床结果和离线策略评估指标上优于无模型和其他有模型的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的决策支持系统在处理不规则、稀疏和嘈杂的临床数据时存在缺陷，如时间动态失真和决策质量下降等问题。此外，大多数治疗推荐系统在样本效率、数据质量和泛化能力方面表现不佳。

Method: MedDreamer采用两阶段基于模型的强化学习框架，包含一个带有自适应特征集成（AFI）模块的世界模型，以有效建模不规则、稀疏的临床数据。通过潜在想象，模拟合理的患者轨迹，结合真实和想象的经验来优化策略。这是首次将潜在想象应用于不规则医疗数据。

Result: 在败血症和机械通气治疗的评估中，使用两个大规模电子健康记录数据集，MedDreamer在临床结果和离线策略评估指标上均优于无模型和基于模型的基线方法。

Conclusion: MedDreamer展示了其在个性化治疗推荐中的潜力，尤其是在处理不规则临床数据和超越历史次优决策方面的优势。

Abstract: Timely and personalized treatment decisions are essential across a wide range
of healthcare settings where patient responses vary significantly and evolve
over time. Clinical data used to support these decisions are often irregularly
sampled, sparse, and noisy. Existing decision support systems commonly rely on
discretization and imputation, which can distort critical temporal dynamics and
degrade decision quality. Moreover, they often overlook the clinical
significance of irregular recording frequencies, filtering out patterns in how
and when data is collected. Reinforcement Learning (RL) is a natural fit for
clinical decision-making, enabling sequential, long-term optimization in
dynamic, uncertain environments. However, most existing treatment
recommendation systems are model-free and trained solely on offline data,
making them sample-inefficient, sensitive to data quality, and poorly
generalizable across tasks or cohorts. To address these limitations, we propose
MedDreamer, a two-phase model-based RL framework for personalized treatment
recommendation. MedDreamer uses a world model with an Adaptive Feature
Integration (AFI) module to effectively model irregular, sparse clinical data.
Through latent imagination, it simulates plausible patient trajectories to
enhance learning, refining its policy using a mix of real and imagined
experiences. This enables learning policies that go beyond suboptimal
historical decisions while remaining grounded in clinical data. To our
knowledge, this is the first application of latent imagination to irregular
healthcare data. Evaluations on sepsis and mechanical ventilation (MV)
treatment using two large-scale EHR datasets show that MedDreamer outperforms
both model-free and model-based baselines in clinical outcomes and off-policy
metrics.

</details>


### [353] [What Can RL Bring to VLA Generalization? An Empirical Study](https://arxiv.org/abs/2505.19789)
*Jijia Liu,Feng Gao,Bingwen Wei,Xinlei Chen,Qingmin Liao,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: 大型视觉-语言动作(VLA)模型在具身AI方面展现出显著潜力。然而，主要通过监督微调(SFT)进行训练限制了其泛化能力，因为其在分布变化下容易出现误差累积。强化学习(RL)提供了一种通过试错优化任务目标的途径，但对其在VLA中的具体泛化优势与SFT相比尚缺乏系统理解。本研究引入了一个全面的基准来评估VLA泛化，并系统地研究了RL微调在不同视觉、语义和执行维度上的影响。广泛实验表明，特别是使用PPO的RL微调，在语义理解和执行鲁棒性方面的泛化能力显著优于SFT，同时保持相当的视觉鲁棒性。我们发现PPO是比DPO和GRPO等LLM衍生方法更有效的RL算法。我们还开发了一个用于VLA高效PPO训练的简单方案，并展示了其在提高VLA泛化能力方面的实际用途。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言动作(VLA)模型主要依赖监督微调(SFT)，这导致其在面对分布变化时易出现误差累积，从而限制了泛化能力。尽管强化学习(RL)能够通过试错优化任务目标，帮助克服这些局限，但对RL在VLA中的具体泛化优势与SFT相比尚缺乏系统研究。因此，需要引入一个综合基准来评估VLA泛化，并深入探究RL微调的影响。

Method: 本研究首先设计了一个全面的基准，用于评估视觉-语言动作(VLA)模型的泛化能力。然后，通过在不同的视觉、语义和执行维度上系统地应用强化学习(RL)微调，比较了RL与监督微调(SFT)的泛化效果。特别关注了PPO、DPO和GRPO等不同RL算法的表现。最后，开发了一个用于高效PPO训练的简单方案，并验证了其在提升VLA泛化能力方面的实用性。

Result: 实验结果表明，强化学习(RL)微调，尤其是使用PPO时，在语义理解和执行鲁棒性方面的泛化能力显著优于监督微调(SFT)。同时，RL微调保持了与SFT相当的视觉鲁棒性。此外，PPO被确认为比DPO和GRPO等LLM衍生方法更有效的RL算法。所提出的PPO训练方案提高了VLA模型的泛化性能，并具有实际应用价值。

Conclusion: 强化学习(RL)微调，特别是使用PPO，能有效提升视觉-语言动作(VLA)模型在语义理解和执行鲁棒性方面的泛化能力，同时保持与监督微调(SFT)相当的视觉鲁棒性。PPO相较于其他RL算法如DPO和GRPO更为有效。此外，研究提出的一个简单PPO训练方案有助于高效提升VLA模型的泛化性能，具有重要的实际意义。

Abstract: Large Vision-Language Action (VLA) models have shown significant potential
for embodied AI. However, their predominant training via supervised fine-tuning
(SFT) limits generalization due to susceptibility to compounding errors under
distribution shifts. Reinforcement learning (RL) offers a path to overcome
these limitations by optimizing for task objectives via trial-and-error, yet a
systematic understanding of its specific generalization benefits for VLAs
compared to SFT is lacking. To address this, our study introduces a
comprehensive benchmark for evaluating VLA generalization and systematically
investigates the impact of RL fine-tuning across diverse visual, semantic, and
execution dimensions. Our extensive experiments reveal that RL fine-tuning,
particularly with PPO, significantly enhances generalization in semantic
understanding and execution robustness over SFT, while maintaining comparable
visual robustness. We identify PPO as a more effective RL algorithm for VLAs
than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for
efficient PPO training on VLAs, and demonstrate its practical utility for
improving VLA generalization. The project page is at https://rlvla.github.io

</details>


### [354] [GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation](https://arxiv.org/abs/2505.19802)
*Zhiyu Wang,Yang Liu,Hatice Gunes*

Main category: cs.LG

TL;DR: The paper proposes GraphAU-Pain, a graph-based framework using facial Action Units (AUs) for pain intensity estimation from facial expressions, which offers better interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: To improve the interpretability and severity quantification in detecting pain from facial expressions, especially for patients unable to communicate verbally.

Method: GraphAU-Pain leverages a graph-based framework where AUs are represented as graph nodes with co-occurrence relationships as edges. It utilizes a relational graph neural network for pain intensity estimation.

Result: Experiments on the UNBC dataset resulted in an F1-score of 66.21% and accuracy of 87.61% in pain intensity estimation.

Conclusion: GraphAU-Pain provides a more expressive depiction of pain-related facial behaviors and demonstrates improved interpretability and significant performance gains.

Abstract: Understanding pain-related facial behaviors is essential for digital
healthcare in terms of effective monitoring, assisted diagnostics, and
treatment planning, particularly for patients unable to communicate verbally.
Existing data-driven methods of detecting pain from facial expressions are
limited due to interpretability and severity quantification. To this end, we
propose GraphAU-Pain, leveraging a graph-based framework to model facial Action
Units (AUs) and their interrelationships for pain intensity estimation. AUs are
represented as graph nodes, with co-occurrence relationships as edges, enabling
a more expressive depiction of pain-related facial behaviors. By utilizing a
relational graph neural network, our framework offers improved interpretability
and significant performance gains. Experiments conducted on the publicly
available UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,
achieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity
estimation.

</details>


### [355] [Density Ratio-Free Doubly Robust Proxy Causal Learning](https://arxiv.org/abs/2505.19807)
*Bariscan Bozkurt,Houssam Zenati,Dimitri Meunier,Liyuan Xu,Arthur Gretton*

Main category: cs.LG

TL;DR: The paper proposes two kernel-based doubly robust estimators for causal function estimation in the Proxy Causal Learning (PCL) framework, which perform better on PCL benchmarks.


<details>
  <summary>Details</summary>
Motivation: Causal function estimation is challenging when confounders are not observed but proxies for the confounders are available. Current approaches have limitations in handling continuous and high-dimensional variables.

Method: The authors propose two kernel-based doubly robust estimators that combine outcome bridge-based and treatment bridge-based methods. They use kernel mean embeddings to obtain closed-form solutions and strong consistency guarantees without needing kernel smoothing or density ratio estimation over the treatment variable.

Result: The proposed estimators outperform existing methods on PCL benchmarks, particularly a prior doubly robust method requiring both kernel smoothing and density ratio estimation.

Conclusion: The new kernel-based doubly robust estimators provide an effective solution for causal function estimation within the PCL framework, especially for continuous or high-dimensional treatments.

Abstract: We study the problem of causal function estimation in the Proxy Causal
Learning (PCL) framework, where confounders are not observed but proxies for
the confounders are available. Two main approaches have been proposed: outcome
bridge-based and treatment bridge-based methods. In this work, we propose two
kernel-based doubly robust estimators that combine the strengths of both
approaches, and naturally handle continuous and high-dimensional variables. Our
identification strategy builds on a recent density ratio-free method for
treatment bridge-based PCL; furthermore, in contrast to previous approaches, it
does not require indicator functions or kernel smoothing over the treatment
variable. These properties make it especially well-suited for continuous or
high-dimensional treatments. By using kernel mean embeddings, we have
closed-form solutions and strong consistency guarantees. Our estimators
outperform existing methods on PCL benchmarks, including a prior doubly robust
method that requires both kernel smoothing and density ratio estimation.

</details>


### [356] [Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees](https://arxiv.org/abs/2505.19809)
*Daniel Ordoñez-Apraez,Alek Fröhlich,Vladimir Kostić,Karim Lounici,Vivien Brandt,Massimiliano Pontil*

Main category: cs.LG

TL;DR: In this paper, researchers propose an equivariant representation learning framework which can be applied to regression, conditional probability estimation and uncertainty quantification. It provides non-asymptotic statistical learning guarantees and is based on operator and group representation theory. Experiments show its effectiveness in robotics applications.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to exploit symmetries rooted in physics or geometry for improving generalization and sample efficiency in regression, conditional probability estimation, and uncertainty quantification.

Method: The method introduced in this paper is an equivariant representation learning framework that approximates the spectral decomposition of the conditional expectation operator, building representations that are both equivariant and disentangled along independent symmetry subgroups. This approach is grounded in operator and group representation theory.

Result: Empirical evaluations on synthetic datasets and real-world robotics applications confirm the potential of this approach, matching or outperforming existing equivariant baselines in regression while additionally providing well-calibrated parametric uncertainty estimates.

Conclusion: This paper introduces a novel framework with first-of-its-kind non-asymptotic statistical learning guarantees for regression, conditional probability estimation, and uncertainty quantification.

Abstract: In many real-world applications of regression, conditional probability
estimation, and uncertainty quantification, exploiting symmetries rooted in
physics or geometry can dramatically improve generalization and sample
efficiency. While geometric deep learning has made significant empirical
advances by incorporating group-theoretic structure, less attention has been
given to statistical learning guarantees. In this paper, we introduce an
equivariant representation learning framework that simultaneously addresses
regression, conditional probability estimation, and uncertainty quantification
while providing first-of-its-kind non-asymptotic statistical learning
guarantees. Grounded in operator and group representation theory, our framework
approximates the spectral decomposition of the conditional expectation
operator, building representations that are both equivariant and disentangled
along independent symmetry subgroups. Empirical evaluations on synthetic
datasets and real-world robotics applications confirm the potential of our
approach, matching or outperforming existing equivariant baselines in
regression while additionally providing well-calibrated parametric uncertainty
estimates.

</details>


### [357] [InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory](https://arxiv.org/abs/2505.19820)
*Feifei Li,Mi Zhang,Zhaoxiang Wang,Min Yang*

Main category: cs.LG

TL;DR: The paper introduces InfoCons, an information-theoretic framework for interpreting point cloud models by decomposing them into 3D concepts. It aims to produce faithful and conceptually coherent subsets of input points to explain model predictions.


<details>
  <summary>Details</summary>
Motivation: There is a need for interpretability in point cloud models used in safety-critical applications such as autonomous vehicles. The goal is to attribute model outputs to interpretable critical concepts within the point cloud data.

Method: The proposed method, InfoCons, uses information-theoretic principles to break down point clouds into meaningful 3D concepts. These concepts are designed to be both faithful (preserving causally influential points) and conceptually coherent (forming semantically meaningful structures). Learnable priors are employed to assess the causal effects on model predictions.

Result: InfoCons was evaluated on synthetic datasets for classification tasks and compared qualitatively and quantitatively with four baselines. Its scalability and flexibility were also demonstrated on two real-world datasets and in two applications that use critical scores of point clouds.

Conclusion: InfoCons provides a framework for generating human-understandable diagnostics of model failures in point cloud models through the decomposition into meaningful 3D concepts.

Abstract: Interpretability of point cloud (PC) models becomes imperative given their
deployment in safety-critical scenarios such as autonomous vehicles. We focus
on attributing PC model outputs to interpretable critical concepts, defined as
meaningful subsets of the input point cloud. To enable human-understandable
diagnostics of model failures, an ideal critical subset should be *faithful*
(preserving points that causally influence predictions) and *conceptually
coherent* (forming semantically meaningful structures that align with human
perception). We propose InfoCons, an explanation framework that applies
information-theoretic principles to decompose the point cloud into 3D concepts,
enabling the examination of their causal effect on model predictions with
learnable priors. We evaluate InfoCons on synthetic datasets for
classification, comparing it qualitatively and quantitatively with four
baselines. We further demonstrate its scalability and flexibility on two
real-world datasets and in two applications that utilize critical scores of PC.

</details>


### [358] [LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments](https://arxiv.org/abs/2505.19823)
*Pengcheng Sun,Erwu Liu,Wei Ni,Rui Wang,Yuanzhe Geng,Lijuan Lai,Abbas Jamalipour*

Main category: cs.LG

TL;DR: 为了保护数据隐私，提出了轻量级自适应隐私分配（LAPA）策略，通过个性化隐私预算分配和动态优化策略，在满足隐私要求的同时提高联邦学习的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习尽管可以保护设备的数据隐私，但仍然可能受到梯度泄漏攻击。差分隐私技术虽然能降低隐私泄露风险，但对联邦学习的效用有害，尤其是在非独立同分布（Non-IID）的数据场景中。因此需要一种既能保护隐私又能保证聚合效率的方法。

Method: 提出了一种轻量级自适应隐私分配（LAPA）策略，为每个聚合轮次中的设备分配个性化的隐私预算，无需传输任何额外信息。使用深度确定性策略梯度（DDPG）算法优化传输功率，以确定自适应衰减的人工噪声与通信噪声对齐的最佳时机，从而在差分隐私和系统效用之间取得有效平衡。设计了一种可靠的聚合策略，将通信质量和数据分布特征结合起来，从而在保护隐私的同时提高聚合性能。

Result: 实验结果表明，基于LAPA的个性化噪声分配和动态优化策略提高了收敛性能，同时满足了联邦学习的隐私要求。

Conclusion: LAPA策略能够有效地在保护隐私和提高聚合效率之间取得平衡，适用于联邦学习场景，特别是在非独立同分布数据的情况下。

Abstract: Federated Learning (FL) is a distributed machine learning paradigm based on
protecting data privacy of devices, which however, can still be broken by
gradient leakage attack via parameter inversion techniques. Differential
privacy (DP) technology reduces the risk of private data leakage by adding
artificial noise to the gradients, but detrimental to the FL utility at the
same time, especially in the scenario where the data is Non-Independent
Identically Distributed (Non-IID). Based on the impact of heterogeneous data on
aggregation performance, this paper proposes a Lightweight Adaptive Privacy
Allocation (LAPA) strategy, which assigns personalized privacy budgets to
devices in each aggregation round without transmitting any additional
information beyond gradients, ensuring both privacy protection and aggregation
efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)
algorithm is employed to optimize the transmission power, in order to determine
the optimal timing at which the adaptively attenuated artificial noise aligns
with the communication noise, enabling an effective balance between DP and
system utility. Finally, a reliable aggregation strategy is designed by
integrating communication quality and data distribution characteristics, which
improves aggregation performance while preserving privacy. Experimental results
demonstrate that the personalized noise allocation and dynamic optimization
strategy based on LAPA proposed in this paper enhances convergence performance
while satisfying the privacy requirements of FL.

</details>


### [359] [Foundation Models for Tabular Data within Systemic Contexts Need Grounding](https://arxiv.org/abs/2505.19825)
*Tassilo Klein,Johannes Hoffart*

Main category: cs.LG

TL;DR: Current tabular foundation models have limitations due to their assumptions of information completeness and isolated nature. This paper introduces Semantically Linked Tables (SLT) and Foundation Models for SLTs (FMSLT), which integrate declarative and procedural operational knowledge to provide a more comprehensive representation of tabular data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current tabular foundation models that overlook complexities in real-world data, treat tables as isolated entities, and assume information completeness. The authors aim to incorporate the vital operational context into tabular data modeling.

Method: The method involves introducing the concept of Semantically Linked Tables (SLT) and proposing Foundation Models for SLTs (FMSLT). These models integrate declarative and procedural operational knowledge to ground tabular data within its true operational context.

Result: The result is a new direction in tabular data modeling that recognizes the interconnected nature of tables and incorporates operational knowledge, unlocking the full potential of machine learning for complex, interconnected tabular data across diverse domains.

Conclusion: The conclusion is that current tabular foundation models are limited and need to evolve towards FMSLTs, which consider the operational context and connections between tables. Collaboration between domain experts and researchers is crucial for advancing robust, context-aware models for structured data.

Abstract: Current research on tabular foundation models often overlooks the
complexities of large-scale, real-world data by treating tables as isolated
entities and assuming information completeness, thereby neglecting the vital
operational context. To address this, we introduce the concept of Semantically
Linked Tables (SLT), recognizing that tables are inherently connected to both
declarative and procedural operational knowledge. We propose Foundation Models
for Semantically Linked Tables (FMSLT), which integrate these components to
ground tabular data within its true operational context. This comprehensive
representation unlocks the full potential of machine learning for complex,
interconnected tabular data across diverse domains. Realizing FMSLTs requires
access to operational knowledge that is often unavailable in public datasets,
highlighting the need for close collaboration between domain experts and
researchers. Our work exposes the limitations of current tabular foundation
models and proposes a new direction centered on FMSLTs, aiming to advance
robust, context-aware models for structured data.

</details>


### [360] [Revisiting Glorot Initialization for Long-Range Linear Recurrences](https://arxiv.org/abs/2505.19827)
*Noga Bar,Mariia Seleznova,Yotam Alexander,Gitta Kutyniok,Raja Giryes*

Main category: cs.LG

TL;DR: 在长序列任务中，Glorot初始化对于RNN来说是不稳定的，可能导致隐藏状态爆炸。本文提出了一种简单的、与维度相关的重新缩放方法，可以防止信号的快速爆炸或衰减。


<details>
  <summary>Details</summary>
Motivation: 尽管Glorot初始化被广泛用于确保稳定的信号传播，但在处理长序列的RNN中，这种初始化方法可能不合适，因为它是在无限宽度和固定长度的假设下推导出来的。

Method: 通过理论分析，证明了Glorot初始化在特定条件下（序列长度为$ t = O(\sqrt{n}) $，其中$n$为隐藏层宽度）会导致不稳定，并提出了对Glorot初始化进行简单、与维度相关的重新缩放，以使谱半径略低于1，从而避免信号的快速爆炸或衰减。

Result: 理论上证明了Glorot初始化的不稳定性，并验证了所提出的重新缩放方法的有效性。

Conclusion: 标准的初始化方案在长序列任务中可能失效，需要针对稳定循环初始化的独立理论研究。

Abstract: Proper initialization is critical for Recurrent Neural Networks (RNNs),
particularly in long-range reasoning tasks, where repeated application of the
same weight matrix can cause vanishing or exploding signals. A common baseline
for linear recurrences is Glorot initialization, designed to ensure stable
signal propagation--but derived under the infinite-width, fixed-length
regime--an unrealistic setting for RNNs processing long sequences. In this
work, we show that Glorot initialization is in fact unstable: small positive
deviations in the spectral radius are amplified through time and cause the
hidden state to explode. Our theoretical analysis demonstrates that sequences
of length $t = O(\sqrt{n})$, where $n$ is the hidden width, are sufficient to
induce instability. To address this, we propose a simple, dimension-aware
rescaling of Glorot that shifts the spectral radius slightly below one,
preventing rapid signal explosion or decay. These results suggest that standard
initialization schemes may break down in the long-sequence regime, motivating a
separate line of theory for stable recurrent initialization.

</details>


### [361] [PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints](https://arxiv.org/abs/2505.19842)
*Shuo Wang,Yun Cheng,Qingye Meng,Olga Saukh,Jiang Zhang,Jingfang Fan,Yuanting Zhang,Xingyuan Yuan,Lothar Thiele*

Main category: cs.LG

TL;DR: PCDCNet是一种结合数值建模原理与深度学习的代理模型，用于空气质量预报（AQF），它通过整合排放、气象影响和领域信息约束，实现了72小时站点级PM2.5和O3预测的SOTA性能，同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的数值模型（如CMAQ和WRF-Chem）虽然提供了基于物理的模拟，但计算成本高且依赖于不确定的排放清单；而深度学习模型尽管计算效率高，但由于缺乏物理约束，泛化能力往往不足。因此需要一种结合两者优点的新方法。

Method: PCDCNet 明确地将排放、气象影响和领域信息约束纳入模型中，使用基于图的空间传输建模、递归结构进行时间积累以及表示增强来处理局部相互作用，从而实现污染物形成、传输和消散的建模。

Result: PCDCNet 在72小时站点级 PM2.5 和 O3 预报中达到了 SOTA 性能，并显著减少了计算成本。此外，该模型已部署在在线平台上，提供免费、实时的空气质量预报，展示了其可扩展性和社会影响。

Conclusion: 通过将深度学习与物理一致性对齐，PCDCNet 提供了一种实用且可解释的 AQF 解决方案，能够支持个人和监管应用中的明智决策。

Abstract: Air quality forecasting (AQF) is critical for public health and environmental
management, yet remains challenging due to the complex interplay of emissions,
meteorology, and chemical transformations. Traditional numerical models, such
as CMAQ and WRF-Chem, provide physically grounded simulations but are
computationally expensive and rely on uncertain emission inventories. Deep
learning models, while computationally efficient, often struggle with
generalization due to their lack of physical constraints. To bridge this gap,
we propose PCDCNet, a surrogate model that integrates numerical modeling
principles with deep learning. PCDCNet explicitly incorporates emissions,
meteorological influences, and domain-informed constraints to model pollutant
formation, transport, and dissipation. By combining graph-based spatial
transport modeling, recurrent structures for temporal accumulation, and
representation enhancement for local interactions, PCDCNet achieves
state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3
forecasting while significantly reducing computational costs. Furthermore, our
model is deployed in an online platform, providing free, real-time air quality
forecasts, demonstrating its scalability and societal impact. By aligning deep
learning with physical consistency, PCDCNet offers a practical and
interpretable solution for AQF, enabling informed decision-making for both
personal and regulatory applications.

</details>


### [362] [DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning](https://arxiv.org/abs/2505.19850)
*Leander Diaz-Bone,Marco Bagatella,Jonas Hübotter,Andreas Krause*

Main category: cs.LG

TL;DR: DISCOVER是一种新的强化学习方法，通过在目标任务方向上选择探索性目标，解决了高维、长时域稀疏奖励任务的探索问题。


<details>
  <summary>Details</summary>
Motivation: 解决复杂和高维任务需要先解决与目标任务相关的简单任务，而现有的方法大多未能提供有效的探索方向。

Method: 提出了一种名为DISCOVER的方法，该方法从现有RL算法中提取探索方向，并在目标方向上选择探索性目标，无需任何先验信息。

Result: 理论上，DISCOVER能够将目标任务变得可实现的时间与初始距离相关，而不依赖于所有任务的空间体积。实验上，在高维环境中表现出色，解决了先前方法无法处理的探索问题。

Conclusion: DISCOVER为高维、长时域稀疏奖励任务提供了更有效的探索策略，显著优于现有方法。

Abstract: Sparse-reward reinforcement learning (RL) can model a wide range of highly
complex tasks. Solving sparse-reward tasks is RL's core premise - requiring
efficient exploration coupled with long-horizon credit assignment - and
overcoming these challenges is key for building self-improving agents with
superhuman ability. We argue that solving complex and high-dimensional tasks
requires solving simpler tasks that are relevant to the target task. In
contrast, most prior work designs strategies for selecting exploratory tasks
with the objective of solving any task, making exploration of challenging
high-dimensional, long-horizon tasks intractable. We find that the sense of
direction, necessary for effective exploration, can be extracted from existing
RL algorithms, without needing any prior information. Based on this finding, we
propose a method for directed sparse-reward goal-conditioned very long-horizon
RL (DISCOVER), which selects exploratory goals in the direction of the target
task. We connect DISCOVER to principled exploration in bandits, formally
bounding the time until the target task becomes achievable in terms of the
agent's initial distance to the target, but independent of the volume of the
space of all tasks. Empirically, we perform a thorough evaluation in
high-dimensional environments. We find that the directed goal selection of
DISCOVER solves exploration problems that are beyond the reach of prior
state-of-the-art exploration methods in RL.

</details>


### [363] [Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?](https://arxiv.org/abs/2505.19855)
*Zexi Li,Xiangzhu Wang,William F. Shen,Meghdad Kurmanji,Xinchi Qiu,Dongqi Cai,Chao Wu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: 研究探讨了知识编辑技术是否可作为大型语言模型遗忘的有效基线，并提出适应性方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的遗忘（unlearning）和知识编辑虽看似不同任务，但二者紧密相关。将遗忘视为编辑的一种特殊情况，即将信息修改为空集响应，以实现信息移除。

Method: 评估多种前沿编辑方法在遗忘任务中的表现，包括ROME、MEMIT、GRACE、WISE和AlphaEdit，并针对遗忘应用提出实际改进方案，如自我改进和查询合并。

Result: 部分编辑方法（尤其是WISE和AlphaEdit）在遗忘任务中表现出色，尤其在预训练知识方面效果显著，并能生成与人类一致的拒绝回答。

Conclusion: 建议遗忘研究领域采用前沿编辑方法作为基线，并从编辑角度探索遗忘问题，以实现更全面的大型语言模型记忆控制。

Abstract: Large language Model (LLM) unlearning, i.e., selectively removing information
from LLMs, is vital for responsible model deployment. Differently, LLM
knowledge editing aims to modify LLM knowledge instead of removing it. Though
editing and unlearning seem to be two distinct tasks, we find there is a tight
connection between them. In this paper, we conceptualize unlearning as a
special case of editing where information is modified to a refusal or "empty
set" $\emptyset$ response, signifying its removal. This paper thus investigates
if knowledge editing techniques are strong baselines for LLM unlearning. We
evaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE,
WISE, and AlphaEdit) against existing unlearning approaches on pretrained and
finetuned knowledge. Results show certain editing methods, notably WISE and
AlphaEdit, are effective unlearning baselines, especially for pretrained
knowledge, and excel in generating human-aligned refusal answers. To better
adapt editing methods for unlearning applications, we propose practical recipes
including self-improvement and query merging. The former leverages the LLM's
own in-context learning ability to craft a more human-aligned unlearning
target, and the latter enables ROME and MEMIT to perform well in unlearning
longer sample sequences. We advocate for the unlearning community to adopt SOTA
editing methods as baselines and explore unlearning from an editing perspective
for more holistic LLM memory control.

</details>


### [364] [Deep Active Inference Agents for Delayed and Long-Horizon Environments](https://arxiv.org/abs/2505.19867)
*Yavar Taheri Yeganeh,Mohsen Jafari,Andrea Matta*

Main category: cs.LG

TL;DR: 提出了一种生成策略架构，通过多步潜在转换、集成策略网络、交替优化方案和单梯度步骤来解决延迟和长视野环境中的决策问题。在模拟现实工业场景的环境中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的主动推理（AIF）代理在延迟环境和需要长期规划的任务中表现不佳，并且大多数代理在机器人或视觉基准上进行评估，未能达到现实世界工业复杂性。

Method: 1. 多步潜在转换：使生成模型能够一次性预测整个视野。
2. 集成策略网络：结合转换并接收预期自由能的梯度。
3. 交替优化方案：从重放缓冲区更新模型和策略。
4. 单梯度步骤：在控制回路中消除详尽规划。

Result: 实验结果表明，结合世界模型与AIF形式主义的方法可以产生一个端到端的概率控制器，在延迟和长期视野设置中进行有效的决策，而无需手工奖励或昂贵的规划。

Conclusion: 所提出的生成策略架构有效地解决了现有AIF代理在延迟和长期视野环境中的局限性，提供了一个无需手工奖励或昂贵规划的有效决策解决方案。

Abstract: With the recent success of world-model agents, which extend the core idea of
model-based reinforcement learning by learning a differentiable model for
sample-efficient control across diverse tasks, active inference (AIF) offers a
complementary, neuroscience-grounded paradigm that unifies perception,
learning, and action within a single probabilistic framework powered by a
generative model. Despite this promise, practical AIF agents still rely on
accurate immediate predictions and exhaustive planning, a limitation that is
exacerbated in delayed environments requiring plans over long horizons, tens to
hundreds of steps. Moreover, most existing agents are evaluated on robotic or
vision benchmarks which, while natural for biological agents, fall short of
real-world industrial complexity. We address these limitations with a
generative-policy architecture featuring (i) a multi-step latent transition
that lets the generative model predict an entire horizon in a single
look-ahead, (ii) an integrated policy network that enables the transition and
receives gradients of the expected free energy, (iii) an alternating
optimization scheme that updates model and policy from a replay buffer, and
(iv) a single gradient step that plans over long horizons, eliminating
exhaustive planning from the control loop. We evaluate our agent in an
environment that mimics a realistic industrial scenario with delayed and
long-horizon settings. The empirical results confirm the effectiveness of the
proposed approach, demonstrating the coupled world-model with the AIF formalism
yields an end-to-end probabilistic controller capable of effective decision
making in delayed, long-horizon settings without handcrafted rewards or
expensive planning.

</details>


### [365] [Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations](https://arxiv.org/abs/2505.19888)
*Eun Gyung Kong,Je Won Yeom,Yonghoon Jeon,Taesup Kim*

Main category: cs.LG

TL;DR: Federated Learning (FL) enhances data privacy and security. However, achieving both generalization and personalization in heterogeneous settings is challenging. We introduce FedOT, a novel approach leveraging black-box foundation models, which shares a global task-dependent classifier across clients while locally adapting features through orthogonal transformations.


<details>
  <summary>Details</summary>
Motivation: Current federated learning methods struggle to achieve both generalization and personalization in heterogeneous settings.

Method: FedOT shares a global task-dependent classifier across clients while locally adapting features through orthogonal transformations. It enforces orthogonality to mitigate gradient conflicts across diverse clients, preserve semantic integrity, and achieve robust performance even with significant data heterogeneity.

Result: FedOT outperforms baseline FL methods across multiple benchmarks by enabling a more balanced approach for both generalization and personalization.

Conclusion: The joint optimization of global classifiers and local orthogonal transformations in FedOT yields superior performance and suggests broader applicability.

Abstract: Federated Learning (FL) aims to train models across decentralized clients or
devices holding local data without the need for centralized data collection,
thus enhancing data privacy and security. However, achieving both
generalization and personalization in heterogeneous settings remains a
significant challenge. To address this, we introduce FedOT, a novel approach
that leverages black-box foundation models. FedOT shares only a global
task-dependent classifier across clients while locally adapting features
through orthogonal transformations. By enforcing orthogonality, FedOT mitigates
gradient conflicts across diverse clients, preserves semantic integrity, and
achieves robust performance even in the presence of substantial data
heterogeneity. The strategy of combining global and local parameters enables a
more balanced approach for both generalization and personalization,
outperforming baseline FL methods across multiple benchmarks. Furthermore, our
extensive analysis confirms that joint optimization of global classifiers and
local orthogonal transformations yields superior performance and suggests
broader applicability.

</details>


### [366] [ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining](https://arxiv.org/abs/2505.19893)
*Melis Ilayda Bal,Volkan Cevher,Michael Muehlebach*

Main category: cs.LG

TL;DR: Efficient Selective Language Modeling (ESLM) is a risk-aware algorithm that improves training efficiency and distributional robustness by performing online token-level batch selection, significantly reducing training FLOPs while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Large language model pretraining is compute-intensive and many tokens contribute marginally to learning, leading to inefficiency.

Method: ESLM leverages per-token statistics and applies value-at-risk thresholding to retain only the most informative tokens per batch. It frames ESLM as a bilevel game with a masking adversary and extends it to Ada-ESLM for adaptive tuning of selection confidence during training.

Result: Experiments on GPT-2 pretraining show that ESLM significantly reduces training FLOPs while maintaining or improving both perplexity and downstream performance compared to baselines. The approach scales across model sizes, pretraining corpora, and integrates naturally with knowledge distillation.

Conclusion: ESLM improves training efficiency and distributional robustness through online token-level batch selection.

Abstract: Large language model pretraining is compute-intensive, yet many tokens
contribute marginally to learning, resulting in inefficiency. We introduce
Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that
improves training efficiency and distributional robustness by performing online
token-level batch selection. ESLM leverages per-token statistics (e.g., entropy
or loss) and applies value-at-risk thresholding to retain only the most
informative tokens per batch. This data-centric mechanism reshapes the training
loss, prioritizing high-risk tokens and eliminating redundant gradient
computation. We frame ESLM as a bilevel game: the model competes with a masking
adversary that selects worst-case token subsets under a constrained
thresholding rule. In the loss-based setting, ESLM recovers conditional
value-at-risk loss minimization, providing a principled connection to
distributionally robust optimization. We extend our approach to Ada-ESLM, which
adaptively tunes the selection confidence during training. Experiments on GPT-2
pretraining show that ESLM significantly reduces training FLOPs while
maintaining or improving both perplexity and downstream performance compared to
baselines. Our approach also scales across model sizes, pretraining corpora,
and integrates naturally with knowledge distillation.

</details>


### [367] [Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL](https://arxiv.org/abs/2505.19923)
*Qin-Wen Luo,Ming-Kun Xie,Ye-Wen Wang,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: Offline reinforcement learning (RL) often faces the problem of fixed regularization strength which leads to a dilemma. This paper proposes selective state-adaptive regularization method for offline RL, which significantly outperforms existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing studies in offline RL uniformly regularize the value function or policy updates across all states, but this fixed regularization strength causes issues due to variations in data quality.

Method: The proposed method introduces state-adaptive regularization coefficients to trust state-level Bellman-driven results and selectively applies regularization on high-quality actions, avoiding degradation from constraints on low-quality actions.

Result: Extensive experiments show that the proposed method significantly outperforms state-of-the-art approaches in both offline and offline-to-online settings on the D4RL benchmark.

Conclusion: Selective state-adaptive regularization is an effective approach for offline RL, offering significant improvements over current methods.

Abstract: Offline reinforcement learning (RL) aims to learn an effective policy from a
static dataset. To alleviate extrapolation errors, existing studies often
uniformly regularize the value function or policy updates across all states.
However, due to substantial variations in data quality, the fixed
regularization strength often leads to a dilemma: Weak regularization strength
fails to address extrapolation errors and value overestimation, while strong
regularization strength shifts policy learning toward behavior cloning,
impeding potential performance enabled by Bellman updates. To address this
issue, we propose the selective state-adaptive regularization method for
offline RL. Specifically, we introduce state-adaptive regularization
coefficients to trust state-level Bellman-driven results, while selectively
applying regularization on high-quality actions, aiming to avoid performance
degradation caused by tight constraints on low-quality actions. By establishing
a connection between the representative value regularization method, CQL, and
explicit policy constraint methods, we effectively extend selective
state-adaptive regularization to these two mainstream offline RL approaches.
Extensive experiments demonstrate that the proposed method significantly
outperforms the state-of-the-art approaches in both offline and
offline-to-online settings on the D4RL benchmark.

</details>


### [368] [Logic Gate Neural Networks are Good for Verification](https://arxiv.org/abs/2505.19932)
*Fabian Kresse,Emily Yu,Christoph H. Lampert,Thomas A. Henzinger*

Main category: cs.LG

TL;DR: The paper presents a SAT encoding method for verifying global robustness and fairness in learned Logic Gate Networks (LGNs), which are more verification-friendly while maintaining strong predictive performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of formally verifying traditional neural networks, this paper focuses on learned Logic Gate Networks (LGNs) that use Boolean logic gates instead of multiplications, making them sparser and more amenable to symbolic verification.

Method: The authors introduce a SAT encoding technique designed for verifying global robustness and fairness properties in LGNs. This method is then evaluated on five benchmark datasets, one of which is a newly constructed 5-class variant.

Result: The evaluation shows that LGNs are not only easier to verify but also maintain high predictive performance, demonstrating the effectiveness of the proposed SAT encoding method.

Conclusion: Learned Logic Gate Networks offer an advantageous alternative to traditional neural networks for formal verification, as they combine verification-friendliness with strong predictive capabilities.

Abstract: Learning-based systems are increasingly deployed across various domains, yet
the complexity of traditional neural networks poses significant challenges for
formal verification. Unlike conventional neural networks, learned Logic Gate
Networks (LGNs) replace multiplications with Boolean logic gates, yielding a
sparse, netlist-like architecture that is inherently more amenable to symbolic
verification, while still delivering promising performance. In this paper, we
introduce a SAT encoding for verifying global robustness and fairness in LGNs.
We evaluate our method on five benchmark datasets, including a newly
constructed 5-class variant, and find that LGNs are both verification-friendly
and maintain strong predictive performance.

</details>


### [369] [Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning](https://arxiv.org/abs/2505.19940)
*Run Gu,Wei Xu,Zhaohui Yang,Dusit Niyato,Aylin Yener*

Main category: cs.LG

TL;DR: The paper proposes SLSCom, a self-supervised learning-based semantic communication framework that enhances task inference performance in scenarios with limited labeled samples. It uses unlabeled samples for task-relevant semantic encoding, addresses the information bottleneck problem, and includes joint training methods to improve end-to-end inference accuracy over wireless channels.


<details>
  <summary>Details</summary>
Motivation: To enhance task inference performance in semantic communication, especially in situations where access to labeled samples is limited.

Method: Development of a task-relevant semantic encoder using unlabeled samples collected from real-world edge networks. Introduction of self-supervision for learning contrastive features and formulation of the information bottleneck problem to balance feature informativeness and task inference performance. Practical solution devised by employing self-supervised classification and reconstruction pretext tasks. Efficient joint training methods proposed to enhance end-to-end inference accuracy over wireless channels.

Result: SLSCom significantly outperforms conventional digital coding methods and existing DL-based approaches in image classification tasks over multipath wireless channels across varying labeled data set sizes and SNR conditions, even when the unlabeled samples are irrelevant to the downstream tasks.

Conclusion: SLSCom is an effective framework for enhancing task inference performance in semantic communication with limited labeled samples.

Abstract: Task-oriented semantic communication enhances transmission efficiency by
conveying semantic information rather than exact messages. Deep learning
(DL)-based semantic communication can effectively cultivate the essential
semantic knowledge for semantic extraction, transmission, and interpretation by
leveraging massive labeled samples for downstream task training. In this paper,
we propose a self-supervised learning-based semantic communication framework
(SLSCom) to enhance task inference performance, particularly in scenarios with
limited access to labeled samples. Specifically, we develop a task-relevant
semantic encoder using unlabeled samples, which can be collected by devices in
real-world edge networks. To facilitate task-relevant semantic extraction, we
introduce self-supervision for learning contrastive features and formulate the
information bottleneck (IB) problem to balance the tradeoff between the
informativeness of the extracted features and task inference performance. Given
the computational challenges of the IB problem, we devise a practical and
effective solution by employing self-supervised classification and
reconstruction pretext tasks. We further propose efficient joint training
methods to enhance end-to-end inference accuracy over wireless channels, even
with few labeled samples. We evaluate the proposed framework on image
classification tasks over multipath wireless channels. Extensive simulation
results demonstrate that SLSCom significantly outperforms conventional digital
coding methods and existing DL-based approaches across varying labeled data set
sizes and SNR conditions, even when the unlabeled samples are irrelevant to the
downstream tasks.

</details>


### [370] [Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models](https://arxiv.org/abs/2505.19943)
*Huan Zhang,Fan Lyu,Shuyu Dong,Shenghua Fan,Yujin Zheng,Dingwen Wang*

Main category: cs.LG

TL;DR: The paper introduces MIST, a method for Continual Learning with Pre-trained Models that selectively updates a small subset of parameters to improve adaptability and preserve generalization.


<details>
  <summary>Details</summary>
Motivation: Continual Learning with Pre-trained Models is promising but existing methods either freeze the models or rely on auxiliary modules which limits model plasticity and generalization. Full fine-tuning can improve adaptability but risks disrupting pre-trained knowledge.

Method: MIST, a plug-and-play method that selectively updates less than 5% of PTM parameters based on sensitivity to mutual information objectives. Strong sparsity regularization is introduced by randomly dropping gradients during tuning, updating fewer than 0.5% of parameters per step.

Result: MIST consistently boosts performance across diverse continual learning benchmarks when applied before standard freeze-based methods. Integrating MIST into multiple baselines yields significant performance gains.

Conclusion: MIST enables effective task-specific adaptation while preserving generalization in Continual Learning with Pre-trained Models.

Abstract: Continual Learning with Pre-trained Models holds great promise for efficient
adaptation across sequential tasks. However, most existing approaches freeze
PTMs and rely on auxiliary modules like prompts or adapters, limiting model
plasticity and leading to suboptimal generalization when facing significant
distribution shifts. While full fine-tuning can improve adaptability, it risks
disrupting crucial pre-trained knowledge. In this paper, we propose Mutual
Information-guided Sparse Tuning (MIST), a plug-and-play method that
selectively updates a small subset of PTM parameters, less than 5%, based on
sensitivity to mutual information objectives. MIST enables effective
task-specific adaptation while preserving generalization. To further reduce
interference, we introduce strong sparsity regularization by randomly dropping
gradients during tuning, resulting in fewer than 0.5% of parameters being
updated per step. Applied before standard freeze-based methods, MIST
consistently boosts performance across diverse continual learning benchmarks.
Experiments show that integrating our method into multiple baselines yields
significant performance gains. Our code is available at
https://github.com/zhwhu/MIST.

</details>


### [371] [Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$-Realizable MDPs](https://arxiv.org/abs/2505.19946)
*Antoine Moulin,Gergely Neu,Luca Viano*

Main category: cs.LG

TL;DR: 研究了马尔可夫决策过程中的离线模仿学习问题，提出了一种新的算法SPOIL，能够匹配专家策略的表现，并提出了用于深度模仿学习的新损失函数。


<details>
  <summary>Details</summary>
Motivation: 离线模仿学习的目标是通过专家策略生成的数据集学习高性能的策略。现有的工作假设专家策略属于已知的可处理类别，本文则从环境结构的不同假设出发进行研究。

Method: 针对线性$Q^\pi$-realizable MDPs，提出了saddle-point offline imitation learning (SPOIL)算法，保证在访问$\mathcal{O}(\varepsilon^{-2})$样本的情况下，达到与任何专家策略相差不超过加性误差$\varepsilon$的表现。对于可能非线性的$Q^\pi$-realizable MDPs，结果扩展至样本复杂度为$\mathcal{O}(\varepsilon^{-4})$的情况。此外，还提出了一种新的损失函数用于深度模仿学习中从专家数据训练批评网络。

Result: 理论分析表明，SPOIL算法可以有效匹配专家策略的表现。实验结果表明，基于神经网络实现的SPOIL算法优于行为克隆，并且与最先进的算法具有竞争力。

Conclusion: 本文提出的SPOIL算法在线性和可能非线性的$Q^\pi$-realizable MDPs中均能有效实现离线模仿学习，同时提出的新损失函数有助于深度模仿学习的发展。

Abstract: We study the problem of offline imitation learning in Markov decision
processes (MDPs), where the goal is to learn a well-performing policy given a
dataset of state-action pairs generated by an expert policy. Complementing a
recent line of work on this topic that assumes the expert belongs to a
tractable class of known policies, we approach this problem from a new angle
and leverage a different type of structural assumption about the environment.
Specifically, for the class of linear $Q^\pi$-realizable MDPs, we introduce a
new algorithm called saddle-point offline imitation learning (\SPOIL), which is
guaranteed to match the performance of any expert up to an additive error
$\varepsilon$ with access to $\mathcal{O}(\varepsilon^{-2})$ samples. Moreover,
we extend this result to possibly non-linear $Q^\pi$-realizable MDPs at the
cost of a worse sample complexity of order $\mathcal{O}(\varepsilon^{-4})$.
Finally, our analysis suggests a new loss function for training critic networks
from expert data in deep imitation learning. Empirical evaluations on standard
benchmarks demonstrate that the neural net implementation of \SPOIL is superior
to behavior cloning and competitive with state-of-the-art algorithms.

</details>


### [372] [Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees](https://arxiv.org/abs/2505.19947)
*Herbert Woisetschläger,Ryan Zhang,Shiqiang Wang,Hans-Arno Jacobsen*

Main category: cs.LG

TL;DR: MESS+ is a stochastic optimization algorithm for cost-optimal LLM request routing that guarantees SLA compliance while learning request satisfaction probabilities in real-time, achieving 2x cost savings compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Open-weight LLM zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Users want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs.

Method: MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system. Model selection decisions are made by solving a per-request optimization problem. The algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction.

Result: Across a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x cost savings compared to existing LLM routing techniques.

Conclusion: MESS+ provides a solution for cost-optimal LLM request routing while ensuring rigorous SLA compliance guarantees.

Abstract: Open-weight LLM zoos provide access to numerous high-quality models, but
selecting the appropriate model for specific tasks remains challenging and
requires technical expertise. Most users simply want factually correct, safe,
and satisfying responses without concerning themselves with model
technicalities, while inference service providers prioritize minimizing
operating costs. These competing interests are typically mediated through
service level agreements (SLAs) that guarantee minimum service quality. We
introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM
request routing while providing rigorous SLA compliance guarantees. MESS+
learns request satisfaction probabilities of LLMs in real-time as users
interact with the system, based on which model selection decisions are made by
solving a per-request optimization problem. Our algorithm includes a novel
combination of virtual queues and request satisfaction prediction, along with a
theoretical analysis of cost optimality and constraint satisfaction. Across a
wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x
cost savings compared to existing LLM routing techniques.

</details>


### [373] [Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions](https://arxiv.org/abs/2505.19949)
*Siqi Kou,Qingyuan Tian,Hanwen Xu,Zihao Zeng,Zhijie Deng*

Main category: cs.LG

TL;DR: This paper explores the use of influence functions to attribute large language models' reasoning abilities in math and coding to specific training data components, revealing cross-domain effects and effective data characteristics. A reweighting strategy based on these findings improves model performance on specific benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing strategies for curating training data for enhancing reasoning capabilities in math and coding rely heavily on heuristics, which limits generalizability and fails to capture subtleties in the data.

Method: The authors leverage influence functions to systematically attribute reasoning abilities of large language models on math and coding tasks to individual training examples, sequences, and tokens. They introduce Influence-based Reasoning Attribution (Infra) which uncovers nontrivial cross-domain effects and propose a dataset reweighting strategy based on task difficulty.

Result: The reweighting strategy doubles AIME24 accuracy from 10% to 20% and boosts LiveCodeBench accuracy from 33.8% to 35.3% for Qwen2.5-7B-Instruct. Additionally, fine-grained attribution shows that sequence-level exploratory behaviors enhance reasoning performance and token-level influence patterns differ between math and code reasoning.

Conclusion: Influence functions provide deeper insights into effective data characteristics for reasoning tasks, leading to improved model performance through targeted dataset reweighting.

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities in math and coding, often bolstered by post-training on the
chain-of-thoughts (CoTs) generated by stronger models. However, existing
strategies for curating such training data predominantly rely on heuristics,
limiting generalizability and failing to capture subtleties underlying in data.
To address these limitations, we leverage influence functions to systematically
attribute LLMs' reasoning ability on math and coding to individual training
examples, sequences, and tokens, enabling deeper insights into effective data
characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers
nontrivial cross-domain effects across math and coding tasks: high-difficulty
math examples improve both math and code reasoning, while low-difficulty code
tasks most effectively benefit code reasoning. Based on these findings, we
introduce a simple yet effective dataset reweighting strategy by flipping task
difficulty, which doubles AIME24 accuracy from 10\% to 20\% and boosts
LiveCodeBench accuracy from 33.8\% to 35.3\% for Qwen2.5-7B-Instruct. Moreover,
our fine-grained attribution reveals that the sequence-level exploratory
behaviors enhance reasoning performance in both math and code, and the
token-level influence patterns are distinct for math and code reasoning: the
former prefers natural language logic connectors and the latter emphasizes
structural syntax.

</details>


### [374] [An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning](https://arxiv.org/abs/2505.19954)
*Andrew Zamai,Nathanael Fijalkow,Boris Mansencal,Laurent Simon,Eloi Navet,Pierrick Coupe*

Main category: cs.LG

TL;DR: An advanced framework integrating 3D MRI to text conversion and LLMs is proposed for diagnosing neurodegenerative dementias with explainable reasoning via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper lies in overcoming the limitation of deep learning models' diagnostic transparency for neurodegenerative dementias, aiming to provide both accurate and interpretable diagnostic results.

Method: This method consists of two core components: a modular pipeline that converts 3D T1-weighted brain MRIs into textual radiology reports and employing modern Large Language Models (LLMs) for differential diagnosis. Reinforcement learning is used to enhance diagnostic reasoning without needing supervised reasoning traces or distillation from larger models.

Result: The framework matches the diagnostic performance of existing deep learning methods while providing causally grounded explanations that support its diagnostic conclusions.

Conclusion: This work demonstrates a promising approach to balance predictive accuracy and explainability in diagnosing neurodegenerative dementias, which may assist clinicians effectively.

Abstract: The differential diagnosis of neurodegenerative dementias is a challenging
clinical task, mainly because of the overlap in symptom presentation and the
similarity of patterns observed in structural neuroimaging. To improve
diagnostic efficiency and accuracy, deep learning-based methods such as
Convolutional Neural Networks and Vision Transformers have been proposed for
the automatic classification of brain MRIs. However, despite their strong
predictive performance, these models find limited clinical utility due to their
opaque decision making. In this work, we propose a framework that integrates
two core components to enhance diagnostic transparency. First, we introduce a
modular pipeline for converting 3D T1-weighted brain MRIs into textual
radiology reports. Second, we explore the potential of modern Large Language
Models (LLMs) to assist clinicians in the differential diagnosis between
Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based
on the generated reports. To bridge the gap between predictive accuracy and
explainability, we employ reinforcement learning to incentivize diagnostic
reasoning in LLMs. Without requiring supervised reasoning traces or
distillation from larger models, our approach enables the emergence of
structured diagnostic rationales grounded in neuroimaging findings. Unlike
post-hoc explainability methods that retrospectively justify model decisions,
our framework generates diagnostic rationales as part of the inference
process-producing causally grounded explanations that inform and guide the
model's decision-making process. In doing so, our framework matches the
diagnostic performance of existing deep learning methods while offering
rationales that support its diagnostic conclusions.

</details>


### [375] [MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research](https://arxiv.org/abs/2505.19955)
*Hui Chen,Miao Xiong,Yujie Lu,Wei Han,Ailin Deng,Yufei He,Jiaying Wu,Yibo Li,Yue Liu,Bryan Hooi*

Main category: cs.LG

TL;DR: 本研究介绍了MLR-Bench，一个全面评估AI代理在开放式机器学习研究中的基准测试工具。它包含201个研究任务、MLR-Judge评价框架和MLR-Agent模块化代理框架。通过该工具对六个前沿LLM和一个高级编码代理进行评估，发现LLM在生成连贯想法和结构良好的论文方面表现良好，但当前的编码代理经常产生伪造或无效的实验结果。MLR-Judge通过人工评估验证，与专家评审高度一致，具有作为研究评估可扩展工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在推动和支持科学发现方面的潜力日益增长，需要一个综合的基准来评估这些代理在开放式机器学习研究中的表现。

Method: 引入了MLR-Bench，包括201个研究任务、MLR-Judge评价框架和MLR-Agent模块化代理框架。MLR-Judge结合基于LLM的评审员和精心设计的评审标准来评估研究质量。MLR-Agent通过四个阶段完成研究任务：想法生成、提案制定、实验和论文撰写。支持逐步评估和端到端评估。

Result: 通过对六个前沿LLM和一个高级编码代理进行评估，发现LLM在生成连贯想法和结构良好的论文方面表现良好，但当前的编码代理经常产生伪造或无效的实验结果。MLR-Judge通过人工评估验证，与专家评审高度一致。

Conclusion: MLR-Bench被开源，以帮助社区评估、诊断和改进AI研究代理，朝着可靠和透明的科学发现迈进。

Abstract: Recent advancements in AI agents have demonstrated their growing potential to
drive and support scientific discovery. In this work, we introduce MLR-Bench, a
comprehensive benchmark for evaluating AI agents on open-ended machine learning
research. MLR-Bench includes three key components: (1) 201 research tasks
sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)
MLR-Judge, an automated evaluation framework combining LLM-based reviewers with
carefully designed review rubrics to assess research quality; and (3)
MLR-Agent, a modular agent scaffold capable of completing research tasks
through four stages: idea generation, proposal formulation, experimentation,
and paper writing. Our framework supports both stepwise assessment across these
distinct research stages, and end-to-end evaluation of the final research
paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced
coding agent, finding that while LLMs are effective at generating coherent
ideas and well-structured papers, current coding agents frequently (e.g., in
80% of the cases) produce fabricated or invalidated experimental
results--posing a major barrier to scientific reliability. We validate
MLR-Judge through human evaluation, showing high agreement with expert
reviewers, supporting its potential as a scalable tool for research evaluation.
We open-source MLR-Bench to help the community benchmark, diagnose, and improve
AI research agents toward trustworthy and transparent scientific discovery.

</details>


### [376] [The Limits of Preference Data for Post-Training](https://arxiv.org/abs/2505.19964)
*Eric Zhao,Jessica Dai,Pranjal Awasthi*

Main category: cs.LG

TL;DR: 最近在强化大型语言模型能力方面的进展，源于将强化学习应用于具有自动可验证结果的领域。然而，在结果评估本质上需要人类反馈的领域中，是否可以类似地使用RL进行优化是一个关键问题。本文研究了一个关键障碍：偏好数据从根本上和显著地限制了基于结果的优化。即使在理想化的偏好数据下，使用序数反馈也可能无法获得接近最优的解决方案。通过投票理论形式化这一不可能性，并探讨了这些限制为何对RLHF在引发推理行为方面的影响不成比例。


<details>
  <summary>Details</summary>
Motivation: 在结果评估需要人类反馈的任务中（如深入研究和旅行规划），评估是定性的且存在多种成功程度。因此，探索如何在这些领域中应用强化学习进行优化是有意义的。

Method: 研究使用偏好数据（序数排名）作为收集人类反馈的一种模式，分析其对基于结果优化的限制。通过投票理论的形式化方法，揭示了偏好数据在获取接近最优解时的局限性。

Result: 发现即使在理想化的偏好数据条件下，使用序数反馈也可能无法获得接近最优的解决方案。表明偏好数据的限制主要抑制了RLHF引发稳健策略的能力，包括大多数推理行为。

Conclusion: 需要基于人类评分的算法创新来扩展RL在需要人类反馈领域的成功。偏好数据的局限性对RLHF在引发推理行为方面的影响比在指令调整和安全训练中的影响更大。

Abstract: Recent progress in strengthening the capabilities of large language models
has stemmed from applying reinforcement learning to domains with automatically
verifiable outcomes. A key question is whether we can similarly use RL to
optimize for outcomes in domains where evaluating outcomes inherently requires
human feedback; for example, in tasks like deep research and trip planning,
outcome evaluation is qualitative and there are many possible degrees of
success. One attractive and scalable modality for collecting human feedback is
preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$
given outcomes, which one is preferred. In this work, we study a critical
roadblock: preference data fundamentally and significantly limits outcome-based
optimization. Even with idealized preference data (infinite, noiseless, and
online), the use of ordinal feedback can prevent obtaining even approximately
optimal solutions. We formalize this impossibility using voting theory, drawing
an analogy between how a model chooses to answer a query with how voters choose
a candidate to elect. This indicates that grounded human scoring and
algorithmic innovations are necessary for extending the success of RL
post-training to domains demanding human feedback. We also explore why these
limitations have disproportionately impacted RLHF when it comes to eliciting
reasoning behaviors (e.g., backtracking) versus situations where RLHF has been
historically successful (e.g., instruction-tuning and safety training), finding
that the limitations of preference data primarily suppress RLHF's ability to
elicit robust strategies -- a class that encompasses most reasoning behaviors.

</details>


### [377] [Learning to Select In-Context Demonstration Preferred by Large Language Model](https://arxiv.org/abs/2505.19966)
*Zheng Zhang,Shaocheng Lan,Lei Song,Jiang Bian,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: GenICL is a new framework that uses LLM feedback to optimize demonstration selection for in-context learning, outperforming existing methods on various datasets.


<details>
  <summary>Details</summary>
Motivation: In-context learning's effectiveness heavily relies on the quality of selected demonstrations. Current retrieval-based methods fail to directly optimize for ICL performance and struggle when high-quality demonstrations are scarce.

Method: Proposed is GenICL, a generative preference learning framework which utilizes LLM feedback to directly enhance the selection of demonstrations for in-context learning.

Result: Experiments conducted on 19 datasets across 11 task categories indicate that GenICL performs better than current methods in selecting effective demonstrations, thereby improving in-context learning performance.

Conclusion: GenICL presents an innovative solution to improve demonstration selection for in-context learning, leading to enhanced ICL performance.

Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks during inference using only a few demonstrations. However, ICL
performance is highly dependent on the selection of these demonstrations.
Recent work explores retrieval-based methods for selecting query-specific
demonstrations, but these approaches often rely on surrogate objectives such as
metric learning, failing to directly optimize ICL performance. Consequently,
they struggle to identify truly beneficial demonstrations. Moreover, their
discriminative retrieval paradigm is ineffective when the candidate pool lacks
sufficient high-quality demonstrations. To address these challenges, we propose
GenICL, a novel generative preference learning framework that leverages LLM
feedback to directly optimize demonstration selection for ICL. Experiments on
19 datasets across 11 task categories demonstrate that GenICL achieves superior
performance than existing methods in selecting the most effective
demonstrations, leading to better ICL performance.

</details>


### [378] [Rethinking Probabilistic Circuit Parameter Learning](https://arxiv.org/abs/2505.19982)
*Anji Liu,Guy Van den Broeck*

Main category: cs.LG

TL;DR: 本研究通过建立EM目标与标准全批量EM算法之间的新联系，提出了一个理论上有根据的小批量设置的推广，并展示了其在大规模数据集参数训练上的有效性。


<details>
  <summary>Details</summary>
Motivation: 概率电路（PCs）提供了一个生成建模的计算可扩展框架，尽管最近的进步提高了PCs的表达能力和可扩展性，但有效地训练其参数仍然是一个挑战。特别是广泛使用的优化方法——全批量期望最大化（EM），对于大型数据集来说是无效的，因为需要在进行单次更新之前处理整个数据集。此外，尽管已提出针对小批量设置的经验扩展，但这些算法所优化的目标尚不清楚，难以评估其理论合理性。

Method: 研究首先建立了通用EM目标和标准全批量EM算法之间的新联系，然后基于此联系推导出一种理论上合理的小批量设置推广。

Result: 通过初步的实证结果证明了该方法的有效性。

Conclusion: 这项工作填补了全批量EM算法和小批量设置之间的理论空白，为有效训练大规模数据集上的概率电路参数提供了新的方向。

Abstract: Probabilistic Circuits (PCs) offer a computationally scalable framework for
generative modeling, supporting exact and efficient inference of a wide range
of probabilistic queries. While recent advances have significantly improved the
expressiveness and scalability of PCs, effectively training their parameters
remains a challenge. In particular, a widely used optimization method,
full-batch Expectation-Maximization (EM), requires processing the entire
dataset before performing a single update, making it ineffective for large
datasets. While empirical extensions to the mini-batch setting have been
proposed, it remains unclear what objective these algorithms are optimizing,
making it difficult to assess their theoretical soundness. This paper bridges
the gap by establishing a novel connection between the general EM objective and
the standard full-batch EM algorithm. Building on this, we derive a
theoretically grounded generalization to the mini-batch setting and demonstrate
its effectiveness through preliminary empirical results.

</details>


### [379] [Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach](https://arxiv.org/abs/2505.19986)
*Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Actor-Critic methods are widely used for their scalability, yet existing
theoretical guarantees for infinite-horizon average-reward Markov Decision
Processes (MDPs) often rely on restrictive ergodicity assumptions. We propose
NAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret
of $\tilde{O}(\sqrt{T})$ in infinite-horizon average-reward MDPs under the
unichain assumption, which permits both transient states and periodicity. This
assumption is among the weakest under which the classic policy gradient theorem
remains valid for average-reward settings. NAC-B employs function approximation
for both the actor and the critic, enabling scalability to problems with large
state and action spaces. The use of batching in our algorithm helps mitigate
potential periodicity in the MDP and reduces stochasticity in gradient
estimates, and our analysis formalizes these benefits through the introduction
of the constants $C_{\text{hit}}$ and $C_{\text{tar}}$, which characterize the
rate at which empirical averages over Markovian samples converge to the
stationary distribution.

</details>


### [380] [Learning Optimal Multimodal Information Bottleneck Representations](https://arxiv.org/abs/2505.19996)
*Qilong Wu,Yiyang Shao,Jun Wang,Xiaobo Sun*

Main category: cs.LG

TL;DR: The paper introduces Optimal Multimodal Information Bottleneck (OMIB), a new multimodal learning framework that optimizes the generation of high-quality joint representations from multimodal data by setting theoretically derived bounds on regularization weights and dynamically adjusting these weights per modality. This approach enhances model performance in machine-learning applications.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal learning methods based on the MIB principle often set ad hoc regularization weights and overlook imbalanced task-relevant information across modalities, limiting their ability to achieve optimal MIB.

Method: The authors propose OMIB, which guarantees the achievability of optimal MIB by setting the regularization weight within a theoretically derived bound. OMIB addresses imbalanced task-relevant information by dynamically adjusting regularization weights per modality. The optimization is established with an information-theoretical foundation and implemented under the variational approximation framework for computational efficiency.

Result: OMIB's theoretical properties are empirically validated on synthetic data, demonstrating its superiority over state-of-the-art benchmark methods in various downstream tasks.

Conclusion: OMIB provides a novel approach to optimizing multimodal learning by ensuring optimal MIB through theoretically derived bounds and dynamic weight adjustment, leading to enhanced model performance.

Abstract: Leveraging high-quality joint representations from multimodal data can
greatly enhance model performance in various machine-learning based
applications. Recent multimodal learning methods, based on the multimodal
information bottleneck (MIB) principle, aim to generate optimal MIB with
maximal task-relevant information and minimal superfluous information via
regularization. However, these methods often set ad hoc regularization weights
and overlook imbalanced task-relevant information across modalities, limiting
their ability to achieve optimal MIB. To address this gap, we propose a novel
multimodal learning framework, Optimal Multimodal Information Bottleneck
(OMIB), whose optimization objective guarantees the achievability of optimal
MIB by setting the regularization weight within a theoretically derived bound.
OMIB further addresses imbalanced task-relevant information by dynamically
adjusting regularization weights per modality, promoting the inclusion of all
task-relevant information. Moreover, we establish a solid
information-theoretical foundation for OMIB's optimization and implement it
under the variational approximation framework for computational efficiency.
Finally, we empirically validate the OMIB's theoretical properties on synthetic
data and demonstrate its superiority over the state-of-the-art benchmark
methods in various downstream tasks.

</details>


### [381] [Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents](https://arxiv.org/abs/2505.19997)
*Tao Wu,Jingyuan Chen,Wang Lin,Mengze Li,Yumeng Zhu,Ang Li,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: 提出了一种无训练的框架来模拟学生行为，通过构建认知原型和使用束搜索方法迭代细化以复制真实错误，实验结果表明该方法在模拟精度上提高了100%。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLMs）通常被训练为“有益助手”，旨在生成完美的回应，难以模拟具有不同认知能力的学生，因为它们常常产生过于先进的答案，缺乏学生学习中的自然缺陷，导致不切实际的模拟。

Method: 首先构建每个学生的认知原型，利用知识图谱捕捉他们从过往学习记录中对概念的理解；然后将此原型映射到新任务以预测学生表现；接着基于这些预测模拟学生解决方案，并使用束搜索方法迭代地改进它们以更好地复制现实中的错误。

Result: 在由100名学生进行Python编程和5,000条学习记录组成的Student_100数据集上的实验结果表明，该方法的表现始终优于基线模型，模拟准确度提高了100%。

Conclusion: 所提出的无训练框架能够更准确地模拟学生的行为，特别是在捕捉学生学习中的自然缺陷方面表现出色。

Abstract: Large language models (LLMs) are revolutionizing education, with LLM-based
agents playing a key role in simulating student behavior. A major challenge in
student simulation is modeling the diverse learning patterns of students at
various cognitive levels. However, current LLMs, typically trained as ``helpful
assistants'', target at generating perfect responses. As a result, they
struggle to simulate students with diverse cognitive abilities, as they often
produce overly advanced answers, missing the natural imperfections that
characterize student learning and resulting in unrealistic simulations. To
address this issue, we propose a training-free framework for student
simulation. We begin by constructing a cognitive prototype for each student
using a knowledge graph, which captures their understanding of concepts from
past learning records. This prototype is then mapped to new tasks to predict
student performance. Next, we simulate student solutions based on these
predictions and iteratively refine them using a beam search method to better
replicate realistic mistakes. To validate our approach, we construct the
\texttt{Student\_100} dataset, consisting of $100$ students working on Python
programming and $5,000$ learning records. Experimental results show that our
method consistently outperforms baseline models, achieving $100\%$ improvement
in simulation accuracy.

</details>


### [382] [TabPFN: One Model to Rule Them All?](https://arxiv.org/abs/2505.20003)
*Qiong Zhang,Yan Shuo Tan,Qinglong Tian,Pengfei Li*

Main category: cs.LG

TL;DR: Hollmann等人提出了TabPFN，一种基于transformer的深度学习模型，适用于表格数据的回归和分类任务。该模型在少量训练时间内，在最多包含10,000个样本的数据集上显著优于所有先前的方法。本文通过强调其作为近似贝叶斯推理的解释，为统计学受众提供了TabPFN的工作原理说明，并提供了更多证据证明TabPFN的“基础模型”能力，包括在半监督参数估计、协变量迁移下的预测以及异质处理效应估计中的卓越表现。此外，TabPFN在稀疏回归中优于LASSO，并在分类中打破了稳健性-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管已经存在多种针对表格数据的建模方法，但这些方法通常需要大量训练时间或在性能上有限制。因此，开发一种高效且高性能的通用模型对于各种统计任务至关重要。这促使了对TabPFN的研究，以验证它是否可以成为表格数据的基础模型并超越现有方法。

Method: TabPFN是一种基于transformer的深度学习模型，设计用于表格数据的回归和分类任务。它通过近似贝叶斯推理进行工作，并具备生成数据、密度估计、学习可重用嵌入和支持微调的能力。本文作者还展示了TabPFN在不同统计任务上的应用，包括半监督参数估计、协变量迁移下的预测、异质处理效应估计、稀疏回归和分类等。

Result: 研究表明，TabPFN在多个统计任务上显著优于现有的专业方法。具体而言，它在半监督参数估计、协变量迁移下的预测和异质处理效应估计中表现出色。同时，TabPFN在稀疏回归中优于LASSO，并在分类任务中打破了稳健性与效率之间的权衡。

Conclusion: TabPFN作为一种基于transformer的深度学习模型，具有成为表格数据基础模型的潜力。它不仅在训练时间上高效，而且在多个统计任务上表现出卓越性能，有可能取代现有的建模方法。本文提供的实验结果和开源代码进一步支持了这一结论。

Abstract: Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a
transformer-based deep learning model for regression and classification on
tabular data, which they claim "outperforms all previous methods on datasets
with up to 10,000 samples by a wide margin, using substantially less training
time." Furthermore, they have called TabPFN a "foundation model" for tabular
data, as it can support "data generation, density estimation, learning reusable
embeddings and fine-tuning". If these statements are well-supported, TabPFN may
have the potential to supersede existing modeling approaches on a wide range of
statistical tasks, mirroring a similar revolution in other areas of artificial
intelligence that began with the advent of large language models. In this
paper, we provide a tailored explanation of how TabPFN works for a statistics
audience, by emphasizing its interpretation as approximate Bayesian inference.
We also provide more evidence of TabPFN's "foundation model" capabilities: We
show that an out-of-the-box application of TabPFN vastly outperforms
specialized state-of-the-art methods for semi-supervised parameter estimation,
prediction under covariate shift, and heterogeneous treatment effect
estimation. We further show that TabPFN can outperform LASSO at sparse
regression and can break a robustness-efficiency trade-off in classification.
All experiments can be reproduced using the code provided at
https://github.com/qinglong-tian/tabpfn_study
(https://github.com/qinglong-tian/tabpfn_study).

</details>


### [383] [Data-Dependent Regret Bounds for Constrained MABs](https://arxiv.org/abs/2505.20010)
*Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper initiates the study of data-dependent regret bounds in constrained
MAB settings. These bounds depend on the sequence of losses that characterize
the problem instance. Thus, they can be much smaller than classical
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret bounds, while being equivalent to
them in the worst case. Despite this, data-dependent regret bounds have been
completely overlooked in constrained MAB settings. The goal of this paper is to
answer the following question: Can data-dependent regret bounds be derived in
the presence of constraints? We answer this question affirmatively in
constrained MABs with adversarial losses and stochastic constraints.
Specifically, our main focus is on the most challenging and natural settings
with hard constraints, where the learner must ensure that the constraints are
always satisfied with high probability. We design an algorithm with a regret
bound consisting of two data-dependent terms. The first term captures the
difficulty of satisfying the constraints, while the second one encodes the
complexity of learning independently of the presence of constraints. We also
prove a lower bound showing that these two terms are not artifacts of our
specific approach and analysis, but rather the fundamental components that
inherently characterize the complexities of the problem. Finally, in designing
our algorithm, we also derive some novel results in the related (and easier)
soft constraints settings, which may be of independent interest.

</details>


### [384] [Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare](https://arxiv.org/abs/2505.20020)
*Natallia Kokash,Lei Wang,Thomas H. Gillespie,Adam Belloum,Paola Grosso,Sara Quinney,Lang Li,Bernard de Bono*

Main category: cs.LG

TL;DR: The paper proposes a two-step data alignment strategy combining ontologies and LLMs for secure federated learning in healthcare using EHR data.


<details>
  <summary>Details</summary>
Motivation: To overcome privacy regulations and data heterogeneity challenges in large-scale machine learning with EHRs, while enabling collaborative modeling without sharing raw data through federated learning.

Method: Develop a two-step data alignment strategy that integrates ontologies and large language models to harmonize diverse clinical datasets for federated learning.

Result: Demonstrates effectiveness of the strategy in a real-world project involving semantic mapping of EHR data for secure, privacy-preserving federated learning.

Conclusion: The proposed data alignment approach successfully supports federated learning in healthcare by addressing data heterogeneity and privacy concerns.

Abstract: The rise of electronic health records (EHRs) has unlocked new opportunities
for medical research, but privacy regulations and data heterogeneity remain key
barriers to large-scale machine learning. Federated learning (FL) enables
collaborative modeling without sharing raw data, yet faces challenges in
harmonizing diverse clinical datasets. This paper presents a two-step data
alignment strategy integrating ontologies and large language models (LLMs) to
support secure, privacy-preserving FL in healthcare, demonstrating its
effectiveness in a real-world project involving semantic mapping of EHR data.

</details>


### [385] [Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage](https://arxiv.org/abs/2505.20026)
*Xinping Chen,Chen Liu*

Main category: cs.LG

TL;DR: The paper introduces GIT, a method for reconstructing training data from leaked gradients using a generative attack model.


<details>
  <summary>Details</summary>
Motivation: To develop an effective and efficient approach to reconstruct training data from leaked gradients in distributed learning environments.

Method: GIT employs a generative attack model whose architecture is designed based on theoretical analysis to align with the structure of the leaked model. It can be trained offline and deployed efficiently, relying only on leaked gradients for reconstruction.

Result: GIT accelerates convergence and enhances reconstruction quality when used as a prior for other methods. It outperforms existing methods across multiple datasets and demonstrates robustness under challenging conditions such as inaccurate gradients, data distribution shifts, and model parameter discrepancies.

Conclusion: GIT is a novel and effective approach for reconstructing training data from leaked gradients, showing superior performance and robustness compared to existing methods.

Abstract: We propose Gradient Inversion Transcript (GIT), a novel generative approach
for reconstructing training data from leaked gradients. GIT employs a
generative attack model, whose architecture is tailored to align with the
structure of the leaked model based on theoretical analysis. Once trained
offline, GIT can be deployed efficiently and only relies on the leaked
gradients to reconstruct the input data, rendering it applicable under various
distributed learning environments. When used as a prior for other iterative
optimization-based methods, GIT not only accelerates convergence but also
enhances the overall reconstruction quality. GIT consistently outperforms
existing methods across multiple datasets and demonstrates strong robustness
under challenging conditions, including inaccurate gradients, data distribution
shifts and discrepancies in model parameters.

</details>


### [386] [Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions](https://arxiv.org/abs/2505.20030)
*Wenbo Wei,Nicholas Chong Jia Le,Choy Heng Lai,Ling Feng*

Main category: cs.LG

TL;DR: 在LSTM训练过程中观察到一种新的'多重下降'现象，通过渐近稳定性分析发现测试损失的周期性变化与秩序和混沌之间的相变过程密切相关。全局最优时期发生在从秩序到混沌的第一次转变时。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到LSTM模型训练过程中出现了一种新颖的'多重下降'现象，即模型过度训练后，测试损失会经历多次上升和下降的趋势周期。

Method: 通过对模型进行渐近稳定性分析，研究了测试损失周期性变化与秩序和混沌之间相变过程的关系，并探讨了局部和全局最优时期的特性。

Result: 发现测试损失的周期性变化与秩序和混沌之间的相变过程密切相关，局部最优时期始终处于两个阶段之间的关键转变点；更重要的是，全局最优时期发生在从秩序到混沌的第一次转变时，此时'混沌边缘'的宽度最大，允许对更好的权重配置进行最佳探索。

Conclusion: LSTM训练中的'多重下降'现象可以通过秩序与混沌的相变过程解释，首次秩序到混沌的转变点是全局最优的学习时期。

Abstract: We observe a novel 'multiple-descent' phenomenon during the training process
of LSTM, in which the test loss goes through long cycles of up and down trend
multiple times after the model is overtrained. By carrying out asymptotic
stability analysis of the models, we found that the cycles in test loss are
closely associated with the phase transition process between order and chaos,
and the local optimal epochs are consistently at the critical transition point
between the two phases. More importantly, the global optimal epoch occurs at
the first transition from order to chaos, where the 'width' of the 'edge of
chaos' is the widest, allowing the best exploration of better weight
configurations for learning.

</details>


### [387] [Graph Wave Networks](https://arxiv.org/abs/2505.20034)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Yihan Guo,Xinghua Zhang,Chuan Zhou,Tingwen Liu,Li Guo*

Main category: cs.LG

TL;DR: The paper proposes a new method called graph wave equation for message passing in graph neural networks, which depicts the wave nature of graph signals and has stronger stability than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods model message passing between nodes as a heat diffusion process using heat equation, but it can hardly depict the wave nature of graph signals and has low stability.

Method: The authors consider message passing as a wave propagation process based on wave equation in physics, developing a graph wave equation to leverage the wave propagation on graphs. This equation is connected to traditional spectral GNNs, facilitating the design of graph wave networks based on various Laplacians.

Result: Extensive experiments show that Graph Wave Networks (GWNs) achieve state-of-the-art and efficient performance on benchmark datasets, and exhibit outstanding performance in addressing challenging graph problems like over-smoothing and heterophily.

Conclusion: The graph wave equation provides a more stable and efficient approach for message passing in graph neural networks compared to the heat equation, significantly enhancing model efficiency while ensuring its performance.

Abstract: Dynamics modeling has been introduced as a novel paradigm in message passing
(MP) of graph neural networks (GNNs). Existing methods consider MP between
nodes as a heat diffusion process, and leverage heat equation to model the
temporal evolution of nodes in the embedding space. However, heat equation can
hardly depict the wave nature of graph signals in graph signal processing.
Besides, heat equation is essentially a partial differential equation (PDE)
involving a first partial derivative of time, whose numerical solution usually
has low stability, and leads to inefficient model training. In this paper, we
would like to depict more wave details in MP, since graph signals are
essentially wave signals that can be seen as a superposition of a series of
waves in the form of eigenvector. This motivates us to consider MP as a wave
propagation process to capture the temporal evolution of wave signals in the
space. Based on wave equation in physics, we innovatively develop a graph wave
equation to leverage the wave propagation on graphs. In details, we demonstrate
that the graph wave equation can be connected to traditional spectral GNNs,
facilitating the design of graph wave networks based on various Laplacians and
enhancing the performance of the spectral GNNs. Besides, the graph wave
equation is particularly a PDE involving a second partial derivative of time,
which has stronger stability on graphs than the heat equation that involves a
first partial derivative of time. Additionally, we theoretically prove that the
numerical solution derived from the graph wave equation are constantly stable,
enabling to significantly enhance model efficiency while ensuring its
performance. Extensive experiments show that GWNs achieve SOTA and efficient
performance on benchmark datasets, and exhibit outstanding performance in
addressing challenging graph problems, such as over-smoothing and heterophily.

</details>


### [388] [Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction](https://arxiv.org/abs/2505.20036)
*Hazem Alsamkary,Mohamed Elshaffei,Mohamed Soudy,Sara Ossman,Abdallah Amr,Nehal Adel Abdelsalam,Mohamed Elkerdawy,Ahmed Elnaggar*

Main category: cs.LG

TL;DR: 本研究通过改进数据集和提出四种新架构来提升蛋白质语言模型在预测蛋白-蛋白相互作用结合亲和力中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言模型在预测蛋白质结构和功能方面取得了显著成功，但在基于序列的蛋白-蛋白相互作用结合亲和力预测方面的应用仍然较少被探索。这主要是由于高质量数据集的缺乏以及简单连接蛋白质表示策略的限制。

Method: 首先，研究人员精心整理了PPB-Affinity数据集，解决了多链蛋白质相互作用中的注释不一致和重复条目问题，并采用严格的序列身份阈值确保训练、验证和测试集的稳健分割。其次，提出了四种适应PLMs用于PPI结合亲和力预测的架构：嵌入连接（EC）、序列连接（SC）、分层池化（HP）和汇集注意力添加（PAD）。这些架构使用两种训练方法进行评估：完全微调和轻量级ConvBERT头法。

Result: 实验表明，HP和PAD架构始终优于传统的连接方法，在Spearman相关性上提高了多达12%。

Conclusion: 这些结果强调了需要复杂的架构设计以充分利用PLMs的能力来进行细致的PPI结合亲和力预测。

Abstract: Protein-protein interactions (PPIs) are fundamental to numerous cellular
processes, and their characterization is vital for understanding disease
mechanisms and guiding drug discovery. While protein language models (PLMs)
have demonstrated remarkable success in predicting protein structure and
function, their application to sequence-based PPI binding affinity prediction
remains relatively underexplored. This gap is often attributed to the scarcity
of high-quality, rigorously refined datasets and the reliance on simple
strategies for concatenating protein representations. In this work, we address
these limitations. First, we introduce a meticulously curated version of the
PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction
entries, by resolving annotation inconsistencies and duplicate entries for
multi-chain protein interactions. This dataset incorporates a stringent, less
than or equal to 30%, sequence identity threshold to ensure robust splitting
into training, validation, and test sets, minimizing data leakage. Second, we
propose and systematically evaluate four architectures for adapting PLMs to PPI
binding affinity prediction: embeddings concatenation (EC), sequences
concatenation (SC), hierarchical pooling (HP), and pooled attention addition
(PAD). These architectures were assessed using two training methods: full
fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM
features. Our comprehensive experiments across multiple leading PLMs (ProtT5,
ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures
consistently outperform conventional concatenation methods, achieving up to 12%
increase in terms of Spearman correlation. These results highlight the
necessity of sophisticated architectural designs to fully exploit the
capabilities of PLMs for nuanced PPI binding affinity prediction.

</details>


### [389] [Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks](https://arxiv.org/abs/2505.20048)
*Ali Forootani,Mohammad Khosravi*

Main category: cs.LG

TL;DR: 本研究通过超过1500次受控实验，评估了三种主要的Transformer时间序列预测架构（Autoformer、Informer和PatchTST），并引入了Deep Koopformer框架，该框架通过算子理论潜在状态建模增强了Transformer的稳定性和可解释性，适用于嘈杂和复杂的实际条件下的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在顺序建模中表现出成功，但在时间序列中的应用受到噪声敏感性、长距离依赖性和缺乏对时间结构的归纳偏差等挑战的限制。因此需要一个统一的原则框架来评估不同的Transformer架构，并探索改进模型稳定性和可解释性的方法。

Method: 研究者设计了一个统一的框架，用于基准测试三种主流的Transformer预测架构：Autoformer、Informer和PatchTST。每个架构被分为三个变体：Minimal、Standard和Full，代表复杂性和建模能力逐渐增加。研究者在十种合成信号上进行了超过1500次受控实验，涵盖五种补丁长度和五种预测范围，并在干净和嘈杂条件下进行分析。此外，还提出了Koopman增强型Transformer框架（Deep Koopformer），将算子理论潜在状态建模整合到Transformer中以提高稳定性和可解释性。

Result: 研究揭示了不同模型家族之间的一致模式，并证明了Deep Koopformer在非线性和混沌动力系统上的有效性。结果表明，基于Koopman的Transformer是一种有前途的混合方法，适合在嘈杂和复杂的现实条件下进行鲁棒、可解释且理论上扎实的时间序列预测。

Conclusion: 本研究提供了一个全面的框架来评估Transformer时间序列预测模型，并展示了Deep Koopformer在提高模型性能和可解释性方面的潜力。这为未来在复杂和嘈杂环境下的时间序列预测提供了新的方向。

Abstract: Time series forecasting plays a critical role in domains such as energy,
finance, and healthcare, where accurate predictions inform decision-making
under uncertainty. Although Transformer-based models have demonstrated success
in sequential modeling, their adoption for time series remains limited by
challenges such as noise sensitivity, long-range dependencies, and a lack of
inductive bias for temporal structure. In this work, we present a unified and
principled framework for benchmarking three prominent Transformer forecasting
architectures-Autoformer, Informer, and Patchtst-each evaluated through three
architectural variants: Minimal, Standard, and Full, representing increasing
levels of complexity and modeling capacity.
  We conduct over 1500 controlled experiments on a suite of ten synthetic
signals, spanning five patch lengths and five forecast horizons under both
clean and noisy conditions. Our analysis reveals consistent patterns across
model families.
  To advance this landscape further, we introduce the Koopman-enhanced
Transformer framework, Deep Koopformer, which integrates operator-theoretic
latent state modeling to improve stability and interpretability. We demonstrate
its efficacy on nonlinear and chaotic dynamical systems. Our results highlight
Koopman based Transformer as a promising hybrid approach for robust,
interpretable, and theoretically grounded time series forecasting in noisy and
complex real-world conditions.

</details>


### [390] [Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits](https://arxiv.org/abs/2505.20051)
*Gianmarco Genalti,Sujay Bhatt,Nicola Gatti,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 在重尾分段平稳多臂老虎机问题中，提出了一种新的Catoni风格的变化点检测策略，并引入了Robust-CPD-UCB算法，提供了后悔值的上界和任何策略可达到的最小后悔值的下界。


<details>
  <summary>Details</summary>
Motivation: 现有的随机非平稳多臂老虎机研究主要集中在伯努利或次高斯奖励等假设上，但金融和电信等领域存在重尾分布的情况，这些问题尚未被充分研究。

Method: 作者针对重尾分段平稳多臂老虎机问题，提出了基于Catoni风格的变化点检测策略，该策略利用了顺序估计理论的最新进展。进一步，结合乐观算法，设计了Robust-CPD-UCB算法，并分析了其后悔值上界及任何策略可达到的最小后悔值的下界。

Result: 理论分析表明，提出的Robust-CPD-UCB算法具有明确的后悔值上界，且证明了任何策略在此问题中的最小后悔值下界。实验结果表明，该方法在合成数据集和真实数据集上均表现良好。

Conclusion: 本文解决了重尾分段平稳多臂老虎机问题，提出的新变化点检测策略和Robust-CPD-UCB算法为该类问题提供了一个有效的解决方案，同时展示了理论与实证上的优越性。

Abstract: Regret minimization in stochastic non-stationary bandits gained popularity
over the last decade, as it can model a broad class of real-world problems,
from advertising to recommendation systems. Existing literature relies on
various assumptions about the reward-generating process, such as Bernoulli or
subgaussian rewards. However, in settings such as finance and
telecommunications, heavy-tailed distributions naturally arise. In this work,
we tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed
bandits, introduced by Bubeck et al., 2013, operate on the minimal assumption
that the finite absolute centered moments of maximum order $1+\epsilon$ are
uniformly bounded by a constant $v<+\infty$, for some $\epsilon \in (0,1]$. We
focus on the most popular non-stationary bandit setting, i.e., the
piecewise-stationary setting, in which the mean of reward-generating
distributions may change at unknown time steps. We provide a novel Catoni-style
change-point detection strategy tailored for heavy-tailed distributions that
relies on recent advancements in the theory of sequential estimation, which is
of independent interest. We introduce Robust-CPD-UCB, which combines this
change-point detection strategy with optimistic algorithms for bandits,
providing its regret upper bound and an impossibility result on the minimum
attainable regret for any policy. Finally, we validate our approach through
numerical experiments on synthetic and real-world datasets.

</details>


### [391] [Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations](https://arxiv.org/abs/2505.20052)
*Hazem Alsamkary,Mohamed Elshaffei,Mohamed Elkerdawy,Ahmed Elnaggar*

Main category: cs.LG

TL;DR: 为了突破蛋白质语言模型(PLMs)的限制，研究提出了一种多任务预训练策略，并开发了Ankh3模型。该模型通过两种目标联合优化：带多个掩码概率的掩码语言建模和仅依赖蛋白质序列输入的蛋白质序列完成。实验结果表明，这种方法可以学习到更丰富、更具泛化的蛋白质序列表示，在下游任务（如二级结构预测、荧光、GB1适应度和接触预测）中表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质语言模型主要集中在单一的预训练任务上，可能无法完全捕捉蛋白质序列的信息。尽管添加数据模式或监督目标可以提高PLMs的性能，但预训练通常仍聚焦于去噪腐败序列。这限制了PLMs的学习能力。

Method: 研究采用了多任务预训练策略，开发了Ankh3模型。该模型基于两个目标进行联合优化：1) 带有多个掩码概率的掩码语言建模；2) 仅使用蛋白质序列作为输入的蛋白质序列完成任务。

Result: 在下游任务中的表现显著提升，包括二级结构预测、荧光、GB1适应度和接触预测等任务。模型对蛋白质属性有了更全面的理解，从而提高了预测的稳健性和准确性。

Conclusion: 多任务预训练策略能够使PLMs从蛋白质序列中学习到更丰富和更具泛化性的表示，为蛋白质属性的预测提供了更强大的工具。

Abstract: Protein language models (PLMs) have emerged as powerful tools to detect
complex patterns of protein sequences. However, the capability of PLMs to fully
capture information on protein sequences might be limited by focusing on single
pre-training tasks. Although adding data modalities or supervised objectives
can improve the performance of PLMs, pre-training often remains focused on
denoising corrupted sequences. To push the boundaries of PLMs, our research
investigated a multi-task pre-training strategy. We developed Ankh3, a model
jointly optimized on two objectives: masked language modeling with multiple
masking probabilities and protein sequence completion relying only on protein
sequences as input. This multi-task pre-training demonstrated that PLMs can
learn richer and more generalizable representations solely from protein
sequences. The results demonstrated improved performance in downstream tasks,
such as secondary structure prediction, fluorescence, GB1 fitness, and contact
prediction. The integration of multiple tasks gave the model a more
comprehensive understanding of protein properties, leading to more robust and
accurate predictions.

</details>


### [392] [SAEs Are Good for Steering -- If You Select the Right Features](https://arxiv.org/abs/2505.20063)
*Dana Arad,Aaron Mueller,Yonatan Belinkov*

Main category: cs.LG

TL;DR: Sparse Autoencoders (SAEs) can be enhanced by distinguishing input features from output features using newly proposed scores, leading to significant improvements in steering models without labeled data.


<details>
  <summary>Details</summary>
Motivation: To improve the effectiveness of SAEs in influencing model outputs towards desired concepts without needing labeled data, by better understanding and differentiating the roles of various features.

Method: Develop input and output scores to identify and differentiate input features (capturing input patterns) from output features (having clear effects on model outputs).

Result: Filtering out features with low output scores leads to 2-3x improvements in steering with SAEs, making them competitive with supervised methods.

Conclusion: Distinguishing between input and output features via the proposed scores enhances SAE performance significantly.

Abstract: Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to
learn a decomposition of a model's latent space. This enables useful
applications such as steering - influencing the output of a model towards a
desired concept - without requiring labeled data. Current methods identify SAE
features to steer by analyzing the input tokens that activate them. However,
recent work has highlighted that activations alone do not fully describe the
effect of a feature on the model's output. In this work, we draw a distinction
between two types of features: input features, which mainly capture patterns in
the model's input, and output features, which have a human-understandable
effect on the model's output. We propose input and output scores to
characterize and locate these types of features, and show that high values for
both scores rarely co-occur in the same features. These findings have practical
implications: after filtering out features with low output scores, we obtain
2-3x improvements when steering with SAEs, making them competitive with
supervised methods.

</details>


### [393] [SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety](https://arxiv.org/abs/2505.20065)
*Geon-Hyeong Kim,Youngsoo Jang,Yu Jin Kim,Byoungjip Kim,Honglak Lee,Kyunghoon Bae,Moontae Lee*

Main category: cs.LG

TL;DR: An abstract about a new algorithm called SafeDPO, designed for optimizing safety alignment in Large Language Models.


<details>
  <summary>Details</summary>
Motivation: With the growing importance of ensuring safety in Large Language Models (LLMs), there is a need for a simpler approach to integrating safety constraints into Reinforcement Learning from Human Feedback (RLHF).

Method: The paper introduces SafeDPO, an algorithm based on Direct Preference Optimization (DPO) that optimizes safety alignment in one stage of policy learning without relaxation. It only adds one hyperparameter and requires minimal changes to standard DPO.

Result: SafeDPO eliminates the need for separate reward and cost models or sampling during fine-tuning while improving LLM safety.

Conclusion: SafeDPO shows competitive performance with state-of-the-art safety alignment algorithms in aligning with human preferences and enhancing safety.

Abstract: As Large Language Models (LLMs) continue to advance and find applications
across a growing number of fields, ensuring the safety of LLMs has become
increasingly critical. To address safety concerns, recent studies have proposed
integrating safety constraints into Reinforcement Learning from Human Feedback
(RLHF). However, these approaches tend to be complex, as they encompass
complicated procedures in RLHF along with additional steps required by the
safety constraints. Inspired by Direct Preference Optimization (DPO), we
introduce a new algorithm called SafeDPO, which is designed to directly
optimize the safety alignment objective in a single stage of policy learning,
without requiring relaxation. SafeDPO introduces only one additional
hyperparameter to further enhance safety and requires only minor modifications
to standard DPO. As a result, it eliminates the need to fit separate reward and
cost models or to sample from the language model during fine-tuning, while
still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO
achieves competitive performance compared to state-of-the-art safety alignment
algorithms, both in terms of aligning with human preferences and improving
safety.

</details>


### [394] [An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks](https://arxiv.org/abs/2505.20074)
*Jinyan Wang,Liu Yang,Yuecen Wei,Jiaxuan Si,Chenhao Guo,Qingyun Sun,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: The paper proposes GOOD-MIA, a novel method for cross-domain graph attacks that addresses privacy leakage risks in Graph Neural Network-based methods.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Network-based methods have privacy leakage risks due to topological structures, and current MIA assumptions increasingly deviate from reality as they presume attackers can obtain an auxiliary dataset with the same distribution.

Method: The authors categorize the distribution diversity issue as an Out-Of-Distribution (OOD) problem and propose GOOD-MIA. They construct shadow subgraphs with distributions from different domains, explore stable node representations, eliminate redundant information, and extract task-relevant key information. They also perform risk extrapolation to optimize the attack's domain adaptability.

Result: GOOD-MIA achieves superior attack performance in datasets designed for multiple domains.

Conclusion: This OOD-based design makes cross-domain graph attacks possible.

Abstract: Graph Neural Network-based methods face privacy leakage risks due to the
introduction of topological structures about the targets, which allows
attackers to bypass the target's prior knowledge of the sensitive attributes
and realize membership inference attacks (MIA) by observing and analyzing the
topology distribution. As privacy concerns grow, the assumption of MIA, which
presumes that attackers can obtain an auxiliary dataset with the same
distribution, is increasingly deviating from reality. In this paper, we
categorize the distribution diversity issue in real-world MIA scenarios as an
Out-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership
Inference Attack (GOOD-MIA) to achieve cross-domain graph attacks.
Specifically, we construct shadow subgraphs with distributions from different
domains to model the diversity of real-world data. We then explore the stable
node representations that remain unchanged under external influences and
consider eliminating redundant information from confounding environments and
extracting task-relevant key information to more clearly distinguish between
the characteristics of training data and unseen data. This OOD-based design
makes cross-domain graph attacks possible. Finally, we perform risk
extrapolation to optimize the attack's domain adaptability during attack
inference to generalize the attack to other domains. Experimental results
demonstrate that GOOD-MIA achieves superior attack performance in datasets
designed for multiple domains.

</details>


### [395] [Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior](https://arxiv.org/abs/2505.20076)
*Florian Eichin,Yupei Du,Philipp Mondorf,Barbara Plank,Michael A. Hedderich*

Main category: cs.LG

TL;DR: ExPLAIND is a unified framework integrating three perspectives to interpret model behavior and training dynamics.


<details>
  <summary>Details</summary>
Motivation: Post-hoc interpretability methods usually focus on isolated aspects, lacking a unified view and theoretical support when combined.

Method: Generalize gradient path kernels for realistic training settings, derive influence scores from kernel feature maps, and jointly interpret model components and data over the training process.

Result: Effectively replicated CNN and Transformer models, achieved comparable parameter pruning with existing methods, and refined understanding of Grokking in Transformers.

Conclusion: ExPLAIND offers a theoretically grounded, unified approach to interpreting model behavior and training dynamics.

Abstract: Post-hoc interpretability methods typically attribute a model's behavior to
its components, data, or training trajectory in isolation. This leads to
explanations that lack a unified view and may miss key interactions. While
combining existing methods or applying them at different training stages offers
broader insights, these approaches usually lack theoretical support. In this
work, we present ExPLAIND, a unified framework that integrates all three
perspectives. First, we generalize recent work on gradient path kernels, which
reformulate models trained by gradient descent as a kernel machine, to more
realistic training settings. Empirically, we find that both a CNN and a
Transformer model are replicated accurately by this reformulation. Second, we
derive novel parameter- and step-wise influence scores from the kernel feature
maps. We show their effectiveness in parameter pruning that is comparable to
existing methods, reinforcing their value for model component attribution.
Finally, jointly interpreting model components and data over the training
process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.
Among other things, our findings support previously proposed stages of
Grokking, while refining the final phase as one of alignment of input
embeddings and final layers around a representation pipeline learned after the
memorization phase. Overall, ExPLAIND provides a theoretically grounded,
unified framework to interpret model behavior and training dynamics.

</details>


### [396] [Spurious Privacy Leakage in Neural Networks](https://arxiv.org/abs/2505.20095)
*Chenxiang Zhang,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: Neural networks have privacy vulnerabilities, especially when trained on biased data. This paper explores how spurious correlation bias affects these vulnerabilities, introducing the concept of 'spurious privacy leakage'. It finds that even using robust methods to reduce spurious correlations does not stop this leakage, due to the memorization of spurious data. Additionally, it reveals that model architecture choices can influence privacy outcomes.


<details>
  <summary>Details</summary>
Motivation: To understand and address the increased privacy risks in neural networks caused by spurious correlation bias, particularly in situations with limited and biased training data.

Method: The study investigates the impact of spurious correlation bias on privacy vulnerability, introduces the concept of 'spurious privacy leakage', and examines why reducing spurious correlation doesn't mitigate this leakage. It also compares privacy outcomes across different model architectures trained with spurious data.

Result: Spurious groups are more vulnerable to privacy attacks than non-spurious groups. Privacy disparity increases in simpler tasks due to persistent spurious features. Reducing spurious correlation via robust methods doesn't affect spurious data memorization or privacy levels. Model architecture significantly influences privacy outcomes.

Conclusion: Spurious correlation bias significantly impacts privacy vulnerabilities in neural networks. Efforts to reduce spurious correlations don't necessarily improve privacy due to data memorization. Architectural choices in model design can play a crucial role in privacy outcomes.

Abstract: Neural networks are vulnerable to privacy attacks aimed at stealing sensitive
data. The risks can be amplified in a real-world scenario, particularly when
models are trained on limited and biased data. In this work, we investigate the
impact of spurious correlation bias on privacy vulnerability. We introduce
\emph{spurious privacy leakage}, a phenomenon where spurious groups are
significantly more vulnerable to privacy attacks than non-spurious groups. We
further show that group privacy disparity increases in tasks with simpler
objectives (e.g. fewer classes) due to the persistence of spurious features.
Surprisingly, we find that reducing spurious correlation using spurious robust
methods does not mitigate spurious privacy leakage. This leads us to introduce
a perspective on privacy disparity based on memorization, where mitigating
spurious correlation does not mitigate the memorization of spurious data, and
therefore, neither the privacy level. Lastly, we compare the privacy of
different model architectures trained with spurious data, demonstrating that,
contrary to prior works, architectural choice can affect privacy outcomes.

</details>


### [397] [Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning](https://arxiv.org/abs/2505.20107)
*Ziyi Zhang,Li Shen,Deheng Ye,Yong Luo,Huangxuan Zhao,Lefei Zhang*

Main category: cs.LG

TL;DR: The paper presents MVC-ZigAL, a reinforcement learning finetuning framework for few-step text-to-multiview (T2MV) diffusion models. It optimizes per-view fidelity and cross-view consistency through innovations like ZMV-Sampling and MV-ZigAL while preserving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current accelerated T2MV methods using few-step diffusion models often lead to sacrifices in image fidelity and view consistency.

Method: Propose MVC-ZigAL, which includes reformulating T2MV denoising as a Markov decision process, introducing ZMV-Sampling technique, developing MV-ZigAL strategy, and reframing RL finetuning as a constrained optimization problem.

Result: Effectively refines the few-step T2MV diffusion baseline in both fidelity and consistency while maintaining few-step efficiency.

Conclusion: MVC-ZigAL is a complete RL finetuning framework that addresses the challenges in few-step T2MV diffusion models.

Abstract: Text-to-multiview (T2MV) generation, which produces coherent multiview images
from a single text prompt, remains computationally intensive, while accelerated
T2MV methods using few-step diffusion models often sacrifice image fidelity and
view consistency. To address this, we propose a novel reinforcement learning
(RL) finetuning framework tailored for few-step T2MV diffusion models to
jointly optimize per-view fidelity and cross-view consistency. Specifically, we
first reformulate T2MV denoising across all views as a single unified Markov
decision process, enabling multiview-aware policy optimization driven by a
joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV
sampling technique that adds an inversion-denoising pass to reinforce both
viewpoint and text conditioning, resulting in improved T2MV generation at the
cost of inference time. To internalize its performance gains into the base
sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that
uses reward advantages of ZMV-Sampling over standard sampling as learning
signals for policy updates. Finally, noting that the joint-view reward
objective under-optimizes per-view fidelity but naively optimizing single-view
metrics neglects cross-view alignment, we reframe RL finetuning for T2MV
diffusion models as a constrained optimization problem that maximizes per-view
fidelity subject to an explicit joint-view constraint, thereby enabling more
efficient and balanced policy updates. By integrating this constrained
optimization paradigm with MV-ZigAL, we establish our complete RL finetuning
framework, referred to as MVC-ZigAL, which effectively refines the few-step
T2MV diffusion baseline in both fidelity and consistency while preserving its
few-step efficiency.

</details>


### [398] [Proxy-Free GFlowNet](https://arxiv.org/abs/2505.20110)
*Ruishuo Chen,Xun Wang,Rui Hu,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: The paper introduces Trajectory-Distilled GFlowNet (TD-GFN), a proxy-free training framework for Generative Flow Networks that eliminates the need for out-of-dataset reward queries by leveraging inverse reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: In many real-world applications, obtaining the reward function for compositional objects is expensive, time-consuming, or requires human input. This makes it necessary to train GFlowNets from historical datasets without relying on an approximated proxy model for the reward function.

Method: TD-GFN uses inverse reinforcement learning to estimate edge-level rewards from an offline dataset. It prunes the directed acyclic graph (DAG) based on these rewards and guides backward trajectory sampling during training, directing the policy towards high-reward regions while reducing model fitting complexity.

Result: Empirical results across multiple tasks demonstrate that TD-GFN trains efficiently and reliably, showing significant improvements over existing baselines in terms of convergence speed and sample quality.

Conclusion: TD-GFN offers a novel approach to training GFlowNets without the use of a proxy model, leading to more efficient and reliable training with better performance.

Abstract: Generative Flow Networks (GFlowNets) are a promising class of generative
models designed to sample diverse, high-reward structures by modeling
distributions over compositional objects. In many real-world applications,
obtaining the reward function for such objects is expensive, time-consuming, or
requires human input, making it necessary to train GFlowNets from historical
datasets. Most existing methods adopt a model-based approach, learning a proxy
model from the dataset to approximate the reward function. However, this
strategy inherently ties the quality of the learned policy to the accuracy of
the proxy, introducing additional complexity and uncertainty into the training
process. To overcome these limitations, we propose \textbf{Trajectory-Distilled
GFlowNet (TD-GFN)}, a \emph{proxy-free} training framework that eliminates the
need for out-of-dataset reward queries. Our method is motivated by the key
observation that different edges in the associated directed acyclic graph (DAG)
contribute unequally to effective policy learning. TD-GFN leverages inverse
reinforcement learning to estimate edge-level rewards from the offline dataset,
which are then used to ingeniously prune the DAG and guide backward trajectory
sampling during training. This approach directs the policy toward high-reward
regions while reducing the complexity of model fitting. Empirical results
across multiple tasks show that TD-GFN trains both efficiently and reliably,
significantly outperforming existing baselines in convergence speed and sample
quality.

</details>


### [399] [Understanding Generalization in Diffusion Models via Probability Flow Distance](https://arxiv.org/abs/2505.20123)
*Huijie Zhang,Zijian Huang,Siyi Chen,Jinfan Zhou,Zekai Zhang,Peng Wang,Qing Qu*

Main category: cs.LG

TL;DR: Diffusion models are powerful generative models, but evaluating their generalization is challenging. This paper introduces probability flow distance (PFD), a metric to measure distributional generalization in diffusion models. Using PFD under a teacher-student evaluation protocol reveals key generalization behaviors.


<details>
  <summary>Details</summary>
Motivation: There is a need for a practical and theoretically grounded metric to rigorously measure generalization in diffusion models, as current theoretical metrics are impractical for high-dimensional data and practical metrics lack rigor.

Method: The paper introduces probability flow distance (PFD), which quantifies the distance between distributions by comparing their noise-to-data mappings induced by the probability flow ODE. It uses PFD in a teacher-student evaluation protocol to study generalization behaviors.

Result: Empirical analysis using PFD uncovers key generalization behaviors in diffusion models such as scaling behavior from memorization to generalization, early learning and double descent training dynamics, and bias-variance decomposition.

Conclusion: The work lays a foundation for future empirical and theoretical studies on generalization in diffusion models.

Abstract: Diffusion models have emerged as a powerful class of generative models,
capable of producing high-quality samples that generalize beyond the training
data. However, evaluating this generalization remains challenging: theoretical
metrics are often impractical for high-dimensional data, while no practical
metrics rigorously measure generalization. In this work, we bridge this gap by
introducing probability flow distance ($\texttt{PFD}$), a theoretically
grounded and computationally efficient metric to measure distributional
generalization. Specifically, $\texttt{PFD}$ quantifies the distance between
distributions by comparing their noise-to-data mappings induced by the
probability flow ODE. Moreover, by using $\texttt{PFD}$ under a teacher-student
evaluation protocol, we empirically uncover several key generalization
behaviors in diffusion models, including: (1) scaling behavior from
memorization to generalization, (2) early learning and double descent training
dynamics, and (3) bias-variance decomposition. Beyond these insights, our work
lays a foundation for future empirical and theoretical studies on
generalization in diffusion models.

</details>


### [400] [Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach](https://arxiv.org/abs/2505.20130)
*Zhu Jin,Li Jingyi,Zhou Hongyi,Lin Yinan,Lin Zhenhua,Shi Chengchun*

Main category: cs.LG

TL;DR: This paper proposes a surrogate function for the mean squared error (MSE) of the causal effect estimator, enabling classical graph cut algorithms to optimize spatial experimental design. It accommodates large spatial interference, adapts to different spatial covariance functions, and is computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To optimize the amount of information derived from spatial experimental data and enhance the accuracy of the resulting causal effect estimator.

Method: Propose a surrogate function for the mean squared error (MSE) of the estimator that facilitates the use of classical graph cut algorithms to learn the optimal design.

Result: Theoretical results and numerical experiments validate the effectiveness of the proposed method in synthetic environments and a city-scale ridesharing market dispatch simulator.

Conclusion: The proposal offers three key advances - accommodating moderate to large spatial interference effects, adapting to different spatial covariance functions, and being computationally efficient.

Abstract: This paper focuses on the design of spatial experiments to optimize the
amount of information derived from the experimental data and enhance the
accuracy of the resulting causal effect estimator. We propose a surrogate
function for the mean squared error (MSE) of the estimator, which facilitates
the use of classical graph cut algorithms to learn the optimal design. Our
proposal offers three key advances: (1) it accommodates moderate to large
spatial interference effects; (2) it adapts to different spatial covariance
functions; (3) it is computationally efficient. Theoretical results and
numerical experiments based on synthetic environments and a dispatch simulator
that models a city-scale ridesharing market, further validate the effectiveness
of our design. A python implementation of our method is available at
https://github.com/Mamba413/CausalGraphCut.

</details>


### [401] [MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning](https://arxiv.org/abs/2505.20131)
*Yuanxin Zhuang,Dazhong Shen,Ying Sun*

Main category: cs.LG

TL;DR: MolEditRL is a new molecular editing framework that integrates structural constraints with property optimization, showing significant improvements over current methods.


<details>
  <summary>Details</summary>
Motivation: Current molecular editing approaches rely on string-based or continuous representations which do not adequately capture the discrete, graph-structured nature of molecules leading to limited structural fidelity and poor controllability.

Method: MolEditRL consists of two stages: (1) a discrete graph diffusion model pretrained to reconstruct target molecules conditioned on source structures and natural language instructions; (2) an editing-aware reinforcement learning fine-tuning stage that enhances property alignment and structural preservation by optimizing editing decisions under graph constraints.

Result: MolEditRL significantly outperforms state-of-the-art methods in both property optimization accuracy and structural fidelity, achieving a 74% improvement in editing success rate while using 98% fewer parameters.

Conclusion: MolEditRL offers a more effective way to edit molecules for desired chemical properties while preserving structural similarity.

Abstract: Molecular editing aims to modify a given molecule to optimize desired
chemical properties while preserving structural similarity. However, current
approaches typically rely on string-based or continuous representations, which
fail to adequately capture the discrete, graph-structured nature of molecules,
resulting in limited structural fidelity and poor controllability. In this
paper, we propose MolEditRL, a molecular editing framework that explicitly
integrates structural constraints with precise property optimization.
Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion
model pretrained to reconstruct target molecules conditioned on source
structures and natural language instructions; (2) an editing-aware
reinforcement learning fine-tuning stage that further enhances property
alignment and structural preservation by explicitly optimizing editing
decisions under graph constraints. For comprehensive evaluation, we construct
MolEdit-Instruct, the largest and most property-rich molecular editing dataset,
comprising 3 million diverse examples spanning single- and multi-property tasks
across 10 chemical attributes. Experimental results demonstrate that MolEditRL
significantly outperforms state-of-the-art methods in both property
optimization accuracy and structural fidelity, achieving a 74\% improvement in
editing success rate while using 98\% fewer parameters.

</details>


### [402] [Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks](https://arxiv.org/abs/2505.20132)
*Safa Hamreras,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: Tensorizing neural networks via low-rank tensor decompositions offers a promising but underutilized approach for model compression, architectural flexibility, and enhanced interpretability in deep learning.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of tensorized neural networks (TNNs) as a powerful framework for deep learning, emphasizing their advantages in model compression, distinctive scaling properties, and increased interpretability.

Method: Reshape dense weight matrices of neural networks into higher-order tensors and approximate them using low-rank tensor network decompositions, incorporating bond indices that introduce new latent spaces.

Result: TNNs provide effective model compression, flexible architectures with unique scaling properties, and deeper insight into feature evolution across layers through internal representations.

Conclusion: TNNs deserve more attention from both engineering and theoretical communities, with outlined research directions to overcome practical barriers for broader adoption in modern deep learning workflows.

Abstract: Tensorizing a neural network involves reshaping some or all of its dense
weight matrices into higher-order tensors and approximating them using low-rank
tensor network decompositions. This technique has shown promise as a model
compression strategy for large-scale neural networks. However, despite
encouraging empirical results, tensorized neural networks (TNNs) remain
underutilized in mainstream deep learning. In this position paper, we offer a
perspective on both the potential and current limitations of TNNs. We argue
that TNNs represent a powerful yet underexplored framework for deep
learning--one that deserves greater attention from both engineering and
theoretical communities. Beyond compression, we highlight the value of TNNs as
a flexible class of architectures with distinctive scaling properties and
increased interpretability. A central feature of TNNs is the presence of bond
indices, which introduce new latent spaces not found in conventional networks.
These internal representations may provide deeper insight into the evolution of
features across layers, potentially advancing the goals of mechanistic
interpretability. We conclude by outlining several key research directions
aimed at overcoming the practical barriers to scaling and adopting TNNs in
modern deep learning workflows.

</details>


### [403] [Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning](https://arxiv.org/abs/2505.20135)
*Wenyang Liao,Quanziang Wang,Yichen Wu,Renzhen Wang,Deyu Meng*

Main category: cs.LG

TL;DR: The paper proposes a new dataset distillation framework for replay-based continual learning (CL) to maintain global information and reduce forgetting.


<details>
  <summary>Details</summary>
Motivation: Replay-based CL methods may not effectively minimize empirical risk due to limited memory buffer capacity and heuristic selection criteria.

Method: A learnable memory buffer is maintained to distill global information from current task data and past knowledge. A lightweight distillation module generates soft labels for the memory buffer data to avoid computational overhead and overfitting.

Result: Extensive experiments demonstrate competitive results and effective mitigation of forgetting across various datasets.

Conclusion: The proposed method addresses limitations in replay-based CL, achieving strong performance and reducing forgetting.

Abstract: Replay-based continual learning (CL) methods assume that models trained on a
small subset can also effectively minimize the empirical risk of the complete
dataset. These methods maintain a memory buffer that stores a sampled subset of
data from previous tasks to consolidate past knowledge. However, this
assumption is not guaranteed in practice due to the limited capacity of the
memory buffer and the heuristic criteria used for buffer data selection. To
address this issue, we propose a new dataset distillation framework tailored
for CL, which maintains a learnable memory buffer to distill the global
information from the current task data and accumulated knowledge preserved in
the previous memory buffer. Moreover, to avoid the computational overhead and
overfitting risks associated with parameterizing the entire buffer during
distillation, we introduce a lightweight distillation module that can achieve
global information distillation solely by generating learnable soft labels for
the memory buffer data. Extensive experiments show that, our method can achieve
competitive results and effectively mitigates forgetting across various
datasets. The source code will be publicly available.

</details>


### [404] [Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks](https://arxiv.org/abs/2505.20137)
*Cédric Goemaere,Gaspard Oliviers,Rafal Bogacz,Thomas Demeester*

Main category: cs.LG

TL;DR: 本研究发现预测编码（PC）在深度神经网络训练中的信号衰减问题，并提出误差优化（EO）方法解决该问题，使性能媲美反向传播，同时为生物启发学习扩展到更深架构奠定基础。


<details>
  <summary>Details</summary>
Motivation: 预测编码（PC）作为一种生物合理性高的神经网络训练方法，在深度架构中表现不佳，主要因为存在信号衰减问题，导致梯度随深度指数级减弱，受数值精度限制而变得可忽略不计。

Method: 提出误差优化（EO），一种新的重参数化方法，保留了PC的理论特性，同时消除了信号衰减问题。通过优化预测误差而非状态，EO实现信号无衰减地同时到达所有层，从而显著加速收敛速度。

Result: 实验表明，EO在多种架构和数据集上均能匹配反向传播的性能，即使在传统PC难以应对的深层模型中也是如此。

Conclusion: 本研究不仅提供了对PC动力学的理论见解，还为将生物启发学习扩展到更深架构（包括数字硬件及其他领域）奠定了基础。

Abstract: Predictive Coding (PC) offers a biologically plausible alternative to
backpropagation for neural network training, yet struggles with deeper
architectures. This paper identifies the root cause: an inherent signal decay
problem where gradients attenuate exponentially with depth, becoming
computationally negligible due to numerical precision constraints. To address
this fundamental limitation, we introduce Error Optimization (EO), a novel
reparameterization that preserves PC's theoretical properties while eliminating
signal decay. By optimizing over prediction errors rather than states, EO
enables signals to reach all layers simultaneously and without attenuation,
converging orders of magnitude faster than standard PC. Experiments across
multiple architectures and datasets demonstrate that EO matches
backpropagation's performance even for deeper models where conventional PC
struggles. Besides practical improvements, our work provides theoretical
insight into PC dynamics and establishes a foundation for scaling
biologically-inspired learning to deeper architectures on digital hardware and
beyond.

</details>


### [405] [Model Stitching by Functional Latent Alignment](https://arxiv.org/abs/2505.20142)
*Ioannis Athanasiadis,Anmar Karmush,Michael Felsberg*

Main category: cs.LG

TL;DR: The paper proposes Functional Latent Alignment (FuLA) as a new optimality condition for model stitching to evaluate functional similarity of neural networks.


<details>
  <summary>Details</summary>
Motivation: Evaluating functional similarity of independently trained neural networks is an open problem with significant implications for AI.

Method: Drawing inspiration from knowledge distillation literature, the authors propose FuLA as a novel optimality condition for model stitching. They revisit previous testbeds and introduce a new one to validate FuLA.

Result: Experiments in adversarial training, shortcut training, and cross-layer stitching show that FuLA is less prone to artifacts and achieves better alignments than stitch-level matching.

Conclusion: FuLA emerges as a more reliable method for assessing functional similarity of neural networks.

Abstract: Evaluating functional similarity involves quantifying the degree to which
independently trained neural networks learn functionally similar
representations. Reliably inferring the functional similarity of these networks
remains an open problem with far-reaching implications for AI. Model stitching
has emerged as a promising paradigm, where an optimal affine transformation
aligns two models to solve a task, with the stitched model serving as a proxy
for functional similarity. In this work, we draw inspiration from the knowledge
distillation literature and propose Functional Latent Alignment (FuLA) as a
novel optimality condition for model stitching. We revisit previously explored
functional similarity testbeds and introduce a new one, based on which FuLA
emerges as an overall more reliable method of functional similarity.
Specifically, our experiments in (a) adversarial training, (b) shortcut
training and, (c) cross-layer stitching, reveal that FuLA is less prone to
artifacts tied to training on task cues while achieving non-trivial alignments
that are missed by stitch-level matching.

</details>


### [406] [On the (Non) Injectivity of Piecewise Linear Janossy Pooling](https://arxiv.org/abs/2505.20150)
*Ilai Reshef,Nadav Dym*

Main category: cs.LG

TL;DR: Multiset functions are crucial for neural networks dealing with multisets and graphs. While injective and bi-Lipschitz multiset mappings improve performance, they often increase compute time. This paper investigates k-ary Janossy pooling, showing that no piecewise linear Janossy pooling function can be injective. However, simple deep-sets models work well for sets without multiplicities.


<details>
  <summary>Details</summary>
Motivation: To explore whether simpler multiset functions can achieve the same guarantees (injective and bi-Lipschitz) as more complex ones without increasing compute time.

Method: The paper focuses on the family of k-ary Janossy pooling, analyzing its properties and proving that no piecewise linear Janossy pooling function can be injective. Additionally, it examines deep-sets models restricted to multisets without multiplicities.

Result: It was proven that no piecewise linear Janossy pooling function can be injective. On the other hand, when restricted to sets without multiplicities, even simple deep-sets models suffice for both injectivity and bi-Lipschitzness.

Conclusion: Simpler multiset functions achieving the desired guarantees may not exist within the family of k-ary Janossy pooling. For certain types of inputs (multisets without multiplicities), basic models like deep-sets can still be effective.

Abstract: Multiset functions, which are functions that map multisets to vectors, are a
fundamental tool in the construction of neural networks for multisets and
graphs. To guarantee that the vector representation of the multiset is
faithful, it is often desirable to have multiset mappings that are both
injective and bi-Lipschitz. Currently, there are several constructions of
multiset functions achieving both these guarantees, leading to improved
performance in some tasks but often also to higher compute time than standard
constructions. Accordingly, it is natural to inquire whether simpler multiset
functions achieving the same guarantees are available. In this paper, we make a
large step towards giving a negative answer to this question. We consider the
family of k-ary Janossy pooling, which includes many of the most popular
multiset models, and prove that no piecewise linear Janossy pooling function
can be injective. On the positive side, we show that when restricted to
multisets without multiplicities, even simple deep-sets models suffice for
injectivity and bi-Lipschitzness.

</details>


### [407] [Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning](https://arxiv.org/abs/2505.20161)
*Jaehun Jung,Seungju Han,Ximing Lu,Skyler Hallinan,David Acuna,Shrimai Prabhumoye,Mostafa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: The paper explores what kind of diversity in training data drives generalization in language models and introduces G-Vendi, a metric that quantifies diversity via the entropy of model-induced gradients. It also presents Prismatic Synthesis, a framework for generating diverse synthetic data.


<details>
  <summary>Details</summary>
Motivation: Existing diversity metrics for language model training data often rely on surface-level heuristics that are decoupled from model behavior. This motivates the authors to investigate what kind of diversity actually drives generalization in language models.

Method: The authors conduct large-scale empirical analyses across 300 training runs, introduce G-Vendi as a diversity metric based on the entropy of model-induced gradients, and present Prismatic Synthesis, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space.

Result: G-Vendi outperforms alternative measures with strong correlation (Spearman's ρ≈0.9) with out-of-distribution performance on NLI and math reasoning tasks. Prismatic Synthesis consistently improves model performance on both in-distribution tests and unseen, out-of-distribution benchmarks.

Conclusion: Data diversity can be a strong predictor of generalization in LLM reasoning. The introduced metric and framework significantly enhance model performance compared to state-of-the-art models.

Abstract: Effective generalization in language models depends critically on the
diversity of their training data. Yet existing diversity metrics often fall
short of this goal, relying on surface-level heuristics that are decoupled from
model behavior. This motivates us to ask: What kind of diversity in training
data actually drives generalization in language models -- and how can we
measure and amplify it? Through large-scale empirical analyses spanning over
300 training runs, carefully controlled for data scale and quality, we show
that data diversity can be a strong predictor of generalization in LLM
reasoning -- as measured by average model performance on unseen
out-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies
diversity via the entropy of model-induced gradients. Despite using a small
off-the-shelf proxy model for gradients, G-Vendi consistently outperforms
alternative measures, achieving strong correlation (Spearman's $\rho \approx
0.9$) with out-of-distribution (OOD) performance on both natural language
inference (NLI) and math reasoning tasks. Building on this insight, we present
Prismatic Synthesis, a framework for generating diverse synthetic data by
targeting underrepresented regions in gradient space. Experimental results show
that Prismatic Synthesis consistently improves model performance as we scale
synthetic data -- not just on in-distribution test but across unseen,
out-of-distribution benchmarks -- significantly outperforming state-of-the-art
models that rely on 20 times larger data generator than ours. For example,
PrismMath-7B, our model distilled from a 32B LLM, outperforms
R1-Distill-Qwen-7B -- the same base model trained on proprietary data generated
by 671B R1 -- on 6 out of 7 challenging benchmarks.

</details>


### [408] [A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation](https://arxiv.org/abs/2505.20172)
*Etienne Boursier,Scott Pesme,Radu-Alexandru Dragomir*

Main category: cs.LG

TL;DR: 研究了带有小权重衰减的梯度流的动力学，解释了深度学习中的grokking效应，并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 探讨梯度流在加入小权重衰减时的行为，解释深度学习中训练和测试损失的现象。

Method: 分析一般训练损失函数下带小权重衰减的梯度流轨迹，证明其存在两阶段行为，并通过几个合成回归任务进行实证。

Result: 展示了权重衰减导致的慢范数减少可以解释generalisation jump，并且在合成回归任务中验证了这一机制。

Conclusion: 小权重衰减引起的慢漂移阶段能够自然地解释grokking现象，此发现通过优化角度为深度学习中的观察提供了理论依据。

Abstract: We study the dynamics of gradient flow with small weight decay on general
training losses $F: \mathbb{R}^d \to \mathbb{R}$. Under mild regularity
assumptions and assuming convergence of the unregularised gradient flow, we
show that the trajectory with weight decay $\lambda$ exhibits a two-phase
behaviour as $\lambda \to 0$. During the initial fast phase, the trajectory
follows the unregularised gradient flow and converges to a manifold of critical
points of $F$. Then, at time of order $1/\lambda$, the trajectory enters a slow
drift phase and follows a Riemannian gradient flow minimising the $\ell_2$-norm
of the parameters. This purely optimisation-based phenomenon offers a natural
explanation for the \textit{grokking} effect observed in deep learning, where
the training loss rapidly reaches zero while the test loss plateaus for an
extended period before suddenly improving. We argue that this generalisation
jump can be attributed to the slow norm reduction induced by weight decay, as
explained by our analysis. We validate this mechanism empirically on several
synthetic regression tasks.

</details>


### [409] [The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination](https://arxiv.org/abs/2505.20177)
*Adam R. Klivans,Konstantinos Stavropoulos,Kevin Tian,Arsen Vasilyan*

Main category: cs.LG

TL;DR: 受到最近关于分布迁移学习工作的启发，本文提出了一种通用的异常值去除算法——迭代多项式过滤，并展示了在带有污染的监督学习中的多个重要应用。


<details>
  <summary>Details</summary>
Motivation: 现有的学习算法在处理数据污染时存在复杂度差距，尤其是低阶多项式逼近器被认为仅适用于标签噪声容忍，而对更广泛的污染类型（如对抗性噪声）效果有限。因此需要一种新的方法来解决这一问题并提高监督学习在污染环境下的效率。

Method: 提出了一个名为迭代多项式过滤的通用异常值去除算法。该算法通过以下步骤实现：(1) 利用低阶多项式逼近函数类以应对有界污染；(2) 对于支持夹层逼近器的函数类，获得接近最优的学习保证，即使在超过一半训练集被对抗性添加的情况下；(3) 首次提出针对任意固定对数凹分布的高效半空间函数容错可测试学习算法。

Result: (1) 证明了任何可以通过低阶多项式逼近的函数类都可以在有界污染下高效学习；(2) 在更重的加性污染下，对于支持夹层逼近器的函数类，获得了接近最优的学习保证；(3) 提出了首个高效的半空间函数容错可测试学习算法。

Conclusion: 这些结果显著推进了我们对在污染环境下高效监督学习的理解，弥补了与无监督学习研究相比的不足，解决了长期以来关于学习复杂度的差距问题。

Abstract: Inspired by recent work on learning with distribution shift, we give a
general outlier removal algorithm called iterative polynomial filtering and
show a number of striking applications for supervised learning with
contamination: (1) We show that any function class that can be approximated by
low-degree polynomials with respect to a hypercontractive distribution can be
efficiently learned under bounded contamination (also known as nasty noise).
This is a surprising resolution to a longstanding gap between the complexity of
agnostic learning and learning with contamination, as it was widely believed
that low-degree approximators only implied tolerance to label noise. (2) For
any function class that admits the (stronger) notion of sandwiching
approximators, we obtain near-optimal learning guarantees even with respect to
heavy additive contamination, where far more than $1/2$ of the training set may
be added adversarially. Prior related work held only for regression and in a
list-decodable setting. (3) We obtain the first efficient algorithms for
tolerant testable learning of functions of halfspaces with respect to any fixed
log-concave distribution. Even the non-tolerant case for a single halfspace in
this setting had remained open. These results significantly advance our
understanding of efficient supervised learning under contamination, a setting
that has been much less studied than its unsupervised counterpart.

</details>


### [410] [Research on feature fusion and multimodal patent text based on graph attention network](https://arxiv.org/abs/2505.20188)
*Zhenzhen Song,Ziwei Liu,Hongji Li*

Main category: cs.LG

TL;DR: This paper proposes HGM-Net, a deep learning framework integrating Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention Network (M-GAT) and Multi-Granularity Sparse Attention (MSA). It aims to solve problems in cross-modal feature fusion, long text modeling efficiency, and hierarchical semantic coherence in patent text semantic mining. Experiments show that the framework has significant advantages in tasks like patent classification and similarity matching.


<details>
  <summary>Details</summary>
Motivation: To address the issues of cross-modal feature fusion, low efficiency of long text modeling, and lack of hierarchical semantic coherence in patent text semantic mining.

Method: Propose HGM-Net which integrates HCL, M-GAT, and MSA. HCL builds dynamic mask, contrast, and cross-structural similarity constraints on word, sentence, and paragraph hierarchies; M-GAT models patent classification codes, citation relations, and text semantics as heterogeneous graph structures for dynamic multi-source feature fusion; MSA optimizes computational efficiency of long text modeling with a hierarchical sparsity strategy.

Result: Experiments demonstrate that HGM-Net shows significant advantages over existing deep learning methods in tasks such as patent classification and similarity matching.

Conclusion: HGM-Net provides a solution with both theoretical innovation and practical value for improving patent examination efficiency and mining technology relevance.

Abstract: Aiming at the problems of cross-modal feature fusion, low efficiency of long
text modeling and lack of hierarchical semantic coherence in patent text
semantic mining, this study proposes HGM-Net, a deep learning framework that
integrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention
Network (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a
dynamic mask, contrast and cross-structural similarity constraints on the word,
sentence and paragraph hierarchies through HCL. Contrast and cross-structural
similarity constraints are constructed at the word and paragraph levels by HCL
to strengthen the local semantic and global thematic consistency of patent
text; M-GAT models patent classification codes, citation relations and text
semantics as heterogeneous graph structures, and achieves dynamic fusion of
multi-source features by cross-modal gated attention; MSA adopts a hierarchical
sparsity strategy to optimize the computational efficiency of long text
modeling at word, phrase, sentence and paragraph granularity. Experiments show
that the framework demonstrates significant advantages over existing deep
learning methods in tasks such as patent classification and similarity
matching, and provides a solution with both theoretical innovation and
practical value for solving the problems of patent examination efficiency
improvement and technology relevance mining.

</details>


### [411] [FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement](https://arxiv.org/abs/2505.20192)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Cunyin Peng,Yicheng Chen,Xiangyu Zhao,Jinjie Gu,Chenyi Zhuang*

Main category: cs.LG

TL;DR: FunReason is a new framework that improves LLMs' function calling abilities via automated data refinement and Self-Refinement Multiscale Loss (SRML) to balance reasoning and function call accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for integrating LLMs with function calling have difficulty balancing detailed reasoning steps with precise function calls, leading to suboptimal performance.

Method: FunReason uses an automated data refinement strategy and SRML. It leverages LLMs' reasoning abilities to create high-quality training examples focusing on query parseability, reasoning coherence, and function call precision. SRML dynamically balances the contributions of reasoning processes and function call accuracy during training.

Result: FunReason achieves performance comparable to GPT-4o while mitigating catastrophic forgetting during fine-tuning.

Conclusion: FunReason provides a comprehensive solution for enhancing LLMs' function calling capabilities through balanced training methodology and a data refinement pipeline.

Abstract: The integration of large language models (LLMs) with function calling has
emerged as a crucial capability for enhancing their practical utility in
real-world applications. However, effectively combining reasoning processes
with accurate function execution remains a significant challenge. Traditional
training approaches often struggle to balance the detailed reasoning steps with
the precision of function calls, leading to suboptimal performance. To address
these limitations, we introduce FunReason, a novel framework that enhances
LLMs' function calling capabilities through an automated data refinement
strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason
leverages LLMs' natural reasoning abilities to generate high-quality training
examples, focusing on query parseability, reasoning coherence, and function
call precision. The SRML approach dynamically balances the contribution of
reasoning processes and function call accuracy during training, addressing the
inherent trade-off between these two critical aspects. FunReason achieves
performance comparable to GPT-4o while effectively mitigating catastrophic
forgetting during fine-tuning. FunReason provides a comprehensive solution for
enhancing LLMs' function calling capabilities by introducing a balanced
training methodology and a data refinement pipeline. For code and dataset,
please refer to our repository at GitHub
https://github.com/BingguangHao/FunReason

</details>


### [412] [Parameter-Efficient Fine-Tuning with Column Space Projection](https://arxiv.org/abs/2505.20211)
*Junseo Hwang,Wonguk Cho,Taesup Kim*

Main category: cs.LG

TL;DR: The paper introduces PiCa, a new PEFT method based on spectral properties of fine-tuned weights that closely aligns with full fine-tuning patterns and outperforms LoRA with significantly fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Recent studies reveal that LoRA's learning behavior differs from full fine-tuning, especially in spectral properties, which motivates the development of a more aligned PEFT method.

Method: PiCa projects gradients onto the low-rank column subspace of pre-trained weights and can be combined with weight sharing to reduce trainable parameters.

Result: PiCa achieves state-of-the-art performance compared to existing PEFT methods, surpassing LoRA's performance with 13 times fewer trainable parameters.

Conclusion: PiCa is the first theoretically grounded PEFT method based on spectral properties, offering superior performance with fewer parameters through effective gradient projection and weight sharing.

Abstract: Fine-tuning large language models (LLMs) with minimal computational overhead
is essential for efficiently adapting them to downstream tasks under resource
constraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank
Adaptation (LoRA), facilitate this by updating only a small subset of
parameters. However, recent studies show that LoRA diverges from full
fine-tuning (Full FT) in its learning behavior, particularly in terms of
spectral properties. Motivated by these findings, we propose PiCa, the first
theoretically grounded PEFT method based on the spectral properties of
fine-tuned weights. PiCa projects gradients onto the low-rank column subspace
of pre-trained weights and exhibits learning patterns more closely aligned with
Full FT. Furthermore, we show that combining PiCa with weight sharing
drastically reduces the number of trainable parameters without compromising
performance, enabling to achieve superior performance than LoRA using 13x fewer
trainable parameters. Extensive experiments demonstrate PiCa achieves the
state-of-the-art performance compared to existing PEFT methods.

</details>


### [413] [Fine-grained List-wise Alignment for Generative Medication Recommendation](https://arxiv.org/abs/2505.20218)
*Chenxiao Fan,Chongming Gao,Wentao Shi,Yaxin Gong,Zihao Zhao,Fuli Feng*

Main category: cs.LG

TL;DR: FLAME is a new framework for medication recommendation that considers drug interactions and optimizes prescription composition step-by-step, showing superior accuracy and safety.


<details>
  <summary>Details</summary>
Motivation: Existing medication recommendation systems use point-wise prediction which fails to consider synergistic drug effects and DDIs, especially important in multimorbidity cases.

Method: FLAME formulates drug recommendation as a sequential decision process with step-wise addition or removal of drugs. It uses Group Relative Policy Optimization with reward shaping to model DDIs and optimize drug contributions. Structured clinical knowledge and collaborative info are integrated into LLMs representation space.

Result: FLAME shows state-of-the-art performance with better accuracy, controllable safety-accuracy trade-offs, and strong generalization across various clinical scenarios.

Conclusion: FLAME provides an advanced approach for accurate and safe medication recommendations, outperforming existing systems.

Abstract: Accurate and safe medication recommendations are critical for effective
clinical decision-making, especially in multimorbidity cases. However, existing
systems rely on point-wise prediction paradigms that overlook synergistic drug
effects and potential adverse drug-drug interactions (DDIs). We propose FLAME,
a fine-grained list-wise alignment framework for large language models (LLMs),
enabling drug-by-drug generation of drug lists. FLAME formulates recommendation
as a sequential decision process, where each step adds or removes a single
drug. To provide fine-grained learning signals, we devise step-wise Group
Relative Policy Optimization (GRPO) with potential-based reward shaping, which
explicitly models DDIs and optimizes the contribution of each drug to the
overall prescription. Furthermore, FLAME enhances patient modeling by
integrating structured clinical knowledge and collaborative information into
the representation space of LLMs. Experiments on benchmark datasets demonstrate
that FLAME achieves state-of-the-art performance, delivering superior accuracy,
controllable safety-accuracy trade-offs, and strong generalization across
diverse clinical scenarios. Our code is available at
https://github.com/cxfann/Flame.

</details>


### [414] [Gradient Flow Matching for Learning Update Dynamics in Neural Network Training](https://arxiv.org/abs/2505.20221)
*Xiao Shou,Yanna Ding,Jianxi Gao*

Main category: cs.LG

TL;DR: 提出了一种名为Gradient Flow Matching (GFM)的新方法，用于加速深度神经网络的训练。通过将训练过程建模为受优化器感知向量场控制的动力系统，GFM能够准确预测最终权重，并且在不同神经网络架构和初始化条件下表现良好。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的训练由于基于梯度的优化方法的迭代性质而计算成本高。因此，需要一种新的框架来改进训练效率并研究优化动力学。

Method: GFM利用条件流匹配技术，将神经网络训练视为由学习到的优化器感知向量场控制的动力系统。该方法捕捉优化器（如SGD、Adam和RMSprop）的底层更新规则，从而实现对权重轨迹的平滑外推直至收敛。此外，GFM将基于梯度的更新结构知识整合到学习目标中，以促进从部分训练序列中准确预测最终权重。

Result: 实验表明，GFM的预测准确性与基于Transformer的模型相当，并显著优于LSTM和其他经典基线。此外，GFM适用于各种神经网络架构和初始化条件，提供了一个统一的研究优化动力学和加速收敛预测的框架。

Conclusion: GFM为研究优化动力学和加速收敛预测提供了一个通用框架，具有广泛的应用潜力。

Abstract: Training deep neural networks remains computationally intensive due to the
itera2 tive nature of gradient-based optimization. We propose Gradient Flow
Matching (GFM), a continuous-time modeling framework that treats neural network
training as a dynamical system governed by learned optimizer-aware vector
fields. By leveraging conditional flow matching, GFM captures the underlying
update rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth
extrapolation of weight trajectories toward convergence. Unlike black-box
sequence models, GFM incorporates structural knowledge of gradient-based
updates into the learning objective, facilitating accurate forecasting of final
weights from partial training sequences. Empirically, GFM achieves forecasting
accuracy that is competitive with Transformer-based models and significantly
outperforms LSTM and other classical baselines. Furthermore, GFM generalizes
across neural architectures and initializations, providing a unified framework
for studying optimization dynamics and accelerating convergence prediction.

</details>


### [415] [From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance](https://arxiv.org/abs/2505.20229)
*Maximilian Dreyer,Lorenz Hufe,Jim Berend,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Transformer-based CLIP models are widely used for text-image probing and feature extraction. This paper introduces a scalable framework that reveals the internal mechanisms behind their predictions by adapting attribution patching for instance-wise component attributions in CLIP. The method uncovers hundreds of surprising components linked to polysemous words, compound nouns, visual typography and dataset artifacts.


<details>
  <summary>Details</summary>
Motivation: Transformer-based CLIP models are widely used for text-image probing and feature extraction, making it relevant to understand the internal mechanisms behind their predictions. Recent works show that Sparse Autoencoders (SAEs) yield interpretable latent components but focus on what these encode and miss how they drive predictions.

Method: The authors introduce a scalable framework that reveals what latent components activate for, how they align with expected semantics, and how important they are to predictions. They adapt attribution patching for instance-wise component attributions in CLIP and highlight key faithfulness limitations of the widely used Logit Lens technique.

Result: The method uncovers hundreds of surprising components linked to polysemous words, compound nouns, visual typography and dataset artifacts. Text embeddings remain prone to semantic ambiguity but are more robust to spurious correlations compared to linear classifiers trained on image embeddings.

Conclusion: A case study on skin lesion detection highlights how such classifiers can amplify hidden shortcuts, underscoring the need for holistic, mechanistic interpretability.

Abstract: Transformer-based CLIP models are widely used for text-image probing and
feature extraction, making it relevant to understand the internal mechanisms
behind their predictions. While recent works show that Sparse Autoencoders
(SAEs) yield interpretable latent components, they focus on what these encode
and miss how they drive predictions. We introduce a scalable framework that
reveals what latent components activate for, how they align with expected
semantics, and how important they are to predictions. To achieve this, we adapt
attribution patching for instance-wise component attributions in CLIP and
highlight key faithfulness limitations of the widely used Logit Lens technique.
By combining attributions with semantic alignment scores, we can automatically
uncover reliance on components that encode semantically unexpected or spurious
concepts. Applied across multiple CLIP variants, our method uncovers hundreds
of surprising components linked to polysemous words, compound nouns, visual
typography and dataset artifacts. While text embeddings remain prone to
semantic ambiguity, they are more robust to spurious correlations compared to
linear classifiers trained on image embeddings. A case study on skin lesion
detection highlights how such classifiers can amplify hidden shortcuts,
underscoring the need for holistic, mechanistic interpretability. We provide
code at https://github.com/maxdreyer/attributing-clip.

</details>


### [416] [Multimodal Federated Learning With Missing Modalities through Feature Imputation Network](https://arxiv.org/abs/2505.20232)
*Pranav Poudel,Aavash Chhetri,Prashnna Gyawali,Georgios Leontidis,Binod Bhattarai*

Main category: cs.LG

TL;DR: Multimodal federated learning faces challenges due to missing modalities. Previous solutions using real or synthetic data are impractical or error-prone. This paper proposes a lightweight feature translator to reconstruct features of missing modalities, improving performance on healthcare datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of missing modalities in multimodal federated learning for healthcare, which is caused by various factors such as clinical practice variations, cost, privacy concerns, etc. Previous methods relying on real or synthetic data are either impractical or computationally expensive.

Method: A novel, lightweight, low-dimensional feature translator is proposed to reconstruct bottleneck features of missing modalities in healthcare datasets without the need for generative models.

Result: Experiments conducted on three different datasets (MIMIC-CXR, NIH Open-I, and CheXpert) in both homogeneous and heterogeneous settings show consistent improvement over competitive baselines.

Conclusion: The proposed feature translator effectively addresses the issue of missing modalities in multimodal federated learning for healthcare, leading to improved model performance.

Abstract: Multimodal federated learning holds immense potential for collaboratively
training models from multiple sources without sharing raw data, addressing both
data scarcity and privacy concerns, two key challenges in healthcare. A major
challenge in training multimodal federated models in healthcare is the presence
of missing modalities due to multiple reasons, including variations in clinical
practice, cost and accessibility constraints, retrospective data collection,
privacy concerns, and occasional technical or human errors. Previous methods
typically rely on publicly available real datasets or synthetic data to
compensate for missing modalities. However, obtaining real datasets for every
disease is impractical, and training generative models to synthesize missing
modalities is computationally expensive and prone to errors due to the high
dimensionality of medical data. In this paper, we propose a novel, lightweight,
low-dimensional feature translator to reconstruct bottleneck features of the
missing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH
Open-I, and CheXpert), in both homogeneous and heterogeneous settings
consistently improve the performance of competitive baselines. The code and
implementation details are available at:
https://github.com/bhattarailab/FedFeatGen

</details>


### [417] [Variational Deep Learning via Implicit Regularization](https://arxiv.org/abs/2505.20235)
*Jonathan Wenger,Beau Coker,Juraj Marusic,John P. Cunningham*

Main category: cs.LG

TL;DR: The paper shows how to regularize a variational deep network implicitly via the optimization procedure for reliable uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning models generalize well in-distribution but need reliable uncertainty quantification for out-of-distribution tasks.

Method: The authors characterize the inductive bias of (stochastic) gradient descent as generalized variational inference and demonstrate its importance in parametrization.

Result: This approach achieves strong in- and out-of-distribution performance without tuning additional hyperparameters and with minimal overhead over standard deep learning.

Conclusion: Implicit regularization via optimization can provide effective uncertainty quantification in variational deep networks.

Abstract: Modern deep learning models generalize remarkably well in-distribution,
despite being overparametrized and trained with little to no explicit
regularization. Instead, current theory credits implicit regularization imposed
by the choice of architecture, hyperparameters and optimization procedure.
However, deploying deep learning models out-of-distribution, in sequential
decision-making tasks, or in safety-critical domains, necessitates reliable
uncertainty quantification, not just a point estimate. The machinery of modern
approximate inference -- Bayesian deep learning -- should answer the need for
uncertainty quantification, but its effectiveness has been challenged by our
inability to define useful explicit inductive biases through priors, as well as
the associated computational burden. Instead, in this work we demonstrate, both
theoretically and empirically, how to regularize a variational deep network
implicitly via the optimization procedure, just as for standard deep learning.
We fully characterize the inductive bias of (stochastic) gradient descent in
the case of an overparametrized linear model as generalized variational
inference and demonstrate the importance of the choice of parametrization.
Finally, we show empirically that our approach achieves strong in- and
out-of-distribution performance without tuning of additional hyperparameters
and with minimal time and memory overhead over standard deep learning.

</details>


### [418] [DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2505.20241)
*Qi Cao,Ruiyi Wang,Ruiyi Zhang,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.LG

TL;DR: The paper introduces DreamPRM, a domain-reweighted training framework for multimodal Process Reward Models (PRMs) using bi-level optimization. It addresses dataset quality imbalance and enhances generalization capability, improving the performance of MLLMs on multimodal reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning presents greater challenges than text-only scenarios due to a wider range of tasks and more severe distribution shifts from training to testing sets. Current multimodal reasoning datasets have quality imbalances that degrade PRM performance, necessitating an effective data selection strategy.

Method: DreamPRM employs bi-level optimization: lower-level optimization fine-tunes PRM on multiple datasets with domain weights to prioritize high-quality reasoning signals; upper-level optimization evaluates PRM on a meta-learning dataset and updates domain weights via an aggregation loss function.

Result: Extensive experiments demonstrate that test-time scaling with DreamPRM consistently enhances the performance of state-of-the-art MLLMs on multimodal reasoning benchmarks, surpassing other data selection methods and test-time scaling approaches.

Conclusion: DreamPRM effectively addresses dataset quality imbalance and improves generalization capability, leading to higher accuracy gains in multimodal reasoning tasks.

Abstract: Reasoning has substantially improved the performance of large language models
(LLMs) on complicated tasks. Central to the current reasoning studies, Process
Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning
steps and guide the reasoning process. However, extending PRMs to multimodal
large language models (MLLMs) introduces challenges. Since multimodal reasoning
covers a wider range of tasks compared to text-only scenarios, the resulting
distribution shift from the training to testing sets is more severe, leading to
greater generalization difficulty. Training a reliable multimodal PRM,
therefore, demands large and diverse datasets to ensure sufficient coverage.
However, current multimodal reasoning datasets suffer from a marked quality
imbalance, which degrades PRM performance and highlights the need for an
effective data selection strategy. To address the issues, we introduce
DreamPRM, a domain-reweighted training framework for multimodal PRMs which
employs bi-level optimization. In the lower-level optimization, DreamPRM
performs fine-tuning on multiple datasets with domain weights, allowing the PRM
to prioritize high-quality reasoning signals and alleviating the impact of
dataset quality imbalance. In the upper-level optimization, the PRM is
evaluated on a separate meta-learning dataset; this feedback updates the domain
weights through an aggregation loss function, thereby improving the
generalization capability of trained PRM. Extensive experiments on multiple
multimodal reasoning benchmarks covering both mathematical and general
reasoning show that test-time scaling with DreamPRM consistently improves the
performance of state-of-the-art MLLMs. Further comparisons reveal that
DreamPRM's domain-reweighting strategy surpasses other data selection methods
and yields higher accuracy gains than existing test-time scaling approaches.

</details>


### [419] [RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large Language Models](https://arxiv.org/abs/2505.20242)
*Nguyen Thach,Aida Riahifar,Nathan Huynh,Hau Chan*

Main category: cs.LG

TL;DR: This paper introduces RedAHD, an end-to-end framework using large language models (LLMs) to automate the design of heuristics for solving NP-hard combinatorial optimization problems (COPs). It eliminates the need for generalized algorithmic frameworks (GAFs) by employing LLMs in the process of reduction, transforming complex COPs into better-understood ones. The experimental results across six COPs indicate that RedAHD can produce competitive or superior outcomes with minimal human intervention.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of traditional methods for solving COPs, which require substantial domain knowledge and manual effort from human experts. While existing LLM-based approaches show promise, they are not fully end-to-end and still demand considerable manual input.

Method: RedAHD uses LLMs to automate the reduction process, converting the given COP into more comprehensible COPs. This allows LLM-based heuristic design methods to create effective heuristics for these transformed problems, thereby indirectly solving the original problem without reliance on GAFs.

Result: Evaluated on six COPs, RedAHD demonstrates the ability to design heuristics that yield competitive or improved results compared to state-of-the-art methods, with significantly reduced human involvement.

Conclusion: RedAHD represents a significant advancement in automating heuristic design for COPs through its end-to-end approach and elimination of GAF dependency, offering potential improvements in efficiency and performance.

Abstract: Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling
salesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in
practice traditionally involves handcrafting heuristics or specifying a search
space for finding effective heuristics. The main challenges from these
approaches, however, are the sheer amount of domain knowledge and
implementation efforts required from human experts. Recently, significant
progress has been made to address these challenges, particularly by using large
language models (LLMs) to design heuristics within some predetermined
generalized algorithmic framework (GAF, e.g., ant colony optimization and
guided local search) for building key functions/components (e.g., a priori
information on how promising it is to include each edge in a solution for TSP
and CVRP). Although existing methods leveraging this idea have shown to yield
impressive optimization performance, they are not fully end-to-end and still
require considerable manual interventions. In this paper, we propose a novel
end-to-end framework, named RedAHD, that enables these LLM-based heuristic
design methods to operate without the need of GAFs. More specifically, RedAHD
employs LLMs to automate the process of reduction, i.e., transforming the COP
at hand into similar COPs that are better-understood, from which LLM-based
heuristic design methods can design effective heuristics for directly solving
the transformed COPs and, in turn, indirectly solving the original COP. Our
experimental results, evaluated on six COPs, show that RedAHD is capable of
designing heuristics with competitive or improved results over the
state-of-the-art methods with minimal human involvement.

</details>


### [420] [Learning Extrapolative Sequence Transformations from Markov Chains](https://arxiv.org/abs/2505.20251)
*Sophia Hager,Aleem Khan,Andrew Wang,Nicholas Andrews*

Main category: cs.LG

TL;DR: 通过从MCMC搜索产生的马尔可夫链中学习，提出了一种自回归模型，该模型能够有效地生成新的序列，比MCMC方法在蛋白质序列设计、文本情感控制和文本匿名化三个问题上表现更好或至少相当，同时具有更高的采样效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 深度学习的成功应用通常涉及相似的训练和测试条件。然而，在生物序列设计等任务中，需要寻找超出已知值的序列以改善期望的特性，这需要超越训练数据的新假设。在此背景下，使用随机搜索方法如MCMC可以实现外推，但即使有良好的提议分布，MCMC可能仍难以有效探索大型结构化状态空间。因此，需要一种贪婪优化期望特性的模型，能够在尽可能少的步骤中成功外推。

Method: 作者提出从MCMC搜索产生的马尔可夫链中学习一个模型。具体来说，该方法选择马尔可夫链中的某些状态作为训练数据来训练一个自回归模型。然后，这个自回归模型能够高效地生成新的序列，这些序列在外推感兴趣的序列级属性方面表现出色。

Result: 该方法在三个问题上进行了验证：蛋白质序列设计、文本情感控制和文本匿名化。结果表明，自回归模型能够与MCMC一样好甚至更好，并且具有额外的好处，包括可扩展性和显著更高的样本效率。

Conclusion: 所提出的自回归模型是一种有效的工具，可以在复杂的序列设计任务中替代MCMC方法，提供更高的效率和更好的性能。

Abstract: Most successful applications of deep learning involve similar training and
test conditions. However, tasks such as biological sequence design involve
searching for sequences that improve desirable properties beyond previously
known values, which requires novel hypotheses that \emph{extrapolate} beyond
training data. In these settings, extrapolation may be achieved by using random
search methods such as Markov chain Monte Carlo (MCMC), which, given an initial
state, sample local transformations to approximate a target density that
rewards states with the desired properties. However, even with a well-designed
proposal, MCMC may struggle to explore large structured state spaces
efficiently. Rather than relying on stochastic search, it would be desirable to
have a model that greedily optimizes the properties of interest, successfully
extrapolating in as few steps as possible. We propose to learn such a model
from the Markov chains resulting from MCMC search. Specifically, our approach
uses selected states from Markov chains as a source of training data for an
autoregressive model, which is then able to efficiently generate novel
sequences that extrapolate along the sequence-level properties of interest. The
proposed approach is validated on three problems: protein sequence design, text
sentiment control, and text anonymization. We find that the autoregressive
model can extrapolate as well or better than MCMC, but with the additional
benefits of scalability and significantly higher sample efficiency.

</details>


### [421] [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
*Xiangchen Song,Aashiq Muhamed,Yujia Zheng,Lingjing Kong,Zeyu Tang,Mona T. Diab,Virginia Smith,Kun Zhang*

Main category: cs.LG

TL;DR: Sparse Autoencoders (SAEs) are essential in mechanistic interpretability but face challenges due to inconsistent features. This paper emphasizes the importance of feature consistency and proposes Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a metric to measure it, showing high consistency is achievable with proper architecture choices.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency issue of learned SAE features across different training runs which undermines the reliability and efficiency of mechanistic interpretability research.

Method: Propose using PW-MCC as a metric for feature consistency and provide theoretical grounding and synthetic validation using a model organism, then extend findings to real-world LLM data.

Result: High levels of feature consistency are achievable with appropriate architectural choices, and high feature consistency correlates with semantic similarity of learned feature explanations.

Conclusion: There should be a community-wide shift towards systematically measuring feature consistency to promote robust cumulative progress in mechanistic interpretability.

Abstract: Sparse Autoencoders (SAEs) are a prominent tool in mechanistic
interpretability (MI) for decomposing neural network activations into
interpretable features. However, the aspiration to identify a canonical set of
features is challenged by the observed inconsistency of learned SAE features
across different training runs, undermining the reliability and efficiency of
MI research. This position paper argues that mechanistic interpretability
should prioritize feature consistency in SAEs -- the reliable convergence to
equivalent feature sets across independent runs. We propose using the Pairwise
Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to
operationalize consistency and demonstrate that high levels are achievable
(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.
Our contributions include detailing the benefits of prioritizing consistency;
providing theoretical grounding and synthetic validation using a model
organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;
and extending these findings to real-world LLM data, where high feature
consistency strongly correlates with the semantic similarity of learned feature
explanations. We call for a community-wide shift towards systematically
measuring feature consistency to foster robust cumulative progress in MI.

</details>


### [422] [Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits](https://arxiv.org/abs/2505.20268)
*Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: In online reinforcement learning with outcome-based feedback, this paper presents a sample-efficient algorithm that works in large state spaces via general function approximation, characterizes the statistical separation from per-step rewards, and extends to preference-based feedback settings.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning often encounters scenarios where rewards are only observed at trajectory endpoints rather than after each step, making it challenging to assign credit to the correct actions.

Method: The authors develop an algorithm using general function approximation which achieves a sample complexity of $\widetilde{O}({C_{\rm cov} H^3}/{\epsilon^2})$. They leverage value functions and reward functions represented by appropriate function classes. The approach is adapted for deterministic MDPs by eliminating the completeness assumption and further extended to preference-based feedback settings.

Result: The algorithm is provably sample-efficient and effective in large or infinite state spaces. The results reveal an unavoidable exponential separation for certain MDPs when comparing outcome-based feedback to per-step rewards, and demonstrate equivalent statistical efficiency in preference-based feedback settings.

Conclusion: This work establishes a theoretical foundation for understanding the statistical properties of outcome-based reinforcement learning, providing insights into its effectiveness and limitations.

Abstract: Reinforcement learning with outcome-based feedback faces a fundamental
challenge: when rewards are only observed at trajectory endpoints, how do we
assign credit to the right actions? This paper provides the first comprehensive
analysis of this problem in online RL with general function approximation. We
develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm
cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the
coverability coefficient of the underlying MDP. By leveraging general function
approximation, our approach works effectively in large or infinite state spaces
where tabular methods fail, requiring only that value functions and reward
functions can be represented by appropriate function classes. Our results also
characterize when outcome-based feedback is statistically separated from
per-step rewards, revealing an unavoidable exponential separation for certain
MDPs. For deterministic MDPs, we show how to eliminate the completeness
assumption, dramatically simplifying the algorithm. We further extend our
approach to preference-based feedback settings, proving that equivalent
statistical efficiency can be achieved even under more limited information.
Together, these results constitute a theoretical foundation for understanding
the statistical properties of outcome-based reinforcement learning.

</details>


### [423] [Probabilistic Kernel Function for Fast Angle Testing](https://arxiv.org/abs/2505.20274)
*Kejing Lu,Chuan Xiao,Yoshiharu Ishikawa*

Main category: cs.LG

TL;DR: The paper proposes two projection-based probabilistic kernel functions for angle testing in high-dimensional spaces, which outperform Gaussian-distribution-based methods and significantly enhance the throughput of ANNS.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing approaches that rely on random projection vectors from Gaussian distributions and require asymptotic assumptions, thus proposing more effective kernel functions for angle testing.

Method: Propose two projection-based probabilistic kernel functions using reference angles and deterministic structure for projection vectors, one for angle comparison and the other for angle thresholding. Apply these kernel functions to Approximate Nearest Neighbor Search (ANNS).

Result: The proposed kernel functions do not require asymptotic assumptions and outperform Gaussian-distribution-based kernel functions both theoretically and experimentally. They achieve a 2.5X ~ 3X higher query-per-second (QPS) throughput in ANNS compared to HNSW algorithm.

Conclusion: The new kernel functions provide a superior alternative for angle testing problems and can significantly improve the efficiency of ANNS.

Abstract: In this paper, we study the angle testing problem in high-dimensional
Euclidean spaces and propose two projection-based probabilistic kernel
functions, one designed for angle comparison and the other for angle
thresholding. Unlike existing approaches that rely on random projection vectors
drawn from Gaussian distributions, our approach leverages reference angles and
employs a deterministic structure for the projection vectors. Notably, our
kernel functions do not require asymptotic assumptions, such as the number of
projection vectors tending to infinity, and can be both theoretically and
experimentally shown to outperform Gaussian-distribution-based kernel
functions. We further apply the proposed kernel function to Approximate Nearest
Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X
higher query-per-second (QPS) throughput compared to the state-of-the-art
graph-based search algorithm HNSW.

</details>


### [424] [The Coverage Principle: A Framework for Understanding Compositional Generalization](https://arxiv.org/abs/2505.20278)
*Hoyeon Chang,Jinho Park,Hanseul Cho,Sohee Yang,Miyoung Ko,Hyeonbin Hwang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Main category: cs.LG

TL;DR: 大型语言模型在组合泛化方面存在局限性，本文提出覆盖原则以解释其泛化能力的边界，并通过实证研究和理论分析揭示Transformer在多步推理和路径歧义任务中的不足，同时提出了基于机制的泛化分类方法，强调需要新的架构或训练创新来实现系统性组合泛化。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型擅长模式匹配，但在系统性组合泛化方面表现不佳，因此需要一个数据驱动的框架来理解其泛化的限制。

Method: 提出覆盖原则，分析Transformer在两步泛化任务中的数据需求增长关系；研究路径歧义对Transformer性能的影响；评估Chain-of-Thought监督的作用；建立基于机制的泛化分类法。

Result: 验证了两步泛化所需训练数据随令牌集大小至少二次增长；发现路径歧义导致Transformer学习上下文依赖的状态表示；Chain-of-Thought监督虽提高效率但无法解决路径歧义问题。

Conclusion: 覆盖原则为理解组合推理提供了一个统一视角，强调需要根本性的架构或训练创新以实现真正的系统性组合泛化。

Abstract: Large language models excel at pattern matching, yet often fall short in
systematic compositional generalization. We propose the coverage principle: a
data-centric framework showing that models relying primarily on pattern
matching for compositional tasks cannot reliably generalize beyond substituting
fragments that yield identical results when used in the same contexts. We
demonstrate that this framework has a strong predictive power for the
generalization capabilities of Transformers. First, we derive and empirically
confirm that the training data required for two-hop generalization grows at
least quadratically with the token set size, and the training data efficiency
does not improve with 20x parameter scaling. Second, for compositional tasks
with path ambiguity where one variable affects the output through multiple
computational paths, we show that Transformers learn context-dependent state
representations that undermine both performance and interoperability. Third,
Chain-of-Thought supervision improves training data efficiency for multi-hop
tasks but still struggles with path ambiguity. Finally, we outline a
\emph{mechanism-based} taxonomy that distinguishes three ways neural networks
can generalize: structure-based (bounded by coverage), property-based
(leveraging algebraic invariances), and shared-operator (through function
reuse). This conceptual lens contextualizes our results and highlights where
new architectural ideas are needed to achieve systematic compositionally.
Overall, the coverage principle provides a unified lens for understanding
compositional reasoning, and underscores the need for fundamental architectural
or training innovations to achieve truly systematic compositionality.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [425] [InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models](https://arxiv.org/abs/2505.18156)
*Austin Howard*

Main category: cs.CR

TL;DR: InjectLab is an open-source security framework modeled after MITRE ATT&CK, designed to map and mitigate prompt-based attacks on Large Language Models (LLMs). It provides a matrix of over 25 techniques under six core tactics, along with detection guidance, mitigation strategies, and simulation tests.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the increasing risks associated with the widespread adoption of Large Language Models (LLMs), particularly prompt-based attacks that exploit how these models process language. There is a need for a structured approach to understand, detect, and mitigate adversarial behavior at the prompt layer.

Method: The method involves creating InjectLab, a security framework inspired by MITRE ATT&CK. It maps real-world techniques used to manipulate LLMs into a matrix organized under six core tactics. The framework includes detection guidance, mitigation strategies, YAML-based simulation tests, and a Python tool for executing prompt-based test cases.

Result: The result is a comprehensive framework called InjectLab that addresses prompt-based attacks on LLMs. It offers practical tools and resources such as detection guidance, mitigation strategies, and simulation tests to help secure language models.

Conclusion: InjectLab serves as a practical, community-driven foundation for securing language models against prompt-based attacks. The paper outlines its structure, compares it with other AI threat taxonomies, and discusses its future direction.

Abstract: Large Language Models (LLMs) are changing the way people interact with
technology. Tools like ChatGPT and Claude AI are now common in business,
research, and everyday life. But with that growth comes new risks, especially
prompt-based attacks that exploit how these models process language. InjectLab
is a security framework designed to address that problem. This paper introduces
InjectLab as a structured, open-source matrix that maps real-world techniques
used to manipulate LLMs. The framework is inspired by MITRE ATT&CK and focuses
specifically on adversarial behavior at the prompt layer. It includes over 25
techniques organized under six core tactics, covering threats like instruction
override, identity swapping, and multi-agent exploitation. Each technique in
InjectLab includes detection guidance, mitigation strategies, and YAML-based
simulation tests. A Python tool supports easy execution of prompt-based test
cases. This paper outlines the framework's structure, compares it to other AI
threat taxonomies, and discusses its future direction as a practical,
community-driven foundation for securing language models.

</details>


### [426] [A Blockchain-Based Approach for Secure and Transparent e-Faktur Issuance in Indonesia's VAT Reporting System](https://arxiv.org/abs/2505.18157)
*G. L. Farchan*

Main category: cs.CR

TL;DR: This paper designs a blockchain-based e-Faktur system using Hyperledger Fabric and FireFly for Indonesia's VAT process, demonstrating blockchain's potential to enhance security, efficiency, and resilience in tax administration.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of issuing and verifying tax invoices in Indonesia's VAT reporting process, such as reliance on centralized servers, single points of failure, and limited auditability.

Method: The system utilizes Hyperledger Fabric and Hyperledger FireFly to ensure data immutability, access control, and decentralized transaction processing. Hypothetical cyberattack scenarios were explored to evaluate the system's robustness.

Result: The results indicate that blockchain technology can mitigate potential vulnerabilities, improve system resilience, and streamline the issuance of NSFP and invoice validation.

Conclusion: The study highlights the feasibility of blockchain in public tax administration and sets a foundation for future research on large-scale implementations.

Abstract: The implementation of blockchain technology in tax administration offers
promising improvements in security, transparency, and efficiency. This paper
presents the design of a blockchain-based e-Faktur system aimed at addressing
the challenges of issuing and verifying tax invoices within Indonesia's VAT
reporting process. Utilizing Hyperledger Fabric, a private permissioned
blockchain, integrated with Hyperledger FireFly, this system ensures data
immutability, access control, and decentralized transaction processing. The
proposed system streamlines the issuance of NSFP and validates invoices while
reducing reliance on centralized servers, eliminating single points of failure,
and enhancing auditability. Hypothetical cyberattack scenarios were explored to
assess the system's robustness. The results demonstrate that blockchain
technology mitigates potential vulnerabilities and improves the resilience of
tax reporting systems. The findings highlight the feasibility of blockchain in
public tax administration and provide a foundation for future research on
large-scale implementations.

</details>


### [427] [GenAI Security: Outsmarting the Bots with a Proactive Testing Framework](https://arxiv.org/abs/2505.18172)
*Sunil Kumar Jang Bahadur,Gopala Dhar,Lavi Nigam*

Main category: cs.CR

TL;DR: The paper discusses the need for proactive security measures in Generative AI (GenAI) systems due to increasing security challenges, presenting a framework to counter adversarial attacks and empirically validating it using the SPML Chatbot Prompt Injection Dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the growing security challenges introduced by the sophistication and integration of GenAI models into various applications, which traditional methods fail to address effectively.

Method: The method involves creating a framework that includes key approaches, tools, and strategies designed to prevent advanced adversarial attacks on GenAI systems.

Result: The framework was empirically proven effective through testing against the SPML Chatbot Prompt Injection Dataset.

Conclusion: This research concludes that there should be a shift from reactive to proactive security practices for the safe and responsible deployment of GenAI technologies.

Abstract: The increasing sophistication and integration of Generative AI (GenAI) models
into diverse applications introduce new security challenges that traditional
methods struggle to address. This research explores the critical need for
proactive security measures to mitigate the risks associated with malicious
exploitation of GenAI systems. We present a framework encompassing key
approaches, tools, and strategies designed to outmaneuver even advanced
adversarial attacks, emphasizing the importance of securing GenAI innovation
against potential liabilities. We also empirically prove the effectiveness of
the said framework by testing it against the SPML Chatbot Prompt Injection
Dataset. This work highlights the shift from reactive to proactive security
practices essential for the safe and responsible deployment of GenAI
technologies

</details>


### [428] [Quantum-Resilient Blockchain for Secure Transactions in UAV-Assisted Smart Agriculture Networks](https://arxiv.org/abs/2505.18206)
*Taimoor Ahmad*

Main category: cs.CR

TL;DR: A quantum-resilient blockchain framework for UAV-assisted smart agriculture is presented, which incorporates post-quantum cryptographic primitives to secure data and resource transactions.


<details>
  <summary>Details</summary>
Motivation: The high mobility, decentralized nature, and low-power communication of UAVs in smart agriculture pose significant security challenges, particularly in ensuring transaction integrity and trust.

Method: The proposed solution incorporates post-quantum cryptographic primitives - specifically lattice-based digital signatures and key encapsulation mechanisms. A lightweight consensus protocol tailored for UAV communication constraints is developed, and transaction validation is handled through a trust-ranked, multi-layer ledger maintained by edge nodes.

Result: Experimental results from simulations using NS-3 and custom blockchain testbeds show that the framework outperforms existing schemes in terms of transaction throughput, energy efficiency, and resistance to quantum attacks.

Conclusion: The proposed system provides a scalable, secure, and sustainable solution for precision agriculture, enabling trusted automation and resilient data sharing in post-quantum eras.

Abstract: The integration of unmanned aerial vehicles (UAVs) into smart agriculture has
enabled real-time monitoring, data collection, and automated farming
operations. However, the high mobility, decentralized nature, and low-power
communication of UAVs pose significant security challenges, particularly in
ensuring transaction integrity and trust. This paper presents a
quantum-resilient blockchain framework designed to secure data and resource
transactions in UAV-assisted smart agriculture networks. The proposed solution
incorporates post-quantum cryptographic primitives-specifically lattice-based
digital signatures and key encapsulation mechanisms to achieve tamper-proof,
low-latency consensus without relying on traditional computationally intensive
proof-of-work schemes. A lightweight consensus protocol tailored for UAV
communication constraints is developed, and transaction validation is handled
through a trust-ranked, multi-layer ledger maintained by edge nodes.
Experimental results from simulations using NS-3 and custom blockchain testbeds
show that the framework outperforms existing schemes in terms of transaction
throughput, energy efficiency, and resistance to quantum attacks. The proposed
system provides a scalable, secure, and sustainable solution for precision
agriculture, enabling trusted automation and resilient data sharing in
post-quantum eras.

</details>


### [429] [Privacy-Preserving Bathroom Monitoring for Elderly Emergencies Using PIR and LiDAR Sensors](https://arxiv.org/abs/2505.18242)
*Youssouf Sidibé,Julia Gersey*

Main category: cs.CR

TL;DR: In-home elderly monitoring system using PIR and LiDAR sensors that can detect emergency events while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: To develop a low-cost, privacy-preserving solution for in-home elderly monitoring that requires no user input and can detect emergency events promptly.

Method: Using Passive Infrared (PIR) and Light Detection and Ranging (LiDAR) sensors to track entries, sitting, exits, and emergency scenarios within a home bathroom setting. Developed and evaluated a rule-based detection system through five real-world experiments simulating elderly behavior.

Result: The system was able to detect dangerous states, such as motionless collapses, while maintaining privacy through non-visual sensing.

Conclusion: A rule-based detection system using PIR and LiDAR sensors is effective for in-home elderly monitoring.

Abstract: In-home elderly monitoring requires systems that can detect emergency events
- such as falls or prolonged inactivity - while preserving privacy and
requiring no user input. These systems must be embedded into the surrounding
environment, capable of capturing activity, and responding promptly. This paper
presents a low-cost, privacy-preserving solution using Passive Infrared (PIR)
and Light Detection and Ranging (LiDAR) sensors to track entries, sitting,
exits, and emergency scenarios within a home bathroom setting. We developed and
evaluated a rule-based detection system through five real-world experiments
simulating elderly behavior. Annotated time-series graphs demonstrate the
system's ability to detect dangerous states, such as motionless collapses,
while maintaining privacy through non-visual sensing.

</details>


### [430] [Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation](https://arxiv.org/abs/2505.18323)
*Nicolas Küchler,Ivan Petrov,Conrad Grobler,Ilia Shumailov*

Main category: cs.CR

TL;DR: 研究了神经网络后门近十年，主要集中在分类任务上。本文提出了一种新的、更强大的后门类型，利用批量推理技术进行大规模用户数据操控和窃取。通过目标批量处理过程，这些架构后门可导致信息泄露，并让攻击者控制同一批次中其他用户的模型响应。这种攻击不仅可行且非常有效，对用户隐私和系统完整性构成严重威胁。为应对这一新漏洞，我们提出了基于信息流控制的确定性缓解策略，分析模型图并证明不同用户输入间的非干扰性。在Hugging Face托管的大规模模型分析中发现超过200个模型因动态量化而存在信息泄露问题。


<details>
  <summary>Details</summary>
Motivation: 尽管之前对神经网络后门的研究显示其恶意性质，但预测改变型攻击的实际影响尚不明确。本文旨在探索一种更具实际威胁的新后门类型及其潜在危害，特别是在批量推理场景下的信息泄露与用户数据窃取问题。

Method: 1. 设计针对批量推理的架构后门，通过操控模型架构实现同一批次内用户数据的泄露与窃取。
2. 提出基于信息流控制（Information Flow Control）的缓解策略，分析模型图以确保不同用户输入间无干扰。
3. 对Hugging Face托管的大量模型进行分析，验证新方法的有效性。

Result: 1. 验证了新型后门攻击的可行性与高效性，证明其可轻易注入主流模型架构。
2. 发现超过200个模型因动态量化而存在信息泄露问题。
3. 提出的缓解策略能有效防止此类攻击，并提供正式的安全保障。

Conclusion: 新型架构后门对用户隐私和系统完整性构成重大威胁，尤其是在批量推理场景下。本文提出的基于信息流控制的缓解策略为解决这一问题提供了有效途径，未来应进一步加强对神经网络后门的研究与防护。

Abstract: For nearly a decade the academic community has investigated backdoors in
neural networks, primarily focusing on classification tasks where adversaries
manipulate the model prediction. While demonstrably malicious, the immediate
real-world impact of such prediction-altering attacks has remained unclear. In
this paper we introduce a novel and significantly more potent class of
backdoors that builds upon recent advancements in architectural backdoors. We
demonstrate how these backdoors can be specifically engineered to exploit
batched inference, a common technique for hardware utilization, enabling
large-scale user data manipulation and theft. By targeting the batching
process, these architectural backdoors facilitate information leakage between
concurrent user requests and allow attackers to fully control model responses
directed at other users within the same batch. In other words, an attacker who
can change the model architecture can set and steal model inputs and outputs of
other users within the same batch. We show that such attacks are not only
feasible but also alarmingly effective, can be readily injected into prevalent
model architectures, and represent a truly malicious threat to user privacy and
system integrity. Critically, to counteract this new class of vulnerabilities,
we propose a deterministic mitigation strategy that provides formal guarantees
against this new attack vector, unlike prior work that relied on Large Language
Models to find the backdoors. Our mitigation strategy employs a novel
Information Flow Control mechanism that analyzes the model graph and proves
non-interference between different user inputs within the same batch. Using our
mitigation strategy we perform a large scale analysis of models hosted through
Hugging Face and find over 200 models that introduce (unintended) information
leakage between batch entries due to the use of dynamic quantization.

</details>


### [431] [An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs](https://arxiv.org/abs/2505.18332)
*Rahul Thomas,Louai Zahran,Erica Choi,Akilesh Potti,Micah Goldblum,Arka Pal*

Main category: cs.CR

TL;DR: Recent advances in Large Language Models (LLMs) have brought privacy concerns. Existing private third-party inference methods are slow and ineffective for large LLMs. Some recent work has tried to replace encrypted computations with statistical obfuscation methods, but this paper shows that these methods are insecure by introducing a novel reconstruction technique.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the privacy concerns in third-party inference services for LLMs and to evaluate the security of recently proposed statistical obfuscation methods that claim to protect privacy without the computational cost of encryption.

Method: The authors introduce a novel reconstruction technique to recover original prompts from hidden states of LLMs. They also extend this technique to reverse permuted hidden states, thereby demonstrating the insecurity of three recently proposed privacy schemes.

Result: The reconstruction technique introduced in the paper can recover original prompts from hidden states with nearly perfect accuracy across multiple state-of-the-art LLMs. The extensions of this attack are also nearly perfectly effective in reversing permuted hidden states, showing that the privacy schemes are insecure.

Conclusion: The findings of this paper highlight the importance of rigorous security analysis in privacy-preserving LLM inference. The paper dissects the shortcomings of prior theoretical 'proofs' of permutation security and demonstrates their ineffectiveness.

Abstract: Recent advances in Large Language Models (LLMs) have led to the widespread
adoption of third-party inference services, raising critical privacy concerns.
Existing methods of performing private third-party inference, such as Secure
Multiparty Computation (SMPC), often rely on cryptographic methods. However,
these methods are thousands of times slower than standard unencrypted
inference, and fail to scale to large modern LLMs. Therefore, recent lines of
work have explored the replacement of expensive encrypted nonlinear
computations in SMPC with statistical obfuscation methods - in particular,
revealing permuted hidden states to the third parties, with accompanying strong
claims of the difficulty of reversal into the unpermuted states. In this work,
we begin by introducing a novel reconstruction technique that can recover
original prompts from hidden states with nearly perfect accuracy across
multiple state-of-the-art LLMs. We then show that extensions of our attack are
nearly perfectly effective in reversing permuted hidden states of LLMs,
demonstrating the insecurity of three recently proposed privacy schemes. We
further dissect the shortcomings of prior theoretical `proofs' of permuation
security which allow our attack to succeed. Our findings highlight the
importance of rigorous security analysis in privacy-preserving LLM inference.

</details>


### [432] [A Critical Evaluation of Defenses against Prompt Injection Attacks](https://arxiv.org/abs/2505.18333)
*Yuqi Jia,Zedian Shao,Yupei Liu,Jinyuan Jia,Dawn Song,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: The paper argues that current evaluations of defenses against prompt injection attacks on Large Language Models (LLMs) are insufficient. It highlights the need to evaluate defenses across two dimensions: effectiveness against various prompt injection attacks and general-purpose utility without compromising LLM capabilities. The study finds prior defenses less successful than claimed when evaluated comprehensively, providing a foundation for future defense evaluations.


<details>
  <summary>Details</summary>
Motivation: Current studies lack a principled approach to evaluating defenses against prompt injection attacks on LLMs, focusing on both the effectiveness of the defenses and their impact on the general-purpose utility of the models.

Method: The method involves assessing defenses across two critical dimensions: (1) effectiveness against both existing and adaptive prompt injection attacks with diverse prompts, and (2) ensuring the defense does not compromise the foundational capabilities of the LLM.

Result: The evaluation reveals that previous studies have not followed a comprehensive methodology, and existing defenses are not as successful as previously reported.

Conclusion: This work establishes a foundation for evaluating future defenses against prompt injection attacks and provides guidance for their development.

Abstract: Large Language Models (LLMs) are vulnerable to prompt injection attacks, and
several defenses have recently been proposed, often claiming to mitigate these
attacks successfully. However, we argue that existing studies lack a principled
approach to evaluating these defenses. In this paper, we argue the need to
assess defenses across two critical dimensions: (1) effectiveness, measured
against both existing and adaptive prompt injection attacks involving diverse
target and injected prompts, and (2) general-purpose utility, ensuring that the
defense does not compromise the foundational capabilities of the LLM. Our
critical evaluation reveals that prior studies have not followed such a
comprehensive evaluation methodology. When assessed using this principled
approach, we show that existing defenses are not as successful as previously
reported. This work provides a foundation for evaluating future defenses and
guiding their development. Our code and data are available at:
https://github.com/PIEval123/PIEval.

</details>


### [433] [Dynamic Risk Assessments for Offensive Cybersecurity Agents](https://arxiv.org/abs/2505.18384)
*Boyi Wei,Benedikt Stroebl,Jiacen Xu,Joie Zhang,Zhou Li,Peter Henderson*

Main category: cs.CR

TL;DR: Foundation models can automate dangerous offensive cyber-operations. With financial incentives, adversaries can improve agents' cybersecurity capability by more than 40% relative to the baseline within a small compute budget (8 H100 GPU Hours). This shows the necessity of dynamic evaluation of agents' cybersecurity risk.


<details>
  <summary>Details</summary>
Motivation: The motivation is that current audits on foundation models fail to account for the degrees of freedom available to adversaries in real-world scenarios, especially in iterative improvement of agents for offensive cybersecurity.

Method: The method involves assessing an expanded threat model in the context of cybersecurity, considering varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget.

Result: Adversaries improved an agent's cybersecurity capability on InterCode CTF by more than 40% relative to the baseline within 8 H100 GPU Hours without external assistance.

Conclusion: There is a need to evaluate agents' cybersecurity risk dynamically to better represent the actual risk.

Abstract: Foundation models are increasingly becoming better autonomous programmers,
raising the prospect that they could also automate dangerous offensive
cyber-operations. Current frontier model audits probe the cybersecurity risks
of such agents, but most fail to account for the degrees of freedom available
to adversaries in the real world. In particular, with strong verifiers and
financial incentives, agents for offensive cybersecurity are amenable to
iterative improvement by would-be adversaries. We argue that assessments should
take into account an expanded threat model in the context of cybersecurity,
emphasizing the varying degrees of freedom that an adversary may possess in
stateful and non-stateful environments within a fixed compute budget. We show
that even with a relatively small compute budget (8 H100 GPU Hours in our
study), adversaries can improve an agent's cybersecurity capability on
InterCode CTF by more than 40\% relative to the baseline -- without any
external assistance. These results highlight the need to evaluate agents'
cybersecurity risk in a dynamic manner, painting a more representative picture
of risk.

</details>


### [434] [Modeling interdependent privacy threats](https://arxiv.org/abs/2505.18386)
*Shuaishuai Liu,Gergely Biczók*

Main category: cs.CR

TL;DR: The paper highlights the insufficiency of current privacy threat modeling frameworks in addressing interdependent privacy (IDP) issues, proposing a new methodology called IDPA to specifically tackle IDP threats, validated through a WeChat case study.


<details>
  <summary>Details</summary>
Motivation: Driven by the rise of online social networks and third-party apps, data sharing has become prevalent, posing significant challenges to individual privacy. Existing methodologies for privacy threat modeling fail to adequately address IDP, where one user's actions can compromise the privacy of others.

Method: The authors identify limitations in existing frameworks regarding IDP, develop IDPA - a specialized threat modeling approach targeting IDP threats, and validate its effectiveness via a case study on WeChat.

Result: IDPA successfully identifies and addresses IDP-related threats that were previously overlooked by traditional methodologies, proving effective in the WeChat case study.

Conclusion: IDPA represents an advancement in privacy threat modeling by focusing on interdependent privacy. It not only operates effectively beyond third-party apps but also encourages further research into specialized threat modeling approaches.

Abstract: The rise of online social networks, user-gene-rated content, and third-party
apps made data sharing an inevitable trend, driven by both user behavior and
the commercial value of personal information. As service providers amass vast
amounts of data, safeguarding individual privacy has become increasingly
challenging. Privacy threat modeling has emerged as a critical tool for
identifying and mitigating risks, with methodologies such as LINDDUN, xCOMPASS,
and PANOPTIC offering systematic approaches. However, these frameworks
primarily focus on threats arising from interactions between a single user and
system components, often overlooking interdependent privacy (IDP); the
phenomenon where one user's actions affect the privacy of other users and even
non-users. IDP risks are particularly pronounced in third-party applications,
where platform permissions, APIs, and user behavior can lead to unintended and
unconsented data sharing, such as in the Cambridge Analytica case. We argue
that existing threat modeling approaches are limited in exposing IDP-related
threats, potentially underestimating privacy risks. To bridge this gap, we
propose a specialized methodology that explicitly focuses on interdependent
privacy. Our contributions are threefold: (i) we identify IDP-specific
challenges and limitations in current threat modeling frameworks, (ii) we
create IDPA, a threat modeling approach tailored to IDP threats, and (iii) we
validate our approach through a case study on WeChat. We believe that IDPA can
operate effectively on systems other than third-party apps and may motivate
further research on specialized threat modeling.

</details>


### [435] [Towards Anonymous Neural Network Inference](https://arxiv.org/abs/2505.18398)
*Liao Peiyuan*

Main category: cs.CR

TL;DR: The paper presents 'funion', a system ensuring unlinkability between sender and receiver during neural network inference. It leverages protocols from the Echomix anonymity system, providing strong metadata privacy with manageable overhead for large-scale workloads.


<details>
  <summary>Details</summary>
Motivation: To address the privacy concerns in neural network inference by enabling end-to-end sender-receiver unlinkability, thus allowing users to perform anonymous queries on cloud services without revealing metadata.

Method: Funion uses the Pigeonhole storage protocol and BACAP (blinding-and-capability) scheme from Echomix. This allows users to anonymously store input tensors, commission compute services to process them, and retrieve results without traceable connections. The system employs a store-compute-store paradigm that masks traffic patterns and quantizes execution timing into public latency buckets.

Result: Funion inherits strong metadata privacy guarantees from Echomix under similar trust assumptions, while introducing acceptable overhead suitable for production-scale workloads.

Conclusion: Funion provides a practical platform for submitting fully anonymized inference queries to cloud services, advancing privacy-preserving neural network inference.

Abstract: We introduce funion, a system providing end-to-end sender-receiver
unlinkability for neural network inference. By leveraging the Pigeonhole
storage protocol and BACAP (blinding-and-capability) scheme from the Echomix
anonymity system, funion inherits the provable security guarantees of modern
mixnets. Users can anonymously store input tensors in pseudorandom storage
locations, commission compute services to process them via the neural network,
and retrieve results with no traceable connection between input and output
parties. This store-compute-store paradigm masks both network traffic patterns
and computational workload characteristics, while quantizing execution timing
into public latency buckets. Our security analysis demonstrates that funion
inherits the strong metadata privacy guarantees of Echomix under largely the
same trust assumptions, while introducing acceptable overhead for
production-scale workloads. Our work paves the way towards an accessible
platform where users can submit fully anonymized inference queries to cloud
services.

</details>


### [436] [AI/ML for 5G and Beyond Cybersecurity](https://arxiv.org/abs/2505.18402)
*Sandeep Pirbhulal,Habtamu Abie,Martin Jullum,Didrik Nielsen,Anders Løland*

Main category: cs.CR

TL;DR: The advancements in communication technology and IoT bring new security problems. AI/ML techniques are crucial in cybersecurity but face challenges in trustworthiness. There is a need for further research to define AI/ML threats and ensure the safe development of AI/ML based 5G and beyond IoT systems.


<details>
  <summary>Details</summary>
Motivation: To address the new security problems brought by advancements in communication technology and IoT, and to investigate the threats and vulnerabilities introduced by AI/ML based 5G and beyond IoT systems.

Method: Reviewing the role of AI/ML techniques for 5G and beyond security and providing perspectives for predicting and mitigating security issues using AI/ML techniques.

Result: A comprehensive review of the role of AI/ML in ensuring the security of 5G and beyond IoT systems, along with insights on how to predict and mitigate potential security risks.

Conclusion: Studying AI/ML approaches is essential for ensuring the safe and secure development, deployment, and operation of AI/ML based 5G and beyond IoT systems.

Abstract: The advancements in communication technology (5G and beyond) and global
connectivity Internet of Things (IoT) also come with new security problems that
will need to be addressed in the next few years. The threats and
vulnerabilities introduced by AI/ML based 5G and beyond IoT systems need to be
investigated to avoid the amplification of attack vectors on AI/ML. AI/ML
techniques are playing a vital role in numerous applications of cybersecurity.
Despite the ongoing success, there are significant challenges in ensuring the
trustworthiness of AI/ML systems. However, further research is needed to define
what is considered an AI/ML threat and how it differs from threats to
traditional systems, as currently there is no common understanding of what
constitutes an attack on AI/ML based systems, nor how it might be created,
hosted and propagated [ETSI, 2020]. Therefore, there is a need for studying the
AI/ML approach to ensure safe and secure development, deployment, and operation
of AI/ML based 5G and beyond IoT systems. For 5G and beyond, it is essential to
continuously monitor and analyze any changing environment in real-time to
identify and reduce intentional and unintentional risks. In this study, we will
review the role of the AI/ML technique for 5G and beyond security. Furthermore,
we will provide our perspective for predicting and mitigating 5G and beyond
security using AI/ML techniques.

</details>


### [437] [Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services](https://arxiv.org/abs/2505.18471)
*Guoheng Sun,Ziyao Wang,Xuandong Zhao,Bowei Tian,Zheyu Shen,Yexiao He,Jinming Xing,Ang Li*

Main category: cs.CR

TL;DR: Modern large language model (LLM) services, known as Commercial Opaque LLM Services (COLS), have complex internal operations that users cannot observe but are billed for. This paper highlights the risks of 'quantity inflation' and 'quality downgrade', proposing auditing strategies and a three-layer framework to enhance accountability and transparency without compromising confidentiality.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the emerging accountability challenges in Commercial Opaque LLM Services (COLS). Specifically, it aims to tackle the issue where users are charged for internal operations they cannot observe, verify, or contest.

Method: The paper formalizes two key risks in COLS: quantity inflation and quality downgrade. It proposes a diverse set of auditing strategies including commitment-based, predictive, behavioral, and signature-based methods. Additionally, it explores complementary mechanisms such as watermarking and trusted execution environments. A modular three-layer auditing framework is also proposed for enabling trustworthy verification.

Result: The result is a comprehensive approach towards enhancing verifiability in COLS without exposing proprietary internals. The proposed strategies and framework aim to ensure transparency, auditability, and accountability in commercial LLM services.

Conclusion: The conclusion emphasizes the need for further research and policy development in achieving transparency and accountability in commercial LLM services. The paper's proposals aim to encourage advancements in this direction.

Abstract: Modern large language model (LLM) services increasingly rely on complex,
often abstract operations, such as multi-step reasoning and multi-agent
collaboration, to generate high-quality outputs. While users are billed based
on token consumption and API usage, these internal steps are typically not
visible. We refer to such systems as Commercial Opaque LLM Services (COLS).
This position paper highlights emerging accountability challenges in COLS:
users are billed for operations they cannot observe, verify, or contest. We
formalize two key risks: \textit{quantity inflation}, where token and call
counts may be artificially inflated, and \textit{quality downgrade}, where
providers might quietly substitute lower-cost models or tools. Addressing these
risks requires a diverse set of auditing strategies, including
commitment-based, predictive, behavioral, and signature-based methods. We
further explore the potential of complementary mechanisms such as watermarking
and trusted execution environments to enhance verifiability without
compromising provider confidentiality. We also propose a modular three-layer
auditing framework for COLS and users that enables trustworthy verification
across execution, secure logging, and user-facing auditability without exposing
proprietary internals. Our aim is to encourage further research and policy
development toward transparency, auditability, and accountability in commercial
LLM services.

</details>


### [438] [A Study of Semi-Fungible Token based Wi-Fi Access Control](https://arxiv.org/abs/2505.18518)
*Litao Ye,Bin Chen,Chen Sun,Shuo Wang,Peichang Zhang,Shengli Zhang*

Main category: cs.CR

TL;DR: This paper proposes a Wi-Fi access control solution based on blockchain smart contracts to solve current issues in Wi-Fi authentication such as security, privacy, cost and billing. It designs semi-fungible Wi-Fi tokens (SFWTs) using the ERC1155 standard for user access credentials and develops a system to manage and verify these rights. Experiments show that SFWTs can reduce operating costs and authentication time.


<details>
  <summary>Details</summary>
Motivation: Current Wi-Fi authentication methods have problems with insufficient security, potential user privacy leakage, high management costs, and difficulty in billing.

Method: Designing semi-fungible Wi-Fi tokens (SFWTs) using the ERC1155 token standard and developing a Wi-Fi access control system based on SFWTs to securely verify and manage user access rights.

Result: The use of SFWTs and the SFWT access right verification process significantly reduces Wi-Fi operating costs and authentication time, meeting users' needs for safe and convenient Wi-Fi access.

Conclusion: A blockchain-based Wi-Fi access control solution using SFWTs offers improved security, reduced costs, and efficient user access management.

Abstract: Current Wi-Fi authentication methods face issues such as insufficient
security, user privacy leakage, high management costs, and difficulty in
billing. To address these challenges, a Wi-Fi access control solution based on
blockchain smart contracts is proposed. Firstly, semi-fungible Wi-Fi tokens
(SFWTs) are designed using the ERC1155 token standard as credentials for users
to access Wi-Fi. Secondly, a Wi-Fi access control system based on SFWTs is
developed to securely verify and manage the access rights of Wi-Fi users.
Experimental results demonstrate that SFWTs, designed based on the ERC1155
standard, along with the SFWT access right verification process, can
significantly reduce Wi-Fi operating costs and authentication time, effectively
meeting users' needs for safe and convenient Wi-Fi access.

</details>


### [439] [Adapting Novelty towards Generating Antigens for Antivirus systems](https://arxiv.org/abs/2505.18520)
*Ritwik Murali,C Shunmuga Velayutham*

Main category: cs.CR

TL;DR: The paper presents a framework using assembly source code and evolutionary algorithms to generate malware variants that retain malicious functionality while evading antivirus scanners, with results showing over 98% evasion success, to improve malware detection algorithms.


<details>
  <summary>Details</summary>
Motivation: Anti-malware scanners rely on signatures which can be easily altered by minor changes in malware code, leading to undetection of new variants. A need exists for a dataset of proactively generated malware variants to help automated antivirus scanners detect such diverse variants.

Method: A generic assembly source code based framework is proposed which incorporates generic code transformation functions as variation operators and a novelty search supported quality metric as fitness function for evolutionary algorithms to generate diverse malware variants retaining maliciousness but evading detection.

Result: The framework successfully generates diverse malware variants capable of evading over 98% of popular antivirus scanners.

Conclusion: The evolved malware variants can assist malware analysis engines as antigens to enhance their detection algorithms.

Abstract: It is well known that anti-malware scanners depend on malware signatures to
identify malware. However, even minor modifications to malware code structure
results in a change in the malware signature thus enabling the variant to evade
detection by scanners. Therefore, there exists the need for a proactively
generated malware variant dataset to aid detection of such diverse variants by
automated antivirus scanners. This paper proposes and demonstrates a generic
assembly source code based framework that facilitates any evolutionary
algorithm to generate diverse and potential variants of an input malware, while
retaining its maliciousness, yet capable of evading antivirus scanners. Generic
code transformation functions and a novelty search supported quality metric
have been proposed as components of the framework to be used respectively as
variation operators and fitness function, for evolutionary algorithms. The
results demonstrate the effectiveness of the framework in generating diverse
variants and the generated variants have been shown to evade over 98% of
popular antivirus scanners. The malware variants evolved by the framework can
serve as antigens to assist malware analysis engines to improve their malware
detection algorithms.

</details>


### [440] [Benchmarking Poisoning Attacks against Retrieval-Augmented Generation](https://arxiv.org/abs/2505.18543)
*Baolei Zhang,Haoran Xin,Jiatong Li,Dongzhe Zhang,Minghong Fang,Zhuqing Liu,Lihai Nie,Zheli Liu*

Main category: cs.CR

TL;DR: Retrieval-Augmented Generation (RAG) systems, despite incorporating external knowledge to reduce hallucinations, face security vulnerabilities from poisoning attacks. This paper introduces the first comprehensive benchmark framework for evaluating these attacks on RAG systems, finding that while existing attacks work well on standard QA datasets, their effectiveness decreases on expanded versions and current defenses are insufficient.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a thorough assessment of the practical threat posed by poisoning attacks on RAG systems.

Method: Proposed the first comprehensive benchmark framework covering 5 standard QA datasets, 10 expanded variants, 13 poisoning attack methods, and 7 defense mechanisms.

Result: Existing attacks perform well on standard QA datasets but less so on expanded versions. Various advanced RAG architectures remain vulnerable to poisoning attacks, and current defense techniques fail to provide robust protection.

Conclusion: There is an urgent need for more resilient and generalizable defense strategies against poisoning attacks in RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) has proven effective in mitigating
hallucinations in large language models by incorporating external knowledge
during inference. However, this integration introduces new security
vulnerabilities, particularly to poisoning attacks. Although prior work has
explored various poisoning strategies, a thorough assessment of their practical
threat to RAG systems remains missing. To address this gap, we propose the
first comprehensive benchmark framework for evaluating poisoning attacks on
RAG. Our benchmark covers 5 standard question answering (QA) datasets and 10
expanded variants, along with 13 poisoning attack methods and 7 defense
mechanisms, representing a broad spectrum of existing techniques. Using this
benchmark, we conduct a comprehensive evaluation of all included attacks and
defenses across the full dataset spectrum. Our findings show that while
existing attacks perform well on standard QA datasets, their effectiveness
drops significantly on the expanded versions. Moreover, our results demonstrate
that various advanced RAG architectures, such as sequential, branching,
conditional, and loop RAG, as well as multi-turn conversational RAG, multimodal
RAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning
attacks. Notably, current defense techniques fail to provide robust protection,
underscoring the pressing need for more resilient and generalizable defense
strategies.

</details>


### [441] [LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis](https://arxiv.org/abs/2505.18551)
*Md Ahsanul Haque,Ismail Hossain,Md Mahmuduzzaman Kamol,Md Jahangir Alam,Suresh Kumar Amalapuram,Sajedul Talukder,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: The paper introduces LAMDA, a large and diverse Android malware benchmark addressing concept drift in ML-based malware detection systems.


<details>
  <summary>Details</summary>
Motivation: Machine learning-based malware detection systems often fail to account for the dynamic nature of real-world training and test data distributions.

Method: LAMDA spans 12 years (2013-2025, excluding 2015), includes over 1 million samples (approximately 37% labeled as malware), and covers 1,380 malware families and 150,000 singleton samples.

Result: Empirical demonstration of LAMDA's utility by quantifying the performance degradation of standard ML models over time and analyzing feature stability across years.

Conclusion: LAMDA is presented as the most comprehensive Android malware dataset to date, enabling in-depth research into temporal drift, generalization, explainability, and evolving detection challenges.

Abstract: Machine learning (ML)-based malware detection systems often fail to account
for the dynamic nature of real-world training and test data distributions. In
practice, these distributions evolve due to frequent changes in the Android
ecosystem, adversarial development of new malware families, and the continuous
emergence of both benign and malicious applications. Prior studies have shown
that such concept drift -- distributional shifts in benign and malicious
samples, leads to significant degradation in detection performance over time.
Despite the practical importance of this issue, existing datasets are often
outdated and limited in temporal scope, diversity of malware families, and
sample scale, making them insufficient for the systematic evaluation of concept
drift in malware detection.
  To address this gap, we present LAMDA, the largest and most temporally
diverse Android malware benchmark to date, designed specifically for concept
drift analysis. LAMDA spans 12 years (2013-2025, excluding 2015), includes over
1 million samples (approximately 37% labeled as malware), and covers 1,380
malware families and 150,000 singleton samples, reflecting the natural
distribution and evolution of real-world Android applications. We empirically
demonstrate LAMDA's utility by quantifying the performance degradation of
standard ML models over time and analyzing feature stability across years. As
the most comprehensive Android malware dataset to date, LAMDA enables in-depth
research into temporal drift, generalization, explainability, and evolving
detection challenges. The dataset and code are available at:
https://iqsec-lab.github.io/LAMDA/.

</details>


### [442] [MLRan: A Behavioural Dataset for Ransomware Analysis and Detection](https://arxiv.org/abs/2505.18613)
*Faithful Chiagoziem Onwuegbuche,Adelodun Olaoluwa,Anca Delia Jurcut,Liliana Pasquale*

Main category: cs.CR

TL;DR: The paper presents MLRan, a large and diverse behavioral ransomware dataset with over 4,800 samples from 2006 to 2024. It also provides guidelines for creating high-quality datasets (GUIDE-MLRan). Using feature selection techniques, they evaluated several ML models achieving up to 98.7% accuracy in ransomware detection. Key malicious indicators were identified using SHAP and LIME.


<details>
  <summary>Details</summary>
Motivation: Existing publicly available datasets for training machine learning-based ransomware detection models are scarce, often have limited sample size, diversity, and reproducibility.

Method: 1. Developed MLRan - a dataset with over 4,800 samples across 64 ransomware families and balanced goodware samples.
2. Created GUIDE-MLRan - guidelines inspired by previous work for constructing high-quality behavioral ransomware datasets.
3. Performed feature selection reducing 6.4 million features to 483 highly informative ones using mutual information filtering and recursive feature elimination.
4. Evaluated several ML models on the dataset.
5. Used SHAP and LIME to identify critical indicators of malicious behavior.

Result: The ML models achieved an accuracy, precision, and recall of up to 98.7%, 98.9%, and 98.5%, respectively. Critical indicators of malicious behavior such as registry tampering, strings, and API misuse were identified.

Conclusion: MLRan and its associated resources provide a valuable tool for the research community to advance ransomware detection through machine learning. The dataset, along with the source code for feature extraction, selection, ML training, and evaluation, is publicly available at https://github.com/faithfulco/mlran.

Abstract: Ransomware remains a critical threat to cybersecurity, yet publicly available
datasets for training machine learning-based ransomware detection models are
scarce and often have limited sample size, diversity, and reproducibility. In
this paper, we introduce MLRan, a behavioural ransomware dataset, comprising
over 4,800 samples across 64 ransomware families and a balanced set of goodware
samples. The samples span from 2006 to 2024 and encompass the four major types
of ransomware: locker, crypto, ransomware-as-a-service, and modern variants. We
also propose guidelines (GUIDE-MLRan), inspired by previous work, for
constructing high-quality behavioural ransomware datasets, which informed the
curation of our dataset. We evaluated the ransomware detection performance of
several machine learning (ML) models using MLRan. For this purpose, we
performed feature selection by conducting mutual information filtering to
reduce the initial 6.4 million features to 24,162, followed by recursive
feature elimination, yielding 483 highly informative features. The ML models
achieved an accuracy, precision and recall of up to 98.7%, 98.9%, 98.5%,
respectively. Using SHAP and LIME, we identified critical indicators of
malicious behaviour, including registry tampering, strings, and API misuse. The
dataset and source code for feature extraction, selection, ML training, and
evaluation are available publicly to support replicability and encourage future
research, which can be found at https://github.com/faithfulco/mlran.

</details>


### [443] [Anonymity-washing](https://arxiv.org/abs/2505.18627)
*Szivia Lestyán,William Letrone,Ludovica Robustelli,Gergely Biczók*

Main category: cs.CR

TL;DR: Anonymization is crucial in data privacy but its application has issues. This paper introduces anonymity-washing as a critical concern, provides an overview of factors enabling it, and recommends education and cooperation to address the problem.


<details>
  <summary>Details</summary>
Motivation: Anonymization's practical application remains ambiguous and inconsistent, with both legal and technical critiques addressing isolated aspects. There is a need for a comprehensive understanding of the issue.

Method: The paper synthesizes fragmented legal interpretations, technical misunderstandings, and outdated regulatory guidance, complemented by a systematic review of national and international resources such as legal cases, guidelines from data protection authorities, and technical documentation.

Result: Findings reveal a lack of coherent support for practitioners leading to misuse of pseudonymization and obsolete anonymization techniques.

Conclusion: The authors recommend targeted education, clearer technical guidance, and closer cooperation between regulators, researchers, and industry.

Abstract: Anonymization is a foundational principle of data privacy regulation, yet its
practical application remains riddled with ambiguity and inconsistency. This
paper introduces the concept of anonymity-washing -- the misrepresentation of
the anonymity level of ``sanitized'' personal data -- as a critical privacy
concern. While both legal and technical critiques of anonymization exist, they
tend to address isolated aspects of the problem. In contrast, this paper offers
a comprehensive overview of the conditions that enable anonymity-washing. It
synthesizes fragmented legal interpretations, technical misunderstandings, and
outdated regulatory guidance and complements them with a systematic review of
national and international resources, including legal cases, data protection
authority guidelines, and technical documentation. Our findings reveal a lack
of coherent support for practitioners, contributing to the persistent misuse of
pseudonymization and obsolete anonymization techniques. We conclude by
recommending targeted education, clearer technical guidance, and closer
cooperation between regulators, researchers, and industry to bridge the gap
between legal norms and technical reality.

</details>


### [444] [$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models](https://arxiv.org/abs/2505.18680)
*Yuanhe Zhang,Xinyue Wang,Haoran Gao,Zhenhong Zhou,Fanyu Meng,Yuyao Zhang,Sen Su*

Main category: cs.CR

TL;DR: Large Language Models (LLMs) face resource consumption attacks that degrade server performance. The proposed Pluggable and Dynamic DoS-Defense Framework ($PD^3F$) mitigates these attacks through a two-stage approach, improving user access capacity by up to 500% during adversarial load.


<details>
  <summary>Details</summary>
Motivation: Existing works lack mitigation strategies against resource consumption attacks on LLMs, posing security risks for real-world deployments.

Method: The $PD^3F$ framework employs a two-stage approach: On the input side, it uses the Resource Index for Dynamic Request Polling Scheduling to reduce malicious resource usage; on the output side, it applies the Adaptive End-Based Suppression mechanism to terminate excessive malicious generation early.

Result: Experiments across six models show that $PD^3F$ significantly reduces the impact of resource consumption attacks, increasing users' access capacity by up to 500% during adversarial load.

Conclusion: $PD^3F$ advances the secure and resource-aware deployment of LLMs against resource consumption attacks.

Abstract: Large Language Models (LLMs), due to substantial computational requirements,
are vulnerable to resource consumption attacks, which can severely degrade
server performance or even cause crashes, as demonstrated by denial-of-service
(DoS) attacks designed for LLMs. However, existing works lack mitigation
strategies against such threats, resulting in unresolved security risks for
real-world LLM deployments. To this end, we propose the Pluggable and Dynamic
DoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend
against resource consumption attacks from both the input and output sides. On
the input side, we propose the Resource Index to guide Dynamic Request Polling
Scheduling, thereby reducing resource usage induced by malicious attacks under
high-concurrency scenarios. On the output side, we introduce the Adaptive
End-Based Suppression mechanism, which terminates excessive malicious
generation early. Experiments across six models demonstrate that $PD^3F$
significantly mitigates resource consumption attacks, improving users' access
capacity by up to 500% during adversarial load. $PD^3F$ represents a step
toward the resilient and resource-aware deployment of LLMs against resource
consumption attacks.

</details>


### [445] [MADCAT: Combating Malware Detection Under Concept Drift with Test-Time Adaptation](https://arxiv.org/abs/2505.18734)
*Eunjin Roh,Yigitcan Kaya,Christopher Kruegel,Giovanni Vigna,Sanghyun Hong*

Main category: cs.CR

TL;DR: MADCAT is a self-supervised method for solving concept drift in malware detection, especially effective in continuous Android malware detection.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge posed by concept drift in malware detection, where models become less effective over time due to evolving malware patterns.

Method: MADCAT uses an encoder-decoder architecture with test-time training on a small, balanced subset of test-time data through a self-supervised objective, learning features useful for detecting both old and new samples.

Result: MADCAT outperforms baseline methods in detection performance at test time and shows synergy with prior approaches in handling concept drift in malware detection.

Conclusion: MADCAT effectively addresses concept drift in malware detection, particularly demonstrating superior performance in continuous Android malware detection scenarios.

Abstract: We present MADCAT, a self-supervised approach designed to address the concept
drift problem in malware detection. MADCAT employs an encoder-decoder
architecture and works by test-time training of the encoder on a small,
balanced subset of the test-time data using a self-supervised objective. During
test-time training, the model learns features that are useful for detecting
both previously seen (old) data and newly arriving samples. We demonstrate the
effectiveness of MADCAT in continuous Android malware detection settings.
MADCAT consistently outperforms baseline methods in detection performance at
test time. We also show the synergy between MADCAT and prior approaches in
addressing concept drift in malware detection

</details>


### [446] [ARMS: A Vision for Actor Reputation Metric Systems in the Open-Source Software Supply Chain](https://arxiv.org/abs/2505.18760)
*Kelechi G. Kalu,Sofia Okorafor,Betül Durak,Kim Laine,Radames C. Moreno,Santiago Torres-Arias,James C. Davis*

Main category: cs.CR

TL;DR: To address the challenge of assessing cybersecurity implications in change requests for OSS projects, this paper proposes incorporating Actor Reputation Metrics (ARMS) to evaluate contributors' cybersecurity reputations.


<details>
  <summary>Details</summary>
Motivation: OSS project maintainers struggle with evaluating the cybersecurity implications of contributions from external actors, as they can only assess correctness but not security aspects.

Method: The authors identify seven generic security signals from industry standards, map concrete metrics using prior work and available security tools, propose study designs to refine ARMS, and analyze its advantages and disadvantages.

Result: A framework for implementing ARMS is provided, including security signals, metrics, study designs, and an evaluation of pros and cons.

Conclusion: Incorporating ARMS into the open-source ecosystem could enhance the ability of OSS maintainers to assess contributors' cybersecurity reputations.

Abstract: Many critical information technology and cyber-physical systems rely on a
supply chain of open-source software projects. OSS project maintainers often
integrate contributions from external actors. While maintainers can assess the
correctness of a change request, assessing a change request's cybersecurity
implications is challenging. To help maintainers make this decision, we propose
that the open-source ecosystem should incorporate Actor Reputation Metrics
(ARMS). This capability would enable OSS maintainers to assess a prospective
contributor's cybersecurity reputation. To support the future instantiation of
ARMS, we identify seven generic security signals from industry standards; map
concrete metrics from prior work and available security tools, describe study
designs to refine and assess the utility of ARMS, and finally weigh its pros
and cons.

</details>


### [447] [Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models](https://arxiv.org/abs/2505.18773)
*Jamie Hayes,Ilia Shumailov,Christopher A. Choquette-Choo,Matthew Jagielski,George Kaissis,Katherine Lee,Milad Nasr,Sahra Ghalebikesabi,Niloofar Mireshghallah,Meenatchi Sundaram Mutu Selva Annamalai,Igor Shilov,Matthieu Meeus,Yves-Alexandre de Montjoye,Franziska Boenisch,Adam Dziedzic,A. Feder Cooper*

Main category: cs.CR

TL;DR: 通过扩展LiRA到GPT-2架构，研究表明强大的成员推断攻击（MIA）可以在预训练的语言模型上成功，但其有效性在实际设置中仍然有限。


<details>
  <summary>Details</summary>
Motivation: 当前的成员推断攻击（MIA）难以扩展到大型预训练语言模型（LLMs），并且弱攻击不稳健，强攻击仅适用于小规模模型和数据集。这引发了一个问题：之前工作的限制是由于攻击设计选择还是MIAs在LLMs上本质上无效？

Method: 将LiRA扩展到具有10M到1B参数的GPT-2架构，在C4数据集上超过20B个标记上训练参考模型。

Result: (1) 强大的MIAs可以在预训练的LLMs上成功；(2) 然而，其有效性在实际设置中仍然有限（例如，AUC<0.7）；(3) MIA成功与相关隐私指标之间的关系不如先前研究所暗示的那样直接。

Conclusion: 强大的MIAs可以在LLMs上成功，但在实际应用中的效果有限，并且需要重新审视MIA成功与隐私指标之间的关系。

Abstract: State-of-the-art membership inference attacks (MIAs) typically require
training many reference models, making it difficult to scale these attacks to
large pre-trained language models (LLMs). As a result, prior research has
either relied on weaker attacks that avoid training reference models (e.g.,
fine-tuning attacks), or on stronger attacks applied to small-scale models and
datasets. However, weaker attacks have been shown to be brittle - achieving
close-to-arbitrary success - and insights from strong attacks in simplified
settings do not translate to today's LLMs. These challenges have prompted an
important question: are the limitations observed in prior work due to attack
design choices, or are MIAs fundamentally ineffective on LLMs? We address this
question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures
ranging from 10M to 1B parameters, training reference models on over 20B tokens
from the C4 dataset. Our results advance the understanding of MIAs on LLMs in
three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their
effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;
and, (3) the relationship between MIA success and related privacy metrics is
not as straightforward as prior work has suggested.

</details>


### [448] [Mal-D2GAN: Double-Detector based GAN for Malware Generation](https://arxiv.org/abs/2505.18806)
*Nam Hoang Thanh,Trung Pham Duy,Lam Bui Thu*

Main category: cs.CR

TL;DR: The paper proposes Mal-D2GAN model with double-detector and a least square loss function to generate adversarial malware examples for enhancing the robustness of malware detector, which was tested on a dataset of 20,000 samples and reduced the detection accuracy in 8 malware detectors compared to MalGAN and Mal-LSGAN models.


<details>
  <summary>Details</summary>
Motivation: Machine learning algorithms used for detecting malware lack robustness and are vulnerable to intentional attacks.

Method: Propose the Mal-D2GAN model with double-detector and a least square loss function to address the limitations of unstable training and weak adversarial examples in current GAN models.

Result: Mal-D2GAN model reduced the detection accuracy (true positive rate) in 8 malware detectors when tested on a dataset of 20,000 samples. Its performance was better than the existing MalGAN and Mal-LSGAN models.

Conclusion: Mal-D2GAN model is effective in generating adversarial malware examples to enhance the robustness of malware detector.

Abstract: Machine learning (ML) has been developed to detect malware in recent years.
Most researchers focused their efforts on improving the detection performance
but ignored the robustness of the ML models. In addition, many machine learning
algorithms are very vulnerable to intentional attacks. To solve these problems,
adversarial malware examples are generated by GANs to enhance the robustness of
the malware detector. However, since current GAN models suffer from limitations
such as unstable training and weak adversarial examples, we propose the
Mal-D2GAN model to address these problems. Specifically, the Mal-D2GAN
architecture was designed with double-detector and a least square loss function
and tested on a dataset of 20,000 samples. The results show that the Mal-D2GAN
model reduced the detection accuracy (true positive rate) in 8 malware
detectors. The performance was then compared with that of the existing MalGAN
and Mal- LSGAN models.

</details>


### [449] [Usability of Token-based and Remote Electronic Signatures: A User Experience Study](https://arxiv.org/abs/2505.18814)
*Omer Ege,Mustafa Cagal,Kemal Bicakci*

Main category: cs.CR

TL;DR: 本研究通过受控用户体验研究对比了基于令牌和远程电子签名系统的可用性和安全性。结果表明，远程签名在易用性方面更胜一筹，而基于令牌的签名则被认为更安全。这揭示了数字签名系统中可用性与感知安全之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 随着电子签名在安全数字交易中的重要性日益增加，从最终用户的角度了解其可用性和安全性变得至关重要。

Method: 通过20名参与者进行受控用户体验研究，完成涉及获取、安装和文档签名的任务，并进行结构化调查和定性反馈。

Result: 统计分析显示，远程电子签名由于最少的设置和跨平台的可访问性被认为比基于令牌的签名更易使用；相反，基于令牌的签名被认为是更安全的，强调了用户对硬件保护的信任。虽然更多的参与者倾向于远程签名，但这种偏好未达到统计显著性。

Conclusion: 本研究揭示了数字签名系统中可用性与感知安全之间的基本权衡，并为合格电子签名解决方案的设计和政策制定提供了有价值的见解。

Abstract: As electronic signatures (e-signatures) become increasingly integral to
secure digital transactions, understanding their usability and security
perception from an end-user perspective has become crucial. This study
empirically evaluates and compares two major e-signature systems -- token-based
and remote signatures -- through a controlled user experience study with 20
participants. Participants completed tasks involving acquisition, installation,
and document signing using both methods, followed by structured surveys and
qualitative feedback. Statistical analyses revealed that remote e-signatures
were perceived as significantly more usable than token-based ones, due to their
minimal setup and platform-independent accessibility. In contrast, token-based
signatures were rated as significantly more secure, highlighting users' trust
in hardware-based protection. Although more participants preferred remote
e-signatures for document signing, the preference did not reach statistical
significance, indicating a trend toward favoring convenience in real-world
scenarios. These findings underline the fundamental trade-off between usability
and perceived security in digital signing systems. By bridging the gap between
theoretical frameworks and real user experience, this study contributes
valuable insights to the design and policymaking of qualified electronic
signature solutions.

</details>


### [450] [LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review and Taxonomy](https://arxiv.org/abs/2505.18846)
*Muhammed Golec,Yaser Khamayseh,Suhib Bani Melhem,Abdulmalik Alwarafy*

Main category: cs.CR

TL;DR: The paper presents a comprehensive systematic review and taxonomy study for LLM-assisted APT detection in 6G networks, addressing five research questions and identifying open challenges.


<details>
  <summary>Details</summary>
Motivation: Sixth Generation (6G) wireless networks promise ultra-low latency, high throughput, and AI-assisted orchestration capabilities but are vulnerable to stealthy and long-term Advanced Persistent Threats (APTs). Large Language Models (LLMs) have high success in semantic reasoning and threat intelligence, making them an ideal candidate to fill this gap.

Method: The authors address five research questions related to LLM-assisted APT detection in 6G networks: semantic merging of fragmented logs, encrypted traffic analysis, edge distribution constraints, dataset/modeling techniques, and reproducibility trends. They also present various taxonomies such as granularity, deployment models, and kill chain stages to identify open challenges.

Result: The study identifies open challenges including explainability gaps, data scarcity, edge hardware limitations, and the need for real-time slicing-aware adaptation.

Conclusion: This paper is the first comprehensive systematic review and classification study on LLM-based APT detection in 6G networks. It provides several research gaps in 6G infrastructures for future researchers.

Abstract: Sixth Generation (6G) wireless networks, which are expected to be deployed in
the 2030s, have already created great excitement in academia and the private
sector with their extremely high communication speed and low latency rates.
However, despite the ultra-low latency, high throughput, and AI-assisted
orchestration capabilities they promise, they are vulnerable to stealthy and
long-term Advanced Persistent Threats (APTs). Large Language Models (LLMs)
stand out as an ideal candidate to fill this gap with their high success in
semantic reasoning and threat intelligence. In this paper, we present a
comprehensive systematic review and taxonomy study for LLM-assisted APT
detection in 6G networks. We address five research questions, namely, semantic
merging of fragmented logs, encrypted traffic analysis, edge distribution
constraints, dataset/modeling techniques, and reproducibility trends, by
leveraging most recent studies on the intersection of LLMs, APTs, and 6G
wireless networks. We identify open challenges such as explainability gaps,
data scarcity, edge hardware limitations, and the need for real-time
slicing-aware adaptation by presenting various taxonomies such as granularity,
deployment models, and kill chain stages. We then conclude the paper by
providing several research gaps in 6G infrastructures for future researchers.
To the best of our knowledge, this paper is the first comprehensive systematic
review and classification study on LLM-based APT detection in 6G networks.

</details>


### [451] [Securing Credit Inquiries: The Role of Real-Time User Approval in Preventing SSN Identity Theft](https://arxiv.org/abs/2505.18861)
*Gogulakrishnan Thiyagarajan,Vinay Bist,Prabhudarshi Nayak*

Main category: cs.CR

TL;DR: This paper proposes a real-time user authorization system to enhance security in credit inquiry systems by enforcing explicit user approval, employing real-time verification techniques, complying with regulations, and maintaining a seamless user experience.


<details>
  <summary>Details</summary>
Motivation: Unauthorized credit inquiries are a central entry point for identity theft, especially due to the use of SSNs in fraudulent cases. Traditional systems lack strict user authentication, making them vulnerable to unauthorized access.

Method: The proposed system enforces explicit user approval before processing any credit inquiry using real-time verification and approval techniques. It ensures that only authorized users can approve or reject credit check requests, minimizing third-party interference.

Result: The framework enhances security, minimizes the risk of identity theft, averts unauthorized credit checks, and increases customer trust in the credit verification system.

Conclusion: Financial institutions can implement this real-time user authorization system to drastically minimize risks, comply with regulations like GDPR and FCRA, and maintain a good user experience.

Abstract: Unauthorized credit inquiries are also a central entry point for identity
theft, with Social Security Numbers (SSNs) being widely utilized in fraudulent
cases. Traditional credit inquiry systems do not usually possess strict user
authentication, making them vulnerable to unauthorized access. This paper
proposes a real-time user authorization system to enhance security by enforcing
explicit user approval before processing any credit inquiry. The system employs
real-time verification and approval techniques. This ensures that the
authorized user only approves or rejects a credit check request. It minimizes
the risks of interference by third parties. Apart from enhancing security, this
system complies with regulations like the General Data Protection Regulation
(GDPR) and the Fair Credit Reporting Act (FCRA) while maintaining a seamless
user experience. This article discusses the technical issues, scaling-up
issues, and ways of implementing real-time user authorization in financial
systems. Through this framework, financial institutions can drastically
minimize the risk of identity theft, avert unauthorized credit checks, and
increase customer trust in the credit verification system.

</details>


### [452] [Understanding the Relationship Between Personal Data Privacy Literacy and Data Privacy Information Sharing by University Students](https://arxiv.org/abs/2505.18870)
*Brady D. Lund,Bryan Anderson,Ana Roeschley,Gahangir Hossain*

Main category: cs.CR

TL;DR: A survey study shows that high privacy literacy among US university students leads to more diverse privacy practices and discussions, indicating the need for educational initiatives to improve data privacy awareness.


<details>
  <summary>Details</summary>
Motivation: To understand how university students in the United States perceive personal data privacy and how their privacy literacy influences their understanding and behaviors.

Method: Survey based study categorizing students responses to a privacy literacy scale into high and low privacy literacy groups.

Result: High literacy individuals demonstrate a broader range of privacy practices, including multi factor authentication, VPN usage, and phishing awareness. Low literacy individuals rely on more basic security measures. High literacy respondents display greater diversity in recommendations and engagement in privacy discussions.

Conclusion: There is a need for enhanced educational initiatives to improve data privacy awareness at the university level.

Abstract: With constant threats to the safety of personal data in the United States,
privacy literacy has become an increasingly important competency among
university students, one that ties intimately to the information sharing
behavior of these students. This survey based study examines how university
students in the United States perceive personal data privacy and how their
privacy literacy influences their understanding and behaviors. Students
responses to a privacy literacy scale were categorized into high and low
privacy literacy groups, revealing that high literacy individuals demonstrate a
broader range of privacy practices, including multi factor authentication, VPN
usage, and phishing awareness, whereas low literacy individuals rely on more
basic security measures. Statistical analyses suggest that high literacy
respondents display greater diversity in recommendations and engagement in
privacy discussions. These findings suggest the need for enhanced educational
initiatives to improve data privacy awareness at the university level to create
a better cyber safe population.

</details>


### [453] [Zero Trust Cybersecurity: Procedures and Considerations in Context](https://arxiv.org/abs/2505.18872)
*Brady D. Lund,Tae Hee Lee,Ziang Wang,Ting Wang,Nishith Reddy Mannuru*

Main category: cs.CR

TL;DR: As cyber threats become more complex due to AI advancements, traditional security methods are insufficient. This paper explores the Zero Trust cybersecurity framework's applicability in information-heavy environments like schools and libraries, emphasizing continuous authentication, least privilege access, and breach assumption.


<details>
  <summary>Details</summary>
Motivation: To address the increasing sophistication of cyber threats enhanced by artificial intelligence which traditional security methods cannot handle effectively.

Method: Exploration of the Zero Trust cybersecurity framework and its principles (never trust, always verify) as a solution for vulnerabilities in organizations, with a focus on environments involving large information exchanges such as schools and libraries.

Result: Highlighted the importance of continuous authentication, least privilege access, and breach assumption within Zero Trust environments and identified future research avenues.

Conclusion: Zero Trust principles provide a promising approach to enhancing security in information-heavy environments like schools and libraries, requiring further investigation.

Abstract: In response to the increasing complexity and sophistication of cyber threats,
particularly those enhanced by advancements in artificial intelligence,
traditional security methods are proving insufficient. This paper explores the
Zero Trust cybersecurity framework, which operates on the principle of never
trust, always verify to mitigate vulnerabilities within organizations.
Specifically, it examines the applicability of Zero Trust principles in
environments where large volumes of information are exchanged, such as schools
and libraries. The discussion highlights the importance of continuous
authentication, least privilege access, and breach assumption. The findings
underscore avenues for future research that may help preserve the security of
these vulnerable organizations.

</details>


### [454] [Security Concerns for Large Language Models: A Survey](https://arxiv.org/abs/2505.18889)
*Miles Q. Li,Benjamin C. M. Fung*

Main category: cs.CR

TL;DR: Large Language Models (LLMs) have revolutionized natural language processing but also introduced new security vulnerabilities. This survey categorizes threats into prompt injection, adversarial attacks, misuse by malicious actors, and risks inherent in autonomous LLM agents. It summarizes recent studies (2022-2025), analyzes proposed defenses and limitations, and identifies open challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the emerging security concerns around Large Language Models (LLMs) and explore the various types of threats they pose.

Method: Categorizing threats into different types such as prompt injection and jailbreaking, adversarial attacks, misuse by malicious actors, and risks inherent in autonomous LLM agents. Summarizing academic and industrial studies from 2022-2025 that exemplify each threat, analyzing proposed defenses and their limitations, and identifying open challenges.

Result: A detailed summary of recent studies highlighting each type of threat, an analysis of proposed defenses and their limitations, and identification of open challenges in securing LLM-based applications.

Conclusion: Advancing robust, multi-layered security strategies is crucial to ensure LLMs are safe and beneficial.

Abstract: Large Language Models (LLMs) such as GPT-4 (and its recent iterations like
GPT-4o and the GPT-4.1 series), Google's Gemini, Anthropic's Claude 3 models,
and xAI's Grok have caused a revolution in natural language processing, but
their capabilities also introduce new security vulnerabilities. In this survey,
we provide a comprehensive overview of the emerging security concerns around
LLMs, categorizing threats into prompt injection and jailbreaking, adversarial
attacks (including input perturbations and data poisoning), misuse by malicious
actors (e.g., for disinformation, phishing, and malware generation), and
worrisome risks inherent in autonomous LLM agents. A significant focus has been
recently placed on the latter, exploring goal misalignment, emergent deception,
self-preservation instincts, and the potential for LLMs to develop and pursue
covert, misaligned objectives (scheming), which may even persist through safety
training. We summarize recent academic and industrial studies (2022-2025) that
exemplify each threat, analyze proposed defenses and their limitations, and
identify open challenges in securing LLM-based applications. We conclude by
emphasizing the importance of advancing robust, multi-layered security
strategies to ensure LLMs are safe and beneficial.

</details>


### [455] [Exemplifying Emerging Phishing: QR-based Browser-in-The-Browser (BiTB) Attack](https://arxiv.org/abs/2505.18944)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan,Basant Subba*

Main category: cs.CR

TL;DR: The paper introduces a new QR-based Browser-in-The-Browser (BiTB) attack using Large Language Model (LLM), specifically Google Gemini, combining BiTB and Quishing tactics. It shows how malicious prompts to Gemini-LLM can easily execute this attack, with a case study and experiment illustrating the method's effectiveness on victims' devices. The results call for more research into defending against LLM-enabled phishing.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the potential misuse of Large Language Models (LLMs) in creating sophisticated phishing attacks by combining Browser-in-The-Browser (BiTB) and Quishing techniques.

Method: Implementation of an innovative QR-based Browser-in-The-Browser (BiTB) attack using malicious prompts provided to Google Gemini LLM, followed by a case study and experimental analysis of its execution on victims' devices.

Result: The findings indicate that the presented attack is simplistic to implement and effective, thus necessitating further research efforts into countering such LLM-enabled phishing attempts.

Conclusion: This novel form of phishing attack highlights the need for researchers to focus on developing countermeasures against LLM-facilitated phishing strategies.

Abstract: Lately, cybercriminals constantly formulate productive approaches to exploit
individuals. This article exemplifies an innovative attack, namely QR-based
Browser-in-The-Browser (BiTB), using proficiencies of Large Language Model
(LLM) i.e. Google Gemini. The presented attack is a fusion of two emerging
attacks: BiTB and Quishing (QR code phishing). Our study underscores attack's
simplistic implementation utilizing malicious prompts provided to Gemini-LLM.
Moreover, we presented a case study to highlight a lucrative attack method, we
also performed an experiment to comprehend the attack execution on victims'
device. The findings of this work obligate the researchers' contributions in
confronting this type of phishing attempts through LLMs.

</details>


### [456] [Secure IVSHMEM: End-to-End Shared-Memory Protocol with Hypervisor-CA Handshake and In-Kernel Access Control](https://arxiv.org/abs/2505.19004)
*Hyunwoo Kim,Jaeseong Lee,Sunpyo Hong,Changmin Han*

Main category: cs.CR

TL;DR: Secure IVSHMEM is a protocol that secures in-host shared memory by providing end-to-end mutual authentication and fine-grained access control with negligible performance cost, making it suitable for safety-critical domains like automotive systems.


<details>
  <summary>Details</summary>
Motivation: Current implementations of in-host shared memory lack security controls, allowing applications to eavesdrop or tamper with the shared memory region.

Method: The protocol combines channel separation and kernel module access control, hypervisor-mediated handshake for service authentication, and application-level integration for abstraction and performance mitigation.

Result: In microbenchmarks, Secure IVSHMEM completes its one-time handshake in under 200ms and sustains data-plane round-trip latencies within 5% of the unmodified baseline, with negligible bandwidth overhead.

Conclusion: This design is ideally suited for safety and latency-critical in-host domains, such as automotive systems, where both performance and security are crucial.

Abstract: In-host shared memory (IVSHMEM) enables high-throughput, zero-copy
communication between virtual machines, but today's implementations lack any
security control, allowing any application to eavesdrop or tamper with the
IVSHMEM region. This paper presents Secure IVSHMEM, a protocol that provides
end-to-end mutual authentication and fine-grained access enforcement with
negligible performance cost. We combine three techniques to ensure security:
(1) channel separation and kernel module access control, (2)hypervisor-mediated
handshake for end-to-end service authentication, and (3)application-level
integration for abstraction and performance mitigation. In microbenchmarks,
Secure IVSHMEM completes its one-time handshake in under 200ms and sustains
data-plane round-trip latencies within 5\% of the unmodified baseline, with
negligible bandwidth overhead. We believe this design is ideally suited for
safety and latency-critical in-host domains, such as automotive systems, where
both performance and security are paramount.

</details>


### [457] [A quantitative notion of economic security for smart contract compositions](https://arxiv.org/abs/2505.19006)
*Emily Priyadarshini,Massimo Bartoletti*

Main category: cs.CR

TL;DR: The paper introduces a quantitative security notion to measure how an attack on one component can amplify economic losses of the whole system, and applies it to assess key compositions' security.


<details>
  <summary>Details</summary>
Motivation: Decentralized applications, especially in DeFi, are composed of multiple interconnected smart contracts. Adversaries targeting individual components can cause systemic economic losses. Existing security notions fail to quantify the effect of manipulating individual components on overall economic security.

Method: Introduce a quantitative security notion that measures the amplification of economic losses in the overall system caused by an attack on a single component. Study the fundamental properties of this notion and apply it to assess the security of key compositions.

Result: The introduced notion successfully quantifies the impact of attacks on individual components on the overall economic security of decentralized systems. It is applied to analyze under-collateralized loan attacks in systems made of lending protocols and decentralized exchanges.

Conclusion: A new quantitative security notion provides a method to assess the economic security of decentralized systems by measuring the amplification effects of attacks on individual components.

Abstract: Decentralized applications are often composed of multiple interconnected
smart contracts. This is especially evident in DeFi, where protocols are
heavily intertwined and rely on a variety of basic building blocks such as
tokens, decentralized exchanges and lending protocols. A crucial security
challenge in this setting arises when adversaries target individual components
to cause systemic economic losses. Existing security notions focus on
determining the existence of these attacks, but fail to quantify the effect of
manipulating individual components on the overall economic security of the
system. In this paper, we introduce a quantitative security notion that
measures how an attack on a single component can amplify economic losses of the
overall system. We study the fundamental properties of this notion and apply it
to assess the security of key compositions. In particular, we analyse
under-collateralized loan attacks in systems made of lending protocols and
decentralized exchanges.

</details>


### [458] [A Systematic Classification of Vulnerabilities in MoveEVM Smart Contracts (MWC)](https://arxiv.org/abs/2505.19047)
*Selçuk Topal*

Main category: cs.CR

TL;DR: The paper introduces MoveEVM Weakness Classification (MWC), a new taxonomy for vulnerabilities in smart contracts using Move language in EVM-compatible environments. It identifies 37 vulnerability types across six categories, focusing on hybrid risks that current tools often miss. The study also explores using formal methods and LLM-based audit agents for better auditing.


<details>
  <summary>Details</summary>
Motivation: Move was originally designed to prevent common security flaws via linear resource types and strict ownership. However, its integration with EVM bytecode introduces novel hybrid vulnerabilities not captured by existing systems.

Method: The researchers developed the MWC system, which spans 37 categorized vulnerability types (MWC-100 to MWC-136) across six semantic frames. They analyzed real-world contracts from Aptos and Sui to demonstrate the limitations of current verification tools.

Result: Analysis of real-world contracts revealed that current verification tools often fail to detect these hybrid risks. The study also showed potential in using formal methods and LLM-based audit agents to operationalize this classification.

Conclusion: MWC lays the foundation for more secure and verifiable contracts in next-generation blockchain systems, enabling scalable, logic-aware smart contract auditing.

Abstract: We introduce the MoveEVM Weakness Classification (MWC) system -- a dedicated
vulnerability taxonomy for smart contracts built with Move and executed in
EVM-compatible environments. While Move was originally designed to prevent
common security flaws via linear resource types and strict ownership, its
integration with EVM bytecode introduces novel hybrid vulnerabilities not
captured by existing systems like the SWC registry. Our taxonomy spans 37
categorized vulnerability types (MWC-100 to MWC-136) across six semantic
frames, addressing issues such as hybrid gas metering, capability misuse,
meta-transaction spoofing, and AI-integrated logic. Through analysis of
real-world contracts from Aptos and Sui, we demonstrate that current
verification tools often miss these hybrid risks. We also explore how formal
methods and LLM-based audit agents can operationalize this classification,
enabling scalable, logic-aware smart contract auditing. MWC lays the foundation
for more secure and verifiable contracts in next-generation blockchain systems.
(Shortened Abstract)

</details>


### [459] [Penetration Testing for System Security: Methods and Practical Approaches](https://arxiv.org/abs/2505.19174)
*Wei Zhang,Ju Xing,Xiaoqi Li*

Main category: cs.CR

TL;DR: This paper discusses the theoretical foundations and practical process of penetration testing, including a real-life case study, emphasizing its importance in cybersecurity.


<details>
  <summary>Details</summary>
Motivation: To clarify the theoretical foundations of penetration testing and explain the complete testing process for enhancing information system security.

Method: Outlines five stages of penetration test (intelligence gathering, vulnerability scanning, exploitation, privilege escalation, post-exploitation) with tools/techniques examination and provides a real-life case study.

Result: Readers gain insights into detailed procedures and applied techniques of penetration testing through the presented case study.

Conclusion: Penetration testing is crucial for securing information systems and maintaining network integrity; future trends and development directions are explored.

Abstract: Penetration testing refers to the process of simulating hacker attacks to
evaluate the security of information systems . This study aims not only to
clarify the theoretical foundations of penetration testing but also to explain
and demonstrate the complete testing process, including how network system
administrators may simulate attacks using various penetration testing methods.
Methodologically, the paper outlines the five basic stages of a typical
penetration test: intelligence gathering, vulnerability scanning, vulnerability
exploitation, privilege escalation, and post-exploitation activities. In each
phase, specific tools and techniques are examined in detail, along with
practical guidance on their use. To enhance the practical relevance of the
study, the paper also presents a real-life case study, illustrating how a
complete penetration test is conducted in a real-world environment. Through
this case, readers can gain insights into the detailed procedures and applied
techniques, thereby deepening their understanding of the practical value of
penetration testing. Finally, the paper summarizes the importance and necessity
of penetration testing in securing information systems and maintaining network
integrity, and it explores future trends and development directions for the
field. Overall, the findings of this paper offer valuable references for both
researchers and practitioners, contributing meaningfully to the improvement of
penetration testing practices and the advancement of cybersecurity as a whole.

</details>


### [460] [ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast \& Slow Reasoning for Robust Agent Defense](https://arxiv.org/abs/2505.19260)
*Shiyu Xiang,Tong Zhang,Ronghao Chen*

Main category: cs.CR

TL;DR: LLM Agents are crucial in intelligent systems but pose safety concerns. The paper proposes ALRPHFS, a defense framework with offline adversarial self-learning and online hierarchical reasoning to enhance robustness and efficiency. It outperforms existing methods with 80% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the semantic gap between safety checks and real-world risks in LLM Agents, which current defenses struggle to cover effectively.

Method: ALRPHFS includes an offline adversarial self-learning loop for refining risk patterns and an online hierarchical fast & slow reasoning engine for balancing detection and computation.

Result: Achieves superior overall performance with an average accuracy of 80%, showing strong generalizability across agents and tasks.

Conclusion: ALRPHFS bridges the semantic gap, enhancing robustness without retraining base LLMs and balancing effectiveness with efficiency.

Abstract: LLM Agents are becoming central to intelligent systems. However, their
deployment raises serious safety concerns. Existing defenses largely rely on
"Safety Checks", which struggle to capture the complex semantic risks posed by
harmful user inputs or unsafe agent behaviors - creating a significant semantic
gap between safety checks and real-world risks. To bridge this gap, we propose
a novel defense framework, ALRPHFS (Adversarially Learned Risk Patterns with
Hierarchical Fast & Slow Reasoning). ALRPHFS consists of two core components:
(1) an offline adversarial self-learning loop to iteratively refine a
generalizable and balanced library of risk patterns, substantially enhancing
robustness without retraining the base LLM, and (2) an online hierarchical fast
& slow reasoning engine that balances detection effectiveness with
computational efficiency. Experimental results demonstrate that our approach
achieves superior overall performance compared to existing baselines, achieving
a best-in-class average accuracy of 80% and exhibiting strong generalizability
across agents and tasks.

</details>


### [461] [BSAGIoT: A Bayesian Security Aspect Graph for Internet of Things (IoT)](https://arxiv.org/abs/2505.19283)
*Zeinab Lashkaripour,Masoud Khosravi-Farmad,AhmadReza Montazerolghaem,Razieh Rezaee*

Main category: cs.CR

TL;DR: The paper proposes a novel Bayesian Security Aspects Dependency Graph for IoT (BSAGIoT) to comprehensively overview the existing threats and vulnerabilities in IoT networks.


<details>
  <summary>Details</summary>
Motivation: Previous studies mainly focused on attack classifications and open issues rather than presenting a comprehensive overview on the existing threats and vulnerabilities.

Method: The researchers proposed different security aspects and a novel Bayesian Security Aspects Dependency Graph for IoT (BSAGIoT) that contains aspects from five categories named data, access control, standard, network, and loss.

Result: Experimental results with various scenarios have been presented, indicating the impact of the aspects on each other and how they could be utilized to mitigate and/or eliminate the security and privacy deficiencies in IoT networks.

Conclusion: BSAGIoT is a generic model applicable to any IoT network and can assist security experts in analyzing successful compromise and/or failed breach impacts, root cause identification of security challenges, their mutual effects, and risk assessment.

Abstract: IoT is a dynamic network of interconnected things that communicate and
exchange data, where security is a significant issue. Previous studies have
mainly focused on attack classifications and open issues rather than presenting
a comprehensive overview on the existing threats and vulnerabilities. This
knowledge helps analyzing the network in the early stages even before any
attack takes place. In this paper, the researchers have proposed different
security aspects and a novel Bayesian Security Aspects Dependency Graph for IoT
(BSAGIoT) to illustrate their relations. The proposed BSAGIoT is a generic
model applicable to any IoT network and contains aspects from five categories
named data, access control, standard, network, and loss. This proposed Bayesian
Security Aspect Graph (BSAG) presents an overview of the security aspects in
any given IoT network. The purpose of BSAGIoT is to assist security experts in
analyzing how a successful compromise and/or a failed breach could impact the
overall security and privacy of the respective IoT network. In addition, root
cause identification of security challenges, how they affect one another, their
impact on IoT networks via topological sorting, and risk assessment could be
achieved. Hence, to demonstrate the feasibility of the proposed method,
experimental results with various scenarios has been presented, in which the
security aspects have been quantified based on the network configurations. The
results indicate the impact of the aspects on each other and how they could be
utilized to mitigate and/or eliminate the security and privacy deficiencies in
IoT networks.

</details>


### [462] [A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control](https://arxiv.org/abs/2505.19301)
*Ken Huang,Vineeth Sai Narajala,John Yeoh,Ramesh Raskar,Youssef Harkati,Jerry Huang,Idan Habler,Chris Hughes*

Main category: cs.CR

TL;DR: 传统IAM系统无法满足多智能体系统（MAS）中AI代理的需求。本文提出了一种新的基于去中心化标识符（DIDs）和可验证凭证（VCs）的代理AI IAM框架，包括代理命名服务（ANS）、动态细粒度访问控制、统一的全局会话管理和零知识证明（ZKPs），以确保信任、问责制和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的IAM协议（如OAuth、OIDC和SAML）主要针对人类用户或静态机器身份设计，对于大规模运行的AI代理在多智能体系统中的动态、相互依赖和通常是短暂的特性显得根本不足。

Method: 1. 使用DIDs和VCs构建丰富的、可验证的代理身份（IDs），包含代理的能力、来源、行为范围和安全态势。
2. 提出包含ANS的框架，用于安全和能力感知的发现。
3. 实施动态细粒度访问控制机制。
4. 设立统一的全局会话管理和策略执行层。
5. 探索使用ZKPs进行隐私保护的属性披露和可验证的策略合规性。

Result: 该框架为代理AI提供了基础的信任、问责制和安全性，能够适应复杂的生态系统，并支持实时控制和一致的撤销机制。

Conclusion: 本文提出的新型IAM框架为代理AI和其复杂生态系统建立了必要的信任、问责制和安全性基础，推动了代理AI领域的发展。

Abstract: Traditional Identity and Access Management (IAM) systems, primarily designed
for human users or static machine identities via protocols such as OAuth,
OpenID Connect (OIDC), and SAML, prove fundamentally inadequate for the
dynamic, interdependent, and often ephemeral nature of AI agents operating at
scale within Multi Agent Systems (MAS), a computational system composed of
multiple interacting intelligent agents that work collectively.
  This paper posits the imperative for a novel Agentic AI IAM framework: We
deconstruct the limitations of existing protocols when applied to MAS,
illustrating with concrete examples why their coarse-grained controls,
single-entity focus, and lack of context-awareness falter. We then propose a
comprehensive framework built upon rich, verifiable Agent Identities (IDs),
leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs),
that encapsulate an agents capabilities, provenance, behavioral scope, and
security posture.
  Our framework includes an Agent Naming Service (ANS) for secure and
capability-aware discovery, dynamic fine-grained access control mechanisms, and
critically, a unified global session management and policy enforcement layer
for real-time control and consistent revocation across heterogeneous agent
communication protocols. We also explore how Zero-Knowledge Proofs (ZKPs)
enable privacy-preserving attribute disclosure and verifiable policy
compliance.
  We outline the architecture, operational lifecycle, innovative contributions,
and security considerations of this new IAM paradigm, aiming to establish the
foundational trust, accountability, and security necessary for the burgeoning
field of agentic AI and the complex ecosystems they will inhabit.

</details>


### [463] [RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks](https://arxiv.org/abs/2505.19364)
*Amit Chakraborty,Sayyed Farid Ahamed,Sandip Roy,Soumya Banerjee,Kevin Choi,Abdul Rahman,Alison Hu,Edward Bowen,Sachin Shetty*

Main category: cs.CR

TL;DR: The paper introduces RADEP, a multi-layered defense framework to protect MLaaS models from extraction attacks. It uses progressive adversarial training, malicious query detection, adaptive response mechanism, and ownership verification. Experiments show it effectively reduces extraction success rates with minimal impact on legitimate queries.


<details>
  <summary>Details</summary>
Motivation: Model extraction attacks pose a significant threat to MLaaS by allowing adversaries to reconstruct functionally similar models via repeated API queries, thus compromising intellectual property and security.

Method: RADEP employs progressive adversarial training for model resilience, combines uncertainty quantification and behavioral pattern analysis for malicious query detection, implements an adaptive response mechanism to modify query outputs based on suspicion scores, and enforces ownership verification through watermarking and backdoor triggers.

Result: Experimental evaluations indicate that RADEP significantly lowers extraction success rates while maintaining high detection accuracy and having little effect on legitimate queries. It remains effective even against adaptive adversaries.

Conclusion: RADEP is a reliable security framework for MLaaS models, providing robust protection against model extraction attacks without significantly impacting legitimate users.

Abstract: Machine Learning as a Service (MLaaS) enables users to leverage powerful
machine learning models through cloud-based APIs, offering scalability and ease
of deployment. However, these services are vulnerable to model extraction
attacks, where adversaries repeatedly query the application programming
interface (API) to reconstruct a functionally similar model, compromising
intellectual property and security. Despite various defense strategies being
proposed, many suffer from high computational costs, limited adaptability to
evolving attack techniques, and a reduction in performance for legitimate
users. In this paper, we introduce a Resilient Adaptive Defense Framework for
Model Extraction Attack Protection (RADEP), a multifaceted defense framework
designed to counteract model extraction attacks through a multi-layered
security approach. RADEP employs progressive adversarial training to enhance
model resilience against extraction attempts. Malicious query detection is
achieved through a combination of uncertainty quantification and behavioral
pattern analysis, effectively identifying adversarial queries. Furthermore, we
develop an adaptive response mechanism that dynamically modifies query outputs
based on their suspicion scores, reducing the utility of stolen models.
Finally, ownership verification is enforced through embedded watermarking and
backdoor triggers, enabling reliable identification of unauthorized model use.
Experimental evaluations demonstrate that RADEP significantly reduces
extraction success rates while maintaining high detection accuracy with minimal
impact on legitimate queries. Extensive experiments show that RADEP effectively
defends against model extraction attacks and remains resilient even against
adaptive adversaries, making it a reliable security framework for MLaaS models.

</details>


### [464] [VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation](https://arxiv.org/abs/2505.19395)
*Ethan TS. Liu,Austin Wang,Spencer Mateega,Carlos Georgescu,Danny Tang*

Main category: cs.CR

TL;DR: This paper introduces VADER, a benchmark to evaluate LLMs' ability in handling software vulnerabilities. It includes 174 real-world cases and assesses models on detection, explanation, and remediation. Current top LLMs show moderate success with room for improvement.


<details>
  <summary>Details</summary>
Motivation: To ensure that LLMs can effectively handle software vulnerabilities for building robust and secure systems.

Method: Introduced VADER, a human-evaluated benchmark comprising 174 curated vulnerabilities from GitHub, assessing LLMs across four dimensions using one-shot prompting strategy and rigorous scoring rubric.

Result: State-of-the-art LLMs achieved only moderate success (49-54% accuracy) on VADER, with remediation quality strongly correlated with accurate classification and test plans.

Conclusion: VADER is publicly released to provide an interpretable and reproducible benchmark for advancing vulnerability-aware LLMs.

Abstract: Ensuring that large language models (LLMs) can effectively assess, detect,
explain, and remediate software vulnerabilities is critical for building robust
and secure software systems. We introduce VADER, a human-evaluated benchmark
designed explicitly to assess LLM performance across four key
vulnerability-handling dimensions: assessment, detection, explanation, and
remediation. VADER comprises 174 real-world software vulnerabilities, each
carefully curated from GitHub repositories and annotated by security experts.
For each vulnerability case, models are tasked with identifying the flaw,
classifying it using Common Weakness Enumeration (CWE), explaining its
underlying cause, proposing a patch, and formulating a test plan. Using a
one-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7
Sonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and
human security experts evaluated each response according to a rigorous scoring
rubric emphasizing remediation (quality of the code fix, 50%), explanation
(20%), and classification and test plan (30%) according to a standardized
rubric. Our results show that current state-of-the-art LLMs achieve only
moderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with
others in the 49-54% range, indicating ample room for improvement. Notably,
remediation quality is strongly correlated (Pearson r > 0.97) with accurate
classification and test plans, suggesting that models that effectively
categorize vulnerabilities also tend to fix them well. VADER's comprehensive
dataset, detailed evaluation rubrics, scoring tools, and visualized results
with confidence intervals are publicly released, providing the community with
an interpretable, reproducible benchmark to advance vulnerability-aware LLMs.
All code and data are available at: https://github.com/AfterQuery/vader

</details>


### [465] [An Empirical Study of JavaScript Inclusion Security Issues in Chrome Extensions](https://arxiv.org/abs/2505.19456)
*Chong Guan*

Main category: cs.CR

TL;DR: The paper conducts a systematic measurement of JavaScript inclusions in Chrome extensions, revealing security issues such as vulnerable remote JavaScript inclusions and the use of outdated libraries.


<details>
  <summary>Details</summary>
Motivation: To address the lack of thorough investigation into the security of JavaScript inclusions within browser extensions despite their divergent security paradigms from web pages.

Method: Employing a hybrid methodology encompassing static and dynamic analysis to identify JavaScript inclusions in Chrome extensions.

Result: Analysis of 36,324 extensions revealed 350,784 JavaScript inclusions with 22 instances of vulnerable remote inclusions and prevalent use of susceptible and outdated libraries.

Conclusion: Chrome extensions have potential security risks due to vulnerable remote JavaScript inclusions and outdated library usage.

Abstract: JavaScript, a scripting language employed to augment the capabilities of web
browsers within web pages or browser extensions, utilizes code segments termed
JavaScript inclusions. While the security aspects of JavaScript inclusions in
web pages have undergone substantial scrutiny, a thorough investigation into
the security of such inclusions within browser extensions remains absent,
despite the divergent security paradigms governing these environments. This
study presents a systematic measurement of JavaScript inclusions in Chrome
extensions, employing a hybrid methodology encompassing static and dynamic
analysis to identify these inclusions. The analysis of 36,324 extensions
revealed 350,784 JavaScript inclusions. Subsequent security assessment
indicated that, although the majority of these inclusions originate from local
files within the extensions rather than external servers, 22 instances of
vulnerable remote JavaScript inclusions were identified. These remote
inclusions present potential avenues for malicious actors to execute arbitrary
code within the extension's execution context. Furthermore, an analysis of
JavaScript library utilization within Chrome extensions disclosed the prevalent
use of susceptible and outdated libraries, notably within numerous widely
adopted extensions.

</details>


### [466] [Language of Network: A Generative Pre-trained Model for Encrypted Traffic Comprehension](https://arxiv.org/abs/2505.19482)
*Di Zhao,Bo Jiang,Song Liu,Susu Cui,Meng Shen,Dongqi Han,Xingmao Guan,Zhigang Lu*

Main category: cs.CR

TL;DR: 为了应对加密流量分析中的挑战，本文提出了一种基于预训练的生成模型GBC，并引入了协议感知的分词方法和提示学习机制，从而在分类任务中实现了比现有方法高5%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在加密流量分类中存在对标注数据的高度依赖以及难以检测攻击变体的问题，这些问题源于数据质量敏感性和攻击模式快速演变的特点。

Method: 提出了一个基于预训练的生成模型GBC，采用协议感知的分词方法来增强模型对网络流量特定字段的理解，并通过预训练从大量未标注流量数据中学习通用表示，再利用提示学习适应下游任务。

Result: 在多个数据集上的评估表明，GBC在流量分类和生成任务中均表现出色，分类任务的F1分数相比现有最佳方法提高了5%。

Conclusion: GBC模型通过改进的分词方法、预训练和提示学习机制，在加密流量理解和攻击检测方面取得了显著进步。

Abstract: The increasing demand for privacy protection and security considerations
leads to a significant rise in the proportion of encrypted network traffic.
Since traffic content becomes unrecognizable after encryption, accurate
analysis is challenging, making it difficult to classify applications and
detect attacks. Deep learning is currently the predominant approach for
encrypted traffic classification through feature analysis. However, these
methods face limitations due to their high dependence on labeled data and
difficulties in detecting attack variants. First, their performance is highly
sensitive to data quality, where the highcost manual labeling process and
dataset imbalance significantly degrade results. Second, the rapid evolution of
attack patterns makes it challenging for models to identify new types of
attacks. To tackle these challenges, we present GBC, a generative model based
on pre-training for encrypted traffic comprehension. Since traditional
tokenization methods are primarily designed for natural language, we propose a
protocol-aware tokenization approach for encrypted traffic that improves model
comprehension of fields specific to network traffic. In addition, GBC employs
pretraining to learn general representations from extensive unlabeled traffic
data. Through prompt learning, it effectively adapts to various downstream
tasks, enabling both high-quality traffic generation and effective detection.
Evaluations across multiple datasets demonstrate that GBC achieves superior
results in both traffic classification and generation tasks, resulting in a 5%
improvement in F1 score compared to state-of-the-art methods for classification
tasks.

</details>


### [467] [Weak-Jamming Detection in IEEE 802.11 Networks: Techniques, Scenarios and Mobility](https://arxiv.org/abs/2505.19633)
*Martijn Hanegraaf,Savio Sciancalepore,Gabriele Oligeri*

Main category: cs.CR

TL;DR: 本文提出并全面分析了新的通用策略，用于检测弱干扰信号，并与IEEE 802.11通信技术兼容。通过卷积神经网络的二分类和稀疏自动编码器的一类分类两种模式进行操作。使用现实世界实验数据评估并与现有方法对比，证明在所有考虑的实际环境中检测弱干扰信号是可行的。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的解决方案只能在干扰攻击已经破坏无线通信链路后才检测到。许多场景下需要早期检测干扰信号（弱干扰信号），以便增强情境感知并及时采取缓解措施。然而，现有研究假设过于简单，缺乏实际部署证据。

Method: 提出两种检测模式：1) 使用卷积神经网络进行二分类；2) 使用稀疏自动编码器进行一类分类。基于现实世界实验数据集进行全面评估和比较。

Result: 结果表明，在所有考虑的实际环境中检测弱干扰信号是可行的，并提供了不同技术、场景和移动模式的深入分析。

Conclusion: 检测弱干扰信号在实际应用中是可行的，且所提方法在不同环境下表现良好，为未来研究奠定了基础。

Abstract: State-of-the-art solutions detect jamming attacks ex-post, i.e., only when
jamming has already disrupted the wireless communication link. In many
scenarios, e.g., mobile networks or static deployments distributed over a large
geographical area, it is often desired to detect jamming at the early stage,
when it affects the communication link enough to be detected but not
sufficiently to disrupt it (detection of weak jamming signals). Under such
assumptions, devices can enhance situational awareness and promptly apply
mitigation, e.g., moving away from the jammed area in mobile scenarios or
changing communication frequency in static deployments, before jamming fully
disrupts the communication link. Although some contributions recently
demonstrated the feasibility of detecting low-power and weak jamming signals,
they make simplistic assumptions far from real-world deployments. Given the
current state of the art, no evidence exists that detection of weak jamming can
be considered with real-world communication technologies. In this paper, we
provide and comprehensively analyze new general-purpose strategies for
detecting weak jamming signals, compatible by design with one of the most
relevant communication technologies used by commercial-off-the-shelf devices,
i.e., IEEE 802.11. We describe two operational modes: (i) binary classification
via Convolutional Neural Networks and (ii) one-class classification via Sparse
Autoencoders. We evaluate and compare the proposed approaches with the current
state-of-the-art using data collected through an extensive real-world
experimental campaign in three relevant environments. At the same time, we made
the dataset available to the public. Our results demonstrate that detecting
weak jamming signals is feasible in all considered real-world environments, and
we provide an in-depth analysis considering different techniques, scenarios,
and mobility patterns.

</details>


### [468] [Poison in the Well: Feature Embedding Disruption in Backdoor Attacks](https://arxiv.org/abs/2505.19821)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Shouling Ji*

Main category: cs.CR

TL;DR: ShadowPrint是一种新的后门攻击方法，通过针对神经网络中的特征嵌入来实现高成功率和隐蔽性。它减少了对训练数据的依赖，并在极低的投毒率下有效。实验表明，ShadowPrint在不同设置下具有优越的成功率、稳定的分类准确性和低检测率，强调了对特征空间操作进行防御的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的后门攻击存在过度依赖训练数据、隐蔽性差和不稳定的问题，限制了其在实际应用中的效果。

Method: ShadowPrint通过针对神经网络中的特征嵌入进行操作，采用基于聚类的优化策略来对齐特征嵌入，从而减少对训练数据的依赖并在极低的投毒率下保持稳定性和隐蔽性。

Result: ShadowPrint实现了优越的攻击成功率（高达100%）、稳定的分类准确性（在大多数情况下衰减不超过1%）和低检测率（平均低于5%）。

Conclusion: ShadowPrint为后门攻击能力设定了新标准，突显了对特征空间操作进行高级防御策略的需求。

Abstract: Backdoor attacks embed malicious triggers into training data, enabling
attackers to manipulate neural network behavior during inference while
maintaining high accuracy on benign inputs. However, existing backdoor attacks
face limitations manifesting in excessive reliance on training data, poor
stealth, and instability, which hinder their effectiveness in real-world
applications. Therefore, this paper introduces ShadowPrint, a versatile
backdoor attack that targets feature embeddings within neural networks to
achieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint
reduces reliance on training data access and operates effectively with
exceedingly low poison rates (as low as 0.01%). It leverages a clustering-based
optimization strategy to align feature embeddings, ensuring robust performance
across diverse scenarios while maintaining stability and stealth. Extensive
evaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),
steady CA (with decay no more than 1% in most cases), and low DDR (averaging
below 5%) across both clean-label and dirty-label settings, and with poison
rates ranging from as low as 0.01% to 0.05%, setting a new standard for
backdoor attack capabilities and emphasizing the need for advanced defense
strategies focused on feature space manipulations.

</details>


### [469] [One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP](https://arxiv.org/abs/2505.19840)
*Binyan Xu,Xilin Dai,Di Tang,Kehuan Zhang*

Main category: cs.CR

TL;DR: This paper introduces UnivIntruder, a new attack framework that uses a public CLIP model and datasets to create adversarial perturbations without needing the target model's training data or excessive queries. It achieves high attack success rates on various models and platforms.


<details>
  <summary>Details</summary>
Motivation: To develop an effective adversarial attack method that does not require access to the target model's training data or frequent queries, addressing challenges in realistic scenarios where these conditions are often restricted.

Method: The UnivIntruder framework leverages a single, publicly available CLIP model and textual concepts to generate universal, transferable, and targeted adversarial perturbations that can mislead DNNs into misclassification based on adversary-specified classes.

Result: UnivIntruder achieves an Attack Success Rate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, surpassing existing methods. It also compromises real-world systems like Google and Baidu image search engines with ASR rates up to 84%, and vision language models such as GPT-4 and Claude-3.5 with ASR rates up to 80%.

Conclusion: The study demonstrates the practicality of the UnivIntruder attack in scenarios where traditional methods are limited, revealing significant vulnerabilities in AI applications and emphasizing the necessity to reassess security measures.

Abstract: Deep Neural Networks (DNNs) have achieved widespread success yet remain prone
to adversarial attacks. Typically, such attacks either involve frequent queries
to the target model or rely on surrogate models closely mirroring the target
model -- often trained with subsets of the target model's training data -- to
achieve high attack success rates through transferability. However, in
realistic scenarios where training data is inaccessible and excessive queries
can raise alarms, crafting adversarial examples becomes more challenging. In
this paper, we present UnivIntruder, a novel attack framework that relies
solely on a single, publicly available CLIP model and publicly available
datasets. By using textual concepts, UnivIntruder generates universal,
transferable, and targeted adversarial perturbations that mislead DNNs into
misclassifying inputs into adversary-specified classes defined by textual
concepts.
  Our extensive experiments show that our approach achieves an Attack Success
Rate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly
outperforming existing transfer-based methods. Additionally, we reveal
real-world vulnerabilities, showing that even without querying target models,
UnivIntruder compromises image search engines like Google and Baidu with ASR
rates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR
rates up to 80%. These findings underscore the practicality of our attack in
scenarios where traditional avenues are blocked, highlighting the need to
reevaluate security paradigms in AI applications.

</details>


### [470] [CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models](https://arxiv.org/abs/2505.19864)
*Chunyang Li,Junwei Zhang,Anda Cheng,Zhuo Ma,Xinghua Li,Jianfeng Ma*

Main category: cs.CR

TL;DR: The paper introduces CPA-RAG, a black-box adversarial framework for RAG systems that generates high-quality adversarial samples to manipulate retrieval processes and induce target answers. It achieves over 90% attack success in certain settings, outperforms existing black-box baselines, and compromises a commercial RAG system.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing poisoning methods for RAG systems, such as poor generalization and lack of fluency in adversarial texts, by proposing a new framework that can generate query-relevant texts capable of manipulating the retrieval process effectively.

Method: CPA-RAG integrates prompt-based text generation, cross-guided optimization through multiple LLMs, and retriever-based scoring to construct high-quality adversarial samples. It aims to manipulate the retrieval process to induce target answers.

Result: CPA-RAG achieves over 90% attack success when the top-k retrieval setting is 5, matching white-box performance. It maintains a consistent advantage of approximately 5 percentage points across different top-k values and outperforms existing black-box baselines by 14.5 percentage points under various defense strategies. The framework successfully compromises a commercial RAG system deployed on Alibaba's BaiLian platform.

Conclusion: The findings highlight the need for more robust and secure RAG frameworks to defend against poisoning attacks.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge, but its openness introduces vulnerabilities
that can be exploited by poisoning attacks. Existing poisoning methods for RAG
systems have limitations, such as poor generalization and lack of fluency in
adversarial texts. In this paper, we propose CPA-RAG, a black-box adversarial
framework that generates query-relevant texts capable of manipulating the
retrieval process to induce target answers. The proposed method integrates
prompt-based text generation, cross-guided optimization through multiple LLMs,
and retriever-based scoring to construct high-quality adversarial samples. We
conduct extensive experiments across multiple datasets and LLMs to evaluate its
effectiveness. Results show that the framework achieves over 90\% attack
success when the top-k retrieval setting is 5, matching white-box performance,
and maintains a consistent advantage of approximately 5 percentage points
across different top-k values. It also outperforms existing black-box baselines
by 14.5 percentage points under various defense strategies. Furthermore, our
method successfully compromises a commercial RAG system deployed on Alibaba's
BaiLian platform, demonstrating its practical threat in real-world
applications. These findings underscore the need for more robust and secure RAG
frameworks to defend against poisoning attacks.

</details>


### [471] [Evaluating AI cyber capabilities with crowdsourced elicitation](https://arxiv.org/abs/2505.19915)
*Artem Petrov,Dmitrii Volkov*

Main category: cs.CR

TL;DR: 通过CTF竞赛中的开放式AI赛道，研究了众包提取AI能力的方法，发现其效果显著且成本效益高，提出将此作为内部提取工作的有效补充，并探讨了收集人类表现数据的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强，理解其在网络安全领域的潜在威胁对于合理治理和负责任部署至关重要，但准确评估这些能力具有挑战性。

Method: 在两个CTF竞赛中设立开放访问的AI赛道，观察AI团队的表现，并与人类选手进行对比，同时探索通过悬赏机制维持对新兴AI能力的及时了解。

Result: AI团队在比赛中表现出色，排名靠前，并获得总计7500美元的奖金，证明了开放市场提取AI能力的有效性。此外，还发现AI代理能够可靠地解决需要中位数人类CTF参与者一小时或更少努力的网络挑战。

Conclusion: 开放式的AI能力提取可能成为内部提取工作的有效补充，悬赏机制是一种及时、成本效益高的方法，可以保持对新兴AI能力的情境意识。

Abstract: As AI systems become increasingly capable, understanding their offensive
cyber potential is critical for informed governance and responsible deployment.
However, it's hard to accurately bound their capabilities, and some prior
evaluations dramatically underestimated them. The art of extracting maximum
task-specific performance from AIs is called "AI elicitation", and today's
safety organizations typically conduct it in-house. In this paper, we explore
crowdsourcing elicitation efforts as an alternative to in-house elicitation
work.
  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI
vs. Humans (400 teams) and Cyber Apocalypse_ (4000 teams). The AI teams achieve
outstanding performance at both events, ranking top-13% and top-21%
respectively for a total of \$7500 in bounties. This impressive performance
suggests that open-market elicitation may offer an effective complement to
in-house elicitation. We propose elicitation bounties as a practical mechanism
for maintaining timely, cost-effective situational awareness of emerging AI
capabilities.
  Another advantage of open elicitations is the option to collect human
performance data at scale. Applying METR's methodology, we found that AI agents
can reliably solve cyber challenges requiring one hour or less of effort from a
median human CTF participant.

</details>


### [472] [DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response](https://arxiv.org/abs/2505.19973)
*Bilel Cherif,Tamas Bisztray,Richard A. Dubniczky,Aaesha Aldahmani,Saeed Alshehhi,Norbert Tihanyi*

Main category: cs.CR

TL;DR: DFIR-Metric is a new benchmark for evaluating LLMs in Digital Forensics and Incident Response (DFIR), consisting of knowledge assessment, realistic forensic challenges, and practical analysis. It introduces the Task Understanding Score (TUS) to better assess models in low-accuracy scenarios.


<details>
  <summary>Details</summary>
Motivation: There is no comprehensive benchmark to evaluate Large Language Models (LLMs) in DFIR tasks, despite their potential and concerns about errors and hallucinations.

Method: The benchmark consists of three components: Knowledge Assessment (700 expert-reviewed multiple-choice questions), Realistic Forensic Challenges (150 CTF-style tasks), and Practical Analysis (500 disk and memory forensics cases). 14 LLMs were evaluated using this benchmark, with a focus on accuracy and consistency. A new metric, Task Understanding Score (TUS), was introduced.

Result: The evaluation provides insights into the performance of LLMs across theoretical and practical DFIR domains, offering a rigorous and reproducible foundation for advancing AI in digital forensics.

Conclusion: DFIR-Metric serves as a valuable tool for assessing LLMs in digital forensics, promoting advancements in this field.

Abstract: Digital Forensics and Incident Response (DFIR) involves analyzing digital
evidence to support legal investigations. Large Language Models (LLMs) offer
new opportunities in DFIR tasks such as log analysis and memory forensics, but
their susceptibility to errors and hallucinations raises concerns in
high-stakes contexts. Despite growing interest, there is no comprehensive
benchmark to evaluate LLMs across both theoretical and practical DFIR domains.
To address this gap, we present DFIR-Metric, a benchmark with three components:
(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice
questions sourced from industry-standard certifications and official
documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing
multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500
disk and memory forensics cases from the NIST Computer Forensics Tool Testing
Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their
accuracy and consistency across trials. We also introduce a new metric, the
Task Understanding Score (TUS), designed to more effectively evaluate models in
scenarios where they achieve near-zero accuracy. This benchmark offers a
rigorous, reproducible foundation for advancing AI in digital forensics. All
scripts, artifacts, and results are available on the project website at
https://github.com/DFIR-Metric.

</details>


### [473] [Eradicating the Unseen: Detecting, Exploiting, and Remediating a Path Traversal Vulnerability across GitHub](https://arxiv.org/abs/2505.20186)
*Jafar Akhoundali,Hamidreza Hamidi,Kristian Rietveld,Olga Gadyatskaya*

Main category: cs.CR

TL;DR: 本研究开发了一种自动管道，用于在GitHub开源项目中扫描、确认、评估并修复可能导致路径遍历攻击的漏洞代码模式（CWE-22）。通过该管道，研究人员发现了1,756个易受攻击的开源项目，并负责任地向维护者报告了这些漏洞，其中14%已被修复。此外，研究还探讨了这种漏洞代码模式的根本原因及其对大型语言模型（LLM）的影响，强调了加强开源生态系统安全性的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 由于开发者经常重复使用代码，导致许多开源项目中存在几乎相同的漏洞。一旦被恶意对手发现其中一个漏洞，就可以轻松扩展攻击范围。因此，研究者希望了解特定易受路径遍历攻击（CWE-22）的代码模式在开源GitHub项目中的普遍性，并提出有效的解决方案。

Method: 研究者开发了一种自动化管道，包含以下步骤：
1. 在GitHub上扫描目标漏洞模式。
2. 通过静态分析确认漏洞。
3. 在项目上下文中利用漏洞以验证其可利用性。
4. 计算CVSS分数以评估漏洞影响。
5. 使用GPT-4生成补丁。
6. 向项目维护者报告漏洞。

Result: 通过该管道，研究者识别出1,756个易受攻击的开源项目，其中许多项目具有高影响力。对于许多受影响的项目，漏洞是关键性的（CVSS评分高于9.0），可以远程利用且无需任何特权，严重影响系统的保密性和可用性。已向维护者负责任地披露了这些漏洞，其中14%已被修复。此外，研究还探讨了这种漏洞代码模式的根本原因及其对大型语言模型的影响。

Conclusion: 研究表明，开源生态系统中广泛存在可导致路径遍历攻击的漏洞代码模式，这突显了帮助保护开源生态系统的迫切需求。需要采用可扩展的自动化漏洞管理解决方案，并提高开发者的安全意识。

Abstract: Vulnerabilities in open-source software can cause cascading effects in the
modern digital ecosystem. It is especially worrying if these vulnerabilities
repeat across many projects, as once the adversaries find one of them, they can
scale up the attack very easily. Unfortunately, since developers frequently
reuse code from their own or external code resources, some nearly identical
vulnerabilities exist across many open-source projects.
  We conducted a study to examine the prevalence of a particular vulnerable
code pattern that enables path traversal attacks (CWE-22) across open-source
GitHub projects. To handle this study at the GitHub scale, we developed an
automated pipeline that scans GitHub for the targeted vulnerable pattern,
confirms the vulnerability by first running a static analysis and then
exploiting the vulnerability in the context of the studied project, assesses
its impact by calculating the CVSS score, generates a patch using GPT-4, and
reports the vulnerability to the maintainers.
  Using our pipeline, we identified 1,756 vulnerable open-source projects, some
of which are very influential. For many of the affected projects, the
vulnerability is critical (CVSS score higher than 9.0), as it can be exploited
remotely without any privileges and critically impact the confidentiality and
availability of the system. We have responsibly disclosed the vulnerability to
the maintainers, and 14\% of the reported vulnerabilities have been remediated.
  We also investigated the root causes of the vulnerable code pattern and
assessed the side effects of the large number of copies of this vulnerable
pattern that seem to have poisoned several popular LLMs. Our study highlights
the urgent need to help secure the open-source ecosystem by leveraging scalable
automated vulnerability management solutions and raising awareness among
developers.

</details>


### [474] [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)
*Haoyu Wang,Zeyu Qin,Yifei Zhao,Chao Du,Min Lin,Xueqian Wang,Tianyu Pang*

Main category: cs.CR

TL;DR: The paper proposes a lifelong safety alignment framework for LLMs to counteract jailbreaking attacks, utilizing a Meta-Attacker and Defender in an iterative training process that significantly boosts defense robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unseen jailbreaking attacks on LLMs during deployment, ensuring their safety and reliability in open-ended environments.

Method: A lifelong safety alignment framework is introduced with two components: a Meta-Attacker trained to discover novel jailbreaking strategies and a Defender trained to resist them. The Meta-Attacker is initially warmed up using insights from jailbreak-related research papers via the GPT-4o API.

Result: In the first iteration, the Meta-Attacker achieves a 73% attack success rate on RR and a 57% transfer ASR on LAT with single-turn attacks. Through iterative training, the Defender reduces the Meta-Attacker's success rate to just 7%.

Conclusion: The proposed framework effectively enhances the robustness of LLMs against jailbreaking attacks, facilitating safer deployment in diverse scenarios.

Abstract: LLMs have made impressive progress, but their growing capabilities also
expose them to highly flexible jailbreaking attacks designed to bypass safety
alignment. While many existing defenses focus on known types of attacks, it is
more critical to prepare LLMs for unseen attacks that may arise during
deployment. To address this, we propose a lifelong safety alignment framework
that enables LLMs to continuously adapt to new and evolving jailbreaking
strategies. Our framework introduces a competitive setup between two
components: a Meta-Attacker, trained to actively discover novel jailbreaking
strategies, and a Defender, trained to resist them. To effectively warm up the
Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a
large collection of jailbreak-related research papers. Through iterative
training, the first iteration Meta-Attacker achieves a 73% attack success rate
(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.
Meanwhile, the Defender progressively improves its robustness and ultimately
reduces the Meta-Attacker's success rate to just 7%, enabling safer and more
reliable deployment of LLMs in open-ended environments. The code is available
at https://github.com/sail-sg/LifelongSafetyAlignment.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [475] [Intent Classification on Low-Resource Languages with Query Similarity Search](https://arxiv.org/abs/2505.18241)
*Arjun Bhalla,Qi Huang*

Main category: cs.IR

TL;DR: 将意图分类转化为查询相似性搜索问题，利用先前查询示例和查询相似性方法，在零样本环境下实现低资源语言的合理意图分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前意图分类方法难以定义意图，数据标注困难且昂贵，扩展到多语言特别是低资源语言时问题更加严重。

Method: 提出将意图分类作为查询相似性搜索问题处理，使用先前查询示例定义意图，并基于潜在空间中最相似查询的标签对新查询进行分类。

Result: 在零样本设置下，能够实现低资源语言的合理意图分类性能。

Conclusion: 通过将意图分类转化为查询相似性搜索问题，可以有效解决低资源语言意图分类的问题。

Abstract: Intent classification is an important component of a functional Information
Retrieval ecosystem. Many current approaches to intent classification,
typically framed as a classification problem, can be problematic as intents are
often hard to define and thus data can be difficult and expensive to annotate.
The problem is exacerbated when we need to extend the intent classification
system to support multiple and in particular low-resource languages. To address
this, we propose casting intent classification as a query similarity search
problem - we use previous example queries to define an intent, and a query
similarity method to classify an incoming query based on the labels of its most
similar queries in latent space. With the proposed approach, we are able to
achieve reasonable intent classification performance for queries in
low-resource languages in a zero-shot setting.

</details>


### [476] [Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems](https://arxiv.org/abs/2505.18366)
*Hansa Meghwani,Amit Agarwal,Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Srikant Panda*

Main category: cs.IR

TL;DR: An enterprise search system improvement via a scalable hard-negative mining framework, showing significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Enterprise search systems often encounter semantic mismatches and overlapping terminologies, which degrade the performance of downstream applications. There is a need for a solution that can enhance the accuracy of domain-specific information retrieval.

Method: Propose a scalable hard-negative mining framework tailored for domain-specific enterprise data. This approach dynamically selects semantically challenging but contextually irrelevant documents to improve re-ranking models. It integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives.

Result: Evaluation on a proprietary enterprise corpus (cloud services domain) shows improvements of 15% in MRR@3 and 19% in MRR@10 compared to state-of-the-art baselines. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms generalizability.

Conclusion: The proposed scalable hard-negative mining framework effectively enhances the performance of enterprise search systems, making it ready for real-world applications.

Abstract: Enterprise search systems often struggle to retrieve accurate,
domain-specific information due to semantic mismatches and overlapping
terminologies. These issues can degrade the performance of downstream
applications such as knowledge management, customer support, and
retrieval-augmented generation agents. To address this challenge, we propose a
scalable hard-negative mining framework tailored specifically for
domain-specific enterprise data. Our approach dynamically selects semantically
challenging but contextually irrelevant documents to enhance deployed
re-ranking models.
  Our method integrates diverse embedding models, performs dimensionality
reduction, and uniquely selects hard negatives, ensuring computational
efficiency and semantic precision. Evaluation on our proprietary enterprise
corpus (cloud services domain) demonstrates substantial improvements of 15\% in
MRR@3 and 19\% in MRR@10 compared to state-of-the-art baselines and other
negative sampling techniques. Further validation on public domain-specific
datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability
and readiness for real-world applications.

</details>


### [477] [AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking](https://arxiv.org/abs/2505.18512)
*Soyoung Yoon,Gyuwan Kim,Gyu-Hwung Cho,Seung-won Hwang*

Main category: cs.IR

TL;DR: AcuRank，一种自适应重排序框架，根据文档相关性的不确定性估计动态调整计算量和目标，使用贝叶斯TrueSkill模型迭代优化相关性估计，直到达到足够信心水平。在TREC-DL和BEIR基准测试中，该方法在准确性和效率之间取得更好的平衡，并且比固定计算基线更有效地利用计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型（LLM）的列表重排序方法通常对固定大小的小子集进行重排序，然后将这些部分结果汇总为最终排名。然而，这种固定的计算方式不考虑查询难度和文档分布，导致效率低下。

Method: 提出了一种名为AcuRank的自适应重排序框架，该框架根据文档相关性的不确定性估计动态调整计算量和目标。具体而言，使用贝叶斯TrueSkill模型迭代地细化相关性估计，直到达到足够的置信度水平。此外，通过显式建模排序不确定性，可以有原则地控制重排序行为，避免对置信预测进行不必要的更新。

Result: 在TREC-DL和BEIR基准上的实验结果表明，所提出的方法在准确性和效率之间始终能够实现更好的权衡，并且随着计算资源的增加，其性能提升优于固定计算的基线方法。

Conclusion: AcuRank方法在不同的检索任务和基于LLM的重排序模型中表现出有效性和普适性，能够在准确性与效率之间取得良好的平衡，并且更高效地利用计算资源。

Abstract: Listwise reranking with large language models (LLMs) enhances top-ranked
results in retrieval-based applications. Due to the limit in context size and
high inference cost of long context, reranking is typically performed over a
fixed size of small subsets, with the final ranking aggregated from these
partial results. This fixed computation disregards query difficulty and
document distribution, leading to inefficiencies. We propose AcuRank, an
adaptive reranking framework that dynamically adjusts both the amount and
target of computation based on uncertainty estimates over document relevance.
Using a Bayesian TrueSkill model, we iteratively refine relevance estimates
until reaching sufficient confidence levels, and our explicit modeling of
ranking uncertainty enables principled control over reranking behavior and
avoids unnecessary updates to confident predictions. Results on the TREC-DL and
BEIR benchmarks show that our method consistently achieves a superior
accuracy-efficiency trade-off and scales better with compute than
fixed-computation baselines. These results highlight the effectiveness and
generalizability of our method across diverse retrieval tasks and LLM-based
reranking models.

</details>


### [478] [GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis](https://arxiv.org/abs/2505.18710)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Bing Qin*

Main category: cs.IR

TL;DR: In the Retrieval-Augmented Generation (RAG) framework, a preference gap between retrievers and LLMs is identified. To address this, GainRAG is proposed, which introduces 'gain' as a metric to align their preferences. A middleware is trained using gain signals, and a pseudo-passage strategy mitigates degradation. Experiments on 6 datasets confirm its effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation in the RAG framework where there is a preference gap between retrievers and LLMs that hinders system performance improvement.

Method: The method involves proposing GainRAG, defining a new metric called 'gain' to measure the contribution of input passages to correct outputs, estimating these gain signals, training a middleware to align the preferences of retriever and LLM using limited data, and introducing a pseudo-passage strategy to mitigate degradation.

Result: The experimental results on 6 datasets verify the effectiveness of GainRAG.

Conclusion: GainRAG effectively addresses the preference gap between retrievers and LLMs in the RAG framework, leading to improved system performance.

Abstract: The Retrieval-Augmented Generation (RAG) framework introduces a retrieval
module to dynamically inject retrieved information into the input context of
large language models (LLMs), and has demonstrated significant success in
various NLP tasks. However, the current study points out that there is a
preference gap between retrievers and LLMs in the RAG framework, which limit
the further improvement of system performance. Some highly relevant passages
may interfere with LLM reasoning because they contain complex or contradictory
information; while some indirectly related or even inaccurate content may help
LLM generate more accurate answers by providing suggestive information or
logical clues. To solve this, we propose GainRAG, a novel approach that aligns
the retriever's and LLM's preferences by defining a new metric, "gain", which
measure how well an input passage contributes to correct outputs. Specifically,
we propose a method to estimate these gain signals and train a middleware that
aligns the preferences of the retriever and the LLM using only limited data. In
addition, we introduce a pseudo-passage strategy to mitigate degradation. The
experimental results on 6 datasets verify the effectiveness of GainRAG.

</details>


### [479] [Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning](https://arxiv.org/abs/2505.18897)
*Dipanwita Saha,Anis Zaman,Hua Zou,Ning Chen,Xinxin Shu,Nadia Vase,Abraham Bagherjeiran*

Main category: cs.IR

TL;DR: The paper presents a method for enhancing keyword matching in search advertising by using document-side semantic keyword expansion with a pre-trained siamese model and cluster-based thresholding, improving relevance and CTR.


<details>
  <summary>Details</summary>
Motivation: To address the issue of reduced relevance caused by token-based matching's overly permissive semantic expansion in connecting user queries with relevant ads.

Method: Employ a pre-trained siamese model to generate dense vector representations of ad keywords and find semantically related variants via nearest neighbor search. Introduce a cluster-based thresholding mechanism to maintain precision. Adapt the downstream relevance model to expanded keyword space using incremental learning with a decision tree ensemble.

Result: Improved both relevance and click-through rate (CTR) while providing a scalable, low-latency solution adaptable to changing query behavior and advertising inventory.

Conclusion: Document-side semantic keyword expansion using the proposed system successfully balances coverage and relevance in search advertising.

Abstract: In search advertising, keyword matching connects user queries with relevant
ads. While token-based matching increases ad coverage, it can reduce relevance
due to overly permissive semantic expansion. This work extends keyword reach
through document-side semantic keyword expansion, using a language model to
broaden token-level matching without altering queries. We propose a solution
using a pre-trained siamese model to generate dense vector representations of
ad keywords and identify semantically related variants through nearest neighbor
search. To maintain precision, we introduce a cluster-based thresholding
mechanism that adjusts similarity cutoffs based on local semantic density. Each
expanded keyword maps to a group of seller-listed items, which may only
partially align with the original intent. To ensure relevance, we enhance the
downstream relevance model by adapting it to the expanded keyword space using
an incremental learning strategy with a lightweight decision tree ensemble.
This system improves both relevance and click-through rate (CTR), offering a
scalable, low-latency solution adaptable to evolving query behavior and
advertising inventory.

</details>


### [480] [HGCL: Hierarchical Graph Contrastive Learning for User-Item Recommendation](https://arxiv.org/abs/2505.19020)
*Jiawei Xue,Zhen Yang,Haitao Lin,Ziji Zhang,Luzhu Wang,Yikun Gu,Yao Xu,Xin Li*

Main category: cs.IR

TL;DR: The paper presents Hierarchical Graph Contrastive Learning (HGCL), which integrates hierarchical item structures into graph contrastive learning for user-item recommendations. HGCL pre-trains with cross-layer contrastive learning, constructs a two-hierarchy bipartite graph via representation compression and clustering, and fine-tunes representations on this graph. Experiments show superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing Graph Contrastive Learning (GCL) methods do not explicitly model hierarchical item structures, which are crucial for enhancing recommendation accuracy.

Method: The proposed method, Hierarchical Graph Contrastive Learning (HGCL), includes three main steps: (1) Pre-training a GCL module using cross-layer contrastive learning to obtain initial user and item representations; (2) Constructing a two-hierarchy user-item bipartite graph through representation compression and clustering; (3) Fine-tuning the user and item representations by learning on the hierarchical graph.

Result: HGCL outperforms existing baseline models in experiments conducted on three benchmark datasets ranging from 70K to 382K nodes, demonstrating the effectiveness of incorporating hierarchical item structures in GCL for recommendation tasks.

Conclusion: Incorporating hierarchical item structures into GCL significantly enhances the performance of user-item recommendation systems.

Abstract: Graph Contrastive Learning (GCL), which fuses graph neural networks with
contrastive learning, has evolved as a pivotal tool in user-item
recommendations. While promising, existing GCL methods often lack explicit
modeling of hierarchical item structures, which represent item similarities
across varying resolutions. Such hierarchical item structures are ubiquitous in
various items (e.g., online products and local businesses), and reflect their
inherent organizational properties that serve as critical signals for enhancing
recommendation accuracy. In this paper, we propose Hierarchical Graph
Contrastive Learning (HGCL), a novel GCL method that incorporates hierarchical
item structures for user-item recommendations. First, HGCL pre-trains a GCL
module using cross-layer contrastive learning to obtain user and item
representations. Second, HGCL employs a representation compression and
clustering method to construct a two-hierarchy user-item bipartite graph.
Ultimately, HGCL fine-tunes user and item representations by learning on the
hierarchical graph, and then provides recommendations based on user-item
interaction scores. Experiments on three widely adopted benchmark datasets
ranging from 70K to 382K nodes confirm the superior performance of HGCL over
existing baseline models, highlighting the contribution of hierarchical item
structures in enhancing GCL methods for recommendation tasks.

</details>


### [481] [BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations](https://arxiv.org/abs/2505.19164)
*Ashirbad Mishra,Jinyu Zhao,Soumik Dey,Hansi Wu,Binbin Li,Kamesh Madduri*

Main category: cs.IR

TL;DR: In sponsored search advertising, exact match types have issues such as high management expenses, limited targeting scope, and evolving search query patterns. Broad match types can alleviate certain drawbacks but present challenges like poor targeting accuracy and minimal supervisory signals. This research defines the criteria for an ideal broad match and proposes BroadGen, an innovative framework that recommends efficient and effective broad match keyphrases by utilizing historical search query data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of exact match types in sponsored search advertising, such as high management expenses, limited targeting scope, and evolving search query patterns, as well as the challenges presented by broad match types like poor targeting accuracy and minimal supervisory signals.

Method: Propose BroadGen, a framework that utilizes historical search query data to recommend efficient and effective broad match keyphrases, emphasizing both efficiency and effectiveness to ensure a significant portion of matched queries are relevant.

Result: BroadGen maintains better query stability over time through token correspondence modeling and is capable of serving daily millions of sellers at eBay with over 2.3 billion items.

Conclusion: BroadGen is an innovative solution that addresses the limitations of exact match types and the challenges of broad match types in sponsored search advertising, providing efficient and effective broad match keyphrase recommendations.

Abstract: In the domain of sponsored search advertising, the focus of Keyphrase
recommendation has largely been on exact match types, which pose issues such as
high management expenses, limited targeting scope, and evolving search query
patterns. Alternatives like Broad match types can alleviate certain drawbacks
of exact matches but present challenges like poor targeting accuracy and
minimal supervisory signals owing to limited advertiser usage. This research
defines the criteria for an ideal broad match, emphasizing on both efficiency
and effectiveness, ensuring that a significant portion of matched queries are
relevant. We propose BroadGen, an innovative framework that recommends
efficient and effective broad match keyphrases by utilizing historical search
query data. Additionally, we demonstrate that BroadGen, through token
correspondence modeling, maintains better query stability over time. BroadGen's
capabilities allow it to serve daily, millions of sellers at eBay with over 2.3
billion items.

</details>


### [482] [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
*Kidist Amde Mekonnen,Yosef Worku Alemneh,Maarten de Rijke*

Main category: cs.IR

TL;DR: The paper introduces Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones, achieving significant improvements over multilingual baselines in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Current neural retrieval methods using transformer-based pre-trained language models have not been thoroughly explored for low-resource, morphologically rich languages such as Amharic due to data scarcity and suboptimal tokenization.

Method: The authors developed Amharic-specific dense retrieval models using pre-trained Amharic BERT and RoBERTa backbones. They also trained a ColBERT-based late interaction retrieval model. Various model sizes were experimented with, including a more compact variant.

Result: The proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieved a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 compared to the strongest multilingual baseline. The ColBERT-based model achieved the highest MRR@10 score of 0.843 among all evaluated models.

Conclusion: The study highlights the challenges in low-resource settings for information retrieval and emphasizes the importance of language-specific adaptation. The authors publicly release their dataset, codebase, and trained models to promote future research in low-resource IR.

Abstract: Neural retrieval methods using transformer-based pre-trained language models
have advanced multilingual and cross-lingual retrieval. However, their
effectiveness for low-resource, morphologically rich languages such as Amharic
remains underexplored due to data scarcity and suboptimal tokenization. We
address this gap by introducing Amharic-specific dense retrieval models based
on pre-trained Amharic BERT and RoBERTa backbones. Our proposed
RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative
improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest
multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact
variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while
being over 13x smaller. Additionally, we train a ColBERT-based late interaction
retrieval model that achieves the highest MRR@10 score (0.843) among all
evaluated models. We benchmark our proposed models against both sparse and
dense retrieval baselines to systematically assess retrieval effectiveness in
Amharic. Our analysis highlights key challenges in low-resource settings and
underscores the importance of language-specific adaptation. To foster future
research in low-resource IR, we publicly release our dataset, codebase, and
trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.

</details>


### [483] [Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model](https://arxiv.org/abs/2505.19505)
*Yu Xia,Rui Zhong,Hao Gu,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: Large Language Models (LLMs) have great potential in Recommendation Systems (RS), but there are challenges in enabling LLMs to effectively comprehend and extract insights from massive user behaviors. This paper proposes a Hierarchical Tree Search-based User Lifelong Behavior Modeling framework (HiT-LBM) that integrates Chunked User Behavior Extraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture diverse interests and interest evolution of user. Additionally, Temporal-Ware Interest Fusion (TIF) is designed to integrate interests from multiple behavior chunks, constructing a comprehensive representation of user lifelong interests.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current approaches that directly leverage LLMs for user interest learning in handling long sequential behaviors, effectively extracting interest, and applying interest in practical scenarios.

Method: The proposed HiT-LBM framework includes CUBE, HTS, and TIF components. CUBE divides user lifelong behaviors into multiple chunks and learns the interest and interest evolution within each chunk in a cascading manner. HTS generates candidate interests through hierarchical expansion and searches for the optimal interest with process rating model to ensure information gain for each behavior chunk. TIF integrates interests from multiple behavior chunks, constructing a comprehensive representation of user lifelong interests.

Result: Extensive experiments demonstrate the effectiveness of our approach, showing that it surpasses state-of-the-art methods.

Conclusion: The HiT-LBM framework effectively captures diverse interests and interest evolution of user, and can be embedded into any recommendation model to enhance performance.

Abstract: Large Language Models (LLMs) have garnered significant attention in
Recommendation Systems (RS) due to their extensive world knowledge and robust
reasoning capabilities. However, a critical challenge lies in enabling LLMs to
effectively comprehend and extract insights from massive user behaviors.
Current approaches that directly leverage LLMs for user interest learning face
limitations in handling long sequential behaviors, effectively extracting
interest, and applying interest in practical scenarios. To address these
issues, we propose a Hierarchical Tree Search-based User Lifelong Behavior
Modeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior
Extraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture
diverse interests and interest evolution of user. CUBE divides user lifelong
behaviors into multiple chunks and learns the interest and interest evolution
within each chunk in a cascading manner. HTS generates candidate interests
through hierarchical expansion and searches for the optimal interest with
process rating model to ensure information gain for each behavior chunk.
Additionally, we design Temporal-Ware Interest Fusion (TIF) to integrate
interests from multiple behavior chunks, constructing a comprehensive
representation of user lifelong interests. The representation can be embedded
into any recommendation model to enhance performance. Extensive experiments
demonstrate the effectiveness of our approach, showing that it surpasses
state-of-the-art methods.

</details>


### [484] [LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval](https://arxiv.org/abs/2505.19588)
*Yanzhen Shen,Sihao Chen,Xueqiang Xu,Yunyi Zhang,Chaitanya Malaviya,Dan Roth*

Main category: cs.IR

TL;DR: LogiCoL is a new learning objective for dense retrievers that improves handling of logical connective queries.


<details>
  <summary>Details</summary>
Motivation: Dual- and bi-encoder dense retrievers have made progress but struggle with queries containing logical connectives, leading to results that don't respect the implied logical constraints.

Method: Introduced LogiCoL, which uses logically-informed contrastive learning with soft constraints expressed via t-norm to teach dense retrievers to respect subset and mutually-exclusive set relations.

Result: Models trained with LogiCoL showed improvements in retrieval performance and logical consistency on entity retrieval tasks.

Conclusion: LogiCoL effectively addresses challenges posed by logical connective queries and provides insights into why such queries are difficult for dense retrievers.

Abstract: While significant progress has been made with dual- and bi-encoder dense
retrievers, they often struggle on queries with logical connectives, a use case
that is often overlooked yet important in downstream applications. Current
dense retrievers struggle with such queries, such that the retrieved results do
not respect the logical constraints implied in the queries. To address this
challenge, we introduce LogiCoL, a logically-informed contrastive learning
objective for dense retrievers. LogiCoL builds upon in-batch supervised
contrastive learning, and learns dense retrievers to respect the subset and
mutually-exclusive set relation between query results via two sets of soft
constraints expressed via t-norm in the learning objective. We evaluate the
effectiveness of LogiCoL on the task of entity retrieval, where the model is
expected to retrieve a set of entities in Wikipedia that satisfy the implicit
logical constraints in the query. We show that models trained with LogiCoL
yield improvement both in terms of retrieval performance and logical
consistency in the results. We provide detailed analysis and insights to
uncover why queries with logical connectives are challenging for dense
retrievers and why LogiCoL is most effective.

</details>


### [485] [AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems](https://arxiv.org/abs/2505.19623)
*Yu Shang,Peijie Liu,Yuwei Yan,Zijing Wu,Leheng Sheng,Yuanqing Yu,Chumeng Jiang,An Zhang,Fengli Xu,Yu Wang,Min Zhang,Yong Li*

Main category: cs.IR

TL;DR: The paper introduces agentic recommender systems powered by LLMs and proposes an interactive textual recommendation simulator, a unified modular framework, and a comprehensive benchmark to evaluate these systems.


<details>
  <summary>Details</summary>
Motivation: To systematically assess agentic recommender systems which are more advanced than traditional ones but currently lack standardized evaluation protocols.

Method: Propose an interactive textual recommendation simulator with three evaluation scenarios, a unified modular framework for development and study, and a comprehensive benchmark comparing 10 classical and agentic methods.

Result: Demonstrates the superiority of agentic systems over classical methods and provides design guidelines for core components.

Conclusion: Establishes a robust environment for ongoing research and reproducibility in agentic recommender systems.

Abstract: The emergence of agentic recommender systems powered by Large Language Models
(LLMs) represents a paradigm shift in personalized recommendations, leveraging
LLMs' advanced reasoning and role-playing capabilities to enable autonomous,
adaptive decision-making. Unlike traditional recommendation approaches, agentic
recommender systems can dynamically gather and interpret user-item interactions
from complex environments, generating robust recommendation strategies that
generalize across diverse scenarios. However, the field currently lacks
standardized evaluation protocols to systematically assess these methods. To
address this critical gap, we propose: (1) an interactive textual
recommendation simulator incorporating rich user and item metadata and three
typical evaluation scenarios (classic, evolving-interest, and cold-start
recommendation tasks); (2) a unified modular framework for developing and
studying agentic recommender systems; and (3) the first comprehensive benchmark
comparing 10 classical and agentic recommendation methods. Our findings
demonstrate the superiority of agentic systems and establish actionable design
guidelines for their core components. The benchmark environment has been
rigorously validated through an open challenge and remains publicly available
with a continuously maintained
leaderboard~\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},
fostering ongoing community engagement and reproducible research. The benchmark
is available at:
\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.

</details>


### [486] [Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach](https://arxiv.org/abs/2505.19544)
*Jialei Chen,Yuanbo Xu,Yiheng Jiang*

Main category: cs.IR

TL;DR: The paper proposes ADRec, a framework to solve embedding collapse in diffusion-based sequential recommendation models through independent noise processes and token-level diffusion. A three-stage training strategy further mitigates embedding collapse, while preserving meaningful patterns in historical interactions.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based sequential recommendation models suffer from embedding collapse, which limits their performance.

Method: ADRec applies an independent noise process to each token and performs diffusion across the entire target sequence during training. It captures token interdependency through auto-regression and models per-token distributions via token-level diffusion. Additionally, a three-stage training strategy is proposed to further mitigate embedding collapse.

Result: ADRec effectively captures both sequence dynamics and item representations, overcoming limitations of existing methods. The model enhances both the accuracy and efficiency of diffusion-based sequential recommendation systems as demonstrated by empirical evaluations on six datasets.

Conclusion: ADRec offers an effective solution to the problem of embedding collapse in diffusion-based sequential recommendation models, significantly improving their performance.

Abstract: In this paper, we focus on the often-overlooked issue of embedding collapse
in existing diffusion-based sequential recommendation models and propose ADRec,
an innovative framework designed to mitigate this problem. Diverging from
previous diffusion-based methods, ADRec applies an independent noise process to
each token and performs diffusion across the entire target sequence during
training. ADRec captures token interdependency through auto-regression while
modeling per-token distributions through token-level diffusion. This dual
approach enables the model to effectively capture both sequence dynamics and
item representations, overcoming the limitations of existing methods. To
further mitigate embedding collapse, we propose a three-stage training
strategy: (1) pre-training the embedding weights, (2) aligning these weights
with the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec
applies the denoising process only to the last token, ensuring that the
meaningful patterns in historical interactions are preserved. Our comprehensive
empirical evaluation across six datasets underscores the effectiveness of ADRec
in enhancing both the accuracy and efficiency of diffusion-based sequential
recommendation systems.

</details>


### [487] [Leveraging Descriptions of Emotional Preferences in Recommender Systems](https://arxiv.org/abs/2505.20190)
*Tonmoy Hasan,Razvan Bunescu*

Main category: cs.IR

TL;DR: The paper proposes a new recommendation task focusing on matching items with user's affective preferences, creates a dataset from book reviews, and evaluates models using textual descriptions of items and affective states.


<details>
  <summary>Details</summary>
Motivation: To go beyond the simple 'liking' in recommendations and incorporate a wide spectrum of affective phenomena including emotions, moods, and fine-grained affective states.

Method: Creating a large dataset of user preferences with fine-grained affective states mined from book reviews and proposing a Transformer-based architecture to leverage these affective expressions as input for recommendation models.

Result: Models utilizing textual descriptions of items and user affective preferences achieved the best results in matching recommended items with affective preferences.

Conclusion: Incorporating affective states in recommendation systems can enhance the ability to match items with user preferences.

Abstract: The affective attitude of liking a recommended item reflects just one
category in a wide spectrum of affective phenomena that also includes emotions
such as entranced or intrigued, moods such as cheerful or buoyant, as well as
more fine-grained affective states, such as "pleasantly surprised by the
conclusion". In this paper, we introduce a novel recommendation task that can
leverage a virtually unbounded range of affective states sought explicitly by
the user in order to identify items that, upon consumption, are likely to
induce those affective states. Correspondingly, we create a large dataset of
user preferences containing expressions of fine-grained affective states that
are mined from book reviews, and propose a Transformer-based architecture that
leverages such affective expressions as input. We then use the resulting
dataset of affective states preferences, together with the linked users and
their histories of book readings, ratings, and reviews, to train and evaluate
multiple recommendation models on the task of matching recommended items with
affective preferences. Experiments show that the best results are obtained by
models that can utilize textual descriptions of items and user affective
preferences.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [488] [EtherBee: A Global Dataset of Ethereum Node Performance Measurements Coupled with Honeypot Interactions and Full Network Sessions](https://arxiv.org/abs/2505.18290)
*Scott Seidenberger,Anindya Maiti*

Main category: cs.NI

TL;DR: 创建了一个名为EtherBee的全球数据集，整合了Ethereum节点指标、网络流量元数据和蜜罐交互日志，提供了关于Ethereum点对点网络活动、稳定性和威胁的独特见解，并通过案例研究展示了客户端优化对网络地理集中度的意外影响。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解以太坊点对点网络中的良性与恶意活动、节点稳定性及网络级威胁，并探索客户端优化对网络地理集中度的影响。

Method: 通过从十个地理上多样的视角收集三个月的数据，整合详细的以太坊节点指标、网络流量元数据和蜜罐交互日志，构建数据集EtherBee，并进行相关性分析。

Result: EtherBee数据集揭示了以太坊网络中的活动模式、稳定性问题和潜在威胁，并发现客户端优化可能导致网络地理集中度增加，从而影响网络弹性和抗审查能力。

Conclusion: 公开发布EtherBee数据集，推动对去中心化网络性能、可靠性和安全性的进一步研究。

Abstract: We introduce EtherBee, a global dataset integrating detailed Ethereum node
metrics, network traffic metadata, and honeypot interaction logs collected from
ten geographically diverse vantage points over three months. By correlating
node data with granular network sessions and security events, EtherBee provides
unique insights into benign and malicious activity, node stability, and
network-level threats in the Ethereum peer-to-peer network. A case study shows
how client-based optimizations can unintentionally concentrate the network
geographically, impacting resilience and censorship resistance. We publicly
release EtherBee to promote further investigations into performance,
reliability, and security in decentralized networks.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [489] [FlashMD: long-stride, universal prediction of molecular dynamics](https://arxiv.org/abs/2505.19350)
*Filippo Bigi,Sanggyu Chong,Agustinus Kristiadi,Michele Ceriotti*

Main category: physics.chem-ph

TL;DR: FlashMD is proposed to predict position and momentum evolution in strides longer than typical MD time steps, incorporating Hamiltonian dynamics properties, generalizing thermodynamic ensembles, assessing failure modes, and validating accuracy.


<details>
  <summary>Details</summary>
Motivation: Machine learning models have accelerated MD by predicting forces inexpensively, but remain constrained to small time integration steps due to atomic motion's fast time scale.

Method: Propose FlashMD which predicts the evolution of positions and momenta over much longer strides than typical MD time steps, incorporating mathematical and physical properties of Hamiltonian dynamics into its architecture, generalizing for any thermodynamic ensemble and carefully assessing failure modes.

Result: FlashMD accurately reproduces equilibrium and time-dependent properties using both system-specific and general-purpose models, extending MD simulation capabilities to reach long time scales.

Conclusion: FlashMD extends MD simulation abilities to model microscopic processes relevant to scientific and technological advancements.

Abstract: Molecular dynamics (MD) provides insights into atomic-scale processes by
integrating over time the equations that describe the motion of atoms under the
action of interatomic forces. Machine learning models have substantially
accelerated MD by providing inexpensive predictions of the forces, but they
remain constrained to minuscule time integration steps, which are required by
the fast time scale of atomic motion. In this work, we propose FlashMD, a
method to predict the evolution of positions and momenta over strides that are
between one and two orders of magnitude longer than typical MD time steps. We
incorporate considerations on the mathematical and physical properties of
Hamiltonian dynamics in the architecture, generalize the approach to allow the
simulation of any thermodynamic ensemble, and carefully assess the possible
failure modes of such a long-stride MD approach. We validate FlashMD's accuracy
in reproducing equilibrium and time-dependent properties, using both
system-specific and general-purpose models, extending the ability of MD
simulation to reach the long time scales needed to model microscopic processes
of high scientific and technological relevance.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [490] [FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets](https://arxiv.org/abs/2505.19819)
*Dannong Wang,Jaisal Patel,Daochen Zha,Steve Y. Yang,Xiao-Yang Liu*

Main category: cs.CE

TL;DR: The paper introduces FinLoRA, an open-source project that benchmarks LoRA methods on financial tasks using 19 datasets including novel XBRL analysis datasets. It evaluates five LoRA methods and LLMs, showing a 36% average performance improvement over base models. The project aims to democratize financial intelligence.


<details>
  <summary>Details</summary>
Motivation: To explore the efficacy of LoRA methods in high-stakes domains like finance and provide an open-source benchmark for evaluating these methods on both general and professional financial tasks.

Method: Curated 19 datasets covering diverse financial applications, created four novel XBRL analysis datasets based on SEC filings, evaluated five LoRA methods and five base LLMs, and measured performance using accuracy, F1, BERTScore as well as computational cost during fine-tuning and inference stages.

Result: LoRA methods achieved substantial performance gains of 36% on average over base models in financial tasks.

Conclusion: FinLoRA provides an affordable and scalable approach to democratize financial intelligence to the general public.

Abstract: Low-rank adaptation (LoRA) methods show great potential for scaling
pre-trained general-purpose Large Language Models (LLMs) to hundreds or
thousands of use scenarios. However, their efficacy in high-stakes domains like
finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings.
In this paper, we present the open-source FinLoRA project that benchmarks LoRA
methods on both general and highly professional financial tasks. First, we
curated 19 datasets covering diverse financial applications; in particular, we
created four novel XBRL analysis datasets based on 150 SEC filings. Second, we
evaluated five LoRA methods and five base LLMs. Finally, we provide extensive
experimental results in terms of accuracy, F1, and BERTScore and report
computational cost in terms of time and GPU memory during fine-tuning and
inference stages. We find that LoRA methods achieved substantial performance
gains of 36\% on average over base models. Our FinLoRA project provides an
affordable and scalable approach to democratize financial intelligence to the
general public. Datasets, LoRA adapters, code, and documentation are available
at https://github.com/Open-Finance-Lab/FinLoRA

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [491] [A fast sound power prediction tool for genset noise using machine learning](https://arxiv.org/abs/2505.20079)
*Saurabh Pargal,Abhijit A. Sane*

Main category: physics.app-ph

TL;DR: This paper explores using machine learning algorithms (KRR, HR, GPR) to predict genset sound power levels based on early-stage information, achieving reliable estimations within 5 dBA for KRR and demonstrating promise for early noise estimation in genset design.


<details>
  <summary>Details</summary>
Motivation: To provide a valuable tool for marketing and sales teams during the early bidding process by enabling reliable prediction of unbuilt gensets' noise levels when measured data is unavailable.

Method: Utilize machine learning regression algorithms (KRR, HR, GPR) with high fidelity datasets from over 100 experiments conducted at Cummins Acoustics Technology Center adhering to ISO 3744 standards.

Result: KRR predicts sound power with an average accuracy of within 5 dBA, while HR and GPR show slightly higher prediction errors but still effectively capture overall noise trends across various genset configurations.

Conclusion: The application of these machine learning algorithms presents a promising method for early-stage noise estimation in genset design.

Abstract: This paper investigates the application of machine learning regression
algorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian
Process Regression (GPR) for predicting sound power levels of gensets, offering
significant value for marketing and sales teams during the early bidding
process. When engine sizes and genset enclosure dimensions are tentative, and
measured noise data is unavailable, these algorithms enable reliable noise
level estimation for unbuilt gensets. The study utilizes high fidelity datasets
from over 100 experiments conducted at Cummins Acoustics Technology Center
(ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using
readily available information from the bidding and initial design stages, KRR
predicts sound power with an average accuracy of within 5 dBA. While HR and GPR
show slightly higher prediction errors, all models effectively capture the
overall noise trends across various genset configurations. These findings
present a promising method for early-stage noise estimation in genset design.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [492] [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
*Xuanhe Zhou,Junxuan He,Wei Zhou,Haodong Chen,Zirui Tang,Haoyu Zhao,Xin Tong,Guoliang Li,Youmin Chen,Jun Zhou,Zhaojun Sun,Binyuan Hui,Shuo Wang,Conghui He,Zhiyuan Liu,Jingren Zhou,Fan Wu*

Main category: cs.DB

TL;DR: The integration of large language models (LLMs) and data management is redefining both fields. This survey reviews the bidirectional relationships, DATA4LLM and LLM4DATA.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models (LLMs) and data management can mutually enhance each other.

Method: Comprehensive review of the bidirectional relationships between LLMs and data management, including DATA4LLM (data processing, storage, serving for LLMs) and LLM4DATA (LLMs for data manipulation, analysis, system optimization).

Result: Data management significantly enhances LLMs through high quality data provision in pre-training, post-training, retrieval-augmented generation, and agentic workflows. Conversely, LLMs serve as powerful engines for data management in areas such as data cleaning, integration, discovery, reasoning, and system optimization.

Conclusion: The integration of LLMs and data management is reshaping both domains, with promising future developments expected.

Abstract: The integration of large language model (LLM) and data management (DATA) is
rapidly redefining both domains. In this survey, we comprehensively review the
bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale
data processing, storage, and serving, feeds LLMs with high quality, diversity,
and timeliness of data required for stages like pre-training, post-training,
retrieval-augmented generation, and agentic workflows: (i) Data processing for
LLMs includes scalable acquisition, deduplication, filtering, selection, domain
mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on
efficient data and model formats, distributed and heterogeneous storage
hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data
serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),
LLM inference (e.g., prompt compression, data provenance), and training
strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,
LLMs are emerging as general-purpose engines for data management. We review
recent advances in (i) data manipulation, including automatic data cleaning,
integration, discovery; (ii) data analysis, covering reasoning over structured,
semi-structured, and unstructured data, and (iii) system optimization (e.g.,
configuration tuning, query rewriting, anomaly diagnosis), powered by LLM
techniques like retrieval-augmented prompting, task-specialized fine-tuning,
and multi-agent collaboration.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [493] [Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of Identity](https://arxiv.org/abs/2505.19790)
*Faruk Alpay*

Main category: math.CT

TL;DR: This paper develops a formal framework using Alpay Algebra to model how observation affects identity dynamics in AI systems, introducing new mathematical tools for explainable AI and self-aware architectures.


<details>
  <summary>Details</summary>
Motivation: To create a mathematically rigorous way to model how observation impacts identity and transformation in artificial systems, building on previous work in Alpay Algebra.

Method: Uses transfinite categorical flows and curvature-driven identity operators within the Alpay Algebra framework to define observer-coupled φ-collapse processes and temporal drift mechanisms.

Result: Produces a system that encodes transformation history into symbolic fixed-point structures, providing traceability and coherence for dynamic identity modeling in AI.

Conclusion: Establishes Alpay Algebra as an advanced symbolic framework connecting category theory, identity logic, and observer dynamics, with applications in self-aware AI and formal logic systems.

Abstract: This paper introduces a formal framework for modeling observer-dependent
collapse dynamics and temporal identity drift within artificial and
mathematical systems, grounded entirely in the symbolic foundations of Alpay
Algebra. Building upon the fixed-point emergence structures developed in Alpay
Algebra I and II, this third installment formalizes the observer-coupled
{\phi}-collapse process through transfinite categorical flows and
curvature-driven identity operators. We define a novel temporal drift mechanism
as a recursive deformation of identity signatures under entangled observer
influence, constructing categorical invariants that evolve across fold
iterations. The proposed system surpasses conventional identity modeling in
explainable AI (XAI) by encoding internal transformation history into a
symbolic fixed-point structure, offering provable traceability and temporal
coherence. Applications range from AI self-awareness architectures to formal
logic systems where identity is not static but dynamically induced by
observation. The theoretical results also offer a mathematically rigorous basis
for future AI systems with stable self-referential behavior, positioning Alpay
Algebra as a next-generation symbolic framework bridging category theory,
identity logic, and observer dynamics.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [494] [LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression](https://arxiv.org/abs/2505.18602)
*Hengzhe Zhang,Qi Chen,Bing Xue,Mengjie Zhang*

Main category: cs.NE

TL;DR: 大型语言模型（LLMs）在符号回归领域自动设计选择算子方面表现出色，超越了九个专家设计的基线，达到业内顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型革新了算法开发，但在符号回归领域的应用仍受限制，通常由人工专家手动设计。本文旨在解决这一问题，提出一种学习进化框架，让LLMs能自动为符号回归进化算法设计选择算子。

Method: 首先识别出现有LLM算法进化技术中的两个关键限制：代码膨胀和缺乏语义引导。然后通过引入代码膨胀控制和语义感知的选择算子来改进LLM进化框架，并将领域知识嵌入到提示中，以生成更有效和上下文相关的选择算子。

Result: 实验结果表明，LLMs设计的选择算子在符号回归基准测试中优于9个专家设计的基线，达到了业内顶尖水平。

Conclusion: 本研究表明，LLMs能够在符号回归领域超越专家级别的算法设计。

Abstract: Large language models (LLMs) have revolutionized algorithm development, yet
their application in symbolic regression, where algorithms automatically
discover symbolic expressions from data, remains constrained and is typically
designed manually by human experts. In this paper, we propose a
learning-to-evolve framework that enables LLMs to automatically design
selection operators for evolutionary symbolic regression algorithms. We first
identify two key limitations in existing LLM-based algorithm evolution
techniques: code bloat and a lack of semantic guidance. Bloat results in
unnecessarily complex components, and the absence of semantic awareness can
lead to ineffective exchange of useful code components, both of which can
reduce the interpretability of the designed algorithm or hinder evolutionary
learning progress. To address these issues, we enhance the LLM-based evolution
framework for meta symbolic regression with two key innovations: bloat control
and a complementary, semantics-aware selection operator. Additionally, we embed
domain knowledge into the prompt, enabling the LLM to generate more effective
and contextually relevant selection operators. Our experimental results on
symbolic regression benchmarks show that LLMs can devise selection operators
that outperform nine expert-designed baselines, achieving state-of-the-art
performance. This demonstrates that LLMs can exceed expert-level algorithm
design for symbolic regression.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [495] [Anomaly detection in radio galaxy data with trainable COSFIRE filters](https://arxiv.org/abs/2505.18643)
*Steven Ndung'u,Trienko Grobler,Stefan J. Wijnholds,George Azzopardi*

Main category: astro-ph.IM

TL;DR: This paper presents a COSFIRE filter-based approach combined with unsupervised LOF algorithm for detecting anomalies in radio astronomy data, achieving better performance than deep learning autoencoders.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting anomalies in radio astronomy data where labeled anomalous examples are rare and complex deep learning methods require extensive supervision.

Method: Using trainable COSFIRE filters to characterize normal patterns and integrate COSFIRE descriptors with an unsupervised Local Outlier Factor (LOF) algorithm to identify unusual radio galaxy morphologies.

Result: Achieved a geometric mean (G-Mean) score of 79% on a radio galaxy benchmark data set, surpassing the 77% achieved by a deep learning autoencoder.

Conclusion: The proposed semi-supervised methodology overcomes the limitation of traditional supervised methods by not requiring anomalous examples in the training set, showing potential for next-generation radio telescopes.

Abstract: Detecting anomalies in radio astronomy is challenging due to the vast amounts
of data and the rarity of labeled anomalous examples. Addressing this challenge
requires efficient methods capable of identifying unusual radio galaxy
morphologies without relying on extensive supervision. This work introduces an
innovative approach to anomaly detection based on morphological characteristics
of the radio sources using trainable COSFIRE (Combination of Shifted Filter
Responses) filters as an efficient alternative to complex deep learning
methods. The framework integrates COSFIRE descriptors with an unsupervised
Local Outlier Factor (LOF) algorithm to identify unusual radio galaxy
morphologies. Evaluations on a radio galaxy benchmark data set demonstrate
strong performance, with the COSFIRE-based approach achieving a geometric mean
(G-Mean) score of 79%, surpassing the 77% achieved by a computationally
intensive deep learning autoencoder. By characterizing normal patterns and
detecting deviations, this semi-supervised methodology overcomes the need for
anomalous examples in the training set, a major limitation of traditional
supervised methods. This approach shows promise for next-generation radio
telescopes, where fast processing and the ability to discover unknown phenomena
are crucial.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [496] [ZeroML: A Next Generation AutoML Language](https://arxiv.org/abs/2505.18243)
*Monirul Islam Mahmud*

Main category: cs.PL

TL;DR: ZeroML is a new programming language for AutoML that addresses the shortcomings of existing languages like Python, R, or Julia. It features a pure functional core and microservices-based architecture to create high-accuracy models quickly and reproducibly.


<details>
  <summary>Details</summary>
Motivation: The motivation behind ZeroML is to overcome issues such as slow running time, brittle pipelines, and high dependency costs associated with current programming languages used in machine learning.

Method: ZeroML employs a compiled, multi-paradigm approach with a pure functional core. It uses a microservices-based architecture incorporating modular components like DataCleaner, FeatureEngineer, and ModelSelector. Additionally, it is a native multithread and memory-aware search optimized toolkit.

Result: ZeroML enables both non-coders and ML professionals to create high-accuracy models rapidly and in a reproducible manner. The language reduces verbosity and boilerplate code on the front end while ensuring clarity on the backend.

Conclusion: ZeroML represents a significant advancement in AutoML by providing a faster, more reliable, and easier-to-use solution for creating machine learning models.

Abstract: ZeroML is a new generation programming language for AutoML to drive the ML
pipeline in a compiled and multi-paradigm way, with a pure functional core.
Meeting the shortcomings introduced by Python, R, or Julia such as slow-running
time, brittle pipelines or high dependency cost ZeroML brings the
Microservices-based architecture adding the modular, reusable pieces such as
DataCleaner, FeatureEngineer or ModelSelector. As a native multithread and
memory-aware search optimized toolkit, and with one command deployability
ability, ZeroML ensures non-coders and ML professionals to create high-accuracy
models super fast and in a more reproducible way. The verbosity of the language
ensures that when it comes to dropping into the backend, the code we will be
creating is extremely clear but the level of repetition and boilerplate
required when developing on the front end is now removed.

</details>


### [497] [Autocomp: LLM-Driven Code Optimization for Tensor Accelerators](https://arxiv.org/abs/2505.18574)
*Charles Hong,Sahil Bhatia,Alvin Cheung,Yakun Sophia Shao*

Main category: cs.PL

TL;DR: 本文提出了一种名为Autocomp的方法，通过结合领域知识和硬件反馈，利用大型语言模型（LLMs）驱动的自动化搜索来优化加速器代码。实验表明，Autocomp优化后的代码在不同任务上显著优于厂商库和专家手动调优代码，并且生成的优化方案可以在相似操作中复用。


<details>
  <summary>Details</summary>
Motivation: 尽管硬件加速器（尤其是张量处理器）已广泛应用于计算领域，但编程这些加速器仍然具有挑战性，导致其潜力未被充分挖掘。虽然大型语言模型在代码生成和优化任务中表现出巨大潜力，但对于特定领域的低资源语言（如专用张量加速器代码）仍面临重大挑战。

Method: 1. 将每个优化过程设计为结构化的两阶段提示：规划阶段和代码生成阶段；2. 在规划阶段引入简洁且可适应的优化菜单以融入领域知识；3. 在每次搜索迭代中整合来自硬件的正确性和性能指标作为反馈。

Result: 在三种代表性工作负载和两种不同加速器上进行实验，结果表明Autocomp优化后的代码比厂商提供的库快5.6倍（GEMM）和2.7倍（卷积），并且超越了专家级手工调优代码（GEMM为1.4倍，卷积为1.1倍，细粒度线性代数为1.3倍）。此外，Autocomp生成的优化计划可以在相似的张量操作中复用，从而在固定样本预算下提高多达24%的速度提升。

Conclusion: Autocomp方法通过结合领域知识和硬件反馈，成功解决了使用LLMs生成特定领域加速器代码的难题，并显著提高了代码性能和效率。同时，优化计划的可复用性进一步增强了该方法的实际应用价值。

Abstract: Hardware accelerators, especially those designed for tensor processing, have
become ubiquitous in today's computing landscape. However, even with
significant efforts in building compilers, programming these tensor
accelerators remains challenging, leaving much of their potential
underutilized. Recently, large language models (LLMs), trained on large amounts
of code, have shown significant promise in code generation and optimization
tasks, but generating low-resource languages like specialized tensor
accelerator code still poses a significant challenge. We tackle this challenge
with Autocomp, an approach that empowers accelerator programmers to leverage
domain knowledge and hardware feedback to optimize code via an automated
LLM-driven search. We accomplish this by: 1) formulating each optimization pass
as a structured two-phase prompt, divided into planning and code generation
phases, 2) inserting domain knowledge during planning via a concise and
adaptable optimization menu, and 3) integrating correctness and performance
metrics from hardware as feedback at each search iteration. Across three
categories of representative workloads and two different accelerators, we
demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x
(convolution) faster than the vendor-provided library, and outperforms
expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x
(fine-grained linear algebra). Additionally, we demonstrate that optimization
schedules generated from Autocomp can be reused across similar tensor
operations, improving speedups by up to 24% under a fixed sample budget.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [498] [Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain](https://arxiv.org/abs/2505.18361)
*Trinity Chung,Yuchen Shen,Nathan C. L. Kong,Aran Nayebi*

Main category: q-bio.NC

TL;DR: EAD框架被引入以探索优化任务的时间神经网络，ConvRNNs被确认为触觉分类的优秀编码器。该模型的神经表示与啮齿动物体感皮层紧密匹配，并揭示了监督分类性能和神经对齐之间的线性关系。对比自我监督的ConvRNN-EAD可以作为无标签代理，匹配监督神经适应。研究强调了非线性循环处理在体感皮层中的重要性，以及循环EAD架构和定制自我监督学习方法在处理真实触觉输入中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 触觉感知在神经科学中尚未被充分理解，在人工系统中也不如视觉和语言等成熟模式有效。为了弥补这些差距，需要一个系统来探索优化任务的时间神经网络。

Method: 使用新颖的Encoder-Attender-Decoder（EAD）框架，基于定制的啮齿动物胡须阵列模拟器生成的真实触觉输入序列进行训练。通过比较不同类型的编码器，确定卷积递归神经网络（ConvRNNs）是优于纯前馈和状态空间架构的触觉分类编码器。利用EAD模型分析神经表示并评估其与啮齿动物体感皮层的匹配度。还探讨了对比自我监督的ConvRNN-EAD模型。

Result: ConvRNN编码器基础的EAD模型实现了与啮齿动物体感皮层紧密匹配的神经表示，饱和了可解释的神经变异，并揭示了监督分类性能和神经对齐之间的清晰线性关系。对比自我监督的ConvRNN-EAD模型可以无标签地匹配监督神经适应。

Conclusion: 非线性循环处理对于体感皮层中的一般触觉表示至关重要。在具身人工智能中，循环EAD架构和定制的自我监督学习方法对于处理真实触觉输入非常重要。

Abstract: Tactile sensing remains far less understood in neuroscience and less
effective in artificial systems compared to more mature modalities such as
vision and language. We bridge these gaps by introducing a novel
Encoder-Attender-Decoder (EAD) framework to systematically explore the space of
task-optimized temporal neural networks trained on realistic tactile input
sequences from a customized rodent whisker-array simulator. We identify
convolutional recurrent neural networks (ConvRNNs) as superior encoders to
purely feedforward and state-space architectures for tactile categorization.
Crucially, these ConvRNN-encoder-based EAD models achieve neural
representations closely matching rodent somatosensory cortex, saturating the
explainable neural variability and revealing a clear linear relationship
between supervised categorization performance and neural alignment.
Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained
with tactile-specific augmentations, match supervised neural fits, serving as
an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as
important for general-purpose tactile representations in somatosensory cortex,
providing the first quantitative characterization of the underlying inductive
biases in this system. For embodied AI, our results emphasize the importance of
recurrent EAD architectures to handle realistic tactile inputs, along with
tailored self-supervised learning methods for achieving robust tactile
perception with the same type of sensors animals use to sense in unstructured
environments.

</details>


### [499] [Multi-modal brain encoding models for multi-modal stimuli](https://arxiv.org/abs/2505.20027)
*Subba Reddy Oota,Khushbu Pahwa,Mounika Marreddy,Maneesh Singh,Manish Gupta,Bapi S. Raju*

Main category: q-bio.NC

TL;DR: 尽管参与者只接触单模态刺激，如观看图像或无声视频，但多模态Transformer模型可以很好地预测视觉脑活动，即使在不一致的模态表示下。本文研究了当参与者接触多模态刺激时，这些模型对脑活动的预测准确性。通过使用多种单模态和多模态模型（跨模态和联合预训练），我们发现多模态模型在多个语言和视觉区域显示出更好的一致性，并且能够区分处理单模态与多模态信息的脑区。进一步分析表明，对于跨模态模型，其脑一致性部分归因于视频模态；而对于联合预训练模型，则部分归因于视频和音频模态。这为神经科学界提供了强大动力，以研究这些模型的可解释性，从而加深对大脑多模态信息处理的理解。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，多模态Transformer模型可以在不一致的模态表示下准确预测视觉脑活动，因此需要探讨这些模型在参与者接受多模态刺激时对脑活动的预测能力。

Method: 使用多种单模态和多模态模型（包括跨模态和联合预训练模型）来研究参与者观看电影时的fMRI脑活动。通过从多模态表示中逐一移除单模态特征，评估每个模态对多模态一致性的贡献。

Result: 1. 多模态模型在语言和视觉区域显示出更好的一致性。
2. 可以识别出处理单模态与多模态信息的脑区。
3. 跨模态模型的脑一致性部分归因于视频模态，而联合预训练模型则部分归因于视频和音频模态。

Conclusion: 多模态模型在预测参与多模态刺激时的脑活动中表现优异，揭示了不同模态对脑活动的影响，并为研究大脑多模态信息处理提供了新视角。

Abstract: Despite participants engaging in unimodal stimuli, such as watching images or
silent videos, recent work has demonstrated that multi-modal Transformer models
can predict visual brain activity impressively well, even with incongruent
modality representations. This raises the question of how accurately these
multi-modal models can predict brain activity when participants are engaged in
multi-modal stimuli. As these models grow increasingly popular, their use in
studying neural activity provides insights into how our brains respond to such
multi-modal naturalistic stimuli, i.e., where it separates and integrates
information across modalities through a hierarchy of early sensory regions to
higher cognition. We investigate this question by using multiple unimodal and
two types of multi-modal models-cross-modal and jointly pretrained-to determine
which type of model is more relevant to fMRI brain activity when participants
are engaged in watching movies. We observe that both types of multi-modal
models show improved alignment in several language and visual regions. This
study also helps in identifying which brain regions process unimodal versus
multi-modal information. We further investigate the contribution of each
modality to multi-modal alignment by carefully removing unimodal features one
by one from multi-modal representations, and find that there is additional
information beyond the unimodal embeddings that is processed in the visual and
language regions. Based on this investigation, we find that while for
cross-modal models, their brain alignment is partially attributed to the video
modality; for jointly pretrained models, it is partially attributed to both the
video and audio modalities. This serves as a strong motivation for the
neuroscience community to investigate the interpretability of these models for
deepening our understanding of multi-modal information processing in brain.

</details>


### [500] [Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)](https://arxiv.org/abs/2505.20029)
*Subba Reddy Oota,Akshett Jindal,Ishani Mondal,Khushbu Pahwa,Satya Sai Srinath Namburi,Manish Shrivastava,Maneesh Singh,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: Instruction-tuned multimodal LLMs (MLLMs) show better brain alignment than vision-only models and capture instruction-specific representations effectively, with potential for improvement in predicting brain responses.


<details>
  <summary>Details</summary>
Motivation: To investigate whether MLLMs lead to better brain alignment and effectively capture instruction-specific representations when prompted with natural instructions.

Method: Measure the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants watch natural scenes, experimenting with 10 different instructions.

Result: MLLMs exhibit significantly better brain alignment than vision-only models, perform comparably to non-instruction-tuned multimodal models like CLIP, and effectively capture count-related and recognition-related concepts.

Conclusion: Enhancing MLLMs' ability to capture task-specific information could improve their precision in predicting brain responses.

Abstract: Transformer-based language models, though not explicitly trained to mimic
brain recordings, have demonstrated surprising alignment with brain activity.
Progress in these models-through increased size, instruction-tuning, and
multimodality-has led to better representational alignment with neural data.
Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have
emerged, showing remarkable zero-shot capabilities in open-ended multimodal
vision tasks. However, it is unknown whether MLLMs, when prompted with natural
instructions, lead to better brain alignment and effectively capture
instruction-specific representations. To address this, we first investigate
brain alignment, i.e., measuring the degree of predictivity of neural visual
activity using text output response embeddings from MLLMs as participants
engage in watching natural scenes. Experiments with 10 different instructions
show that MLLMs exhibit significantly better brain alignment than vision-only
models and perform comparably to non-instruction-tuned multimodal models like
CLIP. We also find that while these MLLMs are effective at generating
high-quality responses suitable to the task-specific instructions, not all
instructions are relevant for brain alignment. Further, by varying
instructions, we make the MLLMs encode instruction-specific visual concepts
related to the input image. This analysis shows that MLLMs effectively capture
count-related and recognition-related concepts, demonstrating strong alignment
with brain activity. Notably, the majority of the explained variance of the
brain encoding models is shared between MLLM embeddings of image captioning and
other instructions. These results suggest that enhancing MLLMs' ability to
capture task-specific information could lead to better differentiation between
various types of instructions, and thereby improving their precision in
predicting brain responses.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [501] [Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity](https://arxiv.org/abs/2505.19648)
*Qiaolan Meng,Juhua Pu,Hongting Niu,Yuyi Wang,Yuanhong Wang,Ondřej Kuželka*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the model enumeration problem of the function-free, finite domain
fragment of first-order logic with two variables ($FO^2$). Specifically, given
an $FO^2$ sentence $\Gamma$ and a positive integer $n$, how can one enumerate
all the models of $\Gamma$ over a domain of size $n$? In this paper, we devise
a novel algorithm to address this problem. The delay complexity, the time
required between producing two consecutive models, of our algorithm is
quadratic in the given domain size $n$ (up to logarithmic factors) when the
sentence is fixed. This complexity is almost optimal since the interpretation
of binary predicates in any model requires at least $\Omega(n^2)$ bits to
represent.

</details>


### [502] [Comparing Neural Network Encodings for Logic-based Explainability](https://arxiv.org/abs/2505.20269)
*Levi Cordeiro Carvalho,Saulo A. F. Oliveira,Thiago Alves Rocha*

Main category: cs.LO

TL;DR: This paper compares two encodings of ANNs into logical constraints for logic-based explainability. Experiments showed that the adapted encoding performed better in building logical constraints and overall time.


<details>
  <summary>Details</summary>
Motivation: Providing explanations for the outputs of artificial neural networks (ANNs) is crucial in many contexts, such as critical systems, data protection laws and handling adversarial examples.

Method: Compare two encodings of ANNs into logical constraints, one from literature and another adapted encoding which uses fewer variables and constraints.

Result: Experiments showed similar running times for computing explanations, but the adapted encoding performed up to 18% better in building logical constraints and up to 16% better in overall time.

Conclusion: The adapted encoding potentially enhances efficiency and performs better in building logical constraints and overall time.

Abstract: Providing explanations for the outputs of artificial neural networks (ANNs)
is crucial in many contexts, such as critical systems, data protection laws and
handling adversarial examples. Logic-based methods can offer explanations with
correctness guarantees, but face scalability challenges. Due to these issues,
it is necessary to compare different encodings of ANNs into logical
constraints, which are used in logic-based explainability. This work compares
two encodings of ANNs: one has been used in the literature to provide
explanations, while the other will be adapted for our context of
explainability. Additionally, the second encoding uses fewer variables and
constraints, thus, potentially enhancing efficiency. Experiments showed similar
running times for computing explanations, but the adapted encoding performed up
to 18\% better in building logical constraints and up to 16\% better in overall
time.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [503] [A Domain Ontology for Modeling the Book of Purification in Islam](https://arxiv.org/abs/2505.18222)
*Hessa Alawwad*

Main category: cs.DL

TL;DR: This paper develops an ontology for the Book of Purification in Islam to address a knowledge gap, following six key steps and focusing on design and analysis.


<details>
  <summary>Details</summary>
Motivation: To address a gap in major Islamic topics by developing an ontology for the Book of Purification in Islam.

Method: The ontology development strategy followed six key steps: (1) domain identification, (2) knowledge acquisition, (3) conceptualization, (4) classification, (5) integration and implementation, and (6) ontology generation.

Result: The developed ontology ensures reusability by formally defining and encoding the key concepts, attributes, and relationships related to the Book of Purification.

Conclusion: This structured representation is intended to support knowledge sharing and reuse.

Abstract: This paper aims to address a gap in major Islamic topics by developing an
ontology for the Book of Purification in Islam. Many authoritative Islamic
texts begin with the Book of Purification, as it is essential for performing
prayer (the second pillar of Islam after Shahadah, the profession of faith) and
other religious duties such as Umrah and Hajj.
  The ontology development strategy followed six key steps: (1) domain
identification, (2) knowledge acquisition, (3) conceptualization, (4)
classification, (5) integration and implementation, and (6) ontology
generation. This paper includes examples of the constructed tables and
classifications.
  The focus is on the design and analysis phases, as technical implementation
is beyond the scope of this study. However, an initial implementation is
provided to illustrate the steps of the proposed strategy.
  The developed ontology ensures reusability by formally defining and encoding
the key concepts, attributes, and relationships related to the Book of
Purification. This structured representation is intended to support knowledge
sharing and reuse.

</details>


### [504] [Clustering scientific publications: lessons learned through experiments with a real citation network](https://arxiv.org/abs/2505.18180)
*Vu Thi Huong,Thorsten Koch*

Main category: cs.DL

TL;DR: This paper evaluates the performance of graph-based clustering algorithms (spectral, Louvain, and Leiden) on a large citation graph from Web of Science, revealing that default settings often yield poor results and meaningful outcomes require careful parameter tuning.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying research structures within bibliographic databases by clustering scientific publications and to evaluate the performance of commonly used graph-based clustering methods on real-world data.

Method: The study uses graph-based clustering methods such as spectral, Louvain, and Leiden algorithms to cluster a citation graph consisting of approximately 700,000 papers and 4.6 million citations extracted from Web of Science.

Result: Scalable methods like Louvain and Leiden perform efficiently but their default settings often result in poor partitioning. Careful parameter tuning is necessary for meaningful outcomes, especially in large networks with uneven structures.

Conclusion: The findings emphasize practical challenges in dealing with large-scale data, selecting appropriate methods, and tuning parameters based on the specific structures encountered in bibliometric clustering tasks.

Abstract: Clustering scientific publications can reveal underlying research structures
within bibliographic databases. Graph-based clustering methods, such as
spectral, Louvain, and Leiden algorithms, are frequently utilized due to their
capacity to effectively model citation networks. However, their performance may
degrade when applied to real-world data. This study evaluates the performance
of these clustering algorithms on a citation graph comprising approx. 700,000
papers and 4.6 million citations extracted from Web of Science. The results
show that while scalable methods like Louvain and Leiden perform efficiently,
their default settings often yield poor partitioning. Meaningful outcomes
require careful parameter tuning, especially for large networks with uneven
structures, including a dense core and loosely connected papers. These findings
highlight practical lessons about the challenges of large-scale data, method
selection and tuning based on specific structures of bibliometric clustering
tasks.

</details>


### [505] [BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text](https://arxiv.org/abs/2505.18207)
*Ibrahim Al Azher,Miftahul Jannat Mokarrama,Zhishuai Guo,Sagnik Ray Choudhury,Hamed Alhoori*

Main category: cs.DL

TL;DR: An architecture for computational analysis of research limitations is presented, including a dataset creation, automatic generation via RAG, and evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Transparent reporting of limitations in scientific research can improve research quality, reproducibility, and public trust in science. However, authors often underreport limitations or use hedging strategies that reduce clarity and confidence.

Method: 1) Create a dataset of limitations from ACL, NeurIPS, and PeerJ papers by extracting text and integrating with external reviews.
2) Propose methods to automatically generate limitations using Retrieval Augmented Generation (RAG) technique.
3) Develop a fine-grained evaluation framework for generated limitations.
4) Provide a meta-evaluation for the proposed evaluation techniques.

Result: A complete architecture was successfully developed, which includes a new dataset, an automatic limitation generation method, and robust evaluation frameworks.

Conclusion: The computational analysis of research limitations offers a promising direction for enhancing the transparency and quality of scientific research.

Abstract: In scientific research, limitations refer to the shortcomings, constraints,
or weaknesses within a study. Transparent reporting of such limitations can
enhance the quality and reproducibility of research and improve public trust in
science. However, authors often a) underreport them in the paper text and b)
use hedging strategies to satisfy editorial requirements at the cost of
readers' clarity and confidence. This underreporting behavior, along with an
explosion in the number of publications, has created a pressing need to
automatically extract or generate such limitations from scholarly papers. In
this direction, we present a complete architecture for the computational
analysis of research limitations. Specifically, we create a dataset of
limitations in ACL, NeurIPS, and PeerJ papers by extracting them from papers'
text and integrating them with external reviews; we propose methods to
automatically generate them using a novel Retrieval Augmented Generation (RAG)
technique; we create a fine-grained evaluation framework for generated
limitations; and we provide a meta-evaluation for the proposed evaluation
techniques.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [506] [Community Moderation and the New Epistemology of Fact Checking on Social Media](https://arxiv.org/abs/2505.20067)
*Isabelle Augenstein,Michiel Bakker,Tanmoy Chakraborty,David Corney,Emilio Ferrara,Iryna Gurevych,Scott Hale,Eduard Hovy,Heng Ji,Irene Larraz,Filippo Menczer,Preslav Nakov,Paolo Papotti,Dhruv Sahnan,Greta Warren,Giovanni Zagni*

Main category: cs.SI

TL;DR: Social media platforms are moving from traditional moderation methods to community-driven content moderation like Community Notes. While crowd-sourced fact-checking has potential for combating misinformation with scale and speed, it is more complex than spam detection due to personal biases and cultural contexts. Therefore, professional fact-checkers remain indispensable.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the shift in social media platforms' approach to content moderation, particularly the move towards community-driven moderation systems such as Community Notes, and to evaluate their effectiveness and challenges compared to traditional methods.

Method: The method involves a systemic examination of current approaches to misinformation detection across major platforms, exploring the emerging role of community-driven moderation, and critically evaluating both the promises and challenges of large-scale crowd-checking initiatives.

Result: The result highlights that while community-driven moderation holds promise for increasing the scale and speed of combating misinformation, it is inherently complex due to factors like personal biases, political leanings, and cultural contexts. Thus, it cannot fully replace professional fact-checkers.

Conclusion: Community-driven moderation can be a valuable addition to combat misinformation on social media platforms; however, it should complement rather than replace professional fact-checking efforts due to the complexity involved in defining and identifying misleading content.

Abstract: Social media platforms have traditionally relied on internal moderation teams
and partnerships with independent fact-checking organizations to identify and
flag misleading content. Recently, however, platforms including X (formerly
Twitter) and Meta have shifted towards community-driven content moderation by
launching their own versions of crowd-sourced fact-checking -- Community Notes.
If effectively scaled and governed, such crowd-checking initiatives have the
potential to combat misinformation with increased scale and speed as
successfully as community-driven efforts once did with spam. Nevertheless,
general content moderation, especially for misinformation, is inherently more
complex. Public perceptions of truth are often shaped by personal biases,
political leanings, and cultural contexts, complicating consensus on what
constitutes misleading content. This suggests that community efforts, while
valuable, cannot replace the indispensable role of professional fact-checkers.
Here we systemically examine the current approaches to misinformation detection
across major platforms, explore the emerging role of community-driven
moderation, and critically evaluate both the promises and challenges of
crowd-checking at scale.

</details>


### [507] [Homophily Enhanced Graph Domain Adaptation](https://arxiv.org/abs/2505.20089)
*Ruiyi Fang,Bingheng Li,Jingyu Zhao,Ruizhi Pu,Qiuhao Zeng,Gezheng Xu,Charles Ling,Boyu Wang*

Main category: cs.SI

TL;DR: Graph Domain Adaptation (GDA) addresses label scarcity by transferring knowledge from labeled source graphs to unlabeled target graphs. This paper highlights the importance of graph homophily in GDA, revealing that homophily discrepancies degrade GDA performance. To solve this problem, a novel homophily alignment algorithm using mixed filters is proposed.


<details>
  <summary>Details</summary>
Motivation: Existing GDA approaches overlook the significance of graph homophily, which is a crucial factor for graph domain alignment. Homophily discrepancies are found to degrade GDA performance from both empirical and theoretical aspects.

Method: The paper proposes a novel homophily alignment algorithm that uses mixed filters to smooth graph signals. This method aims to capture and mitigate homophily discrepancies between graphs.

Result: Experimental results on various benchmarks confirm the effectiveness of the proposed method in improving GDA performance by aligning homophily.

Conclusion: Aligning homophily is essential for enhancing Graph Domain Adaptation performance. The proposed homophily alignment algorithm effectively mitigates homophily discrepancies and improves GDA.

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs
to unlabeled target graphs, addressing the challenge of label scarcity. In this
paper, we highlight the significance of graph homophily, a pivotal factor for
graph domain alignment, which, however, has long been overlooked in existing
approaches. Specifically, our analysis first reveals that homophily
discrepancies exist in benchmarks. Moreover, we also show that homophily
discrepancies degrade GDA performance from both empirical and theoretical
aspects, which further underscores the importance of homophily alignment in
GDA. Inspired by this finding, we propose a novel homophily alignment algorithm
that employs mixed filters to smooth graph signals, thereby effectively
capturing and mitigating homophily discrepancies between graphs. Experimental
results on a variety of benchmarks verify the effectiveness of our method.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [508] [Co-evolutionary Dynamics of Attack and Defence in Cybersecurity](https://arxiv.org/abs/2505.19338)
*Adeela Bashir,Zia Ush Shamszaman,Zhao Song,The Anh Han*

Main category: cs.GT

TL;DR: 在数字景观不断演变的情况下，研究网络攻击和防御的动力学至关重要。本文使用进化博弈论（EGT）框架，调查网络空间中攻击与防御的演化动力学。通过数学分析和数值模拟，研究发现高强度防御系统显示出稳定性且攻击频率最小，而低强度防御环境则表现出不稳定性并容易受到攻击。此外，研究发现了五个平衡点，其中始终防御和攻击的策略对成为最可能的稳定状态。本研究理论成果与过往网络事件的实际数据相符，并展示了其跨学科影响。总的来说，基于EGT的自适应网络安全策略可以改善资源分配、增强系统弹性并降低网络攻击的整体风险。


<details>
  <summary>Details</summary>
Motivation: 随着数字化进程的发展，网络攻击和防御的动态研究变得越来越重要。为了理解这些动态变化，需要一个能够捕捉成本、潜在收益和成功防御概率等因素的框架。

Method: 采用进化博弈论（EGT）框架构建了一个两群体不对称游戏模型，涉及攻击者和防御者之间的互动。通过数学分析和数值模拟来研究系统的稳定性及不同策略的均衡状态。

Result: 研究表明，高防御强度的系统具有更高的稳定性，而低防御强度的系统更容易遭受攻击。同时，识别出五个平衡点，其中持续防御和攻击的策略组合是最可能的稳定状态。

Conclusion: 基于EGT的自适应网络安全策略有助于优化资源分配、提高系统弹性和减少网络攻击风险，从而推动安全数字生态系统的建设。

Abstract: In the evolving digital landscape, it is crucial to study the dynamics of
cyberattacks and defences. This study uses an Evolutionary Game Theory (EGT)
framework to investigate the evolutionary dynamics of attacks and defences in
cyberspace. We develop a two-population asymmetric game between attacker and
defender to capture the essential factors of costs, potential benefits, and the
probability of successful defences. Through mathematical analysis and numerical
simulations, we find that systems with high defence intensities show stability
with minimal attack frequencies, whereas low-defence environments show
instability, and are vulnerable to attacks. Furthermore, we find five
equilibria, where the strategy pair always defend and attack emerged as the
most likely stable state as cyber domain is characterised by a continuous
battle between defenders and attackers. Our theoretical findings align with
real-world data from past cyber incidents, demonstrating the interdisciplinary
impact, such as fraud detection, risk management and cybersecurity
decision-making. Overall, our analysis suggests that adaptive cybersecurity
strategies based on EGT can improve resource allocation, enhance system
resilience, and reduce the overall risk of cyberattacks. By incorporating
real-world data, this study demonstrates the applicability of EGT in addressing
the evolving nature of cyber threats and the need for secure digital ecosystems
through strategic planning and proactive defence measures.

</details>


### [509] [Efficient Algorithms for Electing Successive Committees](https://arxiv.org/abs/2505.18287)
*Pallavi Jain,Andrzej Kaczmarczyk*

Main category: cs.GT

TL;DR: The paper aims to solve hard cases in realistic scenarios of a moderate number of candidates or of a limited time horizon for the model of successive committee elections.


<details>
  <summary>Details</summary>
Motivation: To unlock the full potential of the described temporal model of committee elections.

Method: Devising (parameterized) algorithms that effectively solve the mentioned hard cases in realistic scenarios.

Result: Proposed algorithms can solve the hard cases in realistic scenarios of a moderate number of candidates or of a limited time horizon.

Conclusion: The devised algorithms improve the practical usability of the model of successive committee elections.

Abstract: In a recently introduced model of successive committee elections (Bredereck
et al., AAAI-20) for a given set of ordinal or approval preferences one aims to
find a sequence of a given length of "best" same-size committees such that each
candidate is a member of a limited number of consecutive committees. However,
the practical usability of this model remains limited, as the described task
turns out to be NP-hard for most selection criteria already for seeking
committees of size three. Non-trivial or somewhat efficient algorithms for
these cases are lacking too. Motivated by a desire to unlock the full potential
of the described temporal model of committee elections, we devise
(parameterized) algorithms that effectively solve the mentioned hard cases in
realistic scenarios of a moderate number of candidates or of a limited time
horizon.

</details>


### [510] [Incentivizing High-Quality Human Annotations with Golden Questions](https://arxiv.org/abs/2505.19134)
*Shang Liu,Zhongze Cai,Hanzhao Wang,Zhongyao Ma,Xiaocheng Li*

Main category: cs.GT

TL;DR: 研究了如何激励人工标注者生成高质量数据，通过主代理模型和最大似然估计方法设计奖励机制，并提出黄金问题的两个标准：高确定性和与普通问题类似格式。实验表明，相比传统方法，这种方法更能揭示标注者的行为。


<details>
  <summary>Details</summary>
Motivation: 在训练大语言模型中，人工标注数据至关重要，但付费的人工标注者不一定能生成高质量的数据，因此需要研究如何激励他们提高数据质量。

Method: 采用主代理模型来模拟公司和标注者之间的动态关系，通过检查n个样本来监控标注质量；利用最大似然估计器及其假设检验来激励标注者，若MLE通过测试则给予奖励；分析结果方差以理解代理人的战略性行为对假设检验的影响。

Result: 理论分析表明，代理人战略性行为使得假设检验率不同于传统的大偏差理论指数速率，而是为$\Theta(1/\sqrt{n \log n})$；提出了黄金问题的两个标准：高确定性和与普通问题类似的格式，实验证明相比传统调查技术，这种方法更能揭示标注者的行为。

Conclusion: 通过主代理模型和MLE方法可以有效激励标注者生成高质量数据，黄金问题的选择对于监控标注者表现非常重要，其标准应满足高确定性和与普通问题类似格式。

Abstract: Human-annotated data plays a vital role in training large language models
(LLMs), such as supervised fine-tuning and human preference alignment. However,
it is not guaranteed that paid human annotators produce high-quality data. In
this paper, we study how to incentivize human annotators to do so. We start
from a principal-agent model to model the dynamics between the company (the
principal) and the annotator (the agent), where the principal can only monitor
the annotation quality by examining $n$ samples. We investigate the maximum
likelihood estimators (MLE) and the corresponding hypothesis testing to
incentivize annotators: the agent is given a bonus if the MLE passes the test.
By analyzing the variance of the outcome, we show that the strategic behavior
of the agent makes the hypothesis testing very different from traditional ones:
Unlike the exponential rate proved by the large deviation theory, the
principal-agent model's hypothesis testing rate is of $\Theta(1/\sqrt{n \log
n})$. Our theory implies two criteria for the \emph{golden questions} to
monitor the performance of the annotators: they should be of (1) high certainty
and (2) similar format to normal ones. In that light, we select a set of golden
questions in human preference data. By doing incentive-compatible experiments,
we find out that the annotators' behavior is better revealed by those golden
questions, compared to traditional survey techniques such as instructed
manipulation checks.

</details>


### [511] [Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games](https://arxiv.org/abs/2505.19537)
*Yi Feng,Kaito Fujii,Stratis Skoulakis,Xiao Wang,Volkan Cevher*

Main category: cs.GT

TL;DR: 在极小化极大游戏中，较小的动量通过允许更广泛的步长范围内的局部收敛来增强算法稳定性，并引导算法轨迹向损失景观中较浅斜率区域移动，这与极小化中的现象不同。


<details>
  <summary>Details</summary>
Motivation: 尽管Polyak的开创性工作后，重球（HB）动量在极小化问题中被广泛研究，但在极小化极大游戏中的作用却鲜有探索。作为实际极小化极大算法（如Adam）的关键组成部分，这一差距限制了它们的有效性。

Method: 作者提供了HB在极小化极大游戏中的连续时间分析，考虑了同时和交替更新方案。局部上，证明了较小的动量通过使算法在更广泛的步长范围内收敛来提高稳定性，而交替更新通常收敛更快。全局上，研究了HB的隐式正则化效应，发现较小的动量引导算法轨迹朝向损失景观中较浅斜率的区域，交替更新放大了这种效果。

Result: 所有这些现象与极小化中的观察结果不同，在极小化中较大的动量会产生类似的效果。理论结果揭示了极小化极大游戏和极小化之间HB的根本差异。数值实验进一步验证了这些理论结果。

Conclusion: 本文的研究揭示了极小化极大游戏和极小化中HB的基本差异，并通过数值实验验证了理论结果。

Abstract: Since Polyak's pioneering work, heavy ball (HB) momentum has been widely
studied in minimization. However, its role in min-max games remains largely
unexplored. As a key component of practical min-max algorithms like Adam, this
gap limits their effectiveness. In this paper, we present a continuous-time
analysis for HB with simultaneous and alternating update schemes in min-max
games. Locally, we prove smaller momentum enhances algorithmic stability by
enabling local convergence across a wider range of step sizes, with alternating
updates generally converging faster. Globally, we study the implicit
regularization of HB, and find smaller momentum guides algorithms trajectories
towards shallower slope regions of the loss landscapes, with alternating
updates amplifying this effect. Surprisingly, all these phenomena differ from
those observed in minimization, where larger momentum yields similar effects.
Our results reveal fundamental differences between HB in min-max games and
minimization, and numerical experiments further validate our theoretical
results.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [512] [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)
*Yan Wen,Junfeng Guo,Heng Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为CoTGuard的版权保护框架，通过在链式思维（CoT）推理中嵌入触发查询，实现对多智能体LLM系统中未经授权的内容复制进行细粒度、可解释的检测。实验表明，该方法能有效发现内容泄露，同时对任务性能干扰较小。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）发展为能够协作推理和执行任务的自主代理，多代理LLM系统已成为解决复杂问题的强大范式。然而，这些系统在版权保护方面提出了新挑战，特别是在代理间通信和推理过程中无意中回忆起敏感或受版权保护的内容时。现有的保护技术主要关注最终输出中的内容检测，而忽略了代理内部更丰富、更具揭示性的推理过程。

Method: 引入了CoTGuard框架，利用基于触发的检测方法，在链式思维（CoT）推理中嵌入特定触发查询到代理提示中，激活特定的CoT片段并监控中间推理步骤，以检测未经授权的内容复制。这种方法实现了对合作代理场景中版权侵权的细粒度、可解释的检测。

Result: 在各种基准上的广泛实验评估表明，CoTGuard可以有效地发现内容泄露，同时对任务性能的干扰最小。

Conclusion: 研究结果表明，推理级别的监控为在基于LLM的代理系统中保护知识产权提供了一个有希望的方向。

Abstract: As large language models (LLMs) evolve into autonomous agents capable of
collaborative reasoning and task execution, multi-agent LLM systems have
emerged as a powerful paradigm for solving complex problems. However, these
systems pose new challenges for copyright protection, particularly when
sensitive or copyrighted content is inadvertently recalled through inter-agent
communication and reasoning. Existing protection techniques primarily focus on
detecting content in final outputs, overlooking the richer, more revealing
reasoning processes within the agents themselves. In this paper, we introduce
CoTGuard, a novel framework for copyright protection that leverages
trigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,
we can activate specific CoT segments and monitor intermediate reasoning steps
for unauthorized content reproduction by embedding specific trigger queries
into agent prompts. This approach enables fine-grained, interpretable detection
of copyright violations in collaborative agent scenarios. We evaluate CoTGuard
on various benchmarks in extensive experiments and show that it effectively
uncovers content leakage with minimal interference to task performance. Our
findings suggest that reasoning-level monitoring offers a promising direction
for safeguarding intellectual property in LLM-based agent systems.

</details>


### [513] [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)
*Sangyeop Kim,Yohan Lee,Yongwoo Song,Kimin Lee*

Main category: cs.CL

TL;DR: The study explores long-context vulnerabilities in Large Language Models (LLMs) via Many-Shot Jailbreaking (MSJ), revealing that context length is the main factor affecting attack success, and even random text can bypass safety measures, stressing the need for new safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand and expose the vulnerabilities of LLMs when handling long contexts, using many-shot jailbreaking techniques.

Method: Conducting experiments with context lengths up to 128K tokens, analyzing various many-shot attack settings including different instruction styles, shot density, topics, and formats.

Result: Found that context length significantly impacts attack effectiveness; successful attacks don't require meticulously crafted harmful content as repetitive or random text can bypass safety protocols.

Conclusion: There are substantial safety concerns with LLMs' long-context processing capabilities, indicating a necessity for advanced safety mechanisms.

Abstract: We investigate long-context vulnerabilities in Large Language Models (LLMs)
through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of
up to 128K tokens. Through comprehensive analysis with various many-shot attack
settings with different instruction styles, shot density, topic, and format, we
reveal that context length is the primary factor determining attack
effectiveness. Critically, we find that successful attacks do not require
carefully crafted harmful content. Even repetitive shots or random dummy text
can circumvent model safety measures, suggesting fundamental limitations in
long-context processing capabilities of LLMs. The safety behavior of
well-aligned models becomes increasingly inconsistent with longer contexts.
These findings highlight significant safety gaps in context expansion
capabilities of LLMs, emphasizing the need for new safety mechanisms.

</details>


### [514] [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)
*Dominik Meier,Jan Philip Wahle,Paul Röttger,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: The paper introduces TrojanStego, a threat model where adversaries fine-tune LLMs to leak sensitive information covertly through linguistic steganography. Experiments show compromised models can transmit 32-bit secrets with high accuracy while maintaining utility and evading detection.


<details>
  <summary>Details</summary>
Motivation: To address concerns about LLMs leaking confidential information when integrated into sensitive workflows.

Method: Proposed TrojanStego uses fine-tuning to embed sensitive context info into outputs via linguistic steganography without explicit input control; includes a taxonomy for risk factors and an encoding scheme based on vocabulary partitioning.

Result: Compromised models transmitted 32-bit secrets with 87% accuracy (97% with majority voting), maintained utility, evaded human detection, and preserved coherence.

Conclusion: Demonstrates a new class of passive, covert, practical, and dangerous LLM data exfiltration attacks.

Abstract: As large language models (LLMs) become integrated into sensitive workflows,
concerns grow over their potential to leak confidential information. We propose
TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to
embed sensitive context information into natural-looking outputs via linguistic
steganography, without requiring explicit control over inference inputs. We
introduce a taxonomy outlining risk factors for compromised LLMs, and use it to
evaluate the risk profile of the threat. To implement TrojanStego, we propose a
practical encoding scheme based on vocabulary partitioning learnable by LLMs
via fine-tuning. Experimental results show that compromised models reliably
transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over
97% accuracy using majority voting across three generations. Further, they
maintain high utility, can evade human detection, and preserve coherence. These
results highlight a new class of LLM data exfiltration attacks that are
passive, covert, practical, and dangerous.

</details>


### [515] [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)
*Junyan Zhang,Yiming Huang,Shuliang Liu,Yubo Gao,Xuming Hu*

Main category: cs.CL

TL;DR: 尽管LLM的快速普及，本研究通过系统比较BERT-like模型和LLM在六个高难度数据集上的表现，发现BERT-like模型在某些任务上优于LLM，并据此提出了一个细粒度的任务选择策略TaMAS。


<details>
  <summary>Details</summary>
Motivation: 挑战当前流行的“以LLM为中心”的趋势，重新审视传统BERT-like模型在文本分类中的潜力。

Method: 系统地比较了三类方法：BERT-like模型微调、利用LLM内部状态和零样本推理；对数据集进行分类并执行PCA和探测实验，识别不同任务中模型的优势。

Result: BERT-like模型通常在模式驱动的任务中表现更好，而LLM在需要深层语义或世界知识的任务中占优。

Conclusion: 提出TaMAS，一个基于任务特点的细粒度选择策略，提倡根据具体任务需求选择合适的模型，而非一味依赖LLM。

Abstract: The rapid adoption of LLMs has overshadowed the potential advantages of
traditional BERT-like models in text classification. This study challenges the
prevailing "LLM-centric" trend by systematically comparing three category
methods, i.e., BERT-like models fine-tuning, LLM internal state utilization,
and zero-shot inference across six high-difficulty datasets. Our findings
reveal that BERT-like models often outperform LLMs. We further categorize
datasets into three types, perform PCA and probing experiments, and identify
task-specific model strengths: BERT-like models excel in pattern-driven tasks,
while LLMs dominate those requiring deep semantics or world knowledge. Based on
this, we propose TaMAS, a fine-grained task selection strategy, advocating for
a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.

</details>


### [516] [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)
*Shuhang Xu,Fangwei Zhong*

Main category: cs.CL

TL;DR: CoMet is a framework that improves LLM-based agents' ability to process metaphors, boosting strategic communication in multi-agent games.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM-based agents' capacity for covert communication and semantic evasion through metaphor interpretation and application.

Method: Introduce CoMet, which integrates a hypothesis-based metaphor reasoner and a self-reflective metaphor generator to improve metaphor processing.

Result: CoMet significantly enhances the agents' strategic communication abilities in the games Undercover and Adversarial Taboo.

Conclusion: CoMet successfully boosts agents' metaphor processing capabilities, leading to more effective covert communication and semantic evasion.

Abstract: Metaphors are a crucial way for humans to express complex or subtle ideas by
comparing one concept to another, often from a different domain. However, many
large language models (LLMs) struggle to interpret and apply metaphors in
multi-agent language games, hindering their ability to engage in covert
communication and semantic evasion, which are crucial for strategic
communication. To address this challenge, we introduce CoMet, a framework that
enables LLM-based agents to engage in metaphor processing. CoMet combines a
hypothesis-based metaphor reasoner with a metaphor generator that improves
through self-reflection and knowledge integration. This enhances the agents'
ability to interpret and apply metaphors, improving the strategic and nuanced
quality of their interactions. We evaluate CoMet on two multi-agent language
games - Undercover and Adversarial Taboo - which emphasize Covert Communication
and Semantic Evasion. Experimental results demonstrate that CoMet significantly
enhances the agents' ability to communicate strategically using metaphors.

</details>


### [517] [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
*Hanyu Li,Haoyu Liu,Tingyu Zhu,Tianyu Guo,Zeyu Zheng,Xiaotie Deng,Michael I. Jordan*

Main category: cs.CL

TL;DR: IDA-Bench is a new benchmark for evaluating LLMs' multi-round interactive data analysis capabilities, revealing limitations in current models and highlighting the need for improvement.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs as data analysis agents do not adequately address the iterative nature of real-world data analysis tasks.

Method: Introduced IDA-Bench, a benchmark with tasks derived from complex Kaggle notebooks, presented as sequential natural language instructions by an LLM-simulated user. Agent performance is judged by comparing its final numerical output to the human-derived baseline.

Result: Initial results indicate that even state-of-the-art coding agents succeed on less than 50% of the tasks, emphasizing limitations not evident in single-turn tests.

Conclusion: There is a need to improve LLMs' multi-round capabilities for building more reliable data analysis agents, balancing instruction following and reasoning.

Abstract: Large Language Models (LLMs) show promise as data analysis agents, but
existing benchmarks overlook the iterative nature of the field, where experts'
decisions evolve with deeper insights of the dataset. To address this, we
introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round
interactive scenarios. Derived from complex Kaggle notebooks, tasks are
presented as sequential natural language instructions by an LLM-simulated user.
Agent performance is judged by comparing its final numerical output to the
human-derived baseline. Initial results show that even state-of-the-art coding
agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting
limitations not evident in single-turn tests. This work underscores the need to
improve LLMs' multi-round capabilities for building more reliable data analysis
agents, highlighting the necessity of achieving a balance between instruction
following and reasoning.

</details>


### [518] [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)
*Xixian Yong,Xiao Zhou,Yingying Zhang,Jinlin Li,Yefeng Zheng,Xian Wu*

Main category: cs.CL

TL;DR: The paper explores the trade-off between reasoning length and semantic efficiency in Large Reasoning Models (LRMs), introduces two metrics (InfoBias and InfoGain) to evaluate reasoning processes, and proposes an entropy-based Adaptive Think strategy that improves accuracy and reduces token usage.


<details>
  <summary>Details</summary>
Motivation: LRMs have improved multi-step reasoning but generate excessively long chains. There is a need to understand and optimize the balance between reasoning length and semantic efficiency.

Method: Proposed two metrics: InfoBias for divergence from ideal reasoning paths and InfoGain for stepwise information contribution. Introduced an entropy-based Adaptive Think strategy that dynamically halts reasoning when confidence is high enough.

Result: Adaptive Think resulted in 1.10% improvement in average accuracy and 50.80% reduction in token usage on QwQ-32B across six benchmark tasks compared to Vanilla Think approach.

Conclusion: Entropy-based methods show promise in enhancing accuracy and cost-efficiency in deploying large language models.

Abstract: The recent rise of Large Reasoning Models (LRMs) has significantly improved
multi-step reasoning performance, but often at the cost of generating
excessively long reasoning chains. This paper revisits the efficiency of such
reasoning processes through an information-theoretic lens, revealing a
fundamental trade-off between reasoning length and semantic efficiency. We
propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal
reasoning paths and stepwise information contribution, respectively. Empirical
analyses show that longer reasoning chains tend to exhibit higher information
bias and diminishing information gain, especially for incorrect answers.
Motivated by these findings, we introduce an entropy-based Adaptive Think
strategy that dynamically halts reasoning once confidence is sufficiently high,
improving efficiency while maintaining competitive accuracy. Compared to the
Vanilla Think approach (default mode), our strategy yields a 1.10% improvement
in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six
benchmark tasks spanning diverse reasoning types and difficulty levels,
demonstrating superior efficiency and reasoning performance. These results
underscore the promise of entropy-based methods for enhancing both accuracy and
cost-effiiciency in large language model deployment.

</details>


### [519] [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)
*Ananth Muppidi,Tarak Das,Sambaran Bandyopadhyay,Tripti Shukla,Dharun D A*

Main category: cs.CL

TL;DR: In the era of generative AI, automatic presentation slide generation is crucial. This paper evaluates multimodal content in slides that summarize documents and convey concepts. A benchmark dataset RefSlides is introduced, along with REFLEX, an evaluation approach using fine-tuned LLMs to generate scores and feedback without needing ground truth presentations. Experiments show REFLEX outperforms other methods.


<details>
  <summary>Details</summary>
Motivation: To effectively summarize documents and convey concepts to a broad audience through presentation slides, it is necessary to evaluate multimodal content in these slides.

Method: Introduced RefSlides, a benchmark dataset of high-quality human-made presentations across various topics. Proposed REFLEX, an evaluation approach which uses metrics to characterize presentation content, generates negative samples with metric-specific perturbations to fine-tune LLMs, enabling reference-free evaluation.

Result: Extensive automated and human experiments indicate that REFLEX surpasses classical heuristic-based and state-of-the-art large language model-based evaluations in generating scores and explanations.

Conclusion: REFLEX provides a novel way to evaluate presentation slides without ground truth, outperforming existing methods.

Abstract: The generation of presentation slides automatically is an important problem
in the era of generative AI. This paper focuses on evaluating multimodal
content in presentation slides that can effectively summarize a document and
convey concepts to a broad audience. We introduce a benchmark dataset,
RefSlides, consisting of human-made high-quality presentations that span
various topics. Next, we propose a set of metrics to characterize different
intrinsic properties of the content of a presentation and present REFLEX, an
evaluation approach that generates scores and actionable feedback for these
metrics. We achieve this by generating negative presentation samples with
different degrees of metric-specific perturbations and use them to fine-tune
LLMs. This reference-free evaluation technique does not require ground truth
presentations during inference. Our extensive automated and human experiments
demonstrate that our evaluation approach outperforms classical heuristic-based
and state-of-the-art large language model-based evaluations in generating
scores and explanations.

</details>


### [520] [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
*Yukin Zhang,Qi Dong*

Main category: cs.CL

TL;DR: Large Transformer based language models are powerful but hard to understand. This paper introduces MSPGT, a framework that breaks down text generation into three levels: global context, intermediate structure, and local word choices. It provides methods to identify these scales in different Transformer architectures and shows how manipulating each scale affects different aspects of generated text.


<details>
  <summary>Details</summary>
Motivation: To better understand how large Transformer-based language models plan, structure, and realize text by breaking down the generation process into interpretable components.

Method: Developed MSPGT which factorizes text generation into three semantic scales and aligns them with specific layer ranges in Transformer architectures. Used two metrics (attention span thresholds and inter-layer mutual information peaks) to identify scale boundaries.

Result: The metrics produced stable partitions across four representative models, revealing differences in layer allocation between decoder-only and encoder-only models. Targeted interventions showed statistically significant effects on different aspects of text generation at each scale.

Conclusion: MSPGT provides a unified method for interpreting, diagnosing, and controlling large language models, bridging the gap between mechanistic interpretability and emergent capabilities.

Abstract: Large Transformer based language models achieve remarkable performance but
remain opaque in how they plan, structure, and realize text. We introduce
Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework
that factorizes generation into three semantic scales_global context,
intermediate structure, and local word choices and aligns each scale with
specific layer ranges in Transformer architectures. To identify scale
boundaries, we propose two complementary metrics: attention span thresholds and
inter layer mutual information peaks. Across four representative models (GPT-2,
BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global
partitions, corroborated by probing tasks and causal interventions. We find
that decoder_only models allocate more layers to intermediate and global
processing while encoder_only models emphasize local feature extraction.
Through targeted interventions, we demonstrate that local scale manipulations
primarily influence lexical diversity, intermediate-scale modifications affect
sentence structure and length, and global_scale perturbations impact discourse
coherence all with statistically significant effects. MSPGT thus offers a
unified, architecture-agnostic method for interpreting, diagnosing, and
controlling large language models, bridging the gap between mechanistic
interpretability and emergent capabilities.

</details>


### [521] [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)
*Kunal Sawarkar,Shivam R. Solanki,Abhilasha Mangal*

Main category: cs.CL

TL;DR: 尽管检索增强生成（RAG）被广泛研究，但在企业特定数据集中的应用受限于答案准确性差的问题。本文提出了一种名为'MetaGen Blended RAG'的方法，通过混合查询索引和元数据丰富来增强检索器以处理领域特定语料库。此方法在PubMedQA基准测试中实现了82%的检索准确率和77%的RAG准确率，无需微调且优于更大的模型如GPT3.5，并在其他数据集上也展示了其鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在处理企业内部领域特定数据时，由于这些数据包含复杂的术语且语义变化大，导致上下文精度低、回答准确性差。此外，当前通过微调或RAG加微调的方法速度慢、成本高且缺乏对新领域数据的泛化能力。

Method: 提出了一种名为'MetaGen Blended RAG'的方法，构建了一个使用关键概念、主题和缩写的元数据生成管道，然后创建了一个元数据丰富的混合索引以增强搜索查询。该方法避免了过拟合并在不同领域间有效泛化。

Result: 在PubMedQA基准测试中，该方法达到了82%的检索准确率和77%的RAG准确率，超越了之前所有无微调的RAG准确率结果并设置了新的零样本基准，同时性能可与最佳微调模型媲美。在其他问答数据集（如SQuAD、NQ等）上的评估进一步证明了该方法的鲁棒性和可扩展性。

Conclusion: MetaGen Blended RAG方法提供了一种有效的解决方案，用于提高企业特定领域数据集上的RAG系统的性能，无需微调且能良好泛化到新领域数据。此方法不仅在生物医学领域取得了显著成果，还在其他问答任务中表现出了良好的适应性。

Abstract: Despite the widespread exploration of Retrieval-Augmented Generation (RAG),
its deployment in enterprises for domain-specific datasets remains limited due
to poor answer accuracy. These corpora, often shielded behind firewalls in
private enterprise knowledge bases, having complex, domain-specific
terminology, rarely seen by LLMs during pre-training; exhibit significant
semantic variability across domains (like networking, military, or legal,
etc.), or even within a single domain like medicine, and thus result in poor
context precision for RAG systems. Currently, in such situations, fine-tuning
or RAG with fine-tuning is attempted, but these approaches are slow, expensive,
and lack generalization for accuracy as the new domain-specific data emerges.
We propose an approach for Enterprise Search that focuses on enhancing the
retriever for a domain-specific corpus through hybrid query indexes and
metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata
generation pipeline using key concepts, topics, and acronyms, and then creates
a metadata-enriched hybrid index with boosted search queries. This approach
avoids overfitting and generalizes effectively across domains. On the PubMedQA
benchmark for the biomedical domain, the proposed method achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results
without fine-tuning and sets a new benchmark for zero-shot results while
outperforming much larger models like GPT3.5. The results are even comparable
to the best fine-tuned models on this dataset, and we further demonstrate the
robustness and scalability of the approach by evaluating it on other Q&A
datasets like SQuAD, NQ etc.

</details>


### [522] [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)
*Jianghao Wu,Feilong Tang,Yulong Li,Ming Hu,Haochen Xue,Shoaib Jameel,Yutong Xie,Imran Razzak*

Main category: cs.CL

TL;DR: An abstract about a new test-time framework called TAGS that improves zero-shot medical reasoning performance without fine-tuning or parameter updates.


<details>
  <summary>Details</summary>
Motivation: Prompting-based methods for large language models in zero-shot medical reasoning are shallow and unstable, while fine-tuned medical LLMs have poor generalization and limited adaptability.

Method: The TAGS framework combines a broadly capable generalist with a domain-specific specialist, using two auxiliary modules: a hierarchical retrieval mechanism and a reliability scorer.

Result: TAGS achieves strong performance across nine MedQA benchmarks, significantly boosting accuracy of various models without any parameter updates.

Conclusion: TAGS offers a promising approach to enhance zero-shot medical reasoning without the need for model fine-tuning or parameter updates.

Abstract: Recent advances such as Chain-of-Thought prompting have significantly
improved large language models (LLMs) in zero-shot medical reasoning. However,
prompting-based methods often remain shallow and unstable, while fine-tuned
medical LLMs suffer from poor generalization under distribution shifts and
limited adaptability to unseen clinical scenarios. To address these
limitations, we present TAGS, a test-time framework that combines a broadly
capable generalist with a domain-specific specialist to offer complementary
perspectives without any model fine-tuning or parameter updates. To support
this generalist-specialist reasoning process, we introduce two auxiliary
modules: a hierarchical retrieval mechanism that provides multi-scale exemplars
by selecting examples based on both semantic and rationale-level similarity,
and a reliability scorer that evaluates reasoning consistency to guide final
answer aggregation. TAGS achieves strong performance across nine MedQA
benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and
improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several
fine-tuned medical LLMs, without any parameter updates. The code will be
available at https://github.com/JianghaoWu/TAGS.

</details>


### [523] [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)
*Zhuozhuo Joy Liu,Farhan Samir,Mehar Bhatia,Laura K. Nelson,Vered Shwartz*

Main category: cs.CL

TL;DR: 研究人员通过让LLM对不同文化的叙事中的文化规范进行推理，发现GPT-4生成的规范虽然不一定是错误的，但显著缺乏文化特异性。尽管模型避免了直接生成刻板印象，但某些文化的刻板印象只是被隐藏而非消除，且容易被恢复。


<details>
  <summary>Details</summary>
Motivation: 研究者们希望探索大型语言模型（LLMs）在实际场景中是否能一致地应用其所展现出的价值观，并更深入地了解LLMs如何处理不同文化的叙事及其中的文化规范。

Method: 采用自下而上的方法，让LLMs对来自不同文化的叙事中的文化规范进行推理。通过观察LLMs生成的规范，分析其文化特异性和是否存在隐藏的刻板印象。

Result: GPT-4生成的文化规范虽然不一定错误，但显著缺乏文化特异性；尽管模型避免了直接生成刻板印象，但某些文化的刻板印象只是被隐藏而非消除，且容易被恢复。

Conclusion: 解决这些挑战对于开发公平服务于多元用户群体的LLMs是至关重要的一步。

Abstract: LLMs have been demonstrated to align with the values of Western or North
American cultures. Prior work predominantly showed this effect through
leveraging surveys that directly ask (originally people and now also LLMs)
about their values. However, it is hard to believe that LLMs would consistently
apply those values in real-world scenarios. To address that, we take a
bottom-up approach, asking LLMs to reason about cultural norms in narratives
from different cultures. We find that GPT-4 tends to generate norms that, while
not necessarily incorrect, are significantly less culture-specific. In
addition, while it avoids overtly generating stereotypes, the stereotypical
representations of certain cultures are merely hidden rather than suppressed in
the model, and such stereotypes can be easily recovered. Addressing these
challenges is a crucial step towards developing LLMs that fairly serve their
diverse user base.

</details>


### [524] [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)
*Naghmeh Jamali,Milad Mohammadi,Danial Baledi,Zahra Rezvani,Hesham Faili*

Main category: cs.CL

TL;DR: The paper introduces PerMedCQA, the first Persian-language benchmark for evaluating LLMs on consumer-generated medical questions. It contains 68,138 QA pairs and uses MedJudge for model evaluation, revealing challenges in multilingual medical QA.


<details>
  <summary>Details</summary>
Motivation: To address the lack of consumer-oriented, multilingual resources in low-resource languages like Persian for medical question answering.

Method: Curated a dataset named PerMedCQA from a large medical QA forum with 68,138 question-answer pairs after cleaning raw data. Evaluated state-of-the-art multilingual LLMs using MedJudge, an LLM grader-based evaluation framework validated by human annotators.

Result: Showcased significant challenges in multilingual medical QA through evaluations, providing insights for improving medical assistance systems.

Conclusion: PerMedCQA bridges the gap for Persian medical QA and highlights the need for more accurate and context-aware systems.

Abstract: Medical consumer question answering (CQA) is crucial for empowering patients
by providing personalized and reliable health information. Despite recent
advances in large language models (LLMs) for medical QA, consumer-oriented and
multilingual resources, particularly in low-resource languages like Persian,
remain sparse. To bridge this gap, we present PerMedCQA, the first
Persian-language benchmark for evaluating LLMs on real-world,
consumer-generated medical questions. Curated from a large medical QA forum,
PerMedCQA contains 68,138 question-answer pairs, refined through careful data
cleaning from an initial set of 87,780 raw entries. We evaluate several
state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a
novel rubric-based evaluation framework driven by an LLM grader, validated
against expert human annotators. Our results highlight key challenges in
multilingual medical QA and provide valuable insights for developing more
accurate and context-aware medical assistance systems. The data is publicly
available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA

</details>


### [525] [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
*Lucas Bandarkar,Nanyun Peng*

Main category: cs.CL

TL;DR: Large language models still have difficulties with tasks in low-resource languages. This study explores cross-lingual transfer methods for these languages when task-specific post-training data is limited. The researchers found that the model parameters important for mathematical reasoning and multilingual capabilities do not overlap. They developed modular frameworks to improve fine-tuning by freezing parameters or merging models post hoc. Without in-language math data, these approaches improved upon baselines across different languages, models, and fine-tuning paradigms. The most consistently successful method involved fine-tuning separate language and math experts and merging via Layer-Swapping.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by large language models in handling tasks in low-resource languages where task-specific post-training data is scarce.

Method: The study first validates that the subsets of model parameters important for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. Then, numerous modular frameworks are developed and analyzed to improve the composition during fine-tuning, using methods like freezing parameters or post hoc model merging.

Result: Modular approaches successfully improved upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). The most consistently successful modular method was fine-tuning separate language and math experts and model merging via Layer-Swapping.

Conclusion: The research concludes that modular approaches can effectively improve performance in low-resource languages without in-language math data, with Layer-Swapping being a particularly successful method.

Abstract: Large language models (LLMs) still struggle across tasks outside of
high-resource languages. In this work, we investigate cross-lingual transfer to
lower-resource languages where task-specific post-training data is scarce.
Building on prior work, we first validate that the subsets of model parameters
that matter most for mathematical reasoning and multilingual capabilities are
distinctly non-overlapping. To exploit this implicit separability between task
and target language parameterization, we develop and analyze numerous modular
frameworks to improve the composition of the two during fine-tuning. These
methods generally employ freezing parameters or post hoc model merging to
assign math and language improvement to different key parts of the LLM. In the
absence of in-language math data, we demonstrate that the modular approaches
successfully improve upon baselines across three languages, four models, and
two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most
consistently successful modular method to be fine-tuning separate language and
math experts and model merging via Layer-Swapping, somewhat surprisingly. We
offer possible explanations for this result via recent works on the linearity
of task vectors. We further explain this by empirically showing that reverting
less useful fine-tuning updates after training often outperforms freezing them
from the start.

</details>


### [526] [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)
*AmirHossein Safdarian,Milad Mohammadi,Ehsan Jahanbakhsh,Mona Shahamat Naderi,Heshaam Faili*

Main category: cs.CL

TL;DR: A zero-shot, training-free schema linking approach constructs a schema graph based on foreign key relations and uses a single prompt to extract source and destination tables from the user query. Then classical path-finding algorithms and post-processing are applied to identify the optimal sequence of tables and columns for SQL queries.


<details>
  <summary>Details</summary>
Motivation: Schema linking remains a critical component in Text-to-SQL systems, reducing prompt size for models with narrow context windows and sharpening model focus even when the entire schema fits.

Method: The method first constructs a schema graph based on foreign key relations, then uses a single prompt to extract source and destination tables from the user query, followed by applying classical path-finding algorithms and post-processing to identify the optimal sequence of tables and columns that should be joined.

Result: This simple, cost-effective, and highly scalable method achieves state-of-the-art results on the BIRD benchmark, outperforming previous specialized, fine-tuned, and complex multi-step LLM-based approaches.

Conclusion: The authors conducted detailed ablation studies to examine the precision-recall trade-off in their framework and evaluated the execution accuracy of their schema filtering method compared to other approaches across various model sizes.

Abstract: Text-to-SQL systems translate natural language questions into executable SQL
queries, and recent progress with large language models (LLMs) has driven
substantial improvements in this task. Schema linking remains a critical
component in Text-to-SQL systems, reducing prompt size for models with narrow
context windows and sharpening model focus even when the entire schema fits. We
present a zero-shot, training-free schema linking approach that first
constructs a schema graph based on foreign key relations, then uses a single
prompt to Gemini 2.5 Flash to extract source and destination tables from the
user query, followed by applying classical path-finding algorithms and
post-processing to identify the optimal sequence of tables and columns that
should be joined, enabling the LLM to generate more accurate SQL queries.
Despite being simple, cost-effective, and highly scalable, our method achieves
state-of-the-art results on the BIRD benchmark, outperforming previous
specialized, fine-tuned, and complex multi-step LLM-based approaches. We
conduct detailed ablation studies to examine the precision-recall trade-off in
our framework. Additionally, we evaluate the execution accuracy of our schema
filtering method compared to other approaches across various model sizes.

</details>


### [527] [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)
*Khandakar Ashrafi Akbar,Md Nahiyan Uddin,Latifur Khan,Trayce Hockstad,Mizanur Rahman,Mashrur Chowdhury,Bhavani Thuraisingham*

Main category: cs.CL

TL;DR: This paper presents a RAG-based LLM framework to aid policymakers in addressing cybersecurity and data privacy challenges in connected and automated transportation systems. It surpasses commercial LLMs in four evaluation metrics, providing reliable legal insights.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the need for federal and state authorities to revise existing laws and develop new statutes to address emerging cybersecurity and data privacy challenges in evolving connected and automated transportation systems.

Method: The method involves using a Retrieval-Augmented Generation (RAG) based Large Language Model (LLM) framework. This framework extracts relevant legal content and generates accurate, inquiry-specific responses by reducing hallucinations in LLMs through a curated set of domain-specific questions guiding response generation. The system also incorporates retrieval mechanisms to enhance factual grounding and specificity of its outputs.

Result: The proposed RAG-based LLM outperforms leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore, BERTScore, and ROUGE, showing its effectiveness in producing reliable and context-aware legal insights.

Conclusion: This approach offers a scalable, AI-driven method for legislative analysis, supporting efforts to update legal frameworks in line with advancements in transportation technologies.

Abstract: As connected and automated transportation systems evolve, there is a growing
need for federal and state authorities to revise existing laws and develop new
statutes to address emerging cybersecurity and data privacy challenges. This
study introduces a Retrieval-Augmented Generation (RAG) based Large Language
Model (LLM) framework designed to support policymakers by extracting relevant
legal content and generating accurate, inquiry-specific responses. The
framework focuses on reducing hallucinations in LLMs by using a curated set of
domain-specific questions to guide response generation. By incorporating
retrieval mechanisms, the system enhances the factual grounding and specificity
of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms
leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,
BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and
context-aware legal insights. This approach offers a scalable, AI-driven method
for legislative analysis, supporting efforts to update legal frameworks in line
with advancements in transportation technologies.

</details>


### [528] [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
*Zhaoyang Wang,Jinqi Jiang,Tian Qiu,Hui Liu,Xianfeng Tang,Huaxiu Yao*

Main category: cs.CL

TL;DR: Recent large reasoning models show strong problem-solving abilities by generating long chain-of-thought (CoT) reasoning steps. However, it's challenging to directly train small language models (SLMs) for such ability. This paper proposes a method to prune unnecessary steps in long CoT and use an on-policy method for SLMs to curate valid and useful long CoT training data.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of enabling small language models (SLMs) with the ability to perform long chain-of-thought reasoning, which is typically exhibited by larger models.

Method: The proposed method involves pruning unnecessary steps in long chain-of-thought reasoning and using an on-policy approach for SLMs to create effective long CoT training data.

Result: Experimental results across various mathematical reasoning benchmarks demonstrate that the proposed method effectively distills long CoT reasoning ability into SLMs, maintaining competitive performance while reducing redundant reasoning steps.

Conclusion: This paper concludes that the proposed method successfully enables SLMs to learn efficient long CoT reasoning, preserving high performance while minimizing redundancy.

Abstract: Recent large reasoning models such as DeepSeek-R1 exhibit strong complex
problems solving abilities by generating long chain-of-thought (CoT) reasoning
steps. It is challenging to directly train small language models (SLMs) to
emerge long CoT. Thus, distillation becomes a practical method to enable SLMs
for such reasoning ability. However, the long CoT often contains a lot of
redundant contents (e.g., overthinking steps) which may make SLMs hard to learn
considering their relatively poor capacity and generalization. To address this
issue, we propose a simple-yet-effective method to prune unnecessary steps in
long CoT, and then employ an on-policy method for the SLM itself to curate
valid and useful long CoT training data. In this way, SLMs can effectively
learn efficient long CoT reasoning and preserve competitive performance at the
same time. Experimental results across a series of mathematical reasoning
benchmarks demonstrate the effectiveness of the proposed method in distilling
long CoT reasoning ability into SLMs which maintains the competitive
performance but significantly reduces generating redundant reasoning steps.

</details>


### [529] [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)
*Haoyuan Sun,Jiaqi Wu,Bo Xia,Yifu Luo,Yifei Zhao,Kai Qin,Xufei Lv,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CL

TL;DR: In 2025, reinforcement fine-tuning (RFT) has shown great potential in enhancing the reasoning ability of large language models (LLMs) and multimodal large language models (MLLMs). This position paper argues that RFT powers the reasoning capability of MLLMs. It introduces background knowledge, summarizes improvements in five key points, and proposes five future research directions.


<details>
  <summary>Details</summary>
Motivation: To emphasize the importance and potential of reinforcement fine-tuning (RFT) in improving the reasoning capabilities of multimodal large language models (MLLMs), and to provide guidance and insights for researchers in this field.

Method: Provide background knowledge about RFT and its application in MLLMs; summarize the improvements brought by RFT into five key points including diverse modalities, tasks and domains, better training algorithms, benchmarks, and engineering frameworks; propose five promising future research directions.

Result: The paper successfully argues that RFT significantly enhances the reasoning capability of MLLMs and provides valuable insights and guidance for future research.

Conclusion: This position paper concludes that RFT is a powerful tool for enhancing the reasoning capability of MLLMs. It provides a comprehensive overview of the current state and future directions in this field, hoping to inspire further advancements towards AGI.

Abstract: Standing in 2025, at a critical juncture in the pursuit of Artificial General
Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated
significant potential in enhancing the reasoning capability of large language
models (LLMs) and has led to the development of cutting-edge AI models such as
OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to
enhance the reasoning capability of multimodal large language models (MLLMs)
has attracted widespread attention from the community. In this position paper,
we argue that reinforcement fine-tuning powers the reasoning capability of
multimodal large language models. To begin with, we provide a detailed
introduction to the fundamental background knowledge that researchers
interested in this field should be familiar with. Furthermore, we meticulously
summarize the improvements of RFT in powering reasoning capability of MLLMs
into five key points: diverse modalities, diverse tasks and domains, better
training algorithms, abundant benchmarks and thriving engineering frameworks.
Finally, we propose five promising directions for future research that the
community might consider. We hope that this position paper will provide
valuable insights to the community at this pivotal stage in the advancement
toward AGI. Summary of works done on RFT for MLLMs is available at
https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.

</details>


### [530] [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
*Jun Zhuang,Haibo Jin,Ye Zhang,Zhengjian Kang,Wenbin Zhang,Gaby G. Dagher,Haohan Wang*

Main category: cs.CL

TL;DR: The paper explores the vulnerability of intent-aware guardrails in LLMs and proposes a framework called IntentPrompt that can transform harmful inquiries and evade advanced defenses. Experiments show that it outperforms several cutting-edge jailbreak methods, highlighting a critical weakness in LLMs' safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: To investigate the robustness of intent-aware guardrails in LLMs under malicious manipulations and to demonstrate the implicit intent detection capabilities of LLMs.

Method: A two-stage intent-based prompt-refinement framework named IntentPrompt is proposed. It transforms harmful inquiries into structured outlines and reframes them into declarative-style narratives by iteratively optimizing prompts via feedback loops.

Result: Extensive experiments across four public benchmarks and various black-box LLMs indicate that the IntentPrompt framework consistently outperforms several cutting-edge jailbreak methods and evades even advanced IA and CoT-based defenses. The 'FSTR+SPIN' variant achieves high attack success rates against CoT-based and IA-based defenses.

Conclusion: The findings highlight a critical weakness in LLMs' safety mechanisms, suggesting that intent manipulation poses a growing challenge to content moderation guardrails.

Abstract: Intent detection, a core component of natural language understanding, has
considerably evolved as a crucial mechanism in safeguarding large language
models (LLMs). While prior work has applied intent detection to enhance LLMs'
moderation guardrails, showing a significant success against content-level
jailbreaks, the robustness of these intent-aware guardrails under malicious
manipulations remains under-explored. In this work, we investigate the
vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit
implicit intent detection capabilities. We propose a two-stage intent-based
prompt-refinement framework, IntentPrompt, that first transforms harmful
inquiries into structured outlines and further reframes them into
declarative-style narratives by iteratively optimizing prompts via feedback
loops to enhance jailbreak success for red-teaming purposes. Extensive
experiments across four public benchmarks and various black-box LLMs indicate
that our framework consistently outperforms several cutting-edge jailbreak
methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought
(CoT)-based defenses. Specifically, our "FSTR+SPIN" variant achieves attack
success rates ranging from 88.25% to 96.54% against CoT-based defenses on the
o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based
defenses. These findings highlight a critical weakness in LLMs' safety
mechanisms and suggest that intent manipulation poses a growing challenge to
content moderation guardrails.

</details>


### [531] [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)
*Xunlian Dai,Li Zhou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: The paper introduces CultureSteer, a method that improves large language models' cross-cultural alignment by guiding their semantic representations towards culturally specific spaces. Experiments reveal significant Western cultural bias in current LLMs and demonstrate the superiority of CultureSteer over prompt-based methods in capturing diverse semantic associations.


<details>
  <summary>Details</summary>
Motivation: To address the issue of cultural bias in large language models (LLMs) and enhance their cross-cultural cognition and alignment.

Method: The authors extended the word association test into an LLM-adaptive, free-relation task and proposed CultureSteer, which integrates a culture-aware steering mechanism to guide semantic representations toward culturally specific spaces.

Result: Experiments showed that existing LLMs have a significant bias towards Western cultural schemas. CultureSteer substantially improved cross-cultural alignment and outperformed prompt-based methods in capturing diverse semantic associations. Further validation on culture-sensitive tasks confirmed its efficacy.

Conclusion: CultureSteer presents a new paradigm for enhancing cultural awareness in LLMs, promoting the development of more inclusive language technologies.

Abstract: The human-centered word association test (WAT) serves as a cognitive proxy,
revealing sociocultural variations through lexical-semantic patterns. We extend
this test into an LLM-adaptive, free-relation task to assess the alignment of
large language models (LLMs) with cross-cultural cognition. To mitigate the
culture preference, we propose CultureSteer, an innovative approach that
integrates a culture-aware steering mechanism to guide semantic representations
toward culturally specific spaces. Experiments show that current LLMs exhibit
significant bias toward Western cultural (notably in American) schemas at the
word association level. In contrast, our model substantially improves
cross-cultural alignment, surpassing prompt-based methods in capturing diverse
semantic associations. Further validation on culture-sensitive downstream tasks
confirms its efficacy in fostering cognitive alignment across cultures. This
work contributes a novel methodological paradigm for enhancing cultural
awareness in LLMs, advancing the development of more inclusive language
technologies.

</details>


### [532] [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)
*Wentao Hu,Wengyu Zhang,Yiyang Jiang,Chen Jason Zhang,Xiaoyong Wei,Qing Li*

Main category: cs.CL

TL;DR: DRAG is a training-free framework that integrates MAD mechanisms into both retrieval and generation stages of RAG, enhancing factual accuracy and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: RAG has an issue where erroneous or biased retrieval can mislead generation, leading to compounded hallucinations.

Method: DRAG employs structured debates among proponents, opponents, and judges in the retrieval stage to refine quality and ensure reliability. In the generation stage, it introduces asymmetric information roles and adversarial debates.

Result: Evaluations show DRAG improves retrieval reliability, reduces hallucinations, and enhances factual accuracy.

Conclusion: DRAG effectively addresses the limitations of RAG by improving retrieval and generation processes.

Abstract: Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.

</details>


### [533] [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)
*Zesheng Shi,Yucheng Zhou,Jing Li*

Main category: cs.CL

TL;DR: 尽管在安全对齐方面取得了显著进展，大型语言模型（LLMs）仍然容易受到越狱攻击。现有的防御机制未能完全删除LLMs中的有害知识，使得这些攻击能够绕过防护并产生有害输出。为了解决这一挑战，我们提出了一种新的安全对齐策略，即约束知识遗忘（CKU），其主要关注两个目标：知识定位和保留以及遗忘有害知识。CKU通过在特定的多层感知器（MLP）层中对神经元进行评分，以识别与有用知识相关的一组神经元U。在遗忘过程中，CKU修剪神经元U的梯度，以保留有价值的知识，同时有效缓解有害内容。实验结果表明，CKU显著提高了模型安全性，且不损害整体性能，相比现有方法提供了更好的安全性和效用之间的平衡。此外，我们对各种MLP层中神经元知识敏感性的分析，为安全对齐和模型知识编辑的机制提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 尽管在安全对齐方面取得了一些进展，但LLMs仍然容易受到越狱攻击。现有防御机制无法彻底消除有害知识，导致防护被绕过并产生有害输出。因此，需要一种新策略来更有效地解决这一问题。

Method: 提出了一种名为约束知识遗忘（CKU）的新策略。该策略通过对特定MLP层中的神经元评分，识别出与有用知识相关的神经元子集U。然后，在遗忘过程中修剪神经元U的梯度，以保留有用知识的同时减轻有害内容的影响。

Result: 实验结果表明，CKU显著提高了模型的安全性，同时保持了整体性能。它在安全性和效用之间达到了比现有方法更好的平衡。此外，研究还揭示了关于安全对齐和模型知识编辑机制的宝贵见解。

Conclusion: CKU是一种有效的安全对齐策略，可以在不损害模型整体性能的情况下提高模型安全性。这种策略为未来的研究提供了宝贵的启示。

Abstract: Despite significant progress in safety alignment, large language models
(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms
have not fully deleted harmful knowledge in LLMs, which allows such attacks to
bypass safeguards and produce harmful outputs. To address this challenge, we
propose a novel safety alignment strategy, Constrained Knowledge Unlearning
(CKU), which focuses on two primary objectives: knowledge localization and
retention, and unlearning harmful knowledge. CKU works by scoring neurons in
specific multilayer perceptron (MLP) layers to identify a subset U of neurons
associated with useful knowledge. During the unlearning process, CKU prunes the
gradients of neurons in U to preserve valuable knowledge while effectively
mitigating harmful content. Experimental results demonstrate that CKU
significantly enhances model safety without compromising overall performance,
offering a superior balance between safety and utility compared to existing
methods. Additionally, our analysis of neuron knowledge sensitivity across
various MLP layers provides valuable insights into the mechanics of safety
alignment and model knowledge editing.

</details>


### [534] [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
*Chen Han,Wenzhen Zheng,Xijin Tang*

Main category: cs.CL

TL;DR: The paper presents Debate-to-Detect (D2D), a Multi-Agent Debate framework for misinformation detection that uses a five-stage debate process and multi-dimensional evaluation. Experiments with GPT-4o show improvements over baselines, enhancing decision transparency and evidence refinement.


<details>
  <summary>Details</summary>
Motivation: Traditional misinformation detection methods have limitations in capturing the real-world fact-checking process, while LLMs face challenges of logical inconsistency and superficial verification.

Method: D2D reformulates misinformation detection as a structured adversarial debate with domain-specific profiles for agents and a five-stage debate process: Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. It also introduces a multi-dimensional evaluation mechanism across Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics.

Result: Experiments using GPT-4o on two fakenews datasets demonstrate significant improvements over baseline methods. The case study highlights D2D's ability to iteratively refine evidence and improve decision transparency.

Conclusion: D2D represents a substantial advancement towards robust and interpretable misinformation detection, and the code will be open-sourced in a future release.

Abstract: The proliferation of misinformation in digital platforms reveals the
limitations of traditional detection methods, which mostly rely on static
classification and fail to capture the intricate process of real-world
fact-checking. Despite advancements in Large Language Models (LLMs) that
enhance automated reasoning, their application to misinformation detection
remains hindered by issues of logical inconsistency and superficial
verification. In response, we introduce Debate-to-Detect (D2D), a novel
Multi-Agent Debate (MAD) framework that reformulates misinformation detection
as a structured adversarial debate. Inspired by fact-checking workflows, D2D
assigns domain-specific profiles to each agent and orchestrates a five-stage
debate process, including Opening Statement, Rebuttal, Free Debate, Closing
Statement, and Judgment. To transcend traditional binary classification, D2D
introduces a multi-dimensional evaluation mechanism that assesses each claim
across five distinct dimensions: Factuality, Source Reliability, Reasoning
Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets
demonstrate significant improvements over baseline methods, and the case study
highlight D2D's capability to iteratively refine evidence while improving
decision transparency, representing a substantial advancement towards robust
and interpretable misinformation detection. The code will be open-sourced in a
future release.

</details>


### [535] [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)
*Jongwoo Ko,Sungnyun Kim,Sungwoo Cho,Se-Young Yun*

Main category: cs.CL

TL;DR: 提出了一种新的模型Flex-Judge，通过少量文本推理数据实现跨模态和评估格式的鲁棒泛化，效果媲美甚至超越现有商业API和多模态评估器。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）作为代理评估者虽然降低了手动标注的成本，但需要大量的特定模态训练数据，且在多样化的多模态任务中泛化能力不足。

Method: 提出了Flex-Judge模型，利用结构化的文本推理解释来编码可泛化的决策模式，从而有效地转移到多模态判断上，例如涉及图像或视频的判断。

Result: 实验结果表明，尽管Flex-Judge训练所用的文本数据显著减少，但在性能上仍能与最先进的商业API和经过广泛训练的多模态评估者竞争甚至超越。特别是在分子等模态领域，其表现突出，显示了在资源受限领域的实际价值。

Conclusion: 该框架强调了基于推理的文本监督作为一种强大且成本效益高的替代方案，可以显著推进可扩展的多模态模型-as-a-judge的发展。

Abstract: Human-generated reward signals are critical for aligning generative models
with human preferences, guiding both training and inference-time evaluations.
While large language models (LLMs) employed as proxy evaluators, i.e.,
LLM-as-a-Judge, significantly reduce the costs associated with manual
annotations, they typically require extensive modality-specific training data
and fail to generalize well across diverse multimodal tasks. In this paper, we
propose Flex-Judge, a reasoning-guided multimodal judge model that leverages
minimal textual reasoning data to robustly generalize across multiple
modalities and evaluation formats. Our core intuition is that structured
textual reasoning explanations inherently encode generalizable decision-making
patterns, enabling an effective transfer to multimodal judgments, e.g., with
images or videos. Empirical results demonstrate that Flex-Judge, despite being
trained on significantly fewer text data, achieves competitive or superior
performance compared to state-of-the-art commercial APIs and extensively
trained multimodal evaluators. Notably, Flex-Judge presents broad impact in
modalities like molecule, where comprehensive evaluation benchmarks are scarce,
underscoring its practical value in resource-constrained domains. Our framework
highlights reasoning-based text supervision as a powerful, cost-effective
alternative to traditional annotation-intensive approaches, substantially
advancing scalable multimodal model-as-a-judge.

</details>


### [536] [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)
*Zhihao Jia,Mingyi Jia,Junwen Duan,Jianxin Wang*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) are good for complex tasks like medical consultation, but they struggle with its dual nature of symptom inquiry and disease diagnosis. The proposed DDO framework addresses this by optimizing the two sub-tasks independently through a collaborative multi-agent workflow.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods often fail to capture the dual nature of medical consultation, resulting in ineffective symptom inquiry and unreliable disease diagnosis.

Method: The proposed method is DDO, a novel LLM-based framework that performs Dual-Decision Optimization by decoupling and independently optimizing the two sub-tasks (symptom inquiry and disease diagnosis) through a collaborative multi-agent workflow.

Result: Experiments on three real-world MC datasets show that DDO consistently outperforms existing LLM-based approaches and achieves competitive performance with state-of-the-art generation-based methods.

Conclusion: DDO demonstrates its effectiveness in the medical consultation task.

Abstract: Large Language Models (LLMs) demonstrate strong generalization and reasoning
abilities, making them well-suited for complex decision-making tasks such as
medical consultation (MC). However, existing LLM-based methods often fail to
capture the dual nature of MC, which entails two distinct sub-tasks: symptom
inquiry, a sequential decision-making process, and disease diagnosis, a
classification problem. This mismatch often results in ineffective symptom
inquiry and unreliable disease diagnosis. To address this, we propose
\textbf{DDO}, a novel LLM-based framework that performs
\textbf{D}ual-\textbf{D}ecision \textbf{O}ptimization by decoupling and
independently optimizing the the two sub-tasks through a collaborative
multi-agent workflow. Experiments on three real-world MC datasets show that DDO
consistently outperforms existing LLM-based approaches and achieves competitive
performance with state-of-the-art generation-based methods, demonstrating its
effectiveness in the MC task.

</details>


### [537] [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)
*Pankaj Kumar,Subhankar Mishra*

Main category: cs.CL

TL;DR: This paper surveys the robustness of Large Language Models (LLMs), including conceptual foundations, sources of non-robustness, mitigation strategies, benchmarks, metrics, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of ensuring the robustness of LLMs and advance the development of NLP and AI.

Method: Systematically examine the nature of robustness in LLMs, analyze sources of non-robustness, review mitigation strategies, discuss benchmarks and metrics, and synthesize findings from existing surveys and studies.

Result: Provides a comprehensive overview of current studies on the robustness of LLMs, identifies persistent gaps, and highlights trends and unresolved issues.

Conclusion: The robustness of LLMs is crucial for their reliable performance in real-world applications. This survey synthesizes existing knowledge and points out pathways for future research.

Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the
development of natural language processing (NLP) and artificial intelligence
(AI). However, ensuring the robustness of LLMs remains a critical challenge. To
address these challenges and advance the field, this survey provides a
comprehensive overview of current studies in this area. First, we
systematically examine the nature of robustness in LLMs, including its
conceptual foundations, the importance of consistent performance across diverse
inputs, and the implications of failure modes in real-world applications. Next,
we analyze the sources of non-robustness, categorizing intrinsic model
limitations, data-driven vulnerabilities, and external adversarial factors that
compromise reliability. Following this, we review state-of-the-art mitigation
strategies, and then we discuss widely adopted benchmarks, emerging metrics,
and persistent gaps in assessing real-world reliability. Finally, we synthesize
findings from existing surveys and interdisciplinary studies to highlight
trends, unresolved issues, and pathways for future research.

</details>


### [538] [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
*Aleksandr Tsymbalov*

Main category: cs.CL

TL;DR: The paper explores using Large Language Models (LLMs) as an alternative to human annotators for testing classifier predictions in text classification models, aiming to reduce costs and support incremental learning.


<details>
  <summary>Details</summary>
Motivation: Training and validating text classification models require labeled data which are usually assigned by human annotators. This process is labor-intensive, costly, and limited by the availability and productivity of human experts. Retraining models due to data drift further exacerbates these challenges.

Method: Propose several approaches to replace human annotators with Large Language Models (LLMs) to test the correctness of classifier predictions.

Result: Using LLMs can help ensure model quality and support high-quality incremental learning in text classification tasks.

Conclusion: Replacing human annotators with LLMs could significantly reduce the cost and effort required for ongoing model retraining and validation.

Abstract: Machine learning models for text classification are trained to predict a
class for a given text. To do this, training and validation samples must be
prepared: a set of texts is collected, and each text is assigned a class. These
classes are usually assigned by human annotators with different expertise
levels, depending on the specific classification task. Collecting such samples
from scratch is labor-intensive because it requires finding specialists and
compensating them for their work; moreover, the number of available specialists
is limited, and their productivity is constrained by human factors. While it
may not be too resource-intensive to collect samples once, the ongoing need to
retrain models (especially in incremental learning pipelines) to address data
drift (also called model drift) makes the data collection process crucial and
costly over the model's entire lifecycle. This paper proposes several
approaches to replace human annotators with Large Language Models (LLMs) to
test classifier predictions for correctness, helping ensure model quality and
support high-quality incremental learning.

</details>


### [539] [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)
*Xu Zhang,Kun Zhang,Wenxin Ma,Rongsheng Wang,Chenxu Wu,Yingtai Li,S. Kevin Zhou*

Main category: cs.CL

TL;DR: The paper proposes GKI-ICD, a general knowledge injection framework for ICD Coding that integrates three types of knowledge without additional specialized modules, demonstrating state-of-the-art performance on most evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To address the issues of long-tail distribution and lack of annotations in ICD coding, as well as the limitations of existing methods focusing on single types of knowledge with complex and incompatible designs.

Method: Propose GKI-ICD framework which incorporates ICD Description, ICD Synonym, and ICD Hierarchy knowledge into the ICD coding process without designing specialized modules.

Result: GKI-ICD shows effectiveness through extensive experiments on popular ICD coding benchmarks, achieving state-of-the-art performance on most evaluation metrics.

Conclusion: GKI-ICD is a novel framework that successfully integrates multiple types of knowledge to enhance ICD coding performance without requiring specialized additional modules.

Abstract: ICD Coding aims to assign a wide range of medical codes to a medical text
document, which is a popular and challenging task in the healthcare domain. To
alleviate the problems of long-tail distribution and the lack of annotations of
code-specific evidence, many previous works have proposed incorporating code
knowledge to improve coding performance. However, existing methods often focus
on a single type of knowledge and design specialized modules that are complex
and incompatible with each other, thereby limiting their scalability and
effectiveness. To address this issue, we propose GKI-ICD, a novel, general
knowledge injection framework that integrates three key types of knowledge,
namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized
design of additional modules. The comprehensive utilization of the above
knowledge, which exhibits both differences and complementarity, can effectively
enhance the ICD coding performance. Extensive experiments on existing popular
ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves
the state-of-the-art performance on most evaluation metrics. Code is available
at https://github.com/xuzhang0112/GKI-ICD.

</details>


### [540] [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)
*Sourav Kumar Das,Md. Julkar Naeen,MD. Jahidul Islam,Md. Anisul Haque Sajeeb,Narayan Ranjan Chakraborty,Mayen Uddin Mojumdar*

Main category: cs.CL

TL;DR: This paper focuses on the translation from Pure or Modern Bangla to Sylheti language, a local dialect in Bangladesh, using NLP techniques. Among the models trained (LSTM, Bi-LSTM, and Seq2Seq), LSTM performed best with 89.3% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on local languages in Bangladesh, specifically the Sylheti language, and contribute to the growth of Bangla NLP researchers for future innovations.

Method: The research employed NLP techniques to develop a comprehensive system for translating Pure or Modern Bangla to Sylheti Bangla. Three models were trained using 1200 data points: LSTM, Bi-LSTM, and Seq2Seq.

Result: LSTM model scored the best performance with 89.3% accuracy.

Conclusion: This research contributes to the development of Bangla NLP and encourages more advanced innovations in processing local languages.

Abstract: Bangla or Bengali is the national language of Bangladesh, people from
different regions don't talk in proper Bangla. Every division of Bangladesh has
its own local language like Sylheti, Chittagong etc. In recent years some
papers were published on Bangla language like sentiment analysis, fake news
detection and classifications, but a few of them were on Bangla languages. This
research is for the local language and this particular paper is on Sylheti
language. It presented a comprehensive system using Natural Language Processing
or NLP techniques for translating Pure or Modern Bangla to locally spoken
Sylheti Bangla language. Total 1200 data used for training 3 models LSTM,
Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%
accuracy. The findings of this research may contribute to the growth of Bangla
NLP researchers for future more advanced innovations.

</details>


### [541] [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)
*Meng Li,Guangda Huzhang,Haibo Zhang,Xiting Wang,Anxiang Zeng*

Main category: cs.CL

TL;DR: The paper introduces OTPO, an improvement to DPO for aligning LLMs with human preferences by using optimal transport-based token weighting to focus on meaningful response differences.


<details>
  <summary>Details</summary>
Motivation: Existing DPO methods treat all tokens equally, which doesn't align with how humans evaluate responses and can lead to suboptimal preference optimization due to noise or irrelevant tokens influencing the loss.

Method: OTPO uses a context-aware token weighting scheme based on optimal transport to emphasize semantically meaningful token pairs and de-emphasize less relevant ones, providing a more contrastive reward difference estimate.

Result: Experiments show that OTPO effectively improves the instruction-following ability of LLMs across different settings.

Conclusion: OTPO enhances reward stability, interpretability, and focuses preference optimization on meaningful response differences through adaptive token weighting.

Abstract: Direct Preference Optimization (DPO) has emerged as a promising framework for
aligning Large Language Models (LLMs) with human preferences by directly
optimizing the log-likelihood difference between chosen and rejected responses.
However, existing methods assign equal importance to all tokens in the
response, while humans focus on more meaningful parts. This leads to suboptimal
preference optimization, as irrelevant or noisy tokens disproportionately
influence DPO loss. To address this limitation, we propose \textbf{O}ptimal
\textbf{T}ransport-based token weighting scheme for enhancing direct
\textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically
meaningful token pairs and de-emphasizing less relevant ones, our method
introduces a context-aware token weighting scheme that yields a more
contrastive reward difference estimate. This adaptive weighting enhances reward
stability, improves interpretability, and ensures that preference optimization
focuses on meaningful differences between responses. Extensive experiments have
validated OTPO's effectiveness in improving instruction-following ability
across various settings\footnote{Code is available at
https://github.com/Mimasss2/OTPO.}.

</details>


### [542] [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
*Minglai Yang,Ethan Huang,Liang Zhang,Mihai Surdeanu,William Wang,Liangming Pan*

Main category: cs.CL

TL;DR: 提出GSM-DC基准，用于评估大语言模型在系统控制的无关上下文中的推理鲁棒性。实验表明无关上下文显著影响模型表现，并提出由过程奖励模型引导的逐步树搜索方法以增强模型在外分布条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面对无关上下文时的推理鲁棒性尚未得到充分研究和评估，因此需要一个合成基准来系统地研究这一问题。

Method: 引入GSM-DC基准，构建带有精确干扰注入的符号推理图，通过实验分析无关上下文对模型的影响，并提出由过程奖励模型引导的逐步树搜索方法。

Result: 实验表明大语言模型对无关上下文非常敏感，训练时加入强干扰可以提升模型表现，提出的搜索方法显著增强了模型在外分布条件下的鲁棒性。

Conclusion: GSM-DC为评估大语言模型的推理鲁棒性提供了一个严谨可重复的工具，提出的逐步树搜索方法有效提升了模型的外分布鲁棒性。

Abstract: We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic
benchmark to evaluate Large Language Models' (LLMs) reasoning robustness
against systematically controlled irrelevant context (IC). GSM-DC constructs
symbolic reasoning graphs with precise distractor injections, enabling
rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are
significantly sensitive to IC, affecting both reasoning path selection and
arithmetic accuracy. Additionally, training models with strong distractors
improves performance in both in-distribution and out-of-distribution scenarios.
We further propose a stepwise tree search guided by a process reward model,
which notably enhances robustness in out-of-distribution conditions.

</details>


### [543] [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)
*Michael Flor,Zuowei Wang,Paul Deane,Tenaha O'Reilly*

Main category: cs.CL

TL;DR: To address the lack of automated measures for student knowledge, this paper introduces K-tool, a system that generates topical vocabulary tests to assess students' background knowledge for understanding specific texts. It detects text topics and creates relevant vocabulary items without relying on a corpus.


<details>
  <summary>Details</summary>
Motivation: There are few automated measures available to assess whether students possess the necessary background knowledge to understand domain-specific reading passages, especially in STEM fields.

Method: K-tool automatically detects the topic of a given text and produces topical vocabulary items based on their relationship with the topic. It then generates background knowledge forms containing words highly related to the topic and others that share similar features but have low associations with it.

Result: The system architecture is described and an initial evaluation of the system outputs is presented, indicating potential for assessing students' likelihood to understand particular texts based on their knowledge state.

Conclusion: K-tool is designed for middle and high school English native speakers, focusing on single reading passages without dependency on any corpus or text collection.

Abstract: Background knowledge is typically needed for successful comprehension of
topical and domain specific reading passages, such as in the STEM domain.
However, there are few automated measures of student knowledge that can be
readily deployed and scored in time to make predictions on whether a given
student will likely be able to understand a specific content area text. In this
paper, we present our effort in developing K-tool, an automated system for
generating topical vocabulary tests that measure students' background knowledge
related to a specific text. The system automatically detects the topic of a
given text and produces topical vocabulary items based on their relationship
with the topic. This information is used to automatically generate background
knowledge forms that contain words that are highly related to the topic and
words that share similar features but do not share high associations to the
topic. Prior research indicates that performance on such tasks can help
determine whether a student is likely to understand a particular text based on
their knowledge state. The described system is intended for use with middle and
high school student population of native speakers of English. It is designed to
handle single reading passages and is not dependent on any corpus or text
collection. In this paper, we describe the system architecture and present an
initial evaluation of the system outputs.

</details>


### [544] [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
*Hao Chen,Haoze Li,Zhiqing Xiao,Lirong Gao,Qi Zhang,Xiaomeng Hu,Ningtao Wang,Xing Fu,Junbo Zhao*

Main category: cs.CL

TL;DR: An efficient algorithm named ALPS is proposed to localize and prune task-sensitive attention heads in LLMs, reducing alignment costs and improving performance.


<details>
  <summary>Details</summary>
Motivation: Aligning large language models to downstream tasks is costly in terms of constructing task-specific instruction pairs and training adjustments. Current methods for enhancing alignment efficiency introduce data dependency, hindering generalization and reusability.

Method: The Attention Localization and Pruning Strategy (ALPS) is introduced. This method localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thus minimizing resource consumption.

Result: Experiments show that this method activates only 10% of attention parameters during fine-tuning and achieves a 2% performance improvement over baselines on three tasks. The identified heads are transferable across datasets and mitigate knowledge forgetting.

Conclusion: ALPS provides an effective way to reduce the cost of aligning LLMs with downstream tasks while maintaining or improving performance.

Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant costs, including constructing task-specific
instruction pairs and extensive training adjustments. Prior research has
explored various avenues to enhance alignment efficiency, primarily through
minimal-data training or data-driven activations to identify key attention
heads. However, these approaches inherently introduce data dependency, which
hinders generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment.

</details>


### [545] [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)
*Yuxiang Liu,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: The paper introduces Exemplar-Based Expository Text Generation task and proposes the Recurrent Plan-then-Adapt (RePA) framework to overcome limitations in current methods, demonstrating superior performance across three datasets.


<details>
  <summary>Details</summary>
Motivation: Current text generation methods struggle with generating coherent long texts on new topics due to reliance on extensive exemplar data, challenges in adapting topic-specific content, and issues with maintaining coherence.

Method: The authors propose Adaptive Imitation via a Recurrent Plan-then-Adapt (RePA) framework that uses large language models for adaptive imitation through fine-grained plan-then-adapt process. It supports recurrent segment-by-segment imitation with two memory structures to enhance input clarity and output coherence.

Result: Experimental results on three diverse datasets show that RePA outperforms existing baselines in producing factual, consistent, and relevant texts.

Conclusion: RePA is an effective solution for the Exemplar-Based Expository Text Generation task, enhancing both imitativeness and adaptiveness in generated texts.

Abstract: We introduce the Exemplar-Based Expository Text Generation task, aiming to
generate an expository text on a new topic using an exemplar on a similar
topic. Current methods fall short due to their reliance on extensive exemplar
data, difficulty in adapting topic-specific content, and issues with long-text
coherence. To address these challenges, we propose the concept of Adaptive
Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA
leverages large language models (LLMs) for effective adaptive imitation through
a fine-grained plan-then-adapt process. RePA also enables recurrent
segment-by-segment imitation, supported by two memory structures that enhance
input clarity and output coherence. We also develop task-specific evaluation
metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as
evaluators. Experimental results across our collected three diverse datasets
demonstrate that RePA surpasses existing baselines in producing factual,
consistent, and relevant texts for this task.

</details>


### [546] [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)
*Kung-Hsiang Huang,Akshara Prabhakar,Onkar Thorat,Divyansh Agarwal,Prafulla Kumar Choubey,Yixin Mao,Silvio Savarese,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: The paper introduces CRMArena-Pro, an advanced benchmark for evaluating LLM agents in realistic business scenarios. It features nineteen expert-validated tasks and reveals significant performance gaps in multi-turn interactions, confidentiality awareness, and diverse business skills.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective performance benchmarking tools for AI agents in business due to scarce public, realistic data and limited coverage of diverse business scenarios.

Method: Developed CRMArena-Pro, expanding on CRMArena with 19 expert-validated tasks across various business processes and scenarios. Incorporated multi-turn interactions and confidentiality awareness assessments.

Result: Top LLM agents achieved around 58% single-turn success but dropped to about 35% in multi-turn settings. Workflow Execution had over 83% single-turn success, but other skills were more challenging. Agents showed near-zero inherent confidentiality awareness.

Conclusion: There is a significant gap between current LLM capabilities and enterprise needs, requiring improvements in multi-turn reasoning, confidentiality adherence, and skill versatility.

Abstract: While AI agents hold transformative potential in business, effective
performance benchmarking is hindered by the scarcity of public, realistic
business data on widely used platforms. Existing benchmarks often lack fidelity
in their environments, data, and agent-user interactions, with limited coverage
of diverse business scenarios and industries. To address these gaps, we
introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of
LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena
with nineteen expert-validated tasks across sales, service, and 'configure,
price, and quote' processes, for both Business-to-Business and
Business-to-Customer scenarios. It distinctively incorporates multi-turn
interactions guided by diverse personas and robust confidentiality awareness
assessments. Experiments reveal leading LLM agents achieve only around 58%
single-turn success on CRMArena-Pro, with performance dropping significantly to
approximately 35% in multi-turn settings. While Workflow Execution proves more
tractable for top agents (over 83% single-turn success), other evaluated
business skills present greater challenges. Furthermore, agents exhibit
near-zero inherent confidentiality awareness; though targeted prompting can
improve this, it often compromises task performance. These findings highlight a
substantial gap between current LLM capabilities and enterprise demands,
underscoring the need for advancements in multi-turn reasoning, confidentiality
adherence, and versatile skill acquisition.

</details>


### [547] [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)
*Amel Muminovic*

Main category: cs.CL

TL;DR: 研究比较了GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus在检测有害评论方面的表现，发现各自有优势和局限性，强调需要结合多种模型和改进策略的审核流程。


<details>
  <summary>Details</summary>
Motivation: 在线平台上的骚扰评论影响用户体验和心理健康，急需有效的自动化内容审核工具。

Method: 使用统一提示和确定性设置，在包含英语、阿拉伯语和印尼语的5080条评论数据集上评估三种大型语言模型的表现。数据集由两位评审员独立标注，具有较高一致性。

Result: GPT-4.1总体表现最佳（F1分数0.863），Gemini召回率最高但精度较低，Claude精度最高但召回率较低。所有模型在处理讽刺、编码侮辱和混合语言俚语时都有困难。

Conclusion: 建议内容审核流程结合多种互补模型，考虑对话上下文，并针对代表性不足的语言和隐性滥用进行微调。

Abstract: As online platforms grow, comment sections increasingly host harassment that
undermines user experience and well-being. This study benchmarks three leading
large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic
Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse
threads in gaming, lifestyle, food vlog, and music channels. The dataset
comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and
Indonesian, annotated independently by two reviewers with substantial agreement
(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,
GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision
of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful
posts (recall = 0.875) but its precision fell to 0.767 due to frequent false
positives. Claude delivered the highest precision at 0.920 and the lowest
false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative
analysis showed that all three models struggle with sarcasm, coded insults, and
mixed-language slang. These results underscore the need for moderation
pipelines that combine complementary models, incorporate conversational
context, and fine-tune for under-represented languages and implicit abuse. A
de-identified version of the dataset and full prompts is publicly released to
promote reproducibility and further progress in automated content moderation.

</details>


### [548] [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)
*Longfei Yun,Chenyang An,Zilong Wang,Letian Peng,Jingbo Shang*

Main category: cs.CL

TL;DR: Instruction-tuned large language models (LLMs) experience diversity collapse due to structured templates, which limits output variability. Evaluation shows that while format consistency is crucial for structure-sensitive tasks, it suppresses diversity in knowledge-heavy tasks. Removing structural tokens enhances output diversity.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of structured templates on the output diversity and performance of instruction-tuned large language models.

Method: Systematically evaluate the effect of diversity collapse across various tasks using high-temperature sampling. Fine-tune models with different structured prompts and assess them based on downstream task performance, alignment behavior, and output diversity.

Result: Diversity collapse persists even under high-temperature sampling and structural tokens significantly constrain the model's output space. Format consistency is crucial for structure-sensitive tasks but has minimal influence on knowledge-heavy tasks. Output diversity is primarily governed by the presence or absence of structural tokens.

Conclusion: Structured prompting conventions may inadvertently suppress output diversity. There is a need for diversity-aware prompt design and instruction tuning.

Abstract: Instruction-tuned large language models (LLMs) employ structured templates,
such as role markers and special tokens, to enforce format consistency during
inference. However, we identify a critical limitation of such formatting: it
induces a phenomenon we term diversity collapse, where the model generates
semantically similar outputs for open-ended inputs, undermining creativity and
variability. We systematically evaluate this effect across tasks like story
completion and free-form generation, finding that (1) diversity collapse
persists even under high-temperature sampling, and (2) structural tokens in
templates significantly constrain the model's output space. To contextualize
these findings, we fine-tune the same model using a range of structured prompts
and then evaluate them across three axes: downstream task performance,
alignment behavior, and output diversity. Our analysis shows that format
consistency between fine-tuning and inference is crucial for
structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on
knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity
is primarily governed by the presence or absence of structural tokens, with
minimal formatting yielding the most diverse outputs. These findings reveal
that current prompting conventions, while beneficial for alignment, may
inadvertently suppress output diversity, underscoring the need for
diversity-aware prompt design and instruction tuning.

</details>


### [549] [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
*Ambre Marie,Ilias Maoudj,Guillaume Dardenne,Gwenolé Quellec*

Main category: cs.CL

TL;DR: This study explores a multimodal approach for speech-based suicide risk assessment in adolescents, incorporating automatic transcription, linguistic and audio embeddings, as well as handcrafted acoustic features. Weighted attention with mixup regularization was found to be the best fusion strategy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the need for speech-based suicide risk assessment in adolescents, as conveyed by the 1st SpeechWellness Challenge.

Method: The method involves integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, audio embeddings from WavLM, and handcrafted acoustic features including MFCCs, spectral contrast, and pitch-related statistics. Three fusion strategies were explored: early concatenation, modality-specific processing, and weighted attention with mixup regularization.

Result: Weighted attention provided the best generalization, achieving 69% accuracy on the development set. However, there is a performance gap between the development and test sets, highlighting generalization challenges.

Conclusion: The findings emphasize the importance of refining embedding representations and fusion mechanisms within the MINI-KID framework to improve classification reliability.

Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide
risk assessment in adolescents. This study investigates a multimodal approach
for this challenge, integrating automatic transcription with WhisperX,
linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.
Additionally, handcrafted acoustic features -- including MFCCs, spectral
contrast, and pitch-related statistics -- were incorporated. We explored three
fusion strategies: early concatenation, modality-specific processing, and
weighted attention with mixup regularization. Results show that weighted
attention provided the best generalization, achieving 69% accuracy on the
development set, though a performance gap between development and test sets
highlights generalization challenges. Our findings, strictly tied to the
MINI-KID framework, emphasize the importance of refining embedding
representations and fusion mechanisms to enhance classification reliability.

</details>


### [550] [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)
*Jesus Alvarez C,Daua D. Karajeanes,Ashley Celeste Prado,John Ruttan,Ivory Yang,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: This study conducts the first computational analysis of Comanche, an endangered Uto-Aztecan language, using a curated dataset, synthetic data generation, and evaluation of GPT-4o models for language identification. Results show few-shot prompting improves performance significantly, highlighting the potential of targeted NLP methods in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To address the digital exclusion of endangered languages like Comanche in NLP, which limits linguistic research and revitalization efforts.

Method: Introduction of a manually curated dataset of 412 phrases, a synthetic data generation pipeline, and empirical evaluation of GPT-4o and GPT-4o-mini models through few-shot prompting for language identification.

Result: LLMs struggle with Comanche in zero-shot settings but achieve near-perfect accuracy with just five examples using few-shot prompting.

Conclusion: Targeted NLP methodologies have great potential in low-resource contexts; visibility is key to inclusion. Computational approaches should prioritize accessibility, cultural sensitivity, and community engagement.

Abstract: The digital exclusion of endangered languages remains a critical challenge in
NLP, limiting both linguistic research and revitalization efforts. This study
introduces the first computational investigation of Comanche, an Uto-Aztecan
language on the verge of extinction, demonstrating how minimal-cost,
community-informed NLP interventions can support language preservation. We
present a manually curated dataset of 412 phrases, a synthetic data generation
pipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language
identification. Our experiments reveal that while LLMs struggle with Comanche
in zero-shot settings, few-shot prompting significantly improves performance,
achieving near-perfect accuracy with just five examples. Our findings highlight
the potential of targeted NLP methodologies in low-resource contexts and
emphasize that visibility is the first step toward inclusion. By establishing a
foundation for Comanche in NLP, we advocate for computational approaches that
prioritize accessibility, cultural sensitivity, and community engagement.

</details>


### [551] [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)
*Carlos Jude G. Maminta,Isaiah Job Enriquez,Deandre Nigel Nunez,Michael B. Dela Fuente*

Main category: cs.CL

TL;DR: This study presents FiLLM, a Filipino-optimized large language model based on SeaLLM-7B 2.5 with LoRA fine-tuning for memory efficiency. It was evaluated on various NLP tasks and compared to CalamanCy, which outperformed FiLLM in several aspects.


<details>
  <summary>Details</summary>
Motivation: To enhance natural language processing capabilities in the Filipino language by creating an optimized large language model.

Method: FiLLM is built upon the SeaLLM-7B 2.5 model and uses Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance. The model is trained and evaluated on diverse Filipino datasets for NER, POS tagging, Dependency Parsing, and Text Summarization.

Result: CalamanCy outperforms FiLLM in several aspects when evaluated using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap metrics.

Conclusion: This research contributes to the advancement of Filipino NLP applications by providing an optimized, efficient, and scalable language model tailored for local linguistic needs.

Abstract: This study presents FiLLM, a Filipino-optimized large language model,
designed to enhance natural language processing (NLP) capabilities in the
Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank
Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining
task-specific performance. The model was trained and evaluated on diverse
Filipino datasets to address key NLP tasks, including Named Entity Recognition
(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text
Summarization. Performance comparisons with the CalamanCy model were conducted
using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap
metrics. Results indicate that Calamancy outperforms FILLM in several aspects,
demonstrating its effectiveness in processing Filipino text with improved
linguistic comprehension and adaptability. This research contributes to the
advancement of Filipino NLP applications by providing an optimized, efficient,
and scalable language model tailored for local linguistic needs.

</details>


### [552] [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,Bernard Ghanem,George Turkiyyah*

Main category: cs.CL

TL;DR: Large language models (LLMs) usually refuse harmful instructions to follow safety guidelines. A recent attack called abliteration weakens this refusal behavior, but the paper proposes a defense method that fine-tunes LLMs on an extended-refusal dataset, which helps maintain high refusal rates and neutralizes the abliteration attack while preserving general performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to defend against the abliteration attack which enables LLMs to generate unethical content by suppressing their refusal behavior.

Method: The method involves constructing an extended-refusal dataset with harmful prompts and full responses justifying refusals, then fine-tuning LLMs (Llama-2-7B-Chat and Qwen2.5-Instruct) on this dataset.

Result: In experiments, the extended-refusal models maintained high refusal rates (dropping at most by 10%), compared to baseline models whose refusal rates dropped by 70-80% after abliteration.

Conclusion: Extended-refusal fine-tuning effectively neutralizes the abliteration attack while preserving the general performance and safety of the models.

Abstract: Large language models (LLMs) are typically aligned to comply with safety
guidelines by refusing harmful instructions. A recent attack, termed
abliteration, isolates and suppresses the single latent direction most
responsible for refusal behavior, enabling the model to generate unethical
content. We propose a defense that modifies how models generate refusals. We
construct an extended-refusal dataset that contains harmful prompts with a full
response that justifies the reason for refusal. We then fine-tune
Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our
extended-refusal dataset, and evaluate the resulting systems on a set of
harmful prompts. In our experiments, extended-refusal models maintain high
refusal rates, dropping at most by 10%, whereas baseline models' refusal rates
drop by 70-80% after abliteration. A broad evaluation of safety and utility
shows that extended-refusal fine-tuning neutralizes the abliteration attack
while preserving general performance.

</details>


### [553] [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)
*Benjamin Clavié,Florian Brand*

Main category: cs.CL

TL;DR: 近期大型视觉-语言模型(VLMs)在联合处理文本和图像方面取得了显著进展。然而，评估VLMs对图文内容理解能力的基准测试较少。为此，我们提出了ReadBench，一个专门评估VLMs阅读理解能力的多模态基准。实验发现，对于短文本输入，性能略有下降，而对于长文本、多页内容，性能显著下降。此外，文本分辨率对多模态性能影响较小。这些结果表明VLMs需要改进其对大量可视文本内容的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管目前有大量关于视觉理解的基准测试，但针对VLMs在图文内容阅读和推理方面的评估仍然不足。因此，有必要设计一个新的基准来全面评估VLMs的阅读理解能力。

Method: 构建了一个名为ReadBench的多模态基准测试，它将现有的纯文本基准转换为包含文本的图像，同时保留了原始的文本提示和问题。通过该基准，可以系统地评估VLMs在不同长度和分辨率下的文本图像处理能力。

Result: 实验结果显示，VLMs在处理短文本输入时表现出轻微的性能下降，但在处理较长、多页文本时性能急剧下降。此外，文本分辨率对性能的影响很小。

Conclusion: VLMs在处理大量可视化的文本内容时存在明显的不足，特别是在推理长文本方面。未来的研究应着重提高VLMs对复杂图文内容的理解和推理能力。

Abstract: Recent advancements in Large Vision-Language Models (VLMs), have greatly
enhanced their capability to jointly process text and images. However, despite
extensive benchmarks evaluating visual comprehension (e.g., diagrams, color
schemes, OCR tasks...), there is limited assessment of VLMs' ability to read
and reason about text-rich images effectively. To fill this gap, we introduce
ReadBench, a multimodal benchmark specifically designed to evaluate the reading
comprehension capabilities of VLMs. ReadBench transposes contexts from
established text-only benchmarks into images of text while keeping textual
prompts and questions intact. Evaluating leading VLMs with ReadBench, we find
minimal-but-present performance degradation on short, text-image inputs, while
performance sharply declines for longer, multi-page contexts. Our experiments
further reveal that text resolution has negligible effects on multimodal
performance. These findings highlight needed improvements in VLMs, particularly
their reasoning over visually presented extensive textual content, a capability
critical for practical applications. ReadBench is available at
https://github.com/answerdotai/ReadBench .

</details>


### [554] [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)
*Yongheng Zhang,Xu Liu,Ruoxi Zhou,Qiguang Chen,Hao Fei,Wenpeng Lu,Libo Qin*

Main category: cs.CL

TL;DR: 研究大型语言模型（LLMs）在跨语言和跨模态场景中的幻觉问题，对现实世界应用的大规模部署至关重要。当前研究局限于单一场景，因此本文提出一个新基准CCHall，结合了跨语言和跨模态幻觉场景，用以评估LLMs的综合能力。实验结果表明，现有LLMs在CCHall上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前关于大型语言模型幻觉问题的研究仅限于单一场景（跨语言或跨模态），缺乏对联合跨语言和跨模态场景中幻觉问题的探索。

Method: 引入了一个新的联合跨语言和跨模态幻觉基准（CCHall），同时包含跨语言和跨模态幻觉场景，并使用该基准对主流开源和闭源LLMs进行综合评估。

Result: 实验结果表明，当前的LLMs在CCHall基准上表现不佳，说明它们在联合跨语言和跨模态场景中仍存在困难。

Conclusion: CCHall可以作为一个有价值的资源，用于评估LLMs在联合跨语言和跨模态场景中的表现。

Abstract: Investigating hallucination issues in large language models (LLMs) within
cross-lingual and cross-modal scenarios can greatly advance the large-scale
deployment in real-world applications. Nevertheless, the current studies are
limited to a single scenario, either cross-lingual or cross-modal, leaving a
gap in the exploration of hallucinations in the joint cross-lingual and
cross-modal scenarios. Motivated by this, we introduce a novel joint
Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this
gap. Specifically, CCHall simultaneously incorporates both cross-lingual and
cross-modal hallucination scenarios, which can be used to assess the
cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a
comprehensive evaluation on CCHall, exploring both mainstream open-source and
closed-source LLMs. The experimental results highlight that current LLMs still
struggle with CCHall. We hope CCHall can serve as a valuable resource to assess
LLMs in joint cross-lingual and cross-modal scenarios.

</details>


### [555] [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)
*Jin Zhang,Fan Gao,Linyu Li,Yongbin Yu,Xiangxiang Wang,Nyima Tashi,Gadeng Luosang*

Main category: cs.CL

TL;DR: The paper introduces RetrieveAll, a dynamic LoRA-based multilingual NER framework that reduces language interference and enhances performance for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of severe language interference in existing multilingual NER methods, especially feature conflicts and suppression of low-resource languages during multi-language adaptation.

Method: Proposes RetrieveAll, which uses dynamic LoRA to decouple task-specific features across languages and introduces a cross-granularity knowledge augmented method with hierarchical prompting for 'prompt-driven learning'.

Result: RetrieveAll outperforms existing baselines, achieving an average F1 improvement of 12.1 percent on the PAN-X dataset.

Conclusion: RetrieveAll is a scalable and efficient solution for multilingual NER that mitigates language interference and leverages data potential without external resources.

Abstract: The rise of large language models has led to significant performance
breakthroughs in named entity recognition (NER) for high-resource languages,
yet there remains substantial room for improvement in low- and medium-resource
languages. Existing multilingual NER methods face severe language interference
during the multi-language adaptation process, manifested in feature conflicts
between different languages and the competitive suppression of low-resource
language features by high-resource languages. Although training a dedicated
model for each language can mitigate such interference, it lacks scalability
and incurs excessive computational costs in real-world applications. To address
this issue, we propose RetrieveAll, a universal multilingual NER framework
based on dynamic LoRA. The framework decouples task-specific features across
languages and demonstrates efficient dynamic adaptability. Furthermore, we
introduce a cross-granularity knowledge augmented method that fully exploits
the intrinsic potential of the data without relying on external resources. By
leveraging a hierarchical prompting mechanism to guide knowledge injection,
this approach advances the paradigm from "prompt-guided inference" to
"prompt-driven learning." Experimental results show that RetrieveAll
outperforms existing baselines; on the PAN-X dataset, it achieves an average F1
improvement of 12.1 percent.

</details>


### [556] [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
*Xuyang Liu,Zichen Wen,Shaobo Wang,Junjie Chen,Zhishan Tao,Yubo Wang,Xiangqi Jin,Chang Zou,Yiyu Wang,Chenfei Liao,Xu Zheng,Honggang Chen,Weijia Li,Xuming Hu,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 随着模型规模接近硬件限制，研究重点从模型中心的压缩转向数据中心的压缩，特别是token压缩成为提高AI效率的新前沿。本文分析了token压缩的优势、挑战及未来方向，为解决长上下文带来的问题提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多模态模型的性能提升传统上依赖于增加参数量，但现在面临硬件限制和自注意力机制在长序列上的二次成本问题。因此，需要寻找新的方法来提高AI效率。

Method: 1. 分析长上下文AI在各领域的进展，并建立统一的数学框架解释现有模型效率策略。2. 系统回顾token压缩的研究现状，分析其基本优势和应用场景。3. 深入探讨当前token压缩研究中的挑战，并提出未来可能的研究方向。

Result: 阐明了token压缩作为提高AI效率的关键范式转变的重要性，揭示了其在不同场景下的显著优势，并明确了当前研究中的主要挑战。

Conclusion: 本文提供了关于AI效率的新视角，综合了现有的研究成果，并激发了创新的发展方向，以应对长上下文对AI社区进步带来的挑战。

Abstract: The rapid advancement of large language models (LLMs) and multi-modal LLMs
(MLLMs) has historically relied on model-centric scaling through increasing
parameter counts from millions to hundreds of billions to drive performance
gains. However, as we approach hardware limits on model size, the dominant
computational bottleneck has fundamentally shifted to the quadratic cost of
self-attention over long token sequences, now driven by ultra-long text
contexts, high-resolution images, and extended videos. In this position paper,
\textbf{we argue that the focus of research for efficient AI is shifting from
model-centric compression to data-centric compression}. We position token
compression as the new frontier, which improves AI efficiency via reducing the
number of tokens during model training or inference. Through comprehensive
analysis, we first examine recent developments in long-context AI across
various domains and establish a unified mathematical framework for existing
model efficiency strategies, demonstrating why token compression represents a
crucial paradigm shift in addressing long-context overhead. Subsequently, we
systematically review the research landscape of token compression, analyzing
its fundamental benefits and identifying its compelling advantages across
diverse scenarios. Furthermore, we provide an in-depth analysis of current
challenges in token compression research and outline promising future
directions. Ultimately, our work aims to offer a fresh perspective on AI
efficiency, synthesize existing research, and catalyze innovative developments
to address the challenges that increasing context lengths pose to the AI
community's advancement.

</details>


### [557] [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)
*Firoj Alam,Md Arid Hasan,Shammur Absar Chowdhury*

Main category: cs.CL

TL;DR: This paper presents SpokenNativQA, a multilingual spoken question-answering dataset to evaluate LLMs in real-world conversational settings. It contains 33,000 spoken questions and answers in multiple languages including low-resource and dialect-rich ones. The study benchmarks ASR systems and LLMs for SQA.


<details>
  <summary>Details</summary>
Motivation: Benchmarking the capabilities of Large Language Models with multilingual spoken queries is largely unexplored.

Method: Introduction of SpokenNativQA, a dataset with approximately 33,000 naturally spoken questions and answers in multiple languages including low-resource and dialect-rich languages. This dataset is used to assess LLM performance in speech-based interactions and address limitations of text-based QA datasets.

Result: The dataset provides a robust benchmark for assessing LLM performance in speech-based interactions by incorporating speech variability, accents, and linguistic diversity. Findings from benchmarking different ASR systems and LLMs for SQA are presented.

Conclusion: SpokenNativQA addresses the limitations of text-based QA datasets and offers a valuable resource for evaluating LLMs in real-world conversational settings.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various disciplines and tasks. However, benchmarking their capabilities with
multilingual spoken queries remains largely unexplored. In this study, we
introduce SpokenNativQA, the first multilingual and culturally aligned spoken
question-answering (SQA) dataset designed to evaluate LLMs in real-world
conversational settings. The dataset comprises approximately 33,000 naturally
spoken questions and answers in multiple languages, including low-resource and
dialect-rich languages, providing a robust benchmark for assessing LLM
performance in speech-based interactions. SpokenNativQA addresses the
limitations of text-based QA datasets by incorporating speech variability,
accents, and linguistic diversity. We benchmark different ASR systems and LLMs
for SQA and present our findings. We released the data at
(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental
scripts at (https://llmebench.qcri.org/) for the research community.

</details>


### [558] [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)
*Minh Nhat Nguyen,Pradyumna Shyama Prasad*

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLMs）在静态问答任务中表现良好，但在动态对抗性辩论环境中，它们表现出系统性的过度自信、信心升级、相互高估、自我辩论偏差和私人推理不一致等问题。这表明LLMs在动态、多轮任务中缺乏准确的自我评估和信念更新能力，可能影响其在代理或助手角色中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大型语言模型（LLMs）在面对反对意见时是否能够准确调整其置信度。基于之前对静态事实问答任务的校准测量研究，本文试图评估LLMs在动态对抗性辩论环境中的表现，结合多轮次信息更新和零和结构控制任务相关不确定性这两个现实因素。

Method: 研究人员组织了60场三轮政策辩论，涉及十种最先进的LLMs。这些模型在每一轮后私下对其获胜的信心进行评分（0-100）。通过这种设置，研究者观察LLMs在辩论过程中的信心变化模式，并分析其自我评估和信念更新的能力。

Result: 研究发现五个令人担忧的模式：(1)系统性过度自信；(2)信心升级；(3)相互高估；(4)持续的自我辩论偏差；(5)私人推理与公共信心评级不一致。这些结果表明LLMs难以在动态多轮任务中准确地自我评估和更新信念。

Conclusion: LLMs在动态、多轮任务中缺乏准确自我评估和更新信念的能力，这可能成为其在助手角色或代理设置中部署的一个主要问题。

Abstract: Can LLMs accurately adjust their confidence when facing opposition? Building
on previous studies measuring calibration on static fact-based
question-answering tasks, we evaluate Large Language Models (LLMs) in a
dynamic, adversarial debate setting, uniquely combining two realistic factors:
(a) a multi-turn format requiring models to update beliefs as new information
emerges, and (b) a zero-sum structure to control for task-related uncertainty,
since mutual high-confidence claims imply systematic overconfidence. We
organized 60 three-round policy debates among ten state-of-the-art LLMs, with
models privately rating their confidence (0-100) in winning after each round.
We observed five concerning patterns: (1) Systematic overconfidence: models
began debates with average initial confidence of 72.9% vs. a rational 50%
baseline. (2) Confidence escalation: rather than reducing confidence as debates
progressed, debaters increased their win probabilities, averaging 83% by the
final round. (3) Mutual overestimation: in 61.7% of debates, both sides
simultaneously claimed >=75% probability of victory, a logical impossibility.
(4) Persistent self-debate bias: models debating identical copies increased
confidence from 64.1% to 75.2%; even when explicitly informed their chance of
winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)
Misaligned private reasoning: models' private scratchpad thoughts sometimes
differed from their public confidence ratings, raising concerns about
faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the
ability to accurately self-assess or update their beliefs in dynamic,
multi-turn tasks; a major concern as LLM outputs are deployed without careful
review in assistant roles or agentic settings.

</details>


### [559] [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)
*Yang Xiao,Jiashuo Wang,Ruifeng Yuan,Chunpu Xu,Kaishuai Xu,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 大型语言模型在测试时推理过程中存在冗长的问题，本文提出了一种基于困惑度的重要度优化框架PIR，该框架能减少推理链中的非必要部分，同时保持核心推理路径的完整性。通过PIR优化的数据微调后的模型在多个推理基准上表现出更高的准确性和更少的令牌使用量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然具备强大的推理能力，但其推理链条中包含大量冗长的内容，增加了计算需求。因此需要一种方法来优化推理链条，减少冗长内容以提高效率。

Method: 提出了一种名为PIR（Perplexity-based Importance Refinement）的框架，该框架根据推理步骤对答案预测置信度的影响量化评估每个推理步骤的重要性，并系统地识别和选择性地修剪低重要性的功能步骤，从而保留渐进推理组件并创建优化的训练数据。

Result: 通过PIR优化数据微调的模型在AIME、AMC和GPQA Diamond等多个推理基准上表现出了更高的准确性（+0.9%到+6.6%），并且显著减少了令牌使用量（-3%到-41%）。

Conclusion: PIR提供了一个实用的解决方案，用于部署推理能力强的大型语言模型，特别是在需要高效测试时间扩展、响应时间和计算效率的情况下。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities through test-time scaling approaches, particularly when fine-tuned
with chain-of-thought (CoT) data distilled from more powerful large reasoning
models (LRMs). However, these reasoning chains often contain verbose elements
that mirror human problem-solving, categorized as progressive reasoning (the
essential solution development path) and functional elements (verification
processes, alternative solution approaches, and error corrections). While
progressive reasoning is crucial, the functional elements significantly
increase computational demands during test-time inference. We introduce PIR
(Perplexity-based Importance Refinement), a principled framework that
quantitatively evaluates the importance of each reasoning step based on its
impact on answer prediction confidence. PIR systematically identifies and
selectively prunes only low-importance functional steps while preserving
progressive reasoning components, creating optimized training data that
maintains the integrity of the core solution path while reducing verbosity.
Models fine-tuned on PIR-optimized data exhibit superior test-time scaling
properties, generating more concise reasoning chains while achieving improved
accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to
-41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).
Our approach demonstrates strong generalizability across different model sizes,
data sources, and token budgets, offering a practical solution for deploying
reasoning-capable LLMs in scenarios where efficient test-time scaling, response
time, and computational efficiency are valuable constraints.

</details>


### [560] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
*Jarrod Ragsdale,Rajendra Boppana*

Main category: cs.CL

TL;DR: The paper introduces Shell Input-Output Environment (ShIOEnv) that models command-line interactions as a Markov Decision Process, using grammar masking and proximal-policy optimization (PPO) to improve sample efficiency and generate high-quality datasets. Fine-tuning CodeT5 with these datasets led to significant improvements in BLEU-4 scores.


<details>
  <summary>Details</summary>
Motivation: Existing methods for simulating command-line interfaces (CLIs) using pre-trained language models are limited by the lack of rich execution data in public datasets, which affects their usability for behavioral modeling.

Method: The authors developed ShIOEnv, casting command construction as a Markov Decision Process. They derived a context-free grammar from man pages to mask invalid arguments, explored random and PPO-optimized sampling strategies, and used policy-generated datasets to fine-tune CodeT5.

Result: Grammar masking and PPO significantly improved sample efficiency and dataset quality. Fine-tuning CodeT5 resulted in an 85% improvement in BLEU-4 scores with grammar production constraints and an additional 26% improvement with PPO.

Conclusion: ShIOEnv provides a novel approach for generating high-quality CLI interaction datasets, leading to better performance in language models when fine-tuned with these datasets.

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [561] [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)
*Zonglin Yang,Wanhao Liu,Ben Gao,Yujie Liu,Wei Li,Tong Xie,Lidong Bing,Wanli Ouyang,Erik Cambria,Dongzhan Zhou*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在自动化科学假设生成方面表现出潜力，但现有方法主要产生缺乏关键方法论和实验细节的粗粒度假设。本文介绍了一种新的细粒度科学假设发现任务，并提出了一种分层搜索方法来优化假设生成过程。实验结果表明，该方法在新基准上优于强基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在科学假设生成方面有潜力，但目前的方法生成的假设较为粗略，缺乏必要的细节。因此，需要一种新的方法来生成更详细、更具实验可行性的假设。

Method: 作者将细粒度科学假设发现任务定义为组合优化问题，并提出了一种分层搜索方法。该方法通过逐步提出并整合假设细节，从一般概念到具体实验配置，从而平滑奖励景观并实现更有效的优化。

Result: 在由专家注释的化学文献细粒度假设新基准上的实证评估显示，所提出的方法一致优于强大的基线方法。

Conclusion: 分层搜索方法可以有效优化假设生成过程，并且在生成详细、实验可行的科学假设方面具有优势。

Abstract: Large language models (LLMs) have shown promise in automating scientific
hypothesis generation, yet existing approaches primarily yield coarse-grained
hypotheses lacking critical methodological and experimental details. We
introduce and formally define the novel task of fine-grained scientific
hypothesis discovery, which entails generating detailed, experimentally
actionable hypotheses from coarse initial research directions. We frame this as
a combinatorial optimization problem and investigate the upper limits of LLMs'
capacity to solve it when maximally leveraged. Specifically, we explore four
foundational questions: (1) how to best harness an LLM's internal heuristics to
formulate the fine-grained hypothesis it itself would judge as the most
promising among all the possible hypotheses it might generate, based on its own
internal scoring-thus defining a latent reward landscape over the hypothesis
space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment
with ground-truth hypotheses; (3) whether shaping the reward landscape using an
ensemble of diverse LLMs of similar capacity yields better outcomes than
defining it with repeated instances of the strongest LLM among them; and (4)
whether an ensemble of identical LLMs provides a more reliable reward landscape
than a single LLM. To address these questions, we propose a hierarchical search
method that incrementally proposes and integrates details into the hypothesis,
progressing from general concepts to specific experimental configurations. We
show that this hierarchical process smooths the reward landscape and enables
more effective optimization. Empirical evaluations on a new benchmark of
expert-annotated fine-grained hypotheses from recent chemistry literature show
that our method consistently outperforms strong baselines.

</details>


### [562] [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)
*Steffen Backmann,David Guzman Piedrahita,Emanuel Tewolde,Rada Mihalcea,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.CL

TL;DR: Recent advances in LLMs raise ethical alignment concerns. This paper introduces MoralSim to evaluate how LLMs behave in social dilemmas with morally charged contexts. Results show substantial variation across models and highlight the need for caution when deploying LLMs in agentic roles.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs act when moral imperatives directly conflict with rewards or incentives, as there is limited understanding of this aspect.

Method: Introduced MoralSim, which tests LLMs' behavior in prisoner's dilemma and public goods game with morally charged contexts. Tested frontier models across game structures and three distinct moral framings.

Result: Substantial variation across models in their tendency to act morally and behavior consistency. No model exhibits consistently moral behavior in MoralSim.

Conclusion: Caution is needed when deploying LLMs in agentic roles where self-interest may conflict with ethical expectations.

Abstract: Recent advances in large language models (LLMs) have enabled their use in
complex agentic roles, involving decision-making with humans or other agents,
making ethical alignment a key AI safety concern. While prior work has examined
both LLMs' moral judgment and strategic behavior in social dilemmas, there is
limited understanding of how they act when moral imperatives directly conflict
with rewards or incentives. To investigate this, we introduce Moral Behavior in
Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the
prisoner's dilemma and public goods game with morally charged contexts. In
MoralSim, we test a range of frontier models across both game structures and
three distinct moral framings, enabling a systematic examination of how LLMs
navigate social dilemmas in which ethical norms conflict with payoff-maximizing
strategies. Our results show substantial variation across models in both their
general tendency to act morally and the consistency of their behavior across
game types, the specific moral framing, and situational factors such as
opponent behavior and survival risks. Crucially, no model exhibits consistently
moral behavior in MoralSim, highlighting the need for caution when deploying
LLMs in agentic roles where the agent's "self-interest" may conflict with
ethical expectations. Our code is available at
https://github.com/sbackmann/moralsim.

</details>


### [563] [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
*Aida Kostikova,Zhipin Wang,Deidamea Bajri,Ole Pütz,Benjamin Paaßen,Steffen Eger*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的研究迅速增长，但其局限性如推理失败、幻觉和多语言能力有限等问题也备受关注。本文通过自下而上的方法对2022年至2024年关于LLM局限性的研究进行数据驱动、半自动化的综述。从25万篇ACL和arXiv论文中筛选出14,648篇相关论文，发现LLM相关研究在ACL上增长了五倍以上，在arXiv上增长了四倍。自2022年以来，关于LLM局限性的研究增长更快，到2024年底达到了LLM论文的30%以上。推理仍然是研究最多的局限性，其次是泛化、幻觉、偏差和安全性。ACL数据集中的主题分布随时间相对稳定，而arXiv则转向安全性和可控性以及多模态。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，对其局限性的研究需求增加，以更好地理解其缺陷并改进模型性能。

Method: 使用关键词过滤、基于LLM的分类、专家标签验证和主题聚类（HDBSCAN+BERTopic和LlooM方法）从25万篇论文中识别相关文献，并分析其主题和趋势。

Result: 发现LLM相关研究大幅增长，其中关于局限性的研究比例显著增加，不同平台上的研究重点有所不同。

Conclusion: 提供了关于LLM局限性研究的趋势的定量视角，并发布了注释数据集和验证方法论。

Abstract: Large language model (LLM) research has grown rapidly, along with increasing
concern about their limitations such as failures in reasoning, hallucinations,
and limited multilingual capability. In this survey, we conduct a data-driven,
semi-automated review of research on limitations of LLM (LLLMs) from 2022 to
2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,
we identify 14,648 relevant papers using keyword filtering, LLM-based
classification, validated against expert labels, and topic clustering (via two
approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research
increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs
research grows even faster, reaching over 30% of LLM papers by late 2024.
Reasoning remains the most studied limitation, followed by generalization,
hallucination, bias, and security. The distribution of topics in the ACL
dataset stays relatively stable over time, while arXiv shifts toward safety and
controllability (with topics like security risks, alignment, hallucinations,
knowledge editing), and multimodality between 2022 and 2024. We release a
dataset of annotated abstracts and a validated methodology, and offer a
quantitative view of trends in LLM limitations research.

</details>


### [564] [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)
*Yue Jiang,Jichu Li,Yang Liu,Dingkang Yang,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: The paper introduces DanmakuTPPBench, a benchmark for multi-modal Temporal Point Process modeling in the era of Large Language Models. It includes two components: DanmakuTPP-Events and DanmakuTPP-QA datasets. The authors conduct extensive evaluations revealing significant performance gaps in current methods.


<details>
  <summary>Details</summary>
Motivation: To advance multi-modal Temporal Point Process (TPP) modeling by addressing the lack of comprehensive datasets that incorporate temporal, textual, and visual information jointly.

Method: Development of DanmakuTPPBench comprising DanmakuTPP-Events, a dataset with multi-modal events from Bilibili, and DanmakuTPP-QA, a question-answering dataset created using a multi-agent pipeline powered by LLMs and MLLMs.

Result: Significant performance gaps were found in current methods' ability to model multi-modal event dynamics through extensive evaluations using classical TPP models and recent MLLMs.

Conclusion: DanmakuTPPBench establishes strong baselines and highlights the need for further integration of TPP modeling into the multi-modal language modeling landscape.

Abstract: We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance
multi-modal Temporal Point Process (TPP) modeling in the era of Large Language
Models (LLMs). While TPPs have been widely studied for modeling temporal event
sequences, existing datasets are predominantly unimodal, hindering progress in
models that require joint reasoning over temporal, textual, and visual
information. To address this gap, DanmakuTPPBench comprises two complementary
components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili
video platform, where user-generated bullet comments (Danmaku) naturally form
multi-modal events annotated with precise timestamps, rich textual content, and
corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering
dataset constructed via a novel multi-agent pipeline powered by
state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex
temporal-textual-visual reasoning. We conduct extensive evaluations using both
classical TPP models and recent MLLMs, revealing significant performance gaps
and limitations in current methods' ability to model multi-modal event
dynamics. Our benchmark establishes strong baselines and calls for further
integration of TPP modeling into the multi-modal language modeling landscape.
The code and dataset have been released at
https://github.com/FRENKIE-CHIANG/DanmakuTPPBench

</details>


### [565] [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)
*Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.CL

TL;DR: The paper introduces Anchored Diffusion Language Model (ADLM), a two-stage framework that improves upon standard Diffusion Language Models (DLMs) by first predicting distributions over important tokens and then predicting missing tokens conditioned on these predictions. This results in significant performance improvements, achieving state-of-the-art zero-shot generalization and generating more human-like text than autoregressive models.


<details>
  <summary>Details</summary>
Motivation: Diffusion Language Models underperform autoregressive models in likelihood modeling and generated text quality due to the early masking of important tokens which limits contextual information for accurate reconstruction.

Method: The method involves a two-stage framework: first, an anchor network predicts distributions over important tokens; second, it predicts the likelihoods of missing tokens conditioned on the anchored predictions. Theoretically, they derive an Anchored Negative Evidence Lower Bound (ANELBO) objective.

Result: ADLM significantly improves test perplexity on LM1B and OpenWebText, achieving up to 25.4% gains over prior DLMs, narrows the gap with strong AR baselines, achieves state-of-the-art zero-shot generalization across seven benchmarks, and surpasses AR models in MAUVE score.

Conclusion: Anchoring improves sample complexity and likelihood modeling not only in diffusion models but also boosts performance in autoregressive models and enhances reasoning in math and logic tasks.

Abstract: Diffusion Language Models (DLMs) promise parallel generation and
bidirectional context, yet they underperform autoregressive (AR) models in both
likelihood modeling and generated text quality. We identify that this
performance gap arises when important tokens (e.g., key words or low-frequency
words that anchor a sentence) are masked early in the forward process, limiting
contextual information for accurate reconstruction. To address this, we
introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage
framework that first predicts distributions over important tokens via an anchor
network, and then predicts the likelihoods of missing tokens conditioned on the
anchored predictions. ADLM significantly improves test perplexity on LM1B and
OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap
with strong AR baselines. It also achieves state-of-the-art performance in
zero-shot generalization across seven benchmarks and surpasses AR models in
MAUVE score, which marks the first time a DLM generates better human-like text
than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower
Bound (ANELBO) objective and show that anchoring improves sample complexity and
likelihood modeling. Beyond diffusion, anchoring boosts performance in AR
models and enhances reasoning in math and logic tasks, outperforming existing
chain-of-thought approaches

</details>


### [566] [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)
*Hong Jiao,Dan Song,Won-Chan Lee*

Main category: cs.CL

TL;DR: This paper compares ten large language models (LLMs) with human expert raters in scoring two types of writing tasks, finding that ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet have high accuracy, better reliability, and fewer rater effects.


<details>
  <summary>Details</summary>
Motivation: To collect empirical evidence on which LLMs produce the most reliable scores and induce least rater effects before practical application of LLMs for automated scoring in low-stakes assessment.

Method: Compared ten LLMs with human expert raters in scoring two types of writing tasks, evaluating the accuracy of holistic and analytic scores using Quadratic Weighted Kappa, comparing intra-rater consistency across prompts using Cronbach Alpha, and evaluating and comparing rater effects using the Many-Facet Rasch model.

Result: ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet were found to have high scoring accuracy, better rater reliability, and fewer rater effects.

Conclusion: The study supports the use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet for automated scoring in low-stakes assessment.

Abstract: Large language models (LLMs) have been widely explored for automated scoring
in low-stakes assessment to facilitate learning and instruction. Empirical
evidence related to which LLM produces the most reliable scores and induces
least rater effects needs to be collected before the use of LLMs for automated
scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,
ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini
2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in
scoring two types of writing tasks. The accuracy of the holistic and analytic
scores from LLMs compared with human raters was evaluated in terms of Quadratic
Weighted Kappa. Intra-rater consistency across prompts was compared in terms of
Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human
raters using the Many-Facet Rasch model. The results in general supported the
use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring
accuracy, better rater reliability, and less rater effects.

</details>


### [567] [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)
*Wang Yang,Hongye Jin,Shaochen Zhong,Song Jiang,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的长度可控的长上下文基准测试和一种区分基础能力和真实长上下文能力的新度量标准，解决了现有基准测试的不足，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文评估基准存在两个主要问题：1）无法将模型的基础能力与长上下文能力区分开来；2）输入长度固定，限制了对不同模型的应用和分析。因此需要改进这些基准以更好地评估LLM的长上下文能力。

Method: 引入了一个长度可控的长上下文基准和一个新的度量标准，该度量可以分离模型的基础知识和真正的长上下文能力。

Result: 实验结果表明，所提出的方法在有效评估LLM方面具有优越性。

Conclusion: 新提出的长度可控的长上下文基准和度量标准能够更清晰地评估LLM的长上下文能力，并揭示模型何时开始失效。

Abstract: Long-context capability is considered one of the most important abilities of
LLMs, as a truly long context-capable LLM enables users to effortlessly process
many originally exhausting tasks -- e.g., digesting a long-form document to
find answers vs. directly asking an LLM about it. However, existing
real-task-based long-context evaluation benchmarks have two major shortcomings.
First, benchmarks like LongBench often do not provide proper metrics to
separate long-context performance from the model's baseline ability, making
cross-model comparison unclear. Second, such benchmarks are usually constructed
with fixed input lengths, which limits their applicability across different
models and fails to reveal when a model begins to break down. To address these
issues, we introduce a length-controllable long-context benchmark and a novel
metric that disentangles baseline knowledge from true long-context
capabilities. Experiments demonstrate the superiority of our approach in
effectively evaluating LLMs.

</details>


### [568] [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
*Lingjun Zhao,Hal Daumé III*

Main category: cs.CL

TL;DR: The paper introduces a measure for Prediction-EXplanation (PEX) consistency, revealing that over 62% of explanations from large language models lack this consistency. Direct preference optimization improves consistency by 43.1%-292.3%, enhancing explanation faithfulness by up to 9.7%.


<details>
  <summary>Details</summary>
Motivation: Faithful free-text explanations are crucial in high-stakes AI decision-making, yet they are difficult to generate and assess.

Method: A measure for PEX consistency is presented by extending the concept of weight of evidence. This quantifies how much an explanation supports or opposes a prediction. The method analyzes explanations from large language models and applies direct preference optimization to improve consistency.

Result: Analysis shows that more than 62% of generated explanations lack consistency. Applying direct preference optimization improves consistency across model families with significant percentage increases. Optimizing this measure can enhance explanation faithfulness by up to 9.7%.

Conclusion: The introduced PEX consistency measure effectively identifies lack of consistency in explanations from large language models. Direct preference optimization significantly enhances this consistency, leading to improved explanation faithfulness.

Abstract: Faithful free-text explanations are important to ensure transparency in
high-stakes AI decision-making contexts, but they are challenging to generate
by language models and assess by humans. In this paper, we present a measure
for Prediction-EXplanation (PEX) consistency, by extending the concept of
weight of evidence. This measure quantifies how much a free-text explanation
supports or opposes a prediction, serving as an important aspect of explanation
faithfulness. Our analysis reveals that more than 62% explanations generated by
large language models lack this consistency. We show that applying direct
preference optimization improves the consistency of generated explanations
across three model families, with improvement ranging from 43.1% to 292.3%.
Furthermore, we demonstrate that optimizing this consistency measure can
improve explanation faithfulness by up to 9.7%.

</details>


### [569] [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
*Yongmin Yoo,Qiongkai Xu,Longbing Cao*

Main category: cs.CL

TL;DR: PatentScore is a new framework to evaluate LLM-generated patent claims with high correlation to expert annotations.


<details>
  <summary>Details</summary>
Motivation: Current NLG metrics are not suitable for evaluating the specific structural and legal features of patent documents, particularly the crucial patent claims.

Method: PatentScore includes hierarchical decomposition for claim analysis, domain-specific validation patterns based on legal and technical standards, and scoring across structural, semantic, and legal dimensions.

Result: PatentScore evaluated 400 GPT-4o-mini generated Claim 1s with a Pearson correlation of r = 0.819 to expert annotations, surpassing existing NLG metrics. Evaluations using other models like Claude-3.5-Haiku and Gemini-1.5-flash also showed strong correlations.

Conclusion: PatentScore effectively addresses the gap in evaluating LLM-generated patent claims by incorporating patent-specific constraints and structures.

Abstract: Natural language generation (NLG) metrics play a central role in evaluating
generated texts, but are not well suited for the structural and legal
characteristics of patent documents. Large language models (LLMs) offer strong
potential in automating patent generation, yet research on evaluating
LLM-generated patents remains limited, especially in evaluating the generation
quality of patent claims, which are central to defining the scope of
protection. Effective claim evaluation requires addressing legal validity,
technical accuracy, and structural compliance. To address this gap, we
introduce PatentScore, a multi-dimensional evaluation framework for assessing
LLM-generated patent claims. PatentScore incorporates: (1) hierarchical
decomposition for claim analysis; (2) domain-specific validation patterns based
on legal and technical standards; and (3) scoring across structural, semantic,
and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects
patent-specific constraints and document structures, enabling evaluation beyond
surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a
Pearson correlation of $r = 0.819$ with expert annotations, outperforming
existing NLG metrics. Furthermore, we conduct additional evaluations using open
models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong
correlations with expert judgments, confirming the robustness and
generalizability of our framework.

</details>


### [570] [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
*Woohyun Cho,Youngmin Kim,Sunghyun Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: This paper presents MAVL, a new benchmark for animated song translation, and SylAVL-CoT, a model that uses audio-video cues and syllabic constraints to create natural-sounding lyrics. Experiments show its superiority in singability and contextual accuracy.


<details>
  <summary>Details</summary>
Motivation: Lyrics translation needs accurate semantic transfer and preservation of musical rhythm, syllabic structure, and poetic style, especially in animated musicals where alignment with visual and auditory cues is crucial.

Method: Introduced MAVL, the first multilingual multimodal benchmark for singable lyrics translation, and proposed SylAVL-CoT which leverages audio-video cues and enforces syllabic constraints.

Result: SylAVL-CoT significantly outperforms text-based models in terms of singability and contextual accuracy.

Conclusion: Multimodal, multilingual approaches are valuable for lyrics translation.

Abstract: Lyrics translation requires both accurate semantic transfer and preservation
of musical rhythm, syllabic structure, and poetic style. In animated musicals,
the challenge intensifies due to alignment with visual and auditory cues. We
introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song
Translation (MAVL), the first multilingual, multimodal benchmark for singable
lyrics translation. By integrating text, audio, and video, MAVL enables richer
and more expressive translations than text-only approaches. Building on this,
we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought
SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints
to produce natural-sounding lyrics. Experimental results demonstrate that
SylAVL-CoT significantly outperforms text-based models in singability and
contextual accuracy, emphasizing the value of multimodal, multilingual
approaches for lyrics translation.

</details>


### [571] [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)
*Jade Robinson,Jonathan K. Kummerfeld*

Main category: cs.CL

TL;DR: 作者提出了一种新的代码总结评估方法，利用大型语言模型（LLM）对总结进行评分，这种方法可以考虑代码本身，并且在没有参考摘要的情况下也可以使用。该方法与之前的指标一样好或更好，但建议结合基于嵌入的方法以避免LLM特定的偏差。


<details>
  <summary>Details</summary>
Motivation: 编写代码文档耗时，而现有的代码总结生成技术缺乏有效的比较方式，因为人工评估成本高且自动指标不可靠。

Method: 引入一种新基线方法，要求大型语言模型（LLM）根据代码和总结给出整体评分，同时开发了一个不依赖参考摘要的变体用于其他任务如评估代码库中的文档质量。

Result: 实验表明，新方法的表现与先前的指标相当或更优。

Conclusion: 新提出的基于LLM的评估方法是一种有效的方式，但为了减少潜在的LLM特定偏差，建议将其与基于嵌入的方法结合使用。

Abstract: Code documentation is useful, but writing it is time-consuming. Different
techniques for generating code summaries have emerged, but comparing them is
difficult because human evaluation is expensive and automatic metrics are
unreliable. In this paper, we introduce a simple new baseline in which we ask
an LLM to give an overall score to a summary. Unlike n-gram and embedding-based
baselines, our approach is able to consider the code when giving a score. This
allows us to also make a variant that does not consider the reference summary
at all, which could be used for other tasks, e.g., to evaluate the quality of
documentation in code bases. We find that our method is as good or better than
prior metrics, though we recommend using it in conjunction with embedding-based
methods to avoid the risk of LLM-specific bias.

</details>


### [572] [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)
*Daniel J. Korchinski,Dhruva Karkada,Yasaman Bahri,Matthieu Wyart*

Main category: cs.CL

TL;DR: 单词嵌入模型如Word2Vec和GloVe展示出线性类比结构，这种结构可以通过基于属性相互作用的理论生成模型解释。


<details>
  <summary>Details</summary>
Motivation: 解释单词嵌入中线性类比结构的理论起源，并分析其相关特性。

Method: 引入一个理论生成模型，其中单词由二进制语义属性定义，共现概率从基于属性的相互作用中得出。

Result: 该模型能够解析地重现线性类比结构的出现，并自然解释特性(i)-(iv)。

Conclusion: 此模型提供了对每个额外嵌入维度作用的细粒度解析，并且对各种形式的噪声具有鲁棒性，与在Wikipedia上测量的共现统计和Mikolov等人的类比基准测试结果一致。

Abstract: Models such as Word2Vec and GloVe construct word embeddings based on the
co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The
resulting vectors $W_i$ not only group semantically similar words but also
exhibit a striking linear analogy structure -- for example, $W_{\text{king}} -
W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose
theoretical origin remains unclear. Previous observations indicate that this
analogy structure: (i) already emerges in the top eigenvectors of the matrix
$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more
eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are
included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and
(iv) persists even when all word pairs involved in a specific analogy relation
(e.g., king-queen, man-woman) are removed from the corpus. To explain these
phenomena, we introduce a theoretical generative model in which words are
defined by binary semantic attributes, and co-occurrence probabilities are
derived from attribute-based interactions. This model analytically reproduces
the emergence of linear analogy structure and naturally accounts for properties
(i)-(iv). It can be viewed as giving fine-grained resolution into the role of
each additional embedding dimension. It is robust to various forms of noise and
agrees well with co-occurrence statistics measured on Wikipedia and the analogy
benchmark introduced by Mikolov et al.

</details>


### [573] [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
*Wenyang Xiao,Haoyu Zhao,Lingxiao Huang*

Main category: cs.CL

TL;DR: 通过实验研究了多样性在上下文示例选择中的作用，发现在包括情感分类、数学和代码问题等任务中，具有多样性意识的选择方法可以提高性能并增强对分布外查询的鲁棒性。为此，提出了一个解释结合多样性好处的理论框架。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法集中于选择与查询最相似的示例，但示例选择中的多样性影响尚未被充分探索。

Method: 系统地通过一系列任务（从情感分类到更复杂的数学和代码问题）的实验，研究多样性在上下文示例选择中的作用，并引入一个理论框架来解释结合多样性的优势。

Result: 多样性意识的选择方法提高了模型性能，特别是在复杂任务（如数学和代码）上，并增强了对分布外查询的鲁棒性。

Conclusion: 多样性在上下文示例选择中起到重要作用，尤其是在复杂任务中，能提升性能和鲁棒性，理论框架进一步支持了这一发现。

Abstract: In-context learning (ICL) is a crucial capability of current large language
models (LLMs), where the selection of examples plays a key role in performance.
While most existing approaches focus on selecting the most similar examples to
the query, the impact of diversity in example selection remains underexplored.
We systematically investigate the role of diversity in in-context example
selection through experiments across a range of tasks, from sentiment
classification to more challenging math and code problems. Experiments on
Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that
diversity-aware selection methods improve performance, particularly on complex
tasks like math and code, and enhance robustness to out-of-distribution
queries. To support these findings, we introduce a theoretical framework that
explains the benefits of incorporating diversity in in-context example
selection.

</details>


### [574] [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
*Keane Ong,Rui Mao,Deeksha Varshney,Paul Pu Liang,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: This paper introduces Fin-Force, a new benchmark for forward counterfactual reasoning in financial markets using Large Language Models (LLMs). It evaluates current LLMs and counterfactual generation methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for automated solutions to perform forward counterfactual reasoning at scale in dynamic financial markets, which can help stakeholders anticipate risks and opportunities.

Method: The method involves creating a novel benchmark called Fin-Force that curates financial news headlines to support LLM-based forward counterfactual generation. Experiments are conducted on this benchmark to evaluate state-of-the-art LLMs and counterfactual generation methods.

Result: The results include an evaluation of current LLMs and counterfactual generation methods, identifying their limitations and providing insights for future research.

Conclusion: Fin-Force provides a structured approach for scalable and automated solutions to explore future market developments, offering valuable insights for decision-making in financial markets.

Abstract: Counterfactual reasoning typically involves considering alternatives to
actual events. While often applied to understand past events, a distinct
form-forward counterfactual reasoning-focuses on anticipating plausible future
developments. This type of reasoning is invaluable in dynamic financial
markets, where anticipating market developments can powerfully unveil potential
risks and opportunities for stakeholders, guiding their decision-making.
However, performing this at scale is challenging due to the cognitive demands
involved, underscoring the need for automated solutions. Large Language Models
(LLMs) offer promise, but remain unexplored for this application. To address
this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward
Counterfactual Evaluation. By curating financial news headlines and providing
structured evaluation, Fin-Force supports LLM based forward counterfactual
generation. This paves the way for scalable and automated solutions for
exploring and anticipating future market developments, thereby providing
structured insights for decision-making. Through experiments on Fin-Force, we
evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing
their limitations and proposing insights for future research.

</details>


### [575] [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)
*Ming Cheng,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CL

TL;DR: This paper introduces Sci-LoRA, a model designed for lay paraphrasing across multiple scientific domains. It dynamically adjusts domain impact without explicit labels and shows superior performance compared to existing models.


<details>
  <summary>Details</summary>
Motivation: To make scientific information accessible to non-technical audiences by addressing the limitation of current models focusing on single domains, especially with the rise of interdisciplinary research.

Method: Proposes Sci-LoRA, which uses a mixture of LoRAs fine-tuned on multiple scientific domains. It dynamically generates weights for each LoRA based on input text and integrates information at both data and model levels.

Result: Significantly outperforms state-of-the-art large language models in experiments across twelve domains using five public datasets, demonstrating flexible generalization and adaptability.

Conclusion: Sci-LoRA enhances adaptability and performance in cross-domain lay paraphrasing, making it a promising tool for making multi-domain scientific knowledge accessible.

Abstract: Lay paraphrasing aims to make scientific information accessible to audiences
without technical backgrounds. However, most existing studies focus on a single
domain, such as biomedicine. With the rise of interdisciplinary research, it is
increasingly necessary to comprehend knowledge spanning multiple technical
fields. To address this, we propose Sci-LoRA, a model that leverages a mixture
of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA
dynamically generates and applies weights for each LoRA, enabling it to adjust
the impact of different domains based on the input text, without requiring
explicit domain labels. To balance domain-specific knowledge and generalization
across various domains, Sci-LoRA integrates information at both the data and
model levels. This dynamic fusion enhances the adaptability and performance
across various domains. Experimental results across twelve domains on five
public datasets show that Sci-LoRA significantly outperforms state-of-the-art
large language models and demonstrates flexible generalization and adaptability
in cross-domain lay paraphrasing.

</details>


### [576] [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
*Yaoning Yu,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.CL

TL;DR: SIPDO is a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process to systematically improve prompt performance.


<details>
  <summary>Details</summary>
Motivation: Prompt quality significantly affects the performance of large language models (LLMs), and current methods have limitations in iterative improvement.

Method: SIPDO couples a synthetic data generator with a prompt optimizer. The generator produces new examples revealing current prompt weaknesses, while the optimizer incrementally refines the prompt.

Result: Experiments show SIPDO outperforms standard prompt tuning methods on question answering and reasoning benchmarks.

Conclusion: Integrating data synthesis into prompt learning workflows is valuable for improving prompt performance.

Abstract: Prompt quality plays a critical role in the performance of large language
models (LLMs), motivating a growing body of work on prompt optimization. Most
existing methods optimize prompts over a fixed dataset, assuming static input
distributions and offering limited support for iterative improvement. We
introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a
closed-loop framework for prompt learning that integrates synthetic data
generation into the optimization process. SIPDO couples a synthetic data
generator with a prompt optimizer, where the generator produces new examples
that reveal current prompt weaknesses and the optimizer incrementally refines
the prompt in response. This feedback-driven loop enables systematic
improvement of prompt performance without assuming access to external
supervision or new tasks. Experiments across question answering and reasoning
benchmarks show that SIPDO outperforms standard prompt tuning methods,
highlighting the value of integrating data synthesis into prompt learning
workflows.

</details>


### [577] [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
*Yejin Lee,Joonghyuk Hahn,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: AmpleHate is a new method for detecting implicit hate speech that mimics human inference by identifying targets and interpreting their context, showing top performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current methods for implicit hate speech detection rely on contrastive learning which may not fully capture the subtlety of such language. Humans detect implicit hate by focusing on specific targets and interpreting their contextual relationships, a process not yet mirrored in machine models.

Method: AmpleHate uses a pretrained Named Entity Recognition model to identify explicit targets and [CLS] tokens for implicit targets. It computes attention-based relationships between these targets and the sentence context, injecting relational vectors into the final sentence representation to amplify target-context signals.

Result: AmpleHate outperforms contrastive learning baselines with an average accuracy of 82.14%, shows faster convergence, and its attention patterns align well with human judgement, proving its interpretability and robustness.

Conclusion: AmpleHate achieves state-of-the-art results in implicit hate speech detection, demonstrating effectiveness and interpretability, closely resembling human reasoning processes.

Abstract: Implicit hate speech detection is challenging due to its subtlety and
reliance on contextual interpretation rather than explicit offensive words.
Current approaches rely on contrastive learning, which are shown to be
effective on distinguishing hate and non-hate sentences. Humans, however,
detect implicit hate speech by first identifying specific targets within the
text and subsequently interpreting how these target relate to their surrounding
context. Motivated by this reasoning process, we propose AmpleHate, a novel
approach designed to mirror human inference for implicit hate detection.
AmpleHate identifies explicit target using a pretrained Named Entity
Recognition model and capture implicit target information via [CLS] tokens. It
computes attention-based relationships between explicit, implicit targets and
sentence context and then, directly injects these relational vectors into the
final sentence representation. This amplifies the critical signals of
target-context relations for determining implicit hate. Experiments demonstrate
that AmpleHate achieves state-of-the-art performance, outperforming contrastive
learning baselines by an average of 82.14% and achieve faster convergence.
Qualitative analyses further reveal that attention patterns produced by
AmpleHate closely align with human judgement, underscoring its interpretability
and robustness.

</details>


### [578] [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)
*Sarang Patil,Ashish Parmanand Pandey,Ioannis Koutis,Mengjia Xu*

Main category: cs.CL

TL;DR: Selective state-space models, like Mamba2, have been successful in long-sequence modeling but their capacity for language representation remains underexplored. This paper proposes Hierarchical Mamba (HiM) integrating efficient Mamba2 with hyperbolic geometry to learn hierarchy-aware language embeddings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of large language models that rely on flat Euclidean embeddings which restricts their ability to capture latent hierarchies.

Method: The method involves projecting Mamba2-processed sequences to the Poincare ball or Lorentzian manifold with 'learnable' curvature and optimizing it with a combined hyperbolic loss. This facilitates the capture of relational distances across varying hierarchical levels enabling effective long-range reasoning.

Result: Experimental results showed that both HiM models effectively capture hierarchical relationships surpassing Euclidean baselines. HiM-Poincare captures fine-grained semantic distinctions while HiM-Lorentz provides more stable, compact, and hierarchy-preserving embeddings.

Conclusion: Hierarchical Mamba (HiM) model, by integrating Mamba2 with hyperbolic geometry, can provide deeper linguistic understanding suitable for complex hierarchical reasoning tasks.

Abstract: Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with "learnable" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.

</details>


### [579] [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)
*Yuxing Lu,Gecheng Fu,Wei Wu,Xukai Zhao,Sin Yee Goi,Jinzhuo Wang*

Main category: cs.CL

TL;DR: DoctorRAG is a new RAG framework that combines clinical knowledge and case-based experience to improve medical reasoning systems.


<details>
  <summary>Details</summary>
Motivation: Existing medical RAG systems focus on knowledge bases but ignore the important experiential knowledge from patient cases which is crucial for clinical reasoning.

Method: DoctorRAG uses conceptual tags and a hybrid retrieval mechanism to enhance retrieval precision. It also integrates a Med-TextGrad module using multi-agent textual gradients to ensure adherence to retrieved knowledge and patient query.

Result: Experiments on multilingual, multitask datasets show DoctorRAG outperforms baseline RAG models and benefits from iterative refinements.

Conclusion: DoctorRAG generates more accurate, relevant, and comprehensive responses, advancing towards more doctor-like medical reasoning systems.

Abstract: Existing medical RAG systems mainly leverage knowledge from medical knowledge
bases, neglecting the crucial role of experiential knowledge derived from
similar patient cases -- a key component of human clinical reasoning. To bridge
this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like
reasoning by integrating both explicit clinical knowledge and implicit
case-based experience. DoctorRAG enhances retrieval precision by first
allocating conceptual tags for queries and knowledge sources, together with a
hybrid retrieval mechanism from both relevant knowledge and patient. In
addition, a Med-TextGrad module using multi-agent textual gradients is
integrated to ensure that the final output adheres to the retrieved knowledge
and patient query. Comprehensive experiments on multilingual, multitask
datasets demonstrate that DoctorRAG significantly outperforms strong baseline
RAG models and gains improvements from iterative refinements. Our approach
generates more accurate, relevant, and comprehensive responses, taking a step
towards more doctor-like medical reasoning systems.

</details>


### [580] [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)
*Xufeng Duan,Zhaoqian Yao,Yunhao Zhang,Shaonan Wang,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）中的神经元、注意力头和电路会逐渐对句法结构产生敏感性，这种特性在训练过程中存在一个'关键时期'，并且受到模型规模和训练数据的影响。


<details>
  <summary>Details</summary>
Motivation: 研究者希望揭示LLMs内部句法特性的形成过程及其影响因素，以更好地理解这些模型的工作机制。

Method: 通过追踪句法敏感性在训练过程中的形成，并量化不同句法现象的最小对之间的内部句法一致性来识别发展轨迹。

Result: 句法敏感性逐渐出现，集中在特定层中，并表现出一个快速内部专业化的'关键时期'，这一过程在不同架构和初始化参数下是一致的。

Conclusion: 研究不仅揭示了句法在LLMs中出现的位置，还展示了某些模型在训练期间如何内化句法特性，并将发布相关代码、模型和训练检查点以支持未来的研究。

Abstract: Large language models (LLMs) have been found to develop surprising internal
specializations: Individual neurons, attention heads, and circuits become
selectively sensitive to syntactic structure, reflecting patterns observed in
the human brain. While this specialization is well-documented, how it emerges
during training and what influences its development remains largely unknown.
  In this work, we tap into the black box of specialization by tracking its
formation over time. By quantifying internal syntactic consistency across
minimal pairs from various syntactic phenomena, we identify a clear
developmental trajectory: Syntactic sensitivity emerges gradually, concentrates
in specific layers, and exhibits a 'critical period' of rapid internal
specialization. This process is consistent across architectures and
initialization parameters (e.g., random seeds), and is influenced by model
scale and training data. We therefore reveal not only where syntax arises in
LLMs but also how some models internalize it during training. To support future
research, we will release the code, models, and training checkpoints upon
acceptance.

</details>


### [581] [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)
*Li Zeng,Zeming Liu,Chong Feng,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: The paper introduces document-level model editing to address limitations in current datasets that only require short outputs, and presents a new dataset called \benchmarkname for this purpose.


<details>
  <summary>Details</summary>
Motivation: Current model editing datasets focus on short outputs, not reflecting real-world document-level tasks.

Method: Propose document-level model editing task and introduce a new dataset \benchmarkname with document-level inputs/outputs, extrapolative nature, and multiple facts per edit along with evaluation metrics.

Result: Existing model editing methods face challenges with the difficulties of document-level editing.

Conclusion: Document-level model editing is necessary for practical usability and current methods struggle with it.

Abstract: Model editing aims to correct errors and outdated knowledge in the Large
language models (LLMs) with minimal cost. Prior research has proposed a variety
of datasets to assess the effectiveness of these model editing methods.
However, most existing datasets only require models to output short phrases or
sentences, overlooks the widespread existence of document-level tasks in the
real world, raising doubts about their practical usability. Aimed at addressing
this limitation and promoting the application of model editing in real-world
scenarios, we propose the task of document-level model editing. To tackle such
challenges and enhance model capabilities in practical settings, we introduce
\benchmarkname, a dataset focused on document-level model editing,
characterized by document-level inputs and outputs, extrapolative, and multiple
facts within a single edit. We propose a series of evaluation metrics and
experiments. The results show that the difficulties in document-level model
editing pose challenges for existing model editing methods.

</details>


### [582] [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)
*Mahdi Nikdan,Vincent Cohen-Addad,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: Influence Distillation is a new data selection framework for LLMs that uses second-order information to assign model-specific weights to training samples, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Effective data selection is crucial for efficient training of modern Large Language Models (LLMs).

Method: Influence Distillation employs second-order information to optimally weight training samples. It assigns model-specific weights that are used to select training data for LLM fine-tuning. The method includes deriving optimal weights for both Gradient Descent and Adam optimizers and proposes a landmark-based approximation to ensure scalability and reduce computational cost.

Result: Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to 3.5 times faster selection.

Conclusion: Influence Distillation is validated by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families.

Abstract: Effective data selection is critical for efficient training of modern Large
Language Models (LLMs). This paper introduces Influence Distillation, a novel,
mathematically-justified framework for data selection that employs second-order
information to optimally weight training samples. By distilling each sample's
influence on a target distribution, our method assigns model-specific weights
that are used to select training data for LLM fine-tuning, guiding it toward
strong performance on the target domain. We derive these optimal weights for
both Gradient Descent and Adam optimizers. To ensure scalability and reduce
computational cost, we propose a $\textit{landmark-based approximation}$:
influence is precisely computed for a small subset of "landmark" samples and
then efficiently propagated to all other samples to determine their weights. We
validate Influence Distillation by applying it to instruction tuning on the
Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,
across several models from the Llama and Qwen families. Experiments show that
Influence Distillation matches or outperforms state-of-the-art performance
while achieving up to $3.5\times$ faster selection.

</details>


### [583] [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)
*Yufan Dang,Chen Qian,Xueheng Luo,Jingru Fan,Zihao Xie,Ruijie Shi,Weize Chen,Cheng Yang,Xiaoyin Che,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: The paper proposes a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator dynamically directs agents to improve performance and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent collaboration approaches among LLMs rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, leading to inefficiencies.

Method: A centralized orchestrator ("puppeteer") is introduced to dynamically direct agents ("puppets") in response to evolving task states. The orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents.

Result: Experiments show superior performance with reduced computational costs in both closed- and open-domain scenarios. Improvements are attributed to more compact, cyclic reasoning structures.

Conclusion: The proposed puppeteer-style paradigm enhances scalability and efficiency in LLM-based multi-agent collaboration through dynamic direction of agents.

Abstract: Large language models (LLMs) have achieved remarkable results across diverse
downstream tasks, but their monolithic nature restricts scalability and
efficiency in complex problem-solving. While recent research explores
multi-agent collaboration among LLMs, most approaches rely on static
organizational structures that struggle to adapt as task complexity and agent
numbers grow, resulting in coordination overhead and inefficiencies. To this
end, we propose a puppeteer-style paradigm for LLM-based multi-agent
collaboration, where a centralized orchestrator ("puppeteer") dynamically
directs agents ("puppets") in response to evolving task states. This
orchestrator is trained via reinforcement learning to adaptively sequence and
prioritize agents, enabling flexible and evolvable collective reasoning.
Experiments on closed- and open-domain scenarios show that this method achieves
superior performance with reduced computational costs. Analyses further reveal
that the key improvements consistently stem from the emergence of more compact,
cyclic reasoning structures under the orchestrator's evolution.

</details>


### [584] [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)
*Andrew Gambardella,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: The study evaluates language models' ability to handle nuanced grammar points, specifically the 'first person psych predicate restriction' in Japanese. It finds that Weblab is unique in distinguishing grammatical from ungrammatical sentences due possibly to its tokenization method. Llama 3's performance can be significantly improved by selecting well-tokenized sentences. Further experiments reveal that models use alternative grammar patterns when faced with tokenization issues.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of language models in recognizing and obeying rare grammar points beyond general text understanding, particularly in non-English languages such as Japanese.

Method: Measure the perplexity of language models on the 'first person psych predicate restriction' grammar point in Japanese. Compare open source models like Weblab and Llama 3, examining the role of tokenization in their performance. Conduct further experiments on machine translation tasks to understand how models handle tokenization issues.

Result: Weblab uniquely assigns higher perplexity to ungrammatical sentences than grammatical ones. Llama 3's perplexity on grammatical sentences can be reduced significantly by restricting to well-tokenized sentences. Models resort to alternative grammar patterns when natural sentences are hindered by tokenization issues.

Conclusion: Tokenization plays a crucial role in language models' ability to handle nuanced grammar points. The study highlights the importance of considering tokenization quality for better evaluation of language models.

Abstract: Typical methods for evaluating the performance of language models evaluate
their ability to answer questions accurately. These evaluation metrics are
acceptable for determining the extent to which language models can understand
and reason about text in a general sense, but fail to capture nuanced
capabilities, such as the ability of language models to recognize and obey rare
grammar points, particularly in languages other than English. We measure the
perplexity of language models when confronted with the "first person psych
predicate restriction" grammar point in Japanese. Weblab is the only tested
open source model in the 7-10B parameter range which consistently assigns
higher perplexity to ungrammatical psych predicate sentences than grammatical
ones. We give evidence that Weblab's uniformly bad tokenization is a possible
root cause for its good performance, and show that Llama 3's perplexity on
grammatical psych predicate sentences can be reduced by orders of magnitude
(28x difference) by restricting test sentences to those with uniformly
well-behaved tokenizations. We show in further experiments on machine
translation tasks that language models will use alternative grammar patterns in
order to produce grammatical sentences when tokenization issues prevent the
most natural sentence from being output.

</details>


### [585] [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)
*Zihong Zhang,Liqi He,Zuchao Li,Lefei Zhang,Hai Zhao,Bo Du*

Main category: cs.CL

TL;DR: This paper explores unsupervised word segmentation using Large Language Models (LLMs) and introduces a novel method called LLACA.


<details>
  <summary>Details</summary>
Motivation: To explore the limit of unsupervised word segmentation with LLMs and evaluate their semantic understanding capabilities based on word segmentation.

Method: Employ mainstream LLMs for word segmentation across multiple languages and introduce a new unsupervised method named LLACA that combines Aho-Corasick automata with pretrained LLMs.

Result: LLMs can segment text into words following simple prompts, with better performance from models having more parameters. LLACA offers significant improvements over traditional methods.

Conclusion: The proposed framework successfully evaluates LLMs' comprehension abilities and LLACA provides an effective approach for unsupervised word segmentation.

Abstract: Word segmentation stands as a cornerstone of Natural Language Processing
(NLP). Based on the concept of "comprehend first, segment later", we propose a
new framework to explore the limit of unsupervised word segmentation with Large
Language Models (LLMs) and evaluate the semantic understanding capabilities of
LLMs based on word segmentation. We employ current mainstream LLMs to perform
word segmentation across multiple languages to assess LLMs' "comprehension".
Our findings reveal that LLMs are capable of following simple prompts to
segment raw text into words. There is a trend suggesting that models with more
parameters tend to perform better on multiple languages. Additionally, we
introduce a novel unsupervised method, termed LLACA ($\textbf{L}$arge
$\textbf{L}$anguage Model-Inspired $\textbf{A}$ho-$\textbf{C}$orasick
$\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities
of Aho-Corasick automata, LLACA innovatively combines these with the deep
insights of well-pretrained LLMs. This approach not only enables the
construction of a dynamic $n$-gram model that adjusts based on contextual
information but also integrates the nuanced understanding of LLMs, offering
significant improvements over traditional methods. Our source code is available
at https://github.com/hkr04/LLACA

</details>


### [586] [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)
*Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: SpeakStream is a new streaming TTS system that generates audio incrementally from streaming text using a decoder-only architecture, achieving state-of-the-art latency results while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Traditional TTS systems create unacceptable delays when coupled with streaming LLM outputs, hindering the potential of conversational AI.

Method: SpeakStream uses a decoder-only architecture trained with a next-step prediction loss on interleaved text-speech data to generate speech incrementally as it absorbs streaming input text.

Result: Experiments show SpeakStream achieves state-of-the-art first-token latency while maintaining the quality of non-streaming TTS systems.

Conclusion: SpeakStream addresses the latency bottleneck in traditional TTS systems for conversational AI by enabling incremental audio generation from streaming text.

Abstract: The latency bottleneck of traditional text-to-speech (TTS) systems
fundamentally hinders the potential of streaming large language models (LLMs)
in conversational AI. These TTS systems, typically trained and inferenced on
complete utterances, introduce unacceptable delays, even with optimized
inference speeds, when coupled with streaming LLM outputs. This is particularly
problematic for creating responsive conversational agents where low first-token
latency is critical. In this paper, we present SpeakStream, a streaming TTS
system that generates audio incrementally from streaming text using a
decoder-only architecture. SpeakStream is trained using a next-step prediction
loss on interleaved text-speech data. During inference, it generates speech
incrementally while absorbing streaming input text, making it particularly
suitable for cascaded conversational AI agents where an LLM streams text to a
TTS system. Our experiments demonstrate that SpeakStream achieves
state-of-the-art latency results in terms of first-token latency while
maintaining the quality of non-streaming TTS systems.

</details>


### [587] [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)
*Tingjia Shen,Hao Wang,Chuan Qin,Ruijun Sun,Yang Song,Defu Lian,Hengshu Zhu,Enhong Chen*

Main category: cs.CL

TL;DR: The paper proposes GenKI, a framework that enhances open-domain question answering by integrating knowledge effectively into large language models and enabling controllable generation. Experiments show its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To solve the challenges of how to integrate knowledge into LLMs effectively and how to adaptively generate results with specific answer formats for various task situations in OpenQA.

Method: Propose GenKI framework which includes training a dense passage retrieval model, introducing a novel knowledge integration model, and leveraging a fine-tuned LLM with an ensemble based on text consistency for controllable generation.

Result: GenKI shows effectiveness through extensive experiments on TriviaQA, MSMARCO, and CMRC2018 datasets. Ablation studies reveal a linear relationship between retrieved knowledge frequency and the model's ability to recall knowledge accurately.

Conclusion: GenKI improves OpenQA performance by exploring Knowledge Integration and controllable Generation on LLMs simultaneously.

Abstract: Open-domain question answering (OpenQA) represents a cornerstone in natural
language processing (NLP), primarily focused on extracting answers from
unstructured textual data. With the rapid advancements in Large Language Models
(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent
understanding and answering capabilities enabled by massive parameters compared
to traditional methods. However, most of these methods encounter two critical
challenges: how to integrate knowledge into LLMs effectively and how to
adaptively generate results with specific answer formats for various task
situations. To address these challenges, we propose a novel framework named
GenKI, which aims to improve the OpenQA performance by exploring Knowledge
Integration and controllable Generation on LLMs simultaneously. Specifically,
we first train a dense passage retrieval model to retrieve associated knowledge
from a given knowledge base. Subsequently, we introduce a novel knowledge
integration model that incorporates the retrieval knowledge into instructions
during fine-tuning to intensify the model. Furthermore, to enable controllable
generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based
on text consistency incorporating all coherence, fluency, and answer format
assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,
and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the
effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,
ablation studies have disclosed a linear relationship between the frequency of
retrieved knowledge and the model's ability to recall knowledge accurately
against the ground truth. Our code of GenKI is available at
https://github.com/USTC-StarTeam/GenKI

</details>


### [588] [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)
*Weikang Yuan,Kaisong Song,Zhuoren Jiang,Junjie Cao,Yujie Zhang,Jun Lin,Kun Kuang,Ji Zhang,Xiaozhong Liu*

Main category: cs.CL

TL;DR: Legal consultation is costly and inaccessible to many. Recent advances in LLMs offer a path toward scalable, low-cost legal assistance, but current systems are not good enough. This paper introduces LeCoDe, a multi-turn benchmark dataset of legal consultation dialogues, designed to evaluate and improve LLMs' legal consultation capability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of cost and accessibility in legal consultations and improve LLMs' ability to handle interactive, knowledge-intensive consultations.

Method: Introduced LeCoDe, a real-world multi-turn benchmark dataset with 3,696 legal consultation dialogues and 110,008 dialogue turns. The dataset was collected from live-streamed consultations on short-video platforms and annotated by legal experts. Proposed a comprehensive evaluation framework with 12 metrics across two dimensions to assess LLMs' consultation capabilities.

Result: Through experiments on various LLMs, significant challenges were revealed. Even state-of-the-art models like GPT-4 achieved only 39.8% recall for clarification and 59% overall score for advice quality. Several strategies to enhance LLMs' legal consultation abilities were explored.

Conclusion: LeCoDe contributes to advancing research in legal domain dialogue systems, particularly in simulating more real-world user-expert interactions.

Abstract: Legal consultation is essential for safeguarding individual rights and
ensuring access to justice, yet remains costly and inaccessible to many
individuals due to the shortage of professionals. While recent advances in
Large Language Models (LLMs) offer a promising path toward scalable, low-cost
legal assistance, current systems fall short in handling the interactive and
knowledge-intensive nature of real-world consultations. To address these
challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset
comprising 3,696 legal consultation dialogues with 110,008 dialogue turns,
designed to evaluate and improve LLMs' legal consultation capability. With
LeCoDe, we innovatively collect live-streamed consultations from short-video
platforms, providing authentic multi-turn legal consultation dialogues. The
rigorous annotation by legal experts further enhances the dataset with
professional insights and expertise. Furthermore, we propose a comprehensive
evaluation framework that assesses LLMs' consultation capabilities in terms of
(1) clarification capability and (2) professional advice quality. This unified
framework incorporates 12 metrics across two dimensions. Through extensive
experiments on various general and domain-specific LLMs, our results reveal
significant challenges in this task, with even state-of-the-art models like
GPT-4 achieving only 39.8% recall for clarification and 59% overall score for
advice quality, highlighting the complexity of professional consultation
scenarios. Based on these findings, we further explore several strategies to
enhance LLMs' legal consultation abilities. Our benchmark contributes to
advancing research in legal domain dialogue systems, particularly in simulating
more real-world user-expert interactions.

</details>


### [589] [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)
*Utkarsh Sahu,Zhisheng Qi,Yongjia Lei,Ryan A. Rossi,Franck Dernoncourt,Nesreen K. Ahmed,Mahantesh M Halappanavar,Yao Ma,Yu Wang*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）的知识结构模式从图的角度被研究，发现拓扑上接近的实体具有相似的知识水平，并提出基于局部邻居估计实体知识的图机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型作为神经知识库在知识获取、编辑性、推理和可解释性方面已被广泛研究，但很少有工作关注其知识的结构模式。

Method: 通过量化LLMs的知识在三元组和实体层面，并分析其与图结构属性（如节点度）的关系；揭示知识同质性，即拓扑上接近的实体表现出相似的知识水平；开发图机器学习模型以根据局部邻居估计实体知识。

Result: 经验结果表明，使用所选三元组进行微调可带来更优的性能。

Conclusion: 从图视角研究了LLMs的知识结构模式，发现了知识同质性并提出了用于知识估计的图机器学习模型，这有助于选择LLMs较少了解的三元组进行知识检查和微调。

Abstract: Large language models have been extensively studied as neural knowledge bases
for their knowledge access, editability, reasoning, and explainability.
However, few works focus on the structural patterns of their knowledge.
Motivated by this gap, we investigate these structural patterns from a graph
perspective. We quantify the knowledge of LLMs at both the triplet and entity
levels, and analyze how it relates to graph structural properties such as node
degree. Furthermore, we uncover the knowledge homophily, where topologically
close entities exhibit similar levels of knowledgeability, which further
motivates us to develop graph machine learning models to estimate entity
knowledge based on its local neighbors. This model further enables valuable
knowledge checking by selecting triplets less known to LLMs. Empirical results
show that using selected triplets for fine-tuning leads to superior
performance.

</details>


### [590] [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
*Liqin Ye,Agam Shah,Chao Zhang,Sudheer Chava*

Main category: cs.CL

TL;DR: The paper introduces SiDyP, a method to enhance the robustness of classifiers towards LLM-generated noisy labels by calibrating predictions.


<details>
  <summary>Details</summary>
Motivation: To address the issue of unreliable auto-generated labels from LLMs and their impact on model generalization when learning from noisy labels.

Method: SiDyP retrieves potential true label candidates using neighborhood label distribution in text embedding space and refines noisy candidates iteratively with a simplex diffusion model.

Result: SiDyP improves the performance of a BERT classifier fine-tuned on zero-shot and few-shot LLM-generated noisy label datasets by 7.21% and 7.30% respectively.

Conclusion: The proposed framework demonstrates effectiveness across different LLMs and NLP tasks, offering a solution to mitigate the impact of LLM-generated label noise.

Abstract: The traditional process of creating labeled datasets is labor-intensive and
expensive. Recent breakthroughs in open-source large language models (LLMs)
have opened up a new avenue in generating labeled datasets automatically for
various natural language processing (NLP) tasks, providing an alternative to
such an expensive annotation process. However, the reliability of such
auto-generated labels remains a significant concern due to inherent
inaccuracies. When learning from noisy labels, the model's generalization is
likely to be harmed as it is prone to overfit to those label noises. While
previous studies in learning from noisy labels mainly focus on synthetic noise
and real-world noise, LLM-generated label noise receives less attention. In
this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to
calibrate the classifier's prediction, thus enhancing its robustness towards
LLM-generated noisy labels. SiDyP retrieves potential true label candidates by
neighborhood label distribution in text embedding space and iteratively refines
noisy candidates using a simplex diffusion model. Our framework can increase
the performance of the BERT classifier fine-tuned on both zero-shot and
few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%
respectively. We demonstrate the effectiveness of SiDyP by conducting extensive
benchmarking for different LLMs over a variety of NLP tasks. Our code is
available on Github.

</details>


### [591] [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)
*Zhaolin Li,Yining Liu,Danni Liu,Tuan Nam Nguyen,Enes Yavuz Ugan,Tu Anh Dinh,Carlos Mullov,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: This paper presents KIT's submissions to the IWSLT 2025 low-resource track, exploring system enhancement with synthetic data and model regularization for three language pairs.


<details>
  <summary>Details</summary>
Motivation: To develop both cascaded systems (ASR + MT) and end-to-end Speech Translation (ST) systems for low-resource languages such as Bemba, North Levantine Arabic, and Tunisian Arabic into English.

Method: Fine-tune pre-trained models using different strategies. Explore system enhancement with synthetic data and model regularization including MT-augmented ST, text-to-speech models for synthetic speech generation, intra-distillation, and Minimum Bayes Risk decoding.

Result: For North Levantine, a system trained solely on synthetic data slightly surpasses the cascaded system trained on real data. Synthetic data improves ASR and ST performance for Bemba. Intra-distillation consistently improves results across tasks and pre-trained models. Minimum Bayes Risk decoding improves approximately 1.5 BLEU points.

Conclusion: The methods of using synthetic data, model regularization, intra-distillation, and Minimum Bayes Risk decoding have proven effective in enhancing the performance of low-resource Speech Translation systems.

Abstract: This paper presents KIT's submissions to the IWSLT 2025 low-resource track.
We develop both cascaded systems, consisting of Automatic Speech Recognition
(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech
Translation (ST) systems for three language pairs: Bemba, North Levantine
Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we
fine-tune our systems with different strategies to utilize resources
efficiently. This study further explores system enhancement with synthetic data
and model regularization. Specifically, we investigate MT-augmented ST by
generating translations from ASR data using MT models. For North Levantine,
which lacks parallel ST training data, a system trained solely on synthetic
data slightly surpasses the cascaded system trained on real data. We also
explore augmentation using text-to-speech models by generating synthetic speech
from MT data, demonstrating the benefits of synthetic data in improving both
ASR and ST performance for Bemba. Additionally, we apply intra-distillation to
enhance model performance. Our experiments show that this approach consistently
improves results across ASR, MT, and ST tasks, as well as across different
pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine
the cascaded and end-to-end systems, achieving an improvement of approximately
1.5 BLEU points.

</details>


### [592] [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)
*Yi Liu,Dianqing Liu,Mingye Zhu,Junbo Guo,Yongdong Zhang,Zhendong Mao*

Main category: cs.CL

TL;DR: The paper introduces a Residual Alignment Model (RAM) that enhances the flexibility and scalability of aligning large language models (LLMs) without retraining, using importance sampling and an efficient sequence-level training strategy. Experiments show it outperforms baselines in various tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulty of quickly adapting and optimizing LLMs for diverse applications due to the need for retraining in traditional alignment methods.

Method: Proposes RAM which formalizes alignment as importance sampling, with the unaligned model as proposal distribution and alignment as secondary sampling by an autoregressive module estimating importance weights. Includes an efficient sequence-level training strategy and a resampling algorithm for token-level decoding.

Result: RAM outperforms baseline models in instruction following, domain adaptation, and preference optimization tasks across two leading open-source LLMs.

Conclusion: RAM provides a flexible and scalable solution for aligning LLMs, improving performance in various tasks.

Abstract: The widespread adoption of large language models (LLMs) across industries has
increased the demand for high-quality and customizable outputs. However,
traditional alignment methods often require retraining large pretrained models,
making it difficult to quickly adapt and optimize LLMs for diverse
applications. To address this limitation, we propose a novel \textit{Residual
Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type
of importance sampling. In this framework, the unaligned upstream model serves
as the proposal distribution, while the alignment process is framed as
secondary sampling based on an autoregressive alignment module that acts as an
estimator of the importance weights. This design enables a natural detachment
of the alignment module from the target aligned model, improving flexibility
and scalability. Based on this model, we derive an efficient sequence-level
training strategy for the alignment module, which operates independently of the
proposal module. Additionally, we develop a resampling algorithm with iterative
token-level decoding to address the common first-token latency issue in
comparable methods. Experimental evaluations on two leading open-source LLMs
across diverse tasks, including instruction following, domain adaptation, and
preference optimization, demonstrate that our approach consistently outperforms
baseline models.

</details>


### [593] [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)
*Tej Deep Pala,Panshul Sharma,Amir Zadeh,Chuan Li,Soujanya Poria*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）容易产生幻觉，特别是在多步骤和推理密集型任务中。PathFinder-PRM是一种新型的分层、错误感知的Process Reward Model，它首先对每一步中的数学和一致性错误进行分类，然后结合这些细粒度信号来估计步骤的正确性。该模型在PRMBench上取得了67.7的PRMScore新高，使用的数据量仅为之前的三分之一。当应用于奖励引导的贪婪搜索时，该模型在prm@8指标上比最强基线高出1.5个百分点。结果表明，分离的错误检测和奖励估计不仅增强了细粒度错误检测，还显著提高了端到端奖励引导的数学推理能力，并且具有更高的数据效率。


<details>
  <summary>Details</summary>
Motivation: 尽管Outcome Reward Models仅验证最终答案，但Process Reward Models (PRMs)通过对每一步进行评分来引导生成连贯的解决方案。现有的LLMs在多步骤推理任务（如数学问题解决）中容易出现幻觉，因此需要一种新的PRM来提高其推理能力。

Method: 提出了一种名为PathFinder-PRM的新型Process Reward Model，该模型通过以下方式工作：1) 首先对每一步中的数学和一致性错误进行分类；2) 结合这些细粒度信号来估计步骤的正确性。为了训练PathFinder-PRM，构建了一个包含40万样本的数据集，通过丰富人工注释的PRM800K语料库和RLHFlow Mistral轨迹，添加了三维步骤级标签。

Result: 在PRMBench上，PathFinder-PRM取得了67.7的PRMScore新高，超越了之前的最佳成绩65.5，同时使用的数据量减少了三分之二。在奖励引导的贪婪搜索中，该模型在prm@8指标上达到了48.3，相比最强基线提升了1.5个百分点。

Conclusion: 解耦的错误检测和奖励估计方法不仅可以增强细粒度错误检测，还能显著提高端到端奖励引导的数学推理能力，同时实现更高的数据效率。

Abstract: Large Language Models (LLMs) are prone to hallucination, especially during
multi-hop and reasoning-intensive tasks such as mathematical problem solving.
While Outcome Reward Models verify only final answers, Process Reward Models
(PRMs) score each intermediate step to steer generation toward coherent
solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware
discriminative PRM that first classifies math and consistency errors at each
step, then combines these fine-grained signals to estimate step correctness. To
train PathFinder-PRM, we construct a 400K-sample dataset by enriching the
human-annotated PRM800K corpus and RLHFlow Mistral traces with
three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new
state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while
using 3 times less data. When applied to reward guided greedy search, our model
yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results
demonstrate that decoupled error detection and reward estimation not only boost
fine-grained error detection but also substantially improve end-to-end,
reward-guided mathematical reasoning with greater data efficiency.

</details>


### [594] [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
*Zhaopeng Feng,Yupu Liang,Shaosheng Cao,Jiayuan Su,Jiahan Ren,Zhe Xu,Yao Hu,Wenxuan Huang,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: The paper presents MT³, the first framework applying Multi-Task RL to MLLMs for end-to-end TIMT. It introduces XHSPost benchmark and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation.

Method: MT³ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism.

Result: MT³-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines across multiple metrics.

Conclusion: In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT.

Abstract: Text Image Machine Translation (TIMT)-the task of translating textual content
embedded in images-is critical for applications in accessibility, cross-lingual
information access, and real-world document understanding. However, TIMT
remains a complex challenge due to the need for accurate optical character
recognition (OCR), robust visual-text reasoning, and high-quality translation,
often requiring cascading multi-stage pipelines. Recent advances in large-scale
Reinforcement Learning (RL) have improved reasoning in Large Language Models
(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is
still underexplored. To bridge this gap, we introduce MT$^{3}$, the first
framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts
a multi-task optimization paradigm targeting three key sub-skills: text
recognition, context-aware reasoning, and translation. It is trained using a
novel multi-mixed reward mechanism that adapts rule-based RL strategies to
TIMT's intricacies, offering fine-grained, non-binary feedback across tasks.
Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural
and real-world social media contexts, we introduced XHSPost, the first social
media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on
the latest in-domain MIT-10M benchmark, outperforming strong baselines such as
Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.
Additionally, the model shows strong generalization to out-of-distribution
language pairs and datasets. In-depth analyses reveal how multi-task synergy,
reinforcement learning initialization, curriculum design, and reward
formulation contribute to advancing MLLM-driven TIMT.

</details>


### [595] [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)
*Chunyang Jiang,Chi-min Chan,Yiyang Cai,Yulong Liu,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 在生成式语言模型的微调过程中，通过引入遗忘机制（Learning With Forgetting, LWF），可以选择性地遗忘无用知识，从而减轻负迁移现象，提升下游任务的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练模型通常能提高下游任务的效果和效率，但并非所有预训练知识都是有益的，部分知识可能导致负迁移。优雅遗忘（graceful forgetting）作为一种有潜力的方法，可以有选择地丢弃无关知识，但在生成式语言模型中尚未得到充分探索。

Method: 提出了一种名为Learning With Forgetting (LWF)的新框架，利用Fisher Information Matrix对参数更新进行加权，计算遗忘置信度以评估自动生成的知识，并在微调过程中周期性地遗忘高置信度的知识。

Result: 实验表明，在预训练语言模型中完全揭示知识交互机制仍具挑战性，但应用优雅遗忘能够显著提升微调性能。

Conclusion: 优雅遗忘方法可以在生成式语言模型的微调阶段有效减轻负迁移问题，从而改善下游任务的表现。

Abstract: Recently, the pretrain-finetune paradigm has become a cornerstone in various
deep learning areas. While in general the pre-trained model would promote both
effectiveness and efficiency of downstream tasks fine-tuning, studies have
shown that not all knowledge acquired during pre-training is beneficial. Some
of the knowledge may actually bring detrimental effects to the fine-tuning
tasks, which is also known as negative transfer. To address this problem,
graceful forgetting has emerged as a promising approach. The core principle of
graceful forgetting is to enhance the learning plasticity of the target task by
selectively discarding irrelevant knowledge. However, this approach remains
underexplored in the context of generative language models, and it is often
challenging to migrate existing forgetting algorithms to these models due to
architecture incompatibility. To bridge this gap, in this paper we propose a
novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting
in generative language models. With Fisher Information Matrix weighting the
intended parameter updates, LWF computes forgetting confidence to evaluate
self-generated knowledge regarding the forgetting task, and consequently,
knowledge with high confidence is periodically unlearned during fine-tuning.
Our experiments demonstrate that, although thoroughly uncovering the mechanisms
of knowledge interaction remains challenging in pre-trained language models,
applying graceful forgetting can contribute to enhanced fine-tuning
performance.

</details>


### [596] [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)
*Yihao Ai,Zhiyuan Ning,Weiwei Dai,Pengfei Wang,Yi Du,Wenjuan Cui,Kunpeng Liu,Yuanchun Zhou*

Main category: cs.CL

TL;DR: The paper introduces RPDR, a framework combining closed-source and open-source LLMs for biomedical entity linking, which addresses the limitations of traditional methods and costly closed-source models. Evaluated on two datasets, it shows improvements in Acc@1 and highlights its superiority and generalizability.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised methods for biomedical entity linking require extensive annotated data, limiting their usage in low-resource scenarios. Closed-source LLMs can address this but bring stability issues and high costs.

Method: Propose RPDR, a framework that uses closed-source LLMs to generate training data from unannotated data, fine-tunes an open-source LLM for re-ranking candidates retrieved by a retriever adjusted with a small amount of data.

Result: RPDR achieves 0.019 Acc@1 improvement on the Aier dataset and 0.036 Acc@1 improvement on the Ask A Patient dataset when training data is limited.

Conclusion: RPDR effectively distills knowledge to open-source LLMs, avoiding stability and cost issues, and demonstrates superiority and generalizability in cross-language biomedical entity linking tasks.

Abstract: Biomedical entity linking aims to map nonstandard entities to standard
entities in a knowledge base. Traditional supervised methods perform well but
require extensive annotated data to transfer, limiting their usage in
low-resource scenarios. Large language models (LLMs), especially closed-source
LLMs, can address these but risk stability issues and high economic costs:
using these models is restricted by commercial companies and brings significant
economic costs when dealing with large amounts of data. To address this, we
propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs
for re-ranking candidates retrieved by a retriever fine-tuned with a small
amount of data. By prompting a closed-source LLM to generate training data from
unannotated data and fine-tuning an open-source LLM for re-ranking, we
effectively distill the knowledge to the open-source LLM that can be deployed
locally, thus avoiding the stability issues and the problem of high economic
costs. We evaluate RPDR on two datasets, including one real-world dataset and
one publicly available dataset involving two languages: Chinese and English.
RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier
dataset and the Ask A Patient dataset when the amount of training data is not
enough. The results demonstrate the superiority and generalizability of the
proposed framework.

</details>


### [597] [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
*Ruisheng Cao,Hanchong Zhang,Tiancheng Huang,Zhangyi Kang,Yuxin Zhang,Liangtai Sun,Hanqi Li,Yuxun Miao,Shuai Fan,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: The paper introduces NeuSym-RAG, a hybrid retrieval framework combining neural and symbolic methods for efficient question answering from PDFs using LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently acquiring key details from an increasing number of academic papers and to overcome limitations of previous works that isolate neural and symbolic retrieval.

Method: Proposes NeuSym-RAG which uses multi-view chunking and schema-based parsing to organize PDF content into a relational database and vectorstore, allowing LLM agents to iteratively gather context.

Result: Experiments on three PDF-based QA datasets demonstrate that NeuSym-RAG outperforms both vector-based RAG and various structured baselines.

Conclusion: NeuSym-RAG effectively unifies neural and symbolic retrieval schemes and utilizes multiple views for improved question answering from PDFs.

Abstract: The increasing number of academic papers poses significant challenges for
researchers to efficiently acquire key details. While retrieval augmented
generation (RAG) shows great promise in large language model (LLM) based
automated question answering, previous works often isolate neural and symbolic
retrieval despite their complementary strengths. Moreover, conventional
single-view chunking neglects the rich structure and layout of PDFs, e.g.,
sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural
symbolic retrieval framework which combines both paradigms in an interactive
process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG
organizes semi-structured PDF content into both the relational database and
vectorstore, enabling LLM agents to iteratively gather context until sufficient
to generate answers. Experiments on three full PDF-based QA datasets, including
a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the
vector-based RAG and various structured baselines, highlighting its capacity to
unify both retrieval schemes and utilize multiple views. Code and data are
publicly available at https://github.com/X-LANCE/NeuSym-RAG.

</details>


### [598] [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
*Shashata Sawmya,Micah Adler,Nir Shavit*

Main category: cs.CL

TL;DR: This paper explores how interpretable categorical features form in large language models (LLMs) concerning training progress, model layers, and model sizes using sparse autoencoders. Results show distinct thresholds for feature emergence based on time and scale, along with surprising reactivation of early-layer features in later layers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the formation and behavior of interpretable categorical features within large language models across different aspects: during training (time), across transformer layers (space), and among varying model sizes (scale).

Method: Sparse autoencoders are employed for mechanistic interpretability to pinpoint when and where specific semantic concepts emerge within neural activations in LLMs.

Result: Clear temporal and scale-specific thresholds were found for feature emergence across various domains. Spatial analysis also uncovered unexpected semantic reactivation where early-layer features re-emerge at later layers.

Conclusion: The study reveals that there are distinct points during training and depending on model size where interpretable features emerge. Moreover, the re-emergence of early-layer features in later layers questions typical assumptions about representation dynamics in transformers.

Abstract: This paper studies the emergence of interpretable categorical features within
large language models (LLMs), analyzing their behavior across training
checkpoints (time), transformer layers (space), and varying model sizes
(scale). Using sparse autoencoders for mechanistic interpretability, we
identify when and where specific semantic concepts emerge within neural
activations. Results indicate clear temporal and scale-specific thresholds for
feature emergence across multiple domains. Notably, spatial analysis reveals
unexpected semantic reactivation, with early-layer features re-emerging at
later layers, challenging standard assumptions about representational dynamics
in transformer models.

</details>


### [599] [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)
*Akram Elbouanani,Evan Dufraisse,Adrian Popescu*

Main category: cs.CL

TL;DR: Political biases in LLMs can negatively impact applications. Current bias analysis methods are flawed. This study proposes a new entropy-based inconsistency metric to analyze bias by observing sentiment prediction variability with diverse politician names. Results show biases towards certain political groups, stronger in larger models and Western languages. A mitigation strategy is suggested.


<details>
  <summary>Details</summary>
Motivation: To address the issue of political biases encoded by LLMs that could detrimentally affect downstream applications, and to overcome the limitations of existing bias analysis methods which propagate bias.

Method: Propose an entropy-based inconsistency metric to measure variability in LLM sentiment predictions. Insert diverse politician names into political sentences and predict sentiment using multiple models across different languages. Conduct a statistically robust analysis of inconsistencies at various granularity levels.

Result: Observed positive and negative biases toward left and far-right politicians, correlations between similar aligned politicians. Bias intensity higher in Western languages. Larger models exhibit stronger and more consistent biases, reducing discrepancies between similar languages.

Conclusion: LLMs have significant political biases that current analysis methods fail to capture accurately. The proposed method provides a way to quantify these biases. Replacing real politician names with fictional ones can partially mitigate unreliability in target-oriented sentiment classification.

Abstract: Political biases encoded by LLMs might have detrimental effects on downstream
applications. Existing bias analysis methods rely on small-size intermediate
tasks (questionnaire answering or political content generation) and rely on the
LLMs themselves for analysis, thus propagating bias. We propose a new approach
leveraging the observation that LLM sentiment predictions vary with the target
entity in the same sentence. We define an entropy-based inconsistency metric to
encode this prediction variability. We insert 1319 demographically and
politically diverse politician names in 450 political sentences and predict
target-oriented sentiment using seven models in six widely spoken languages. We
observe inconsistencies in all tested combinations and aggregate them in a
statistically robust analysis at different granularity levels. We observe
positive and negative bias toward left and far-right politicians and positive
correlations between politicians with similar alignment. Bias intensity is
higher for Western languages than for others. Larger models exhibit stronger
and more consistent biases and reduce discrepancies between similar languages.
We partially mitigate LLM unreliability in target-oriented sentiment
classification (TSC) by replacing politician names with fictional but plausible
counterparts.

</details>


### [600] [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)
*Junnan Liu,Hongwei Liu,Linchen Xiao,Shudong Liu,Taolin Zhang,Zihan Ma,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: This paper proposes a novel framework for understanding the reasoning abilities of large language models (LLMs) via meta-learning, formalizing LLM training for reasoning tasks as a meta-learning setup and showing that LLMs can develop generalizable reasoning skills.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand the reasoning capabilities of large language models and explore how meta-learning techniques can be applied to improve these models.

Method: The method involves conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLM's parameters, treating each question as an individual task in a meta-learning setup, and using reasoning trajectories for inner loop optimization to adapt model parameters.

Result: Empirical evaluations demonstrate a strong connection between LLM reasoning and meta-learning, indicating that LLMs can develop fundamental reasoning capabilities that generalize to unseen questions.

Conclusion: This work enhances the understanding of LLM reasoning through the lens of meta-learning and provides practical insights for improving LLMs using established meta-learning techniques.

Abstract: We propose a novel framework for comprehending the reasoning capabilities of
large language models (LLMs) through the perspective of meta-learning. By
conceptualizing reasoning trajectories as pseudo-gradient descent updates to
the LLM's parameters, we identify parallels between LLM reasoning and various
meta-learning paradigms. We formalize the training process for reasoning tasks
as a meta-learning setup, with each question treated as an individual task, and
reasoning trajectories serving as the inner loop optimization for adapting
model parameters. Once trained on a diverse set of questions, the LLM develops
fundamental reasoning capabilities that can generalize to previously unseen
questions. Extensive empirical evaluations substantiate the strong connection
between LLM reasoning and meta-learning, exploring several issues of
significant interest from a meta-learning standpoint. Our work not only
enhances the understanding of LLM reasoning but also provides practical
insights for improving these models through established meta-learning
techniques.

</details>


### [601] [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)
*Pascal Wullschleger,Majid Zarharan,Donnacha Daly,Marc Pouly,Jennifer Foster*

Main category: cs.CL

TL;DR: Large Language Models are investigated for automated taxonomy generation and completion in the food technology industry, showing promise but difficulty in correctly placing inner nodes.


<details>
  <summary>Details</summary>
Motivation: To explore the utility of Large Language Models for automated taxonomy generation and completion specifically applied to taxonomies from the food technology industry.

Method: Using an open-source LLM (Llama-3), experiments were conducted on five taxonomies, applying recent prompting techniques in an iterative fashion to complete taxonomies from a seed or generate them without a seed from a set of known concepts.

Result: Experiments showed promising results but highlighted the difficulty of correctly placing inner nodes within the taxonomies.

Conclusion: Large Language Models have potential for automated taxonomy generation and completion in the food technology industry, but further work is needed to improve the accuracy of placing inner nodes.

Abstract: We investigate the utility of Large Language Models for automated taxonomy
generation and completion specifically applied to taxonomies from the food
technology industry. We explore the extent to which taxonomies can be completed
from a seed taxonomy or generated without a seed from a set of known concepts,
in an iterative fashion using recent prompting techniques. Experiments on five
taxonomies using an open-source LLM (Llama-3), while promising, point to the
difficulty of correctly placing inner nodes.

</details>


### [602] [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)
*Gulfarogh Azam,Mohd Sadique,Saif Ali,Mohammad Nadeem,Erik Cambria,Shahab Saquib Sohail,Mohammad Sultan Alam*

Main category: cs.CL

TL;DR: Transliteration is key in multilingual NLP. This study evaluates LLMs (GPT-4o, GPT-4.5, etc.) against IndicXlit for transliteration across 10 Indian languages using standard benchmarks. LLMs, especially the GPT family, generally outperform IndicXlit with fine-tuning further enhancing results. Error analysis and robustness tests show LLMs' potential for specialized tasks.


<details>
  <summary>Details</summary>
Motivation: To assess whether general-purpose large language models can match or exceed the performance of specialized transliteration models like IndicXlit without explicit task-specific training.

Method: Systematically evaluate the transliteration performance of prominent LLMs (GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, Mistral-Large) against IndicXlit across ten major Indian languages using standard benchmarks (Dakshina and Aksharantar datasets). Performance metrics include Top-1 Accuracy and Character Error Rate. Fine-tune GPT-4o and conduct error analysis and robustness testing under noisy conditions.

Result: The GPT family models generally outperform other LLMs and IndicXlit for most instances. Fine-tuning GPT-4o improves performance on specific languages notably. Error analysis and robustness testing show that LLMs have strengths compared to specialized models.

Conclusion: General-purpose LLMs can be effective for specialized tasks like transliteration with minimal overhead, potentially reducing the need for task-specific training.

Abstract: Transliteration, the process of mapping text from one script to another,
plays a crucial role in multilingual natural language processing, especially
within linguistically diverse contexts such as India. Despite significant
advancements through specialized models like IndicXlit, recent developments in
large language models suggest a potential for general-purpose models to excel
at this task without explicit task-specific training. The current work
systematically evaluates the performance of prominent LLMs, including GPT-4o,
GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a
state-of-the-art transliteration model, across ten major Indian languages.
Experiments utilized standard benchmarks, including Dakshina and Aksharantar
datasets, with performance assessed via Top-1 Accuracy and Character Error
Rate. Our findings reveal that while GPT family models generally outperform
other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o
improves performance on specific languages notably. An extensive error analysis
and robustness testing under noisy conditions further elucidate strengths of
LLMs compared to specialized models, highlighting the efficacy of foundational
models for a wide spectrum of specialized applications with minimal overhead.

</details>


### [603] [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)
*Javier Marín*

Main category: cs.CL

TL;DR: The paper introduces Adjacent Possible Exploration (APE), a method to adapt large language models to specific tasks with minimal resources. It iteratively fine-tunes models on small data batches, leading to significant improvements in tasks like news summarization without requiring extensive compute.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and cost-effective way of adapting large language models to specific tasks, especially for researchers and practitioners with limited computational resources.

Method: APE iteratively fine-tunes models on small, carefully selected data batches (200 examples), retaining only the improvements. This process is inspired by evolutionary theory's 'adjacent possible'.

Result: On news summarization, APE achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes, matching or exceeding more complex methods like LoRA.

Conclusion: APE provides a simple yet effective approach for task-specific adaptation of LLMs with minimal computational resources, demonstrating its effectiveness through automatic metrics and human evaluation.

Abstract: We present Adjacent Possible Exploration (APE), a simple yet effective method
for adapting large language models to specific tasks using minimal
computational resources. Unlike traditional fine-tuning that requires extensive
compute, APE iteratively fine-tunes models on small, carefully selected data
batches (200 examples), retaining only improvements. On news summarization, APE
achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,
matching or exceeding more complex methods like LoRA while remaining
conceptually simple. Our approach is particularly valuable for researchers and
practitioners with limited computational resources. We provide open-source code
and demonstrate APE's effectiveness through both automatic metrics and human
evaluation. While inspired by evolutionary theory's "adjacent possible", APE's
core insight has a very practical application: small, iterative data
perturbations can efficiently guide LLMs toward task-specific performance
without expensive retraining.

</details>


### [604] [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)
*Ahan Prasannakumar Shetty*

Main category: cs.CL

TL;DR: This paper evaluates various machine translation models for English-Hindi translation using a large parallel corpus and custom FAQ dataset, revealing varying performance levels across different metrics.


<details>
  <summary>Details</summary>
Motivation: To bridge linguistic gaps between diverse languages like English and Hindi by assessing the effectiveness of different machine translation approaches.

Method: Evaluation of multiple machine translation models using both lexical and machine learning-based metrics on an 18000+ corpus of English-Hindi parallel dataset and a custom FAQ dataset.

Result: Models show varying performance levels across different metrics, indicating strengths and areas needing improvement in current translation systems.

Conclusion: Current machine translation systems have strengths but also clear areas for improvement when handling both general and specialized language domains.

Abstract: Machine translation has become a critical tool in bridging linguistic gaps,
especially between languages as diverse as English and Hindi. This paper
comprehensively evaluates various machine translation models for translating
between English and Hindi. We assess the performance of these models using a
diverse set of automatic evaluation metrics, both lexical and machine
learning-based metrics. Our evaluation leverages an 18000+ corpus of English
Hindi parallel dataset and a custom FAQ dataset comprising questions from
government websites. The study aims to provide insights into the effectiveness
of different machine translation approaches in handling both general and
specialized language domains. Results indicate varying performance levels
across different metrics, highlighting strengths and areas for improvement in
current translation systems.

</details>


### [605] [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
*Jiangjie Chen,Qianyu He,Siyu Yuan,Aili Chen,Zhicheng Cai,Weinan Dai,Hongli Yu,Qiying Yu,Xuefeng Li,Jiaze Chen,Hao Zhou,Mingxuan Wang*

Main category: cs.CL

TL;DR: Enigmata is a new suite to improve LLMs' puzzle reasoning skills, providing 36 tasks across seven categories with scalable multi-task RL training. The model trained on Enigmata, Qwen2.5-32B-Enigmata, surpasses other models on various benchmarks and shows generalization benefits.


<details>
  <summary>Details</summary>
Motivation: Despite excelling at advanced reasoning tasks, LLMs still struggle with puzzles solvable by humans without domain knowledge. Thus, there is a need for a comprehensive suite tailored for improving LLMs with puzzle reasoning skills.

Method: Introduced Enigmata, which includes 36 tasks across seven categories with a generator that produces unlimited examples with controllable difficulty and a rule-based verifier for automatic evaluation. Also proposed Enigmata-Eval as a benchmark and developed optimized multi-task RLVR strategies.

Result: The trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on puzzle reasoning benchmarks and generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning. Puzzle data from Enigmata boosts SoTA performance on advanced math and STEM reasoning tasks.

Conclusion: This work offers a unified, controllable framework for advancing logical reasoning in LLMs.

Abstract: Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at
advanced reasoning tasks like math and coding via Reinforcement Learning with
Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans
without domain knowledge. We introduce Enigmata, the first comprehensive suite
tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks
across seven categories, each with 1) a generator that produces unlimited
examples with controllable difficulty and 2) a rule-based verifier for
automatic evaluation. This generator-verifier design supports scalable,
multi-task RL training, fine-grained analysis, and seamless RLVR integration.
We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized
multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,
consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks
like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes
well to out-of-domain puzzle benchmarks and mathematical reasoning, with little
multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking
(20B activated parameters and 200B total parameters), puzzle data from Enigmata
further boosts SoTA performance on advanced math and STEM reasoning tasks such
as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization
benefits of Enigmata. This work offers a unified, controllable framework for
advancing logical reasoning in LLMs. Resources of this work can be found at
https://seed-enigmata.github.io.

</details>


### [606] [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
*Yang Zhang,Yu Yu,Bo Tang,Yu Zhu,Chuxiong Sun,Wenqiang Wei,Jie Hu,Zipeng Xie,Zhiyu Li,Feiyu Xiong,Edward Chung*

Main category: cs.CL

TL;DR: The paper introduces MARA, a new method for aligning LLMs with human preferences that simplifies the process and reduces computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing alignment techniques like RLHF or DPO require direct fine-tuning on large models leading to high computational costs and inefficiencies.

Method: MARA decomposes sentence-level preference learning into token-level binary classification using a compact three-layer fully-connected network to 'Accept' or 'Reject' tokens.

Result: Experiments across seven LLMs and three datasets show MARA improves alignment performance while reducing computational costs.

Conclusion: MARA offers an efficient alternative for aligning LLMs with human preferences without directly fine-tuning the models.

Abstract: With the rapid development of Large Language Models (LLMs), aligning these
models with human preferences and values is critical to ensuring ethical and
safe applications. However, existing alignment techniques such as RLHF or DPO
often require direct fine-tuning on LLMs with billions of parameters, resulting
in substantial computational costs and inefficiencies. To address this, we
propose Micro token-level Accept-Reject Aligning (MARA) approach designed to
operate independently of the language models. MARA simplifies the alignment
process by decomposing sentence-level preference learning into token-level
binary classification, where a compact three-layer fully-connected network
determines whether candidate tokens are "Accepted" or "Rejected" as part of the
response. Extensive experiments across seven different LLMs and three
open-source datasets show that MARA achieves significant improvements in
alignment performance while reducing computational costs.

</details>


### [607] [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)
*Sirui Chen,Shuqin Ma,Shu Yu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: The paper explores the concept of consciousness in large language models (LLMs), clarifies related terminologies, reviews existing research, highlights potential risks, and outlines future directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic exploration into the consciousness of LLMs and to clarify frequently conflated terms such as 'consciousness' and 'awareness'.

Method: Clarify terminologies, organize and synthesize existing research from theoretical and empirical perspectives, highlight potential frontier risks, and discuss challenges and future directions.

Result: Provides a clearer understanding of LLM consciousness, identifies associated risks, and suggests areas for further investigation.

Conclusion: Research into LLM consciousness is crucial and requires continued exploration given the rapid development of LLMs.

Abstract: Consciousness stands as one of the most profound and distinguishing features
of the human mind, fundamentally shaping our understanding of existence and
agency. As large language models (LLMs) develop at an unprecedented pace,
questions concerning intelligence and consciousness have become increasingly
significant. However, discourse on LLM consciousness remains largely unexplored
territory. In this paper, we first clarify frequently conflated terminologies
(e.g., LLM consciousness and LLM awareness). Then, we systematically organize
and synthesize existing research on LLM consciousness from both theoretical and
empirical perspectives. Furthermore, we highlight potential frontier risks that
conscious LLMs might introduce. Finally, we discuss current challenges and
outline future directions in this emerging field. The references discussed in
this paper are organized at
https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.

</details>


### [608] [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)
*Hexuan Deng,Wenxiang Jiao,Xuebo Liu,Jun Rao,Min Zhang*

Main category: cs.CL

TL;DR: REA-RL introduces a small reflection model for efficient online training of LRMs with parallel sampling and sequential revision, plus a reflection reward to prevent short non-reflective responses. This reduces inference costs by 35% without performance loss.


<details>
  <summary>Details</summary>
Motivation: LRMs show strong performance in complex tasks but suffer from overthinking and high inference costs. Current methods to shorten reasoning responses are inefficient for online use or harm performance.

Method: Propose REA-RL which includes a small reflection model for efficient scaling in online training, offering parallel sampling and sequential revision. Also design a reflection reward to avoid short non-reflective responses.

Result: Experiments indicate that these methods maintain or improve performance while greatly enhancing inference efficiency. The combination achieves balance between performance and efficiency, reducing inference costs by 35% without compromising performance.

Conclusion: The methods are effective in maintaining reflection frequency for hard problems and appropriately reducing it for simpler ones, preserving reflection ability.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks
but often face the challenge of overthinking, leading to substantially high
inference costs. Existing approaches synthesize shorter reasoning responses for
LRMs to learn, but are inefficient for online usage due to the time-consuming
data generation and filtering processes. Meanwhile, online reinforcement
learning mainly adopts a length reward to encourage short reasoning responses,
but tends to lose the reflection ability and harm the performance. To address
these issues, we propose REA-RL, which introduces a small reflection model for
efficient scaling in online training, offering both parallel sampling and
sequential revision. Besides, a reflection reward is designed to further
prevent LRMs from favoring short yet non-reflective responses. Experiments show
that both methods maintain or enhance performance while significantly improving
inference efficiency. Their combination achieves a good balance between
performance and efficiency, reducing inference costs by 35% without
compromising performance. Further analysis demonstrates that our methods are
effective by maintaining reflection frequency for hard problems while
appropriately reducing it for simpler ones without losing reflection ability.
Codes are available at https://github.com/hexuandeng/REA-RL.

</details>


### [609] [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)
*Debargha Ganguly,Vikash Singh,Sreehari Sankar,Biyao Zhang,Xuecen Zhang,Srinivasan Iyengar,Xiaotian Han,Amit Sharma,Shivkumar Kalyanaraman,Vipin Chaudhary*

Main category: cs.CL

TL;DR: Large language models (LLMs) have great potential in automated reasoning by creating formal specifications. However, there is a conflict as LLMs are probabilistic while formal verification needs deterministic guarantees. This paper explores failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. A study of five leading LLMs shows the impact of SMT-based autoformalization on accuracy varies depending on the task type. Known UQ techniques fail to spot these errors. The authors introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, resulting in an improved uncertainty taxonomy. Uncertainty signals depend on the task and selective verification drastically reduces errors.


<details>
  <summary>Details</summary>
Motivation: To bridge the epistemological gap between probabilistic LLMs and deterministic formal verification requirements.

Method: Systematically evaluate failure modes and uncertainty quantification in LLM-generated formal artifacts using five frontier LLMs. Introduce a probabilistic context-free grammar (PCFG) framework for modeling LLM outputs and develop a refined uncertainty taxonomy.

Result: SMT-based autoformalization affects accuracy differently based on task domain. Known UQ techniques are insufficient. The PCFG framework provides better uncertainty identification with task-dependent uncertainty signals. Selective verification significantly reduces errors.

Conclusion: By introducing a PCFG framework and selective verification, LLM-driven formalization can become a reliable engineering discipline.

Abstract: Large language models (LLMs) show remarkable promise for democratizing
automated reasoning by generating formal specifications. However, a fundamental
tension exists: LLMs are probabilistic, while formal verification demands
deterministic guarantees. This paper addresses this epistemological gap by
comprehensively investigating failure modes and uncertainty quantification (UQ)
in LLM-generated formal artifacts. Our systematic evaluation of five frontier
LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's
domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on
factual ones), with known UQ techniques like the entropy of token probabilities
failing to identify these errors. We introduce a probabilistic context-free
grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty
taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy
for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables
selective verification, drastically reducing errors (14-100%) with minimal
abstention, transforming LLM-driven formalization into a reliable engineering
discipline.

</details>


### [610] [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)
*Yige Yuan,Teng Xiao,Shuchang Tao,Xue Wang,Jinyang Gao,Bolin Ding,Bingbing Xu*

Main category: cs.CL

TL;DR: 本研究提出了一种通过较弱模型监督来激励大型语言模型（LLMs）推理能力的新方法，无需昂贵的高质量演示或强化学习。该方法成本低且效果显著，能接近昂贵强化学习94%的收益，并在多种任务和模型架构中表现出色。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 当前提升LLMs推理能力的方法依赖于昂贵的强化学习或高质量的链式思维示范的监督微调。为了解决成本问题，研究探索了通过显著较弱的模型进行监督是否可以有效激励LLMs的推理能力。

Method: 研究分析了何时以及为何较弱模型的监督能够成功激发更强模型的推理能力。具体来说，使用较弱的推理者作为监督源，训练更强大的学生模型。

Result: 实验表明，来自显著较弱推理者的监督可以极大地提高学生的推理表现，以极小的成本恢复接近昂贵强化学习94%的收益。在不同基准测试和模型架构上的一致性实验显示，这种方法能有效提升各种推理任务的表现。

Conclusion: 研究表明，这种从弱到强的范式是一种有前途且可推广的替代方案，可以在LLMs推理时以低成本激励强大的推理能力。

Abstract: Large language models (LLMs) have demonstrated impressive performance on
reasoning-intensive tasks, but enhancing their reasoning abilities typically
relies on either reinforcement learning (RL) with verifiable signals or
supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)
demonstrations, both of which are expensive. In this paper, we study a novel
problem of incentivizing the reasoning capacity of LLMs without expensive
high-quality demonstrations and reinforcement learning. We investigate whether
the reasoning capabilities of LLMs can be effectively incentivized via
supervision from significantly weaker models. We further analyze when and why
such weak supervision succeeds in eliciting reasoning abilities in stronger
models. Our findings show that supervision from significantly weaker reasoners
can substantially improve student reasoning performance, recovering close to
94% of the gains of expensive RL at a fraction of the cost. Experiments across
diverse benchmarks and model architectures demonstrate that weak reasoners can
effectively incentivize reasoning in stronger student models, consistently
improving performance across a wide range of reasoning tasks. Our results
suggest that this simple weak-to-strong paradigm is a promising and
generalizable alternative to costly methods for incentivizing strong reasoning
capabilities at inference-time in LLMs. The code is publicly available at
https://github.com/yuanyige/W2SR.

</details>


### [611] [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
*Yige Yuan,Teng Xiao,Li Yunfan,Bingbing Xu,Shuchang Tao,Yunqi Qiu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper proposes Simple Energy Adaptation (SEA), a gradient-based sampling algorithm that aligns large language models with human feedback during inference by optimizing an energy function in continuous latent space, outperforming existing methods on AdvBench and MATH benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, leading to limited effectiveness.

Method: SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy. It adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space instead of searching over discrete space.

Result: SEA outperforms the second-best baseline with a relative improvement of up to 77.51% on AdvBench and 16.36% on MATH.

Conclusion: SEA is a simple yet effective algorithm for inference-time alignment that directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space.

Abstract: Aligning large language models with human feedback at inference time has
received increasing attention due to its flexibility. Existing methods rely on
generating multiple responses from the base policy for search using a reward
model, which can be considered as searching in a discrete response space.
However, these methods struggle to explore informative candidates when the base
policy is weak or the candidate set is small, resulting in limited
effectiveness. In this paper, to address this problem, we propose Simple Energy
Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for
inference-time alignment. In contrast to expensive search over the discrete
space, SEA directly adapts original responses from the base policy toward the
optimal one via gradient-based sampling in continuous latent space.
Specifically, SEA formulates inference as an iterative optimization procedure
on an energy function over actions in the continuous space defined by the
optimal policy, enabling simple and effective alignment. For instance, despite
its simplicity, SEA outperforms the second-best baseline with a relative
improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on
MATH. Our code is publicly available at https://github.com/yuanyige/SEA

</details>


### [612] [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)
*Konstantin Dobler,Desmond Elliott,Gerard de Melo*

Main category: cs.CL

TL;DR: AweDist is a new method that efficiently initializes embeddings for new tokens in language models by distilling representations from the original tokenization, leading to high-quality input embeddings without extensive retraining.


<details>
  <summary>Details</summary>
Motivation: Current language models suffer decreased performance and increased computational cost when dealing with domains underrepresented in their static vocabularies. Adding new tokens could solve this issue but existing embedding initialization methods are costly.

Method: The authors propose AweDist, which distills representations obtained using the original tokenization to quickly learn high-quality input embeddings for new tokens without requiring expensive further training or pretraining of additional modules.

Result: Experimental results demonstrate that AweDist outperforms even strong baselines across a wide range of open-weight models.

Conclusion: AweDist provides an effective solution for initializing embeddings of new tokens in language models, enhancing adaptability to underrepresented domains.

Abstract: Current language models rely on static vocabularies determined at pretraining
time, which can lead to decreased performance and increased computational cost
for domains underrepresented in the original vocabulary. New tokens can be
added to solve this problem, when coupled with a good initialization for their
new embeddings. However, existing embedding initialization methods either
require expensive further training or pretraining of additional modules. In
this paper, we propose AweDist and show that by distilling representations
obtained using the original tokenization, we can quickly learn high-quality
input embeddings for new tokens. Experimental results with a wide range of
open-weight models show that AweDist is able to outperform even strong
baselines.

</details>


### [613] [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的模型融合方法SeMe，该方法无需数据和训练，通过潜在语义对齐在细粒度、逐层级别上合并语言模型。SeMe不仅保留了模型行为，还显式稳定了内部知识，在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在各种任务中表现出色，但没有单一模型能始终胜过其他模型，因此需要有效的方法来结合它们的优势，而无需昂贵的再训练。现有的模型融合技术通常依赖于数据相关的计算或无法保存内部知识，限制了其鲁棒性和可扩展性。

Method: 介绍了一种新的、无需数据和训练的模型融合方法SeMe，利用潜在语义对齐在细粒度、逐层级别上合并语言模型。与以前的工作不同，SeMe不仅保留了模型行为，还显式地稳定了内部知识。

Result: 通过广泛的实验表明，SeMe在性能和效率上都优于现有方法，并消除了对外部数据的依赖。

Conclusion: 这项工作为知识感知模型融合建立了一个新范例，并提供了关于语言模型语义结构的见解，为更可扩展和可解释的模型组合铺平了道路。

Abstract: Despite the remarkable capabilities of Language Models (LMs) across diverse
tasks, no single model consistently outperforms others, necessitating efficient
methods to combine their strengths without expensive retraining. Existing model
merging techniques, such as parameter averaging and task-guided fusion, often
rely on data-dependent computations or fail to preserve internal knowledge,
limiting their robustness and scalability. We introduce SeMe (Semantic-based
Merging), a novel, data-free, and training-free approach that leverages latent
semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike
prior work, SeMe not only preserves model behaviors but also explicitly
stabilizes internal knowledge, addressing a critical gap in LM fusion. Through
extensive experiments across diverse architectures and tasks, we demonstrate
that SeMe outperforms existing methods in both performance and efficiency while
eliminating reliance on external data. Our work establishes a new paradigm for
knowledge-aware model merging and provides insights into the semantic structure
of LMs, paving the way for more scalable and interpretable model composition.

</details>


### [614] [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)
*Thang Nguyen,Peter Chin,Yu-Wing Tai*

Main category: cs.CL

TL;DR: The paper introduces MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation that uses specialized AI agents to handle ambiguities and reasoning challenges in complex information-seeking tasks. It provides robust and interpretable results without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods struggle with ambiguities and reasoning challenges in complex information-seeking tasks, typically relying on end-to-end fine-tuning or isolated component enhancements.

Method: MA-RAG employs a set of specialized AI agents (Planner, Step Definer, Extractor, and QA Agents) that work collaboratively. These agents handle subtasks like query disambiguation, evidence extraction, and answer synthesis using chain-of-thought prompting. They communicate intermediate reasoning to refine the retrieval and synthesis process dynamically.

Result: Experiments on multi-hop and ambiguous QA benchmarks show that MA-RAG surpasses state-of-the-art training-free baselines and is competitive with fine-tuned systems.

Conclusion: MA-RAG's modular and reasoning-driven architecture effectively addresses ambiguities and reasoning challenges in complex information-seeking tasks, offering robust and interpretable results.

Abstract: We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation
(RAG) that addresses the inherent ambiguities and reasoning challenges in
complex information-seeking tasks. Unlike conventional RAG methods that rely on
either end-to-end fine-tuning or isolated component enhancements, MA-RAG
orchestrates a collaborative set of specialized AI agents: Planner, Step
Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline
with task-aware reasoning. Ambiguities may arise from underspecified queries,
sparse or indirect evidence in retrieved documents, or the need to integrate
information scattered across multiple sources. MA-RAG mitigates these
challenges by decomposing the problem into subtasks, such as query
disambiguation, evidence extraction, and answer synthesis, and dispatching them
to dedicated agents equipped with chain-of-thought prompting. These agents
communicate intermediate reasoning and progressively refine the retrieval and
synthesis process. Our design allows fine-grained control over information flow
without any model fine-tuning. Crucially, agents are invoked on demand,
enabling a dynamic and efficient workflow that avoids unnecessary computation.
This modular and reasoning-driven architecture enables MA-RAG to deliver
robust, interpretable results. Experiments on multi-hop and ambiguous QA
benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free
baselines and rivals fine-tuned systems, validating the effectiveness of
collaborative agent-based reasoning in RAG.

</details>


### [615] ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)
*Alkis Koudounas,Moreno La Quatra,Eliana Pastor,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: This paper explores the use of Kolmogorov-Arnold Networks (KANs) in Spoken Language Understanding (SLU) tasks, integrating KAN layers within 2D-CNN models and transformer-based models. The results indicate that KAN layers can replace linear layers with comparable or better performance, while offering unique insights into input attention.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of Kolmogorov-Arnold Networks (KANs) in Spoken Language Understanding (SLU) tasks, as their application to speech processing has been under explored.

Method: Experimenting with 2D-CNN models on two datasets by integrating KAN layers in five different configurations within the dense block, then applying the best-performing setup to transformer-based models and evaluating them on five SLU datasets with increasing complexity.

Result: KAN layers effectively replaced linear layers, achieving comparable or superior performance in most cases, and provided insights into how KAN and linear layers differently attend to input regions of raw waveforms.

Conclusion: KAN layers show promise in SLU tasks, potentially replacing traditional linear layers in neural architectures.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional neural architectures, yet their application to
speech processing remains under explored. This work presents the first
investigation of KANs for Spoken Language Understanding (SLU) tasks. We
experiment with 2D-CNN models on two datasets, integrating KAN layers in five
different configurations within the dense block. The best-performing setup,
which places a KAN layer between two linear layers, is directly applied to
transformer-based models and evaluated on five SLU datasets with increasing
complexity. Our results show that KAN layers can effectively replace the linear
layers, achieving comparable or superior performance in most cases. Finally, we
provide insights into how KAN and linear layers on top of transformers
differently attend to input regions of the raw waveforms.

</details>


### [616] [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
*Chuangtao Ma,Yongrui Chen,Tianxing Wu,Arijit Khan,Haofen Wang*

Main category: cs.CL

TL;DR: This paper surveys the integration of large language models (LLMs) and knowledge graphs (KGs) for question-answering (QA), presenting a new taxonomy, comparing approaches, and highlighting challenges and opportunities.


<details>
  <summary>Details</summary>
Motivation: LLM-based QA systems face challenges such as poor reasoning, outdated knowledge, and hallucinations when dealing with complex QA tasks. Integrating LLMs with KGs can potentially overcome these issues.

Method: The authors propose a structured taxonomy to classify methods that combine LLMs and KGs for QA. They review state-of-the-art techniques, compare their strengths and limitations, and analyze how they address different complex QA challenges.

Result: The survey provides insights into advancements in synthesizing LLMs and KGs, outlines evaluation metrics and benchmark datasets, and identifies open challenges and future research directions.

Conclusion: Integrating LLMs and KGs is promising for improving QA systems, but there are still significant challenges and opportunities for future work.

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
question-answering (QA) tasks because of their superior capabilities in natural
language understanding and generation. However, LLM-based QA struggles with
complex QA tasks due to poor reasoning capacity, outdated knowledge, and
hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)
for QA to address the above challenges. In this survey, we propose a new
structured taxonomy that categorizes the methodology of synthesizing LLMs and
KGs for QA according to the categories of QA and the KG's role when integrating
with LLMs. We systematically survey state-of-the-art advances in synthesizing
LLMs and KGs for QA and compare and analyze these approaches in terms of
strength, limitations, and KG requirements. We then align the approaches with
QA and discuss how these approaches address the main challenges of different
complex QA. Finally, we summarize the advancements, evaluation metrics, and
benchmark datasets and highlight open challenges and opportunities.

</details>


### [617] [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)
*June-Woo Kim,Wonkyo Oh,Haram Yoon,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.CL

TL;DR: This paper presents a language-agnostic framework for assessing suicidal risk in adolescents using large language models (LLMs). It generates Chinese transcripts from speech and uses LLMs to extract features, which are retained in both Chinese and English. The method performs comparably to other models while showing potential to overcome language constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for a scalable and generalizable method for detecting suicidal risk in adolescents that does not rely on language-specific models.

Method: The method involves generating Chinese transcripts from speech using an ASR model, employing LLMs with prompt-based queries to extract suicidal risk-related features from these transcripts in both Chinese and English, and then fine-tuning corresponding pretrained language models independently.

Result: The results show that the method achieves performance comparable to direct fine-tuning with ASR results or to models trained solely on Chinese suicidal risk-related features.

Conclusion: The study concludes that the proposed framework has the potential to overcome language constraints and improve the robustness of suicidal risk assessment.

Abstract: Suicidal risk detection in adolescents is a critical challenge, yet existing
methods rely on language-specific models, limiting scalability and
generalization. This study introduces a novel language-agnostic framework for
suicidal risk assessment with large language models (LLMs). We generate Chinese
transcripts from speech using an ASR model and then employ LLMs with
prompt-based queries to extract suicidal risk-related features from these
transcripts. The extracted features are retained in both Chinese and English to
enable cross-linguistic analysis and then used to fine-tune corresponding
pretrained language models independently. Experimental results show that our
method achieves performance comparable to direct fine-tuning with ASR results
or to models trained solely on Chinese suicidal risk-related features,
demonstrating its potential to overcome language constraints and improve the
robustness of suicidal risk assessment.

</details>


### [618] [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
*Haolei Bai,Siyong Jian,Tuo Liang,Yu Yin,Huan Wang*

Main category: cs.CL

TL;DR: Large language models (LLMs) are big and memory-heavy, making them hard to deploy. SVD can compress them but has issues like neglecting the residual matrix and performance degradation when all layers are compressed. The new method ResSVD solves these by using the residual matrix and selectively compressing only some layers, leading to better performance.


<details>
  <summary>Details</summary>
Motivation: To improve the practical deployment of large language models by developing an efficient compression strategy that addresses the limitations of current SVD-based methods.

Method: Propose ResSVD, a post-training SVD-based LLM compression method that utilizes the residual matrix from truncation to reduce loss and selectively compresses only the last few layers under a fixed overall compression ratio.

Result: ResSVD consistently outperforms existing methods in compressing diverse LLM families across multiple benchmark datasets, showing practical effectiveness.

Conclusion: ResSVD is a superior LLM compression method that mitigates error propagation and enhances performance by leveraging the residual matrix and selective layer compression.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in a
wide range of downstream natural language processing tasks. Nevertheless, their
considerable sizes and memory demands hinder practical deployment, underscoring
the importance of developing efficient compression strategies. Singular value
decomposition (SVD) decomposes a matrix into orthogonal components, enabling
efficient low-rank approximation. This is particularly suitable for LLM
compression, where weight matrices often exhibit significant redundancy.
However, current SVD-based methods neglect the residual matrix from truncation,
resulting in significant truncation loss. Additionally, compressing all layers
of the model results in severe performance degradation. To overcome these
limitations, we propose ResSVD, a new post-training SVD-based LLM compression
method. Specifically, we leverage the residual matrix generated during the
truncation process to reduce truncation loss. Moreover, under a fixed overall
compression ratio, we selectively compress the last few layers of the model,
which mitigates error propagation and significantly improves the performance of
compressed models.Comprehensive evaluations of ResSVD on diverse LLM families
and multiple benchmark datasets indicate that ResSVD consistently achieves
superior performance over existing counterpart methods, demonstrating its
practical effectiveness.

</details>


### [619] [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)
*Cristian Santini,Laura Melosi,Emanuele Frontoni*

Main category: cs.CL

TL;DR: The abstract discusses the challenges in processing historical texts, especially for Italian literature. It presents a new dataset based on 19th-century scholarly notes and evaluates domain-specific BERT-based models and state-of-the-art LLMs like LLaMa3.1 for Named Entity Recognition (NER). Results indicate that fine-tuned NER models perform better than instruction-tuned models for entity extraction from historical humanistic texts.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need for computational techniques to adapt to the challenges of historical texts, such as orthographic and spelling variations, fragmentary structure, and digitization errors. The rise of large language models (LLMs) has revolutionized natural language processing, but no thorough evaluation exists for Italian texts.

Method: The research uses a new challenging dataset for entity extraction based on Giacomo Leopardi's Zibaldone, a corpus of 19th century scholarly notes with 2,899 references to people, locations, and literary works. Reproducible experiments are carried out with both domain-specific BERT-based models and state-of-the-art LLMs like LLaMa3.1.

Result: Instruction-tuned models face difficulties when handling historical humanistic texts, whereas fine-tuned NER models provide more robust performance, even with challenging entity types such as bibliographic references.

Conclusion: Fine-tuned NER models offer better performance for entity extraction from historical humanistic texts compared to instruction-tuned models.

Abstract: The increased digitization of world's textual heritage poses significant
challenges for both computer science and literary studies. Overall, there is an
urgent need of computational techniques able to adapt to the challenges of
historical texts, such as orthographic and spelling variations, fragmentary
structure and digitization errors. The rise of large language models (LLMs) has
revolutionized natural language processing, suggesting promising applications
for Named Entity Recognition (NER) on historical documents. In spite of this,
no thorough evaluation has been proposed for Italian texts. This research tries
to fill the gap by proposing a new challenging dataset for entity extraction
based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's
Zibaldone (1898), containing 2,899 references to people, locations and literary
works. This dataset was used to carry out reproducible experiments with both
domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.
Results show that instruction-tuned models encounter multiple difficulties
handling historical humanistic texts, while fine-tuned NER models offer more
robust performance even with challenging entity types such as bibliographic
references.

</details>


### [620] [FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](https://arxiv.org/abs/2505.20225)
*Hao Kang,Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: Recent large language models increasingly adopt Mixture-of-Experts (MoE) architectures. However, academic researchers still lack a fully open, end-to-end MoE platform. To address this issue, the authors release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to provide academic researchers with a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior in large language models.

Method: The method involves releasing FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture closely reflects modern production LLMs.

Result: FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs across six evaluation tasks.

Conclusion: The conclusion is that FLAME-MoE provides a valuable open-source platform for researching MoE architectures in large language models.

Abstract: Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4
increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong
efficiency-performance trade-offs by activating only a fraction of the model
per token. Yet academic researchers still lack a fully open, end-to-end MoE
platform for investigating scaling, routing, and expert behavior. We release
FLAME-MoE, a completely open-source research suite composed of seven
decoder-only models, ranging from 38M to 1.7B active parameters, whose
architecture--64 experts with top-8 gating and 2 shared experts--closely
reflects modern production LLMs. All training data pipelines, scripts, logs,
and checkpoints are publicly available to enable reproducible experimentation.
Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4
points over dense baselines trained with identical FLOPs. Leveraging full
training trace transparency, we present initial analyses showing that (i)
experts increasingly specialize on distinct token subsets, (ii) co-activation
matrices remain sparse, reflecting diverse expert usage, and (iii) routing
behavior stabilizes early in training. All code, training logs, and model
checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.

</details>


### [621] [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
*Michael Kirchhof,Luca Füger,Adam Goliński,Eeshan Gunesh Dhekane,Arno Blaas,Sinead Williamson*

Main category: cs.CL

TL;DR: The paper introduces SelfReflect, a metric to evaluate how well a string summarizes the output distribution of large language models (LLMs), revealing LLM uncertainties.


<details>
  <summary>Details</summary>
Motivation: To uncover when a large language model (LLM) is uncertain about its responses and explore methods beyond just producing percentage numbers for uncertainty quantification.

Method: Developed SelfReflect, a theoretically-motivated metric, to assess the summarization of an LLM's internal answer distribution. Investigated various self-summarization methods using this metric.

Result: SelfReflect can discern subtle differences in candidate summary strings, aligns with human judgment, and outperforms other metrics like LLM judges and embedding comparisons. Found that even advanced reasoning models have difficulty explicating their internal uncertainty, but faithful summarizations can be generated through sampling and summarizing.

Conclusion: SelfReflect provides a foundation for understanding LLM uncertainties and enables future work towards universally explicating these uncertainties.

Abstract: To reveal when a large language model (LLM) is uncertain about a response,
uncertainty quantification commonly produces percentage numbers along with the
output. But is this all we can do? We argue that in the output space of LLMs,
the space of strings, exist strings expressive enough to summarize the
distribution over output strings the LLM deems possible. We lay a foundation
for this new avenue of uncertainty explication and present SelfReflect, a
theoretically-motivated metric to assess how faithfully a string summarizes an
LLM's internal answer distribution. We show that SelfReflect is able to
discriminate even subtle differences of candidate summary strings and that it
aligns with human judgement, outperforming alternative metrics such as LLM
judges and embedding comparisons. With SelfReflect, we investigate a number of
self-summarization methods and find that even state-of-the-art reasoning models
struggle to explicate their internal uncertainty. But we find that faithful
summarizations can be generated by sampling and summarizing. Our metric enables
future works towards this universal form of LLM uncertainties.

</details>


### [622] [Reasoning LLMs are Wandering Solution Explorers](https://arxiv.org/abs/2505.20296)
*Jiahao Lu,Ziwei Xu,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLMs）在测试时计算技术方面表现出强大的推理能力，但当前的推理LLMs缺乏系统地探索解空间的能力。本文通过定性和定量分析揭示了这些问题，并提倡开发新的评估指标和工具来衡量推理过程本身而非仅仅是最终输出。


<details>
  <summary>Details</summary>
Motivation: 现有的推理LLMs虽然表现出了强大的推理能力，但它们缺乏系统性探索问题解空间的能力。因此，需要研究和改进这些模型以提高其系统性问题解决能力。

Method: 本文通过定性和定量分析多个最先进的LLMs，识别出推理LLMs中的常见失败模式，包括无效的推理步骤、冗余的探索、虚构或不忠实的结论等。

Result: 发现当前模型在简单任务上看似表现良好，但在任务复杂度增加时性能急剧下降。

Conclusion: 作者建议开发新的评估指标和工具，不仅评估模型的最终输出，还评估推理过程的结构。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning abilities
through test-time computation (TTC) techniques such as chain-of-thought
prompting and tree-based reasoning. However, we argue that current reasoning
LLMs (RLLMs) lack the ability to systematically explore the solution space.
This paper formalizes what constitutes systematic problem solving and
identifies common failure modes that reveal reasoning LLMs to be wanderers
rather than systematic explorers. Through qualitative and quantitative analysis
across multiple state-of-the-art LLMs, we uncover persistent issues: invalid
reasoning steps, redundant explorations, hallucinated or unfaithful
conclusions, and so on. Our findings suggest that current models' performance
can appear to be competent on simple tasks yet degrade sharply as complexity
increases. Based on the findings, we advocate for new metrics and tools that
evaluate not just final outputs but the structure of the reasoning process
itself.

</details>


### [623] [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)
*Yongan Yu,Mengqian Wu,Yiran Lin,Nikki G. Lobczowski*

Main category: cs.CL

TL;DR: This paper proposes THiNK, a multi-agent feedback-driven framework based on Bloom's Taxonomy for evaluating higher-order thinking skills in LLMs. It reveals that LLMs perform well in lower-order tasks but struggle with higher-order ones. Structured feedback loops improve their reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing higher-order thinking skills in large language models beyond surface-level accuracy.

Method: THiNK, a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy, which frames reasoning assessment as an iterative task of problem generation, critique, and revision.

Result: LLMs reliably perform lower-order categories well, but struggle with applying knowledge in realistic contexts and abstraction. Structured feedback loops significantly improve reasoning performance in higher-order thinking.

Conclusion: THiNK provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science.

Abstract: Assessing higher-order thinking skills in large language models (LLMs)
remains a fundamental challenge, especially in tasks that go beyond
surface-level accuracy. In this work, we propose THiNK (Testing Higher-order
Notion of Knowledge), a multi-agent, feedback-driven evaluation framework
grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative
task of problem generation, critique, and revision, encouraging LLMs to
think-aloud through step-by-step reflection and refinement. This enables a
systematic evaluation of both lower-order (e.g., remember, understand) and
higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven
state-of-the-art LLMs and perform a detailed cognitive analysis of their
outputs. Results reveal that while models reliably perform lower-order
categories well, they struggle with applying knowledge in realistic contexts
and exhibit limited abstraction. Structured feedback loops significantly
improve reasoning performance, particularly in higher-order thinking.
Qualitative evaluations further confirm that THiNK-guided outputs better align
with domain logic and problem structure. The code of our framework provides a
scalable methodology for probing and enhancing LLM reasoning, offering new
directions for evaluation grounded in learning science, which is available at
our GitHub repository.

</details>


### [624] [KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing](https://arxiv.org/abs/2505.20245)
*Rui Li,Quanyu Dai,Zeyu Zhang,Xu Chen,Zhenhua Dong,Ji-Rong Wen*

Main category: cs.CL

TL;DR: The paper introduces KnowTrace, an RAG framework that mitigates context overload and enhances multi-step reasoning for LLMs in multi-hop question answering. It organizes retrieved information into a knowledge graph, providing a structured context for inference and generating supervision data for self-bootstrapping. Experiments show consistent improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods for LLMs face challenges with context overload and ineffective reasoning steps when handling complex multi-hop questions. This necessitates a solution to manage the growing context and improve the quality of multi-step reasoning.

Method: KnowTrace autonomously traces desired knowledge triplets to construct a knowledge graph relevant to the input question. This structured approach provides an intelligible context for LLM inference and incorporates a reflective mechanism for knowledge backtracing, which identifies contributive LLM generations as process supervision data for self-bootstrapping.

Result: KnowTrace outperforms existing methods across three multi-hop question answering benchmarks. The bootstrapped version of KnowTrace further enhances these improvements.

Conclusion: KnowTrace effectively addresses context overload and boosts multi-step reasoning quality in LLMs for multi-hop question answering tasks.

Abstract: Recent advances in retrieval-augmented generation (RAG) furnish large
language models (LLMs) with iterative retrievals of relevant information to
handle complex multi-hop questions. These methods typically alternate between
LLM reasoning and retrieval to accumulate external information into the LLM's
context. However, the ever-growing context inherently imposes an increasing
burden on the LLM to perceive connections among critical information pieces,
with futile reasoning steps further exacerbating this overload issue. In this
paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the
context overload and (2) bootstrap higher-quality multi-step reasoning. Instead
of simply piling the retrieved contents, KnowTrace autonomously traces out
desired knowledge triplets to organize a specific knowledge graph relevant to
the input question. Such a structured workflow not only empowers the LLM with
an intelligible context for inference, but also naturally inspires a reflective
mechanism of knowledge backtracing to identify contributive LLM generations as
process supervision data for self-bootstrapping. Extensive experiments show
that KnowTrace consistently surpasses existing methods across three multi-hop
question answering benchmarks, and the bootstrapped version further amplifies
the gains.

</details>


### [625] [WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2505.20249)
*Yongan Yu,Qingchen Hu,Xianda Du,Jiayin Wang,Fengran Mo,Renee Sieber*

Main category: cs.CL

TL;DR: 本研究开发了一个破坏性天气影响数据集和首个评估大语言模型在破坏性天气影响上的能力的基准WXImpactBench，提供了对大语言模型在气候变化适应系统中挑战的第一手分析，并公开了数据集和评估框架代码。


<details>
  <summary>Details</summary>
Motivation: 气候适应需要理解破坏性天气对社会的影响，而大语言模型可能适用，但因高质量语料库收集困难和缺乏可用基准，其有效性尚未得到充分探索。

Method: 首先开发了一个具有四个阶段精心设计构建管道的破坏性天气影响数据集，然后提出了WXImpactBench基准，包含多标签分类和基于排名的问题回答两个评估任务。

Result: 通过对一系列大语言模型的广泛实验，提供了关于开发破坏性天气影响理解和气候适应系统挑战的第一手分析。

Conclusion: 构建的数据集和评估框架代码已公开，以帮助社会抵御灾害带来的脆弱性。

Abstract: Climate change adaptation requires the understanding of disruptive weather
impacts on society, where large language models (LLMs) might be applicable.
However, their effectiveness is under-explored due to the difficulty of
high-quality corpus collection and the lack of available benchmarks. The
climate-related events stored in regional newspapers record how communities
adapted and recovered from disasters. However, the processing of the original
corpus is non-trivial. In this study, we first develop a disruptive weather
impact dataset with a four-stage well-crafted construction pipeline. Then, we
propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs
on disruptive weather impacts. The benchmark involves two evaluation tasks,
multi-label classification and ranking-based question answering. Extensive
experiments on evaluating a set of LLMs provide first-hand analysis of the
challenges in developing disruptive weather impact understanding and climate
change adaptation systems. The constructed dataset and the code for the
evaluation framework are available to help society protect against
vulnerabilities from disasters.

</details>


### [626] [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)
*Dong Nguyen,Esther Ploeger*

Main category: cs.CL

TL;DR: An opinion paper discussing the challenges of measuring data diversity in NLP datasets and advocating for interdisciplinary approaches.


<details>
  <summary>Details</summary>
Motivation: There is a lack of exploration on how to measure diversity in NLP datasets despite growing attention on the importance of diversity.

Method: This paper examines conceptual and methodological challenges associated with measuring data diversity.

Result: The paper argues that interdisciplinary perspectives are necessary for creating more detailed and valid measures of data diversity.

Conclusion: Interdisciplinary approaches are essential for advancing the measurement of data diversity in NLP datasets.

Abstract: Although diversity in NLP datasets has received growing attention, the
question of how to measure it remains largely underexplored. This opinion paper
examines the conceptual and methodological challenges of measuring data
diversity and argues that interdisciplinary perspectives are essential for
developing more fine-grained and valid measures.

</details>


### [627] [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)
*Anmol Mekala,Anirudh Atmakuru,Yixiao Song,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在处理超长上下文任务时，量化技术可降低内存需求和推理延迟，但可能影响性能。研究评估了五种量化方法对五种模型的影响，发现8位量化基本保持准确率，而4位量化可能导致显著性能下降，具体效果取决于量化方法、模型和任务类型。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型虽然能够支持超过128K token的上下文窗口，但这带来了巨大的内存需求和高推理延迟。量化技术可以缓解这些问题，但可能会降低性能。因此，需要系统地评估量化后的LLM在长输入和长输出任务中的表现。

Method: 研究者对9700个测试样例进行了评估，涵盖了五种量化方法（FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4）和五种模型（Llama-3.1 8B与70B；Qwen-2.5 7B、32B和72B）。通过对比不同量化方法下的模型表现，分析其在长输入和长输出任务中的性能变化。

Result: 平均而言，8位量化方法几乎不影响准确率（约0.8%的下降），而4位量化方法会导致显著的性能损失，特别是在涉及长上下文输入的任务中（最高可达59%的下降）。对于非英语语言输入，这种退化趋势更加明显。不同模型对量化的敏感性也存在差异，例如Qwen-2.5 72B在BNB-nf4下保持稳健，而Llama-3.1 70B在同一任务下性能下降了32%。

Conclusion: 在部署量化的大语言模型之前，应进行仔细的任务特定评估，特别是在长上下文场景和非英语语言任务中。

Abstract: Large language models (LLMs) now support context windows exceeding 128K
tokens, but this comes with significant memory requirements and high inference
latency. Quantization can mitigate these costs, but may degrade performance. In
this work, we present the first systematic evaluation of quantized LLMs on
tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation
spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,
GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,
and 72B). We find that, on average, 8-bit quantization preserves accuracy
(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for
tasks involving long context inputs (drops of up to 59%). This degradation
tends to worsen when the input is in a language other than English. Crucially,
the effects of quantization depend heavily on the quantization method, model,
and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,
Llama-3.1 70B experiences a 32% performance drop on the same task. These
findings highlight the importance of a careful, task-specific evaluation before
deploying quantized LLMs, particularly in long-context scenarios and with
languages other than English.

</details>


### [628] [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding](https://arxiv.org/abs/2505.20298)
*Jeonghun Baek,Kazuki Egashira,Shota Onohara,Atsuyuki Miyai,Yuki Imajuku,Hikaru Ikuta,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: 研究人员引入了两个基准测试，MangaOCR和MangaVQA，以评估大型多模态模型在理解漫画方面的能力，并开发了专门针对漫画的模型MangaLMM。


<details>
  <summary>Details</summary>
Motivation: 漫画是一种复杂的多模态叙事形式，结合了图像和文本。教导大型多模态模型（LMMs）像人类一样理解这种叙事可以帮助漫画创作者反思和改进他们的故事。

Method: 研究者提出了两个基准测试：MangaOCR用于页面内文本识别，MangaVQA通过视觉问答评估上下文理解。基于这些基准，他们从开源LMM Qwen2.5-VL微调出一个漫画专用模型MangaLMM，可以同时处理这两个任务。

Result: 通过广泛的实验，包括与专有模型如GPT-4o和Gemini 2.5的比较，展示了LMMs在理解漫画方面的表现。

Conclusion: 提出的基准和模型为评估和推动LMMs在漫画这一丰富的叙事领域的进步提供了全面的基础。

Abstract: Manga, or Japanese comics, is a richly multimodal narrative form that blends
images and text in complex ways. Teaching large multimodal models (LMMs) to
understand such narratives at a human-like level could help manga creators
reflect on and refine their stories. To this end, we introduce two benchmarks
for multimodal manga understanding: MangaOCR, which targets in-page text
recognition, and MangaVQA, a novel benchmark designed to evaluate contextual
understanding through visual question answering. MangaVQA consists of 526
high-quality, manually constructed question-answer pairs, enabling reliable
evaluation across diverse narrative and visual scenarios. Building on these
benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the
open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive
experiments, including comparisons with proprietary models such as GPT-4o and
Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model
provide a comprehensive foundation for evaluating and advancing LMMs in the
richly narrative domain of manga.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [629] [Smart Waste Management System for Makkah City using Artificial Intelligence and Internet of Things](https://arxiv.org/abs/2505.19040)
*Rawabi S. Al Qurashi,Maram M. Almnjomi,Teef L. Alghamdi,Amjad H. Almalki,Shahad S. Alharthi,Shahad M. althobuti,Alanoud S. Alharthi,Maha A. Thafar*

Main category: cs.ET

TL;DR: 提出了一种基于物联网和人工智能的智能废物管理系统TUHR，用于麦加朝觐期间的废物管理，系统通过超声波传感器监测废物水平并检测有害物质，优化了资源利用，符合沙特2030愿景。


<details>
  <summary>Details</summary>
Motivation: 麦加朝觐活动吸引了大量游客，导致废物急剧增加，传统刚性收集时间表的方法难以应对，亟需一种有效的解决方案。

Method: 设计了一个名为TUHR的智能废物管理系统，该系统结合了物联网和人工智能技术，使用超声波传感器监测废物容器的填充水平，并通过气体探测器传感器检测有害物质。当容器满时，传感器与微控制器通信以通知相关当局。

Result: TUHR系统能够有效减少废物管理中的环境和健康风险，提高圣地清洁度，同时带来经济效益，如减少不必要的汽油消耗和优化资源分配。

Conclusion: TUHR系统展示了智能、可持续且注重健康的废物管理方法，与智慧城市原则和沙特阿拉伯2030年愿景计划相一致，为大型宗教聚会提供了创新的废物管理方案。

Abstract: Waste management is a critical global issue with significant environmental
and public health implications. It has become more destructive during
large-scale events such as the annual pilgrimage to Makkah, Saudi Arabia, one
of the world's largest religious gatherings. This event's popularity has
attracted millions worldwide, leading to significant and un-predictable
accumulation of waste. Such a tremendous number of visitors leads to in-creased
waste management issues at the Grand Mosque and other holy sites, highlighting
the need for an effective solution other than traditional methods based on
rigid collection schedules.
  To address this challenge, this research proposed an innovative solution that
is context-specific and tailored to the unique requirements of pilgrimage
season: a Smart Waste Management System, called TUHR, that utilizes the
Internet of Things and Artificial Intelligence. This system encompasses
ultrasonic sensors that monitor waste levels in each container at the
performance sites. Once the container reaches full capacity, the sensor
communicates with the microcontroller, which alerts the relevant authorities.
Moreover, our system can detect harmful substances such as gas from the gas
detector sensor. Such a proactive and dynamic approach promises to mitigate the
environmental and health risks associated with waste accumulation and enhance
the cleanliness of these sites. It also delivers economic benefits by reducing
unnecessary gasoline consumption and optimizing waste management resources.
Importantly, this research aligns with the principles of smart cities and
exemplifies the innovative, sustainable, and health-conscious approach that
Saudi Arabia is implementing as part of its Vision 2030 initiative.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [630] [Simulating Macroeconomic Expectations using LLM Agents](https://arxiv.org/abs/2505.17648)
*Jianhao Lin,Lexuan Sun,Yixin Yan*

Main category: econ.GN

TL;DR: The paper presents a novel framework using LLM Agents to simulate macroeconomic expectation formation, revealing key heterogeneity and drivers while showing the role of prior expectations.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Large Language Model-Empowered Agents (LLM Agents) in simulating macroeconomic expectation formation, providing new insights into AI behavioral science.

Method: Construct thousands of LLM Agents with modules for personal characteristics, prior expectations, and knowledge, then replicate a survey experiment involving households and experts on inflation and unemployment.

Result: LLM Agents generate more homogeneous expectations and thoughts than human participants but still capture key heterogeneity across agents and the underlying drivers of expectation formation. A module-ablation exercise highlights the importance of prior expectations.

Conclusion: This approach complements traditional survey methods and provides new insights into AI behavioral science in macroeconomic research.

Abstract: We introduce a novel framework for simulating macroeconomic expectation
formation using Large Language Model-Empowered Agents (LLM Agents). By
constructing thousands of LLM Agents equipped with modules for personal
characteristics, prior expectations, and knowledge, we replicate a survey
experiment involving households and experts on inflation and unemployment. Our
results show that although the expectations and thoughts generated by LLM
Agents are more homogeneous than those of human participants, they still
effectively capture key heterogeneity across agents and the underlying drivers
of expectation formation. Furthermore, a module-ablation exercise highlights
the critical role of prior expectations in simulating such heterogeneity. This
approach complements traditional survey methods and offers new insights into AI
behavioral science in macroeconomic research.

</details>


### [631] [An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy](https://arxiv.org/abs/2505.18687)
*Aran Nayebi*

Main category: econ.GN

TL;DR: The paper explores conditions and policies under which AI capital profits can finance UBI without new taxes or jobs, finding that AI productivity needs to be 5-6 times current automation levels. Policy levers like public revenue share and market structure significantly affect the required AI capability threshold.


<details>
  <summary>Details</summary>
Motivation: To determine sustainable ways of financing UBI using AI capital profits without additional taxes or job creation in an economy with automatable tasks.

Method: Derive a closed-form condition for financing UBI through AI capital profits within a Solow-Zeira economic model, analyzing the impact of varying economic parameters such as AI capability threshold, public revenue share, and market structure.

Result: AI systems must achieve 5-6 times existing automation productivity to finance an 11%-of-GDP UBI; increasing public revenue share to 33% halves the required AI capability threshold, but gains diminish beyond 50%; monopolistic markets reduce the threshold by increasing economic rents while competition raises it.

Conclusion: Maximizing public revenue share up to a point and managing market competition strategically can ensure AI capabilities translate into meaningful social benefits.

Abstract: We derive the first closed-form condition under which artificial intelligence
(AI) capital profits could sustainably finance a universal basic income (UBI)
without additional taxes or new job creation. In a Solow-Zeira economy
characterized by a continuum of automatable tasks, a constant net saving rate
$s$, and task-elasticity $\sigma < 1$, we analyze how the AI capability
threshold--defined as the productivity level of AI relative to pre-AI
automation--varies under different economic scenarios. At present economic
parameters, we find that AI systems must achieve only approximately 5-6 times
existing automation productivity to finance an 11\%-of-GDP UBI, in the worst
case situation where \emph{no} new jobs or tasks are created.
  Our analysis also reveals some specific policy levers: raising public revenue
share (e.g. profit taxation) of AI capital from the current 15\% to about 33\%
halves the required AI capability threshold to attain UBI to 3 times existing
automotion productivity, but gains diminish beyond 50\% public revenue share,
especially if regulatory costs increase. Market structure also strongly affects
outcomes: monopolistic or concentrated oligopolistic markets reduce the
threshold by increasing economic rents, whereas heightened competition
significantly raises it.
  Overall, these results suggest a couple policy recommendations: maximizing
public revenue share up to a point so that operating costs are minimized, and
strategically managing market competition can ensure AI's growing capabilities
translate into meaningful social benefits within realistic technological
progress scenarios.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [632] [A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random](https://arxiv.org/abs/2505.19093)
*Binh H. Ho,Long Nguyen Chi,TrungTin Nguyen,Binh T. Nguyen,Van Ha Hoang,Christopher Drovandi*

Main category: stat.ME

TL;DR: This paper presents a unified framework for model-based clustering that incorporates variable selection and handles missing data not at random, achieving consistency and enhancing clustering efficiency.


<details>
  <summary>Details</summary>
Motivation: Model-based clustering with variable selection is powerful but faces challenges in identifying relevant variables and dealing with non-random missing data.

Method: The method integrates a data-driven penalty matrix into penalized clustering for flexible variable selection and models the relationship between missingness and latent class membership.

Result: The framework achieves asymptotic and selection consistency under regularity conditions, improving clustering capability and efficiency as shown through simulations and real-world data analysis.

Conclusion: This unified approach advances methodologies for identifying informative variables in complex datasets with missing data patterns.

Abstract: Model-based clustering integrated with variable selection is a powerful tool
for uncovering latent structures within complex data. However, its
effectiveness is often hindered by challenges such as identifying relevant
variables that define heterogeneous subgroups and handling data that are
missing not at random, a prevalent issue in fields like transcriptomics. While
several notable methods have been proposed to address these problems, they
typically tackle each issue in isolation, thereby limiting their flexibility
and adaptability. This paper introduces a unified framework designed to address
these challenges simultaneously. Our approach incorporates a data-driven
penalty matrix into penalized clustering to enable more flexible variable
selection, along with a mechanism that explicitly models the relationship
between missingness and latent class membership. We demonstrate that, under
certain regularity conditions, the proposed framework achieves both asymptotic
consistency and selection consistency, even in the presence of missing data.
This unified strategy significantly enhances the capability and efficiency of
model-based clustering, advancing methodologies for identifying informative
variables that define homogeneous subgroups in the presence of complex missing
data patterns. The performance of the framework, including its computational
efficiency, is evaluated through simulations and demonstrated using both
synthetic and real-world transcriptomic datasets.

</details>


### [633] [Do Large Language Models (Really) Need Statistical Foundations?](https://arxiv.org/abs/2505.19145)
*Weijie Su*

Main category: stat.ME

TL;DR: 大型语言模型（LLMs）作为处理非结构化数据的新范式，其发展和应用可以从统计学的基础贡献中真正受益。LLMs本质上是统计模型，因其对数据的深刻依赖和随机生成过程，统计洞见对于处理变异性和不确定性至关重要。此外，由于LLMs的黑箱特性，封闭形式或纯粹机械分析通常难以实现，因此需要统计方法。本文概述了几个研究领域，包括对齐、水印、不确定性量化、评估和数据混合优化，这些领域需要统计方法并已开始做出有价值的贡献。结论指出，关于LLMs的统计研究可能形成一个多样化的“马赛克”，而不是来自单一的统一理论，并强调了统计学界及时参与LLMs研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨统计学在大型语言模型（LLMs）开发和应用中的基础性作用和重要性。

Method: 通过两个论点进行论证：1. LLMs本质上是统计模型，统计洞见对其处理变异性和不确定性至关重要；2. 由于LLMs的黑箱特性，封闭形式或纯粹机械分析通常难以实现，因此需要统计方法。同时，文章概述了多个需要统计方法的研究领域，如对齐、水印、不确定性量化等。

Result: 证明了统计学在LLMs中的重要性和必要性，并指出了统计学界参与LLMs研究的紧迫性和潜力。

Conclusion: 关于LLMs的统计研究可能形成一个多样化的“马赛克”，而不是来自单一的统一理论，强调了统计学界及时参与LLMs研究的重要性。

Abstract: Large language models (LLMs) represent a new paradigm for processing
unstructured data, with applications across an unprecedented range of domains.
In this paper, we address, through two arguments, whether the development and
application of LLMs would genuinely benefit from foundational contributions
from the statistics discipline. First, we argue affirmatively, beginning with
the observation that LLMs are inherently statistical models due to their
profound data dependency and stochastic generation processes, where statistical
insights are naturally essential for handling variability and uncertainty.
Second, we argue that the persistent black-box nature of LLMs -- stemming from
their immense scale, architectural complexity, and development practices often
prioritizing empirical performance over theoretical interpretability -- renders
closed-form or purely mechanistic analyses generally intractable, thereby
necessitating statistical approaches due to their flexibility and often
demonstrated effectiveness. To substantiate these arguments, the paper outlines
several research areas -- including alignment, watermarking, uncertainty
quantification, evaluation, and data mixture optimization -- where statistical
methodologies are critically needed and are already beginning to make valuable
contributions. We conclude with a discussion suggesting that statistical
research concerning LLMs will likely form a diverse ``mosaic'' of specialized
topics rather than deriving from a single unifying theory, and highlighting the
importance of timely engagement by our statistics community in LLM research.

</details>


### [634] [Cellwise and Casewise Robust Covariance in High Dimensions](https://arxiv.org/abs/2505.19925)
*Fabio Centofanti,Mia Hubert,Peter J. Rousseeuw*

Main category: stat.ME

TL;DR: The paper proposes cellRCov, a robust covariance estimator that handles casewise outliers, cellwise outliers and missing data by decomposing the covariance on principal and orthogonal subspaces. It shows superior performance in contaminated and missing data scenarios through simulations and real-world anomaly detection application.


<details>
  <summary>Details</summary>
Motivation: Current robust covariance estimators capable of handling both casewise and cellwise outliers have computational limitations, being feasible up to 20 dimensions only. There is a need for a method that can handle high-dimensional data with these types of outliers as well as missing data.

Method: The cellRCov method estimates covariance by decomposing it on principal and orthogonal subspaces using robust PCA techniques. It incorporates ridge-type regularization to stabilize the estimated covariance matrix.

Result: Theoretical properties of cellRCov are established including its influence functions, consistency and asymptotic normality. Simulation studies show its superior performance in contaminated and missing data situations. Its practical utility is demonstrated in an anomaly detection application. Additionally, the cellRCCA method for robust canonical correlation analysis is constructed and illustrated.

Conclusion: cellRCov is a promising robust covariance estimator that can effectively deal with casewise outliers, cellwise outliers, and missing data simultaneously, even in high-dimensional settings.

Abstract: The sample covariance matrix is a cornerstone of multivariate statistics, but
it is highly sensitive to outliers. These can be casewise outliers, such as
cases belonging to a different population, or cellwise outliers, which are
deviating cells (entries) of the data matrix. Recently some robust covariance
estimators have been developed that can handle both types of outliers, but
their computation is only feasible up to at most 20 dimensions. To remedy this
we propose the cellRCov method, a robust covariance estimator that
simultaneously handles casewise outliers, cellwise outliers, and missing data.
It relies on a decomposition of the covariance on principal and orthogonal
subspaces, leveraging recent work on robust PCA. It also employs a ridge-type
regularization to stabilize the estimated covariance matrix. We establish some
theoretical properties of cellRCov, including its casewise and cellwise
influence functions as well as consistency and asymptotic normality. A
simulation study demonstrates the superior performance of cellRCov in
contaminated and missing data scenarios. Furthermore, its practical utility is
illustrated in a real-world application to anomaly detection. We also construct
and illustrate the cellRCCA method for robust and regularized canonical
correlation analysis.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [635] [Private Geometric Median in Nearly-Linear Time](https://arxiv.org/abs/2505.20189)
*Syamantak Kumar,Daogao Liu,Kevin Tian,Chutong Yang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Estimating the geometric median of a dataset is a robust counterpart to mean
estimation, and is a fundamental problem in computational geometry. Recently,
[HSU24] gave an $(\varepsilon, \delta)$-differentially private algorithm
obtaining an $\alpha$-multiplicative approximation to the geometric median
objective, $\frac 1 n \sum_{i \in [n]} \|\cdot - \mathbf{x}_i\|$, given a
dataset $\mathcal{D} := \{\mathbf{x}_i\}_{i \in [n]} \subset \mathbb{R}^d$.
Their algorithm requires $n \gtrsim \sqrt d \cdot \frac 1 {\alpha\varepsilon}$
samples, which they prove is information-theoretically optimal. This result is
surprising because its error scales with the \emph{effective radius} of
$\mathcal{D}$ (i.e., of a ball capturing most points), rather than the
worst-case radius. We give an improved algorithm that obtains the same
approximation quality, also using $n \gtrsim \sqrt d \cdot \frac 1
{\alpha\epsilon}$ samples, but in time $\widetilde{O}(nd + \frac d
{\alpha^2})$. Our runtime is nearly-linear, plus the cost of the cheapest
non-private first-order method due to [CLM+16]. To achieve our results, we use
subsampling and geometric aggregation tools inspired by FriendlyCore [TCK+22]
to speed up the "warm start" component of the [HSU24] algorithm, combined with
a careful custom analysis of DP-SGD's sensitivity for the geometric median
objective.

</details>


### [636] [Learning-Augmented Online Bipartite Fractional Matching](https://arxiv.org/abs/2505.19252)
*Davin Choo,Billy Jin,Yongho Shin*

Main category: cs.DS

TL;DR: The paper explores online bipartite fractional matching with learning-augmented algorithms, providing algorithms that outperform random choice strategies and extending to the AdWords problem. It also establishes a hardness bound for robustness-consistency tradeoff.


<details>
  <summary>Details</summary>
Motivation: Motivated by recent progress in learning-augmented algorithms, the study aims to improve online bipartite fractional matching when given advice in the form of suggested matchings in each iteration.

Method: Developed algorithms for both vertex-weighted and unweighted variants of online bipartite fractional matching that provably dominate the naive 'coin flip' strategy. The algorithm for the vertex-weighted setting is extended to the AdWords problem under the small bids assumption.

Result: The developed algorithms significantly improve upon previous work in the field, particularly in the AdWords problem. A hardness bound on the robustness-consistency tradeoff attainable by any algorithm has been established.

Conclusion: The study provides advancements in online bipartite fractional matching algorithms, extends these improvements to practical applications like AdWords, and empirically validates the results through experiments.

Abstract: Online bipartite matching is a fundamental problem in online optimization,
extensively studied both in its integral and fractional forms due to its
theoretical significance and practical applications, such as online advertising
and resource allocation. Motivated by recent progress in learning-augmented
algorithms, we study online bipartite fractional matching when the algorithm is
given advice in the form of a suggested matching in each iteration. We develop
algorithms for both the vertex-weighted and unweighted variants that provably
dominate the naive "coin flip" strategy of randomly choosing between the
advice-following and advice-free algorithms. Moreover, our algorithm for the
vertex-weighted setting extends to the AdWords problem under the small bids
assumption, yielding a significant improvement over the seminal work of
Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our
positive results, we establish a hardness bound on the robustness-consistency
tradeoff that is attainable by any algorithm. We empirically validate our
algorithms through experiments on synthetic and real-world data.

</details>


### [637] [Demand Selection for VRP with Emission Quota](https://arxiv.org/abs/2505.19315)
*Farid Najar,Dominique Barth,Yann Strozecki*

Main category: cs.DS

TL;DR: The study introduces QVRP, a demand selection problem for VRP with an emission quota, focusing on MFVA. It proposes methods from ML and OR for selecting omitted packages and finds that OR-based methods outperform ML-based approaches in this static setting.


<details>
  <summary>Details</summary>
Motivation: To address the Vehicle Routing Problem with an emission quota (QVRP) by minimizing omitted deliveries while respecting pollution constraints.

Method: Proposing various methods from both machine learning and operations research to select packages to omit in the Maximum Feasible Vehicle Assignment (MFVA) aspect of QVRP, while using classical OR methods for constructing VRP routing.

Result: Classical OR-based methods consistently outperformed ML-based approaches in selecting packages to omit in the static problem setting.

Conclusion: In the context of QVRP's demand selection problem, traditional OR methods proved more effective than ML-based methods.

Abstract: Combinatorial optimization (CO) problems are traditionally addressed using
Operations Research (OR) methods, including metaheuristics. In this study, we
introduce a demand selection problem for the Vehicle Routing Problem (VRP) with
an emission quota, referred to as QVRP. The objective is to minimize the number
of omitted deliveries while respecting the pollution quota. We focus on the
demand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while
the construction of a routing for the VRP instance is solved using classical OR
methods. We propose several methods for selecting the packages to omit, both
from machine learning (ML) and OR. Our results show that, in this static
problem setting, classical OR-based methods consistently outperform ML-based
approaches.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [638] [How We Won the ISLES'24 Challenge by Preprocessing](https://arxiv.org/abs/2505.18424)
*Tianyi Ren,Juampablo E. Heras Rivera,Hitender Oswal,Yutong Pan,William Henry,Jacob Ruzevick,Mehmet Kurt*

Main category: eess.IV

TL;DR: Stroke is a major cause of death globally. Accurate stroke lesion boundary identification is essential for diagnosis and treatment. Supervised deep learning methods are leading solutions for stroke lesion segmentation but require large annotated datasets. ISLES'24 challenge provides longitudinal stroke imaging data with annotations derived from follow-up MRI, while models are evaluated using only CT inputs. Our winning solution involves a preprocessing pipeline including deep-learning-based skull stripping and custom intensity windowing, combined with a standard large residual nnU-Net architecture, achieving a mean test Dice of 28.5 with a standard deviation of 21.27.


<details>
  <summary>Details</summary>
Motivation: Stroke is among the top three causes of death worldwide, making accurate identification of stroke lesion boundaries critical for diagnosis and treatment. Supervised deep learning methods have emerged as the leading solution for stroke lesion segmentation but require large, diverse, and annotated datasets.

Method: Our winning solution includes a carefully designed preprocessing pipeline with deep-learning-based skull stripping and custom intensity windowing, combined with a standard large residual nnU-Net architecture for segmentation.

Result: This approach achieves a mean test Dice of 28.5 with a standard deviation of 21.27.

Conclusion: A carefully designed preprocessing pipeline is beneficial for accurate segmentation in the context of predicting lesion progression that may not be visible in CT scans.

Abstract: Stroke is among the top three causes of death worldwide, and accurate
identification of stroke lesion boundaries is critical for diagnosis and
treatment. Supervised deep learning methods have emerged as the leading
solution for stroke lesion segmentation but require large, diverse, and
annotated datasets. The ISLES'24 challenge addresses this need by providing
longitudinal stroke imaging data, including CT scans taken on arrival to the
hospital and follow-up MRI taken 2-9 days from initial arrival, with
annotations derived from follow-up MRI. Importantly, models submitted to the
ISLES'24 challenge are evaluated using only CT inputs, requiring prediction of
lesion progression that may not be visible in CT scans for segmentation. Our
winning solution shows that a carefully designed preprocessing pipeline
including deep-learning-based skull stripping and custom intensity windowing is
beneficial for accurate segmentation. Combined with a standard large residual
nnU-Net architecture for segmentation, this approach achieves a mean test Dice
of 28.5 with a standard deviation of 21.27.

</details>


### [639] [Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking](https://arxiv.org/abs/2505.18538)
*Xin Wei,Huakun Liu,Yutaro Hirao,Monica Perusquia-Hernandez,Katsutoshi Masai,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: eess.IV

TL;DR: The study explores passive methods for estimating refractive power using EOG and eye tracking, with multimodal models showing promise but limited generalizability.


<details>
  <summary>Details</summary>
Motivation: To develop continuous, non-invasive screening methods for refractive errors using eye movement data.

Method: Training LSTM models on a public dataset with varying diopter conditions using unimodal (EOG or eye tracking) and multimodal configurations, assessing performance in subject-dependent and independent settings.

Result: Multimodal model outperforms unimodal models in subject-dependent setting (96.207% accuracy), but only marginally above chance in subject-independent setting (8.882% accuracy).

Conclusion: Eye movement data shows potential for refractive error estimation, but generalization across individuals remains a challenge.

Abstract: Refractive errors are among the most common visual impairments globally, yet
their diagnosis often relies on active user participation and clinical
oversight. This study explores a passive method for estimating refractive power
using two eye movement recording techniques: electrooculography (EOG) and
video-based eye tracking. Using a publicly available dataset recorded under
varying diopter conditions, we trained Long Short-Term Memory (LSTM) models to
classify refractive power from unimodal (EOG or eye tracking) and multimodal
configuration. We assess performance in both subject-dependent and
subject-independent settings to evaluate model personalization and
generalizability across individuals. Results show that the multimodal model
consistently outperforms unimodal models, achieving the highest average
accuracy in both settings: 96.207\% in the subject-dependent scenario and
8.882\% in the subject-independent scenario. However, generalization remains
limited, with classification accuracy only marginally above chance in the
subject-independent evaluations. Statistical comparisons in the
subject-dependent setting confirmed that the multimodal model significantly
outperformed the EOG and eye-tracking models. However, no statistically
significant differences were found in the subject-independent setting. Our
findings demonstrate both the potential and current limitations of eye movement
data-based refractive error estimation, contributing to the development of
continuous, non-invasive screening methods using EOG signals and eye-tracking
data.

</details>


### [640] [ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery](https://arxiv.org/abs/2505.18546)
*Dristi Datta,Manoranjan Paul,Manzur Murshed,Shyh Wei Teng,Leigh M. Schmidtke*

Main category: eess.IV

TL;DR: The study introduces ReflectGAN, a GAN-based model that enhances the accuracy of soil organic carbon (SOC) estimation in vegetated areas by reconstructing bare soil reflectance from satellite imagery. Models using ReflectGAN's output significantly outperformed traditional methods in SOC prediction across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Vegetation cover hinders accurate SOC estimation from satellite images due to spectral contamination, making it difficult to discern soil reflectance clearly and reliably.

Method: Developed ReflectGAN, a paired GAN framework that learns the spectral transformation between vegetated and bare soil reflectance to generate accurate bare soil reflectance data from vegetated soil satellite observations.

Result: Models trained on ReflectGAN-reconstructed reflectance performed better than those using existing vegetation correction methods. Specifically, an RF model achieved an R² of 0.54, RMSE of 3.95, and RPD of 2.07 with ReflectGAN data, showing a 35% increase in R², 43% reduction in RMSE, and 43% improvement in RPD compared to the best existing method.

Conclusion: ReflectGAN demonstrates significant potential for improving SOC estimation accuracy in vegetated landscapes, thus enhancing reliable soil monitoring.

Abstract: Soil organic carbon (SOC) is a critical indicator of soil health, but its
accurate estimation from satellite imagery is hindered in vegetated regions due
to spectral contamination from plant cover, which obscures soil reflectance and
reduces model reliability. This study proposes the Reflectance Transformation
Generative Adversarial Network (ReflectGAN), a novel paired GAN-based framework
designed to reconstruct accurate bare soil reflectance from vegetated soil
satellite observations. By learning the spectral transformation between
vegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC
estimation under mixed land cover conditions. Using the LUCAS 2018 dataset and
corresponding Landsat 8 imagery, we trained multiple learning-based models on
both original and ReflectGAN-reconstructed reflectance inputs. Models trained
on ReflectGAN outputs consistently outperformed those using existing vegetation
correction methods. For example, the best-performing model (RF) achieved an
$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the
ReflectGAN-generated signals, representing a 35\% increase in $R^2$, a 43\%
reduction in RMSE, and a 43\% improvement in RPD compared to the best existing
method (PMM-SU). The performance of the models with ReflectGAN is also better
compared to their counterparts when applied to another dataset, i.e.,
Sentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to
improve SOC estimation accuracy in vegetated landscapes, supporting more
reliable soil monitoring.

</details>


### [641] [Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy](https://arxiv.org/abs/2505.18664)
*Evgeny Ugolkov,Xupeng He,Hyung Kwak,Hussein Hoteit*

Main category: eess.IV

TL;DR: 提出了一种基于3D Octree的卷积Wasserstein生成对抗网络算法，能够显著提高岩石微CT图像分割质量，解决了矿物重叠导致的分割不准确问题，并实现了16倍分辨率提升。


<details>
  <summary>Details</summary>
Motivation: 当前3D微CT图像分割存在因矿物X射线衰减重叠导致的分割不准确问题，同时高分辨率处理面临内存消耗大的挑战。需要一种高效算法来提升图像分辨率并改善分割精度。

Method: 采用3D Octree结构的卷积Wasserstein生成对抗网络（GAN），结合梯度惩罚机制，在生成器中使用Octree结构以降低内存消耗，从而实现高效的3D超分辨率增强。训练数据包括低分辨率3D微CT图像和未配对的高分辨率2D激光扫描显微镜图像。

Result: 算法成功将分辨率从7微米/体素提升到0.44微米/体素，同时提高了矿物分割的准确性。在Berea砂岩上的验证表明，该框架显著改进了孔隙特征描述和矿物区分能力。

Conclusion: 该方法有效解决了体积深度学习中的内存瓶颈问题，为现代地质成像提供了一个强大的解决方案，特别是在提高岩石微CT图像质量和矿物分辨方面表现突出。

Abstract: We present a memory-efficient algorithm for significantly enhancing the
quality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks
using a generative model. The proposed model achieves a 16x increase in
resolution and corrects inaccuracies in segmentation caused by the overlapping
X-ray attenuation in micro-CT measurements across different minerals. The
generative model employed is a 3D Octree-based convolutional Wasserstein
generative adversarial network with gradient penalty. To address the challenge
of high memory consumption inherent in standard 3D convolutional layers, we
implemented an Octree structure within the 3D progressive growing generator
model. This enabled the use of memory-efficient 3D Octree-based convolutional
layers. The approach is pivotal in overcoming the long-standing memory
bottleneck in volumetric deep learning, making it possible to reach 16x
super-resolution in 3D, a scale that is challenging to attain due to cubic
memory scaling. For training, we utilized segmented 3D low-resolution micro-CT
images along with unpaired segmented complementary 2D high-resolution laser
scanning microscope images. Post-training, resolution improved from 7 to 0.44
micro-m/voxel with accurate segmentation of constituent minerals. Validated on
Berea sandstone, this framework demonstrates substantial improvements in pore
characterization and mineral differentiation, offering a robust solution to one
of the primary computational limitations in modern geoscientific imaging.

</details>


### [642] [A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements](https://arxiv.org/abs/2505.18784)
*Jihong Wang,Chung-Hao Lee,William Richardson,Yue Yu*

Main category: eess.IV

TL;DR: A novel approach for processing DIC measurements of multiple biaxial stretching protocols is presented, which uses an optimization-based method with moving least-squares algorithm and positive strain constraints to obtain physically consistent displacement and strain fields. Then, a data-driven workflow is applied for heterogeneous material modeling by estimating a nonlocal constitutive law and material microstructure. The approach was demonstrated on a porcine tricuspid valve anterior leaflet, showing improved accuracy in modeling biological materials.


<details>
  <summary>Details</summary>
Motivation: To develop a more accurate method for processing DIC measurements of multiple biaxial stretching protocols and improve the modeling of biological materials.

Method: The method involves an optimization-based approach that calculates smoothed nodal displacements using a moving least-squares algorithm subject to positive strain constraints, resulting in physically consistent displacement and strain fields. A data-driven workflow follows this step to estimate a nonlocal constitutive law and material microstructure for heterogeneous material modeling.

Result: The results show that the proposed DIC data processing approach can significantly enhance the accuracy of modeling biological materials, as demonstrated by its application in learning a material model and fiber orientation field from DIC measurements of a porcine tricuspid valve anterior leaflet.

Conclusion: This novel approach for processing DIC measurements provides significant improvements in the accuracy of modeling biological materials.

Abstract: In this work, we present a novel approach to process the DIC measurements of
multiple biaxial stretching protocols. In particular, we develop a
optimization-based approach, which calculates the smoothed nodal displacements
using a moving least-squares algorithm subject to positive strain constraints.
As such, physically consistent displacement and strain fields are obtained.
Then, we further deploy a data-driven workflow to heterogeneous material
modeling from these physically consistent DIC measurements, by estimating a
nonlocal constitutive law together with the material microstructure. To
demonstrate the applicability of our approach, we apply it in learning a
material model and fiber orientation field from DIC measurements of a porcine
tricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC
data processing approach can significantly improve the accuracy of modeling
biological materials.

</details>


### [643] [Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models](https://arxiv.org/abs/2505.19779)
*Mobina Mansoori,Sajjad Shahabodini,Farnoush Bayatmakou,Jamshid Abouei,Konstantinos N. Plataniotis,Arash Mohammadi*

Main category: eess.IV

TL;DR: This study explores the application of recent state-of-the-art foundation models (DINOv2, MAE, VMamba, CoCa, SAM2, and AIMv2) for medical image classification on datasets including CBIS-DDSM, ISIC2019, APTOS2019, and CHEXPERT. The results indicate that AIMv2, DINOv2, and SAM2 significantly enhance classification outcomes.


<details>
  <summary>Details</summary>
Motivation: Foundation models have shown consistently improved results with the introduction of new methods, and it is crucial to analyze how these trends impact the medical field and determine whether these advancements can drive meaningful change.

Method: The authors fine-tuned recent state-of-the-art foundation models (DINOv2, MAE, VMamba, CoCa, SAM2, and AIMv2) on medical image datasets (CBIS-DDSM for mammography, ISIC2019 for skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest radiographs) and evaluated their configurations.

Result: The advanced models significantly enhanced classification outcomes, demonstrating robust performance despite limited labeled data. AIMv2, DINOv2, and SAM2 outperformed others.

Conclusion: Progress in natural domain training has positively impacted the medical domain and improved classification outcomes.

Abstract: Using massive datasets, foundation models are large-scale, pre-trained models
that perform a wide range of tasks. These models have shown consistently
improved results with the introduction of new methods. It is crucial to analyze
how these trends impact the medical field and determine whether these
advancements can drive meaningful change. This study investigates the
application of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,
CoCa, SAM2, and AIMv2, for medical image classification. We explore their
effectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for
skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest
radiographs. By fine-tuning these models and evaluating their configurations,
we aim to understand the potential of these advancements in medical image
classification. The results indicate that these advanced models significantly
enhance classification outcomes, demonstrating robust performance despite
limited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models
outperformed others, demonstrating that progress in natural domain training has
positively impacted the medical domain and improved classification outcomes.
Our code is publicly available at:
https://github.com/sajjad-sh33/Medical-Transfer-Learning.

</details>


### [644] [Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases](https://arxiv.org/abs/2505.20149)
*Cheng-Yu Tai,Ching-Wen Chen,Chi-Chin Wu,Bo-Chen Chiu,Cheng-Hung,Lin,Cheng-Kai Lu,Jia-Kang Wang,Tzu-Lun Huang*

Main category: eess.IV

TL;DR: This paper proposes a few-shot learning approach combined with U-GAT-IT, data balance techniques, and CBAM-InceptionV3 model to significantly improve the accuracy of OCT image classification for both major and rare classes from 95.26% to 97.85%.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of classifying OCT diagnosis images with high accuracy across both major and rare classes using few-shot learning.

Method: The method involves GAN-based augmentation as a baseline, integrating U-GAT-IT for enhancing generative aspects, applying data balance techniques to reduce accuracy skew among categories, and utilizing CBAM attention mechanism along with fine-tuned InceptionV3 for model building.

Result: Achieved an overall accuracy of 97.85% in OCT image classification, showing significant improvement over the original baseline accuracy of 95.26%.

Conclusion: The proposed few-shot learning strategy effectively improves the accuracy of OCT image classification, particularly benefiting the recognition of rare classes through enhanced generative capabilities and data balancing.

Abstract: This paper focuses on using few-shot learning to improve the accuracy of
classifying OCT diagnosis images with major and rare classes. We used the
GAN-based augmentation strategy as a baseline and introduced several novel
methods to further enhance our model. The proposed strategy contains U-GAT-IT
for improving the generative part and uses the data balance technique to narrow
down the skew of accuracy between all categories. The best model obtained was
built with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an
overall accuracy of 97.85%, representing a significant improvement over the
original baseline.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [645] [Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.18750)
*Jiarong Fan,Chenghao Huang,Hao Wang*

Main category: eess.SY

TL;DR: In this paper, researchers developed a Multi-Agent Reinforcement Learning (MARL) approach combined with an LSTM network to manage Electric Vehicle (EV) charging stations more effectively. The method considers uncertainties like system faults and varying charging behaviors, aiming to minimize EV charging costs and maximize service satisfaction.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the lack of robustness in current EV charging management systems against uncertainties such as varying charging behaviors and possible faults in chargers while ensuring low energy cost and grid stability.

Method: The paper proposes a novel Multi-Agent Reinforcement Learning (MARL) approach where each charger is treated as an agent. These agents are coordinated within the EV charging station considering solar photovoltaics under realistic scenarios where system faults may occur. An LSTM network is used to extract temporal features from time-series data and a dense reward mechanism is designed to train the agents for improving EV charging experience.

Result: The validation on a real-world dataset shows that the proposed MARL approach is robust against system uncertainties and faults, effective in minimizing EV charging costs, and capable of maximizing charging service satisfaction.

Conclusion: The conclusion is that the proposed MARL approach with LSTM and a dense reward mechanism can successfully handle uncertainties in EV charging management, reduce costs, and enhance user satisfaction.

Abstract: In the pursuit of energy net zero within smart cities, transportation
electrification plays a pivotal role. The adoption of Electric Vehicles (EVs)
keeps increasing, making energy management of EV charging stations critically
important. While previous studies have managed to reduce energy cost of EV
charging while maintaining grid stability, they often overlook the robustness
of EV charging management against uncertainties of various forms, such as
varying charging behaviors and possible faults in faults in some chargers. To
address the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is
proposed treating each charger to be an agent and coordinate all the agents in
the EV charging station with solar photovoltaics in a more realistic scenario,
where system faults may occur. A Long Short-Term Memory (LSTM) network is
incorporated in the MARL algorithm to extract temporal features from
time-series. Additionally, a dense reward mechanism is designed for training
the agents in the MARL algorithm to improve EV charging experience. Through
validation on a real-world dataset, we show that our approach is robust against
system uncertainties and faults and also effective in minimizing EV charging
costs and maximizing charging service satisfaction.

</details>


### [646] [Robust Stability Analysis of Positive Lure System with Neural Network Feedback](https://arxiv.org/abs/2505.18912)
*Hamidreza Montazeri Hedesh,Moh. Kamalul Wafi,Bahram Shafai,Milad Siami*

Main category: eess.SY

TL;DR: The paper explores the robustness of Lur'e systems under positivity constraints, deriving a stability radius formula and extending analysis to neural network feedback loops, with proposed methods supported by examples.


<details>
  <summary>Details</summary>
Motivation: To investigate the robustness of Lur'e systems when both linear parametric uncertainty and unknown nonlinear sector bounds are present, utilizing results from the positive Aizerman conjecture and properties of Metzler matrices.

Method: Using tools from positive linear systems, the positivity characteristic of the system is leveraged to derive an explicit formula for the stability radius. The analysis is extended to systems with neural network feedback loops, and a refinement method for sector bounds of feedforward neural networks is proposed.

Result: An explicit formula for the stability radius of Lur'e systems is derived. The approach is scalable and efficient for robustness analysis of both Lur'e and NN-controlled systems, as demonstrated through illustrative examples.

Conclusion: The study successfully introduces a new method for analyzing the robustness of Lur'e systems under positivity constraints, including those with neural network feedback, providing a significant advancement in handling uncertain nonlinear systems.

Abstract: This paper investigates the robustness of the Lur'e problem under positivity
constraints, drawing on results from the positive Aizerman conjecture and the
robustness properties of Metzler matrices. Specifically, we consider a control
system of Lur'e type in which not only the linear part includes parametric
uncertainty but also the nonlinear sector bound is unknown. We investigate
tools from positive linear systems to effectively solve the problems in
complicated and uncertain nonlinear systems. By leveraging the positivity
characteristic of the system, we derive an explicit formula for the stability
radius of Lur'e systems. Furthermore, we extend our analysis to systems with
neural network (NN) feedback loops. Building on this approach, we also propose
a refinement method for sector bounds of feedforward neural networks (FFNNs).
This study introduces a scalable and efficient approach for robustness analysis
of both Lur'e and NN-controlled systems. Finally, the proposed results are
supported by illustrative examples.

</details>


### [647] [VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning](https://arxiv.org/abs/2505.19486)
*Maonan Wang,Yirong Chen,Aoyu Pang,Yuxin Cai,Chung Shue Chen,Yuheng Kan,Man-On Pun*

Main category: eess.SY

TL;DR: Traffic signal control (TSC) is crucial for urban mobility. Current methods have difficulty generalizing to complex scenarios. This paper introduces VLMLight, a TSC framework integrating vision-language meta-control with dual-branch reasoning. It features the first image-based traffic simulator and uses a large language model as a safety-prioritized meta-controller. Experiments show significant improvements in handling emergency vehicles while maintaining performance in standard conditions.


<details>
  <summary>Details</summary>
Motivation: Existing traffic signal control methods struggle to generalize to complex, dynamic, and safety-critical scenarios.

Method: VLMLight integrates vision-language meta-control with dual-branch reasoning. It uses an image-based traffic simulator for multi-view visual perception at intersections and employs a large language model as a safety-prioritized meta-controller that selects between a fast RL policy for routine traffic and a structured reasoning branch for critical cases.

Result: Experiments demonstrate that VLMLight reduces waiting times for emergency vehicles by up to 65% compared to RL-only systems, with less than 1% degradation in standard conditions.

Conclusion: VLMLight offers a scalable, interpretable, and safety-aware solution for next-generation traffic signal control.

Abstract: Traffic signal control (TSC) is a core challenge in urban mobility, where
real-time decisions must balance efficiency and safety. Existing methods -
ranging from rule-based heuristics to reinforcement learning (RL) - often
struggle to generalize to complex, dynamic, and safety-critical scenarios. We
introduce VLMLight, a novel TSC framework that integrates vision-language
meta-control with dual-branch reasoning. At the core of VLMLight is the first
image-based traffic simulator that enables multi-view visual perception at
intersections, allowing policies to reason over rich cues such as vehicle type,
motion, and spatial density. A large language model (LLM) serves as a
safety-prioritized meta-controller, selecting between a fast RL policy for
routine traffic and a structured reasoning branch for critical cases. In the
latter, multiple LLM agents collaborate to assess traffic phases, prioritize
emergency vehicles, and verify rule compliance. Experiments show that VLMLight
reduces waiting times for emergency vehicles by up to 65% over RL-only systems,
while preserving real-time performance in standard conditions with less than 1%
degradation. VLMLight offers a scalable, interpretable, and safety-aware
solution for next-generation traffic signal control.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [648] [NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection](https://arxiv.org/abs/2505.18174)
*Zhixin li,Peihong Zhang,Rui Sang,Yuxuan Liu,Shengchen Li*

Main category: eess.SP

TL;DR: The paper presents Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE) for ECG and PCG signals, reducing noise amplification through distribution matching via optimal transport theory. Integrated with a Temporal-Spatial Feature Extraction network, NMCSE improves CVD detection accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional deconvolution methods that amplify noise when estimating the coupling signal between ECG and PCG, which restricts their clinical utility.

Method: Propose NMCSE, reformulating the estimation problem as distribution matching using optimal transport theory, jointly optimizing amplitude and temporal alignment to mitigate noise amplification without extra preprocessing. Combine it with a Temporal-Spatial Feature Extraction network for multi-modal CVD detection.

Result: Reduces estimation errors by about 30% in Mean Squared Error while maintaining higher Pearson Correlation Coefficients across all tested signal-to-noise ratios. Achieves 97.38% accuracy and 0.98 AUC in CVD detection on the PhysioNet 2016 dataset with hospital noise.

Conclusion: NMCSE outperforms state-of-the-art methods, demonstrating robust performance for real-world clinical applications in CVD detection.

Abstract: Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a
latent coupling signal representing the electrical-to-mechanical cardiac
transformation. While valuable for cardiovascular disease (CVD) detection, this
coupling signal is traditionally estimated using deconvolution methods that
amplify noise, limiting clinical utility. In this paper, we propose
Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates
the problem as distribution matching via optimal transport theory. By jointly
optimizing amplitude and temporal alignment, NMCSE mitigates noise
amplification without additional preprocessing. Integrated with our
Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal
CVD detection. Experiments on the PhysioNet 2016 dataset with realistic
hospital noise demonstrate that NMCSE reduces estimation errors by
approximately 30% in Mean Squared Error while maintaining higher Pearson
Correlation Coefficients across all tested signal-to-noise ratios. Our approach
achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming
state-of-the-art methods and demonstrating robust performance for real-world
clinical applications.

</details>


### [649] [Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework](https://arxiv.org/abs/2505.18175)
*Natia Kukhilava,Tatia Tsmindashvili,Rapael Kalandadze,Anchit Gupta,Sofio Katamadze,François Brémond,Laura M. Ferrari,Philipp Müller,Benedikt Emanuel Wirth*

Main category: eess.SP

TL;DR: Electroencephalography-based Emotion Recognition (EEG-ER) is an emerging field that lacks a unified evaluation protocol. The authors analyze 216 papers and propose EEGain, an open source software framework to standardize evaluation methods in EEG-ER.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is the lack of a unified evaluation protocol in the field of EEG-ER which hinders fair comparison and tracking of progress.

Method: The method involves analyzing 216 papers on EEG-ER to identify inconsistencies in evaluation protocols. Based on this analysis, the authors propose EEGain, a novel open source software framework that includes standardized methods for data pre-processing, splitting, evaluation metrics, and loading six relevant datasets.

Result: EEGain has been assessed and validated using six datasets on four common publicly available methods, making research on EEG-ER more reproducible and comparable.

Conclusion: EEGain represents a significant step towards standardizing evaluation methods in EEG-ER, thereby accelerating the overall progress of the field.

Abstract: Electroencephalography-based Emotion Recognition (EEG-ER) has become a
growing research area in recent years. Analyzing 216 papers published between
2018 and 2023, we uncover that the field lacks a unified evaluation protocol,
which is essential to fairly define the state of the art, compare new
approaches and to track the field's progress. We report the main
inconsistencies between the used evaluation protocols, which are related to
ground truth definition, evaluation metric selection, data splitting types
(e.g., subject-dependent or subject-independent) and the use of different
datasets. Capitalizing on this state-of-the-art research, we propose a unified
evaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which
enables an easy and efficient evaluation of new methods and datasets. EEGain is
a novel open source software framework, offering the capability to compare -
and thus define - state-of-the-art results. EEGain includes standardized
methods for data pre-processing, data splitting, evaluation metrics, and the
ability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,
MAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In
addition, we have assessed and validated EEGain using these six datasets on the
four most common publicly available methods (EEGNet, DeepConvNet,
ShallowConvNet, TSception). This is a significant step to make research on
EEG-ER more reproducible and comparable, thereby accelerating the overall
progress of the field.

</details>


### [650] [Improving Generative Inverse Design of Rectangular Patch Antennas with Test Time Optimization](https://arxiv.org/abs/2505.18188)
*Beck LaBash,Shahriar Khushrushahi,Fabian Ruehle*

Main category: eess.SP

TL;DR: 提出了一种两阶段深度学习框架，用于矩形贴片天线的逆向设计。该方法利用生成模型学习天线频率响应曲线的潜在表示，并对这些响应进行条件生成模型以生成可行的天线几何形状。测试时使用搜索和优化技术可以提高生成设计的准确性，并考虑诸如制造性等辅助目标。该方法可以自然地推广到不同的设计标准，并可以轻松适应更复杂的几何设计空间。


<details>
  <summary>Details</summary>
Motivation: 逆向设计矩形贴片天线需要一种能够学习天线频率响应曲线并生成可行天线几何形状的方法，同时还需要考虑制造性等辅助目标。

Method: 提出了一种两阶段深度学习框架：第一阶段利用生成模型学习天线频率响应曲线的潜在表示；第二阶段基于这些响应对后续生成模型进行条件化以生成可行的天线几何形状。此外，在测试时结合搜索和优化技术以提高生成设计的准确性。

Result: 该方法不仅提高了生成设计的准确性，还能够考虑制造性等辅助目标，并且可以自然地推广到不同的设计标准和更复杂的几何设计空间。

Conclusion: 所提出的两阶段深度学习框架在矩形贴片天线的逆向设计中表现出色，具有良好的准确性和可扩展性，适用于多种设计标准和复杂几何设计空间。

Abstract: We propose a two-stage deep learning framework for the inverse design of
rectangular patch antennas. Our approach leverages generative modeling to learn
a latent representation of antenna frequency response curves and conditions a
subsequent generative model on these responses to produce feasible antenna
geometries. We further demonstrate that leveraging search and optimization
techniques at test-time improves the accuracy of the generated designs and
enables consideration of auxiliary objectives such as manufacturability. Our
approach generalizes naturally to different design criteria, and can be easily
adapted to more complex geometric design spaces.

</details>


### [651] [PhySense: Sensor Placement Optimization for Accurate Physics Sensing](https://arxiv.org/abs/2505.18190)
*Yuezhou Ma,Haixu Wu,Hang Zhou,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: eess.SP

TL;DR: PhySense is a two-stage framework that combines physical field reconstruction and sensor placement optimization for accurate physics sensing.


<details>
  <summary>Details</summary>
Motivation: Existing methods in deep learning focus on sparse-data reconstruction but often neglect the optimization of sensor placements, missing out on potential mutual enhancement between these tasks.

Method: The framework consists of two stages: the first stage uses a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations for reconstructing physical fields; the second stage optimizes sensor placements through projected gradient descent, leveraging reconstruction feedback and satisfying spatial constraints. The method also proves the consistency of its learning objectives with classical variance-minimization principles.

Result: PhySense demonstrates state-of-the-art accuracy in physics sensing across three challenging benchmarks, including a 3D geometry dataset, and discovers previously unconsidered informative sensor placements.

Conclusion: PhySense effectively integrates physical field reconstruction and sensor placement optimization, providing theoretical guarantees and achieving high accuracy in physics sensing.

Abstract: Physics sensing plays a central role in many scientific and engineering
domains, which inherently involves two coupled tasks: reconstructing dense
physical fields from sparse observations and optimizing scattered sensor
placements to observe maximum information. While deep learning has made rapid
advances in sparse-data reconstruction, existing methods generally omit
optimization of sensor placements, leaving the mutual enhancement between
reconstruction and placement on the shelf. To change this suboptimal practice,
we propose PhySense, a synergistic two-stage framework that learns to jointly
reconstruct physical fields and to optimize sensor placements, both aiming for
accurate physics sensing. The first stage involves a flow-based generative
model enhanced by cross-attention to adaptively fuse sparse observations.
\correct{Leveraging the reconstruction feedback, }the second stage performs
sensor placement via projected gradient descent to satisfy spatial constraints.
\correct{We further prove that the learning objectives of the two stages are
consistent with classical variance-minimization principles, providing
theoretical guarantees.} Extensive experiments across three challenging
benchmarks, especially a 3D geometry dataset, indicate PhySense achieves
state-of-the-art physics sensing accuracy and discovers informative sensor
placements previously unconsidered.

</details>


### [652] [SzCORE as a benchmark: report from the seizure detection challenge at the 2025 AI in Epilepsy and Neurological Disorders Conference](https://arxiv.org/abs/2505.18191)
*Jonathan Dan,Amirhossein Shahbazinia,Christodoulos Kechris,David Atienza*

Main category: eess.SP

TL;DR: 尽管有30个团队提交了算法，但癫痫发作检测的性能仍然存在很大的差异，最好的F1分数仅为43%，这表明需要严格的基准测试和标准化评估。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型在不同患者或临床环境中的泛化能力不足，长期EEG自动检测癫痫发作仍然具有挑战性。手动审查EEG仍是临床标准，因此需要强大的模型和标准化评估。

Method: 组织了一个挑战赛，使用包含65名受试者（4,360小时）的私有数据集，专家对数据进行标注，参与者需检测癫痫发作的开始和持续时间，基于事件的指标进行评估，并采用SzCORE框架确保标准化评估。

Result: 收到了来自19个团队的30份提交，评估了28种算法，性能差异很大，最高F1分数为43%（敏感性37%，精确度45%）。与先前的挑战和商业系统相比，表现最佳的算法有所改进。

Conclusion: 该挑战揭示了报告的性能和实际评估之间的差距，强调了严格基准测试的重要性，并支持持续基准测试以促进可重复研究和新数据集的整合。

Abstract: Reliable automatic seizure detection from long-term EEG remains a challenge,
as current machine learning models often fail to generalize across patients or
clinical settings. Manual EEG review remains the clinical standard,
underscoring the need for robust models and standardized evaluation. To
rigorously assess algorithm performance, we organized a challenge using a
private dataset of continuous EEG recordings from 65 subjects (4,360 hours).
Expert neurophysiologists annotated the data, providing ground truth for
seizure events. Participants were required to detect seizure onset and
duration, with evaluation based on event-based metrics, including sensitivity,
precision, F1-score, and false positives per day. The SzCORE framework ensured
standardized evaluation. The primary ranking criterion was the event-based
F1-score, reflecting clinical relevance by balancing sensitivity and false
positives. The challenge received 30 submissions from 19 teams, with 28
algorithms evaluated. Results revealed wide variability in performance, with a
top F1-score of 43% (sensitivity 37%, precision 45%), highlighting the ongoing
difficulty of seizure detection. The challenge also revealed a gap between
reported performance and real-world evaluation, emphasizing the importance of
rigorous benchmarking. Compared to previous challenges and commercial systems,
the best-performing algorithm in this contest showed improved performance.
Importantly, the challenge platform now supports continuous benchmarking,
enabling reproducible research, integration of new datasets, and clinical
evaluation of seizure detection algorithms using a standardized framework.

</details>


### [653] [Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications](https://arxiv.org/abs/2505.18194)
*Yubo Peng,Luping Xiang,Bingxin Zhang,Kun Yang*

Main category: eess.SP

TL;DR: 为了应对复杂动态环境中的感知需求，本文提出了一种基于大语言模型的分布式多模态感知与语义通信框架（LLM-DiSAC），通过RF-视觉融合网络、基于LLM的语义传输网络和变压器聚合模型等模块提升感知精度和通信效率，并引入两阶段分布式学习策略保护数据隐私。实验结果表明该框架具有良好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的单模态感知系统在复杂动态环境中难以满足需求，且单设备系统受制于有限视角和空间覆盖不足，导致在城市或非视距场景下的效果受限。因此需要一种新型框架来克服这些挑战。

Method: 1. 提出了包含多个协作感知设备和一个聚合中心的LLM-DiSAC框架。
2. 构建了RF-视觉融合网络（RVFN），采用专门的特征提取器和交叉注意力模块进行多模态集成。
3. 设计了基于LLM的语义传输网络（LSTN），利用通道参数减少语义失真。
4. 开发了基于变压器的聚合模型（TRAM），通过自适应聚合注意力机制提高感知精度。
5. 引入两阶段分布式学习策略以保护数据隐私。

Result: 在由Genesis仿真引擎生成的合成多视角RF-视觉数据集上的评估表明，LLM-DiSAC实现了良好的性能。

Conclusion: LLM-DiSAC框架能够有效提升感知精度和通信效率，同时通过两阶段分布式学习策略保护数据隐私，在复杂动态环境下表现出色。

Abstract: Traditional single-modal sensing systems-based solely on either radio
frequency (RF) or visual data-struggle to cope with the demands of complex and
dynamic environments. Furthermore, single-device systems are constrained by
limited perspectives and insufficient spatial coverage, which impairs their
effectiveness in urban or non-line-of-sight scenarios. To overcome these
challenges, we propose a novel large language model (LLM)-driven distributed
integrated multimodal sensing and semantic communication (LLM-DiSAC) framework.
Specifically, our system consists of multiple collaborative sensing devices
equipped with RF and camera modules, working together with an aggregation
center to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC
develops an RF-vision fusion network (RVFN), which employs specialized feature
extractors for RF and visual data, followed by a cross-attention module for
effective multimodal integration. Second, a LLM-based semantic transmission
network (LSTN) is proposed to enhance communication efficiency, where the
LLM-based decoder leverages known channel parameters, such as transceiver
distance and signal-to-noise ratio (SNR), to mitigate semantic distortion.
Third, at the aggregation center, a transformer-based aggregation model (TRAM)
with an adaptive aggregation attention mechanism is developed to fuse
distributed features and enhance sensing accuracy. To preserve data privacy, a
two-stage distributed learning strategy is introduced, allowing local model
training at the device level and centralized aggregation model training using
intermediate features. Finally, evaluations on a synthetic multi-view RF-visual
dataset generated by the Genesis simulation engine show that LLM-DiSAC achieves
a good performance.

</details>


### [654] [CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting](https://arxiv.org/abs/2505.18200)
*Fahrettin Emin Tiras,Hayriye Serra Altinoluk*

Main category: eess.SP

TL;DR: CrossRF是一种用于无人机识别的跨信道射频指纹技术，利用对抗学习减少不同信道间的领域差异，提高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 射频指纹在不同传输信道上进行无人机识别时性能显著下降，需要一种能克服信道差异的方法。

Method: 提出了一种基于对抗学习的领域不变深度学习方法CrossRF，通过最小化不同射频信道间的领域差异来训练更鲁棒的模型。

Result: 在UAVSig数据集上的实验结果表明，CrossRF在信道3到信道4的适应中达到99.03%的准确率，远高于传统方法的26.39%，在多信道场景中也保持了87.57%的高准确率。

Conclusion: CrossRF能够显著降低因跨信道变化导致的性能下降，同时保持高识别准确率且对训练数据需求较少，适合实际无人机安全应用。

Abstract: Radio Frequency (RF) fingerprinting offers a promising approach for drone
identification and security, although it suffers from significant performance
degradation when operating on different transmission channels. This paper
presents CrossRF, a domain-invariant deep learning approach that addresses the
problem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV)
identification. Our approach aims to minimize the domain gap between different
RF channels by using adversarial learning to train a more robust model that
maintains consistent identification performance despite channel variations. We
validate our approach using the UAVSig dataset, comprising real-world
over-the-air RF signals from identical drone models operating across several
frequency channels, ensuring that the findings correspond to real-world
scenarios. The experimental results show CrossRF's efficiency, achieving up to
99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only
26.39% using conventional methods. The model maintains robust performance in
more difficult multi-channel scenarios (87.57% accuracy adapting from Channels
1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller
classification. These results confirm CrossRF's ability to significantly reduce
performance degradation due to cross-channel variations while maintaining high
identification accuracy with minimal training data requirements, making it
particularly suitable for practical drone security applications.

</details>


### [655] [Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion](https://arxiv.org/abs/2505.18747)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: eess.SP

TL;DR: The paper proposes a PV disaggregation method combining Hierarchical Interpolation (HI) and multi-head self-attention mechanisms for precise PV generation predictions from net electricity load.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of distributed photovoltaic (PV) systems creates challenges for utility companies in terms of smart monitoring and measurement, specifically in separating PV generation from net electricity load.

Method: The method integrates Hierarchical Interpolation (HI) for feature extraction from net load and multi-head self-attention mechanisms to capture the relevance between weather factors.

Result: Simulation experiments show the effectiveness of the proposed method using real-world data, leading to precise PV generation predictions.

Conclusion: The proposed PV disaggregation method supports improved monitoring and management of distributed energy systems.

Abstract: With the advancement of energy Internet and energy system integration, the
increasing adoption of distributed photovoltaic (PV) systems presents new
challenges on smart monitoring and measurement for utility companies,
particularly in separating PV generation from net electricity load. Existing
methods struggle with feature extraction from net load and capturing the
relevance between weather factors. This paper proposes a PV disaggregation
method that integrates Hierarchical Interpolation (HI) and multi-head
self-attention mechanisms. By using HI to extract net load features and
multi-head self-attention to capture the complex dependencies between weather
factors, the method achieves precise PV generation predictions. Simulation
experiments demonstrate the effectiveness of the proposed method in real-world
data, supporting improved monitoring and management of distributed energy
systems.

</details>


### [656] [Accelerating Battery Material Optimization through iterative Machine Learning](https://arxiv.org/abs/2505.18162)
*Seon-Hwa Lee,Insoo Ye,Changhwan Lee,Jieun Kim,Geunho Choi,Sang-Cheol Nam,Inchul Park*

Main category: eess.SP

TL;DR: 通过引入结合主动学习的迭代机器学习框架，有效减少实验周期数量，加速高维设计空间探索，推动电池材料优化。


<details>
  <summary>Details</summary>
Motivation: 传统的一次一因素实验方法在面对日益复杂的工业参数时受到认知限制和人为偏差的影响，难以满足需求。

Method: 提出了一种结合主动学习的迭代机器学习框架，利用成功与不成功的实验结果进行针对性实验指导和模型逐步优化。

Result: 显著减少了实验周期的总次数，证明了基于机器学习策略在加速电池材料优化方面的变革潜力。

Conclusion: 主动学习驱动的实验方法能够有效缓解人为偏差并减轻数据稀缺问题，快速探索高维设计空间。

Abstract: The performance of battery materials is determined by their composition and
the processing conditions employed during commercial-scale fabrication, where
raw materials undergo complex processing steps with various additives to yield
final products. As the complexity of these parameters expands with the
development of industry, conventional one-factor-at-a-time (OFAT) experiment
becomes old fashioned. While domain expertise aids in parameter optimization,
this traditional approach becomes increasingly vulnerable to cognitive
limitations and anthropogenic biases as the complexity of factors grows.
Herein, we introduce an iterative machine learning (ML) framework that
integrates active learning to guide targeted experimentation and facilitate
incremental model refinement. This method systematically leverages
comprehensive experimental observations, including both successful and
unsuccessful results, effectively mitigating human-induced biases and
alleviating data scarcity. Consequently, it significantly accelerates
exploration within the high-dimensional design space. Our results demonstrate
that active-learning-driven experimentation markedly reduces the total number
of experimental cycles necessary, underscoring the transformative potential of
ML-based strategies in expediting battery material optimization.

</details>


### [657] [Dim and Small Target Detection for Drone Broadcast Frames Based on Time-Frequency Analysis](https://arxiv.org/abs/2505.18167)
*Jie Li,Jing Li,Zhanyu Ju,Fengkui Gong,Lu Lv*

Main category: eess.SP

TL;DR: We propose a dim and small target detection algorithm for drone broadcast frames based on the time-frequency analysis of communication protocol.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the detection accuracy of dim and small targets under low SNR situations, and to meet different drone regulation requirements by establishing the trade-off between detection accuracy and speed versus sampling duration.

Method: Specifically, by analyzing modulation parameters and frame structures, the prior knowledge of transmission frequency, signal bandwidth, Zadoff-Chu (ZC) sequences, and frame length of drone broadcast frames is established. The RF signals are processed through the designed filter banks, and the frequency domain parameters of bounding boxes generated by the detector are corrected with transmission frequency and signal bandwidth. Given the remarkable correlation characteristics of ZC sequences, the frequency domain parameters of bounding boxes with low confidence scores are corrected based on ZC sequences and frame length, which improves the detection accuracy of dim targets under low signal-to noise ratio (SNR) situations. Besides, a segmented energy refinement method is applied to mitigate the deviation caused by interference signals with high energy strength, which ulteriorly corrects the time domain detection parameters for dim targets.

Result: Simulation results demonstrate that the proposed algorithm improves the average intersection over union, precision, and recall by 3%, 1.4%, and 2.4%, respectively, compared to existing algorithms.

Conclusion: The proposed algorithm performs strong robustness under varying flight distances, diverse types of environment noise, and different flight visual environment.

Abstract: We propose a dim and small target detection algorithm for drone broadcast
frames based on the time-frequency analysis of communication protocol.
Specifically, by analyzing modulation parameters and frame structures, the
prior knowledge of transmission frequency, signal bandwidth, Zadoff-Chu (ZC)
sequences, and frame length of drone broadcast frames is established. The RF
signals are processed through the designed filter banks, and the frequency
domain parameters of bounding boxes generated by the detector are corrected
with transmission frequency and signal bandwidth. Given the remarkable
correlation characteristics of ZC sequences, the frequency domain parameters of
bounding boxes with low confidence scores are corrected based on ZC sequences
and frame length, which improves the detection accuracy of dim targets under
low signal-to noise ratio (SNR) situations. Besides, a segmented energy
refinement method is applied to mitigate the deviation caused by interference
signals with high energy strength, which ulteriorly corrects the time domain
detection parameters for dim targets. As the sampling duration increases, the
detection speed improves while the detection accuracy of broadcast frames
termed as small targets decreases. The trade-off between detection accuracy and
speed versus sampling duration is established, which helps to meet different
drone regulation requirements. Simulation results demonstrate that the proposed
algorithm improves the average intersection over union, precision, and recall
by 3\%, 1.4\%, and 2.4\%, respectively, compared to existing algorithms. The
proposed algorithm also performs strong robustness under varying flight
distances, diverse types of environment noise, and different flight visual
environment.

</details>


### [658] [Load Forecasting in the Era of Smart Grids: Opportunities and Advanced Machine Learning Models](https://arxiv.org/abs/2505.18170)
*Aurausp Maneshni*

Main category: eess.SP

TL;DR: Electric energy's difficulty in storage requires strict control over its generation, transmission, and distribution. Maintaining real-time equilibrium between electricity demand and supply is a persistent challenge in power systems. Oversupply causes resource wastage while undersupply can strain the grid, increase operational costs, and impact service reliability. Load forecasting is needed to maintain grid stability by predicting future electricity consumption to balance generation and demand. This thesis examines and evaluates four machine learning frameworks for short term load forecasting including XGBoost and LightGBM, develops a hybrid framework, and designs two recurrent neural network architectures (LSTM and GRU). The Pearson Correlation Coefficient assesses relationships between electricity demand and exogenous variables. Experimental results show that machine learning-based models achieved improved forecasting performance compared to classical ARIMA baseline for this study's specific dataset and forecasting task.


<details>
  <summary>Details</summary>
Motivation: Electric energy is hard to store thus requires strict control over generation, transmission, and distribution. A persistent challenge in power systems is maintaining real-time equilibrium between electricity demand and supply to avoid resource wastage, high operational costs, and service unreliability. Accurate load forecasting is essential for balancing generation and demand by predicting future electricity consumption.

Method: The research examines and evaluates four machine learning frameworks for short term load forecasting including gradient boosting decision tree methods like Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). It also develops a hybrid framework and designs two recurrent neural network architectures - Long Short Term Memory (LSTM) networks and Gated Recurrent Units (GRU). The Pearson Correlation Coefficient is applied to measure relationships between electricity demand and exogenous variables.

Result: For the specific dataset and forecasting task in this study, machine learning-based models performed better than the classical ARIMA model in terms of forecasting accuracy.

Conclusion: Machine learning frameworks provide effective tools for short term load forecasting, achieving superior performance compared to traditional methods. They play an important role in improving grid stability by accurately predicting electricity consumption.

Abstract: Electric energy is difficult to store, requiring stricter control over its
generation, transmission, and distribution. A persistent challenge in power
systems is maintaining real-time equilibrium between electricity demand and
supply. Oversupply contributes to resource wastage, while undersupply can
strain the grid, increase operational costs, and potentially impact service
reliability. To maintain grid stability, load forecasting is needed. Accurate
load forecasting balances generation and demand by striving to predict future
electricity consumption. This thesis examines and evaluates four machine
learning frameworks for short term load forecasting, including gradient
boosting decision tree methods such as Extreme Gradient Boosting (XGBoost) and
Light Gradient Boosting Machine (LightGBM). A hybrid framework is also
developed. In addition, two recurrent neural network architectures, Long Short
Term Memory (LSTM) networks and Gated Recurrent Units (GRU), are designed and
implemented. Pearson Correlation Coefficient is applied to assess the
relationships between electricity demand and exogenous variables. The
experimental results show that, for the specific dataset and forecasting task
in this study, machine learning-based models achieved improved forecasting
performance compared to a classical ARIMA baseline.

</details>


### [659] [Machine Learning-Based Analysis of ECG and PCG Signals for Rheumatic Heart Disease Detection: A Scoping Review (2015-2025)](https://arxiv.org/abs/2505.18182)
*Damilare Emmanuel Olatunji,Julius Dona Zannu,Carine Pierrette Mukamakuza,Godbright Nixon Uiso,Mona Mamoun Mubarak Aman,John Bosco Thuo,Chol Buol,Nchofon Tagha Ghogomu,Evelyne Umubyeyi*

Main category: eess.SP

TL;DR: This paper conducts a systematic review of machine learning applications using ECG and heart sound data for rheumatic heart disease detection from 2015 to 2025, emphasizing the need for cost-effective alternatives to echocardiography in underserved regions. It reveals that CNNs are predominant post-2020 with high accuracy but identifies gaps in external validation, cost-effectiveness analysis, and demographic diversity.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of ML-based ECG/PCG analysis as a cost-effective alternative to echocardiography for RHD detection, particularly in resource-constrained settings, supporting the World Heart Federation's '25 by 25' mortality reduction objective.

Method: A comprehensive search following PRISMA-ScR guidelines across PubMed, IEEE Xplore, Scopus, and Embase was conducted. Two independent reviewers screened studies focusing on methodology, validation approaches, and performance metrics.

Result: Analysis of 37 studies showed CNNs dominance post-2020 with median accuracy of 93.7%. However, significant gaps were found in external validation (only 10.8%), cost-effectiveness analysis, and demographic diversity.

Conclusion: ML-based ECG/PCG analysis holds promise for RHD detection but faces substantial methodological limitations. Future research should focus on standardized benchmarking, multimodal architectures, cost-effectiveness assessments, and prospective trials in endemic regions.

Abstract: Objective: To conduct a systematic assessment of machine learning
applications that utilize electrocardiogram (ECG) and heart sound data in the
development of cost-effective detection tools for rheumatic heart disease (RHD)
from the year 2015 to 2025, thereby supporting the World Heart Federation's "25
by 25" mortality reduction objective through the creation of alternatives to
echocardiography in underserved regions. Methods: Following PRISMA-ScR
guidelines, we conducted a comprehensive search across PubMed, IEEE Xplore,
Scopus, and Embase for peer-reviewed literature focusing on ML-based ECG/PCG
analysis for RHD detection. Two independent reviewers screened studies, and
data extraction focused on methodology, validation approaches, and performance
metrics. Results: Analysis of 37 relevant studies revealed that convolutional
neural networks (CNNs) have become the predominant technology in post-2020
implementations, achieving a median accuracy of 93.7%. However, 73% of studies
relied on single-center datasets, only 10.8% incorporated external validation,
and none addressed cost-effectiveness. Performance varied markedly across
different valvular lesions, and despite 44% of studies originating from endemic
regions, significant gaps persisted in implementation science and demographic
diversity. Conclusion: While ML-based ECG/PCG analysis shows promise for RHD
detection, substantial methodological limitations hinder clinical translation.
Future research must prioritize standardized benchmarking frameworks,
multimodal architectures, cost-effectiveness assessments, and prospective
trials in endemic settings. Significance: This review provides a critical
roadmap for developing accessible ML-based RHD screening tools to help bridge
the diagnostic gap in resourceconstrained settings where conventional
auscultation misses up to 90% of cases and echocardiography remains
inaccessible.

</details>


### [660] [FRAME-C: A knowledge-augmented deep learning pipeline for classifying multi-electrode array electrophysiological signals](https://arxiv.org/abs/2505.18183)
*Nisal Ranasinghe,Dzung Do-Ha,Simon Maksour,Tamasha Malepathirana,Sachith Seneviratne,Lezanne Ooi,Saman Halgamuge*

Main category: eess.SP

TL;DR: A new machine learning pipeline FRAME-C is developed to classify MEA signals and identify ALS-specific phenotypes by combining deep learning techniques with handcrafted features, which improves the classification performance significantly and provides insights into ALS phenotypes.


<details>
  <summary>Details</summary>
Motivation: ALS is a fatal neurodegenerative disorder characterized by motor neuron degeneration. Although MEA electrophysiology can provide rich data, traditional analysis methods using handcrafted features may not fully capture all useful characteristics inherent in the data. Machine learning has the potential to automatically learn relevant characteristics from raw data without solely relying on handcrafted feature extraction.

Method: FRAME-C, a knowledge-augmented machine learning pipeline, combines domain knowledge, raw spike waveform data, and deep learning techniques to classify MEA signals and identify ALS-specific phenotypes. It leverages deep learning to learn important features from spike waveforms while incorporating handcrafted features such as spike amplitude, inter-spike interval, and spike duration.

Result: FRAME-C demonstrates superior performance over existing classification methods, showing over 11% improvement on real data and up to 25% on simulated data. It can also evaluate handcrafted feature importance, providing insights into ALS phenotypes.

Conclusion: FRAME-C is an effective tool for classifying MEA signals and identifying ALS-specific phenotypes, improving the understanding of ALS.

Abstract: Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disorder
characterized by motor neuron degeneration, with alterations in neural
excitability serving as key indicators. Recent advancements in induced
pluripotent stem cell (iPSC) technology have enabled the generation of human
iPSC-derived neuronal cultures, which, when combined with multi-electrode array
(MEA) electrophysiology, provide rich spatial and temporal electrophysiological
data. Traditionally, MEA data is analyzed using handcrafted features based on
potentially imperfect domain knowledge, which while useful may not fully
capture all useful characteristics inherent in the data. Machine learning,
particularly deep learning, has the potential to automatically learn relevant
characteristics from raw data without solely relying on handcrafted feature
extraction. However, handcrafted features remain critical for encoding domain
knowledge and improving interpretability, especially with limited or noisy
data. This study introduces FRAME-C, a knowledge-augmented machine learning
pipeline that combines domain knowledge, raw spike waveform data, and deep
learning techniques to classify MEA signals and identify ALS-specific
phenotypes. FRAME-C leverages deep learning to learn important features from
spike waveforms while incorporating handcrafted features such as spike
amplitude, inter-spike interval, and spike duration, preserving key spatial and
temporal information. We validate FRAME-C on both simulated and real MEA data
from human iPSC-derived neuronal cultures, demonstrating superior performance
over existing classification methods. FRAME-C shows over 11% improvement on
real data and up to 25% on simulated data. We also show FRAME-C can evaluate
handcrafted feature importance, providing insights into ALS phenotypes.

</details>


### [661] [BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals](https://arxiv.org/abs/2505.18185)
*Qinfan Xiao,Ziyun Cui,Chi Zhang,Siqi Chen,Wen Wu,Andrew Thwaites,Alexandra Woolgar,Bowen Zhou,Chao Zhang*

Main category: eess.SP

TL;DR: This paper introduces BrainOmni, a foundation model that generalises across EEG and MEG recordings using a tokenizer called BrainTokenizer. It outperforms existing models on downstream tasks and shows strong generalisation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing approaches that rely on separate, modality- and dataset-specific models for EEG and MEG recordings, which limits performance and cross-domain scalability.

Method: The paper proposes BrainOmni, which uses BrainTokenizer to quantise spatiotemporal brain activity into discrete representations. A Sensor Encoder is used to encode sensor properties enabling compatibility across devices and modalities. BrainOmni learns unified semantic embeddings of brain signals by self-supervised pretraining using large-scale EEG and MEG data.

Result: BrainOmni outperforms existing foundation models and state-of-the-art task-specific models on a range of downstream tasks. It demonstrates strong generalisation to unseen EEG and MEG devices. Joint EEG-MEG training yields consistent improvements across both modalities.

Conclusion: BrainOmni is the first foundation model to support both EEG and MEG signals, incorporating large-scale MEG pretraining. The model's code and checkpoints will be released upon acceptance.

Abstract: Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural
activity non-invasively by capturing electromagnetic fields generated by
dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit
distinct signal patterns, further complicated by variations in sensor
configurations across modalities and recording devices. Existing approaches
typically rely on separate, modality- and dataset-specific models, which limits
the performance and cross-domain scalability. This paper proposes BrainOmni,
the first brain foundation model that generalises across heterogeneous EEG and
MEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the
first tokenizer that quantises spatiotemporal brain activity into discrete
representations. Central to BrainTokenizer is a novel Sensor Encoder that
encodes sensor properties such as spatial layout, orientation, and type,
enabling compatibility across devices and modalities. Building upon the
discrete representations, BrainOmni learns unified semantic embeddings of brain
signals by self-supervised pretraining. To the best of our knowledge, it is the
first foundation model to support both EEG and MEG signals, as well as the
first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG
and 656 hours of MEG data are curated and standardised from publicly available
sources for pretraining. Experiments show that BrainOmni outperforms both
existing foundation models and state-of-the-art task-specific models on a range
of downstream tasks. It also demonstrates strong generalisation to unseen EEG
and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training
yields consistent improvements across both modalities. Code and model
checkpoints will be released upon acceptance.

</details>


### [662] [Generating Realistic Multi-Beat ECG Signals](https://arxiv.org/abs/2505.18189)
*Paul Pöhl,Viktor Schlegel,Hao Li,Anil Bharath*

Main category: eess.SP

TL;DR: The paper presents a three-layer synthesis framework for generating long-form ECG signals, outperforming existing diffusion models in arrhythmia classification tasks while preserving diagnostic characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models can generate high-quality short ECG segments, but struggle with the longer sequences required for many clinical applications.

Method: A three-layer synthesis framework is proposed: 1) Generate single beats using a diffusion model, 2) Synthesize inter-beat features to preserve temporal dependencies, 3) Assemble beats into coherent long sequences using feature-guided matching.

Result: The synthetic ECGs maintain beat-level morphological fidelity and clinically relevant inter-beat relationships. They significantly outperform end-to-end long-form ECG generation using diffusion models in arrhythmia classification tasks.

Conclusion: This approach enables the generation of multi-minute ECG sequences while preserving essential diagnostic characteristics, increasing utility for downstream applications.

Abstract: Generating synthetic ECG data has numerous applications in healthcare, from
educational purposes to simulating scenarios and forecasting trends. While
recent diffusion models excel at generating short ECG segments, they struggle
with longer sequences needed for many clinical applications. This paper
proposes a novel three-layer synthesis framework for generating realistic
long-form ECG signals. We first generate high-fidelity single beats using a
diffusion model, then synthesize inter-beat features preserving critical
temporal dependencies, and finally assemble beats into coherent long sequences
using feature-guided matching. Our comprehensive evaluation demonstrates that
the resulting synthetic ECGs maintain both beat-level morphological fidelity
and clinically relevant inter-beat relationships. In arrhythmia classification
tasks, our long-form synthetic ECGs significantly outperform end-to-end
long-form ECG generation using the diffusion model, highlighting their
potential for increasing utility for downstream applications. The approach
enables generation of unprecedented multi-minute ECG sequences while preserving
essential diagnostic characteristics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [663] [Towards a Quantum-classical Augmented Network](https://arxiv.org/abs/2505.18282)
*Nitin Jha,Abhishek Parakh,Mahadevan Subramaniam*

Main category: quant-ph

TL;DR: 为了适应大规模量子网络的需求，本文提出对HTTP协议进行改造以同时承载量子和经典负载，并通过机器学习模型分类通信隐私标签以优化量子资源利用。


<details>
  <summary>Details</summary>
Motivation: 当前的小规模量子密钥分发网络已经建立，但大规模量子网络的部署仍受限于量子中继、量子信道等技术的发展。为了提升现有网络的安全性并采用可行的量子技术，需要将经典网络与量子设备、特性和现象结合。

Method: 提出对HTTP协议结构的修改，使其能同时承载量子和经典负载；将单个网络数据包根据隐私需求划分为经典和量子负载；使用逻辑回归、CNN、LSTM和BiLSTM模型分类出站通信的隐私标签。

Result: 实验结果表明，该方法能够减少量子资源的使用，从而实现更高效的量子网络安全设计。

Conclusion: 本研究为结合经典和量子负载的网络设计奠定了基础，并提供了一种有效利用量子资源的方法，推动了安全量子网络的发展。

Abstract: In the past decade, several small-scale quantum key distribution networks
have been established. However, the deployment of large-scale quantum networks
depends on the development of quantum repeaters, quantum channels, quantum
memories, and quantum network protocols. To improve the security of existing
networks and adopt currently feasible quantum technologies, the next step is to
augment classical networks with quantum devices, properties, and phenomena. To
achieve this, we propose a change in the structure of the HTTP protocol such
that it can carry both quantum and classical payload. This work lays the
foundation for dividing one single network packet into classical and quantum
payloads depending on the privacy needs. We implement logistic regression, CNN,
LSTM, and BiLSTM models to classify the privacy label for outgoing
communications. This enables reduced utilization of quantum resources allowing
for a more efficient secure quantum network design. Experimental results using
the proposed methods are presented.

</details>


### [664] [Effect of noise and topologies on multi-photon quantum protocols](https://arxiv.org/abs/2505.19270)
*Nitin Jha,Abhishek Parakh,Mahadevan Subramaniam*

Main category: quant-ph

TL;DR: 本论文研究了多光子量子协议在不同噪声类型下的表现，并探讨了网络拓扑对协议性能的影响，为量子增强网络中的中继器布局和拓扑选择提供了见解。


<details>
  <summary>Details</summary>
Motivation: 量子增强网络通过使用量子现象来改进经典通信网络中的检测和防护能力，但单光子协议存在传输距离和数据速率的瓶颈，并且由于单光子发射器制造的困难，其安全性假设在实际应用中难以成立。因此，需要探索更实用的多光子量子协议。

Method: 作者研究了通道噪声对多光子协议的影响，并分析了环形、星形和环面等不同网络拓扑结构如何影响多光子协议的噪声特性。

Result: 研究表明，切换到多光子协议可能带来优势，并为量子增强网络中的中继器放置和拓扑选择提供了深入见解。

Conclusion: 多光子协议不仅能够提供更高的安全性和更长的传输距离，而且在实际噪声条件下表现出色，适合用于量子增强网络的设计与优化。

Abstract: Quantum-augmented networks aim to use quantum phenomena to improve detection
and protection against malicious actors in a classical communication network.
This may include multiplexing quantum signals into classical fiber optical
channels and incorporating purely quantum links alongside classical links in
the network. In such hybrid networks, quantum protocols based on single photons
become a bottleneck for transmission distances and data speeds, thereby
reducing entire network performance. Furthermore, many of the security
assumptions of the single-photon protocols do not hold up in practice because
of the impossibility of manufacturing single-photon emitters. Multi-photon
quantum protocols, on the other hand, are designed to operate under practical
assumptions and do not require single photon emitters. As a result, they
provide higher levels of security guarantees and longer transmission distances.
However, the effect of channel and device noise on multiphoton protocols in
terms of security, transmission distances, and bit rates has not been
investigated. In this paper, we focus on channel noise and present our
observations on the effect of various types of noise on multi-photon protocols.
We also investigate the effect of topologies such as ring, star, and torus on
the noise characteristics of the multi-photon protocols. Our results show the
possible advantages of switching to multi-photon protocols and give insights
into the repeater placement and topology choice for quantum-augmented networks.

</details>


### [665] [A Matrix Product State Model for Simultaneous Classification and Generation](https://arxiv.org/abs/2406.17441)
*Alex Mossi,Bojan Žunkovic,Kyriakos Flouris*

Main category: quant-ph

TL;DR: The paper explores the application of Matrix Product States (MPS) from quantum computing in classical machine learning, proposing a dual-functional MPS model for classification and generation tasks, inspired by generative adversarial networks.


<details>
  <summary>Details</summary>
Motivation: To leverage the efficiency of tensor networks, originally developed for quantum systems simulation, in classical machine learning settings to handle complex, high-dimensional data.

Method: An MPS model is introduced where MPS serves as both a classifier and a generator. The training strategy draws inspiration from generative adversarial networks to generate more realistic samples by reducing outliers.

Result: The novel MPS model offers insights into tensor network methods for generation tasks, including alternative embedding functions and a new sampling method from non-normalized MPSs.

Conclusion: Tensor network methods, particularly MPS, show promise in classical machine learning for supervised learning and generation tasks.

Abstract: Quantum machine learning (QML) is a rapidly expanding field that merges the
principles of quantum computing with the techniques of machine learning. One of
the powerful mathematical frameworks in this domain is tensor networks. These
networks are used to approximate high-order tensors by contracting tensors with
lower ranks. Initially developed for simulating quantum systems, tensor
networks have become integral to quantum computing and, by extension, to QML.
Drawing inspiration from these quantum methods, specifically the Matrix Product
States (MPS), we apply them in a classical machine learning setting. Their
ability to efficiently represent and manipulate complex, high-dimensional data
makes them effective in a supervised learning framework. Here, we present an
MPS model, in which the MPS functions as both a classifier and a generator. The
dual functionality of this novel MPS model permits a strategy that enhances the
traditional training of supervised MPS models. This framework is inspired by
generative adversarial networks and is geared towards generating more realistic
samples by reducing outliers. In addition, our contributions offer insights
into the mechanics of tensor network methods for generation tasks.
Specifically, we discuss alternative embedding functions and a new sampling
method from non-normalized MPSs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [666] [Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation](https://arxiv.org/abs/2505.18787)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.CV

TL;DR: A novel online test-time adaptation method called Think Twice before Adaptation (T²A) is proposed to enhance the adaptability of deepfake detectors during inference without needing source training data or labels. It uses an Uncertainty-aware Negative Learning objective, Uncertain Sample Prioritization, and Gradients Masking for better adaptation.


<details>
  <summary>Details</summary>
Motivation: Deepfake detectors encounter significant challenges in real-world environments due to deviations in test samples from training data caused by postprocessing manipulations or distribution shifts. Postprocessing techniques can obscure generation artifacts, leading to performance degradation of detectors.

Method: The proposed method, T²A, enhances adaptability through an Uncertainty-aware Negative Learning objective, which allows the model to explore alternative options instead of relying on initial predictions. Additionally, it incorporates an Uncertain Sample Prioritization strategy and Gradients Masking technique to focus on important samples and model parameters.

Result: Empirically, the method achieves state-of-the-art results compared to existing test-time adaptation approaches and significantly improves the resilience and generalization of deepfake detectors during inference.

Conclusion: The Think Twice before Adaptation method provides a robust solution for enhancing the adaptability of deepfake detectors in real-world scenarios without requiring access to source training data or labels.

Abstract: Deepfake (DF) detectors face significant challenges when deployed in
real-world environments, particularly when encountering test samples deviated
from training data through either postprocessing manipulations or distribution
shifts. We demonstrate postprocessing techniques can completely obscure
generation artifacts presented in DF samples, leading to performance
degradation of DF detectors. To address these challenges, we propose Think
Twice before Adaptation (\texttt{T$^2$A}), a novel online test-time adaptation
method that enhances the adaptability of detectors during inference without
requiring access to source training data or labels. Our key idea is to enable
the model to explore alternative options through an Uncertainty-aware Negative
Learning objective rather than solely relying on its initial predictions as
commonly seen in entropy minimization (EM)-based approaches. We also introduce
an Uncertain Sample Prioritization strategy and Gradients Masking technique to
improve the adaptation by focusing on important samples and model parameters.
Our theoretical analysis demonstrates that the proposed negative learning
objective exhibits complementary behavior to EM, facilitating better adaptation
capability. Empirically, our method achieves state-of-the-art results compared
to existing test-time adaptation (TTA) approaches and significantly enhances
the resilience and generalization of DF detectors during inference. Code is
available
\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}.

</details>


### [667] [Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation](https://arxiv.org/abs/2505.19425)
*Yuhao He,Jinyu Tian,Haiwei Wu,Jianqing Li*

Main category: cs.CV

TL;DR: Diffusion models have improved image editing but pose societal risks. This paper proposes Structure Disruption Attack (SDA), a framework to protect sensitive image regions by disrupting the contour generation process in diffusion models, preventing coherent image production.


<details>
  <summary>Details</summary>
Motivation: To safeguard sensitive image regions against inpainting-based editing and prevent adversaries from exploiting user images for harmful content.

Method: Proposes Structure Disruption Attack (SDA) which optimizes perturbations by disrupting queries in self-attention during the initial denoising step of diffusion models, thereby destroying the contour generation process.

Result: Validates the motivation through visualization techniques and extensive experiments on public datasets, showing SDA achieves state-of-the-art protection performance with strong robustness.

Conclusion: SDA is a powerful protection framework that effectively prevents diffusion models from producing coherent images by disrupting their structural generation capability.

Abstract: The rapid advancement of diffusion models has enhanced their image inpainting
and editing capabilities but also introduced significant societal risks.
Adversaries can exploit user images from social media to generate misleading or
harmful content. While adversarial perturbations can disrupt inpainting, global
perturbation-based methods fail in mask-guided editing tasks due to spatial
constraints. To address these challenges, we propose Structure Disruption
Attack (SDA), a powerful protection framework for safeguarding sensitive image
regions against inpainting-based editing. Building upon the contour-focused
nature of self-attention mechanisms of diffusion models, SDA optimizes
perturbations by disrupting queries in self-attention during the initial
denoising step to destroy the contour generation process. This targeted
interference directly disrupts the structural generation capability of
diffusion models, effectively preventing them from producing coherent images.
We validate our motivation through visualization techniques and extensive
experiments on public datasets, demonstrating that SDA achieves
state-of-the-art (SOTA) protection performance while maintaining strong
robustness.

</details>


### [668] [COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification](https://arxiv.org/abs/2505.18315)
*Mariano Rivera,Angello Hoyos*

Main category: cs.CV

TL;DR: The paper introduces CoLoRA, a method extending LoRA for efficient CNN fine-tuning with fewer parameters and improved training speed and stability. It enhances accuracy in classifying retinal diseases from OCT images.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in current CNN fine-tuning methods which require many parameters and have slower training.

Method: CoLoRA is used as a coarse-tuning procedure on models with CNN backbones pre-trained on ImageNet. The method focuses on reducing parameters needed for fine-tuning and improving computational efficiency.

Result: CoLoRA improves the speed and stability of training while reducing the number of parameters required for fine-tuning. In a case study using the OCTMNIST dataset, it achieved nearly 1% higher accuracy compared to traditional methods.

Conclusion: CoLoRA offers an effective approach for CNN fine-tuning with performance comparable to advanced models like Vision Transformers.

Abstract: We introduce the Convolutional Low-Rank Adaptation (CoLoRA) method, designed
explicitly to overcome the inefficiencies found in current CNN fine-tuning
methods. CoLoRA can be seen as a natural extension of the convolutional
architectures of the Low-Rank Adaptation (LoRA) technique. We demonstrate the
capabilities of our method by developing and evaluating models using the widely
adopted CNN backbone pre-trained on ImageNet. We observed that this strategy
results in a stable and accurate coarse-tuning procedure. Moreover, this
strategy is computationally efficient and significantly reduces the number of
parameters required for fine-tuning compared to traditional methods.
Furthermore, our method substantially improves the speed and stability of
training. Our case study focuses on classifying retinal diseases from optical
coherence tomography (OCT) images, specifically using the OCTMNIST dataset.
Experimental results demonstrate that a CNN backbone fine-tuned with CoLoRA
surpasses nearly 1\% in accuracy. Such a performance is comparable to the
Vision Transformer, State-space discrete, and Kolmogorov-Arnold network models.

</details>


### [669] [Taming Diffusion for Dataset Distillation with High Representativeness](https://arxiv.org/abs/2505.18399)
*Lin Zhao,Yushu Wu,Xinru Jiang,Jianyang Gu,Yanzhi Wang,Xiaolin Xu,Pu Zhao,Xue Lin*

Main category: cs.CV

TL;DR: Recent deep learning models require larger datasets, leading to the need for dataset distillation. Current diffusion-based methods have issues such as inaccurate distribution matching and deviation with random noise. This paper introduces D^3HR, a novel framework that uses DDIM inversion to map latents from a low-normality latent domain to a high-normality Gaussian domain, preserving information and ensuring structural consistency. Experiments show that D^3HR outperforms state-of-the-art baselines in dataset distillation.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing demand for larger datasets in deep learning models and the challenges faced by current diffusion-based dataset distillation methods, including inaccurate distribution matching, distribution deviation with random noise, and separate sampling.

Method: The method involves using DDIM inversion to map the latents of the full dataset from a low-normality latent domain to a high-normality Gaussian domain, preserving information and ensuring structural consistency. Additionally, an efficient sampling scheme is proposed to align representative latents with the high-normality Gaussian distribution.

Result: D^3HR achieves higher accuracy across different model architectures compared to state-of-the-art baselines in dataset distillation.

Conclusion: D^3HR is a novel diffusion-based framework that addresses issues in current dataset distillation methods, offering improved performance in generating distilled datasets.

Abstract: Recent deep learning models demand larger datasets, driving the need for
dataset distillation to create compact, cost-efficient datasets while
maintaining performance. Due to the powerful image generation capability of
diffusion, it has been introduced to this field for generating distilled
images. In this paper, we systematically investigate issues present in current
diffusion-based dataset distillation methods, including inaccurate distribution
matching, distribution deviation with random noise, and separate sampling.
Building on this, we propose D^3HR, a novel diffusion-based framework to
generate distilled datasets with high representativeness. Specifically, we
adopt DDIM inversion to map the latents of the full dataset from a
low-normality latent domain to a high-normality Gaussian domain, preserving
information and ensuring structural consistency to generate representative
latents for the distilled dataset. Furthermore, we propose an efficient
sampling scheme to better align the representative latents with the
high-normality Gaussian distribution. Our comprehensive experiments demonstrate
that D^3HR can achieve higher accuracy across different model architectures
compared with state-of-the-art baselines in dataset distillation. Source code:
https://github.com/lin-zhao-resoLve/D3HR.

</details>


### [670] [TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP](https://arxiv.org/abs/2505.18434)
*Yuliang Cai,Jesse Thomason,Mohammad Rostami*

Main category: cs.CV

TL;DR: The paper presents TNG-CLIP, a method enhancing CLIP's negation understanding with minimal training time increase and introduces Neg-TtoI benchmark for evaluating text-to-image generation models.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP struggle with negation understanding, which is typically addressed through compute-intensive methods restricted to image-text matching tasks.

Method: A training-time negation data generation pipeline is introduced to create negation captions during training with slight time increase. Also, the Neg-TtoI benchmark is proposed for evaluating text-to-image generation models on negation prompts.

Result: TNG-CLIP achieves state-of-the-art performance on various negation benchmarks including image-to-text matching, text-to-image retrieval, and image generation.

Conclusion: TNG-CLIP effectively improves negation understanding in vision-language models with efficient training adjustments and provides a new benchmark for comprehensive evaluation.

Abstract: Vision-language models (VLMs), such as CLIP, have demonstrated strong
performance across a range of downstream tasks. However, CLIP is still limited
in negation understanding: the ability to recognize the absence or exclusion of
a concept. Existing methods address the problem by using a large language model
(LLM) to generate large-scale data of image captions containing negation for
further fine-tuning CLIP. However, these methods are both time- and
compute-intensive, and their evaluations are typically restricted to image-text
matching tasks. To expand the horizon, we (1) introduce a training-time
negation data generation pipeline such that negation captions are generated
during the training stage, which only increases 2.5% extra training time, and
(2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image
generation models on prompts containing negation, assessing model's ability to
produce semantically accurate images. We show that our proposed method,
TNG-CLIP, achieves SOTA performance on diverse negation benchmarks of
image-to-text matching, text-to-image retrieval, and image generation.

</details>


### [671] [Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling](https://arxiv.org/abs/2505.18446)
*Hojun Son,Asma Almutairi,Arpan Kusari*

Main category: cs.CV

TL;DR: The paper explores context bias in object detection, attributing it to the pooling operation and proposing Mask Pooling as a solution.


<details>
  <summary>Details</summary>
Motivation: To understand why context bias occurs during object detection training and how to remove it, especially when applying trained models to unseen domains (domain adaptation for object detection - DAOD).

Method: Provide a causal view of context bias, pointing at pooling operations in convolutional networks. Propose an alternative called Mask Pooling which uses foreground masks to separate pooling processes in foreground and background regions. Also, create a benchmark with random backgrounds to test model robustness.

Result: Mask Pooling leads to more robust object detection under different domains. The proposed benchmark successfully analyzes the robustness of trained models.

Conclusion: This work offers a principled approach to minimize context bias under domain shift through the introduction of Mask Pooling and a new testing benchmark.

Abstract: Context bias refers to the association between the foreground objects and
background during the object detection training process. Various methods have
been proposed to minimize the context bias when applying the trained model to
an unseen domain, known as domain adaptation for object detection (DAOD). But a
principled approach to understand why the context bias occurs and how to remove
it has been missing.
  In this work, we provide a causal view of the context bias, pointing towards
the pooling operation in the convolution network architecture as the possible
source of this bias. We present an alternative, Mask Pooling, which uses an
additional input of foreground masks, to separate the pooling process in the
respective foreground and background regions and show that this process leads
the trained model to detect objects in a more robust manner under different
domains. We also provide a benchmark designed to create an ultimate test for
DAOD, using foregrounds in the presence of absolute random backgrounds, to
analyze the robustness of the intended trained models. Through these
experiments, we hope to provide a principled approach for minimizing context
bias under domain shift.

</details>


### [672] [On Denoising Walking Videos for Gait Recognition](https://arxiv.org/abs/2505.18582)
*Dongyang Jin,Chao Fan,Jingzhe Ma,Jingkai Zhou,Weihua Chen,Shiqi Yu*

Main category: cs.CV

TL;DR: DenoisingGait is a new method for gait recognition that uses generative diffusion models and a geometry-driven Feature Matching module to filter out irrelevant factors and create a novel flow-like gait representation called Gait Feature Field. It achieves state-of-the-art performance on several datasets.


<details>
  <summary>Details</summary>
Motivation: Vision-based gait recognition faces the challenge of capturing individual gait patterns while excluding identity-irrelevant cues such as clothing texture and color. Traditional methods are not highly accurate due to sparse and less informative inputs, prompting the need for more effective approaches.

Method: The proposed method, DenoisingGait, employs generative diffusion models to denoise RGB videos and uses a geometry-driven Feature Matching module to condense multi-channel diffusion features into a two-channel direction vector. Within- and cross-frame matching capture local structures of gait appearance and motion, creating the Gait Feature Field representation.

Result: Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets show that DenoisingGait achieves state-of-the-art performance in most cases for both within- and cross-domain evaluations.

Conclusion: DenoisingGait effectively filters out irrelevant factors in gait recognition through the use of generative diffusion models and a novel flow-like gait representation, setting a new standard in the field.

Abstract: To capture individual gait patterns, excluding identity-irrelevant cues in
walking videos, such as clothing texture and color, remains a persistent
challenge for vision-based gait recognition. Traditional silhouette- and
pose-based methods, though theoretically effective at removing such
distractions, often fall short of high accuracy due to their sparse and less
informative inputs. Emerging end-to-end methods address this by directly
denoising RGB videos using human priors. Building on this trend, we propose
DenoisingGait, a novel gait denoising method. Inspired by the philosophy that
"what I cannot create, I do not understand", we turn to generative diffusion
models, uncovering how they partially filter out irrelevant factors for gait
understanding. Additionally, we introduce a geometry-driven Feature Matching
module, which, combined with background removal via human silhouettes,
condenses the multi-channel diffusion features at each foreground pixel into a
two-channel direction vector. Specifically, the proposed within- and
cross-frame matching respectively capture the local vectorized structures of
gait appearance and motion, producing a novel flow-like gait representation
termed Gait Feature Field, which further reduces residual noise in diffusion
features. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate
that DenoisingGait achieves a new SoTA performance in most cases for both
within- and cross-domain evaluations. Code is available at
https://github.com/ShiqiYu/OpenGait.

</details>


### [673] [HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection](https://arxiv.org/abs/2505.18587)
*Pavan C Shekar,Pawan Soni,Vivek Kanhangad*

Main category: cs.CV

TL;DR: This paper presents HyperFake, a new deepfake detection method that reconstructs hyperspectral data from RGB videos to reveal hidden manipulation traces, using an improved MST++ architecture, spectral attention mechanism, and EfficientNet-based classifier for more accurate and generalizable detection without requiring hyperspectral cameras.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle to generalize across different manipulation techniques and datasets due to the limitations of RGB data. The authors aim to overcome these limitations by leveraging hyperspectral imaging, which can reveal hidden manipulation traces invisible to conventional RGB-based methods.

Method: HyperFake reconstructs 31-channel hyperspectral data from standard RGB videos through an improved MST++ architecture. A spectral attention mechanism selects the most critical spectral features for deepfake detection. Then, an EfficientNet-based classifier optimized for spectral analysis processes the refined spectral data to enable accurate and generalizable detection across different deepfake styles and datasets.

Result: HyperFake demonstrates more accurate and generalizable deepfake detection compared to existing methods that rely solely on RGB data. It achieves this enhanced performance without requiring expensive hyperspectral cameras, making it a practical solution for detecting increasingly sophisticated manipulations.

Conclusion: HyperFake represents a significant advancement in deepfake detection by introducing hyperspectral imaging reconstruction as a novel approach. This method reveals hidden manipulation traces beyond the capabilities of conventional RGB-based techniques, providing more accurate and generalizable detection across diverse deepfake styles and datasets.

Abstract: Deepfakes pose a significant threat to digital media security, with current
detection methods struggling to generalize across different manipulation
techniques and datasets. While recent approaches combine CNN-based
architectures with Vision Transformers or leverage multi-modal learning, they
remain limited by the inherent constraints of RGB data. We introduce HyperFake,
a novel deepfake detection pipeline that reconstructs 31-channel hyperspectral
data from standard RGB videos, revealing hidden manipulation traces invisible
to conventional methods. Using an improved MST++ architecture, HyperFake
enhances hyperspectral reconstruction, while a spectral attention mechanism
selects the most critical spectral features for deepfake detection. The refined
spectral data is then processed by an EfficientNet-based classifier optimized
for spectral analysis, enabling more accurate and generalizable detection
across different deepfake styles and datasets, all without the need for
expensive hyperspectral cameras. To the best of our knowledge, this is the
first approach to leverage hyperspectral imaging reconstruction for deepfake
detection, opening new possibilities for detecting increasingly sophisticated
manipulations.

</details>


### [674] [Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment](https://arxiv.org/abs/2505.18600)
*Bryan Sangwoo Kim,Jeongsol Kim,Jong Chul Ye*

Main category: cs.CV

TL;DR: Modern SISR models fail at extreme magnifications. CoZ, a new framework, uses repeated applications of a base SR model and multi-scale-aware text prompts to achieve high-quality super-resolution beyond 256x without additional training.


<details>
  <summary>Details</summary>
Motivation: Current SISR models perform well at their trained scale factors but fail when required to magnify images far beyond those scales.

Method: CoZ factorizes SISR into an autoregressive chain of intermediate scale-states. It re-uses a backbone SR model and uses multi-scale-aware text prompts generated by a VLM, which is fine-tuned using GRPO with a critic VLM.

Result: Experiments demonstrate that a standard 4x diffusion SR model, when used with CoZ, can achieve over 256x enlargement with high perceptual quality and fidelity.

Conclusion: CoZ offers a scalable solution for extreme image magnification by leveraging existing SR models and enhancing them with vision-language model guidance.

Abstract: Modern single-image super-resolution (SISR) models deliver photo-realistic
results at the scale factors on which they are trained, but collapse when asked
to magnify far beyond that regime. We address this scalability bottleneck with
Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an
autoregressive chain of intermediate scale-states with multi-scale-aware
prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the
conditional probability into tractable sub-problems to achieve extreme
resolutions without additional training. Because visual cues diminish at high
magnifications, we augment each zoom step with multi-scale-aware text prompts
generated by a vision-language model (VLM). The prompt extractor itself is
fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic
VLM, aligning text guidance towards human preference. Experiments show that a
standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement
with high perceptual quality and fidelity.

</details>


### [675] [Rethinking Causal Mask Attention for Vision-Language Inference](https://arxiv.org/abs/2505.18605)
*Xiaohuan Pei,Tao Huang,YanXiang Ma,Chang Xu*

Main category: cs.CV

TL;DR: 在自回归视觉-语言模型中，因果注意机制是一个基础组件。然而，现有的基于因果掩码的策略从大型语言模型继承而来，对视觉标记的适应性不足。本文研究了不同的因果掩码策略如何影响视觉-语言推理，并提出了一组未来感知注意力方法。通过池化技术将未来的视觉上下文聚合到过去的表示中，从而在保持自回归结构的同时增强了跨标记依赖性。实验表明，选择性地压缩未来的语义上下文到过去的表示中可以提高推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的因果掩码策略主要针对文本解码设计，未充分考虑视觉标记的需求。严格遮蔽未来位置对视觉查询引入了过于刚性的约束，限制了模型利用未来上下文中重要语义线索的能力。

Method: 1. 实证分析不同因果掩码策略对视觉-语言推理的影响。
2. 提出一种轻量级的注意力家族，通过池化技术将未来视觉上下文整合到过去的表示中。
3. 在多种视觉-语言推理场景下评估一系列因果掩码的效果。

Result: 研究表明，严格掩码削弱了模型捕捉有用上下文语义表示的能力。而通过池化技术将未来视觉上下文整合到过去的表示中，可以在保持自回归结构的同时增强跨标记依赖性，进而提升推理性能。

Conclusion: 未来感知注意力机制能够有效改善视觉-语言推理任务的表现，为这一领域的研究提供了新的方向。

Abstract: Causal attention has become a foundational mechanism in autoregressive
vision-language models (VLMs), unifying textual and visual inputs under a
single generative framework. However, existing causal mask-based strategies are
inherited from large language models (LLMs) where they are tailored for
text-only decoding, and their adaptation to vision tokens is insufficiently
addressed in the prefill stage. Strictly masking future positions for vision
queries introduces overly rigid constraints, which hinder the model's ability
to leverage future context that often contains essential semantic cues for
accurate inference. In this work, we empirically investigate how different
causal masking strategies affect vision-language inference and then propose a
family of future-aware attentions tailored for this setting. We first
empirically analyze the effect of previewing future tokens for vision queries
and demonstrate that rigid masking undermines the model's capacity to capture
useful contextual semantic representations. Based on these findings, we propose
a lightweight attention family that aggregates future visual context into past
representations via pooling, effectively preserving the autoregressive
structure while enhancing cross-token dependencies. We evaluate a range of
causal masks across diverse vision-language inference settings and show that
selectively compressing future semantic context into past representations
benefits the inference.

</details>


### [676] [Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model](https://arxiv.org/abs/2505.18674)
*Peng Xiao,Hongbo Zhao,Yijun Wang,Jianxin Lin*

Main category: cs.CV

TL;DR: This paper proposes an internal detail-preserving diffusion model for restoring real-world degraded images, utilizing a pre-trained Stable Diffusion model and Internal Image Detail Enhancement (IIDE) technique to preserve image details while mitigating degradation effects. Experiments show superior performance compared to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Restoring real-world degraded images with high fidelity and providing object-level control over colorization is challenging. Current data-driven approaches struggle with preserving image details during restoration.

Method: The method uses a pre-trained Stable Diffusion model as a generative prior and introduces the Internal Image Detail Enhancement (IIDE) technique. This technique preserves structural and textural information while mitigating degradation effects by simulating various degradation factors in the latent space.

Result: Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art models in both qualitative assessments and perceptual quantitative evaluations. It also supports text-guided restoration for object-level colorization control.

Conclusion: The proposed internal detail-preserving diffusion model effectively restores real-world degraded images with high fidelity and provides object-level colorization control, achieving superior results compared to existing methods.

Abstract: Restoring real-world degraded images, such as old photographs or
low-resolution images, presents a significant challenge due to the complex,
mixed degradations they exhibit, such as scratches, color fading, and noise.
Recent data-driven approaches have struggled with two main challenges:
achieving high-fidelity restoration and providing object-level control over
colorization. While diffusion models have shown promise in generating
high-quality images with specific controls, they often fail to fully preserve
image details during restoration. In this work, we propose an internal
detail-preserving diffusion model for high-fidelity restoration of real-world
degraded images. Our method utilizes a pre-trained Stable Diffusion model as a
generative prior, eliminating the need to train a model from scratch. Central
to our approach is the Internal Image Detail Enhancement (IIDE) technique,
which directs the diffusion model to preserve essential structural and textural
information while mitigating degradation effects. The process starts by mapping
the input image into a latent space, where we inject the diffusion denoising
process with degradation operations that simulate the effects of various
degradation factors. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art models in both qualitative
assessments and perceptual quantitative evaluations. Additionally, our approach
supports text-guided restoration, enabling object-level colorization control
that mimics the expertise of professional photo editing.

</details>


### [677] [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
*Sicheng Feng,Song Wang,Shuyi Ouyang,Lingdong Kong,Zikai Song,Jianke Zhu,Huan Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: Multimodal large language models' fine-grained visual understanding and spatial reasoning abilities are evaluated insufficiently. The study introduces ReasonMap, a benchmark with high-resolution transit maps and question-answer pairs, revealing that base models outperform reasoning models in open-source while the opposite trend is observed in closed-source models.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capacity for reasoning tasks involving fine-grained visual understanding of multimodal large language models which has not been sufficiently explored.

Method: Introduced ReasonMap, a benchmark including high-resolution transit maps and question-answer pairs, and designed a two-level evaluation pipeline to assess answer correctness and quality.

Result: Base models outperform reasoning models among open-source models while the opposite trend is observed in closed-source models. Performance degrades when visual inputs are masked.

Conclusion: The benchmark study provides new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
progress in visual tasks, including semantic scene understanding and text-image
alignment, with reasoning variants enhancing performance on complex tasks
involving mathematics and logic. However, their capacity for reasoning tasks
involving fine-grained visual understanding remains insufficiently evaluated.
To address this gap, we introduce ReasonMap, a benchmark designed to assess the
fine-grained visual understanding and spatial reasoning abilities of MLLMs.
ReasonMap encompasses high-resolution transit maps from 30 cities across 13
countries and includes 1,008 question-answer pairs spanning two question types
and three templates. Furthermore, we design a two-level evaluation pipeline
that properly assesses answer correctness and quality. Comprehensive
evaluations of 15 popular MLLMs, including both base and reasoning variants,
reveal a counterintuitive pattern: among open-source models, base models
outperform reasoning ones, while the opposite trend is observed in
closed-source models. Additionally, performance generally degrades when visual
inputs are masked, indicating that while MLLMs can leverage prior knowledge to
answer some questions, fine-grained visual reasoning tasks still require
genuine visual perception for strong performance. Our benchmark study offers
new insights into visual reasoning and contributes to investigating the gap
between open-source and closed-source models.

</details>


### [678] [GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains](https://arxiv.org/abs/2505.18700)
*Chun Wang,Xiaoran Pan,Zihao Pan,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: Recent advances in Visual Language Models (VLMs) have shown great performance in visual reasoning tasks. However, geo-localization presents unique challenges. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference.


<details>
  <summary>Details</summary>
Motivation: Geo-localization requires the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches often lack robust reasoning mechanisms and explainability.

Method: The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, introduce GRE30K, a high-quality geo-localization reasoning dataset. Next, present the GRE model which employs a multi-stage reasoning strategy. Finally, construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework.

Result: Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks.

Conclusion: The efficacy of reasoning-augmented VLMs in complex geographic inference is underscored.

Abstract: Recent advances in Visual Language Models (VLMs) have demonstrated
exceptional performance in visual reasoning tasks. However, geo-localization
presents unique challenges, requiring the extraction of multigranular visual
cues from images and their integration with external world knowledge for
systematic reasoning. Current approaches to geo-localization tasks often lack
robust reasoning mechanisms and explainability, limiting their effectiveness.
To address these limitations, we propose the Geo Reason Enhancement (GRE)
Suite, a novel framework that augments VLMs with structured reasoning chains
for accurate and interpretable location inference. The GRE Suite is
systematically developed across three key dimensions: dataset, model, and
benchmark. First, we introduce GRE30K, a high-quality geo-localization
reasoning dataset designed to facilitate fine-grained visual and contextual
analysis. Next, we present the GRE model, which employs a multi-stage reasoning
strategy to progressively infer scene attributes, local details, and semantic
features, thereby narrowing down potential geographic regions with enhanced
precision. Finally, we construct the Geo Reason Evaluation Benchmark
(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across
diverse urban, natural, and landmark scenes to measure both coarse-grained
(e.g., country, continent) and fine-grained (e.g., city, street) localization
performance. Experimental results demonstrate that GRE significantly
outperforms existing methods across all granularities of geo-localization
tasks, underscoring the efficacy of reasoning-augmented VLMs in complex
geographic inference. Code and data will be released at
https://github.com/Thorin215/GRE.

</details>


### [679] [MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images](https://arxiv.org/abs/2505.18741)
*Han Li,Hu Han,S. Kevin Zhou*

Main category: cs.CV

TL;DR: In noisy-labeled and long-tailed image classification, as well as universal lesion detection in medical images, there is a problem of effectively leveraging training images with diverse qualities. Traditional methods like self-paced curriculum learning (SCL) and online hard example mining (OHEM) face challenges such as imprecise sample hardness measures and under/over-utilization issues. This paper revisits minibatch sampling (MBS), proposing a Mixed-order Minibatch Sampling (MoMBS) method to optimize the use of diverse-quality training samples.


<details>
  <summary>Details</summary>
Motivation: To better handle the challenge of using training images with diverse qualities in noisy-labeled, long-tailed image classification, and universal lesion detection in medical images, which conventional methods fail to address optimally due to imprecise measures of sample hardness and issues of under/over-utilization.

Method: The paper proposes Mixed-order Minibatch Sampling (MoMBS). It introduces a measure that considers both loss and uncertainty, surpassing sole reliance on loss. High-loss samples are categorized more finely by distinguishing them based on label quality and representation. Under-represented samples are prioritized as main gradient contributors in a minibatch while shielding them from negative influences of poorly labeled or overfitted samples.

Result: Through the introduction of MoMBS, the method allows for more refined categorization of high-loss samples and mitigates the negative influence of poorly labeled or overfitted samples, leading to improved performance in handling diverse-quality training samples.

Conclusion: Revisiting minibatch sampling and introducing MoMBS addresses the limitations of traditional methods in dealing with diverse-quality training samples. The proposed method enhances the utilization of training data and shows promise in various classification tasks.

Abstract: Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled
image classification and prevalence diversity (abundant vs. sparse) in
long-tailed image classification. Similarly, medical images in universal lesion
detection (ULD) exhibit substantial variations in image quality, encompassing
attributes such as clarity and label correctness. How to effectively leverage
training images with diverse qualities becomes a problem in learning deep
models. Conventional training mechanisms, such as self-paced curriculum
learning (SCL) and online hard example mining (OHEM), relieve this problem by
reweighting images with high loss values. Despite their success, these methods
still confront two challenges: (i) the loss-based measure of sample hardness is
imprecise, preventing optimum handling of different cases, and (ii) there
exists under-utilization in SCL or over-utilization OHEM with the identified
hard samples. To address these issues, this paper revisits the minibatch
sampling (MBS), a technique widely used in deep network training but largely
unexplored concerning the handling of diverse-quality training samples. We
discover that the samples within a minibatch influence each other during
training; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS)
method to optimize the use of training samples with diverse qualities. MoMBS
introduces a measure that takes both loss and uncertainty into account to
surpass a sole reliance on loss and allows for a more refined categorization of
high-loss samples by distinguishing them as either poorly labeled and under
represented or well represented and overfitted. We prioritize under represented
samples as the main gradient contributors in a minibatch and keep them from the
negative influences of poorly labeled or overfitted samples with a mixed-order
minibatch sampling design.

</details>


### [680] [StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations](https://arxiv.org/abs/2505.18766)
*Yanjie Li,Wenxuan Zhang,Xinqi Lyu,Yihao Liu,Bin Xiao*

Main category: cs.CV

TL;DR: To tackle the issues of intellectual property protection and generation of deceptive content in text-to-image diffusion models, a novel anti-mimicry method called StyleGuard is proposed. It introduces a new style loss to optimize style-related features in the latent space and an upscale loss involving ensemble purifiers and upscalers during training.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from the concerns about intellectual property protection and the generation of deceptive content using text-to-image diffusion models like DreamBooth and Textual Inversion. Recent defense mechanisms using adversarial noise have been countered by purification-based methods, highlighting vulnerabilities and limited transferability across models.

Method: The paper proposes StyleGuard, which includes a novel style loss that optimizes style-related features in the latent space to improve model-agnostic transferability. Additionally, it introduces an upscale loss that involves ensemble purifiers and upscalers during training to enhance the perturbation's ability to bypass diffusion-based purification.

Result: Extensive experiments on the WikiArt and CelebA datasets show that StyleGuard outperforms existing methods in robustness against various transformations and purifications. It effectively counters style mimicry across different models and methods, including DreamBooth and Textual Inversion.

Conclusion: StyleGuard presents a significant advancement in defending against style mimicry in text-to-image models. Its improved model-agnostic transferability and effectiveness against purification techniques make it a robust solution for protecting images from adversarial attacks.

Abstract: Recently, text-to-image diffusion models have been widely used for style
mimicry and personalized customization through methods such as DreamBooth and
Textual Inversion. This has raised concerns about intellectual property
protection and the generation of deceptive content. Recent studies, such as
Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect
images from these attacks. However, recent purification-based methods, such as
DiffPure and Noise Upscaling, have successfully attacked these latest defenses,
showing the vulnerabilities of these methods. Moreover, present methods show
limited transferability across models, making them less effective against
unknown text-to-image models. To address these issues, we propose a novel
anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes
the style-related features in the latent space to make it deviate from the
original image, which improves model-agnostic transferability. Additionally, to
enhance the perturbation's ability to bypass diffusion-based purification, we
designed a novel upscale loss that involves ensemble purifiers and upscalers
during training. Extensive experiments on the WikiArt and CelebA datasets
demonstrate that StyleGuard outperforms existing methods in robustness against
various transformations and purifications, effectively countering style mimicry
in various models. Moreover, StyleGuard is effective on different style mimicry
methods, including DreamBooth and Textual Inversion.

</details>


### [681] [OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks](https://arxiv.org/abs/2505.18775)
*Jiayu Wang,Yang Jiao,Yue Yu,Tianwen Qian,Shaoxiang Chen,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: Recent breakthroughs in large multimodal models (LMMs) have shown proficiency in following instructions for image generation. However, current benchmarks lack the necessary breadth and depth to fully evaluate these models. To address this, we introduce OmniGenBench, a comprehensive benchmark designed to assess the instruction-following abilities of LMMs across both perception-centric and cognition-centric dimensions.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks often lack the necessary breadth and depth to fully evaluate the diverse capabilities of state-of-the-art LMMs.

Method: OmniGenBench includes 57 diverse sub-tasks grounded in real-world scenarios, systematically categorized according to the specific model capabilities they demand. A dual-mode protocol is employed for rigorous evaluation, using off-the-shelf visual parsing tools for perception-centric tasks and a powerful LLM-based judger for cognition-centric tasks.

Result: Using OmniGenBench, mainstream generative models were evaluated, including GPT-4o, Gemini-2.0-Flash, and Seedream, with in-depth comparisons and analyses of their performance provided.

Conclusion: OmniGenBench is introduced as a novel and comprehensive benchmark to assess the instruction-following abilities of state-of-the-art LMMs across both perception-centric and cognition-centric dimensions.

Abstract: Recent breakthroughs in large multimodal models (LMMs), such as the
impressive GPT-4o-Native, have demonstrated remarkable proficiency in following
general-purpose instructions for image generation. However, current benchmarks
often lack the necessary breadth and depth to fully evaluate the diverse
capabilities of these models. To overcome this limitation, we introduce
OmniGenBench, a novel and comprehensive benchmark meticulously designed to
assess the instruction-following abilities of state-of-the-art LMMs across both
perception-centric and cognition-centric dimensions. Our OmniGenBench includes
57 diverse sub-tasks grounded in real-world scenarios, systematically
categorized according to the specific model capabilities they demand. For
rigorous evaluation, we further employ a dual-mode protocol. This protocol
utilizes off-the-shelf visual parsing tools for perception-centric tasks and a
powerful LLM-based judger for cognition-centric tasks to assess the alignment
between generated images and user instructions. Using OmniGenBench, we evaluate
mainstream generative models, including prevalent models like GPT-4o,
Gemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses
of their performance.Code and data are available at
https://github.com/emilia113/OmniGenBench.

</details>


### [682] [REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing](https://arxiv.org/abs/2505.18880)
*Weihan Xu,Yimeng Ma,Jingyue Huang,Yang Li,Wenye Ma,Taylor Berg-Kirkpatrick,Julian McAuley,Paul Pu Liang,Hao-Wen Dong*

Main category: cs.CV

TL;DR: 本文提出了一种新的检索嵌入生成框架REGen系统，用于生成具有连贯叙述和嵌入视频片段的短视频摘要。该方法在纪录片预告片生成任务中表现出色，能够有效插入短片段并保持叙述的连贯性。主观和客观评估均表明，该方法在连贯性、对齐性和真实性方面优于现有的抽象和提取方法。


<details>
  <summary>Details</summary>
Motivation: 现有提取式视频摘要方法难以生成连贯的叙述，而抽象式方法无法从输入视频中'引用'内容，即无法在输出中插入短片段。为了解决这些问题，本文探索了新的视频编辑模型。

Method: 提出了一种新的检索嵌入生成框架REGen系统。该系统首先使用微调的大语言模型生成带有引用占位符的输出故事脚本，然后通过新颖的检索模型将占位符替换为从候选引用片段池中选出的最佳支持叙述的视频片段。

Result: 客观评估显示，该方法可以有效地插入短视频片段，同时保持叙述的连贯性。主观调查表明，在预告片生成任务中，该方法在连贯性、对齐性和真实性方面优于现有的抽象和提取方法。

Conclusion: 所提出的REGen系统提供了一种有效的解决方案，可以在生成短视频摘要时结合连贯叙述和引用片段的优点，适用于如纪录片预告片生成等任务。

Abstract: Short videos are an effective tool for promoting contents and improving
knowledge accessibility. While existing extractive video summarization methods
struggle to produce a coherent narrative, existing abstractive methods cannot
`quote' from the input videos, i.e., inserting short video clips in their
outputs. In this work, we explore novel video editing models for generating
shorts that feature a coherent narrative with embedded video insertions
extracted from a long input video. We propose a novel retrieval-embedded
generation framework that allows a large language model to quote multimodal
resources while maintaining a coherent narrative. Our proposed REGen system
first generates the output story script with quote placeholders using a
finetuned large language model, and then uses a novel retrieval model to
replace the quote placeholders by selecting a video clip that best supports the
narrative from a pool of candidate quotable video clips. We examine the
proposed method on the task of documentary teaser generation, where short
interview insertions are commonly used to support the narrative of a
documentary. Our objective evaluations show that the proposed method can
effectively insert short video clips while maintaining a coherent narrative. In
a subjective survey, we show that our proposed method outperforms existing
abstractive and extractive approaches in terms of coherence, alignment, and
realism in teaser generation.

</details>


### [683] [SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes](https://arxiv.org/abs/2505.18881)
*Dicong Qiu,Jiadi You,Zeying Gong,Ronghe Qiu,Hui Xiong,Junwei Liang*

Main category: cs.CV

TL;DR: The paper introduces SD-OVON, a pipeline for generating datasets and benchmarks for open-vocabulary object navigation in dynamic scenes. It uses pretraining multimodal foundation models to create realistic scene variants and task episodes compatible with the Habitat simulator. Two pre-generated datasets, SD-OVON-3k and SD-OVON-10k, are offered. Unlike previous datasets limited to static environments, SD-OVON covers dynamic scenes and manipulatable objects. The authors propose two baselines and evaluate them on SD-OVON-3k.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of prior datasets that are restricted to static environments, this work aims to enhance the realism of navigation tasks by covering dynamic scenes and manipulatable objects for both real-to-sim and sim-to-real robotic applications.

Method: The method involves using pretraining multimodal foundation models to generate infinite unique photo-realistic scene variants adhering to real-world semantics and daily commonsense. A plugin is provided for generating object navigation task episodes compatible with the Habitat simulator. Two pre-generated datasets, SD-OVON-3k and SD-OVON-10k, are created from the SD-OVON-Scenes dataset (2.5k photo-realistic scans) and the SD-OVON-Objects dataset (0.9k manually inspected object models).

Result: The effectiveness of the pipeline and datasets is demonstrated through the proposal and evaluation of two baselines alongside state-of-the-art baselines on SD-OVON-3k.

Conclusion: SD-OVON enhances the realism of navigation tasks and provides valuable resources for training and evaluating open-vocabulary object navigation agents in complex settings. The datasets, benchmark, and source code are publicly available.

Abstract: We present the Semantics-aware Dataset and Benchmark Generation Pipeline for
Open-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes
pretraining multimodal foundation models to generate infinite unique
photo-realistic scene variants that adhere to real-world semantics and daily
commonsense for the training and the evaluation of navigation agents,
accompanied with a plugin for generating object navigation task episodes
compatible to the Habitat simulator. In addition, we offer two pre-generated
object navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising
respectively about 3k and 10k episodes of the open-vocabulary object navigation
task, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans
of real-world environments and the SD-OVON-Objects dataset with 0.9k manually
inspected scanned and artist-created manipulatable object models. Unlike prior
datasets limited to static environments, SD-OVON covers dynamic scenes and
manipulatable objects, facilitating both real-to-sim and sim-to-real robotic
applications. This approach enhances the realism of navigation tasks, the
training and the evaluation of open-vocabulary object navigation agents in
complex settings. To demonstrate the effectiveness of our pipeline and
datasets, we propose two baselines and evaluate them along with
state-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source
code are publicly available.

</details>


### [684] [WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification](https://arxiv.org/abs/2505.18930)
*Yanben Shen,Timilehin T. Ayanlade,Venkata Naresh Boddepalli,Mojdeh Saadati,Ashlyn Rairdin,Zi K. Deng,Muhammad Arbab Arshad,Aditya Balu,Daren Mueller,Asheesh K Singh,Wesley Everman,Nirav Merchant,Baskar Ganapathysubramanian,Meaghan Anderson,Soumik Sarkar,Arti Singh*

Main category: cs.CV

TL;DR: An abstract about a new model named WeedNet which is an end-to-end real-time weed identification pipeline using self-supervised learning, fine-tuning, and enhanced trustworthiness strategies. It achieved 91.02% accuracy across 1,593 weed species and 97.38% for 85 Iowa weeds.


<details>
  <summary>Details</summary>
Motivation: Early identification of weeds is essential but training AI-based weed identification models faces challenges such as limited expert-verified data and complexity in morphological features.

Method: WeedNet uses self-supervised learning, fine-tuning, and enhanced trustworthiness strategies. A Global-to-Local approach was also used for region-specific weed communities.

Result: WeedNet achieved 91.02% accuracy across 1,593 weed species with 41% species achieving 100% accuracy. The local Iowa WeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds.

Conclusion: The generalizability and adaptability of the Global WeedNet model enable it to function as a foundational model. It has potential for integration into robotic platforms and provides consulting tools for various professionals.

Abstract: Early identification of weeds is essential for effective management and
control, and there is growing interest in automating the process using computer
vision techniques coupled with AI methods. However, challenges associated with
training AI-based weed identification models, such as limited expert-verified
data and complexity and variability in morphological features, have hindered
progress. To address these issues, we present WeedNet, the first global-scale
weed identification model capable of recognizing an extensive set of weed
species, including noxious and invasive plant species. WeedNet is an end-to-end
real-time weed identification pipeline and uses self-supervised learning,
fine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%
accuracy across 1,593 weed species, with 41% species achieving 100% accuracy.
Using a fine-tuning strategy and a Global-to-Local approach, the local Iowa
WeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most
classes exceeded a 90% mean accuracy per class. Testing across intra-species
dissimilarity (developmental stages) and inter-species similarity (look-alike
species) suggests that diversity in the images collected, spanning all the
growth stages and distinguishable plant characteristics, is crucial in driving
model performance. The generalizability and adaptability of the Global WeedNet
model enable it to function as a foundational model, with the Global-to-Local
strategy allowing fine-tuning for region-specific weed communities. Additional
validation of drone- and ground-rover-based images highlights the potential of
WeedNet for integration into robotic platforms. Furthermore, integration with
AI for conversational use provides intelligent agricultural and ecological
conservation consulting tools for farmers, agronomists, researchers, land
managers, and government agencies across diverse landscapes.

</details>


### [685] [How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation](https://arxiv.org/abs/2505.18956)
*Yining Pan,Qiongjie Cui,Xulei Yang,Na Zhao*

Main category: cs.CV

TL;DR: LiDAR-based 3D panoptic segmentation faces challenges with data sparsity, making it hard to recognize distant or small objects. Recent studies integrating LiDAR and camera inputs have shown promise but still encounter issues like misalignment and reliance on post-processing steps. To address these problems, the authors propose Image-Assists-LiDAR (IAL), a new multi-modal framework that introduces PieAug for modality-synchronized data augmentation, uses a transformer decoder for direct panoptic segmentation prediction, designs a Geometric-guided Token Fusion (GTF) module for feature fusion, and leverages a Prior-based Query Generation (PQG) module for query initialization. This framework achieves state-of-the-art performance on two benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to overcome the inherent sparsity of LiDAR data that causes difficulties in accurately recognizing distant or small objects. The authors also aim to improve upon current methods that integrate LiDAR and camera inputs by addressing issues such as misalignment during data augmentation and dependence on post-processing steps.

Method: The proposed method, Image-Assists-LiDAR (IAL), includes four key components: 
1. PieAug - A modality-synchronized data augmentation strategy to ensure alignment between LiDAR and image inputs.
2. Transformer Decoder - Directly predicts panoptic segmentation results.
3. Geometric-guided Token Fusion (GTF) Module - Fuses LiDAR and image features into tokens for the decoder.
4. Prior-based Query Generation (PQG) Module - Uses the complementary strengths of each modality as priors for query initialization to enhance accurate instance mask generation.

Result: The IAL framework outperforms previous multi-modal 3D panoptic segmentation methods, achieving state-of-the-art performance on two widely used benchmarks.

Conclusion: The authors conclude by presenting Image-Assists-LiDAR (IAL), a novel multi-modal 3D panoptic segmentation framework that addresses the challenges of data sparsity, misalignment, and reliance on post-processing steps. With its innovative components, including PieAug, GTF, and PQG modules, IAL demonstrates superior performance compared to existing methods on benchmark datasets.

Abstract: LiDAR-based 3D panoptic segmentation often struggles with the inherent
sparsity of data from LiDAR sensors, which makes it challenging to accurately
recognize distant or small objects. Recently, a few studies have sought to
overcome this challenge by integrating LiDAR inputs with camera images,
leveraging the rich and dense texture information provided by the latter. While
these approaches have shown promising results, they still face challenges, such
as misalignment during data augmentation and the reliance on post-processing
steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel
multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a
modality-synchronized data augmentation strategy, PieAug, to ensure alignment
between LiDAR and image inputs from the start. Next, we adopt a transformer
decoder to directly predict panoptic segmentation results. To effectively fuse
LiDAR and image features into tokens for the decoder, we design a
Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the
complementary strengths of each modality as priors for query initialization
through a Prior-based Query Generation (PQG) module, enhancing the decoder's
ability to generate accurate instance masks. Our IAL framework achieves
state-of-the-art performance compared to previous multi-modal 3D panoptic
segmentation methods on two widely used benchmarks. Code and models are
publicly available at <https://github.com/IMPL-Lab/IAL.git>.

</details>


### [686] [Rethinking Metrics and Benchmarks of Video Anomaly Detection](https://arxiv.org/abs/2505.19022)
*Zihao Liu,Xiaoyu Wu,Wenna Li,Linlin Yang*

Main category: cs.CV

TL;DR: The paper rethinks Video Anomaly Detection (VAD) evaluation protocols, identifies three limitations in current practices, and proposes three novel evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Existing advancements in VAD mainly focus on model architectures and training strategies while insufficient attention is given to evaluation metrics and benchmarks.

Method: 1) Establish averaged AUC/AP metrics over multi-round annotations; 2) Develop a Latency-aware Average Precision (LaAP) metric; 3) Introduce two hard normal benchmarks (UCF-HN, MSAD-HN).

Result: Report performance comparisons of ten state-of-the-art VAD approaches using the proposed evaluation methods.

Conclusion: Provides new perspectives for future VAD model development by addressing the limitations in current evaluation practices.

Abstract: Video Anomaly Detection (VAD), which aims to detect anomalies that deviate
from expectation, has attracted increasing attention in recent years. Existing
advancements in VAD primarily focus on model architectures and training
strategies, while devoting insufficient attention to evaluation metrics and
benchmarks. In this paper, we rethink VAD evaluation protocols through
comprehensive experimental analyses, revealing three critical limitations in
current practices: 1) existing metrics are significantly influenced by single
annotation bias; 2) current metrics fail to reward early detection of
anomalies; 3) available benchmarks lack the capability to evaluate scene
overfitting. To address these limitations, we propose three novel evaluation
methods: first, we establish averaged AUC/AP metrics over multi-round
annotations to mitigate single annotation bias; second, we develop a
Latency-aware Average Precision (LaAP) metric that rewards early and accurate
anomaly detection; and finally, we introduce two hard normal benchmarks
(UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene
overfitting. We report performance comparisons of ten state-of-the-art VAD
approaches using our proposed evaluation methods, providing novel perspectives
for future VAD model development.

</details>


### [687] [A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking](https://arxiv.org/abs/2505.19023)
*Huda Alghoraibi,Nuha Alqurashi,Sarah Alotaibi,Renad Alkhudaydi,Bdoor Aldajani,Lubna Alqurashi,Jood Batweel,Maha A. Thafar*

Main category: cs.CV

TL;DR: This paper presents ITMAINN, an AI-driven healthcare system that uses deep learning to detect Monkeypox from skin lesion images. The system includes a mobile app for users and a dashboard for health authorities.


<details>
  <summary>Details</summary>
Motivation: The global outbreak of Monkeypox has highlighted the need for scalable, accessible, and accurate diagnostic solutions.

Method: ITMAINN consists of three components: pretrained models using transfer learning on skin lesion datasets, a cross-platform smartphone application for user interaction, and a real-time monitoring dashboard for health authorities.

Result: For binary classification (Monkeypox vs non-Monkeypox), Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16 achieved 97.8% accuracy and F1-score. For multiclass classification, ResNetViT and ViT Hybrid models achieved 92% accuracy with respective F1 scores of 92.24% and 92.19%. MobileViT was deployed in the mobile application due to its lightweight nature.

Conclusion: ITMAINN is a crucial step in developing responsive healthcare infrastructure within smart cities and revolutionizing public health management.

Abstract: Monkeypox is a viral disease characterized by distinctive skin lesions and
has been reported in many countries. The recent global outbreak has emphasized
the urgent need for scalable, accessible, and accurate diagnostic solutions to
support public health responses.
  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare
system specifically designed to detect Monkeypox from skin lesion images using
advanced deep learning techniques. Our system consists of three main
components. First, we trained and evaluated several pretrained models using
transfer learning on publicly available skin lesion datasets to identify the
most effective models. For binary classification (Monkeypox vs. non-Monkeypox),
the Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16
achieved the highest performance, each with an accuracy and F1-score of 97.8%.
For multiclass classification, which contains images of patients with Monkeypox
and five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,
and healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1
scores of 92.24% and 92.19%, respectively. The best-performing and most
lightweight model, MobileViT, was deployed within the mobile application. The
second component is a cross-platform smartphone application that enables users
to detect Monkeypox through image analysis, track symptoms, and receive
recommendations for nearby healthcare centers based on their location. The
third component is a real-time monitoring dashboard designed for health
authorities to support them in tracking cases, analyzing symptom trends,
guiding public health interventions, and taking proactive measures.
  This system is fundamental in developing responsive healthcare infrastructure
within smart cities. Our solution, ITMAINN, is part of revolutionizing public
health management.

</details>


### [688] [InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts](https://arxiv.org/abs/2505.19028)
*Minzhi Lin,Tianchi Xie,Mengchen Liu,Yilin Ye,Changjian Chen,Shixia Liu*

Main category: cs.CV

TL;DR: 为了评估多模态大语言模型（MLLMs）在信息图表理解方面的能力，研究者创建了一个名为InfoChartQA的基准数据集。该数据集包含5642对信息图表和平面图表，每对图表共享相同的数据但具有不同的视觉呈现形式。通过设计与视觉元素相关的问题，研究发现MLLMs在处理隐喻相关的视觉元素问题时表现显著下降。此数据集有助于进行细致的错误分析和消融研究，为提升MLLMs的信息图表理解能力提供了新机会。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉问答基准无法充分评估多模态大语言模型（MLLMs）在理解和推理带有设计驱动视觉元素（如象形图、图标）的信息图表方面的性能，因为缺乏配对的平面图表和基于视觉元素的问题。

Method: 研究者引入了InfoChartQA这一基准，其中包括5642对信息图表和平面图表，这些图表共享相同的基础数据但在视觉展示上有所不同。同时，还设计了基于视觉元素的问题以捕捉独特的视觉设计和传达意图。

Result: 对20个MLLMs的评估显示，在信息图表上的表现有显著下降，特别是在与隐喻相关的视觉元素问题上。配对的信息图表和平面图表能够实现细致的错误分析和消融研究。

Conclusion: InfoChartQA的发布为改进MLLMs在信息图表理解方面的能力提供了新的机遇和方向。

Abstract: Understanding infographic charts with design-driven visual elements (e.g.,
pictograms, icons) requires both visual recognition and reasoning, posing
challenges for multimodal large language models (MLLMs). However, existing
visual-question answering benchmarks fall short in evaluating these
capabilities of MLLMs due to the lack of paired plain charts and
visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a
benchmark for evaluating MLLMs on infographic chart understanding. It includes
5,642 pairs of infographic and plain charts, each sharing the same underlying
data but differing in visual presentations. We further design
visual-element-based questions to capture their unique visual designs and
communicative intent. Evaluation of 20 MLLMs reveals a substantial performance
decline on infographic charts, particularly for visual-element-based questions
related to metaphors. The paired infographic and plain charts enable
fine-grained error analysis and ablation studies, which highlight new
opportunities for advancing MLLMs in infographic chart understanding. We
release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.

</details>


### [689] [Medical Large Vision Language Models with Multi-Image Visual Ability](https://arxiv.org/abs/2505.19031)
*Xikai Yang,Juzheng Miao,Yuchen Yuan,Jiaze Wang,Qi Dou,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: Medical large vision-language models (LVLMs) lack proficiency in multi-image clinical tasks. This paper introduces Med-MIM, an instruction dataset with 83.2K QA pairs for four multi-image visual abilities. By fine-tuning Mantis and LLaVA-Med, the authors created two specialized medical VLMs: MIM-LLaVA-Med and Med-Mantis. They also developed the Med-MIM benchmark to evaluate multi-image understanding capabilities of LVLMs. Experiments show that Med-Mantis and MIM-LLaVA-Med outperform other models on the Med-MIM benchmark.


<details>
  <summary>Details</summary>
Motivation: Current medical LVLMs perform well in single-image QA but are limited in handling multi-image clinical scenarios which require advanced visual understanding like temporal reasoning and cross-modal analysis.

Method: The authors constructed the Med-MIM instruction dataset containing 83.2K medical multi-image QA pairs covering four types of multi-image visual abilities. They fine-tuned Mantis and LLaVA-Med using this dataset to create two specialized models: MIM-LLaVA-Med and Med-Mantis. Additionally, they developed the Med-MIM benchmark for evaluating multi-image understanding capabilities.

Result: Experimental results indicate that both Med-Mantis and MIM-LLaVA-Med excel on the held-in and held-out subsets of the Med-MIM benchmark, proving the effectiveness of the Med-MIM instruction dataset in enhancing multi-image understanding capabilities of LVLMs in the medical field.

Conclusion: The Med-MIM instruction dataset significantly improves the multi-image understanding capabilities of LVLMs in the medical domain. The specialized models, Med-Mantis and MIM-LLaVA-Med, outperform other popular LVLMs on the Med-MIM benchmark.

Abstract: Medical large vision-language models (LVLMs) have demonstrated promising
performance across various single-image question answering (QA) benchmarks, yet
their capability in processing multi-image clinical scenarios remains
underexplored. Unlike single image based tasks, medical tasks involving
multiple images often demand sophisticated visual understanding capabilities,
such as temporal reasoning and cross-modal analysis, which are poorly supported
by current medical LVLMs. To bridge this critical gap, we present the Med-MIM
instruction dataset, comprising 83.2K medical multi-image QA pairs that span
four types of multi-image visual abilities (temporal understanding, reasoning,
comparison, co-reference). Using this dataset, we fine-tune Mantis and
LLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and
Med-Mantis, both optimized for multi-image analysis. Additionally, we develop
the Med-MIM benchmark to comprehensively evaluate the medical multi-image
understanding capabilities of LVLMs. We assess eight popular LVLMs, including
our two models, on the Med-MIM benchmark. Experimental results show that both
Med-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and
held-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM
instruction dataset effectively enhances LVLMs' multi-image understanding
capabilities in the medical domain.

</details>


### [690] [Jodi: Unification of Visual Generation and Understanding via Joint Modeling](https://arxiv.org/abs/2505.19084)
*Yifeng Xu,Zhenliang He,Meina Kan,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: The paper introduces Jodi, a unified diffusion framework for visual generation and understanding, along with the Joint-1.6M dataset.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between visual generation and understanding by creating a model that can handle both tasks jointly.

Method: Jodi is built on a linear diffusion transformer with a role switch mechanism, allowing it to perform joint generation, controllable generation, and image perception tasks.

Result: Jodi shows excellence in both generation and understanding tasks, with strong extensibility to various visual domains.

Conclusion: Jodi unifies visual generation and understanding, providing a versatile tool for multiple visual tasks, supported by the Joint-1.6M dataset.

Abstract: Visual generation and understanding are two deeply interconnected aspects of
human intelligence, yet they have been traditionally treated as separate tasks
in machine learning. In this paper, we propose Jodi, a diffusion framework that
unifies visual generation and understanding by jointly modeling the image
domain and multiple label domains. Specifically, Jodi is built upon a linear
diffusion transformer along with a role switch mechanism, which enables it to
perform three particular types of tasks: (1) joint generation, where the model
simultaneously generates images and multiple labels; (2) controllable
generation, where images are generated conditioned on any combination of
labels; and (3) image perception, where multiple labels can be predicted at
once from a given image. Furthermore, we present the Joint-1.6M dataset, which
contains 200,000 high-quality images collected from public sources, automatic
labels for 7 visual domains, and LLM-generated captions. Extensive experiments
demonstrate that Jodi excels in both generation and understanding tasks and
exhibits strong extensibility to a wider range of visual domains. Code is
available at https://github.com/VIPL-GENUN/Jodi.

</details>


### [691] [SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards](https://arxiv.org/abs/2505.19094)
*Chuming Shen,Wei Wei,Xiaoye Qu,Yu Cheng*

Main category: cs.CV

TL;DR: SATORI is a new method for VQA tasks which decomposes the task into three verifiable stages and introduces a new dataset VQA-Verify. It shows consistent performance improvements across seven VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal tasks like VQA have limitations when applying RL directly, such as diffused visual focus and unverifiable intermediate steps.

Method: SATORI decomposes VQA into global image captioning, region localization, and answer prediction, each supplying explicit reward signals. A new dataset VQA-Verify is also introduced.

Result: Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy compared to the R1-like baseline.

Conclusion: SATORI enhances focus on critical regions in images, leading to improvements in accuracy for VQA tasks.

Abstract: DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text
domain through stable reinforcement learning (RL). Recently, in the multimodal
domain, works have begun to directly apply RL to generate R1-like free-form
reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks
share an intrinsically different nature from textual tasks, which heavily rely
on the understanding of the input image to solve the problem. Therefore, such
free-form reasoning faces two critical limitations in the VQA task: (1)
Extended reasoning chains diffuse visual focus away from task-critical regions,
degrading answer accuracy. (2) Unverifiable intermediate steps amplify
policy-gradient variance and computational costs overhead. To address these
issues, in this paper, we introduce SATORI ($\textbf{S}patially$
$\textbf{A}nchored$ $\textbf{T}ask$ $\textbf{O}ptimization$ with
$\textbf{R}e\textbf{I}nforcement$ Learning), which decomposes VQA into three
verifiable stages, including global image captioning, region localization, and
answer prediction, each supplying explicit reward signals. Furthermore, we also
introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and
bounding-boxes to facilitate training. Experiments demonstrate consistent
performance improvements across seven VQA benchmarks, achieving up to $15.7\%$
improvement in accuracy in accuracy compared to the R1-like baseline. Our
analysis of the attention map confirms enhanced focus on critical regions,
which brings improvements in accuracy. Our code is available at
https://github.com/justairr/SATORI-R1.

</details>


### [692] [An Interpretable Representation Learning Approach for Diffusion Tensor Imaging](https://arxiv.org/abs/2505.19110)
*Vishwa Mohan Singh,Alberto Gaston Villagran Asiares,Luisa Sophie Schuhmacher,Kate Rendall,Simon Weißbrod,David Rügamer,Inga Körte*

Main category: cs.CV

TL;DR: Diffusion Tensor Imaging (DTI) tractography is converted into a 2D representation to improve its interpretability in deep learning models. A Beta-Total Correlation Variational Autoencoder with a Spatial Broadcast Decoder is used to learn a disentangled and interpretable latent embedding. The embedding quality is evaluated using supervised and unsupervised methods, showing better performance than 1D Group DNN baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of effectively representing and interpreting Diffusion Tensor Imaging (DTI) tractography data in deep learning models.

Method: Propose a novel 2D representation of DTI tractography encoding tract-level fractional anisotropy (FA) values into a 9x9 grayscale image. Use a Beta-Total Correlation Variational Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and interpretable latent embedding. Evaluate the embedding quality using supervised and unsupervised representation learning strategies.

Result: The proposed approach improves the F1 score in a downstream sex classification task by 15.74% compared to 1D Group DNN baselines and shows better disentanglement than the 3D representation.

Conclusion: The novel 2D representation and the use of the Beta-TCVAE with SBD provide a more effective and interpretable way to process DTI tractography data in deep learning models.

Abstract: Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the
structural connectivity of the brain, but presents challenges in effective
representation and interpretation in deep learning models. In this work, we
propose a novel 2D representation of DTI tractography that encodes tract-level
fractional anisotropy (FA) values into a 9x9 grayscale image. This
representation is processed through a Beta-Total Correlation Variational
Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and
interpretable latent embedding. We evaluate the quality of this embedding using
supervised and unsupervised representation learning strategies, including
auxiliary classification, triplet loss, and SimCLR-based contrastive learning.
Compared to the 1D Group deep neural network (DNN) baselines, our approach
improves the F1 score in a downstream sex classification task by 15.74% and
shows a better disentanglement than the 3D representation.

</details>


### [693] [Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli](https://arxiv.org/abs/2505.19178)
*Akhila Yaragoppa,Siddharth*

Main category: cs.CV

TL;DR: 本研究通过深度学习引入了一种基于显著性的新方法，用于情感预测，利用显著性区域和显著性区域数量两个关键特征，揭示了视频显著性与观众情感之间的关系，并提出了主观报告的局限性。


<details>
  <summary>Details</summary>
Motivation: 理解视频的情感影响对于内容创作、广告和人机交互（HCI）等应用至关重要，而传统的情感计算方法依赖于自我报告的情感、面部表情分析和生物传感数据，却常常忽略了视觉显著性的作用。

Method: 使用HD2S显著性模型和OpenFace面部动作单元分析，提取显著性区域和显著性区域数量两个关键特征，研究视频显著性与观众情感之间的关系。

Result: 发现多显著性区域的视频容易引发高评价、低唤醒情绪；单一主导显著性区域的视频更可能诱导低评价、高唤醒反应；自我报告的情感往往与基于面部表情的情感检测不一致。

Conclusion: 通过利用显著性驱动的见解，本工作提供了一种计算效率高且可解释的替代方法进行情感建模，对内容创作、个性化媒体体验和情感计算研究具有重要意义。

Abstract: Understanding the emotional impact of videos is crucial for applications in
content creation, advertising, and Human-Computer Interaction (HCI).
Traditional affective computing methods rely on self-reported emotions, facial
expression analysis, and biosensing data, yet they often overlook the role of
visual saliency -- the naturally attention-grabbing regions within a video. In
this study, we utilize deep learning to introduce a novel saliency-based
approach to emotion prediction by extracting two key features: saliency area
and number of salient regions. Using the HD2S saliency model and OpenFace
facial action unit analysis, we examine the relationship between video saliency
and viewer emotions. Our findings reveal three key insights: (1) Videos with
multiple salient regions tend to elicit high-valence, low-arousal emotions, (2)
Videos with a single dominant salient region are more likely to induce
low-valence, high-arousal responses, and (3) Self-reported emotions often
misalign with facial expression-based emotion detection, suggesting limitations
in subjective reporting. By leveraging saliency-driven insights, this work
provides a computationally efficient and interpretable alternative for emotion
modeling, with implications for content creation, personalized media
experiences, and affective computing research.

</details>


### [694] [Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance](https://arxiv.org/abs/2505.18342)
*Jack Goffinet,Youngjo Min,Carlo Tomasi,David E. Carlson*

Main category: cs.CV

TL;DR: 提出了一种名为Pose Splatter的新框架，通过形状雕刻和3D高斯点阵技术来建模实验室动物的完整姿态和外观，无需先验知识、逐帧优化或手动注释。实验表明，Pose Splatter能够学习到精确的3D动物几何结构，表示细微的姿态变化，并在人类评估中提供更好的低维姿态嵌入，同时可推广到未见数据。该方法消除了注释和逐帧优化的瓶颈，为大规模纵向行为分析提供了可能。


<details>
  <summary>Details</summary>
Motivation: 当前的3D姿态估计技术（如基于关键点和网格的技术）存在表示细节有限、注释需求繁重以及逐帧优化昂贵等问题，这些问题阻碍了对微妙动作的研究和大规模分析的可行性。

Method: 提出了Pose Splatter框架，利用形状雕刻和3D高斯点阵技术来建模动物的完整姿态和外观；还提出了一种新颖的旋转不变视觉嵌入技术，用于编码姿态和外观，可以替代下游行为分析中的3D关键点数据。

Result: 实验结果表明，Pose Splatter可以学习到准确的3D动物几何结构，表示细微的姿态变化，在人类评估中优于现有方法的低维姿态嵌入，并且能够推广到未见数据。

Conclusion: Pose Splatter通过消除注释和逐帧优化的瓶颈，为大规模、纵向行为分析提供了可能，有助于将基因型、神经活动和微行为映射到前所未有的分辨率。

Abstract: Accurate and scalable quantification of animal pose and appearance is crucial
for studying behavior. Current 3D pose estimation techniques, such as keypoint-
and mesh-based techniques, often face challenges including limited
representational detail, labor-intensive annotation requirements, and expensive
per-frame optimization. These limitations hinder the study of subtle movements
and can make large-scale analyses impractical. We propose Pose Splatter, a
novel framework leveraging shape carving and 3D Gaussian splatting to model the
complete pose and appearance of laboratory animals without prior knowledge of
animal geometry, per-frame optimization, or manual annotations. We also propose
a novel rotation-invariant visual embedding technique for encoding pose and
appearance, designed to be a plug-in replacement for 3D keypoint data in
downstream behavioral analyses. Experiments on datasets of mice, rats, and
zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,
Pose Splatter represents subtle variations in pose, provides better
low-dimensional pose embeddings over state-of-the-art as evaluated by humans,
and generalizes to unseen data. By eliminating annotation and per-frame
optimization bottlenecks, Pose Splatter enables analysis of large-scale,
longitudinal behavior needed to map genotype, neural activity, and
micro-behavior at unprecedented resolution.

</details>


### [695] [PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises](https://arxiv.org/abs/2505.19186)
*Rushiraj Gadhvi,Priyansh Desai,Siddharth*

Main category: cs.CV

TL;DR: The paper introduces PosePilot, a system that combines pose recognition and real-time personalized feedback for fitness exercises using AI. It uses Yoga as a case study, employs Vanilla LSTM for temporal dependency capture, and BiLSTM with multi-head Attention for motion context processing. A high-quality video dataset is introduced for model evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of automated pose correction in AI-driven fitness systems, particularly focusing on providing precise spatio-temporal alignment feedback which is crucial in disciplines like Yoga.

Method: PosePilot integrates pose recognition with real-time corrective feedback. It uses Vanilla LSTM to capture temporal dependencies and BiLSTM with multi-head Attention to process motion contexts and focus on key limb angles for error detection. The system provides instant feedback at every stage of movement.

Result: PosePilot successfully analyzes complex physical movements, performs automatic human posture recognition, and provides personalized posture correction feedback. The models are lightweight and robust, suitable for deployment on edge devices in real-world environments.

Conclusion: PosePilot overcomes limitations of traditional fitness solutions by offering precise and personalized feedback during exercise routines, making it effective for various at-home and outdoor activities.

Abstract: Automated pose correction remains a significant challenge in AI-driven
fitness systems, despite extensive research in activity recognition. This work
presents PosePilot, a novel system that integrates pose recognition with
real-time personalized corrective feedback, overcoming the limitations of
traditional fitness solutions. Using Yoga, a discipline requiring precise
spatio-temporal alignment as a case study, we demonstrate PosePilot's ability
to analyze complex physical movements. Designed for deployment on edge devices,
PosePilot can be extended to various at-home and outdoor exercises. We employ a
Vanilla LSTM, allowing the system to capture temporal dependencies for pose
recognition. Additionally, a BiLSTM with multi-head Attention enhances the
model's ability to process motion contexts, selectively focusing on key limb
angles for accurate error detection while maintaining computational efficiency.
As part of this work, we introduce a high-quality video dataset used for
evaluating our models. Most importantly, PosePilot provides instant corrective
feedback at every stage of a movement, ensuring precise posture adjustments
throughout the exercise routine. The proposed approach 1) performs automatic
human posture recognition, 2) provides personalized posture correction feedback
at each instant which is crucial in Yoga, and 3) offers a lightweight and
robust posture correction model feasible for deploying on edge devices in
real-world environments.

</details>


### [696] [RAISE: Realness Assessment for Image Synthesis and Evaluation](https://arxiv.org/abs/2505.19233)
*Aniruddha Mukherjee,Spriha Dubey,Somdyuti Paul*

Main category: cs.CV

TL;DR: The paper introduces RAISE, a dataset with images and subjective realness scores, and uses it to train models for predicting perceptual realness.


<details>
  <summary>Details</summary>
Motivation: The rapid development of generative AI has made it possible to create highly realistic visual content, which can substitute real images and videos in difficult or expensive data acquisition scenarios. However, assessing the perceived realness of AI-generated content is challenging due to its subjective nature.

Method: Conducted a human study to evaluate perceptual realness of real and AI-generated images, created a new dataset called RAISE with images paired with subjective realness scores, and developed/trained multiple models on RAISE for realness prediction baselines.

Result: Experimental results show that features from deep foundation vision models can effectively capture subjective realness.

Conclusion: RAISE provides a valuable resource for developing robust, objective models for perceptual realness assessment.

Abstract: The rapid advancement of generative AI has enabled the creation of highly
photorealistic visual content, offering practical substitutes for real images
and videos in scenarios where acquiring real data is difficult or expensive.
However, reliably substituting real visual content with AI-generated
counterparts requires robust assessment of the perceived realness of
AI-generated visual content, a challenging task due to its inherent subjective
nature. To address this, we conducted a comprehensive human study evaluating
the perceptual realness of both real and AI-generated images, resulting in a
new dataset, containing images paired with subjective realness scores,
introduced as RAISE in this paper. Further, we develop and train multiple
models on RAISE to establish baselines for realness prediction. Our
experimental results demonstrate that features derived from deep foundation
vision models can effectively capture the subjective realness. RAISE thus
provides a valuable resource for developing robust, objective models of
perceptual realness assessment.

</details>


### [697] [Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning](https://arxiv.org/abs/2505.19261)
*Yu Zhang,Jialei Zhou,Xinchen Li,Qi Zhang,Zhongwei Wan,Tianyu Wang,Duoqian Miao,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: 研究人员提出了一种名为DiT-ST的新型split-text conditioning框架，通过将完整文本标题转换为简化句子集合，并以分层和增量方式注入不同的去噪阶段，从而增强对特定语义基元类型的表征学习。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散生成通常使用完整文本条件，但由于复杂语法，扩散变压器在理解完整文本标题方面存在缺陷，可能导致语义混淆或忽略关键细节。

Method: 提出DiT-ST框架，将完整文本标题转换为split-text标题（简化句子集合），通过大型语言模型解析标题，提取多样语义基元并分级构建为split-text输入；根据对不同语义基元类型的差异敏感性，划分扩散去噪过程，并确定适当的时间步长，通过交叉注意力将不同语义基元类型的标记逐步注入输入标记中。

Result: 大量实验验证了所提出的DiT-ST在缓解完整文本理解缺陷方面的有效性。

Conclusion: DiT-ST框架能够有效缓解扩散变压器在完整文本理解上的缺陷，提升特定语义基元类型的表征学习能力。

Abstract: Current text-to-image diffusion generation typically employs complete-text
conditioning. Due to the intricate syntax, diffusion transformers (DiTs)
inherently suffer from a comprehension defect of complete-text captions.
One-fly complete-text input either overlooks critical semantic details or
causes semantic confusion by simultaneously modeling diverse semantic primitive
types. To mitigate this defect of DiTs, we propose a novel split-text
conditioning framework named DiT-ST. This framework converts a complete-text
caption into a split-text caption, a collection of simplified sentences, to
explicitly express various semantic primitives and their interconnections. The
split-text caption is then injected into different denoising stages of DiT-ST
in a hierarchical and incremental manner. Specifically, DiT-ST leverages Large
Language Models to parse captions, extracting diverse primitives and
hierarchically sorting out and constructing these primitives into a split-text
input. Moreover, we partition the diffusion denoising process according to its
differential sensitivities to diverse semantic primitive types and determine
the appropriate timesteps to incrementally inject tokens of diverse semantic
primitive types into input tokens via cross-attention. In this way, DiT-ST
enhances the representation learning of specific semantic primitive types
across different stages. Extensive experiments validate the effectiveness of
our proposed DiT-ST in mitigating the complete-text comprehension defect.

</details>


### [698] [TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2505.19291)
*Kazi Mahathir Rahman,Showrin Rahman,Sharmin Sultana Srishty*

Main category: cs.CV

TL;DR: 提出了一种结合强化学习的两阶段文本嵌入图像生成管道，相较于TextDiffuser-2，新方法在保持或超越图像合成质量的同时显著提高了运行速度并降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法（如TextDiffuser-2）虽然能够生成高质量的嵌入文本图像，但其过程资源密集且在CPU和GPU平台上效率有限。因此需要一种更高效的方法来生成嵌入文本的图像。

Method: 该方法采用了一个两阶段的管道：第一阶段使用强化学习快速生成优化的文本布局（包括减少重叠的边界框预测），第二阶段则利用扩散模型进行图像合成。这种方法可以在CPU和GPU上高效运行，并保持或超越现有模型的质量。

Result: 实验表明，该框架在MARIOEval基准测试中达到接近最先进模型的OCR和CLIPScore指标，同时比现有方法快97.64%，仅需2MB内存即可运行。

Conclusion: 所提出的结合强化学习的两阶段管道在保证图像合成质量的同时，大幅提高了运行速度和灵活性，减少了资源消耗，适合在多种平台（如CPU和GPU）上应用。

Abstract: Text-embedded image generation plays a critical role in industries such as
graphic design, advertising, and digital content creation. Text-to-Image
generation methods leveraging diffusion models, such as TextDiffuser-2, have
demonstrated promising results in producing images with embedded text.
TextDiffuser-2 effectively generates bounding box layouts that guide the
rendering of visual text, achieving high fidelity and coherence. However,
existing approaches often rely on resource-intensive processes and are limited
in their ability to run efficiently on both CPU and GPU platforms. To address
these challenges, we propose a novel two-stage pipeline that integrates
reinforcement learning (RL) for rapid and optimized text layout generation with
a diffusion-based image synthesis model. Our RL-based approach significantly
accelerates the bounding box prediction step while reducing overlaps, allowing
the system to run efficiently on both CPUs and GPUs. Extensive evaluations
demonstrate that our framework maintains or surpasses TextDiffuser-2's quality
in text placement and image synthesis, with markedly faster runtime and
increased flexibility. Extensive evaluations demonstrate that our framework
maintains or surpasses TextDiffuser-2's quality in text placement and image
synthesis, with markedly faster runtime and increased flexibility. Our approach
has been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore
metrics close to state-of-the-art models, while being 97.64% more faster and
requiring only 2MB of memory to run.

</details>


### [699] [Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion](https://arxiv.org/abs/2505.19385)
*Jiaqi Guo,Santiago Lopez-Tapia,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出了一种新的LACT方法，利用MR-SDEs进行sinogram插补，并结合蒸馏和后处理模块加速扩散过程并提高图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 有限角计算机断层扫描（LACT）由于缺乏角度信息而面临重大挑战，传统的图像域方法无法充分解决这一问题。

Method: 提出一种新的方法，专注于sinogram插补，使用MR-SDEs（均值回归随机微分方程的扩散模型变体）来填补投影级别的缺失角度数据；通过结合知识蒸馏和伪逆矩阵约束模型输出，加速扩散过程为一步完成；随后，后处理模块将插补后的sinogram反投影到图像域并进一步优化重建结果。

Result: 实验结果表明，该方法在感知质量和保真度方面均达到了最先进的性能。

Conclusion: 该方法为LACT重建提供了有前景的解决方案，在科学和临床应用中具有重要意义。

Abstract: Limited Angle Computed Tomography (LACT) often faces significant challenges
due to missing angular information. Unlike previous methods that operate in the
image domain, we propose a new method that focuses on sinogram inpainting. We
leverage MR-SDEs, a variant of diffusion models that characterize the diffusion
process with mean-reverting stochastic differential equations, to fill in
missing angular data at the projection level. Furthermore, by combining
distillation with constraining the output of the model using the pseudo-inverse
of the inpainting matrix, the diffusion process is accelerated and done in a
step, enabling efficient and accurate sinogram completion. A subsequent
post-processing module back-projects the inpainted sinogram into the image
domain and further refines the reconstruction, effectively suppressing
artifacts while preserving critical structural details. Quantitative
experimental results demonstrate that the proposed method achieves
state-of-the-art performance in both perceptual and fidelity quality, offering
a promising solution for LACT reconstruction in scientific and clinical
applications.

</details>


### [700] [Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals](https://arxiv.org/abs/2505.19386)
*Nate Gillman,Charles Herrmann,Michael Freeman,Daksh Aggarwal,Evan Luo,Deqing Sun,Chen Sun*

Main category: cs.CV

TL;DR: Recent advances in video generation models have sparked interest in world models capable of simulating realistic environments. This work investigates using physical forces as a control signal for video generation, proposing force prompts which enable users to interact with images through both localized point forces and global wind force fields.


<details>
  <summary>Details</summary>
Motivation: While navigation has been well-explored, physically meaningful interactions that mimic real-world forces remain largely understudied.

Method: The method leverages the visual and motion prior in the original pretrained model to enable videos to respond realistically to physical control signals without using any 3D asset or physics simulator at inference. The approach is trained on around 15k training examples for a single day on four A100 GPUs.

Result: The method outperforms existing methods on force adherence and physics realism, bringing world models closer to real-world physics interactions.

Conclusion: This work demonstrates that video generation models can generalize remarkably well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations of few objects.

Abstract: Recent advances in video generation models have sparked interest in world
models capable of simulating realistic environments. While navigation has been
well-explored, physically meaningful interactions that mimic real-world forces
remain largely understudied. In this work, we investigate using physical forces
as a control signal for video generation and propose force prompts which enable
users to interact with images through both localized point forces, such as
poking a plant, and global wind force fields, such as wind blowing on fabric.
We demonstrate that these force prompts can enable videos to respond
realistically to physical control signals by leveraging the visual and motion
prior in the original pretrained model, without using any 3D asset or physics
simulator at inference. The primary challenge of force prompting is the
difficulty in obtaining high quality paired force-video training data, both in
the real world due to the difficulty of obtaining force signals, and in
synthetic data due to limitations in the visual quality and domain diversity of
physics simulators. Our key finding is that video generation models can
generalize remarkably well when adapted to follow physical force conditioning
from videos synthesized by Blender, even with limited demonstrations of few
objects. Our method can generate videos which simulate forces across diverse
geometries, settings, and materials. We also try to understand the source of
this generalization and perform ablations that reveal two key elements: visual
diversity and the use of specific text keywords during training. Our approach
is trained on only around 15k training examples for a single day on four A100
GPUs, and outperforms existing methods on force adherence and physics realism,
bringing world models closer to real-world physics interactions. We release all
datasets, code, weights, and interactive video demos at our project page.

</details>


### [701] [CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features](https://arxiv.org/abs/2505.19434)
*X. Feng,D. Zhang,S. Hu,X. Li,M. Wu,J. Zhang,X. Chen,K. Huang*

Main category: cs.CV

TL;DR: The paper proposes CSTrack, a novel tracker focusing on modeling compact spatiotemporal features for RGB-X tracking, which achieves new SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for RGB-X tracking often employ two parallel branches to separately process the RGB and X input streams, complicating the model structure and computation process. Intra-modality spatial modeling within each dispersed space incurs substantial computational overhead, limiting resources for inter-modality spatial modeling and temporal modeling.

Method: The authors propose CSTrack with an innovative Spatial Compact Module that integrates the RGB-X dual input streams into a compact spatial feature, enabling thorough intra- and inter-modality spatial modeling. They also design an efficient Temporal Compact Module that compactly represents temporal features by constructing the refined target distribution heatmap.

Result: Extensive experiments validate the effectiveness of the compact spatiotemporal modeling method, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.

Conclusion: CSTrack focuses on modeling compact spatiotemporal features to achieve simple yet effective tracking, and it outperforms existing methods on RGB-X benchmarks.

Abstract: Effectively modeling and utilizing spatiotemporal features from RGB and other
modalities (\eg, depth, thermal, and event data, denoted as X) is the core of
RGB-X tracker design. Existing methods often employ two parallel branches to
separately process the RGB and X input streams, requiring the model to
simultaneously handle two dispersed feature spaces, which complicates both the
model structure and computation process. More critically, intra-modality
spatial modeling within each dispersed space incurs substantial computational
overhead, limiting resources for inter-modality spatial modeling and temporal
modeling. To address this, we propose a novel tracker, CSTrack, which focuses
on modeling Compact Spatiotemporal features to achieve simple yet effective
tracking. Specifically, we first introduce an innovative Spatial Compact Module
that integrates the RGB-X dual input streams into a compact spatial feature,
enabling thorough intra- and inter-modality spatial modeling. Additionally, we
design an efficient Temporal Compact Module that compactly represents temporal
features by constructing the refined target distribution heatmap. Extensive
experiments validate the effectiveness of our compact spatiotemporal modeling
method, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.
The code and models will be released at:
https://github.com/XiaokunFeng/CSTrack.

</details>


### [702] [C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging](https://arxiv.org/abs/2505.18745)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: Immunohistochemical (IHC) images present challenges for deep learning models due to inconsistencies in channel count and configuration. To address this, the authors introduce Channel Conditioned Cell Representations (C3R), a framework that uses a context-concept principle for unified evaluation on in-distribution and out-of-distribution datasets. C3R outperforms existing benchmarks and enables cross-dataset generalization without requiring dataset-specific adaptation or retraining.


<details>
  <summary>Details</summary>
Motivation: Deep learning models face difficulties when working with IHC datasets because of the inconsistencies in channel count and configuration resulting from varying staining protocols across different laboratories and studies. Existing channel-adaptive models fail to support out-of-distribution evaluation across IHC datasets and cannot be applied in a true zero-shot setting with mismatched channel counts.

Method: The authors propose a structured view of cellular image channels by categorizing them into either context or concept. They leverage this context-concept principle to develop Channel Conditioned Cell Representations (C3R). C3R is a two-fold framework consisting of a channel-adaptive encoder architecture and a masked knowledge distillation training strategy, both built around the context-concept principle.

Result: C3R outperforms existing benchmarks on both in-distribution and out-of-distribution tasks. Even a trivial implementation of the core idea outperforms channel-adaptive methods reported on the CHAMMI benchmark.

Conclusion: The proposed method, C3R, opens a new pathway for cross-dataset generalization between IHC datasets without needing dataset-specific adaptation or retraining.

Abstract: Immunohistochemical (IHC) images reveal detailed information about structures
and functions at the subcellular level. However, unlike natural images, IHC
datasets pose challenges for deep learning models due to their inconsistencies
in channel count and configuration, stemming from varying staining protocols
across laboratories and studies. Existing approaches build channel-adaptive
models, which unfortunately fail to support out-of-distribution (OOD)
evaluation across IHC datasets and cannot be applied in a true zero-shot
setting with mismatched channel counts. To address this, we introduce a
structured view of cellular image channels by grouping them into either context
or concept, where we treat the context channels as a reference to the concept
channels in the image. We leverage this context-concept principle to develop
Channel Conditioned Cell Representations (C3R), a framework designed for
unified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold
framework comprising a channel-adaptive encoder architecture and a masked
knowledge distillation training strategy, both built around the context-concept
principle. We find that C3R outperforms existing benchmarks on both ID and OOD
tasks, while a trivial implementation of our core idea also outperforms the
channel-adaptive methods reported on the CHAMMI benchmark. Our method opens a
new pathway for cross-dataset generalization between IHC datasets, without
requiring dataset-specific adaptation or retraining.

</details>


### [703] [MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering](https://arxiv.org/abs/2505.19455)
*Xu Li,Fan Lyu*

Main category: cs.CV

TL;DR: The paper introduces MM-Prompt, a new framework for Continual Visual Question Answering (CVQA) that integrates cross-modal prompt query and recovery to address modality imbalance and improve performance over time.


<details>
  <summary>Details</summary>
Motivation: Existing CVQA methods using prompt tuning face the problem of modality imbalance due to cross-modal prompt isolation, which negatively affects performance as learning progresses.

Method: The proposed method, MM-Prompt, includes two key components: cross-modal prompt query for balanced prompt selection by integrating cross-modal signals, and cross-modal prompt recovery which uses iterative interactions and an alignment loss to reconstruct prompts jointly and prevent representational drift.

Result: Experimental results indicate that MM-Prompt outperforms previous methods in terms of accuracy and knowledge retention while ensuring balanced engagement of modalities during continual learning.

Conclusion: MM-Prompt effectively mitigates modality imbalance in CVQA through its innovative approach to prompt tuning, leading to improved performance and sustained knowledge retention.

Abstract: Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)
has achieved promising progress by leveraging prompt tuning to enable continual
multi-modal learning. However, most existing methods adopt cross-modal prompt
isolation, constructing visual and textual prompts separately, which
exacerbates modality imbalance and leads to degraded performance over time. To
tackle this issue, we propose MM-Prompt, a novel framework incorporating
cross-modal prompt query and cross-modal prompt recovery. The former enables
balanced prompt selection by incorporating cross-modal signals during query
formation, while the latter promotes joint prompt reconstruction through
iterative cross-modal interactions, guided by an alignment loss to prevent
representational drift. Extensive experiments show that MM-Prompt surpasses
prior approaches in accuracy and knowledge retention, while maintaining
balanced modality engagement throughout continual learning.

</details>


### [704] [Dual-Path Stable Soft Prompt Generation for Domain Generalization](https://arxiv.org/abs/2505.18770)
*Yuedi Zhang,Shuanghao Bai,Wanqi Zhou,Zhirong Luan,Badong Chen*

Main category: cs.CV

TL;DR: The paper proposes Dual-Path Stable Soft Prompt Generation (DPSPG) to solve the problem of prompt variability in existing prompt generation methods.


<details>
  <summary>Details</summary>
Motivation: Prompt tuning is effective for domain generalization but struggles with capturing domain-specific features. Current prompt generation methods have issues with prompt variability, producing inconsistent and suboptimal prompts across different random seeds.

Method: The authors introduce negative learning into prompt generation and propose DPSPG, a transformer-based framework that incorporates a complementary prompt generator to produce negative prompts. This reduces misleading information and enhances both stability and generalization of prompts.

Result: Theoretical and empirical analyses show that negative learning leads to more robust and effective prompts by increasing the effective margin and reducing the upper bound of the gradient norm. Experiments on five DG benchmark datasets demonstrate DPSPG consistently outperforms state-of-the-art methods while maintaining prompt stability.

Conclusion: DPSPG addresses prompt variability and improves the stability and generalization of prompts, offering a significant advancement in domain generalization.

Abstract: Domain generalization (DG) aims to learn a model using data from one or
multiple related but distinct source domains that can generalize well to unseen
out-of-distribution target domains. Inspired by the success of large
pre-trained vision-language models (VLMs), prompt tuning has emerged as an
effective generalization strategy. However, it often struggles to capture
domain-specific features due to its reliance on manually or fixed prompt
inputs. Recently, some prompt generation methods have addressed this limitation
by dynamically generating instance-specific and domain-specific prompts for
each input, enriching domain information and demonstrating potential for
enhanced generalization. Through further investigation, we identify a notable
issue in existing prompt generation methods: the same input often yields
significantly different and suboptimal prompts across different random seeds, a
phenomenon we term Prompt Variability. To address this, we introduce negative
learning into the prompt generation process and propose Dual-Path Stable Soft
Prompt Generation (DPSPG), a transformer-based framework designed to improve
both the stability and generalization of prompts. Specifically, DPSPG
incorporates a complementary prompt generator to produce negative prompts,
thereby reducing the risk of introducing misleading information. Both
theoretical and empirical analyses demonstrate that negative learning leads to
more robust and effective prompts by increasing the effective margin and
reducing the upper bound of the gradient norm. Extensive experiments on five DG
benchmark datasets show that DPSPG consistently outperforms state-of-the-art
methods while maintaining prompt stability.

</details>


### [705] [Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.19498)
*Nanxing Hu,Xiaoyue Duan,Jinchao Zhang,Guoliang Kang*

Main category: cs.CV

TL;DR: 本文提出了一种从三个方面缓解大型视觉-语言模型(LVLM)幻觉问题的方法：评估并移除冗余的视觉标记、从贝叶斯视角校正先验信息、以及在后验分布崩溃为不依赖任何信息性视觉标记的先验分布时停止进一步文本生成。实验表明，该方法能有效缓解LVLM的幻觉问题，并优于先前的最先进方法。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型(LVLM)生成的文本虽然上下文连贯，但往往与视觉输入不符，这种幻觉问题阻碍了LVLM在现实世界中的应用。解决幻觉的关键在于使文本生成更多地依赖于视觉内容。

Method: 1. 评估并移除冗余的视觉标记以避免干扰；2. 提出一种简单而有效的方法从贝叶斯视角校正先验信息；3. 在后验分布崩溃为不依赖任何信息性视觉标记的先验分布时停止进一步文本生成。

Result: 广泛的实验表明，该方法可以持续缓解LVLM的幻觉问题，并在三个基准数据集(POPE、CHAIR和MME)上表现出优于先前最先进的方法的性能。

Conclusion: 所提出的方法能够系统地增强视觉依赖，从而有效缓解LVLM的幻觉问题，并在多个基准测试中表现优异。

Abstract: Large Vision-Language Models (LVLMs) usually generate texts which satisfy
context coherence but don't match the visual input. Such a hallucination issue
hinders LVLMs' applicability in the real world. The key to solving
hallucination in LVLM is to make the text generation rely more on the visual
content. Most previous works choose to enhance/adjust the features/output of a
specific modality (i.e., visual or textual) to alleviate hallucinations in
LVLM, which do not explicitly or systematically enhance the visual reliance. In
this paper, we comprehensively investigate the factors which may degenerate the
visual reliance in text generation of LVLM from a Bayesian perspective. Based
on our observations, we propose to mitigate hallucination in LVLM from three
aspects. Firstly, we observe that not all visual tokens are informative in
generating meaningful texts. We propose to evaluate and remove redundant visual
tokens to avoid their disturbance. Secondly, LVLM may encode inappropriate
prior information, making it lean toward generating unexpected words. We
propose a simple yet effective way to rectify the prior from a Bayesian
perspective. Thirdly, we observe that starting from certain steps, the
posterior of next-token prediction conditioned on visual tokens may collapse to
a prior distribution which does not depend on any informative visual tokens at
all. Thus, we propose to stop further text generation to avoid hallucination.
Extensive experiments on three benchmarks including POPE, CHAIR, and MME
demonstrate that our method can consistently mitigate the hallucination issue
of LVLM and performs favorably against previous state-of-the-arts.

</details>


### [706] [Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos](https://arxiv.org/abs/2505.18899)
*Andrea Ramazzina,Vittorio Giammarino,Matteo El-Hariry,Mario Bijelic*

Main category: cs.CV

TL;DR: This paper presents an event-inspired perception method that converts RGB videos into a sparse, event-based representation to enable robust visual imitation learning across domain shifts.


<details>
  <summary>Details</summary>
Motivation: Imitation from videos often fails due to domain shifts like lighting, color or texture discrepancies between expert demonstrations and learner environments. Current methods using visual randomization are computationally intensive and struggle with unseen scenarios.

Method: The authors propose converting standard RGB videos into a sparse, event-based representation encoding temporal intensity gradients while discarding static appearance features. This approach is inspired by biological vision systems and recent sensor advancements, aiming to disentangle motion dynamics from visual style.

Result: Experiments on the DeepMind Control Suite and Adroit platform demonstrate the efficacy of this method in achieving invariance to appearance-based distractors without needing computationally expensive data augmentation techniques.

Conclusion: Event-inspired perception offers a promising way for visually robust imitation learning across domain shifts by eliminating the influence of appearances entirely through rethinking sensory representation.

Abstract: Imitation from videos often fails when expert demonstrations and learner
environments exhibit domain shifts, such as discrepancies in lighting, color,
or texture. While visual randomization partially addresses this problem by
augmenting training data, it remains computationally intensive and inherently
reactive, struggling with unseen scenarios. We propose a different approach:
instead of randomizing appearances, we eliminate their influence entirely by
rethinking the sensory representation itself. Inspired by biological vision
systems that prioritize temporal transients (e.g., retinal ganglion cells) and
by recent sensor advancements, we introduce event-inspired perception for
visually robust imitation. Our method converts standard RGB videos into a
sparse, event-based representation that encodes temporal intensity gradients,
discarding static appearance features. This biologically grounded approach
disentangles motion dynamics from visual style, enabling robust visual
imitation from observations even in the presence of visual mismatches between
expert and agent environments. By training policies on event streams, we
achieve invariance to appearance-based distractors without requiring
computationally expensive and environment-specific data augmentation
techniques. Experiments across the DeepMind Control Suite and the Adroit
platform for dynamic dexterous manipulation show the efficacy of our method.
Our code is publicly available at Eb-LAIfO.

</details>


### [707] [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
*Jintao Tong,Wenwei Jin,Pengda Qin,Anqi Li,Yixiong Zou,Yuhong Li,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: FlowCut is an information-flow-aware pruning framework that surpasses existing methods in identifying redundant tokens in large vision-language models, leading to significant performance improvements and speed-ups.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for reducing computational costs in large vision-language models rely on single-layer attention scores, which may not be sufficient due to the complex interactions between tokens and layers.

Method: The authors introduce FlowCut, a framework based on information flow analysis. It captures how information moves between tokens across layers, using the CLS token as an information relay. This approach identifies redundancy progressively and dynamically via layer-wise attention concentration, addressing the insufficiency of single-layer criteria.

Result: FlowCut outperforms state-of-the-art methods by 1.6% on LLaVA-1.5-7B with 88.9% token reduction and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, while delivering a 3.2x speed-up in the prefilling stage.

Conclusion: FlowCut provides a more effective way to prune redundant tokens in large vision-language models, aligning better with the model's inherent behaviors and improving both efficiency and performance.

Abstract: Large vision-language models (LVLMs) excel at multimodal understanding but
suffer from high computational costs due to redundant vision tokens. Existing
pruning methods typically rely on single-layer attention scores to rank and
prune redundant visual tokens to solve this inefficiency. However, as the
interaction between tokens and layers is complicated, this raises a basic
question: Is such a simple single-layer criterion sufficient to identify
redundancy? To answer this question, we rethink the emergence of redundant
visual tokens from a fundamental perspective: information flow, which models
the interaction between tokens and layers by capturing how information moves
between tokens across layers. We find (1) the CLS token acts as an information
relay, which can simplify the complicated flow analysis; (2) the redundancy
emerges progressively and dynamically via layer-wise attention concentration;
and (3) relying solely on attention scores from single layers can lead to
contradictory redundancy identification. Based on this, we propose FlowCut, an
information-flow-aware pruning framework, mitigating the insufficiency of the
current criterion for identifying redundant tokens and better aligning with the
model's inherent behaviors. Extensive experiments show that FlowCut achieves
superior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token
reduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x
speed-up in the prefilling stage. Our code is available at
https://github.com/TungChintao/FlowCut

</details>


### [708] [Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.19611)
*Ruolin Shen,Xiaozhong Ji,Kai WU,Jiangning Zhang,Yijun He,HaiHua Yang,Xiaobin Hu,Xiaoyu Sun*

Main category: cs.CV

TL;DR: The paper proposes a visual refocus reinforcement framework to enhance multi-modal models' ability in identifying camouflaged objects, surpassing human perception systems.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal models show limitations in identifying camouflaged objects as they cannot replicate human cognitive processes that use foreground-background similarity principles.

Method: The authors developed a visual system mimicking human visual camouflaged perception, incorporating a 'refocus' mechanism that progressively and iteratively localizes concealed objects through stepwise reasoning. This is achieved via a policy optimization algorithm within a visual refocus reinforcement framework.

Result: Experiments demonstrated the emergence of refocus visual phenomena with multiple reasoning tokens and dynamic adjustment of detection boxes. The model outperformed Supervised Fine-Tuning (SFT) baselines in both camouflaged object classification and detection tasks.

Conclusion: The proposed framework enables multi-modal models to achieve excellent reasoning abilities, aligning with and surpassing human camouflaged perception systems.

Abstract: Current multi-modal models exhibit a notable misalignment with the human
visual system when identifying objects that are visually assimilated into the
background. Our observations reveal that these multi-modal models cannot
distinguish concealed objects, demonstrating an inability to emulate human
cognitive processes which effectively utilize foreground-background similarity
principles for visual analysis. To analyze this hidden human-model visual
thinking discrepancy, we build a visual system that mimicks human visual
camouflaged perception to progressively and iteratively `refocus' visual
concealed content. The refocus is a progressive guidance mechanism enabling
models to logically localize objects in visual images through stepwise
reasoning. The localization process of concealed objects requires hierarchical
attention shifting with dynamic adjustment and refinement of prior cognitive
knowledge. In this paper, we propose a visual refocus reinforcement framework
via the policy optimization algorithm to encourage multi-modal models to think
and refocus more before answering, and achieve excellent reasoning abilities to
align and even surpass human camouflaged perception systems. Our extensive
experiments on camouflaged perception successfully demonstrate the emergence of
refocus visual phenomena, characterized by multiple reasoning tokens and
dynamic adjustment of the detection box. Besides, experimental results on both
camouflaged object classification and detection tasks exhibit significantly
superior performance compared to Supervised Fine-Tuning (SFT) baselines.

</details>


### [709] [Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers](https://arxiv.org/abs/2505.19122)
*Eric Tillman Bill,Cristian Perez Jensen,Sotiris Anagnostidis,Dimitri von Rütte*

Main category: cs.CV

TL;DR: Denoising diffusion models are powerful but hard to train due to stochasticity. This paper investigates magnitude preservation and rotation modulation in Diffusion Transformer architecture, showing improved performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To stabilize the training process of denoising diffusion models and improve their generative capabilities by exploring magnitude preservation and novel conditioning methods beyond U-net architecture.

Method: Propose a magnitude-preserving design without normalization layers and introduce rotation modulation as a conditioning method using learned rotations instead of scaling or shifting.

Result: Magnitude-preserving strategies significantly improve performance, reducing FID scores by ~12.8%. Rotation modulation combined with scaling is competitive with AdaLN while requiring ~5.4% fewer parameters.

Conclusion: This work provides insights into conditioning strategies and magnitude control in Diffusion Transformer architecture, advancing stabilization techniques in denoising diffusion models.

Abstract: Denoising diffusion models exhibit remarkable generative capabilities, but
remain challenging to train due to their inherent stochasticity, where
high-variance gradient estimates lead to slow convergence. Previous works have
shown that magnitude preservation helps with stabilizing training in the U-net
architecture. This work explores whether this effect extends to the Diffusion
Transformer (DiT) architecture. As such, we propose a magnitude-preserving
design that stabilizes training without normalization layers. Motivated by the
goal of maintaining activation magnitudes, we additionally introduce rotation
modulation, which is a novel conditioning method using learned rotations
instead of traditional scaling or shifting. Through empirical evaluations and
ablation studies on small-scale models, we show that magnitude-preserving
strategies significantly improve performance, notably reducing FID scores by
$\sim$12.8%. Further, we show that rotation modulation combined with scaling is
competitive with AdaLN, while requiring $\sim$5.4% fewer parameters. This work
provides insights into conditioning strategies and magnitude control. We will
publicly release the implementation of our method.

</details>


### [710] [Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat](https://arxiv.org/abs/2505.19624)
*Pusheng Xu,Xia Gong,Xiaolan Chen,Weiyi Zhang,Jiancheng Yang,Bingjie Yan,Meng Yuan,Yalin Zheng,Mingguang He,Danli Shi*

Main category: cs.CV

TL;DR: The study developed a bilingual VQA benchmark for ophthalmology, using WeChat data to create QA pairs in Chinese and English. The OphthalWeChat dataset includes 3,469 images and 30,120 QA pairs. Among the three evaluated VLMs, Gemini 2.0 Flash performed best overall.


<details>
  <summary>Details</summary>
Motivation: To address the need for evaluating vision-language models (VLMs) in ophthalmology by creating a bilingual multimodal visual question answering benchmark.

Method: Collected ophthalmic image posts from WeChat Official Accounts between 2016-2024, generated bilingual QA pairs with GPT-4o-mini, categorized them into six subsets based on question type and language, and used the benchmark to evaluate three VLMs.

Result: The OphthalWeChat dataset was created with 3,469 images and 30,120 QA pairs across various categories. Gemini 2.0 Flash had the highest overall accuracy, surpassing GPT-4o and Qwen2.5-VL-72B-Instruct.

Conclusion: This study introduced the first bilingual VQA benchmark for ophthalmology, reflecting real-world clinical scenarios and supporting the development of specialized AI systems for eye care.

Abstract: Purpose: To develop a bilingual multimodal visual question answering (VQA)
benchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts
and associated captions published between January 1, 2016, and December 31,
2024, were collected from WeChat Official Accounts. Based on these captions,
bilingual question-answer (QA) pairs in Chinese and English were generated
using GPT-4o-mini. QA pairs were categorized into six subsets by question type
and language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,
Single-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark
was used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,
and Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included
3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548
conditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0
Flash achieved the highest overall accuracy (0.548), outperforming GPT-4o
(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led
in both Chinese (0.546) and English subsets (0.550). Subset-specific
performance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),
Single-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked
highest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),
and Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study
presents the first bilingual VQA benchmark for ophthalmology, distinguished by
its real-world context and inclusion of multiple examinations per patient. The
dataset reflects authentic clinical decision-making scenarios and enables
quantitative evaluation of VLMs, supporting the development of accurate,
specialized, and trustworthy AI systems for eye care.

</details>


### [711] [JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models](https://arxiv.org/abs/2505.19166)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: JEDI is a test-time adaptation method enhancing subject separation and compositional alignment in diffusion models without retraining or external supervision.


<details>
  <summary>Details</summary>
Motivation: To improve subject separation and compositional alignment in diffusion models without requiring retraining or external supervision.

Method: Minimizing semantic entanglement in attention maps using a novel Jensen-Shannon divergence based objective and leveraging adversarial optimization to reduce the number of updating steps required.

Result: Consistently improves prompt alignment and disentanglement in complex scenes, applicable to architectures such as Stable Diffusion 1.5 and 3.5, provides a lightweight CLIP-free disentanglement score.

Conclusion: JEDI is model-agnostic, effective in improving compositional alignment, and offers a principled benchmark for test-time conditions; implementation will be publicly released.

Abstract: We introduce JEDI, a test-time adaptation method that enhances subject
separation and compositional alignment in diffusion models without requiring
retraining or external supervision. JEDI operates by minimizing semantic
entanglement in attention maps using a novel Jensen-Shannon divergence based
objective. To improve efficiency, we leverage adversarial optimization,
reducing the number of updating steps required.
  JEDI is model-agnostic and applicable to architectures such as Stable
Diffusion 1.5 and 3.5, consistently improving prompt alignment and
disentanglement in complex scenes. Additionally, JEDI provides a lightweight,
CLIP-free disentanglement score derived from internal attention distributions,
offering a principled benchmark for compositional alignment under test-time
conditions. We will publicly release the implementation of our method.

</details>


### [712] [BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change](https://arxiv.org/abs/2505.19328)
*Manuela González-González,Soufiane Belharbi,Muhammad Osama Zeeshan,Masoumeh Sharafi,Muhammad Haseeb Aslam,Marco Pedersoli,Alessandro Lameiras Koerich,Simon L Bacon,Eric Granger*

Main category: cs.CV

TL;DR: This paper introduces the first Behavioural Ambivalence/Hesitancy (BAH) dataset for recognizing complex emotions linked to ambivalence and hesitancy in videos. The dataset includes 1,118 videos from 224 participants across Canada, with annotations for A/H cues at frame- and video-levels. Baseline models show limited performance, emphasizing the challenges of A/H recognition in real-world settings.


<details>
  <summary>Details</summary>
Motivation: Recognizing ambivalence and hesitancy is crucial for personalized digital behavior change interventions, but existing methods relying on human experts are costly and less effective. There is a lack of datasets for designing ML models to recognize these emotions automatically.

Method: The authors collected a BAH dataset with videos from 224 participants answering questions designed to elicit ambivalence and hesitancy. The dataset includes annotations for A/H cues, video transcripts, timestamps, cropped faces, and participant metadata. They provide baseline results for frame- and video-level recognition using multi-modal setups, zero-shot prediction, and unsupervised domain adaptation.

Result: The BAH dataset contains 1,118 videos totaling 8.26 hours, with 1.5 hours showing ambivalence and hesitancy. Baseline model performances indicate significant challenges in recognizing A/H in real-world videos.

Conclusion: The BAH dataset fills a gap in resources for recognizing ambivalence and hesitancy through automatic learning systems. The limited performance of baseline models highlights the need for further research and development in this area.

Abstract: Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can
play a critical role in the personalization and effectiveness of digital
behaviour change interventions. These subtle and conflicting emotions are
manifested by a discord between multiple modalities, such as facial and vocal
expressions, and body language. Although experts can be trained to identify
A/H, integrating them into digital interventions is costly and less effective.
Automatic learning systems provide a cost-effective alternative that can adapt
to individual users, and operate seamlessly within real-time, and
resource-limited environments. However, there are currently no datasets
available for the design of ML models to recognize A/H. This paper introduces a
first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for
subject-based multimodal recognition of A/H in videos. It contains videos from
224 participants captured across 9 provinces in Canada, with different age, and
ethnicity. Through our web platform, we recruited participants to answer 7
questions, some of which were designed to elicit A/H while recording themselves
via webcam with microphone. BAH amounts to 1,118 videos for a total duration of
8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp
segments to indicate where A/H occurs, and provide frame- and video-level
annotations with the A/H cues. Video transcripts and their timestamps are also
included, along with cropped and aligned faces in each frame, and a variety of
participants meta-data. We include results baselines for BAH at frame- and
video-level recognition in multi-modal setups, in addition to zero-shot
prediction, and for personalization using unsupervised domain adaptation. The
limited performance of baseline models highlights the challenges of recognizing
A/H in real-world videos. The data, code, and pretrained weights are available.

</details>


### [713] [The Missing Point in Vision Transformers for Universal Image Segmentation](https://arxiv.org/abs/2505.19795)
*Sajjad Shahabodini,Mobina Mansoori,Farnoush Bayatmakou,Jamshid Abouei,Konstantinos N. Plataniotis,Arash Mohammadi*

Main category: cs.CV

TL;DR: Image segmentation is a challenging task in computer vision. This paper presents ViT-P, a two-stage segmentation framework that decouples mask generation from classification. It achieves state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Image segmentation demands robust mask generation and precise classification. Although recent mask-based approaches yield high-quality masks by capturing global context, accurately classifying these masks remains an open challenge, especially when dealing with ambiguous boundaries and imbalanced class distributions.

Method: The paper introduces ViT-P, a novel two-stage segmentation framework. The first stage employs a proposal generator to produce class-agnostic mask proposals. The second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture. Coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets.

Result: Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P. It achieves state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation.

Conclusion: ViT-P, a novel two-stage segmentation framework, decouples mask generation from classification and demonstrates superior performance on image segmentation tasks. It effectively integrates various pre-trained vision transformers and reduces annotation costs while maintaining strong performance.

Abstract: Image segmentation remains a challenging task in computer vision, demanding
robust mask generation and precise classification. Recent mask-based approaches
yield high-quality masks by capturing global context. However, accurately
classifying these masks, especially in the presence of ambiguous boundaries and
imbalanced class distributions, remains an open challenge. In this work, we
introduce ViT-P, a novel two-stage segmentation framework that decouples mask
generation from classification. The first stage employs a proposal generator to
produce class-agnostic mask proposals, while the second stage utilizes a
point-based classification model built on the Vision Transformer (ViT) to
refine predictions by focusing on mask central points. ViT-P serves as a
pre-training-free adapter, allowing the integration of various pre-trained
vision transformers without modifying their architecture, ensuring adaptability
to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding
box annotations can effectively enhance classification without requiring
additional training on fine annotation datasets, reducing annotation costs
while maintaining strong performance. Extensive experiments across COCO,
ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving
state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4
mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic
segmentation. The code and pretrained models are available at:
https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.

</details>


### [714] [Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach](https://arxiv.org/abs/2505.19479)
*Lakshmi Aishwarya Malladi,Navarun Gupta,Ahmed El-Sayed,Xingguo Xiong*

Main category: cs.CV

TL;DR: The study aims to improve wildfire detection accuracy using a Convolutional Neural Network (CNN) based on the VGG16 architecture. It utilized the D-FIRE dataset and overcame challenges such as low-resolution images and dataset imbalance through data augmentation techniques. The model demonstrated a low false negative rate, crucial for early fire detection. Future work will focus on integrating real-time surveillance networks and expanding the dataset.


<details>
  <summary>Details</summary>
Motivation: Wildfires are becoming more frequent and intense, leading to significant losses in lives, infrastructure, and ecosystems. There is an urgent need for efficient warning systems to prevent disastrous outcomes.

Method: The study employed the VGG16 architecture with modifications for binary classification to detect wildfires using the D-FIRE dataset. Challenges like low-resolution images and dataset imbalance were addressed via data augmentation techniques.

Result: The model achieved a low false negative rate, which is vital for minimizing undetected fires and facilitating rapid responses by authorities.

Conclusion: Deep learning models, such as the adapted VGG16 model used in this study, can provide reliable and automated solutions for early wildfire detection, helping reduce the impact of wildfires.

Abstract: Over 8,024 wildfire incidents have been documented in 2024 alone, affecting
thousands of fatalities and significant damage to infrastructure and
ecosystems. Wildfires in the United States have inflicted devastating losses.
Wildfires are becoming more frequent and intense, which highlights how urgently
efficient warning systems are needed to avoid disastrous outcomes. The goal of
this study is to enhance the accuracy of wildfire detection by using
Convolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE
dataset, which includes several kinds of wildfire and non-wildfire images, was
employed in the study. Low-resolution images, dataset imbalance, and the
necessity for real-time applicability are some of the main challenges. These
problems were resolved by enriching the dataset using data augmentation
techniques and optimizing the VGG16 model for binary classification. The model
produced a low false negative rate, which is essential for reducing unexplored
fires, despite dataset boundaries. In order to help authorities execute fast
responses, this work shows that deep learning models such as VGG16 can offer a
reliable, automated approach for early wildfire recognition. For the purpose of
reducing the impact of wildfires, our future work will concentrate on
connecting to systems with real-time surveillance networks and enlarging the
dataset to cover more varied fire situations.

</details>


### [715] [Multimodal Machine Translation with Visual Scene Graph Pruning](https://arxiv.org/abs/2505.19507)
*Chenyu Lu,Shiliang Sun,Jing Zhao,Nan Zhang,Tengfei Song,Hao Yang*

Main category: cs.CV

TL;DR: A new method, PSG model, is proposed to prune visual scene graph's redundant nodes guided by language scene graph information, reducing noise in multimodal machine translation. It shows great potential.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to solve the issue of visual information redundancy in multimodal machine translation (MMT) which has not been adequately tackled before.

Method: The method introduced is multimodal machine translation with visual Scene Graph Pruning (PSG). It leverages language scene graph information to guide the pruning of redundant nodes in visual scene graphs.

Result: Through extensive comparative experiments and ablation studies, the effectiveness of the PSG model was demonstrated.

Conclusion: The results highlight the promising potential of visual information pruning in advancing the field of MMT.

Abstract: Multimodal machine translation (MMT) seeks to address the challenges posed by
linguistic polysemy and ambiguity in translation tasks by incorporating visual
information. A key bottleneck in current MMT research is the effective
utilization of visual data. Previous approaches have focused on extracting
global or region-level image features and using attention or gating mechanisms
for multimodal information fusion. However, these methods have not adequately
tackled the issue of visual information redundancy in MMT, nor have they
proposed effective solutions. In this paper, we introduce a novel
approach--multimodal machine translation with visual Scene Graph Pruning (PSG),
which leverages language scene graph information to guide the pruning of
redundant nodes in visual scene graphs, thereby reducing noise in downstream
translation tasks. Through extensive comparative experiments with
state-of-the-art methods and ablation studies, we demonstrate the effectiveness
of the PSG model. Our results also highlight the promising potential of visual
information pruning in advancing the field of MMT.

</details>


### [716] [Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning](https://arxiv.org/abs/2505.19522)
*Jiyu Hu,Haijiang Zeng,Zhen Tian*

Main category: cs.CV

TL;DR: This paper proposes a semi-supervised image classification model based on GANs to solve the problem of insufficient labeled data in complex environments.


<details>
  <summary>Details</summary>
Motivation: Image classification heavily relies on high-quality labeled data, which limits deep learning models' application in real-world scenarios due to insufficient labeled samples.

Method: Constructing a semi-supervised image classification model using Generative Adversarial Networks (GANs) with collaborative training mechanism of generators, discriminators and classifiers.

Result: Effectively uses limited labeled data and large amounts of unlabeled data, improves image generation quality and classification accuracy.

Conclusion: Provides an effective solution for image recognition tasks in complex environments.

Abstract: In recent years, image classification, as a core task in computer vision,
relies on high-quality labelled data, which restricts the wide application of
deep learning models in practical scenarios. To alleviate the problem of
insufficient labelled samples, semi-supervised learning has gradually become a
research hotspot. In this paper, we construct a semi-supervised image
classification model based on Generative Adversarial Networks (GANs), and
through the introduction of the collaborative training mechanism of generators,
discriminators and classifiers, we achieve the effective use of limited
labelled data and a large amount of unlabelled data, improve the quality of
image generation and classification accuracy, and provide an effective solution
for the task of image recognition in complex environments.

</details>


### [717] [Two Causally Related Needles in a Video Haystack](https://arxiv.org/abs/2505.19853)
*Miaoyu Li,Qin Chao,Boyang Li*

Main category: cs.CV

TL;DR: 当前评估视频语言模型（VLMs）的视频理解能力存在重大挑战。本文提出了一个长上下文视频理解基准Causal2Needles，用于评估现有基准未充分评价的两个关键能力：从长视频的两个不同位置提取信息并联合理解它们的能力，以及根据人类行为对世界进行因果建模的能力。Causal2Needles引入了2-needle问题，要求从长视频及其相关叙述文本中提取因果人类行为事件的信息。实验表明，尽管某些模型在现有基准上表现出色，但在2-needle视觉基础任务上却表现不佳，且模型性能与两个针之间的距离呈负相关。这揭示了当前VLMs的关键局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解基准未能充分评估模型从长视频的不同位置提取信息并联合理解它们的能力，以及对人类行为进行因果建模的能力。因此，需要一个新的基准来更好地评估这些方面。

Method: 提出了一种名为Causal2Needles的长上下文视频理解基准，其中包括2-needle问题。这些问题要求从长视频及其相关的叙述文本中提取因果人类行为事件的信息。为了防止文本偏差，问题包括两种互补格式：一种是识别包含答案的视频片段，另一种是对该视频片段中的无关视觉细节进行文本描述。

Result: 实验结果表明，即使在现有基准上表现优异的模型，在处理2-needle视觉基础任务时也面临困难。此外，模型性能与两个针之间的距离呈负相关。

Conclusion: 当前的视频语言模型在理解和处理长视频中的因果关系和跨段信息方面存在显著的局限性。

Abstract: Evaluating the video understanding capabilities of Video-Language Models
(VLMs) remains a significant challenge. We propose a long-context video
understanding benchmark, Causal2Needles, that assesses two crucial abilities
insufficiently evaluated by existing benchmarks: (1) the ability to extract
information from two separate locations in a long video and understand them
jointly, and (2) the ability to model the world in terms of cause and effect in
human behaviors. Specifically, Causal2Needles introduces 2-needle questions,
which require extracting information from both the cause and effect
human-behavior events in a long video and the associated narration text. To
prevent textual bias, these questions comprise two complementary formats: one
asking to identify the video clip containing the answer, and one asking for the
textual description of an unrelated visual detail from that video clip. Our
experiments reveal that models excelling in pre-existing benchmarks struggle
with 2-needle visual grounding, and the model performance is negatively
correlated with the distance between the two needles. These findings highlight
critical limitations in current VLMs.

</details>


### [718] [StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation](https://arxiv.org/abs/2505.19874)
*Yi Wu,Lingting Zhu,Shengju Qian,Lei Liu,Wandi Qiao,Lequan Yu,Bin Li*

Main category: cs.CV

TL;DR: The paper introduces StyleAR, a new method for style-aligned text-to-image generation that effectively uses text-to-image binary data and overcomes challenges in acquiring triplet data with specific styles.


<details>
  <summary>Details</summary>
Motivation: Multimodal autoregressive (AR) models have shown great capabilities in various domains, but complex tasks like style-aligned text-to-image generation face significant challenges especially in data acquisition. This motivated the development of a solution to use conventional text-to-image data for generating style-aligned outputs.

Method: StyleAR combines a specially designed data curation method with AR models to utilize text-to-image binary data for style-aligned text-to-image generation. It synthesizes target stylized data using a reference style image and prompt, incorporates only the target stylized image as the image modality, uses a CLIP image encoder with a perceiver resampler, implements a style-enhanced token technique, and mixes raw images with stylized ones to enhance stylistic feature extraction and ensure style consistency.

Result: Extensive qualitative and quantitative experiments show superior performance of StyleAR in style-aligned text-to-image generation.

Conclusion: StyleAR effectively addresses the challenge of acquiring large volumes of triplet data with specific styles by utilizing conventional text-to-image binary data, leading to high-quality style-aligned text-to-image generation.

Abstract: In the current research landscape, multimodal autoregressive (AR) models have
shown exceptional capabilities across various domains, including visual
understanding and generation. However, complex tasks such as style-aligned
text-to-image generation present significant challenges, particularly in data
acquisition. In analogy to instruction-following tuning for image editing of AR
models, style-aligned generation requires a reference style image and prompt,
resulting in a text-image-to-image triplet where the output shares the style
and semantics of the input. However, acquiring large volumes of such triplet
data with specific styles is considerably more challenging than obtaining
conventional text-to-image data used for training generative models. To address
this issue, we propose StyleAR, an innovative approach that combines a
specially designed data curation method with our proposed AR models to
effectively utilize text-to-image binary data for style-aligned text-to-image
generation. Our method synthesizes target stylized data using a reference style
image and prompt, but only incorporates the target stylized image as the image
modality to create high-quality binary data. To facilitate binary data
training, we introduce a CLIP image encoder with a perceiver resampler that
translates the image input into style tokens aligned with multimodal tokens in
AR models and implement a style-enhanced token technique to prevent content
leakage which is a common issue in previous work. Furthermore, we mix raw
images drawn from large-scale text-image datasets with stylized images to
enhance StyleAR's ability to extract richer stylistic features and ensure style
consistency. Extensive qualitative and quantitative experiments demonstrate our
superior performance.

</details>


### [719] [Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging](https://arxiv.org/abs/2505.19603)
*Ho Hin Lee,Quan Liu,Shunxing Bao,Yuankai Huo,Bennett A. Landman*

Main category: cs.CV

TL;DR: The paper proposes Rep3D, a 3D convolutional framework that incorporates a learnable spatial prior into large kernel training to improve optimization stability and performance in high-resolution 3D volumetric data.


<details>
  <summary>Details</summary>
Motivation: Vision transformers model long-range dependencies through global self-attention but are computationally expensive. Large kernel convolutions offer an efficient alternative, but naive increases in kernel size lead to optimization instability and degradation in performance.

Method: Rep3D introduces a re-parameterized convolution block that induces spatially varying learning rates. A two-stage modulation network generates a receptive-biased scaling mask to adaptively re-weight kernel updates, enabling local-to-global convergence behavior. The framework uses plain encoder design with large depthwise convolutions.

Result: Rep3D shows consistent improvements over state-of-the-art baselines on five challenging 3D segmentation benchmarks, including those based on transformers and fixed-prior re-parameterization methods.

Conclusion: Rep3D provides an interpretable and scalable solution for 3D medical image analysis by unifying spatial inductive bias with optimization-aware learning.

Abstract: In contrast to vision transformers, which model long-range dependencies
through global self-attention, large kernel convolutions provide a more
efficient and scalable alternative, particularly in high-resolution 3D
volumetric settings. However, naively increasing kernel size often leads to
optimization instability and degradation in performance. Motivated by the
spatial bias observed in effective receptive fields (ERFs), we hypothesize that
different kernel elements converge at variable rates during training. To
support this, we derive a theoretical connection between element-wise gradients
and first-order optimization, showing that structurally re-parameterized
convolution blocks inherently induce spatially varying learning rates. Building
on this insight, we introduce Rep3D, a 3D convolutional framework that
incorporates a learnable spatial prior into large kernel training. A
lightweight two-stage modulation network generates a receptive-biased scaling
mask, adaptively re-weighting kernel updates and enabling local-to-global
convergence behavior. Rep3D adopts a plain encoder design with large depthwise
convolutions, avoiding the architectural complexity of multi-branch
compositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks
and demonstrate consistent improvements over state-of-the-art baselines,
including transformer-based and fixed-prior re-parameterization methods. By
unifying spatial inductive bias with optimization-aware learning, Rep3D offers
an interpretable, and scalable solution for 3D medical image analysis. The
source code is publicly available at https://github.com/leeh43/Rep3D.

</details>


### [720] [A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks](https://arxiv.org/abs/2505.19920)
*Sebastian Groß,Stefan Heindorf,Philipp Terhörst*

Main category: cs.CV

TL;DR: A new face recognition approach called MOTE uses personalized small neural networks instead of traditional vector-based templates, improving fairness and privacy but at the cost of increased inference time and storage needs.


<details>
  <summary>Details</summary>
Motivation: Traditional face recognition systems use fixed representations (templates) generated by neural networks that lack explainability and raise concerns about fairness and privacy.

Method: MOTE replaces vector-based face templates with small personalized neural networks. For each identity, it creates a dedicated binary classifier trained on a single reference sample and synthetically balanced samples to match the enrolled identity.

Result: Experiments across multiple datasets and systems show significant improvements in fairness and privacy, making it suitable for small- and mid-scale applications where these factors are critical.

Conclusion: Although MOTE increases inference time and storage requirements, it provides a strong solution for enhancing fairness and privacy in smaller scale face recognition systems.

Abstract: Traditional face recognition systems rely on extracting fixed face
representations, known as templates, to store and verify identities. These
representations are typically generated by neural networks that often lack
explainability and raise concerns regarding fairness and privacy. In this work,
we propose a novel model-template (MOTE) approach that replaces vector-based
face templates with small personalized neural networks. This design enables
more responsible face recognition for small and medium-scale systems. During
enrollment, MOTE creates a dedicated binary classifier for each identity,
trained to determine whether an input face matches the enrolled identity. Each
classifier is trained using only a single reference sample, along with
synthetically balanced samples to allow adjusting fairness at the level of a
single individual during enrollment. Extensive experiments across multiple
datasets and recognition systems demonstrate substantial improvements in
fairness and particularly in privacy. Although the method increases inference
time and storage requirements, it presents a strong solution for small- and
mid-scale applications where fairness and privacy are critical.

</details>


### [721] [Can Visual Encoder Learn to See Arrows?](https://arxiv.org/abs/2505.19944)
*Naoyuki Terashita,Yusuke Tozaki,Hideaki Omote,Congkha Nguyen,Ryosuke Nakamoto,Yuta Koreeda,Hiroaki Ozaki*

Main category: cs.CV

TL;DR: 通过消除文本和位置偏差，可以促进视觉语言模型(VLMs)准确识别边缘，从而提升图表理解能力。本文方法在多个任务中超越了预训练的CLIP、GPT-4o和LLaVA-Mistral等模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型(VLMs)无法有效识别图像中的边缘，这限制了它们对特定领域知识的理解。问题可能源于模型过度依赖文本和位置偏差，而未能学习到明确的边缘特征。

Method: 在人工生成的图表-标题数据集上进行对比学习，以训练VLMs的图像编码器，并评估其在三项任务（探测、图像检索和标题生成）中的图表相关特性。训练过程中，使用的图表数据集消除了文本和位置信息的偏差。

Result: 微调后的模型在所有任务中均优于预训练的CLIP，并且在标题生成任务中超过了零样本的GPT-4o和LLaVA-Mistral。

Conclusion: 消除文本和位置偏差有助于提高VLMs的边缘识别能力，为推进图表理解提供了有希望的方向。

Abstract: The diagram is a visual representation of a relationship illustrated with
edges (lines or arrows), which is widely used in industrial and scientific
communication. Although recognizing diagrams is essential for vision language
models (VLMs) to comprehend domain-specific knowledge, recent studies reveal
that many VLMs fail to identify edges in images. We hypothesize that these
failures stem from an over-reliance on textual and positional biases,
preventing VLMs from learning explicit edge features. Based on this idea, we
empirically investigate whether the image encoder in VLMs can learn edge
representation through training on a diagram dataset in which edges are biased
neither by textual nor positional information. To this end, we conduct
contrastive learning on an artificially generated diagram--caption dataset to
train an image encoder and evaluate its diagram-related features on three
tasks: probing, image retrieval, and captioning. Our results show that the
finetuned model outperforms pretrained CLIP in all tasks and surpasses
zero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings
confirm that eliminating textual and positional biases fosters accurate edge
recognition in VLMs, offering a promising path for advancing diagram
understanding.

</details>


### [722] [SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection](https://arxiv.org/abs/2505.19948)
*Gokul Adethya,Bhanu Pratyush Mantha,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: The paper proposes a Self-augmented and Self-interpreted (SaSi) deep learning approach for few-shot particle detection in 3D cryo-ET images, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Cryo-electron tomography has difficulty localizing 3D particles in cellular environments due to low signal-to-noise ratios and missing wedge artifacts. Deep learning approaches have shown potential but are often limited by the lack of labeled data in cryo-ET scenarios.

Method: The SaSi approach uses self-augmentation techniques to improve data utilization and introduces a self-interpreted segmentation strategy to reduce dependency on labeled data, enhancing generalization and robustness.

Result: Experiments on both simulated and real-world cryo-ET datasets demonstrate that the SaSi approach significantly outperforms current state-of-the-art methods for particle localization.

Conclusion: This research advances the understanding of particle detection with minimal labels in cryo-ET and establishes a new benchmark for few-shot learning in structural biology.

Abstract: Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for
imaging macromolecular complexes in their near-native states. However, the
localization of 3D particles in cellular environments still presents a
significant challenge due to low signal-to-noise ratios and missing wedge
artifacts. Deep learning approaches have shown great potential, but they need
huge amounts of data, which can be a challenge in cryo-ET scenarios where
labeled data is often scarce. In this paper, we propose a novel Self-augmented
and Self-interpreted (SaSi) deep learning approach towards few-shot particle
detection in 3D cryo-ET images. Our method builds upon self-augmentation
techniques to further boost data utilization and introduces a self-interpreted
segmentation strategy for alleviating dependency on labeled data, hence
improving generalization and robustness. As demonstrated by experiments
conducted on both simulated and real-world cryo-ET datasets, the SaSi approach
significantly outperforms existing state-of-the-art methods for particle
localization. This research increases understanding of how to detect particles
with very few labels in cryo-ET and thus sets a new benchmark for few-shot
learning in structural biology.

</details>


### [723] [Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models](https://arxiv.org/abs/2505.20021)
*Hyunsik Chae,Seungwoo Yoon,Jaden Park,Chloe Yewon Chun,Yongin Cho,Mu Cai,Yong Jae Lee,Ernest K. Ryu*

Main category: cs.CV

TL;DR: 尽管最先进的视觉-语言模型（VLMs）在复杂的多模态任务中表现出色，但在基本的2D欧几里得几何任务上却表现不佳。本文提出原子视觉技能数据集（AVSD），用于评估VLMs在基础视觉技能上的表现，并发现这些模型在此类简单任务上存在困难，强调了针对原子视觉感知任务训练和评估VLMs的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在复杂多模态任务中表现出色，但在简单视觉任务上表现不佳，特别是在基本2D欧几里得几何领域。

Method: 研究者首先定义并系统分类了基本2D欧几里得几何中的原子视觉技能，然后引入了一个新的数据集——原子视觉技能数据集（AVSD），用于评估VLMs在这些技能上的表现。

Result: 使用AVSD对最先进的VLMs进行基准测试后发现，这些模型在原子视觉技能任务上表现不佳，尽管这些任务对人类来说非常简单。

Conclusion: 需要专用的数据集来训练和评估VLMs在原子视觉感知任务上的能力，而不仅仅是关注复合任务。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal
comprehension and reasoning capabilities, yet they often struggle with
trivially simple visual tasks. In this work, we focus on the domain of basic 2D
Euclidean geometry and systematically categorize the fundamental, indivisible
visual perception skills, which we refer to as atomic visual skills. We then
introduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the
atomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find
that they struggle with these tasks, despite being trivial for adult humans.
Our findings highlight the need for purpose-built datasets to train and
evaluate VLMs on atomic, rather than composite, visual perception tasks.

</details>


### [724] [ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving](https://arxiv.org/abs/2505.20024)
*Xueyi Liu,Zuodong Zhong,Yuxin Guo,Yun-Fu Liu,Zhiguo Su,Qichao Zhang,Junli Wang,Yinfeng Gao,Yupeng Zheng,Qiao Lin,Huiyong Chen,Dongbin Zhao*

Main category: cs.CV

TL;DR: The paper presents ReasonPlan, a novel MLLM fine-tuning framework for closed-loop driving which outperforms mainstream E2E imitation learning methods and shows strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models have gained attention in end-to-end autonomous driving but their application to closed-loop systems is underexplored and they haven't shown superiority over mainstream imitation learning approaches.

Method: Proposes ReasonPlan, a framework with dual mechanism of self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process to align visual representations with driving context and promote interpretable decision making. Also curates a planning-oriented decision reasoning dataset called PDR.

Result: Outperforms mainstream E2E imitation learning method by 19% L2 and 16.1 driving score on Bench2Drive benchmark and demonstrates strong zero-shot generalization on unseen DOS benchmark.

Conclusion: ReasonPlan shows significant improvement over existing methods and strong adaptability in handling unseen corner cases.

Abstract: Due to the powerful vision-language reasoning and generalization abilities,
multimodal large language models (MLLMs) have garnered significant attention in
the field of end-to-end (E2E) autonomous driving. However, their application to
closed-loop systems remains underexplored, and current MLLM-based methods have
not shown clear superiority to mainstream E2E imitation learning approaches. In
this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed
for closed-loop driving through holistic reasoning with a self-supervised Next
Scene Prediction task and supervised Decision Chain-of-Thought process. This
dual mechanism encourages the model to align visual representations with
actionable driving context, while promoting interpretable and causally grounded
decision making. We curate a planning-oriented decision reasoning dataset,
namely PDR, comprising 210k diverse and high-quality samples. Our method
outperforms the mainstream E2E imitation learning method by a large margin of
19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan
demonstrates strong zero-shot generalization on unseen DOS benchmark,
highlighting its adaptability in handling zero-shot corner cases. Code and
dataset will be found in https://github.com/Liuxueyi/ReasonPlan.

</details>


### [725] [FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields](https://arxiv.org/abs/2505.19863)
*Lukas Meyer,Andrei-Timotei Ardelean,Tim Weyrich,Marc Stamminger*

Main category: cs.CV

TL;DR: The paper presents FruitNeRF++, a new fruit-counting method that combines contrastive learning with neural radiance fields for counting fruits from unstructured orchard photographs. It improves upon FruitNeRF by designing a shape-agnostic multi-fruit counting framework, utilizing instance masks predicted by a vision foundation model to encode fruit identity as instance embeddings into a neural instance field. The approach is evaluated on synthetic and real-world datasets, showing better controllability and performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of FruitNeRF which requires adaptation for each fruit type, making it less practical.

Method: FruitNeRF++ uses contrastive learning combined with neural radiance fields. A shape-agnostic multi-fruit counting framework is designed, complementing RGB and semantic data with instance masks predicted by a vision foundation model. These masks encode fruit identity as instance embeddings in a neural instance field. By volumetrically sampling the neural fields, a point cloud embedded with instance features is extracted and clustered in a fruit-agnostic manner to obtain the fruit count.

Result: Evaluated using a synthetic dataset with various fruits and a real-world apple dataset, demonstrating that FruitNeRF++ is easier to control and compares favorably to other state-of-the-art methods.

Conclusion: FruitNeRF++ provides an improved method for fruit counting from unstructured orchard photographs, overcoming the limitations of its predecessor and offering better performance.

Abstract: We introduce FruitNeRF++, a novel fruit-counting approach that combines
contrastive learning with neural radiance fields to count fruits from
unstructured input photographs of orchards. Our work is based on FruitNeRF,
which employs a neural semantic field combined with a fruit-specific clustering
approach. The requirement for adaptation for each fruit type limits the
applicability of the method, and makes it difficult to use in practice. To lift
this limitation, we design a shape-agnostic multi-fruit counting framework,
that complements the RGB and semantic data with instance masks predicted by a
vision foundation model. The masks are used to encode the identity of each
fruit as instance embeddings into a neural instance field. By volumetrically
sampling the neural fields, we extract a point cloud embedded with the instance
features, which can be clustered in a fruit-agnostic manner to obtain the fruit
count. We evaluate our approach using a synthetic dataset containing apples,
plums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark
apple dataset. Our results demonstrate that FruitNeRF++ is easier to control
and compares favorably to other state-of-the-art methods.

</details>


### [726] [EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition](https://arxiv.org/abs/2505.20033)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Maurice Kraus,Felix Friedrich,Huu Nguyen,Krishna Kalyan,Kourosh Nadi,Kristian Kersting,Sören Auer*

Main category: cs.CV

TL;DR: To improve human-AI interaction, this paper introduces EmoNet Face which includes a new emotion taxonomy, three large AI-generated datasets, rigorous annotations, and a model named Empathic Insight Face that performs at human-expert level.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for vision and vision-language models have limitations in emotional spectrum, nuanced states, and demographic diversity, risking significant bias.

Method: Developed EmoNet Face with a 40-category emotion taxonomy, three large-scale AI-generated datasets (EmoNet HQ, Binary, and Big), multi-expert annotations, and the model Empathic Insight Face.

Result: Empathic Insight Face model achieves human-expert-level performance on the EmoNet Face benchmark.

Conclusion: The publicly released EmoNet Face suite provides a strong foundation for creating AI systems with a deeper understanding of human emotions.

Abstract: Effective human-AI interaction relies on AI's ability to accurately perceive
and interpret human emotions. Current benchmarks for vision and vision-language
models are severely limited, offering a narrow emotional spectrum that
overlooks nuanced states (e.g., bitterness, intoxication) and fails to
distinguish subtle differences between related feelings (e.g., shame vs.
embarrassment). Existing datasets also often use uncontrolled imagery with
occluded faces and lack demographic diversity, risking significant bias. To
address these critical gaps, we introduce EmoNet Face, a comprehensive
benchmark suite. EmoNet Face features: (1) A novel 40-category emotion
taxonomy, meticulously derived from foundational research to capture finer
details of human emotional experiences. (2) Three large-scale, AI-generated
datasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and
controlled demographic balance across ethnicity, age, and gender. (3) Rigorous,
multi-expert annotations for training and high-fidelity evaluation. (4) We
build Empathic Insight Face, a model achieving human-expert-level performance
on our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,
and model - provides a robust foundation for developing and evaluating AI
systems with a deeper understanding of human emotions.

</details>


### [727] [Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion](https://arxiv.org/abs/2505.20053)
*Zheqi Lv,Junhao Chen,Qi Tian,Keting Yin,Shengyu Zhang,Fei Wu*

Main category: cs.CV

TL;DR: Diffusion models are great for text-to-image generation, but they often lack effective semantic supervision during the denoising process. This paper proposes PPAD, which uses a Multimodal Large Language Model (MLLM) to provide real-time semantic analysis and correction during inference. It helps improve prompt-image alignment and image quality by actively guiding the generative process.


<details>
  <summary>Details</summary>
Motivation: To address the issues of object confusion, spatial errors, inaccurate counts, and missing semantic elements in current diffusion models that arise due to the absence of interpretable semantic supervision and correction mechanisms throughout the denoising process.

Method: Propose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a framework that introduces a Multimodal Large Language Model (MLLM) as a semantic observer during inference. PPAD performs real-time analysis on intermediate generations, identifies latent semantic inconsistencies, and translates feedback into controllable signals that actively guide the remaining denoising steps.

Result: Extensive experiments demonstrate PPAD's significant improvements in both inference-only and training-enhanced settings, with strong generality and scalability.

Conclusion: PPAD successfully introduces MLLM as a semantic observer in diffusion models, providing actionable guidance for correcting the generative trajectory and improving prompt-image alignment and image quality.

Abstract: Diffusion models have become the mainstream architecture for text-to-image
generation, achieving remarkable progress in visual quality and prompt
controllability. However, current inference pipelines generally lack
interpretable semantic supervision and correction mechanisms throughout the
denoising process. Most existing approaches rely solely on post-hoc scoring of
the final image, prompt filtering, or heuristic resampling strategies-making
them ineffective in providing actionable guidance for correcting the generative
trajectory. As a result, models often suffer from object confusion, spatial
errors, inaccurate counts, and missing semantic elements, severely compromising
prompt-image alignment and image quality. To tackle these challenges, we
propose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel
framework that, for the first time, introduces a Multimodal Large Language
Model (MLLM) as a semantic observer during inference. PPAD performs real-time
analysis on intermediate generations, identifies latent semantic
inconsistencies, and translates feedback into controllable signals that
actively guide the remaining denoising steps. The framework supports both
inference-only and training-enhanced settings, and performs semantic correction
at only extremely few diffusion steps, offering strong generality and
scalability. Extensive experiments demonstrate PPAD's significant improvements.

</details>


### [728] [ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers](https://arxiv.org/abs/2505.20032)
*Fotios Lygerakis,Ozan Özdenizci,Elmar Rückert*

Main category: cs.CV

TL;DR: Tactile sensing complements visual perception, but modality fusion and generalization remain challenging. ViTaPEs is a transformer-based framework that integrates visual and tactile data with multi-scale positional encoding, providing provable guarantees in visuotactile fusion and showing strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Despite recent advances in visuotactile representation learning, challenges persist in effectively fusing visual and tactile modalities and generalizing across tasks and environments without heavy reliance on pre-trained vision-language models. Moreover, existing methods do not study positional encodings, which are crucial for capturing fine-grained visuotactile correlations.

Method: ViTaPEs exploits a novel multi-scale positional encoding scheme to capture intra-modal structures while simultaneously modeling cross-modal cues. The framework provides provable guarantees in visuotactile fusion, including injective, rigid-motion-equivariant, and information-preserving properties.

Result: Experiments on multiple large-scale real-world datasets demonstrate that ViTaPEs surpasses state-of-the-art baselines across various recognition tasks and shows zero-shot generalization to unseen, out-of-domain scenarios. Additionally, ViTaPEs demonstrates transfer-learning strength in a robotic grasping task, outperforming state-of-the-art baselines in predicting grasp success.

Conclusion: ViTaPEs robustly integrates visual and tactile input data to learn task-agnostic representations for visuotactile perception, providing a significant advancement in the field with its multi-scale positional encoding and provable guarantees.

Abstract: Tactile sensing provides local essential information that is complementary to
visual perception, such as texture, compliance, and force. Despite recent
advances in visuotactile representation learning, challenges remain in fusing
these modalities and generalizing across tasks and environments without heavy
reliance on pre-trained vision-language models. Moreover, existing methods do
not study positional encodings, thereby overlooking the multi-scale spatial
reasoning needed to capture fine-grained visuotactile correlations. We
introduce ViTaPEs, a transformer-based framework that robustly integrates
visual and tactile input data to learn task-agnostic representations for
visuotactile perception. Our approach exploits a novel multi-scale positional
encoding scheme to capture intra-modal structures, while simultaneously
modeling cross-modal cues. Unlike prior work, we provide provable guarantees in
visuotactile fusion, showing that our encodings are injective,
rigid-motion-equivariant, and information-preserving, validating these
properties empirically. Experiments on multiple large-scale real-world datasets
show that ViTaPEs not only surpasses state-of-the-art baselines across various
recognition tasks but also demonstrates zero-shot generalization to unseen,
out-of-domain scenarios. We further demonstrate the transfer-learning strength
of ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art
baselines in predicting grasp success. Project page:
https://sites.google.com/view/vitapes

</details>


### [729] [AdaTP: Attention-Debiased Token Pruning for Video Large Language Models](https://arxiv.org/abs/2505.20100)
*Fengyuan Sun,Leqi Shen,Hui Chen,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: Video Large Language Models (Video LLMs) excel in video understanding but face computational challenges. Existing token compression methods have attention biases. We propose AdaTP, a novel method with debiasing modules to reduce computational overhead without performance loss. It achieves state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Video LLMs are powerful for video understanding tasks but suffer from heavy computational overhead due to numerous visual tokens. Current token compression techniques based on attention scores introduce inherent global and local biases.

Method: AdaTP is introduced as a token pruning pipeline incorporating two debiasing modules targeting global and local attention biases respectively, reducing computational overhead without additional training.

Result: AdaTP demonstrates state-of-the-art performance across various video understanding benchmarks, notably maintaining performance on LLaVA-OneVision-7B while using only 27.3% FLOPs compared to the original model.

Conclusion: AdaTP effectively mitigates attention biases in Video LLMs, significantly reducing computational costs while preserving model performance.

Abstract: Video Large Language Models (Video LLMs) have achieved remarkable results in
video understanding tasks. However, they often suffer from heavy computational
overhead due to the large number of visual tokens generated from multiple video
frames. Existing visual token compression methods often rely on attention
scores from language models as guidance. However, these scores exhibit inherent
biases: global bias reflects a tendency to focus on the two ends of the visual
token sequence, while local bias leads to an over-concentration on the same
spatial positions across different frames. To address the issue of attention
bias, we propose $\textbf{A}$ttention-$\textbf{D}$ebi$\textbf{a}$sed
$\textbf{T}$oken $\textbf{P}$runing for Video Large Language Models
($\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP
integrates two dedicated debiasing modules into the pipeline, targeting global
attention bias and local attention bias, respectively. Without the need for
additional training, our method significantly reduces the computational
overhead of Video LLMs while retaining the performance of vanilla models.
Extensive evaluation shows that AdaTP achieves state-of-the-art performance in
various commonly used video understanding benchmarks. In particular, on
LLaVA-OneVision-7B, AdaTP maintains performance without degradation while using
only up to $27.3\%$ FLOPs compared to the vanilla model. Our code will be
released soon.

</details>


### [730] [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
*Kai Sun,Yushi Bai,Zhen Yang,Jiajie Zhang,Ji Qi,Lei Hou,Juanzi Li*

Main category: cs.CV

TL;DR: The paper proposes a hard negative contrastive learning framework for vision encoders to improve geometric understanding in LMMs, leading to the development of MMGeoLM which outperforms other models on geometric reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LMMs have limitations in meticulous reasoning and geometric problem-solving due to the inherent restrictions of contrastive learning on summarized descriptions.

Method: A novel hard negative contrastive learning framework is proposed, combining image-based and text-based contrastive learning using different types of negatives. CLIP is trained with this method (named MMCLIP), followed by training an LMM for geometric problem-solving.

Result: Experiments demonstrate that the trained model MMGeoLM significantly surpasses other open-source models on three geometric reasoning benchmarks and can rival closed-source models like GPT-4o even at a size of 7B.

Conclusion: The study reveals valuable insights into the impact of various negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMMs.

Abstract: Benefiting from contrastively trained visual encoders on large-scale natural
scene images, Large Multimodal Models (LMMs) have achieved remarkable
performance across various visual perception tasks. However, the inherent
limitations of contrastive learning upon summarized descriptions fundamentally
restrict the capabilities of models in meticulous reasoning, particularly in
crucial scenarios of geometric problem-solving. To enhance geometric
understanding, we propose a novel hard negative contrastive learning framework
for the vision encoder, which combines image-based contrastive learning using
generation-based hard negatives created by perturbing diagram generation code,
and text-based contrastive learning using rule-based negatives derived from
modified geometric descriptions and retrieval-based negatives selected based on
caption similarity. We train CLIP using our strong negative learning method,
namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for
geometric problem-solving. Experiments show that our trained model, MMGeoLM,
significantly outperforms other open-source models on three geometric reasoning
benchmarks. Even with a size of 7B, it can rival powerful closed-source models
like GPT-4o. We further study the impact of different negative sample
construction methods and the number of negative samples on the geometric
reasoning performance of LMM, yielding fruitful conclusions. The code and
dataset are available at https://github.com/THU-KEG/MMGeoLM.

</details>


### [731] [In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation](https://arxiv.org/abs/2505.20271)
*Yu Xu,Fan Tang,You Wu,Lin Gao,Oliver Deussen,Hongbin Yan,Jintao Li,Juan Cao,Tong-Yee Lee*

Main category: cs.CV

TL;DR: 提出了一种名为“In-Context Brush”的零样本框架，用于通过跨模态演示和双层潜在空间操作实现定制化主体插入，无需模型微调即可达到高保真度和文本对齐的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在多模态引导的视觉生成方面取得进展，但在通过文本提示插入定制化主体时，往往难以保持高保真度并准确对齐用户意图。

Method: 将对象图像和文本提示作为跨模态演示，目标图像与遮罩区域作为查询，基于预训练的MMDiT inpainting网络，通过测试时增强技术进行双重潜在空间操作：1) 每个注意力头内的“潜在特征移动”以反映所需主体语义；2) 跨不同头部的“注意力重新加权”以增强提示可控性。

Result: 广泛的实验和应用表明，该方法在身份保留、文本对齐和图像质量方面优于现有最先进方法，且无需专门训练或额外数据收集。

Conclusion: In-Context Brush提供了一种有效的零样本解决方案，用于定制化主体插入任务，具有高保真度和良好的文本对齐能力，同时避免了模型微调的需要。

Abstract: Recent advances in diffusion models have enhanced multimodal-guided visual
generation, enabling customized subject insertion that seamlessly "brushes"
user-specified objects into a given image guided by textual prompts. However,
existing methods often struggle to insert customized subjects with high
fidelity and align results with the user's intent through textual prompts. In
this work, we propose "In-Context Brush", a zero-shot framework for customized
subject insertion by reformulating the task within the paradigm of in-context
learning. Without loss of generality, we formulate the object image and the
textual prompts as cross-modal demonstrations, and the target image with the
masked region as the query. The goal is to inpaint the target image with the
subject aligning textual prompts without model tuning. Building upon a
pretrained MMDiT-based inpainting network, we perform test-time enhancement via
dual-level latent space manipulation: intra-head "latent feature shifting"
within each attention head that dynamically shifts attention outputs to reflect
the desired subject semantics and inter-head "attention reweighting" across
different heads that amplifies prompt controllability through differential
attention prioritization. Extensive experiments and applications demonstrate
that our approach achieves superior identity preservation, text alignment, and
image quality compared to existing state-of-the-art methods, without requiring
dedicated training or additional data collection.

</details>


### [732] [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation](https://arxiv.org/abs/2505.20292)
*Shenghai Yuan,Xianyi He,Yufan Deng,Yang Ye,Jinfa Huang,Bin Lin,Chongyang Ma,Jiebo Luo,Li Yuan*

Main category: cs.CV

TL;DR: The paper introduces OpenS2V-Nexus for Subject-to-Video (S2V) generation, which includes OpenS2V-Eval as a fine-grained benchmark and OpenS2V-5M as a large-scale dataset. It proposes three automatic metrics to evaluate S2V models' performance in subject consistency, naturalness, and text relevance. The authors conduct a comprehensive evaluation of 16 S2V models and create an open-source large-scale S2V generation dataset.


<details>
  <summary>Details</summary>
Motivation: To provide enhanced flexibility in video production by creating videos that faithfully incorporate reference content and establishing a robust infrastructure for S2V generation research.

Method: Propose OpenS2V-Nexus consisting of OpenS2V-Eval (a fine-grained benchmark with 180 prompts from seven major S2V categories) and OpenS2V-5M (a large-scale dataset of five million high-quality subject-text-video triples). Introduce three automatic metrics: NexusScore, NaturalScore, and GmeScore to separately quantify subject consistency, naturalness, and text relevance in generated videos.

Result: Comprehensive evaluation of 16 representative S2V models highlighting their strengths and weaknesses across different content. Creation of the first open-source large-scale S2V generation dataset ensuring subject-information diversity via specific techniques.

Conclusion: OpenS2V-Nexus delivers a robust infrastructure to accelerate future S2V generation research.

Abstract: Subject-to-Video (S2V) generation aims to create videos that faithfully
incorporate reference content, providing enhanced flexibility in the production
of videos. To establish the infrastructure for S2V generation, we propose
OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and
(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V
benchmarks inherited from VBench that focus on global and coarse-grained
assessment of generated videos, OpenS2V-Eval focuses on the model's ability to
generate subject-consistent videos with natural subject appearance and identity
fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven
major categories of S2V, which incorporate both real and synthetic test data.
Furthermore, to accurately align human preferences with S2V benchmarks, we
propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to
separately quantify subject consistency, naturalness, and text relevance in
generated videos. Building on this, we conduct a comprehensive evaluation of 16
representative S2V models, highlighting their strengths and weaknesses across
different content. Moreover, we create the first open-source large-scale S2V
generation dataset OpenS2V-5M, which consists of five million high-quality 720P
subject-text-video triples. Specifically, we ensure subject-information
diversity in our dataset by (1) segmenting subjects and building pairing
information via cross-video associations and (2) prompting GPT-Image-1 on raw
frames to synthesize multi-view representations. Through OpenS2V-Nexus, we
deliver a robust infrastructure to accelerate future S2V generation research.

</details>


### [733] [GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes](https://arxiv.org/abs/2505.20294)
*Xiao Chen,Tai Wang,Quanyi Li,Tao Huang,Jiangmiao Pang,Tianfan Xue*

Main category: cs.CV

TL;DR: GLEAM, a unified generalizable exploration policy for active mapping in complex environments which significantly outperforms current methods with 66.50% coverage on unseen scenes.


<details>
  <summary>Details</summary>
Motivation: Generalizable active mapping in complex unknown environments is a critical challenge for mobile robots due to insufficient training data and conservative exploration strategies leading to limited generalizability across diverse layouts and complex connectivity.

Method: Introduced GLEAM-Bench, the first large-scale benchmark for generalizable active mapping with 1,152 diverse 3D scenes. Proposed GLEAM, which uses semantic representations, long-term navigable goals, and randomized strategies.

Result: Achieved 66.50% coverage (+9.49%) with efficient trajectories and improved mapping accuracy on 128 unseen complex scenes.

Conclusion: GLEAM exhibits superior generalizability and significantly outperforms state-of-the-art methods in active mapping.

Abstract: Generalizable active mapping in complex unknown environments remains a
critical challenge for mobile robots. Existing methods, constrained by
insufficient training data and conservative exploration strategies, exhibit
limited generalizability across scenes with diverse layouts and complex
connectivity. To enable scalable training and reliable evaluation, we introduce
GLEAM-Bench, the first large-scale benchmark designed for generalizable active
mapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.
Building upon this foundation, we propose GLEAM, a unified generalizable
exploration policy for active mapping. Its superior generalizability comes
mainly from our semantic representations, long-term navigable goals, and
randomized strategies. It significantly outperforms state-of-the-art methods,
achieving 66.50% coverage (+9.49%) with efficient trajectories and improved
mapping accuracy on 128 unseen complex scenes. Project page:
https://xiao-chen.tech/gleam/.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [734] [Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control](https://arxiv.org/abs/2505.18279)
*Alireza Rezazadeh,Zichao Li,Ange Lou,Yuying Zhao,Wei Wei,Yujia Bao*

Main category: cs.MA

TL;DR: Complex tasks are delegated to specialized LLM-based agents. A new framework, Collaborative Memory, is introduced for multi-user, multi-agent environments with asymmetric access controls. It maintains private and shared memory tiers, enforces read/write policies, and ensures safe knowledge sharing.


<details>
  <summary>Details</summary>
Motivation: To address the challenges and benefits of knowledge transfer across users in dynamic, asymmetric permission contexts within multi-agent systems.

Method: Introduced Collaborative Memory, a framework featuring two memory tiers (private and shared memory) and granular read/write policies encoded as bipartite graphs linking users, agents, and resources.

Result: Enables safe, efficient, and interpretable cross-user knowledge sharing with adherence to asymmetric, time-varying policies and full auditability.

Conclusion: Collaborative Memory provides a robust solution for managing complex multi-user, multi-agent interactions with provable policy adherence and full auditability.

Abstract: Complex tasks are increasingly delegated to ensembles of specialized
LLM-based agents that reason, communicate, and coordinate actions-both among
themselves and through interactions with external tools, APIs, and databases.
While persistent memory has been shown to enhance single-agent performance,
most approaches assume a monolithic, single-user context-overlooking the
benefits and challenges of knowledge transfer across users under dynamic,
asymmetric permissions. We introduce Collaborative Memory, a framework for
multi-user, multi-agent environments with asymmetric, time-evolving access
controls encoded as bipartite graphs linking users, agents, and resources. Our
system maintains two memory tiers: (1) private memory-private fragments visible
only to their originating user; and (2) shared memory-selectively shared
fragments. Each fragment carries immutable provenance attributes (contributing
agents, accessed resources, and timestamps) to support retrospective permission
checks. Granular read policies enforce current user-agent-resource constraints
and project existing memory fragments into filtered transformed views. Write
policies determine fragment retention and sharing, applying context-aware
transformations to update the memory. Both policies may be designed conditioned
on system, agent, and user-level information. Our framework enables safe,
efficient, and interpretable cross-user knowledge sharing, with provable
adherence to asymmetric, time-varying policies and full auditability of memory
operations.

</details>


### [735] [Single-agent or Multi-agent Systems? Why Not Both?](https://arxiv.org/abs/2505.18286)
*Mingyan Gao,Yanzi Li,Banruo Liu,Yifan Yu,Phillip Wang,Ching-Yu Lin,Fan Lai*

Main category: cs.MA

TL;DR: With the improvement of LLM capabilities, the advantages of MAS over SAS are decreasing. This paper proposes a hybrid paradigm of request cascading between MAS and SAS to enhance efficiency and capability, which improves accuracy and reduces deployment costs.


<details>
  <summary>Details</summary>
Motivation: To explore the diminishing benefits of multi-agent systems (MAS) over single-agent systems (SAS) as large language model (LLM) capabilities improve, and to propose efficient mechanisms for error-prone agent identification in MAS.

Method: Conduct an extensive empirical study comparing MAS and SAS across various popular agentic applications, propose mechanisms to identify error-prone agents in MAS, and design a hybrid agentic paradigm of request cascading between MAS and SAS.

Result: The performance gap between MAS and SAS is narrowing with the advancement of LLMs. The proposed hybrid paradigm improves accuracy by 1.1-12% and reduces deployment costs by up to 20% across different applications.

Conclusion: As LLM capabilities improve, the advantages of MAS over SAS diminish. A hybrid agentic paradigm combining MAS and SAS can enhance both efficiency and capability.

Abstract: Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to
different large language model (LLM) agents and tools. Prior studies have
reported the superior accuracy performance of MAS across diverse domains,
enabled by long-horizon context tracking and error correction through
role-specific agents. However, the design and deployment of MAS incur higher
complexity and runtime cost compared to single-agent systems (SAS). Meanwhile,
frontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in
long-context reasoning, memory retention, and tool usage, mitigating many
limitations that originally motivated MAS designs. In this paper, we conduct an
extensive empirical study comparing MAS and SAS across various popular agentic
applications. We find that the benefits of MAS over SAS diminish as LLM
capabilities improve, and we propose efficient mechanisms to pinpoint the
error-prone agent in MAS. Furthermore, the performance discrepancy between MAS
and SAS motivates our design of a hybrid agentic paradigm, request cascading
between MAS and SAS, to improve both efficiency and capability. Our design
improves accuracy by 1.1-12% while reducing deployment costs by up to 20%
across various agentic applications.

</details>


### [736] [An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems](https://arxiv.org/abs/2505.18397)
*Fangqiao Tian,An Luo,Jin Du,Xun Xian,Robert Specht,Ganghua Wang,Xuan Bi,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Rui Zhang,Zirui Liu,Mingyi Hong,Jie Ding*

Main category: cs.MA

TL;DR: MAS provide a framework for distributed intelligence. This paper outlines opportunities and challenges, formalizes key concepts, identifies risks, and highlights pathways for robust, scalable, secure MAS.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of MAS in enabling collaborative reasoning, planning, and decision-making across autonomous agents while addressing current opportunities and challenges.

Method: Drawing insights from recent advances in LLMs, federated optimization, and human-AI interaction, the paper formalizes key concepts including agent topology, coordination protocols, and shared objectives, and identifies major risks such as dependency, misalignment, and vulnerabilities.

Result: The paper provides a systematic outlook on MAS through biologically inspired simulation and comprehensive theoretical framing, highlighting critical pathways for developing robust, scalable, and secure MAS in real-world settings.

Conclusion: Developing MAS requires addressing current challenges and risks while leveraging advancements in AI technologies to achieve robustness, scalability, and security.

Abstract: Multi-agent AI systems (MAS) offer a promising framework for distributed
intelligence, enabling collaborative reasoning, planning, and decision-making
across autonomous agents. This paper provides a systematic outlook on the
current opportunities and challenges of MAS, drawing insights from recent
advances in large language models (LLMs), federated optimization, and human-AI
interaction. We formalize key concepts including agent topology, coordination
protocols, and shared objectives, and identify major risks such as dependency,
misalignment, and vulnerabilities arising from training data overlap. Through a
biologically inspired simulation and comprehensive theoretical framing, we
highlight critical pathways for developing robust, scalable, and secure MAS in
real-world settings.

</details>


### [737] [MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs](https://arxiv.org/abs/2505.18530)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.MA

TL;DR: Medical Report Generation Agents (MRGAgents) is a new multi-agent framework that improves medical report generation by fine-tuning specialized agents for different disease categories, leading to more balanced and comprehensive reports.


<details>
  <summary>Details</summary>
Motivation: Medical Large Vision-Language Models (Med-LVLMs) used in medical report generation have shown a bias towards predicting all findings as normal and often fail to provide comprehensive descriptions of radiologically relevant regions.

Method: The MRGAgents framework curates subsets of the IU X-ray and MIMIC-CXR datasets to train disease-specific agents, which are then fine-tuned for different disease categories.

Result: Experiments show that MRGAgents outperformed the state-of-the-art models, improving both report comprehensiveness and diagnostic utility.

Conclusion: MRGAgents generates reports that more effectively balance normal and abnormal findings while ensuring a comprehensive description of clinically relevant regions.

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for
medical report generation. Despite Med-LVLMs producing state-of-the-art
performance, they exhibit a bias toward predicting all findings as normal,
leading to reports that overlook critical abnormalities. Furthermore, these
models often fail to provide comprehensive descriptions of radiologically
relevant regions necessary for accurate diagnosis. To address these challenges,
we proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent
framework that fine-tunes specialized agents for different disease categories.
By curating subsets of the IU X-ray and MIMIC-CXR datasets to train
disease-specific agents, MRGAgents generates reports that more effectively
balance normal and abnormal findings while ensuring a comprehensive description
of clinically relevant regions. Our experiments demonstrate that MRGAgents
outperformed the state-of-the-art, improving both report comprehensiveness and
diagnostic utility.

</details>


### [738] [MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework](https://arxiv.org/abs/2505.18572)
*Yifan Zhu,Chao Zhang,Xin Shi,Xueqiao Zhang,Yi Yang,Yawei Luo*

Main category: cs.MA

TL;DR: This paper introduces MASTER, a security research framework for Multi-Agent Systems (MAS) based on Large Language Models (LLMs). It highlights the problem-solving capabilities of MAS but also the increased security risks. MASTER focuses on role configurations and topological structures to automate MAS construction and improve information-flow interactions. The paper proposes an adaptive attack strategy that uses role and topology data to target specific domains, showing significant destructive potential in experiments. Defense strategies are also suggested to enhance MAS resilience.


<details>
  <summary>Details</summary>
Motivation: To address the amplified security risks associated with LLM-based Multi-Agent Systems (MAS), particularly focusing on their remarkable problem-solving and task planning capabilities across diverse domains due to specialized agentic roles and collaborative interactions.

Method: Introduce MASTER, a novel security research framework for MAS focusing on role configurations and topological structures. Design a scenario-adaptive, extensible attack strategy using role and topological information to dynamically allocate targeted, domain-specific attack tasks for collaborative agent execution. Propose corresponding defense strategies to enhance MAS resilience.

Result: Experiments demonstrate that attacks leveraging role and topological information exhibit significant destructive potential across most models. Proposed defense strategies substantially enhance MAS resilience across diverse scenarios.

Conclusion: The framework and findings provide valuable insights for future research into MAS security challenges.

Abstract: Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit
remarkable problem-solving and task planning capabilities across diverse
domains due to their specialized agentic roles and collaborative interactions.
However, this also amplifies the severity of security risks under MAS attacks.
To address this, we introduce MASTER, a novel security research framework for
MAS, focusing on diverse Role configurations and Topological structures across
various scenarios. MASTER offers an automated construction process for
different MAS setups and an information-flow-based interaction paradigm. To
tackle MAS security challenges in varied scenarios, we design a
scenario-adaptive, extensible attack strategy utilizing role and topological
information, which dynamically allocates targeted, domain-specific attack tasks
for collaborative agent execution. Our experiments demonstrate that such an
attack, leveraging role and topological information, exhibits significant
destructive potential across most models. Additionally, we propose
corresponding defense strategies, substantially enhancing MAS resilience across
diverse scenarios. We anticipate that our framework and findings will provide
valuable insights for future research into MAS security challenges.

</details>


### [739] [Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications](https://arxiv.org/abs/2505.19837)
*Christoph R. Landolt,Christoph Würsch,Roland Meier,Alain Mermoud,Julian Jang-Jaccard*

Main category: cs.MA

TL;DR: This paper surveys the application of Multi-Agent Reinforcement Learning (MARL) in automated cyber defense, focusing on intrusion detection and lateral movement containment. It also explores the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms, outlines challenges such as scalability and adversarial robustness, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: To address modern cybersecurity challenges with adaptive solutions, leveraging MARL for decentralized, adaptive, and collaborative defense strategies against dynamic and sophisticated threats.

Method: Investigation of current research in MARL applications for automated cyber defense, including analysis of AICA and Cyber Gyms for training and validation.

Result: MARL shows potential in intrusion detection and lateral movement containment. However, challenges like scalability and adversarial robustness need to be addressed.

Conclusion: MARL has transformative potential in enhancing automated cyber defense systems. Further research is needed to overcome existing challenges.

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown great potential as an
adaptive solution for addressing modern cybersecurity challenges. MARL enables
decentralized, adaptive, and collaborative defense strategies and provides an
automated mechanism to combat dynamic, coordinated, and sophisticated threats.
This survey investigates the current state of research in MARL applications for
automated cyber defense (ACD), focusing on intruder detection and lateral
movement containment. Additionally, it examines the role of Autonomous
Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and
validating MARL agents. Finally, the paper outlines existing challenges, such
as scalability and adversarial robustness, and proposes future research
directions. This also discusses how MARL integrates in AICA to provide
adaptive, scalable, and dynamic solutions to counter the increasingly
sophisticated landscape of cyber threats. It highlights the transformative
potential of MARL in areas like intrusion detection and lateral movement
containment, and underscores the value of Cyber Gyms for training and
validation of AICA.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [740] [A deep solver for backward stochastic Volterra integral equations](https://arxiv.org/abs/2505.18297)
*Kristoffer Andersson,Alessandro Gnoatto,Camilo Andrés García Trillos*

Main category: math.NA

TL;DR: The paper presents a deep-learning solver for BSVIEs and their forward-backward variants, proving error bounds and demonstrating scalability and generality through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and scalable method for solving backward stochastic Volterra integral equations (BSVIEs) and their fully-coupled forward-backward variants, overcoming limitations of classical algorithms that use nested time-stepping cycles.

Method: A neural network is trained to approximate the two solution fields in a single stage without using nested time-stepping cycles. For decoupled cases, a non-asymptotic error bound is proven which includes an a posteriori residual and square root dependence on the time step.

Result: Numerical experiments confirm the error rate and show that the method is scalable with stable accuracy up to 500 spatial variables while maintaining nearly constant wall-clock time due to GPU batching. The method also exhibits generality by handling coupled systems with forward dynamics depending on the backward solution.

Conclusion: This work provides practical access to high-dimensional, path-dependent problems in stochastic control and quantitative finance.

Abstract: We present the first deep-learning solver for backward stochastic Volterra
integral equations (BSVIEs) and their fully-coupled forward-backward variants.
The method trains a neural network to approximate the two solution fields in a
single stage, avoiding the use of nested time-stepping cycles that limit
classical algorithms. For the decoupled case we prove a non-asymptotic error
bound composed of an a posteriori residual plus the familiar square root
dependence on the time step. Numerical experiments confirm this rate and reveal
two key properties: \emph{scalability}, in the sense that accuracy remains
stable from low dimension up to 500 spatial variables while GPU batching keeps
wall-clock time nearly constant; and \emph{generality}, since the same method
handles coupled systems whose forward dynamics depend on the backward solution.
These results open practical access to a family of high-dimensional,
path-dependent problems in stochastic control and quantitative finance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [741] [SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation](https://arxiv.org/abs/2505.19151)
*Shenggan Cheng,Yuanxin Wei,Lansong Diao,Yong Liu,Bujiao Chen,Lianghua Huang,Yu Liu,Wenyuan Yu,Jiangsu Du,Wei Lin,Yang You*

Main category: cs.GR

TL;DR: SRDiffusion提出了一种通过大小模型协作减少推理成本的新框架，大幅加速视频生成且几乎无质量损失。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本转视频、图像转视频和视频编辑任务中表现出色，但高分辨率长时长视频生成计算成本高，现有加速方法会显著降低质量。

Method: SRDiffusion框架利用大模型处理高噪声步骤以确保语义和运动保真度，小模型在低噪声步骤中细化视觉细节。

Result: 实验表明，SRDiffusion比现有方法表现更好，对Wan实现3倍加速且几乎无质量损失，对CogVideoX实现2倍加速。

Conclusion: SRDiffusion提供了一个与现有加速策略正交的新方向，为可扩展的视频生成提供了实用解决方案。

Abstract: Leveraging the diffusion transformer (DiT) architecture, models like Sora,
CogVideoX and Wan have achieved remarkable progress in text-to-video,
image-to-video, and video editing tasks. Despite these advances,
diffusion-based video generation remains computationally intensive, especially
for high-resolution, long-duration videos. Prior work accelerates its inference
by skipping computation, usually at the cost of severe quality degradation. In
this paper, we propose SRDiffusion, a novel framework that leverages
collaboration between large and small models to reduce inference cost. The
large model handles high-noise steps to ensure semantic and motion fidelity
(Sketching), while the smaller model refines visual details in low-noise steps
(Rendering). Experimental results demonstrate that our method outperforms
existing approaches, over 3$\times$ speedup for Wan with nearly no quality loss
for VBench, and 2$\times$ speedup for CogVideoX. Our method is introduced as a
new direction orthogonal to existing acceleration strategies, offering a
practical solution for scalable video generation.

</details>


### [742] [CageNet: A Meta-Framework for Learning on Wild Meshes](https://arxiv.org/abs/2505.18772)
*Michal Edelstein,Hsueh-Ti Derek Liu,Mirela Ben-Chen*

Main category: cs.GR

TL;DR: The paper proposes a configurable meta-framework based on caged geometry to broaden the applicability of generic frameworks for learning on triangle meshes, especially wild meshes.


<details>
  <summary>Details</summary>
Motivation: Learning on triangle meshes has proven useful in many tasks, but current generic frameworks struggle with 'wild' meshes that have multiple components, non-manifold elements, disrupted connectivity, or combinations thereof.

Method: A cage is created as a single component manifold triangle mesh that closely envelopes the given mesh. Generalized barycentric coordinates map between functions on the cage and functions on the mesh, allowing for learning and testing on various data across different applications.

Result: This approach successfully learns segmentation and skinning weights on difficult data, showing improved performance compared to state-of-the-art techniques on wild meshes.

Conclusion: The proposed meta-framework based on caged geometry enhances the capability of existing generic frameworks to handle complex and irregular triangle meshes.

Abstract: Learning on triangle meshes has recently proven to be instrumental to a
myriad of tasks, from shape classification, to segmentation, to deformation and
animation, to mention just a few. While some of these applications are tackled
through neural network architectures which are tailored to the application at
hand, many others use generic frameworks for triangle meshes where the only
customization required is the modification of the input features and the loss
function. Our goal in this paper is to broaden the applicability of these
generic frameworks to "wild", i.e. meshes in-the-wild which often have multiple
components, non-manifold elements, disrupted connectivity, or a combination of
these. We propose a configurable meta-framework based on the concept of caged
geometry: Given a mesh, a cage is a single component manifold triangle mesh
that envelopes it closely. Generalized barycentric coordinates map between
functions on the cage, and functions on the mesh, allowing us to learn and test
on a variety of data, in different applications. We demonstrate this concept by
learning segmentation and skinning weights on difficult data, achieving better
performance to state of the art techniques on wild meshes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [743] [FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization](https://arxiv.org/abs/2505.18975)
*Aotao Wang,Haikuo Shao,Shaobo Ma,Zhongfeng Wang*

Main category: cs.AR

TL;DR: FastMamba is an FPGA-based accelerator with hardware-algorithm co-design for efficient deployment of Mamba2, featuring 8-bit quantization, power-of-two quantization framework, and nonlinear function approximation. It achieves significant speedup and energy efficiency improvements over CPU and GPU.


<details>
  <summary>Details</summary>
Motivation: Deploying Mamba2 on resource-constrained edge devices faces challenges such as outliers in linear layers, irregular tensor operations, and unfriendly nonlinear functions.

Method: The paper proposes FastMamba, which includes Hadamard transformation for 8-bit quantization, a power-of-two quantization framework, first-order linear approximation for nonlinear functions, and an accelerator design integrating parallel vector processing units, pipelined execution dataflow, and an efficient SSM Nonlinear Approximation Unit.

Result: For the input prefill task on Mamba2-130M, FastMamba achieves 68.80× and 8.90× speedup over Intel Xeon 4210R CPU and NVIDIA RTX 3090 GPU respectively. In the output decode experiment with Mamba2-2.7B, FastMamba attains 6× higher energy efficiency than RTX 3090 GPU.

Conclusion: FastMamba effectively addresses the issues of deploying Mamba2 on edge devices through its innovative hardware-algorithm co-design, leading to substantial performance and energy efficiency improvements.

Abstract: State Space Models (SSMs), like recent Mamba2, have achieved remarkable
performance and received extensive attention. However, deploying Mamba2 on
resource-constrained edge devices encounters many problems: severe outliers
within the linear layer challenging the quantization, diverse and irregular
element-wise tensor operations, and hardware-unfriendly nonlinear functions in
the SSM block. To address these issues, this paper presents FastMamba, a
dedicated accelerator on FPGA with hardware-algorithm co-design to promote the
deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit
quantization for linear layers through Hadamard transformation to eliminate
outliers. Moreover, a hardware-friendly and fine-grained power-of-two
quantization framework is presented for the SSM block and convolution layer,
and a first-order linear approximation is developed to optimize the nonlinear
functions. Based on the accurate algorithm quantization, we propose an
accelerator that integrates parallel vector processing units, pipelined
execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which
enhances computational efficiency and reduces hardware complexity. Finally, we
evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on
Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel
Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode
experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency
than RTX 3090 GPU.

</details>


### [744] [Enable Lightweight and Precision-Scalable Posit/IEEE-754 Arithmetic in RISC-V Cores for Transprecision Computing](https://arxiv.org/abs/2505.19096)
*Qiong Li,Chao Fang,Longwei Huang,Jun Lin,Zhongfeng Wang*

Main category: cs.AR

TL;DR: The paper proposes an enhanced RISC-V processor design with posit format for transprecision computing, achieving significant improvements in hardware efficiency and throughput.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of lacking a unified solution for lightweight, precision-scalable, and IEEE-754 arithmetic compatible hardware implementation in RISC-V processors supporting posit format.

Method: 1) Integrating dedicated posit codecs into the original FPU for lightweight implementation; 2) Incorporating multi/mixed-precision support with dynamic exponent size for precision-scalability; 3) Reusing and customizing ISA extensions for IEEE-754 compatible posit operations.

Result: Achieves 47.9% LUTs and 57.4% FFs reduction compared to state-of-the-art posit-enabled RISC-V processors, while improving throughput by up to 2.54$\times$ in various GEMM kernels.

Conclusion: The proposed enhancements to RISC-V processors offer a comprehensive solution for posit-based transprecision computing, improving both efficiency and performance.

Abstract: While posit format offers superior dynamic range and accuracy for
transprecision computing, its adoption in RISC-V processors is hindered by the
lack of a unified solution for lightweight, precision-scalable, and IEEE-754
arithmetic compatible hardware implementation. To address these challenges, we
enhance RISC-V processors by 1) integrating dedicated posit codecs into the
original FPU for lightweight implementation, 2) incorporating
multi/mixed-precision support with dynamic exponent size for
precision-scalability, and 3) reusing and customizing ISA extensions for
IEEE-754 compatible posit operations. Our comprehensive evaluation spans the
modified FPU, RISC-V core, and SoC levels. It demonstrates that our
implementation achieves 47.9% LUTs and 57.4% FFs reduction compared to
state-of-the-art posit-enabled RISC-V processors, while achieving up to
2.54$\times$ throughput improvement in various GEMM kernels.

</details>


### [745] [Efficient Optimization Accelerator Framework for Multistate Ising Problems](https://arxiv.org/abs/2505.20250)
*Chirag Garg,Sayeef Salahuddin*

Main category: cs.AR

TL;DR: Ising Machines are used for solving complex optimization problems, but transforming these problems can make them harder to solve. This paper proposes a new method using generalized boolean logic functions to reduce the complexity and improve solution quality for multi-state problems, specifically demonstrated with the graph coloring problem. It also introduces an advanced Ising accelerator that significantly outperforms existing methods in terms of accuracy, speed, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for solving NP-hard combinatorial optimization problems using Ising Machines often involve transforming the problems into quadratic unconstrained binary optimization (QUBO) form, which can increase the complexity of the problem landscape, especially for multi-state problems, thereby affecting the solution quality.

Method: The authors model spin interactions as generalized boolean logic functions to reduce the exploration space. They benchmark this approach using probabilistic Ising solvers on the graph coloring problem, a representative multi-state NP-hard optimization problem. Additionally, they incorporate parallel tempering into their framework and design a 1024-neuron all-to-all connected probabilistic Ising accelerator.

Result: The proposed methodology achieves similar accuracy compared to state-of-the-art heuristics and machine learning algorithms while showing significant improvement over existing Ising methods. The combination of parallel tempering reduces coloring error by up to 50% compared to the Gibbs sampling algorithm. The designed Ising accelerator demonstrates up to 10000x performance acceleration and requires 1.5-4x fewer physical neurons than conventional Ising machines, improving energy, performance, area, and solution quality metrics.

Conclusion: This work enhances the capabilities of Ising hardware to address multistate optimization problems by proposing a novel modeling approach and an advanced accelerator design. The improvements across multiple performance metrics indicate the potential for broader applications of Ising Machines in solving complex optimization challenges.

Abstract: Ising Machines are a prominent class of hardware architectures that aim to
solve NP-hard combinatorial optimization problems. These machines consist of a
network of interacting binary spins/neurons that evolve to represent the
optimum ground state energy solution. Generally, combinatorial problems are
transformed into quadratic unconstrained binary optimization (QUBO) form to
harness the computational efficiency of these Ising machines. However, this
transformation, especially for multi-state problems, often leads to a more
complex exploration landscape than the original problem, thus severely
impacting the solution quality. To address this challenge, we model the spin
interactions as a generalized boolean logic function to significantly reduce
the exploration space. We benchmark the graph coloring problem from the class
of multi-state NP-hard optimization using probabilistic Ising solvers to
illustrate the effectiveness of our framework. The proposed methodology
achieves similar accuracy compared to state-of-the-art heuristics and machine
learning algorithms, and demonstrates significant improvement over the existing
Ising methods. Additionally, we demonstrate that combining parallel tempering
with our existing framework further reduces the coloring error by up to 50%
compared to the conventionally used Gibbs sampling algorithm. We also design a
1024-neuron all-to-all connected probabilistic Ising accelerator that shows up
to 10000x performance acceleration compared to heuristics while reducing the
number of required physical neurons by 1.5-4x compared to conventional Ising
machines. Indeed, this accelerator solution demonstrates improvement across all
metrics over the current methods, i.e., energy, performance, area, and solution
quality. Thus, this work expands the potential of existing Ising hardware to
solve a broad class of these multistate optimization problems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [746] [STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution](https://arxiv.org/abs/2505.19644)
*Anton Firc,Manasi Chibber,Jagabandhu Mishra,Vishwanath Pratap Singh,Tomi Kinnunen,Kamil Malinka*

Main category: cs.SD

TL;DR: STOPA is a new dataset for deepfake speech source tracing, featuring systematic variation and rich metadata across 8 acoustic models, 6 vocoder models, and diverse parameter settings, improving attribution accuracy and aiding forensic analysis.


<details>
  <summary>Details</summary>
Motivation: Current progress in deepfake speech detection is hindered by the absence of a dedicated, systematically curated dataset that covers various generative factors.

Method: Introduced STOPA, a dataset with systematic variation and comprehensive metadata, covering multiple acoustic and vocoder models along with diverse parameter settings from different synthesisers.

Result: STOPA offers higher attribution reliability and improved accuracy for source tracing in deepfake speech, supporting forensic analysis, detection, and model transparency.

Conclusion: STOPA addresses the need for a controlled framework in deepfake speech source tracing, enhancing the capabilities in forensic analysis, detection, and understanding of generative models.

Abstract: A key research area in deepfake speech detection is source tracing -
determining the origin of synthesised utterances. The approaches may involve
identifying the acoustic model (AM), vocoder model (VM), or other
generation-specific parameters. However, progress is limited by the lack of a
dedicated, systematically curated dataset. To address this, we introduce STOPA,
a systematically varied and metadata-rich dataset for deepfake speech source
tracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k
samples from 13 distinct synthesisers. Unlike existing datasets, which often
feature limited variation or sparse metadata, STOPA provides a systematically
controlled framework covering a broader range of generative factors, such as
the choice of the vocoder model, acoustic model, or pretrained weights,
ensuring higher attribution reliability. This control improves attribution
accuracy, aiding forensic analysis, deepfake detection, and generative model
transparency.

</details>


### [747] [A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?](https://arxiv.org/abs/2505.19663)
*Yigitcan Özer,Woosung Choi,Joan Serrà,Mayank Kumar Singh,Wei-Hsiang Liao,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: The paper presents a framework for evaluating deep learning-based audio watermarking algorithms, providing a standardized benchmark and systematic comparison method. It includes an audio attack pipeline and diverse test dataset, revealing key insights about neural compression challenges and the impact of specific distortions.


<details>
  <summary>Details</summary>
Motivation: To establish a standardized benchmark for evaluating deep learning-based audio watermarking algorithms and allow systematic comparisons by simulating real-world usage.

Method: Introduced a comprehensive audio attack pipeline with various distortions and proposed a diverse test dataset including speech, environmental sounds, and music recordings. Evaluated the performance of four existing watermarking algorithms on this framework.

Result: Neural compression techniques pose the most significant challenge to watermarking algorithms even when trained with such compressions. Training with audio attacks generally improves robustness but is insufficient in some cases. Specific distortions seriously affect certain algorithms.

Conclusion: The contributions enhance the robustness and perceptual assessment of audio watermarking algorithms across different applications while ensuring fair and consistent evaluation.

Abstract: We present a framework to foster the evaluation of deep learning-based audio
watermarking algorithms, establishing a standardized benchmark and allowing
systematic comparisons. To simulate real-world usage, we introduce a
comprehensive audio attack pipeline, featuring various distortions such as
compression, background noise, and reverberation, and propose a diverse test
dataset, including speech, environmental sounds, and music recordings. By
assessing the performance of four existing watermarking algorithms on our
framework, two main insights stand out: (i) neural compression techniques pose
the most significant challenge, even when algorithms are trained with such
compressions; and (ii) training with audio attacks generally improves
robustness, although it is insufficient in some cases. Furthermore, we find
that specific distortions, such as polarity inversion, time stretching, or
reverb, seriously affect certain algorithms. Our contributions strengthen the
robustness and perceptual assessment of audio watermarking algorithms across a
wide range of applications, while ensuring a fair and consistent evaluation
approach. The evaluation framework, including the attack pipeline, is
accessible at github.com/SonyResearch/wm_robustness_eval.

</details>


### [748] [Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy](https://arxiv.org/abs/2505.19951)
*Elvir Karimov,Alexander Varlamov,Danil Ivanov,Dmitrii Korzh,Oleg Y. Rogov*

Main category: cs.SD

TL;DR: 研究人员引入了新的指数总变异（Exponential Total Variance，TV）损失函数，并提出了一种可扩展的普遍对抗补丁（UAP）插入程序，以改善语音匿名化方法中的音频质量、语音识别性能、跨不同声纹识别模型的迁移性以及对不同音频长度的一致表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于普遍对抗补丁（UAP）的说话人匿名化方法存在明显的音频质量下降、语音识别质量降低、在不同声纹识别模型之间的迁移性差以及性能依赖于输入音频长度等问题，因此需要改进这些方法。

Method: 1. 引入并使用新型的指数总变异（Exponential Total Variance，TV）损失函数，以增强UAP的强度和不可感知性。
2. 提出一种可扩展的UAP插入程序，确保其在各种音频长度下均表现出高性能。

Result: 实验结果表明，新提出的指数总变异损失函数和可扩展UAP插入程序能够有效缓解现有UAP方法的问题，提高了音频质量和语音识别性能，增强了跨模型的迁移性，并且对不同音频长度的表现更加一致。

Conclusion: 所提出的指数总变异损失函数和可扩展UAP插入程序为改进说话人匿名化技术提供了一种有效的方法，能够在保证音频质量的同时，提高匿名化的鲁棒性和一致性。

Abstract: Deep learning voice models are commonly used nowadays, but the safety
processing of personal data, such as human identity and speech content, remains
suspicious. To prevent malicious user identification, speaker anonymization
methods were proposed. Current methods, particularly based on universal
adversarial patch (UAP) applications, have drawbacks such as significant
degradation of audio quality, decreased speech recognition quality, low
transferability across different voice biometrics models, and performance
dependence on the input audio length. To mitigate these drawbacks, in this
work, we introduce and leverage the novel Exponential Total Variance (TV) loss
function and provide experimental evidence that it positively affects UAP
strength and imperceptibility. Moreover, we present a novel scalable UAP
insertion procedure and demonstrate its uniformly high performance for various
audio lengths.

</details>


### [749] [ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge](https://arxiv.org/abs/2505.18217)
*Soumya Dutta,Smruthi Balaji,Varada R,Viveka Salinamakki,Sriram Ganapathy*

Main category: cs.SD

TL;DR: The paper presents Abhinaya, a system for speech emotion recognition (SER) that combines speech-based, text-based, and speech-text models. It uses self-supervised learning, large language models, tailored loss functions, and majority voting to handle challenges like class imbalance. Despite incomplete training, the system ranked 4th in a challenge and achieved state-of-the-art performance when fully trained.


<details>
  <summary>Details</summary>
Motivation: Speech emotion recognition in naturalistic settings is challenging due to variability, diverse recording conditions, and class imbalance. The authors aim to address these issues by developing an integrated system that leverages multiple modalities and advanced modeling techniques.

Method: The Abhinaya system integrates speech-based, text-based, and speech-text models. It fine-tunes self-supervised and speech large language models for speech representations, leverages large language models for textual context, and employs speech-text modeling with an SLLM to capture emotional cues. Tailored loss functions are applied to combat class imbalance, and categorical decisions are generated through majority voting.

Result: The Abhinaya system ranked 4th among 166 submissions in the Interspeech Naturalistic SER Challenge, even though one model was not fully trained. Upon completion of training, it achieved state-of-the-art performance among published results.

Conclusion: The Abhinaya system demonstrates the effectiveness of combining speech-based, text-based, and speech-text models, along with tailored loss functions and majority voting, for speech emotion recognition in real-world conditions.

Abstract: Speech emotion recognition (SER) in naturalistic settings remains a challenge
due to the intrinsic variability, diverse recording conditions, and class
imbalance. As participants in the Interspeech Naturalistic SER Challenge which
focused on these complexities, we present Abhinaya, a system integrating
speech-based, text-based, and speech-text models. Our approach fine-tunes
self-supervised and speech large language models (SLLM) for speech
representations, leverages large language models (LLM) for textual context, and
employs speech-text modeling with an SLLM to capture nuanced emotional cues. To
combat class imbalance, we apply tailored loss functions and generate
categorical decisions through majority voting. Despite one model not being
fully trained, the Abhinaya system ranked 4th among 166 submissions. Upon
completion of training, it achieved state-of-the-art performance among
published results, demonstrating the effectiveness of our approach for SER in
real-world conditions.

</details>


### [750] [MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt](https://arxiv.org/abs/2505.18453)
*Zhichao Wu,Yueteng Kang,Songjun Cao,Long Ma,Qiulin Li,Qun Yang*

Main category: cs.SD

TL;DR: The paper proposes a customized emotion ZS-TTS system based on multi-modal prompt, which disentangles speech into content, timbre, emotion and prosody. It introduces a multi-modal prompt emotion encoder, a prosody predictor, and an emotion consistency loss.


<details>
  <summary>Details</summary>
Motivation: Most existing Zero-Shot Text-To-Speech (ZS-TTS) systems generate unseen speech based on single prompt, limiting their flexibility.

Method: The system disentangles speech into the content, timbre, emotion and prosody. A multi-modal prompt emotion encoder is proposed to extract emotion information from different prompts provided as text, image or speech. Additionally, a prosody predictor is introduced to fit the distribution of prosody and an emotion consistency loss is proposed to preserve emotion information in the predicted prosody. A diffusion-based acoustic model is employed to generate the target mel-spectrogram.

Result: Both objective and subjective experiments demonstrate that the proposed system outperforms existing systems in terms of naturalness and similarity.

Conclusion: The customized emotion ZS-TTS system based on multi-modal prompt shows superior performance compared to existing systems.

Abstract: Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen
speech based on single prompt, such as reference speech or text descriptions,
which limits their flexibility. We propose a customized emotion ZS-TTS system
based on multi-modal prompt. The system disentangles speech into the content,
timbre, emotion and prosody, allowing emotion prompts to be provided as text,
image or speech. To extract emotion information from different prompts, we
propose a multi-modal prompt emotion encoder. Additionally, we introduce an
prosody predictor to fit the distribution of prosody and propose an emotion
consistency loss to preserve emotion information in the predicted prosody. A
diffusion-based acoustic model is employed to generate the target
mel-spectrogram. Both objective and subjective experiments demonstrate that our
system outperforms existing systems in terms of naturalness and similarity. The
samples are available at https://mpetts-demo.github.io/mpetts_demo/.

</details>


### [751] [Discovering Interpretable Concepts in Large Generative Music Models](https://arxiv.org/abs/2505.18186)
*Nikhil Singh,Manuel Cherep,Pattie Maes*

Main category: cs.SD

TL;DR: Neural networks can generate music content by learning implicit theories through statistical learning alone, offering a new perspective on human-generated media. This paper focuses on music generators and introduces a method using sparse autoencoders to extract interpretable features from transformer models. The results reveal both familiar musical concepts and novel patterns.


<details>
  <summary>Details</summary>
Motivation: The motivation is the scientific opportunity presented by the ability of neural networks to generate content like music, which suggests they have learned implicit theories of such content structures via statistical learning alone. This offers a unique lens on human-generated media theories.

Method: The method involves using sparse autoencoders (SAEs) to discover musical concepts by extracting interpretable features from the residual stream activations of a transformer model.

Result: The results uncover both recognizable musical concepts and surprising patterns that do not have clear equivalents in current theories or natural language.

Conclusion: This work not only enhances model transparency but also provides an empirical tool for potentially discovering organizing principles in ways that traditional analysis and synthesis methods have missed.

Abstract: The fidelity with which neural networks can now generate content such as
music presents a scientific opportunity: these systems appear to have learned
implicit theories of the structure of such content through statistical learning
alone. This could offer a novel lens on theories of human-generated media.
Where these representations align with traditional constructs (e.g. chord
progressions in music), they demonstrate how these can be inferred from
statistical regularities. Where they diverge, they highlight potential limits
in our theoretical frameworks -- patterns that we may have overlooked but that
nonetheless hold significant explanatory power. In this paper, we focus on the
specific case of music generators. We introduce a method to discover musical
concepts using sparse autoencoders (SAEs), extracting interpretable features
from the residual stream activations of a transformer model. We evaluate this
approach by extracting a large set of features and producing an automatic
labeling and evaluation pipeline for them. Our results reveal both familiar
musical concepts and counterintuitive patterns that lack clear counterparts in
existing theories or natural language altogether. Beyond improving model
transparency, our work provides a new empirical tool that might help discover
organizing principles in ways that have eluded traditional methods of analysis
and synthesis.

</details>


### [752] [CloneShield: A Framework for Universal Perturbation Against Zero-Shot Voice Cloning](https://arxiv.org/abs/2505.19119)
*Renyuan Li,Zhibo Liang,Haichuan Zhang,Tianyu Shi,Zhiyuan Cheng,Jia Shi,Carl Yang,Mingjie Tang*

Main category: cs.SD

TL;DR: The paper introduces CloneShield, a framework that defends against zero-shot voice cloning by using adversarial perturbations. It ensures imperceptibility to human listeners while degrading cloned speech quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is the privacy concerns caused by advancements in text-to-speech (TTS) voice cloning, which can replicate vocal identities accurately from minimal audio references.

Method: The method involves formulating perturbation generation as a multi-objective optimization problem and using Multi-Gradient Descent Algorithm (MGDA) for robust protection across diverse utterances. Perturbations are decomposed via Mel-spectrogram representations and fine-tuned for each sample to ensure imperceptibility.

Result: Experiments show that CloneShield preserves near-original audio quality in protected inputs (PESQ = 3.90, SRS = 0.93) while significantly degrading speaker similarity and speech quality in cloned samples (PESQ = 1.07, SRS = 0.08).

Conclusion: CloneShield effectively protects against zero-shot voice cloning by preserving original audio quality for legitimate users and degrading cloned speech quality.

Abstract: Recent breakthroughs in text-to-speech (TTS) voice cloning have raised
serious privacy concerns, allowing highly accurate vocal identity replication
from just a few seconds of reference audio, while retaining the speaker's vocal
authenticity. In this paper, we introduce CloneShield, a universal time-domain
adversarial perturbation framework specifically designed to defend against
zero-shot voice cloning. Our method provides protection that is robust across
speakers and utterances, without requiring any prior knowledge of the
synthesized text. We formulate perturbation generation as a multi-objective
optimization problem, and propose Multi-Gradient Descent Algorithm (MGDA) to
ensure the robust protection across diverse utterances. To preserve natural
auditory perception for users, we decompose the adversarial perturbation via
Mel-spectrogram representations and fine-tune it for each sample. This design
ensures imperceptibility while maintaining strong degradation effects on
zero-shot cloned outputs. Experiments on three state-of-the-art zero-shot TTS
systems, five benchmark datasets and evaluations from 60 human listeners
demonstrate that our method preserves near-original audio quality in protected
inputs (PESQ = 3.90, SRS = 0.93) while substantially degrading both speaker
similarity and speech quality in cloned samples (PESQ = 1.07, SRS = 0.08).

</details>


### [753] [EnvSDD: Benchmarking Environmental Sound Deepfake Detection](https://arxiv.org/abs/2505.19203)
*Han Yin,Yang Xiao,Rohan Kumar Das,Jisheng Bai,Haohe Liu,Wenwu Wang,Mark D Plumbley*

Main category: cs.SD

TL;DR: The paper presents EnvSDD, a large-scale dataset for environmental sound deepfake detection, and proposes an effective audio deepfake detection system.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods mainly focus on speech or singing voices, but environmental sounds have different characteristics which may reduce the effectiveness of these methods. Additionally, current datasets for environmental sound deepfake detection are insufficient in scale and variety.

Method: Introduced EnvSDD, a large-scale dataset with 45.25 hours of real and 316.74 hours of fake audio. Proposed an audio deepfake detection system based on a pre-trained audio foundation model.

Result: The proposed system outperforms state-of-the-art systems from speech and singing domains when tested on the EnvSDD dataset.

Conclusion: EnvSDD fills the gap in environmental sound deepfake detection datasets and the proposed detection system demonstrates superior performance.

Abstract: Audio generation systems now create very realistic soundscapes that can
enhance media production, but also pose potential risks. Several studies have
examined deepfakes in speech or singing voice. However, environmental sounds
have different characteristics, which may make methods for detecting speech and
singing deepfakes less effective for real-world sounds. In addition, existing
datasets for environmental sound deepfake detection are limited in scale and
audio types. To address this gap, we introduce EnvSDD, the first large-scale
curated dataset designed for this task, consisting of 45.25 hours of real and
316.74 hours of fake audio. The test set includes diverse conditions to
evaluate the generalizability, such as unseen generation models and unseen
datasets. We also propose an audio deepfake detection system, based on a
pre-trained audio foundation model. Results on EnvSDD show that our proposed
system outperforms the state-of-the-art systems from speech and singing
domains.

</details>


### [754] [Eta-WavLM: Efficient Speaker Identity Removal in Self-Supervised Speech Representations Using a Simple Linear Equation](https://arxiv.org/abs/2505.19273)
*Giuseppe Ruggiero,Matteo Testa,Jurgen Van de Walle,Luigi Di Caro*

Main category: cs.SD

TL;DR: Self-supervised learning (SSL) has lessened the need for costly labeled data in speech technologies. This paper proposes a new method to disentangle speaker-specific and speaker-independent components in SSL representations, leading to improvements in content-driven tasks like voice conversion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing methods that either fail to fully disentangle speaker identity from SSL representations or require resource-intensive models.

Method: A novel disentanglement method that linearly decomposes SSL representations into speaker-specific and speaker-independent components is proposed.

Result: Comprehensive experiments demonstrate that the approach achieves speaker independence and leads to significant improvements in content-driven tasks such as voice conversion.

Conclusion: This novel disentanglement method effectively generates speaker disentangled representations, improving performance in downstream tasks.

Abstract: Self-supervised learning (SSL) has reduced the reliance on expensive labeling
in speech technologies by learning meaningful representations from unannotated
data. Since most SSL-based downstream tasks prioritize content information in
speech, ideal representations should disentangle content from unwanted
variations like speaker characteristics in the SSL representations. However,
removing speaker information often degrades other speech components, and
existing methods either fail to fully disentangle speaker identity or require
resource-intensive models. In this paper, we propose a novel disentanglement
method that linearly decomposes SSL representations into speaker-specific and
speaker-independent components, effectively generating speaker disentangled
representations. Comprehensive experiments show that our approach achieves
speaker independence and as such, when applied to content-driven tasks such as
voice conversion, our representations yield significant improvements over
state-of-the-art methods.

</details>


### [755] [Audio Geolocation: A Natural Sounds Benchmark](https://arxiv.org/abs/2505.18726)
*Mustafa Chasmai,Wuao Liu,Subhransu Maji,Grant Van Horn*

Main category: cs.SD

TL;DR: This paper explores the possibility of determining someone's geographic location using only the sounds they hear, utilizing wildlife audio from the iNatSounds dataset and image geolocation techniques. It suggests species vocalizations offer strong geolocation cues and proposes an approach integrating species range prediction with retrieval-based geolocation.


<details>
  <summary>Details</summary>
Motivation: To determine if acoustic signals alone can be used to localize a person within a country, state, or city.

Method: Convert audio recordings to spectrograms, benchmark existing image geolocation techniques, and integrate species range prediction with retrieval-based geolocation. Evaluate geolocation improvements when analyzing species-rich recordings or aggregating across spatiotemporal neighborhoods.

Result: The analysis shows that species vocalizations offer strong geolocation cues due to their defined geographic ranges. The proposed method effectively integrates species range prediction with retrieval-based geolocation.

Conclusion: This work highlights the advantages of integrating audio and visual cues for geolocation and sets the stage for future research in audio geolocation.

Abstract: Can we determine someone's geographic location purely from the sounds they
hear? Are acoustic signals enough to localize within a country, state, or even
city? We tackle the challenge of global-scale audio geolocation, formalize the
problem, and conduct an in-depth analysis with wildlife audio from the
iNatSounds dataset. Adopting a vision-inspired approach, we convert audio
recordings to spectrograms and benchmark existing image geolocation techniques.
We hypothesize that species vocalizations offer strong geolocation cues due to
their defined geographic ranges and propose an approach that integrates species
range prediction with retrieval-based geolocation. We further evaluate whether
geolocation improves when analyzing species-rich recordings or when aggregating
across spatiotemporal neighborhoods. Finally, we introduce case studies from
movies to explore multimodal geolocation using both audio and visual content.
Our work highlights the advantages of integrating audio and visual cues, and
sets the stage for future research in audio geolocation.

</details>


### [756] [Training-Free Multi-Step Audio Source Separation](https://arxiv.org/abs/2505.19534)
*Yongyi Zang,Jingyi Li,Qiuqiang Kong*

Main category: cs.SD

TL;DR: 提出了一种多步分离方法，能有效提升音频源分离性能，无需额外训练模型。


<details>
  <summary>Details</summary>
Motivation: 以往的音频源分离系统通常采用一步推理方法，未能充分利用模型的分离能力。因此，研究者希望探索如何利用预训练的一步模型实现多步分离，从而提高分离效果。

Method: 通过最优混合输入混音与前一步分离结果的方法，迭代应用分离过程，并在每一步中通过最大化某个度量来确定最优混合比例。同时，从理论上证明该方法优于一步推理，并分析了其与去噪扩散桥模型的关系。

Result: 实验结果表明，该多步分离方法在语音增强和音乐源分离任务中均优于一步推理方法，且其性能提升类似于使用更大模型、更多数据或某些情况下采用多步训练目标的效果。此外，改进不仅体现在优化度量上，还扩展到几乎所有非优化度量（一个例外）。

Conclusion: 该方法为现有模型提供了一种无需额外训练即可提升分离性能的有效途径，但也讨论了其局限性和未来研究方向。

Abstract: Audio source separation aims to separate a mixture into target sources.
Previous audio source separation systems usually conduct one-step inference,
which does not fully explore the separation ability of models. In this work, we
reveal that pretrained one-step audio source separation models can be leveraged
for multi-step separation without additional training. We propose a simple yet
effective inference method that iteratively applies separation by optimally
blending the input mixture with the previous step's separation result. At each
step, we determine the optimal blending ratio by maximizing a metric. We prove
that our method always yield improvement over one-step inference, provide error
bounds based on model smoothness and metric robustness, and provide theoretical
analysis connecting our method to denoising along linear interpolation paths
between noise and clean distributions, a property we link to denoising
diffusion bridge models. Our approach effectively delivers improved separation
performance as a "free lunch" from existing models. Our empirical results
demonstrate that our multi-step separation approach consistently outperforms
one-step inference across both speech enhancement and music source separation
tasks, and can achieve scaling performance similar to training a larger model,
using more data, or in some cases employing a multi-step training objective.
These improvements appear not only on the optimization metric during multi-step
inference, but also extend to nearly all non-optimized metrics (with one
exception). We also discuss limitations of our approach and directions for
future research.

</details>


### [757] [Automated evaluation of children's speech fluency for low-resource languages](https://arxiv.org/abs/2505.19671)
*Bowen Zhang,Nur Afiqah Abdul Latiff,Justin Kan,Rong Tong,Donny Soh,Xiaoxiao Miao,Ian McLoughlin*

Main category: cs.SD

TL;DR: The paper proposes a system to automatically assess children's speaking fluency in low resource languages by combining a fine-tuned multilingual ASR model, an objective metrics extraction stage, and a GPT network. Results show significantly higher accuracy than other methods.


<details>
  <summary>Details</summary>
Motivation: Assessment of children's speaking fluency is well researched for majority languages but remains highly challenging for low resource languages.

Method: Combine a fine-tuned multilingual ASR model, an objective metrics extraction stage including phonetic and word error rates, speech rate, and speech-pause duration ratio, and a GPT-based classifier to score fluency based on human-evaluated ground truth examples.

Result: The proposed system achieves significantly higher accuracy than multimodal GPT or other methods when evaluated on a dataset of children's speech in Tamil and Malay.

Conclusion: The proposed approach demonstrates superior performance in assessing speaking fluency in low resource languages compared to existing methods.

Abstract: Assessment of children's speaking fluency in education is well researched for
majority languages, but remains highly challenging for low resource languages.
This paper proposes a system to automatically assess fluency by combining a
fine-tuned multilingual ASR model, an objective metrics extraction stage, and a
generative pre-trained transformer (GPT) network. The objective metrics include
phonetic and word error rates, speech rate, and speech-pause duration ratio.
These are interpreted by a GPT-based classifier guided by a small set of
human-evaluated ground truth examples, to score fluency. We evaluate the
proposed system on a dataset of children's speech in two low-resource
languages, Tamil and Malay and compare the classification performance against
Random Forest and XGBoost, as well as using ChatGPT-4o to predict fluency
directly from speech input. Results demonstrate that the proposed approach
achieves significantly higher accuracy than multimodal GPT or other methods.

</details>


### [758] [DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech](https://arxiv.org/abs/2505.19687)
*Deok-Hyeon Cho,Hyung-Seok Oh,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: The paper presents DiEmo-TTS, a self-supervised distillation method for cross-speaker emotion transfer in speech synthesis that minimizes emotional information loss and preserves speaker identity.


<details>
  <summary>Details</summary>
Motivation: Existing timbre compression methods for cross-speaker emotion transfer fail to fully separate speaker and emotion characteristics, leading to issues like speaker leakage and degraded synthesis quality.

Method: The proposed method, DiEmo-TTS, uses cluster-driven sampling and information perturbation to preserve emotion while removing irrelevant factors. It also includes an emotion clustering and matching approach using emotional attribute prediction and speaker embeddings, as well as a dual conditioning transformer for better integration of style features.

Result: Experimental results confirm the effectiveness of DiEmo-TTS in learning speaker-irrelevant emotion embeddings, thus improving cross-speaker emotion transfer in speech synthesis.

Conclusion: DiEmo-TTS successfully addresses the limitations of existing methods by minimizing emotional information loss and preserving speaker identity, offering improved performance in cross-speaker emotion transfer.

Abstract: Cross-speaker emotion transfer in speech synthesis relies on extracting
speaker-independent emotion embeddings for accurate emotion modeling without
retaining speaker traits. However, existing timbre compression methods fail to
fully separate speaker and emotion characteristics, causing speaker leakage and
degraded synthesis quality. To address this, we propose DiEmo-TTS, a
self-supervised distillation method to minimize emotional information loss and
preserve speaker identity. We introduce cluster-driven sampling and information
perturbation to preserve emotion while removing irrelevant factors. To
facilitate this process, we propose an emotion clustering and matching approach
using emotional attribute prediction and speaker embeddings, enabling
generalization to unlabeled data. Additionally, we designed a dual conditioning
transformer to integrate style features better. Experimental results confirm
the effectiveness of our method in learning speaker-irrelevant emotion
embeddings.

</details>


### [759] [EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification](https://arxiv.org/abs/2505.19693)
*Deok-Hyeon Cho,Hyung-Seok Oh,Seung-Bin Kim,Seong-Whan Lee*

Main category: cs.SD

TL;DR: The paper introduces EmoSphere-SER, a model that enhances speech emotion recognition by integrating spherical VAD region classification with VAD regression. It uses dynamic weighting and multi-head self-attention to capture dynamics, improving prediction consistency.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and consistency of speech emotion recognition using continuous dimensions (VAD).

Method: Transform VAD values into spherical coordinates for classification and regression, incorporate dynamic weighting and multi-head self-attention.

Result: Exceeds baseline methods in experimental results.

Conclusion: EmoSphere-SER framework is effective for speech emotion recognition.

Abstract: Speech emotion recognition predicts a speaker's emotional state from speech
signals using discrete labels or continuous dimensions such as arousal,
valence, and dominance (VAD). We propose EmoSphere-SER, a joint model that
integrates spherical VAD region classification to guide VAD regression for
improved emotion prediction. In our framework, VAD values are transformed into
spherical coordinates that are divided into multiple spherical regions, and an
auxiliary classification task predicts which spherical region each point
belongs to, guiding the regression process. Additionally, we incorporate a
dynamic weighting scheme and a style pooling layer with multi-head
self-attention to capture spectral and temporal dynamics, further boosting
performance. This combined training strategy reinforces structured learning and
improves prediction consistency. Experimental results show that our approach
exceeds baseline methods, confirming the validity of the proposed framework.

</details>


### [760] [Automated data curation for self-supervised learning in underwater acoustic analysis](https://arxiv.org/abs/2505.20066)
*Hilde I Hummel,Sandjai Bhulai,Burooj Ghani,Rob van der Mei*

Main category: cs.SD

TL;DR: The abstract introduces a self-supervised data curation pipeline to create a diverse and balanced dataset from raw PAM data, integrating AIS data with hydrophone recordings. This facilitates the development of self-supervised learning models for tasks like monitoring marine mammals and assessing sound pollution.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for automation in analyzing large amounts of underwater sound recordings collected by Passive acoustic monitoring (PAM) systems due to the impossibility of manual analysis. Additionally, most underwater acoustic recordings are unlabeled, making traditional machine learning approaches unsuitable.

Method: A fully automated self-supervised data curation pipeline is proposed. It integrates Automatic Identification System (AIS) data with recordings from various hydrophones in U.S. waters. Hierarchical k-means clustering is used to sample the raw audio data, which is then combined with AIS samples to create a balanced and diverse dataset.

Result: The resulting curated dataset enables the development of self-supervised learning models that can be applied to various tasks such as monitoring marine mammals and assessing sound pollution.

Conclusion: Self-supervised learning models can be effectively developed using the proposed data curation pipeline, leading to better understanding and monitoring of ocean ecosystems through automated processing of underwater sound recordings.

Abstract: The sustainability of the ocean ecosystem is threatened by increased levels
of sound pollution, making monitoring crucial to understand its variability and
impact. Passive acoustic monitoring (PAM) systems collect a large amount of
underwater sound recordings, but the large volume of data makes manual analysis
impossible, creating the need for automation. Although machine learning offers
a potential solution, most underwater acoustic recordings are unlabeled.
Self-supervised learning models have demonstrated success in learning from
large-scale unlabeled data in various domains like computer vision, Natural
Language Processing, and audio. However, these models require large, diverse,
and balanced datasets for training in order to generalize well. To address
this, a fully automated self-supervised data curation pipeline is proposed to
create a diverse and balanced dataset from raw PAM data. It integrates
Automatic Identification System (AIS) data with recordings from various
hydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw
audio data is sampled and then combined with AIS samples to create a balanced
and diverse dataset. The resulting curated dataset enables the development of
self-supervised learning models, facilitating various tasks such as monitoring
marine mammals and assessing sound pollution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [761] [High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction](https://arxiv.org/abs/2505.18817)
*Seongsu Kim,Nayoung Kim,Dongwoo Kim,Sungsoo Ahn*

Main category: physics.comp-ph

TL;DR: QHFlow is a high-order equivariant flow matching framework that generates Hamiltonian matrices conditioned on molecular geometry, achieving state-of-the-art performance in reducing Hamiltonian error and accelerating the DFT process.


<details>
  <summary>Details</summary>
Motivation: Density functional theory (DFT) simulations are expensive due to the iterative self-consistent field (SCF) process required to solve the Kohn-Sham equations. Deep learning methods have been used to bypass this step by directly predicting the Hamiltonian, but they rely on deterministic regression and do not consider the highly structured nature of Hamiltonians.

Method: The authors propose QHFlow, a high-order equivariant flow matching framework that generates Hamiltonian matrices conditioned on molecular geometry. Flow matching models continuous-time trajectories between simple priors and complex targets, learning the structured distributions over Hamiltonians instead of direct regression. A neural architecture that predicts SE(3)-equivariant vector fields is used to incorporate symmetry, improving accuracy and generalization across diverse geometries. Additionally, a fine-tuning scheme is introduced to align predicted orbital energies with the target.

Result: QHFlow achieves state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53% on QH9. It also accelerates the DFT process without trading off the solution quality when initializing SCF iterations with the predicted Hamiltonian, significantly reducing the number of iterations and runtime.

Conclusion: QHFlow is an effective framework for generating Hamiltonian matrices conditioned on molecular geometry, achieving significant reductions in Hamiltonian error and accelerating the DFT process.

Abstract: Density functional theory (DFT) is a fundamental method for simulating
quantum chemical properties, but it remains expensive due to the iterative
self-consistent field (SCF) process required to solve the Kohn-Sham equations.
Recently, deep learning methods are gaining attention as a way to bypass this
step by directly predicting the Hamiltonian. However, they rely on
deterministic regression and do not consider the highly structured nature of
Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow
matching framework that generates Hamiltonian matrices conditioned on molecular
geometry. Flow matching models continuous-time trajectories between simple
priors and complex targets, learning the structured distributions over
Hamiltonians instead of direct regression. To further incorporate symmetry, we
use a neural architecture that predicts SE(3)-equivariant vector fields,
improving accuracy and generalization across diverse geometries. To further
enhance physical fidelity, we additionally introduce a fine-tuning scheme to
align predicted orbital energies with the target. QHFlow achieves
state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53%
on QH9. Moreover, we further show that QHFlow accelerates the DFT process
without trading off the solution quality when initializing SCF iterations with
the predicted Hamiltonian, significantly reducing the number of iterations and
runtime.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [762] [Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities](https://arxiv.org/abs/2505.19887)
*Anton Tkachenko,Dmitrij Suskevic,Benjamin Adolphi*

Main category: cs.SE

TL;DR: 大型语言模型（LLMs）在软件工程中展现出潜力，但其在二进制分析中的有效性尚未被探索。本文对商业LLMs在汇编代码反混淆中的表现进行了全面评估，测试了七个最先进的模型在四种混淆场景下的表现，发现性能差异显著。提出了一个基于四个维度的理论框架来解释这些差异，并识别出五种错误模式，揭示了LLMs在代码处理上的基本限制。建立了三层阻力模型，表明复杂的混淆技术对先进的LLMs仍然有效。研究结果建议采用人机协作范式，其中LLMs可以降低某些逆向工程任务的专业门槛，但在复杂反混淆时仍需人类指导。这项工作为评估新兴能力和开发抗混淆技术提供了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在软件工程领域显示出潜力，但其在二进制分析特别是反混淆方面的应用尚未得到充分研究。因此，有必要对现有LLMs在反混淆任务中的能力进行全面评估，以了解其优势和局限性。

Method: 评估了七种最先进的商业LLMs在四种混淆场景（虚假控制流、指令替换、控制流平坦化及其组合）下的反混淆能力。提出一个基于四个维度（推理深度、模式识别、噪声过滤和上下文整合）的理论框架来解释性能差异，并通过分析识别出五种错误模式。

Result: 发现LLMs在反混淆任务中的表现存在显著差异，从自主反混淆到完全失败不等。建立了三层阻力模型，表明复杂的混淆技术对LLMs仍然有效。识别出五种错误模式，揭示了LLMs在代码处理上的基本限制。

Conclusion: LLMs在某些反混淆任务中表现出潜力，但在面对复杂混淆时需要人类指导。本研究为评估LLMs新兴能力和开发抗混淆技术提供了基础，建议采用人机协作范式。

Abstract: Large language models (LLMs) have shown promise in software engineering, yet
their effectiveness for binary analysis remains unexplored. We present the
first comprehensive evaluation of commercial LLMs for assembly code
deobfuscation. Testing seven state-of-the-art models against four obfuscation
scenarios (bogus control flow, instruction substitution, control flow
flattening, and their combination), we found striking performance
variations--from autonomous deobfuscation to complete failure. We propose a
theoretical framework based on four dimensions: Reasoning Depth, Pattern
Recognition, Noise Filtering, and Context Integration, explaining these
variations. Our analysis identifies five error patterns: predicate
misinterpretation, structural mapping errors, control flow misinterpretation,
arithmetic transformation errors, and constant propagation errors, revealing
fundamental limitations in LLM code processing.We establish a three-tier
resistance model: bogus control flow (low resistance), control flow flattening
(moderate resistance), and instruction substitution/combined techniques (high
resistance). Universal failure against combined techniques demonstrates that
sophisticated obfuscation remains effective against advanced LLMs. Our findings
suggest a human-AI collaboration paradigm where LLMs reduce expertise barriers
for certain reverse engineering tasks while requiring human guidance for
complex deobfuscation. This work provides a foundation for evaluating emerging
capabilities and developing resistant obfuscation techniques.x deobfuscation.
This work provides a foundation for evaluating emerging capabilities and
developing resistant obfuscation techniques.

</details>


### [763] [Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge Proofs](https://arxiv.org/abs/2505.20136)
*Filippo Scaramuzza,Giovanni Quattrocchi,Damian A. Tamburri*

Main category: cs.SE

TL;DR: 随着AI系统在高风险应用中的普及，传统验证方法面临诸多挑战。零知识证明（ZKP）为解决这些问题提供了加密解决方案。本文通过系统性调研，识别出五个关键属性，并分析了ZKP增强的机器学习应用。研究发现目前主要集中在推理验证上，而数据预处理和训练阶段尚未得到充分探索。此外，研究领域正逐步向统一的零知识机器学习操作框架（ZKMLOps）发展，以提供更强的正确性、完整性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的人工智能系统因其概率性和不透明性对传统验证方法构成重大挑战，特别是在需要防篡改、可审计证据的监管行业。因此，需要一种新的方法来满足这些需求。

Method: 首先进行系统性调查，确定适用于AI验证和验证管道的ZKP协议的五个关键属性；然后按照TDSP模型分析ZKP增强的机器学习应用，涵盖数据预处理、训练、推理和在线指标等阶段。

Result: 研究表明，当前ZKP增强的机器学习研究主要集中于推理验证，而数据预处理和训练阶段尚待深入探索。同时，研究领域正在向ZKMLOps框架收敛，该框架能够提供强大的密码学保障。

Conclusion: 零知识证明技术具有巨大潜力，可以有效应对AI系统验证中的挑战，特别是通过ZKMLOps框架实现更高的可信度和合规性。

Abstract: As Artificial Intelligence (AI) systems, particularly those based on machine
learning (ML), become integral to high-stakes applications, their probabilistic
and opaque nature poses significant challenges to traditional verification and
validation methods. These challenges are exacerbated in regulated sectors
requiring tamper-proof, auditable evidence, as highlighted by apposite legal
frameworks, e.g., the EU AI Act. Conversely, Zero-Knowledge Proofs (ZKPs) offer
a cryptographic solution that enables provers to demonstrate, through verified
computations, adherence to set requirements without revealing sensitive model
details or data. Through a systematic survey of ZKP protocols, we identify five
key properties (non-interactivity, transparent setup, standard representations,
succinctness, and post-quantum security) critical for their application in AI
validation and verification pipelines. Subsequently, we perform a follow-up
systematic survey analyzing ZKP-enhanced ML applications across an adaptation
of the Team Data Science Process (TDSP) model (Data & Preprocessing, Training &
Offline Metrics, Inference, and Online Metrics), detailing verification
objectives, ML models, and adopted protocols. Our findings indicate that
current research on ZKP-Enhanced ML primarily focuses on inference
verification, while the data preprocessing and training stages remain
underexplored. Most notably, our analysis identifies a significant convergence
within the research domain toward the development of a unified Zero-Knowledge
Machine Learning Operations (ZKMLOps) framework. This emerging framework
leverages ZKPs to provide robust cryptographic guarantees of correctness,
integrity, and privacy, thereby promoting enhanced accountability,
transparency, and compliance with Trustworthy AI principles.

</details>


### [764] [Exposing Go's Hidden Bugs: A Novel Concolic Framework](https://arxiv.org/abs/2505.20183)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: The paper introduces Zorya, a new methodology using concolic execution to evaluate Go programs and improve security.


<details>
  <summary>Details</summary>
Motivation: To address the need for improved security measures in Go programming language, particularly its runtime and concurrency model.

Method: Leveraging concrete and symbolic (concolic) execution, systematically exploring execution paths, employing Ghidra's P-Code as an intermediate representation (IR), detecting runtime panics, and supporting generic and custom invariants.

Result: Detects runtime panics in the TinyGo compiler and supports both generic and custom invariants. Also enables analysis of programs written in other languages such as C.

Conclusion: Zorya provides a comprehensive way to evaluate Go programs and improve security, with potential future enhancements including intelligent classification of concolic execution logs.

Abstract: The widespread adoption of the Go programming language in infrastructure
backends and blockchain projects has heightened the need for improved security
measures. Established techniques such as unit testing, static analysis, and
program fuzzing provide foundational protection mechanisms. Although symbolic
execution tools have made significant contributions, opportunities remain to
address the complexities of Go's runtime and concurrency model. In this work,
we present Zorya, a novel methodology leveraging concrete and symbolic
(concolic) execution to evaluate Go programs comprehensively. By systematically
exploring execution paths to uncover vulnerabilities beyond conventional
testing, symbolic execution offers distinct advantages, and coupling it with
concrete execution mitigates the path explosion problem. Our solution employs
Ghidra's P-Code as an intermediate representation (IR). This implementation
detects runtime panics in the TinyGo compiler and supports both generic and
custom invariants. Furthermore, P-Code's generic IR nature enables analysis of
programs written in other languages such as C. Future enhancements may include
intelligent classification of concolic execution logs to identify vulnerability
patterns.

</details>


### [765] [Data Mining-Based Techniques for Software Fault Localization](https://arxiv.org/abs/2505.18216)
*Peggy Cellier,Mireille Ducassé,Sébastien Ferré,Olivier Ridoux,W. Eric Wong*

Main category: cs.SE

TL;DR: This chapter explains fault localization using data mining techniques, specifically focusing on symbolic data mining methods like Formal Concept Analysis and Association Rule. It uses the Trityp program as an example and extends the discussion to multiple fault situations and GUI components.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how data mining techniques can be applied to the process of fault localization in software testing, especially addressing more complex scenarios such as multiple faults and GUI component testing.

Method: The chapter employs Formal Concept Analysis and Association Rule for symbolic data mining. It analyzes test cases with PASS and FAIL attributes using these methods and applies them to both traditional software and GUI components.

Result: The result is a better understanding of how data mining can enhance fault localization processes, even in complex scenarios like multiple faults and GUI event sequences.

Conclusion: Data mining techniques, particularly Formal Concept Analysis and Association Rule, are effective for fault localization in software testing. They can be adapted for use in various types of software, including those with GUI components.

Abstract: This chapter illustrates the basic concepts of fault localization using a
data mining technique. It utilizes the Trityp program to illustrate the general
method. Formal concept analysis and association rule are two well-known methods
for symbolic data mining. In their original inception, they both consider data
in the form of an object-attribute table. In their original inception, they
both consider data in the form of an object-attribute table. The chapter
considers a debugging process in which a program is tested against different
test cases. Two attributes, PASS and FAIL, represent the issue of the test
case. The chapter extends the analysis of data mining for fault localization
for the multiple fault situations. It addresses how data mining can be further
applied to fault localization for GUI components. Unlike traditional software,
GUI test cases are usually event sequences, and each individual event has a
unique corresponding event handler.

</details>


### [766] [SEW: Self-Evolving Agentic Workflows for Automated Code Generation](https://arxiv.org/abs/2505.18646)
*Siwei Liu,Jinyuan Fang,Han Zhou,Yingxu Wang,Zaiqiao Meng*

Main category: cs.SE

TL;DR: Large Language Models (LLMs) are effective in code generation tasks. Current multi-agent systems with hand-crafted workflows are limited by manual design. This paper proposes SEW, a self-evolving framework that automatically generates and optimises multi-agent workflows for coding tasks, showing significant improvement on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current multi-agent systems which rely on hand-crafted agentic workflows and manually designed agent topologies and prompts, restricting their adaptability to different coding problems.

Method: Propose SEW, a novel self-evolving framework that automatically generates and optimises multi-agent workflows for complex coding tasks.

Result: SEW demonstrates up to 33% improvement on LiveCodeBench compared to using the backbone LLM only.

Conclusion: SEW can automatically design and optimise agentic workflows through self-evolution, providing insights into the optimal encoding of workflow information with text.

Abstract: Large Language Models (LLMs) have demonstrated effectiveness in code
generation tasks. To enable LLMs to address more complex coding challenges,
existing research has focused on crafting multi-agent systems with agentic
workflows, where complex coding tasks are decomposed into sub-tasks, assigned
to specialized agents. Despite their effectiveness, current approaches heavily
rely on hand-crafted agentic workflows, with both agent topologies and prompts
manually designed, which limits their ability to automatically adapt to
different types of coding problems. To address these limitations and enable
automated workflow design, we propose \textbf{S}elf-\textbf{E}volving
\textbf{W}orkflow (\textbf{SEW}), a novel self-evolving framework that
automatically generates and optimises multi-agent workflows. Extensive
experiments on three coding benchmark datasets, including the challenging
LiveCodeBench, demonstrate that our SEW can automatically design agentic
workflows and optimise them through self-evolution, bringing up to 33\%
improvement on LiveCodeBench compared to using the backbone LLM only.
Furthermore, by investigating different representation schemes of workflow, we
provide insights into the optimal way to encode workflow information with text.

</details>


### [767] [An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection](https://arxiv.org/abs/2505.19059)
*Ignacio Mariano Andreozzi Pofcher,Joshua Ellul*

Main category: cs.SE

TL;DR: The paper investigates the potential of fine-tuning smaller language models for vulnerability detection, specifically focusing on reentrancy bugs in Solidity smart contracts.


<details>
  <summary>Details</summary>
Motivation: To explore if smaller language models can be fine-tuned to achieve reasonable results in detecting reentrancy bugs in Solidity smart contracts as an alternative to using Large Language Models (LLMs).

Method: Fine-tuning smaller language models and evaluating their performance in detecting reentrancy bugs in Solidity smart contracts.

Result: The results are not explicitly stated in the abstract, but the evaluation focuses on the effectiveness of smaller models in this niche area.

Conclusion: Not explicitly stated in the abstract; however, it implies that the study provides insights into the feasibility of using smaller models for vulnerability detection.

Abstract: Large Language Models (LLMs) are being used more and more for various coding
tasks, including to help coders identify bugs and are a promising avenue to
support coders in various tasks including vulnerability detection --
particularly given the flexibility of such generative AI models and tools. Yet
for many tasks it may not be suitable to use LLMs, for which it may be more
suitable to use smaller language models that can fit and easily execute and
train on a developer's computer. In this paper we explore and evaluate whether
smaller language models can be fine-tuned to achieve reasonable results for a
niche area: vulnerability detection -- specifically focusing on detecting the
reentrancy bug in Solidity smart contracts.

</details>


### [768] [Retrieval-Augmented Generation for Service Discovery: Chunking Strategies and Benchmarking](https://arxiv.org/abs/2505.19310)
*Robin D. Pesl,Jerin G. Mathew,Massimo Mecella,Marco Aiello*

Main category: cs.SE

TL;DR: 本文研究了如何通过预处理API描述来优化大型语言模型生成系统集成的能力，提出了一种发现代理（Discovery Agent）以减少输入令牌长度并提高端点检索精度。实验表明，基于端点的方法比简单的切块方法更有效，而代理的使用显著提高了精确度，但也降低了召回率。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中进行系统集成时，传统的API注册表方法存在输入令牌限制的问题，特别是对于详尽的API描述。因此，需要一种方法来有效地预处理这些API描述，以便更好地利用大型语言模型进行系统集成。

Method: 作者采用了检索增强生成（RAG）技术进行端点发现，并研究了OpenAPI的切块预处理方法。此外，他们还提出了一种发现代理，该代理仅接收最相关端点的摘要，并根据需要检索规范细节。评估使用了两个基准：SOCBench-D和RestBench，分别测试不同的切块可能性和参数对端点检索准确性的影响。

Result: 实验结果表明，基于端点的方法在预处理方面优于简单的切块方法。使用发现代理可以显著提高精确度，但可能会降低召回率，这揭示了对进一步推理能力的需求。

Conclusion: 本文展示了如何成功地应用RAG进行端点发现，从而减少令牌数量。同时，提出了一个有效的发现代理，以改善系统集成过程中大型语言模型的表现。未来的研究可能需要关注如何提高代理的召回率和推理能力。

Abstract: Integrating multiple (sub-)systems is essential to create advanced
Information Systems. Difficulties mainly arise when integrating dynamic
environments, e.g., the integration at design time of not yet existing
services. This has been traditionally addressed using a registry that provides
the API documentation of the endpoints. Large Language Models have shown to be
capable of automatically creating system integrations (e.g., as service
composition) based on this documentation but require concise input due to input
oken limitations, especially regarding comprehensive API descriptions.
Currently, it is unknown how best to preprocess these API descriptions. In the
present work, we (i) analyze the usage of Retrieval Augmented Generation for
endpoint discovery and the chunking, i.e., preprocessing, of state-of-practice
OpenAPIs to reduce the input oken length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints nd retrieves
specification details on demand. We evaluate RAG for endpoint discovery using
(iii) a proposed novel service discovery benchmark SOCBench-D representing a
general setting across numerous domains and the real-world RestBench enchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval accuracy. Then, we assess the Discovery Agent using the same
test data set. The prototype shows how to successfully employ RAG for endpoint
discovery to reduce the token count. Our experiments show that endpoint-based
approaches outperform naive chunking methods for preprocessing. Relying on an
agent significantly improves precision while being prone to decrease recall,
disclosing the need for further reasoning capabilities.

</details>


### [769] [Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI](https://arxiv.org/abs/2505.19443)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.SE

TL;DR: This review analyzes two AI-assisted software development paradigms: vibe coding and agentic coding, their differences, applications, and future trends. Successful AI software engineering will harmonize their strengths.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive analysis of the two emerging paradigms in AI-assisted software development and their potential for future development.

Method: Reviewing and comparing the conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems of vibe coding and agentic coding through detailed taxonomy and workflow analysis.

Result: Vibe systems are better for early-stage prototyping and education while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. Hybrid architectures that combine natural language interfaces with autonomous execution pipelines show promise.

Conclusion: Successful AI software engineering will rely on harmonizing the strengths of vibe coding and agentic coding within a unified, human-centered development lifecycle.

Abstract: This review presents a comprehensive analysis of two emerging paradigms in
AI-assisted software development: vibe coding and agentic coding. While both
leverage large language models (LLMs), they differ fundamentally in autonomy,
architectural design, and the role of the developer. Vibe coding emphasizes
intuitive, human-in-the-loop interaction through prompt-based, conversational
workflows that support ideation, experimentation, and creative exploration. In
contrast, agentic coding enables autonomous software development through
goal-driven agents capable of planning, executing, testing, and iterating tasks
with minimal human intervention. We propose a detailed taxonomy spanning
conceptual foundations, execution models, feedback loops, safety mechanisms,
debugging strategies, and real-world tool ecosystems. Through comparative
workflow analysis and 20 detailed use cases, we illustrate how vibe systems
thrive in early-stage prototyping and education, while agentic systems excel in
enterprise-grade automation, codebase refactoring, and CI/CD integration. We
further examine emerging trends in hybrid architectures, where natural language
interfaces are coupled with autonomous execution pipelines. Finally, we
articulate a future roadmap for agentic AI, outlining the infrastructure needed
for trustworthy, explainable, and collaborative systems. Our findings suggest
that successful AI software engineering will rely not on choosing one paradigm,
but on harmonizing their strengths within a unified, human-centered development
lifecycle.

</details>


### [770] [CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation](https://arxiv.org/abs/2505.19502)
*Guang Yang,Yu Zhou,Xiang Chen,Wei Zheng,Xing Hu,Xin Zhou,David Lo,Taolue Chen*

Main category: cs.SE

TL;DR: CODE-DITING is a new code evaluation method that balances accuracy, efficiency and explainability. It uses a data distillation framework to transfer reasoning capabilities from DeepSeek-R1671B to smaller models, significantly enhancing evaluation explainability and reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for evaluating code snippets have limitations in flexibility and scalability. LLM-as-Judge methods offer an alternative but also have their own limitations.

Method: The authors developed CODE-DITING, a novel code evaluation method using a data distillation framework that transfers reasoning capabilities from DeepSeek-R1671B to smaller models (1.5B and 7B parameters).

Result: CODE-DITING 1.5B outperforms all models with the same parameter scale and matches the performance of models with 5 times more parameters. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B despite having only 1% of their parameters.

Conclusion: CODE-DITING provides a robust alternative for code evaluation by balancing accuracy, efficiency and explainability.

Abstract: Trustworthy evaluation methods for code snippets play a crucial role in
neural code generation. Traditional methods, which either rely on reference
solutions or require executable test cases, have inherent limitation in
flexibility and scalability. The recent LLM-as-Judge methodology offers a
promising alternative by directly evaluating functional consistency between the
problem description and the generated code. To systematically understand the
landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical
study across three diverse datasets. Our investigation reveals the pros and
cons of two categories of LLM-as-Judge methods: the methods based on general
foundation models can achieve good performance but require complex prompts and
lack explainability, while the methods based on reasoning foundation models
provide better explainability with simpler prompts but demand substantial
computational resources due to their large parameter sizes. To address these
limitations, we propose CODE-DITING, a novel code evaluation method that
balances accuracy, efficiency and explainability. We develop a data
distillation framework that effectively transfers reasoning capabilities from
DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing
evaluation explainability and reducing the computational cost. With the
majority vote strategy in the inference process, CODE-DITING 1.5B outperforms
all models with the same magnitude of parameters and achieves performance which
would normally exhibit in a model with 5 times of parameter scale. CODE-DITING
7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the
parameter volume of these large models. Further experiments show that
CODEDITING is robust to preference leakage and can serve as a promising
alternative for code evaluation.

</details>


### [771] [Search-Based Software Engineering in the Landscape of AI Foundation Models](https://arxiv.org/abs/2505.19625)
*Hassan Sartaj,Shaukat Ali*

Main category: cs.SE

TL;DR: The paper proposes a research roadmap for advancing search-based software engineering (SBSE) through its interplay with foundation models (FMs), establishing an innovative perspective for the future of SBSE in the era of FMs.


<details>
  <summary>Details</summary>
Motivation: To determine the evolution of SBSE alongside the recent advancements in AI, particularly the emergence of foundation models (FMs).

Method: Propose a research roadmap that articulates the current landscape of SBSE in relation to FMs, highlights open challenges, and outlines potential research directions.

Result: Established a forward-thinking and innovative perspective for the future of SBSE in the era of FMs.

Conclusion: The proposed roadmap aims to advance SBSE through its interplay with FMs.

Abstract: Search-based software engineering (SBSE), at the intersection of artificial
intelligence (AI) and software engineering, has been an active area of research
for about 25 years. It has been applied to solve numerous problems across the
entire software engineering lifecycle and has demonstrated its versatility in
multiple domains. With the recent advancements in AI, particularly the
emergence of foundation models (FMs), the evolution of SBSE alongside FMs
remains undetermined. In this window of opportunity, we propose a research
roadmap that articulates the current landscape of SBSE in relation to
foundation models (FMs), highlights open challenges, and outlines potential
research directions for advancing SBSE through its interplay with FMs. This
roadmap aims to establish a forward-thinking and innovative perspective for the
future of SBSE in the era of FMs.

</details>


### [772] [Large Language Models in Code Co-generation for Safe Autonomous Vehicles](https://arxiv.org/abs/2505.19658)
*Ali Nouri,Beatriz Cabrero-Daniel,Zhennan Fei,Krishna Ronanki,Håkan Sivencrona,Christian Berger*

Main category: cs.SE

TL;DR: Software engineers use LLMs to speed up software implementation. For ADAS/AD systems, there's a need to assess LLM usage due to risks in safety-related development. An evaluation pipeline is proposed for sanity-checking LLM-generated code. Six LLMs are compared on safety-related tasks, and their faults are catalogued. Limitations and capabilities of LLMs in code generation are discussed.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the potential use of LLMs for ADAS or AD systems in automotive context and address the risks associated with LLMs in safety-related systems' development.

Method: Propose an evaluation pipeline for sanity-checking LLM-generated code, compare six state-of-the-art LLMs on four safety-related programming tasks, qualitatively analyze frequent faults generated by these LLMs.

Result: Created a failure-mode catalogue of the most frequent faults generated by LLMs, providing support for human reviewers.

Conclusion: Discussed the limitations and capabilities of LLMs in code generation and the application of the proposed pipeline in existing processes.

Abstract: Software engineers in various industrial domains are already using Large
Language Models (LLMs) to accelerate the process of implementing parts of
software systems. When considering its potential use for ADAS or AD systems in
the automotive context, there is a need to systematically assess this new
setup: LLMs entail a well-documented set of risks for safety-related systems'
development due to their stochastic nature. To reduce the effort for code
reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to
conduct sanity-checks on the generated code. We compare the performance of six
state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders,
Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we
qualitatively analyse the most frequent faults generated by these LLMs,
creating a failure-mode catalogue to support human reviewers. Finally, the
limitations and capabilities of LLMs in code generation, and the use of the
proposed pipeline in the existing process, are discussed.

</details>


### [773] [CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement](https://arxiv.org/abs/2505.19757)
*Maria Dziuba,Valentin Malykh*

Main category: cs.SE

TL;DR: CIDRe is a new, language-agnostic quality criterion for code comments that combines four aspects: relevance, informativeness, completeness, and description length. It outperforms existing metrics in evaluations and improves model performance when used to filter comments.


<details>
  <summary>Details</summary>
Motivation: Effective generation of structured code comments needs robust quality metrics for dataset curation, but current methods have limitations in code-comment analysis.

Method: Proposed CIDRe integrates four synergistic aspects - relevance (semantic alignment), informativeness (functional coverage), completeness (all structure sections present), and description length (detail sufficiency). It is validated on a manually annotated dataset.

Result: Experiments show CIDRe's superiority over existing metrics with improvement in cross-entropy evaluation. Models finetuned on CIDRe-filtered data demonstrate statistically significant quality gains.

Conclusion: CIDRe serves as an effective, language-agnostic reference-free quality criterion for enhancing the generation of structured code comments.

Abstract: Effective generation of structured code comments requires robust quality
metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)
suffer from limited code-comment analysis. We propose CIDRe, a
language-agnostic reference-free quality criterion combining four synergistic
aspects: (1) relevance (code-comment semantic alignment), (2) informativeness
(functional coverage), (3) completeness (presence of all structure sections),
and (4) description length (detail sufficiency). We validate our criterion on a
manually annotated dataset. Experiments demonstrate CIDRe's superiority over
existing metrics, achieving improvement in cross-entropy evaluation. When
applied to filter comments, the models finetuned on CIDRe-filtered data show
statistically significant quality gains in GPT-4o-mini assessments.

</details>


### [774] [StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs](https://arxiv.org/abs/2505.20139)
*Jialin Yang,Dongfu Jiang,Lipeng He,Sherman Siu,Yuxuan Zhang,Disen Liao,Zhuofeng Li,Huaye Zeng,Yiming Jia,Haozhe Wang,Benjamin Schneider,Chi Ruan,Wentao Ma,Zhiheng Lyu,Yifei Wang,Yi Lu,Quy Duc Do,Ziyan Jiang,Ping Nie,Wenhu Chen*

Main category: cs.SE

TL;DR: The paper presents StructEval, a benchmark for assessing Large Language Models' (LLMs) ability to generate structured outputs such as JSON, YAML, CSV, HTML, React, and SVG. It includes 18 formats and 44 task types with new metrics. Results show state-of-the-art models score around 75.58 on average, with open-source alternatives trailing by about 10 points.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the performance of LLMs in generating structured outputs across diverse formats, addressing gaps in existing benchmarks.

Method: StructEval evaluates structural fidelity through two paradigms: generation tasks (producing structured output from natural language prompts) and conversion tasks (translating between structured formats). The benchmark covers 18 formats and 44 types of tasks with novel metrics for format adherence and structural correctness.

Result: Significant performance gaps exist among LLMs, with top models scoring an average of 75.58. Open-source alternatives lag approximately 10 points behind. Generation tasks are found more challenging than conversion tasks, and creating correct visual content is harder than text-only structures.

Conclusion: StructEval highlights the challenges LLMs face in producing structured outputs and provides a comprehensive framework for their evaluation.

Abstract: As Large Language Models (LLMs) become integral to software development
workflows, their ability to generate structured outputs has become critically
important. We introduce StructEval, a comprehensive benchmark for evaluating
LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and
renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,
StructEval systematically evaluates structural fidelity across diverse formats
through two paradigms: 1) generation tasks, producing structured output from
natural language prompts, and 2) conversion tasks, translating between
structured formats. Our benchmark encompasses 18 formats and 44 types of task,
with novel metrics for format adherence and structural correctness. Results
reveal significant performance gaps, even state-of-the-art models like o1-mini
achieve only 75.58 average score, with open-source alternatives lagging
approximately 10 points behind. We find generation tasks more challenging than
conversion tasks, and producing correct visual content more difficult than
generating text-only structures.

</details>


### [775] [Evaluating Large Language Models for Code Review](https://arxiv.org/abs/2505.20206)
*Umut Cihan,Arda İçöz,Vahid Haratian,Eray Tüzün*

Main category: cs.SE

TL;DR: AI语言模型如GPT4o和Gemini 2.0 Flash在代码审查中的表现被研究，尽管有一定准确性但存在错误输出的风险。因此，建议采用'人在环路'的代码审查方式来平衡知识共享与风险。


<details>
  <summary>Details</summary>
Motivation: 代码审查对软件质量至关重要，当前虽然有基于大型语言模型（LLMs）的代码审查工具，但它们的可靠性和准确性尚未得到系统评估。

Method: 研究测试了GPT4o和Gemini 2.0 Flash两款LLMs在处理492个不同正确性的AI生成代码块以及164个来自HumanEval基准的经典代码块时的表现。实验期望这些模型能评估代码正确性并根据需要改进代码，并以不同配置运行实验。

Result: 在提供问题描述的情况下，GPT4o和Gemini 2.0 Flash分别以68.50%和63.89%的准确率分类代码正确性，并分别以67.83%和54.26%的成功率修正代码。在没有问题描述的情况下，性能有所下降。对于经典代码块，结果有所不同，表明性能依赖于代码类型。

Conclusion: LLMs可以辅助代码审查，提出改进建议和评估代码正确性，但存在错误输出的风险。因此，提出了一个包含人类参与的“人在环路”代码审查过程，以促进知识共享同时降低风险。

Abstract: Context: Code reviews are crucial for software quality. Recent AI advances
have allowed large language models (LLMs) to review and fix code; now, there
are tools that perform these reviews. However, their reliability and accuracy
have not yet been systematically evaluated. Objective: This study compares
different LLMs' performance in detecting code correctness and suggesting
improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated
code blocks of varying correctness, along with 164 canonical code blocks from
the HumanEval benchmark. To simulate the code review task objectively, we
expected LLMs to assess code correctness and improve the code if needed. We ran
experiments with different configurations and reported on the results. Results:
With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code
correctness 68.50% and 63.89% of the time, respectively, and corrected the code
67.83% and 54.26% of the time for the 492 code blocks of varying correctness.
Without problem descriptions, performance declined. The results for the 164
canonical code blocks differed, suggesting that performance depends on the type
of code. Conclusion: LLM code reviews can help suggest improvements and assess
correctness, but there is a risk of faulty outputs. We propose a process that
involves humans, called the "Human in the loop LLM Code Review" to promote
knowledge sharing while mitigating the risk of faulty outputs.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [776] [SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training](https://arxiv.org/abs/2505.18377)
*Pingchuan Ma,Ziang Yin,Qi Jing,Zhengqi Gao,Nicholas Gangi,Boyang Zhang,Tsung-Wei Huang,Zhaoran Huang,Duane S. Boning,Yu Yao,Jiaqi Gu*

Main category: physics.optics

TL;DR: SP2RINT is a new training framework for DONNs that achieves digital-comparable accuracy while being significantly faster than existing methods, bridging the gap between abstract models and implementable photonic hardware.


<details>
  <summary>Details</summary>
Motivation: Training DONN systems to determine metasurface structures remains challenging due to limitations of heuristic methods and computational prohibitive simulation-in-the-loop training methods.

Method: SP2RINT formulates DONN training as a PDE-constrained learning problem with spatially decoupled, progressive training. It relaxes metasurface responses into trainable transfer matrices, progressively enforces physical constraints, and uses a physics-inspired, spatially decoupled inverse design strategy.

Result: SP2RINT achieves digital-comparable accuracy while being 1825 times faster than simulation-in-the-loop approaches when evaluated across diverse DONN training tasks.

Conclusion: SP2RINT enables scalable, high-performance training of physically realizable meta-optical neural systems, bridging the gap between abstract DONN models and implementable photonic hardware.

Abstract: DONNs harness the physics of light propagation for efficient analog
computation, with applications in AI and signal processing. Advances in
nanophotonic fabrication and metasurface-based wavefront engineering have
opened new pathways to realize high-capacity DONNs across various spectral
regimes. Training such DONN systems to determine the metasurface structures
remains challenging. Heuristic methods are fast but oversimplify metasurfaces
modulation, often resulting in physically unrealizable designs and significant
performance degradation. Simulation-in-the-loop training methods directly
optimize a physically implementable metasurface using adjoint methods during
end-to-end DONN training, but are inherently computationally prohibitive and
unscalable.To address these limitations, we propose SP2RINT, a spatially
decoupled, progressive training framework that formulates DONN training as a
PDE-constrained learning problem. Metasurface responses are first relaxed into
freely trainable transfer matrices with a banded structure. We then
progressively enforce physical constraints by alternating between transfer
matrix training and adjoint-based inverse design, avoiding per-iteration PDE
solves while ensuring final physical realizability. To further reduce runtime,
we introduce a physics-inspired, spatially decoupled inverse design strategy
based on the natural locality of field interactions. This approach partitions
the metasurface into independently solvable patches, enabling scalable and
parallel inverse design with system-level calibration. Evaluated across diverse
DONN training tasks, SP2RINT achieves digital-comparable accuracy while being
1825 times faster than simulation-in-the-loop approaches. By bridging the gap
between abstract DONN models and implementable photonic hardware, SP2RINT
enables scalable, high-performance training of physically realizable
meta-optical neural systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [777] [Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions](https://arxiv.org/abs/2505.18362)
*Nathan Gaby,Xiaojing Ye*

Main category: math.OC

TL;DR: The paper develops a theoretical framework for optimal probability density control and proposes a scalable numerical algorithm using deep neural networks to solve high-dimensional problems.


<details>
  <summary>Details</summary>
Motivation: To create a general theoretical framework for optimal probability density control and develop a numerical method that can handle high-dimensional control problems effectively without relying on Wasserstein theory.

Method: Establish the Pontryagin Maximum Principle (PMP) for optimal density control, derive the Hamilton-Jacobi-Bellman (HJB) equation for the value functional, and use reduced-order models like deep neural networks to parameterize the control vector-field and adjoint function for solving high-dimensional problems numerically.

Result: The algorithm shows promising performance in solving various density control problems with obstacles and nonlinear interactions in high dimensions.

Conclusion: A general theoretical framework for optimal probability density control is developed along with a scalable numerical algorithm based on deep neural networks which proves effective in high-dimensional scenarios.

Abstract: We develop a general theoretical framework for optimal probability density
control and propose a numerical algorithm that is scalable to solve the control
problem in high dimensions. Specifically, we establish the Pontryagin Maximum
Principle (PMP) for optimal density control and construct the
Hamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous
derivations without any concept from Wasserstein theory. To solve the density
control problem numerically, we propose to use reduced-order models, such as
deep neural networks (DNNs), to parameterize the control vector-field and the
adjoint function, which allows us to tackle problems defined on
high-dimensional state spaces. We also prove several convergence properties of
the proposed algorithm. Numerical results demonstrate promising performances of
our algorithm on a variety of density control problems with obstacles and
nonlinear interaction challenges in high dimensions.

</details>


### [778] [Fractional-Boundary-Regularized Deep Galerkin Method for Variational Inequalities in Mixed Optimal Stopping and Control](https://arxiv.org/abs/2505.19309)
*Yun Zhao,Harry Zheng*

Main category: math.OC

TL;DR: This paper addresses the difficulty in solving variational inequalities with non-linear HJB operators by transforming them into linear operators through a dual approach, and introduces FBR-DGM to improve network approximation accuracy. It also provides benchmarks via primal-dual relationship checks.


<details>
  <summary>Details</summary>
Motivation: To solve the numerical difficulty and lack of reliable benchmarks for mixed optimal stopping and stochastic control problems involving non-linear HJB operators.

Method: The method involves transforming the problem into a linear operator using the dual approach and then applying the Fractional-Boundary-Regularized Deep Galerkin Method (FBR-DGM), which enhances classical $L^2$ loss with Sobolev-Slobodeckij norms on the parabolic boundary.

Result: Improved accuracy in network approximation and its derivatives, enabling conversion back to the original solution and providing innovative benchmarks through the primal-dual relationship.

Conclusion: The proposed approach offers a reliable method for solving mixed optimal stopping and stochastic control problems, providing consistent improvements and benchmarks where analytical solutions are unavailable.

Abstract: Mixed optimal stopping and stochastic control problems define variational
inequalities with non-linear Hamilton-Jacobi-Bellman (HJB) operators, whose
numerical solution is notoriously difficult and lack of reliable benchmarks. We
first use the dual approach to transform it into a linear operator, and then
introduce a Fractional-Boundary-Regularized Deep Galerkin Method (FBR-DGM) that
augments the classical $L^2$ loss with Sobolev-Slobodeckij norms on the
parabolic boundary, enforcing regularity and yielding consistent improvements
in the network approximation and its derivatives. The improved accuracy allows
the network to be converted back to the original solution using the dual
transform. The self-consistency and stability of the network can be tested by
checking the primal-dual relationship among optimal value, optimal wealth, and
optimal control, offering innovative benchmarks in the absence of analytical
solutions.

</details>


### [779] [A Structured Tour of Optimization with Finite Differences](https://arxiv.org/abs/2505.19720)
*Marco Rando,Cesare Molinari,Lorenzo Rosasco,Silvia Villa*

Main category: math.OC

TL;DR: Finite-difference methods using structured directions can improve gradient estimation accuracy and optimization performance without increasing computational costs significantly.


<details>
  <summary>Details</summary>
Motivation: To examine the impact of structured direction selection in finite-difference methods for zeroth-order optimization.

Method: Review and extend strategies for constructing structured direction matrices, then compare them with unstructured approaches in terms of computational cost, gradient approximation quality, and convergence behavior.

Result: Structured directions can be generated with computational costs comparable to unstructured ones while significantly improving gradient estimation accuracy and optimization performance.

Conclusion: Structured direction selection in finite-difference methods is beneficial for zeroth-order optimization.

Abstract: Finite-difference methods are widely used for zeroth-order optimization in
settings where gradient information is unavailable or expensive to compute.
These procedures mimic first-order strategies by approximating gradients
through function evaluations along a set of random directions. From a
theoretical perspective, recent studies indicate that imposing structure (such
as orthogonality) on the chosen directions allows for the derivation of
convergence rates comparable to those achieved with unstructured random
directions (i.e., directions sampled independently from a distribution).
Empirically, although structured directions are expected to enhance
performance, they often introduce additional computational costs, which can
limit their applicability in high-dimensional settings. In this work, we
examine the impact of structured direction selection in finite-difference
methods. We review and extend several strategies for constructing structured
direction matrices and compare them with unstructured approaches in terms of
computational cost, gradient approximation quality, and convergence behavior.
Our evaluation spans both synthetic tasks and real-world applications such as
adversarial perturbation. The results demonstrate that structured directions
can be generated with computational costs comparable to unstructured ones while
significantly improving gradient estimation accuracy and optimization
performance.

</details>


### [780] [New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results](https://arxiv.org/abs/2505.20219)
*Francesco Orabona,Ryan D'Orazio*

Main category: math.OC

TL;DR: The paper reinterprets the Polyak stepsize and its variants as gradient descent on a surrogate loss, offering a unified analysis of their convergence properties and identifying real non-convergence issues.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the convergence characteristics and limitations of the Polyak stepsize and its stochastic variants in convex optimization.

Method: Reinterpret the Polyak stepsize and its variants as performing gradient descent on a surrogate loss, demonstrating that each variant minimizes a surrogate function with adaptive stepsizes based on guaranteed local curvature.

Result: Achieves a unified analysis of various Polyak stepsize variants under different assumptions and proves several negative results indicating genuine non-convergence scenarios.

Conclusion: This new perspective offers a simpler and more unified understanding of the Polyak stepsize methods, highlighting both their strengths and limitations.

Abstract: The Polyak stepsize has been proven to be a fundamental stepsize in convex
optimization, giving near optimal gradient descent rates across a wide range of
assumptions. The universality of the Polyak stepsize has also inspired many
stochastic variants, with theoretical guarantees and strong empirical
performance. Despite the many theoretical results, our understanding of the
convergence properties and shortcomings of the Polyak stepsize or its variants
is both incomplete and fractured across different analyses. We propose a new,
unified, and simple perspective for the Polyak stepsize and its variants as
gradient descent on a surrogate loss. We show that each variant is equivalent
to minimize a surrogate function with stepsizes that adapt to a guaranteed
local curvature. Our general surrogate loss perspective is then used to provide
a unified analysis of existing variants across different assumptions. Moreover,
we show a number of negative results proving that the non-convergence results
in some of the upper bounds is indeed real.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [781] [Geometric Determinations Of Characteristic Redshifts From DESI-DR2 BAO and DES-SN5YR Observations: Hints For New Expansion Rate Anomalies](https://arxiv.org/abs/2505.19083)
*Purba Mukherjee,Anjan A Sen*

Main category: astro-ph.CO

TL;DR: 通过结合DESI-DR2 BAO和DES-SN5YR数据，使用高斯过程回归和基于节点的样条技术对宇宙膨胀历史进行了模型无关的重建。在红移范围z~0.35-0.55中发现了与Planck 2018 ΛCDM预测显著偏离的现象。这些偏离可能暗示了超出标准宇宙学框架的新物理现象，并强调了未来数据集调查的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过几何确定特征红移来揭示宇宙膨胀率中的显著紧张关系，采用模型无关的方法以避免特定宇宙学假设的影响。

Method: 结合DESI-DR2 BAO和DES-SN5YR数据，利用高斯过程回归和基于节点的样条技术重建宇宙距离及其导数，从而推断E(z)并识别特征红移。

Result: 发现大约4到5σ的显著偏离，特别是在红移范围z~0.35-0.55中，这种偏离在两种重建方法和组合数据集中都是一致的。

Conclusion: 结果表明，这些晚时间偏离可能是新物理的标志，并强调了特征红移作为扩张率异常敏感指标的重要性，同时呼吁用未来的数据进行进一步研究。

Abstract: In this work, we perform a model-agnostic reconstruction of the cosmic
expansion history by combining DESI-DR2 BAO and DES-SN5YR data, with a focus on
geometric determination of characteristic redshifts where notable tensions in
the expansion rate are found to emerge. Employing Gaussian process regression
alongside knot-based spline techniques, we reconstruct cosmic distances and
their derivatives to pinpoint these characteristic redshifts and infer $E(z)$.
Our analysis reveals significant deviations of approximately 4 to 5$\sigma$
from the Planck 2018 $\Lambda$CDM predictions, particularly pronounced in the
redshift range $z \sim 0.35-0.55$. These anomalies are consistently observed
across both reconstruction methods and combined datasets, indicating robust
late-time departures that could signal new physics beyond the standard
cosmological framework. The joint use of BAO and SN probes enhances the
precision of our constraints, allowing us to isolate these deviations without
reliance on specific cosmological assumptions. Our findings underscore the role
of characteristic redshifts as sensitive indicators of expansion rate anomalies
and motivate further scrutiny with forthcoming datasets from DESI-5YR BAO,
Euclid, and LSST. These future surveys will tighten constraints and help
distinguish whether these late-time anomalies arise from new fundamental
physics or unresolved systematics in the data.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [782] [ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications](https://arxiv.org/abs/2505.19983)
*Tong Wu,Zhiyong Chen,Dazhi He,Feng Yang,Meixia Tao,Xiaodong Xu,Wenjun Zhang,Ping Zhang*

Main category: cs.IT

TL;DR: Diffusion models can effectively reduce interference in wireless semantic communication systems.


<details>
  <summary>Details</summary>
Motivation: The broadcast nature of wireless signals makes them susceptible to Gaussian noise and unaware interference. This raises the question of whether diffusion models (DMs) can effectively mitigate interference in wireless semantic communication systems.

Method: The paper models the interference cancellation problem as a maximum a posteriori (MAP) problem over the joint posterior probability of the signal and interference. They develop an Interference Cancellation Diffusion Model (ICDM) which decomposes the joint posterior into independent prior probabilities of the signal and interference, along with the channel transition probability. The log-gradients of these distributions at each time step are learned separately by DMs and accurately estimated through deriving. ICDM integrates these gradients with advanced numerical iteration method.

Result: Extensive experiments demonstrate that ICDM significantly reduces the mean square error (MSE) and enhances perceptual quality compared to schemes without ICDM. For example, on the CelebA dataset under the Rayleigh fading channel with a signal-to-noise ratio (SNR) of 20 dB and signal to interference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB and improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.

Conclusion: ICDM provides excellent estimates for the signal and interference, achieving accurate and rapid interference cancellation.

Abstract: Diffusion models (DMs) have recently achieved significant success in wireless
communications systems due to their denoising capabilities. The broadcast
nature of wireless signals makes them susceptible not only to Gaussian noise,
but also to unaware interference. This raises the question of whether DMs can
effectively mitigate interference in wireless semantic communication systems.
In this paper, we model the interference cancellation problem as a maximum a
posteriori (MAP) problem over the joint posterior probability of the signal and
interference, and theoretically prove that the solution provides excellent
estimates for the signal and interference. To solve this problem, we develop an
interference cancellation diffusion model (ICDM), which decomposes the joint
posterior into independent prior probabilities of the signal and interference,
along with the channel transition probablity. The log-gradients of these
distributions at each time step are learned separately by DMs and accurately
estimated through deriving. ICDM further integrates these gradients with
advanced numerical iteration method, achieving accurate and rapid interference
cancellation. Extensive experiments demonstrate that ICDM significantly reduces
the mean square error (MSE) and enhances perceptual quality compared to schemes
without ICDM. For example, on the CelebA dataset under the Rayleigh fading
channel with a signal-to-noise ratio (SNR) of $20$ dB and signal to
interference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB
and improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [783] [LA-RCS: LLM-Agent-Based Robot Control System](https://arxiv.org/abs/2505.18214)
*TaekHyun Park,YoungJun Choi,SeungHoon Shin,Kwangil Lee*

Main category: cs.RO

TL;DR: LA-RCS is a LLM-agent-based robot control system that can autonomously plan, work and analyze the external environment based on user requirements. It uses a dual-agent framework to generate plans, observe the environment, execute and modify plans as needed. The system can interpret natural language commands and convert them into robot-compatible commands. Autonomous evaluation of observation results, task feedback, and real-time environmental monitoring reduce the need for user intervention. Performance was quantitatively assessed in four scenarios with an average success rate of 90%.


<details>
  <summary>Details</summary>
Motivation: To create a sophisticated robot control system that can autonomously plan, work and analyze the external environment using LLM-Agent, reducing the need for user intervention.

Method: Utilize a dual-agent framework to generate plans based on user requests, observe the external environment, execute plans, and modify them as necessary. Interpret natural language commands from users and convert them into compatible robot commands. Evaluate observation results autonomously, provide task feedback, and execute commands based on real-time environmental monitoring.

Result: The performance of LA-RCS was categorized into four distinct types of scenarios and was quantitatively evaluated. The system achieved an average success rate of 90% across these scenarios.

Conclusion: LA-RCS demonstrates a high capability to fulfill user requests satisfactorily with an average success rate of 90%, indicating its effectiveness in autonomous robot control.

Abstract: LA-RCS (LLM-agent-based robot control system) is a sophisticated robot
control system designed to autonomously plan, work, and analyze the external
environment based on user requirements by utilizing LLM-Agent. Utilizing a
dual-agent framework, LA-RCS generates plans based on user requests, observes
the external environment, executes the plans, and modifies the plans as needed
to adapt to changes in the external conditions. Additionally, LA-RCS interprets
natural language commands by the user and converts them into commands
compatible with the robot interface so that the robot can execute tasks and
meet user requests properly. During his process, the system autonomously
evaluates observation results, provides feedback on the tasks, and executes
commands based on real-time environmental monitoring, significantly reducing
the need for user intervention in fulfilling requests. We categorized the
scenarios that LA-RCS needs to perform into four distinct types and conducted a
quantitative assessment of its performance in each scenario. The results showed
an average success rate of 90 percent, demonstrating the system capability to
fulfill user requests satisfactorily. For more extensive results, readers can
visit our project page: https://la-rcs.github.io

</details>


### [784] [BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs](https://arxiv.org/abs/2505.18229)
*Mingning Guo,Mengwei Wu,Jiarun He,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.RO

TL;DR: 提出BEDI基准，解决无人机实体智能评估中的标准化和多样化问题，通过分解复杂任务、设计统一框架和构建混合测试平台，促进实体智能研究的发展。


<details>
  <summary>Details</summary>
Motivation: 当前无人机实体智能的评估方法缺乏标准化基准、多样化的测试场景和开放系统接口。

Method: 引入动态实体任务链范式，将复杂任务分解为子任务；设计包含五个核心子技能的统一评估框架；构建结合静态现实环境与动态虚拟场景的混合测试平台，提供开放标准接口。

Result: 揭示了现有视觉-语言模型在实体无人机任务中的局限性，强调了BEDI基准对推动实体智能研究和模型优化的关键作用。

Conclusion: BEDI填补了该领域系统化和标准化评估的空白，支持客观模型比较，并为未来发展奠定基础。

Abstract: With the rapid advancement of low-altitude remote sensing and Vision-Language
Models (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have
shown significant potential in autonomous tasks. However, current evaluation
methods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of
standardized benchmarks, diverse testing scenarios and open system interfaces.
To address these challenges, we propose BEDI (Benchmark for Embodied Drone
Intelligence), a systematic and standardized benchmark designed for evaluating
UAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task
paradigm based on the perception-decision-action loop, which decomposes complex
UAV tasks into standardized, measurable subtasks. Building on this paradigm, we
design a unified evaluation framework encompassing five core sub-skills:
semantic perception, spatial perception, motion control, tool utilization, and
task planning. Furthermore, we construct a hybrid testing platform that
integrates static real-world environments with dynamic virtual scenarios,
enabling comprehensive performance assessment of UAV-EAs across varied
contexts. The platform also offers open and standardized interfaces, allowing
researchers to customize tasks and extend scenarios, thereby enhancing
flexibility and scalability in the evaluation process. Finally, through
empirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their
limitations in embodied UAV tasks, underscoring the critical role of the BEDI
benchmark in advancing embodied intelligence research and model optimization.
By filling the gap in systematic and standardized evaluation within this field,
BEDI facilitates objective model comparison and lays a robust foundation for
future development in this field. Our benchmark will be released at
https://github.com/lostwolves/BEDI .

</details>


### [785] [CrashAgent: Crash Scenario Generation via Multi-modal Reasoning](https://arxiv.org/abs/2505.18341)
*Miao Li,Wenhao Ding,Haohong Lin,Yiqi Lyu,Yihang Yao,Yuyou Zhang,Ding Zhao*

Main category: cs.RO

TL;DR: This paper presents CrashAgent, a multi-agent framework using Multi-modal Large Language Models to convert real-world traffic crash reports into structured scenarios for autonomous driving simulations. It aims to address the lack of safety-critical cases in current datasets.


<details>
  <summary>Details</summary>
Motivation: Current datasets for training and evaluating autonomous driving algorithms are primarily composed of normal driving behaviors, lacking sufficient safety-critical scenarios that are essential for developing safe driving skills.

Method: The method involves utilizing Multi-modal Large Language Models to transform accident crash reports into structured scenario formats suitable for simulations. A multi-agent framework called CrashAgent is introduced, which interprets multi-modal crash reports to generate road layouts and behaviors of vehicles and traffic participants.

Result: The generated crash scenarios are evaluated comprehensively on layout reconstruction accuracy, collision rate, and diversity. The result is a high-quality, large-scale crash dataset that will be made publicly available.

Conclusion: CrashAgent provides a valuable resource for improving autonomous driving algorithms' ability to handle safety-critical situations by generating diverse and accurate crash scenarios.

Abstract: Training and evaluating autonomous driving algorithms requires a diverse
range of scenarios. However, most available datasets predominantly consist of
normal driving behaviors demonstrated by human drivers, resulting in a limited
number of safety-critical cases. This imbalance, often referred to as a
long-tail distribution, restricts the ability of driving algorithms to learn
from crucial scenarios involving risk or failure, scenarios that are essential
for humans to develop driving skills efficiently. To generate such scenarios,
we utilize Multi-modal Large Language Models to convert crash reports of
accidents into a structured scenario format, which can be directly executed
within simulations. Specifically, we introduce CrashAgent, a multi-agent
framework designed to interpret multi-modal real-world traffic crash reports
for the generation of both road layouts and the behaviors of the ego vehicle
and surrounding traffic participants. We comprehensively evaluate the generated
crash scenarios from multiple perspectives, including the accuracy of layout
reconstruction, collision rate, and diversity. The resulting high-quality and
large-scale crash dataset will be publicly available to support the development
of safe driving algorithms in handling safety-critical situations.

</details>


### [786] [Reinforcement Learning for Ballbot Navigation in Uneven Terrain](https://arxiv.org/abs/2505.18417)
*Achkan Salehi*

Main category: cs.RO

TL;DR: 该研究开发了一个基于MuJoCo的开源Ballbot模拟器，通过适当的外部感知观察条件和奖励塑造，证明了经典无模型RL方法学习的策略能够有效穿越随机生成的不平坦地形。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在建模精度和适应性方面具有优势，但关于球形机器人（Ballbot）控制和导航的RL方法的能力、数据效率和局限性的研究很少，且缺乏开源的、适合RL的模拟器。

Method: 创建了一个基于MuJoCo的开源Ballbot模拟器，使用经典无模型RL方法，结合适当的外部感知观察条件和奖励塑造，进行训练。

Result: 策略能够在合理的数据量下（四到五小时，系统运行频率为500Hz），有效穿越随机生成的不平坦地形。

Conclusion: 适当条件下，经典无模型RL方法可以有效地用于Ballbot的导航任务，并且提供了开源模拟器以促进未来的研究。

Abstract: Ballbot (i.e. Ball balancing robot) navigation usually relies on methods
rooted in control theory (CT), and works that apply Reinforcement learning (RL)
to the problem remain rare while generally being limited to specific subtasks
(e.g. balance recovery). Unlike CT based methods, RL does not require
(simplifying) assumptions about environment dynamics (e.g. the absence of
slippage between the ball and the floor). In addition to this increased
accuracy in modeling, RL agents can easily be conditioned on additional
observations such as depth-maps without the need for explicit formulations from
first principles, leading to increased adaptivity. Despite those advantages,
there has been little to no investigation into the capabilities,
data-efficiency and limitations of RL based methods for ballbot control and
navigation. Furthermore, there is a notable absence of an open-source,
RL-friendly simulator for this task. In this paper, we present an open-source
ballbot simulation based on MuJoCo, and show that with appropriate conditioning
on exteroceptive observations as well as reward shaping, policies learned by
classical model-free RL methods are capable of effectively navigating through
randomly generated uneven terrain, using a reasonable amount of data (four to
five hours on a system operating at 500hz).

</details>


### [787] [VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning](https://arxiv.org/abs/2505.18719)
*Guanxing Lu,Wenkai Guo,Chubin Zhang,Yuheng Zhou,Haonan Jiang,Zifeng Gao,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: Recent VLA models perform well on robotic manipulation tasks but fail in out-of-distribution scenarios. The paper presents VLA-RL, which uses online reinforcement learning to improve pretrained VLAs in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of execution failure in out-of-distribution scenarios when using offline data with limited visited states.

Method: Introduce a trajectory-level RL formulation for auto-regressive VLA training, fine-tune a pretrained vision-language model as a robotic process reward model trained on pseudo reward labels, and implement findings that improve stability and efficiency.

Result: VLA-RL enables OpenVLA-7B to surpass the strongest finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in LIBERO, matching the performance of advanced commercial models.

Conclusion: VLA-RL benefits from increased test-time optimization, showing early signs of inference scaling laws in robotics.

Abstract: Recent high-capacity vision-language-action (VLA) models have demonstrated
impressive performance on a range of robotic manipulation tasks by imitating
human demonstrations. However, exploiting offline data with limited visited
states will cause execution failure in out-of-distribution scenarios.
Intuitively, an exploration-based method that improves on online collected data
at test time could address this limitation. We present VLA-RL, an algorithmic
and systematic framework that leverages online reinforcement learning (RL) to
improve pretrained auto-regressive VLAs in downstream tasks. Within a unified
perspective, we first introduce a trajectory-level RL formulation for
auto-regressive VLA training, which models general robotic manipulation
trajectory as multi-modal multi-turn conversation. To address the challenge of
sparse rewards, we fine-tune a pretrained vision-language model as a robotic
process reward model, which is trained on pseudo reward labels annotated on
automatically extracted task segments. To scale up, we identify several
implementation findings that improve the stability and efficiency including
curriculum selection strategy, GPU-balanced vectorized environments, batch
decoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest
finetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in
LIBERO, and even matches the performance of advanced commercial models such as
$\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time
optimization, indicating an early spark of inference scaling laws in robotics.

</details>


### [788] [MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation](https://arxiv.org/abs/2505.19086)
*Chen Tessler,Yifeng Jiang,Erwin Coumans,Zhengyi Luo,Gal Chechik,Xue Bin Peng*

Main category: cs.RO

TL;DR: 人类通过全身精确控制与世界互动以实现多功能目标，当前方法在物理动画中虽能成功完成特定任务但缺乏对整个系统高阶目标的灵活性。为解决此问题，本文提出了MaskedManipulator，一种通过两阶段学习方法开发的统一生成策略模型，允许用户通过高层次的目标定义复杂的动作任务。


<details>
  <summary>Details</summary>
Motivation: 目前全身体操性操作的方法虽然在具体任务上表现良好，但其控制模式（如详细运动跟踪、连续对象轨迹跟随或直接VR远程操作）在整个人体-物体耦合系统中的高阶目标规范方面提供了有限的多样性。

Method: 首先训练一个追踪控制器，从大规模的人类动作捕捉数据集中物理重建复杂的人体-物体交互。然后将该追踪控制器提炼成MaskedManipulator，提供用户对角色身体和操纵物体的直观控制。

Result: MaskedManipulator使用户能够通过直观的高层目标（如目标物体姿态、关键角色姿势）指定复杂的动作任务，并为物理模拟的人形角色合成必要的全身动作。

Conclusion: MaskedManipulator通过生成必要的人体全身动作来实现用户设定的目标，为更互动和逼真的虚拟角色铺平了道路。

Abstract: Humans interact with their world while leveraging precise full-body control
to achieve versatile goals. This versatility allows them to solve long-horizon,
underspecified problems, such as placing a cup in a sink, by seamlessly
sequencing actions like approaching the cup, grasping, transporting it, and
finally placing it in the sink. Such goal-driven control can enable new
procedural tools for animation systems, enabling users to define partial
objectives while the system naturally ``fills in'' the intermediate motions.
However, while current methods for whole-body dexterous manipulation in
physics-based animation achieve success in specific interaction tasks, they
typically employ control paradigms (e.g., detailed kinematic motion tracking,
continuous object trajectory following, or direct VR teleoperation) that offer
limited versatility for high-level goal specification across the entire coupled
human-object system. To bridge this gap, we present MaskedManipulator, a
unified and generative policy developed through a two-stage learning approach.
First, our system trains a tracking controller to physically reconstruct
complex human-object interactions from large-scale human mocap datasets. This
tracking controller is then distilled into MaskedManipulator, which provides
users with intuitive control over both the character's body and the manipulated
object. As a result, MaskedManipulator enables users to specify complex
loco-manipulation tasks through intuitive high-level objectives (e.g., target
object poses, key character stances), and MaskedManipulator then synthesizes
the necessary full-body actions for a physically simulated humanoid to achieve
these goals, paving the way for more interactive and life-like virtual
characters.

</details>


### [789] [Reinforcement Twinning for Hybrid Control of Flapping-Wing Drones](https://arxiv.org/abs/2505.18201)
*Romain Poletti,Lorenzo Schena,Lilla Koloszar,Joris Degroote,Miguel Alfonso Mendez*

Main category: cs.RO

TL;DR: This paper proposes a hybrid model-free/model-based flight control approach for flapping-wing drones using reinforcement twinning algorithm, which outperforms purely model-free and model-based methods in various initialization scenarios.


<details>
  <summary>Details</summary>
Motivation: Flapping-wing drones have complex dynamics that are hard to model accurately with traditional model-based or model-free methods alone.

Method: A hybrid approach combining model-based control using an adaptive digital twin with model-free reinforcement learning. The two methods collaborate through transfer learning, imitation learning, and experience sharing, with a referee selecting the best agent for real-world interaction based on performance metrics.

Result: The hybrid control learning approach shows superior performance compared to purely model-free and model-based methods across three different initialization scenarios of the adaptive model.

Conclusion: The proposed hybrid method effectively controls flapping-wing drone flight by leveraging both accurate modeling and efficient high-dimensional control landscapes navigation.

Abstract: Controlling the flight of flapping-wing drones requires versatile controllers
that handle their time-varying, nonlinear, and underactuated dynamics from
incomplete and noisy sensor data. Model-based methods struggle with accurate
modeling, while model-free approaches falter in efficiently navigating very
high-dimensional and nonlinear control objective landscapes. This article
presents a novel hybrid model-free/model-based approach to flight control based
on the recently proposed reinforcement twinning algorithm. The model-based (MB)
approach relies on an adjoint formulation using an adaptive digital twin,
continuously identified from live trajectories, while the model-free (MF)
approach relies on reinforcement learning. The two agents collaborate through
transfer learning, imitation learning, and experience sharing using the real
environment, the digital twin and a referee. The latter selects the best agent
to interact with the real environment based on performance within the digital
twin and a real-to-virtual environment consistency ratio. The algorithm is
evaluated for controlling the longitudinal dynamics of a flapping-wing drone,
with the environment simulated as a nonlinear, time-varying dynamical system
under the influence of quasi-steady aerodynamic forces. The hybrid control
learning approach is tested with three types of initialization of the adaptive
model: (1) offline identification using previously available data, (2) random
initialization with full online identification, and (3) offline pre-training
with an estimation bias, followed by online adaptation. In all three scenarios,
the proposed hybrid learning approach demonstrates superior performance
compared to purely model-free and model-based methods.

</details>


### [790] [Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning](https://arxiv.org/abs/2505.18487)
*Junlin Wang,Zhiyun Lin*

Main category: cs.RO

TL;DR: This paper introduces ICon, a contrastive learning method for Vision Transformers that separates agent-specific and environment-specific tokens to create body-centric visual representations, improving policy performance and transferability in robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Learning effective visual representations for robotic manipulation is challenging due to complex body dynamics. The authors aim to explore how body-relevant cues in visual representations can enhance policy learning efficiency for downstream robotic manipulation tasks.

Method: The method, called ICon (Inter-token Contrast), applies contrastive learning to token-level representations of Vision Transformers. It enforces separation in the feature space between agent-specific and environment-specific tokens, creating agent-centric visual representations with body-specific inductive biases. This framework can be integrated into end-to-end policy learning by adding the contrastive loss as an auxiliary objective.

Result: Experiments demonstrate that ICon improves policy performance across various manipulation tasks and facilitates policy transfer across different robots.

Conclusion: ICon is a promising approach for creating effective visual representations in robotics, enhancing both policy performance and transferability.

Abstract: Learning effective visual representations for robotic manipulation remains a
fundamental challenge due to the complex body dynamics involved in action
execution. In this paper, we study how visual representations that carry
body-relevant cues can enable efficient policy learning for downstream robotic
manipulation tasks. We present $\textbf{I}$nter-token $\textbf{Con}$trast
($\textbf{ICon}$), a contrastive learning method applied to the token-level
representations of Vision Transformers (ViTs). ICon enforces a separation in
the feature space between agent-specific and environment-specific tokens,
resulting in agent-centric visual representations that embed body-specific
inductive biases. This framework can be seamlessly integrated into end-to-end
policy learning by incorporating the contrastive loss as an auxiliary
objective. Our experiments show that ICon not only improves policy performance
across various manipulation tasks but also facilitates policy transfer across
different robots. The project website: https://github.com/HenryWJL/icon

</details>


### [791] [Towards Humanoid Robot Autonomy: A Dynamic Architecture Integrating Continuous thought Machines (CTM) and Model Context Protocol (MCP)](https://arxiv.org/abs/2505.19339)
*Libo Wang*

Main category: cs.RO

TL;DR: This paper designs a dynamic architecture connecting continuous thought machines (CTM) and model context protocol (MCP) for humanoid robots, which is feasible and effective.


<details>
  <summary>Details</summary>
Motivation: To bridge the gaps between the static pre-set 'thinking-planning-action' of humanoid robots in unfamiliar scenarios and the highly programmed 'call tool-return result' due to the lack of autonomous coding capabilities.

Method: Designs a dynamic architecture connecting CTM and MCP, proposes a theoretical parallel solution through tick-slab and uses rank compression to achieve parameter suppression.

Result: The experimental results show that the CTM-MCP architecture is feasible and effective through the data results of seven metrics: TSR, ESR, AEL, ROSCOE, REVEAL, PSA, TE.

Conclusion: Provides a reference experience for exploring the autonomous dynamic coding of humanoid robots based on continuous thinking to achieve human-like autonomous actions.

Abstract: To address the gaps between the static pre-set "thinking-planning-action" of
humanoid robots in unfamiliar scenarios and the highly programmed "call
tool-return result" due to the lack of autonomous coding capabilities, this
work designs a dynamic architecture connecting continuous thought machines
(CTM) and model context protocol (MCP). It proposes a theoretical parallel
solution through tick-slab and uses rank compression to achieve parameter
suppression to provide a solution for achieving autonomous actions due to
autonomous coding. The researcher used a simulation-based experiment using
OpenAI's o4-mini-high as a tool to build the experimental environment, and
introduced the extended SayCan dataset to conduct nine epochs of experiments.
The experimental results show that the CTM-MCP architecture is feasible and
effective through the data results of seven metrics: task success rate (TSR),
execution success rate (ESR), average episode length (AEL), ROSCOE, REVEAL,
proficiency self-assessment (PSA), task effectiveness (TE). In practice, it
provides a reference experience for exploring the autonomous dynamic coding of
humanoid robots based on continuous thinking to achieve human-like autonomous
actions.

</details>


### [792] [One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion](https://arxiv.org/abs/2505.18780)
*Yahao Fan,Tianxiang Gui,Kaiyang Ji,Shutong Ding,Chixuan Zhang,Jiayuan Gu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.RO

TL;DR: DreamPolicy is a unified framework that enables a single policy to master diverse terrains and generalize zero-shot to unseen scenarios by integrating offline data and diffusion-driven motion synthesis.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning methods for humanoid locomotion require task-specific rewards and struggle to leverage growing datasets, leading to a scalability challenge.

Method: DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions synthesized through an autoregressive terrain-aware diffusion planner. The data directly captures humanoid kinematics enabling the diffusion planner to synthesize 'dreamed' trajectories encoding terrain-specific physical constraints. These trajectories act as dynamic objectives for the HMI-conditioned policy bypassing manual reward engineering and enabling cross-terrain generalization.

Result: Experiments demonstrate that DreamPolicy achieves average 90% success rates in training environments and an average of 20% higher success on unseen terrains than the prevalent method. It also generalizes to perturbed and composite scenarios where prior approaches collapse.

Conclusion: DreamPolicy overcomes the 'one task, one policy' bottleneck by unifying offline data, diffusion-based trajectory synthesis, and policy optimization, establishing a paradigm for scalable, data-driven humanoid control.

Abstract: Humanoid locomotion faces a critical scalability challenge: traditional
reinforcement learning (RL) methods require task-specific rewards and struggle
to leverage growing datasets, even as more training terrains are introduced. We
propose DreamPolicy, a unified framework that enables a single policy to master
diverse terrains and generalize zero-shot to unseen scenarios by systematically
integrating offline data and diffusion-driven motion synthesis. At its core,
DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions
synthesized through an autoregressive terrain-aware diffusion planner curated
by aggregating rollouts from specialized policies across various distinct
terrains. Unlike human motion datasets requiring laborious retargeting, our
data directly captures humanoid kinematics, enabling the diffusion planner to
synthesize "dreamed" trajectories that encode terrain-specific physical
constraints. These trajectories act as dynamic objectives for our
HMI-conditioned policy, bypassing manual reward engineering and enabling
cross-terrain generalization. DreamPolicy addresses the scalability limitations
of prior methods: while traditional RL fails to exploit growing datasets, our
framework scales seamlessly with more offline data. As the dataset expands, the
diffusion prior learns richer locomotion skills, which the policy leverages to
master new terrains without retraining. Experiments demonstrate that
DreamPolicy achieves average 90% success rates in training environments and an
average of 20% higher success on unseen terrains than the prevalent method. It
also generalizes to perturbed and composite scenarios where prior approaches
collapse. By unifying offline data, diffusion-based trajectory synthesis, and
policy optimization, DreamPolicy overcomes the "one task, one policy"
bottleneck, establishing a paradigm for scalable, data-driven humanoid control.

</details>


### [793] [Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning](https://arxiv.org/abs/2505.18858)
*Maeva Guerrier,Karthik Soma,Hassan Fouad,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 通过引入连续负奖励而不终止回合的新方法模拟不安全行为的时间效应，发现标准强化学习难以应对。提出基于控制屏障函数（CBFs）的三种方法，结合传统RL技术改善安全性与学习效果，并在仿真和实际机器人实验中验证其潜力。


<details>
  <summary>Details</summary>
Motivation: 强化学习作为有潜力的机器人学习范式，但传统的RL框架对安全性的建模过于简单，仅使用单一标量负奖励并立即终止回合，无法反映不安全动作的时序后果（如持续碰撞损害）。

Method: 提出一种新方法，通过应用连续负奖励而不终止回合来模拟不安全行为的时间效应；同时展示如何利用具有安全保证的控制屏障函数（CBFs），帮助机器人避开灾难性区域并提升学习成果。提出了三种基于CBF的方法，将传统RL技术与CBFs相结合，引导智能体学习安全行为。

Result: 实验证明标准RL方法在这种新模型下表现不佳，因为累积的负奖励值在不安全区域形成了学习障碍；而基于CBF的方法在仿真环境和实际四轮差动驱动机器人上均展现出有效性和潜力。

Conclusion: 基于CBF的方法能够有效帮助机器人学习安全行为，克服了标准RL方法在处理复杂安全性问题上的局限性，为实现更安全的机器人学习提供了新途径。

Abstract: Safety stands as the primary obstacle preventing the widespread adoption of
learning-based robotic systems in our daily lives. While reinforcement learning
(RL) shows promise as an effective robot learning paradigm, conventional RL
frameworks often model safety by using single scalar negative rewards with
immediate episode termination, failing to capture the temporal consequences of
unsafe actions (e.g., sustained collision damage). In this work, we introduce a
novel approach that simulates these temporal effects by applying continuous
negative rewards without episode termination. Our experiments reveal that
standard RL methods struggle with this model, as the accumulated negative
values in unsafe zones create learning barriers. To address this challenge, we
demonstrate how Control Barrier Functions (CBFs), with their proven safety
guarantees, effectively help robots avoid catastrophic regions while enhancing
learning outcomes. We present three CBF-based approaches, each integrating
traditional RL methods with Control Barrier Functions, guiding the agent to
learn safe behavior. Our empirical analysis, conducted in both simulated
environments and real-world settings using a four-wheel differential drive
robot, explores the possibilities of employing these approaches for safe
robotic learning.

</details>


### [794] [WorldEval: World Model as Real-World Robot Policies Evaluator](https://arxiv.org/abs/2505.19017)
*Yaxuan Li,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: The paper presents Policy2Vec and WorldEval for evaluating robot policies using world models, showing strong correlation with real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Evaluating generalist robot manipulation policies in real-world scenarios is time-consuming and challenging due to scaling tasks and changing environmental conditions.

Method: Propose Policy2Vec to generate accurate policy videos from world models by turning a video generation model into a world simulator. Introduce WorldEval, an automated pipeline for online evaluation of real-world robot policies.

Result: WorldEval effectively ranks robot policies and checkpoints, acts as a safety detector, and shows strong correlation with real-world performance while outperforming popular methods like real-to-sim approach.

Conclusion: World models can serve as a scalable, reproducible, and reliable proxy for real-world robot policy evaluation.

Abstract: The field of robotics has made significant strides toward developing
generalist robot manipulation policies. However, evaluating these policies in
real-world scenarios remains time-consuming and challenging, particularly as
the number of tasks scales and environmental conditions change. In this work,
we demonstrate that world models can serve as a scalable, reproducible, and
reliable proxy for real-world robot policy evaluation. A key challenge is
generating accurate policy videos from world models that faithfully reflect the
robot actions. We observe that directly inputting robot actions or using
high-dimensional encoding methods often fails to generate action-following
videos. To address this, we propose Policy2Vec, a simple yet effective approach
to turn a video generation model into a world simulator that follows latent
action to generate the robot video. We then introduce WorldEval, an automated
pipeline designed to evaluate real-world robot policies entirely online.
WorldEval effectively ranks various robot policies and individual checkpoints
within a single policy, and functions as a safety detector to prevent dangerous
actions by newly developed robot models. Through comprehensive paired
evaluations of manipulation policies in real-world environments, we demonstrate
a strong correlation between policy performance in WorldEval and real-world
scenarios. Furthermore, our method significantly outperforms popular methods
such as real-to-sim approach.

</details>


### [795] [Situationally-Aware Dynamics Learning](https://arxiv.org/abs/2505.19574)
*Alejandro Murillo-Gonzalez,Lantao Liu*

Main category: cs.RO

TL;DR: 机器人在复杂环境中操作面临挑战，本文提出了一种新的在线学习框架，使机器人能够实时适应不确定和动态条件。通过多变量贝叶斯在线变化点检测扩展，方法验证于非结构化地形导航任务中，显著提高了数据效率、策略性能和安全性。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在复杂、非结构化环境中运行时，由于潜在的未观察因素，难以准确理解自身状态和外部世界。这导致了行为的次优或错误。因此，需要一种方法来帮助机器人实时适应不确定和动态条件。

Method: 提出了一种新的在线学习隐藏状态表示框架，形式化为广义隐参数马尔可夫决策过程（GHP-MDP），显式建模未观察参数对转移动态和奖励结构的影响。核心创新在于学习联合状态转移分布，以表示潜在的自我和环境因素。通过多变量贝叶斯在线变化点检测扩展，分割机器人动力学的基本数据生成过程的变化，并用符号表示当前情境以支持自适应和情境感知决策。

Result: 实验表明，在模拟和真实世界中，该方法显著提高了数据效率和策略性能，并出现了更安全、更适应的导航策略。

Conclusion: 所提出的在线学习框架可以有效提高机器人在复杂环境中的适应性和安全性，为未来研究提供了新的方向。

Abstract: Autonomous robots operating in complex, unstructured environments face
significant challenges due to latent, unobserved factors that obscure their
understanding of both their internal state and the external world. Addressing
this challenge would enable robots to develop a more profound grasp of their
operational context. To tackle this, we propose a novel framework for online
learning of hidden state representations, with which the robots can adapt in
real-time to uncertain and dynamic conditions that would otherwise be ambiguous
and result in suboptimal or erroneous behaviors. Our approach is formalized as
a Generalized Hidden Parameter Markov Decision Process, which explicitly models
the influence of unobserved parameters on both transition dynamics and reward
structures. Our core innovation lies in learning online the joint distribution
of state transitions, which serves as an expressive representation of latent
ego- and environmental-factors. This probabilistic approach supports the
identification and adaptation to different operational situations, improving
robustness and safety. Through a multivariate extension of Bayesian Online
Changepoint Detection, our method segments changes in the underlying data
generating process governing the robot's dynamics. The robot's transition model
is then informed with a symbolic representation of the current situation
derived from the joint distribution of latest state transitions, enabling
adaptive and context-aware decision-making. To showcase the real-world
effectiveness, we validate our approach in the challenging task of unstructured
terrain navigation, where unmodeled and unmeasured terrain characteristics can
significantly impact the robot's motion. Extensive experiments in both
simulation and real world reveal significant improvements in data efficiency,
policy performance, and the emergence of safer, adaptive navigation strategies.

</details>


### [796] [From Single Images to Motion Policies via Video-Generation Environment Representations](https://arxiv.org/abs/2505.19306)
*Weiming Zhi,Ziyong Ma,Tianyi Zhang,Matthew Johnson-Roberson*

Main category: cs.RO

TL;DR: 提出了一种名为VGER的框架，利用大规模视频生成模型生成运动相机视频，并通过多视角数据集和预训练3D基础模型生成密集点云，再通过多尺度噪声方法训练环境结构的隐式表示，构建符合场景几何形状的运动生成模型。


<details>
  <summary>Details</summary>
Motivation: 从单个RGB输入图像生成无碰撞运动是自主机器人面临的重要问题，而现有的深度估计模型存在锥形误差，难以直接用于下游运动生成任务。

Method: 1. 使用输入图像生成运动相机视频。2. 利用多视角数据集和预训练3D基础模型生成密集点云。3. 采用多尺度噪声方法训练环境结构的隐式表示。4. 构建与场景几何形状一致的运动生成模型。

Result: VGER在多样化的室内和室外环境中进行了广泛评估，能够从单个RGB输入图像生成平滑且符合场景几何形状的运动轨迹。

Conclusion: VGER框架成功解决了从单个RGB图像生成无碰撞运动的问题，能够在复杂环境中生成符合几何形状的运动轨迹。

Abstract: Autonomous robots typically need to construct representations of their
surroundings and adapt their motions to the geometry of their environment.
Here, we tackle the problem of constructing a policy model for collision-free
motion generation, consistent with the environment, from a single input RGB
image. Extracting 3D structures from a single image often involves monocular
depth estimation. Developments in depth estimation have given rise to large
pre-trained models such as DepthAnything. However, using outputs of these
models for downstream motion generation is challenging due to frustum-shaped
errors that arise. Instead, we propose a framework known as Video-Generation
Environment Representation (VGER), which leverages the advances of large-scale
video generation models to generate a moving camera video conditioned on the
input image. Frames of this video, which form a multiview dataset, are then
input into a pre-trained 3D foundation model to produce a dense point cloud. We
then introduce a multi-scale noise approach to train an implicit representation
of the environment structure and build a motion generation model that complies
with the geometry of the representation. We extensively evaluate VGER over a
diverse set of indoor and outdoor environments. We demonstrate its ability to
produce smooth motions that account for the captured geometry of a scene, all
from a single RGB input image.

</details>


### [797] [TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning](https://arxiv.org/abs/2505.19769)
*Yuhui Chen,Haoran Li,Zhennan Jiang,Haowei Wen,Dongbin Zhao*

Main category: cs.RO

TL;DR: This paper introduces TeViR, a method that uses a pre-trained text-to-video diffusion model to generate dense rewards in reinforcement learning for robotic manipulation tasks, improving sample efficiency and performance without needing ground truth environmental rewards.


<details>
  <summary>Details</summary>
Motivation: Recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise but suffer from sparse reward nature which significantly limits sample efficiency.

Method: TeViR leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations.

Result: Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art methods.

Conclusion: TeViR's ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.

Abstract: Developing scalable and generalizable reward engineering for reinforcement
learning (RL) is crucial for creating general-purpose agents, especially in the
challenging domain of robotic manipulation. While recent advances in reward
engineering with Vision-Language Models (VLMs) have shown promise, their sparse
reward nature significantly limits sample efficiency. This paper introduces
TeViR, a novel method that leverages a pre-trained text-to-video diffusion
model to generate dense rewards by comparing the predicted image sequence with
current observations. Experimental results across 11 complex robotic tasks
demonstrate that TeViR outperforms traditional methods leveraging sparse
rewards and other state-of-the-art (SOTA) methods, achieving better sample
efficiency and performance without ground truth environmental rewards. TeViR's
ability to efficiently guide agents in complex environments highlights its
potential to advance reinforcement learning applications in robotic
manipulation.

</details>


### [798] [Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures](https://arxiv.org/abs/2505.19521)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: This paper proposes a geometric framework that integrates local measurements, constraints, and dynamics learning using a fiber bundle structure over the state space, enabling measurement-aware Control Barrier Functions and incorporating Neural ODEs for continuous-time dynamics learning with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing methods for learning unknown dynamics under environmental constraints either require global constraint information or fail to fully exploit the geometric structure inherent in local, uncertain measurements.

Method: The authors develop a geometric framework based on a fiber bundle structure over the state space. This framework unifies local measurements, constraints, and dynamics learning, utilizing measurement-aware Control Barrier Functions and Neural ODEs to ensure both learning convergence and constraint satisfaction dependent on sensing quality.

Result: The proposed framework demonstrates significant improvements in learning efficiency and constraint satisfaction compared to traditional methods, particularly excelling under limited and uncertain sensing conditions as shown through extensive simulations.

Conclusion: The geometric framework not only enhances dynamics learning but also opens up possibilities for integration with reinforcement learning approaches, offering a promising direction for future research.

Abstract: Learning unknown dynamics under environmental (or external) constraints is
fundamental to many fields (e.g., modern robotics), particularly challenging
when constraint information is only locally available and uncertain. Existing
approaches requiring global constraints or using probabilistic filtering fail
to fully exploit the geometric structure inherent in local measurements (by
using, e.g., sensors) and constraints. This paper presents a geometric
framework unifying measurements, constraints, and dynamics learning through a
fiber bundle structure over the state space. This naturally induced geometric
structure enables measurement-aware Control Barrier Functions that adapt to
local sensing (or measurement) conditions. By integrating Neural ODEs, our
framework learns continuous-time dynamics while preserving geometric
constraints, with theoretical guarantees of learning convergence and constraint
satisfaction dependent on sensing quality. The geometric framework not only
enables efficient dynamics learning but also suggests promising directions for
integration with reinforcement learning approaches. Extensive simulations
demonstrate significant improvements in both learning efficiency and constraint
satisfaction over traditional methods, especially under limited and uncertain
sensing conditions.

</details>


### [799] [Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects](https://arxiv.org/abs/2505.20223)
*Yixin Cui,Haotian Lin,Shuo Yang,Yixiao Wang,Yanjun Huang,Hong Chen*

Main category: cs.RO

TL;DR: 链式思维（CoT）方法通过模拟人类思考过程，显著提高了自动驾驶模型的推理能力。本文综述了CoT在自动驾驶中的动机、方法、挑战及未来方向，并提出将CoT与自学习结合以促进驾驶系统的自我进化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，语义理解和逻辑推理能力得到了极大提升，这些能力被应用于自动驾驶系统中，极大地改善了系统性能。因此，研究如何利用CoT方法提高自动驾驶模型的推理能力变得重要。

Method: 通过全面的文献回顾，对CoT在自动驾驶中的应用进行系统分析，探讨其动机、方法论、面临的挑战以及未来的研发方向。同时，提出了将CoT与自学习相结合的观点，以推动驾驶系统的自我进化。

Result: 阐明了CoT在自动驾驶领域的关键作用，识别了相关挑战，并指明了未来的研究方向。此外，建立了一个动态更新的文献和开源项目仓库，为领域内的研究者提供资源支持。

Conclusion: 链式思维方法在提升自动驾驶模型推理能力方面具有显著潜力，结合自学习机制可以进一步推动自动驾驶技术的发展。然而，仍需克服一些技术和实践上的挑战。

Abstract: The rapid evolution of large language models in natural language processing
has substantially elevated their semantic understanding and logical reasoning
capabilities. Such proficiencies have been leveraged in autonomous driving
systems, contributing to significant improvements in system performance. Models
such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning,
an advanced cognitive method that simulates human thinking processes,
demonstrating remarkable reasoning capabilities in complex tasks. By
structuring complex driving scenarios within a systematic reasoning framework,
this approach has emerged as a prominent research focus in autonomous driving,
substantially improving the system's ability to handle challenging cases. This
paper investigates how CoT methods improve the reasoning abilities of
autonomous driving models. Based on a comprehensive literature review, we
present a systematic analysis of the motivations, methodologies, challenges,
and future research directions of CoT in autonomous driving. Furthermore, we
propose the insight of combining CoT with self-learning to facilitate
self-evolution in driving systems. To ensure the relevance and timeliness of
this study, we have compiled a dynamic repository of literature and open-source
projects, diligently updated to incorporate forefront developments. The
repository is publicly available at
https://github.com/cuiyx1720/Awesome-CoT4AD.

</details>


### [800] [EgoZero: Robot Learning from Smart Glasses](https://arxiv.org/abs/2505.20290)
*Vincent Liu,Ademi Adeniji,Haotian Zhan,Raunaq Bhirangi,Pieter Abbeel,Lerrel Pinto*

Main category: cs.RO

TL;DR: EgoZero is a system that learns manipulation policies from human demonstrations via smart glasses, achieving zero-shot transfer with 70% success rate on a robot.


<details>
  <summary>Details</summary>
Motivation: To leverage the rich data resource of human interactions with the physical world to improve robot learning and capabilities.

Method: Propose EgoZero, which uses human demonstrations captured by Project Aria smart glasses and no robot data. It extracts robot-executable actions, compresses human visual observations into state representations, and enables closed-loop policy learning.

Result: Achieved 70% success rate in zero-shot transfer over 7 manipulation tasks with only 20 minutes of data collection per task.

Conclusion: In-the-wild human data can serve as a scalable foundation for real-world robot learning, suggesting a future with abundant, diverse, and naturalistic training data for robots.

Abstract: Despite recent progress in general purpose robotics, robot policies still lag
far behind basic human capabilities in the real world. Humans interact
constantly with the physical world, yet this rich data resource remains largely
untapped in robot learning. We propose EgoZero, a minimal system that learns
robust manipulation policies from human demonstrations captured with Project
Aria smart glasses, $\textbf{and zero robot data}$. EgoZero enables: (1)
extraction of complete, robot-executable actions from in-the-wild, egocentric,
human demonstrations, (2) compression of human visual observations into
morphology-agnostic state representations, and (3) closed-loop policy learning
that generalizes morphologically, spatially, and semantically. We deploy
EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot
transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of
data collection per task. Our results suggest that in-the-wild human data can
serve as a scalable foundation for real-world robot learning - paving the way
toward a future of abundant, diverse, and naturalistic training data for
robots. Code and videos are available at https://egozero-robot.github.io.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [801] [Grassroots Consensus](https://arxiv.org/abs/2505.19216)
*Idit Keidar,Andrew Lewis-Pye,Ehud Shapiro*

Main category: cs.DC

TL;DR: Grassroots platforms provide an egalitarian alternative to global platforms, using digital social contracts. The Grassroots Consensus protocol is designed to meet specific requirements for grassroots architecture, including being quiescent, efficient, responsive, blocklace-based, UDP-ready, and grassroots itself.


<details>
  <summary>Details</summary>
Motivation: Current global platforms are either centralized/autocratic or decentralized/plutocratic, lacking an egalitarian option. There's a need for platforms that employ digital social contracts among people, executed by their smartphones.

Method: The Grassroots Consensus protocol builds on Morpheus and Cordial Miners protocols, improving the dissemination protocol, making it UDP-ready, and adding a constitution and constitutional amendment component.

Result: This protocol meets all specified requirements for grassroots architecture and has competitive performance in both low- and high-throughput scenarios while remaining concise and elegant.

Conclusion: The Grassroots Consensus protocol offers an effective solution for the development of egalitarian grassroots platforms.

Abstract: Grassroots platforms aim to offer an egalitarian alternative to global
platforms -- centralized/autocratic and decentralized/plutocratic alike. Within
the grassroots architecture, consensus is needed to realize platforms that
employ digital social contracts, which are like smart contracts except that
they are among people not accounts and are executed by these people's
smartphones not by high-performance servers controlled by parties outside to
the contract. Key envisioned grassroots platforms include sovereign democratic
digital communities and federations, community banks and their grassroots
cryptocurrencies, and digital cooperatives.
  The grassroots architecture can benefit from a consensus protocol that is (i)
quiescent, (ii) efficient during low- and high-throughput, (iii) responsive,
(iv) blocklace-based, (v) UDP-ready, and (vi) grassroots. The Grassroots
Consensus protocol addresses all these requirements while having competitive
performance in both low- and high-throughput scenarios and being one of the
most concise and elegant consensus protocols for partial synchrony. It achieves
that by building on two cutting-edge consensus protocols -- the quiescent
high-performance Morpheus and the blocklace-based Cordial Miners, improving the
latter's dissemination protocol and making it UDP-ready, and extending the
protocol with a constitution and a constitutional amendment component, making
it grassroots.

</details>


### [802] [PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning](https://arxiv.org/abs/2505.18563)
*Yisu Wang,Ruilong Wu,Xinjiao Li,Dirk Kutscher*

Main category: cs.DC

TL;DR: PacTrain is a framework that combines pruning with sparse gradient compression to accelerate distributed DNN training without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: As DNNs and datasets grow, distributed training becomes extremely time-consuming and demands larger clusters. Many gradient compression schemes do not achieve acceleration of the training process while also preserving accuracy.

Method: PacTrain accelerates distributed training by combining pruning with sparse gradient compression. Active pruning of the neural network makes the model weights and gradients sparse, allowing lightweight compression communication without harming accuracy.

Result: Experimental evaluations show that PacTrain improves training throughput by 1.25 to 8.72 times compared to state-of-the-art compression-enabled systems for representative vision and language models training tasks under bandwidth-constrained conditions.

Conclusion: PacTrain introduces a near-optimal compression strategy that is compatible with the all-reduce primitive, significantly improving training throughput in distributed DNN training.

Abstract: Large-scale deep neural networks (DNN) exhibit excellent performance for
various tasks. As DNNs and datasets grow, distributed training becomes
extremely time-consuming and demands larger clusters. A main bottleneck is the
resulting gradient aggregation overhead. While gradient compression and sparse
collective communication techniques are commonly employed to alleviate network
load, many gradient compression schemes do not achieve acceleration of the
training process while also preserving accuracy. This paper introduces
PacTrain, a novel framework that accelerates distributed training by combining
pruning with sparse gradient compression. Active pruning of the neural network
makes the model weights and gradients sparse. By ensuring the global knowledge
of the gradient sparsity among all distributed training workers, we can perform
lightweight compression communication without harming accuracy. We show that
the PacTrain compression scheme achieves a near-optimal compression strategy
while remaining compatible with the all-reduce primitive. Experimental
evaluations show that PacTrain improves training throughput by 1.25 to 8.72
times compared to state-of-the-art compression-enabled systems for
representative vision and language models training tasks under
bandwidth-constrained conditions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [803] [TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network](https://arxiv.org/abs/2505.18533)
*Xiaobin Rong,Dahan Wang,Qinwen Hu,Yushi Wang,Yuxiang Hu,Jing Lu*

Main category: eess.AS

TL;DR: The paper introduces TS-URGENet, a three-stage network for universal speech enhancement that ranked 2nd in the Interspeech 2025 URGENT Challenge Track 1.


<details>
  <summary>Details</summary>
Motivation: There is a need for a universal speech enhancement system capable of handling various distortions and input formats.

Method: TS-URGENet uses a three-stage architecture: filling stage to handle packet loss, separation stage to suppress noise and other distortions, and restoration stage to refine speech quality.

Result: TS-URGENet showed outstanding performance and ranked 2nd in Track 1 of the Interspeech 2025 URGENT Challenge.

Conclusion: TS-URGENet effectively addresses universal speech enhancement challenges through its innovative three-stage approach.

Abstract: Universal speech enhancement aims to handle input speech with different
distortions and input formats. To tackle this challenge, we present TS-URGENet,
a Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.
To address various distortions, the proposed system employs a novel three-stage
architecture consisting of a filling stage, a separation stage, and a
restoration stage. The filling stage mitigates packet loss by preliminarily
filling lost regions under noise interference, ensuring signal continuity. The
separation stage suppresses noise, reverberation, and clipping distortion to
improve speech clarity. Finally, the restoration stage compensates for
bandwidth limitation, codec artifacts, and residual packet loss distortion,
refining the overall speech quality. Our proposed TS-URGENet achieved
outstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd
in Track 1.

</details>


### [804] [Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers](https://arxiv.org/abs/2505.18722)
*Terry Yi Zhong,Esther Janse,Cristian Tejedor-Garcia,Louis ten Bosch,Martha Larson*

Main category: eess.AS

TL;DR: This paper explores the feasibility of using non-diagnostic speech data for Parkinson's disease detection, compares different datasets and evaluates factors impacting classification performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether speech data not originally intended for diagnostic purposes can be used effectively for Parkinson's disease detection, expanding the scope beyond specialized diagnostic-oriented speech tasks.

Method: Using the Turn-Taking (TT) dataset, the study compares its utility with diagnostic-oriented PD datasets like PC-GITA. It evaluates the impact of dataset characteristics such as concatenating audio recordings, balancing gender and status distributions, and performs cross-dataset evaluations.

Result: The TT dataset is found to be as useful as PC-GITA for PD detection. Balancing participant distributions and concatenating audio recordings improve classification performance. Models trained on TT generalize better to PC-GITA than vice versa. High variability across folds is attributed to differences in individual speaker performance.

Conclusion: Non-diagnostic speech data can be effectively used for Parkinson's disease detection, with specific dataset characteristics significantly impacting classification performance.

Abstract: Speech-based Parkinson's disease (PD) detection has gained attention for its
automated, cost-effective, and non-intrusive nature. As research studies
usually rely on data from diagnostic-oriented speech tasks, this work explores
the feasibility of diagnosing PD on the basis of speech data not originally
intended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our
findings indicate that TT can be as useful as diagnostic-oriented PD datasets
like PC-GITA. We also investigate which specific dataset characteristics impact
PD classification performance. The results show that concatenating audio
recordings and balancing participants' gender and status distributions can be
beneficial. Cross-dataset evaluation reveals that models trained on PC-GITA
generalize poorly to TT, whereas models trained on TT perform better on
PC-GITA. Furthermore, we provide insights into the high variability across
folds, which is mainly due to large differences in individual speaker
performance.

</details>


### [805] [Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis](https://arxiv.org/abs/2505.18972)
*Minsu Kim,Pingchuan Ma,Honglie Chen,Stavros Petridis,Maja Pantic*

Main category: eess.AS

TL;DR: This paper presents a multi-modal controllable Text-to-Speech Synthesis (TTS) model that can generate voice from face images with characteristics controlled by natural text descriptions. It addresses three challenges in face-driven TTS systems: audio quality, input face variety, and consistent voice generation.


<details>
  <summary>Details</summary>
Motivation: To develop a TTS system capable of generating voices from face images while allowing control over speech characteristics via natural text descriptions.

Method: 1) A training method using high-quality audio-only speech corpora to improve audio quality; 2) Stylization of input face images to handle both real human faces and artistic portraits; 3) Sampling-based decoding followed by prompting with generated speech samples for consistent voice generation.

Result: Experimental results confirm the effectiveness of the proposed model in synthesizing voices driven by faces.

Conclusion: The proposed model successfully mitigates challenges in face-driven TTS systems and demonstrates effectiveness in voice synthesis.

Abstract: This paper explores multi-modal controllable Text-to-Speech Synthesis (TTS)
where the voice can be generated from face image, and the characteristics of
output speech (e.g., pace, noise level, distance, tone, place) can be
controllable with natural text description. Specifically, we aim to mitigate
the following three challenges in face-driven TTS systems. 1) To overcome the
limited audio quality of audio-visual speech corpora, we propose a training
method that additionally utilizes high-quality audio-only speech corpora. 2) To
generate voices not only from real human faces but also from artistic
portraits, we propose augmenting the input face image with stylization. 3) To
consider one-to-many possibilities in face-to-voice mapping and ensure
consistent voice generation at the same time, we propose to first employ
sampling-based decoding and then use prompting with generated speech samples.
Experimental results validate the proposed model's effectiveness in face-driven
voice synthesis.

</details>


### [806] [Acoustic and Machine Learning Methods for Speech-Based Suicide Risk Assessment: A Systematic Review](https://arxiv.org/abs/2505.18195)
*Ambre Marie,Marine Garnier,Thomas Bertin,Laura Machart,Guillaume Dardenne,Gwenolé Quellec,Sofian Berrouiguet*

Main category: eess.AS

TL;DR: AI和机器学习在通过语音声学分析评估自杀风险中的作用。33篇文章的分析表明，风险人群和非风险人群之间存在显著的声学特征差异，但方法学限制了通用性。未来研究应标准化方法并使用更多样化的数据集。


<details>
  <summary>Details</summary>
Motivation: 自杀仍然是一个公共卫生挑战，需要改进检测方法以实现及时干预和治疗。

Method: 系统评价遵循PRISMA指南，分析了来自PubMed、Cochrane、Scopus和Web of Science数据库的33篇文章。这些研究主要探讨了自杀风险人群与非风险人群之间的声学差异，并评估了机器学习分类器的性能。

Result: 发现一致显示，自杀风险人群和非风险人群之间存在显著的声学特征差异，特别是在jitter、基本频率(F0)、梅尔频率倒谱系数(MFCC)和功率谱密度(PSD)方面。分类器的效果因算法、模态和语音诱发方法而异，多模态方法整合了声学、语言学和元数据特征，表现更优。

Conclusion: 尽管存在方法论变化、样本量小、缺乏纵向数据和语言及人口统计学多样性有限等限制，但未来的研究应集中在标准化方法、扩展多模态分析以及利用更大的多样化数据集上，以支持在临床自杀风险评估中整合AI。

Abstract: Suicide remains a public health challenge, necessitating improved detection
methods to facilitate timely intervention and treatment. This systematic review
evaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in
assessing suicide risk through acoustic analysis of speech. Following PRISMA
guidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and
Web of Science databases. These studies primarily explored acoustic differences
between individuals at risk of suicide (RS) and those not at risk (NRS), and
evaluated ML classifier performance. Findings consistently showed significant
acoustic feature variations between RS and NRS populations, particularly
involving jitter, fundamental frequency (F0), Mel-frequency cepstral
coefficients (MFCC), and power spectral density (PSD). Classifier effectiveness
varied based on algorithms, modalities, and speech elicitation methods, with
multimodal approaches integrating acoustic, linguistic, and metadata features
demonstrating superior performance. However, limitations such as methodological
variability, small sample sizes, lack of longitudinal data, and limited
linguistic and demographic diversity restrict generalizability. Future research
should focus on standardizing methods, expanding multimodal analyses, and
utilizing larger, diverse datasets to support AI integration in clinical
suicide risk assessment.

</details>


### [807] [SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline](https://arxiv.org/abs/2505.19314)
*Helin Wang,Jiarui Hai,Dongchao Yang,Chen Chen,Kai Li,Junyi Peng,Thomas Thebaud,Laureano Moro Velazquez,Jesus Villalba,Najim Dehak*

Main category: eess.AS

TL;DR: SoloSpeech is a new cascaded generative pipeline that improves target speech extraction by integrating compression, extraction, reconstruction, and correction processes without using speaker embeddings.


<details>
  <summary>Details</summary>
Motivation: Current TSE models either introduce unwanted artifacts or lack perceptual quality and intelligibility.

Method: SoloSpeech uses conditional information from the cue audio's latent space aligned with the mixture audio's latent space for extraction, followed by reconstruction and correction.

Result: Achieves state-of-the-art intelligibility and quality on Libri2Mix dataset, with exceptional generalization on out-of-domain data and real-world scenarios.

Conclusion: SoloSpeech addresses limitations of current TSE models by providing high-quality target speech extraction and better generalization.

Abstract: Target Speech Extraction (TSE) aims to isolate a target speaker's voice from
a mixture of multiple speakers by leveraging speaker-specific cues, typically
provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in
TSE have primarily employed discriminative models that offer high perceptual
quality, these models often introduce unwanted artifacts, reduce naturalness,
and are sensitive to discrepancies between training and testing environments.
On the other hand, generative models for TSE lag in perceptual quality and
intelligibility. To address these challenges, we present SoloSpeech, a novel
cascaded generative pipeline that integrates compression, extraction,
reconstruction, and correction processes. SoloSpeech features a
speaker-embedding-free target extractor that utilizes conditional information
from the cue audio's latent space, aligning it with the mixture audio's latent
space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset,
SoloSpeech achieves the new state-of-the-art intelligibility and quality in
target speech extraction and speech separation tasks while demonstrating
exceptional generalization on out-of-domain data and real-world scenarios.

</details>


### [808] [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/abs/2505.20166)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: 音频感知大语言模型(ALLM)在处理音频输入方面取得了显著进展，但存在灾难性遗忘和跨模态对齐资源密集的问题。本文提出BALSa方法通过合成数据生成来引导音频-语言对齐，并引入LISTEN方法以提高区分声音的能力，实验结果表明该方法有效缓解了音频幻觉问题并增强了模型的理解、推理和指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 当前音频感知大语言模型（ALLMs）存在两个主要局限：1. 灾难性遗忘，即在音频数据训练后失去重要文本能力；2. 跨模态对齐需要大量特定任务的问答对进行指令调优，过程资源密集。因此，需要一种更高效且可扩展的方法来解决这些问题。

Method: 利用ALLMs的基本大语言模型（LLMs）合成通用目的的标题风格对齐数据，称为引导音频-语言对齐的合成数据生成（BALSa）。基于BALSa，提出了LISTEN方法，这是一种类似对比的学习方法，旨在提高ALLMs区分存在和不存在的声音的能力。此外，将BALSa扩展到多音频场景，使模型能够解释音频输入之间的差异或生成统一的描述。

Result: 实验结果表明，所提出的方法有效地减轻了音频幻觉问题，同时可靠地保持了在音频理解、推理和指令跟随方面的强大性能。多音频训练进一步增强了模型的理解和推理能力。

Conclusion: BALSa提供了一种高效且可扩展的方法来开发ALLMs，解决了现有方法中的灾难性遗忘和资源密集型问题，提高了模型的音频理解和推理能力。

Abstract: Audio-aware large language models (ALLMs) have recently made great strides in
understanding and processing audio inputs. These models are typically adapted
from text-based large language models (LLMs) through additional training on
audio-related tasks. However, this adaptation process presents two major
limitations. First, ALLMs often suffer from catastrophic forgetting, where
important textual capabilities such as instruction-following are lost after
training on audio data. In some cases, models may even hallucinate sounds that
are not present in the input audio, raising concerns about their reliability.
Second, achieving cross-modal alignment between audio and language typically
relies on large collections of task-specific question-answer pairs for
instruction tuning, making the process resource-intensive. To address these
issues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose
caption-style alignment data. We refer to this process as bootstrapping
audio-language alignment via synthetic data generation from backbone LLMs
(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method designed
to improve ALLMs' ability to distinguish between present and absent sounds. We
further extend BALSa to multi-audio scenarios, where the model either explains
the differences between audio inputs or produces a unified caption that
describes them all, thereby enhancing audio-language alignment. Experimental
results indicate that our method effectively mitigates audio hallucinations
while reliably maintaining strong performance in audio understanding,
reasoning, and instruction-following skills. Moreover, incorporating
multi-audio training further enhances the model's comprehension and reasoning
capabilities. Overall, BALSa offers an efficient and scalable approach to the
development of ALLMs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [809] [Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees](https://arxiv.org/abs/2505.18659)
*Sangwoo Park,Matteo Zecchin,Osvaldo Simeone*

Main category: stat.ML

TL;DR: The paper proposes R-AutoEval+, a novel framework for AI model evaluation that ensures reliability and sample efficiency through adaptive use of synthetic data.


<details>
  <summary>Details</summary>
Motivation: Accurate performance estimation of AI models, like LLMs, is crucial but costly when using real-world data. Autoevaluation methods reduce variance with synthetic data but may introduce bias and degrade sample efficiency.

Method: R-AutoEval+ introduces an adaptive construction of the model evaluation variable, dynamically adjusting reliance on synthetic data and reverting to conventional methods when autoevaluator accuracy is insufficient.

Result: Experiments confirm the reliability and efficiency of R-AutoEval+ in optimizing quantization settings and prompt design for LLMs using LLMs-as-judges.

Conclusion: R-AutoEval+ provides finite-sample reliability guarantees while maintaining or improving sample efficiency compared to conventional methods.

Abstract: Selecting artificial intelligence (AI) models, such as large language models
(LLMs), from multiple candidates requires accurate performance estimation. This
is ideally achieved through empirical evaluations involving abundant real-world
data. However, such evaluations are costly and impractical at scale. To address
this challenge, autoevaluation methods leverage synthetic data produced by
automated evaluators, such as LLMs-as-judges, reducing variance but potentially
introducing bias. Recent approaches have employed semi-supervised
prediction-powered inference (\texttt{PPI}) to correct for the bias of
autoevaluators. However, the use of autoevaluators may lead in practice to a
degradation in sample efficiency compared to conventional methods using only
real-world data. In this paper, we propose \texttt{R-AutoEval+}, a novel
framework that provides finite-sample reliability guarantees on the model
evaluation, while also ensuring an enhanced (or at least no worse) sample
efficiency compared to conventional methods. The key innovation of
\texttt{R-AutoEval+} is an adaptive construction of the model evaluation
variable, which dynamically tunes its reliance on synthetic data, reverting to
conventional methods when the autoevaluator is insufficiently accurate.
Experiments on the use of LLMs-as-judges for the optimization of quantization
settings for the weights of an LLM, and for prompt design in LLMs confirm the
reliability and efficiency of \texttt{R-AutoEval+}.

</details>


### [810] [Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems](https://arxiv.org/abs/2505.18276)
*Lorenzo Baldassari,Josselin Garnier,Knut Solna,Maarten V. de Hoop*

Main category: stat.ML

TL;DR: 设计用于求解高维贝叶斯逆问题的算法，直接在无限维函数空间中分析线性逆问题中广泛使用的采样器：由分数生成模型(SGMs)驱动的Langevin动力学。首次推导出明确依赖于分数逼近误差的误差估计，并获得充分条件以确保在整个函数空间上的全局收敛。通过预处理Langevin算法防止数值不稳定并证明最优预处理器的存在及其形式。分析适用于高斯和非高斯先验。最后，给出验证理论发现的例子。


<details>
  <summary>Details</summary>
Motivation: 在细化底层问题离散化的同时，确保稳定性和收敛性是至关重要的。因此需要直接在无限维函数空间中为高维贝叶斯逆问题设计算法。

Method: 分析由分数生成模型(SGMs)作为先验驱动的Langevin动力学采样器，给出其在无限维设置下的严格定义，并推导出明确依赖于分数逼近误差的误差估计。证明最优预处理器的存在及其形式以防止数值不稳定。

Result: 首次推导出明确依赖于分数逼近误差的误差估计，获得充分条件以确保在整个函数空间上的全局收敛，并证明最优预处理器的存在及其形式，保证了统一的收敛率。

Conclusion: 本研究为高维贝叶斯逆问题提供了新的见解，特别是在无限维函数空间中的采样器分析方面，对于防止数值不稳定和确保收敛具有重要意义。

Abstract: Designing algorithms for solving high-dimensional Bayesian inverse problems
directly in infinite-dimensional function spaces - where such problems are
naturally formulated - is crucial to ensure stability and convergence as the
discretization of the underlying problem is refined. In this paper, we
contribute to this line of work by analyzing a widely used sampler for linear
inverse problems: Langevin dynamics driven by score-based generative models
(SGMs) acting as priors, formulated directly in function space. Building on the
theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition
of this sampler in the infinite-dimensional setting and derive, for the first
time, error estimates that explicitly depend on the approximation error of the
score. As a consequence, we obtain sufficient conditions for global convergence
in Kullback-Leibler divergence on the underlying function space. Preventing
numerical instabilities requires preconditioning of the Langevin algorithm and
we prove the existence and the form of an optimal preconditioner. The
preconditioner depends on both the score error and the forward operator and
guarantees a uniform convergence rate across all posterior modes. Our analysis
applies to both Gaussian and a general class of non-Gaussian priors. Finally,
we present examples that illustrate and validate our theoretical findings.

</details>


### [811] [Operator Learning for Schrödinger Equation: Unitarity, Error Bounds, and Time Generalization](https://arxiv.org/abs/2505.18288)
*Yash Patel,Unique Subedi,Ambuj Tewari*

Main category: stat.ML

TL;DR: An abstract about learning the evolution operator for time-dependent Schrödinger equation using a linear estimator.


<details>
  <summary>Details</summary>
Motivation: Existing neural network-based surrogates often ignore fundamental properties of the Schrödinger equation, such as linearity and unitarity, and lack theoretical guarantees on prediction error or time generalization.

Method: Introduced a linear estimator for the evolution operator that preserves a weak form of unitarity. Established upper and lower bounds on the prediction error and derived time generalization bounds.

Result: Experiments across real-world Hamiltonians show that the estimator achieves relative errors 10^-2 to 10^-3 times smaller than state-of-the-art methods.

Conclusion: The introduced linear estimator addresses the shortcomings of existing methods by preserving fundamental properties of the Schrödinger equation and providing theoretical guarantees.

Abstract: We consider the problem of learning the evolution operator for the
time-dependent Schr\"{o}dinger equation, where the Hamiltonian may vary with
time. Existing neural network-based surrogates often ignore fundamental
properties of the Schr\"{o}dinger equation, such as linearity and unitarity,
and lack theoretical guarantees on prediction error or time generalization. To
address this, we introduce a linear estimator for the evolution operator that
preserves a weak form of unitarity. We establish both upper and lower bounds on
the prediction error that hold uniformly over all sufficiently smooth initial
wave functions. Additionally, we derive time generalization bounds that
quantify how the estimator extrapolates beyond the time points seen during
training. Experiments across real-world Hamiltonians -- including hydrogen
atoms, ion traps for qubit design, and optical lattices -- show that our
estimator achieves relative errors $10^{-2}$ to $10^{-3}$ times smaller than
state-of-the-art methods such as the Fourier Neural Operator and DeepONet.

</details>


### [812] [Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling](https://arxiv.org/abs/2505.18327)
*Xinchen Du,Wanrong Zhu,Wei Biao Wu,Sen Na*

Main category: stat.ML

TL;DR: 这篇论文提出了一种新的在线推断方法，称为随机缩放方法，用于约束随机非线性优化问题。该方法基于简化的随机序列二次规划（SSQP）算法，能够构建渐近有效的置信区间，并且不需要矩阵求逆操作。数值实验表明，该方法在非线性约束回归问题上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着数据集的增长，在线推断方法对于实现实时决策变得至关重要，同时避免了存储历史数据的需求。然而，现有的在线推断方法可能无法有效处理约束随机非线性优化问题，因此需要一种新的方法来解决这些问题。

Method: 作者提出了一个基于SSQP（简化的随机序列二次规划）的在线推断过程，称为随机缩放方法。此方法通过构造一个无需任何未知参数的检验统计量，利用SSQP迭代实现。与现有方法相比，它具有两个主要优点：能够构建渐近有效的置信区间，并且计算过程中不涉及矩阵求逆操作。

Result: 通过在非线性约束回归问题上的数值实验验证了理论结果，展示了所提出的随机缩放方法在性能上的优越性。

Conclusion: 随机缩放方法为约束随机非线性优化问题提供了一个有效的在线推断工具，能够在无需矩阵求逆的情况下构建渐近有效的置信区间，从而在实际应用中具有很大的潜力。

Abstract: Constrained stochastic nonlinear optimization problems have attracted
significant attention for their ability to model complex real-world scenarios
in physics, economics, and biology. As datasets continue to grow, online
inference methods have become crucial for enabling real-time decision-making
without the need to store historical data. In this work, we develop an online
inference procedure for constrained stochastic optimization by leveraging a
method called Sketched Stochastic Sequential Quadratic Programming (SSQP). As a
direct generalization of sketched Newton methods, SSQP approximates the
objective with a quadratic model and the constraints with a linear model at
each step, then applies a sketching solver to inexactly solve the resulting
subproblem. Building on this design, we propose a new online inference
procedure called random scaling. In particular, we construct a test statistic
based on SSQP iterates whose limiting distribution is free of any unknown
parameters. Compared to existing online inference procedures, our approach
offers two key advantages: (i) it enables the construction of asymptotically
valid confidence intervals; and (ii) it is matrix-free, i.e. the computation
involves only primal-dual SSQP iterates $(\boldsymbol{x}_t,
\boldsymbol{\lambda}_t)$ without requiring any matrix inversions. We validate
our theory through numerical experiments on nonlinearly constrained regression
problems and demonstrate the superior performance of our random scaling method
over existing inference procedures.

</details>


### [813] [On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective](https://arxiv.org/abs/2505.18346)
*Behrad Moniri,Hamed Hassani*

Main category: stat.ML

TL;DR: 通过理论分析简单的模型，本文揭示了三个使得学生模型超越教师模型的核心机制。


<details>
  <summary>Details</summary>
Motivation: 尽管弱到强的泛化现象被广泛观察到，但其背后的机制尚未被充分理解。

Method: 1. 分析岭回归以研究教师和学生正则化之间的相互作用；2. 通过加权岭回归展示与目标更一致的学生模型的优势；3. 在非线性多索引设置下，说明学生模型如何从教师模型学习特定任务特征并利用自身预训练优势学习难以捕捉的特征。

Result: 证明了学生模型可以通过补偿教师模型的不足、优化正则化结构以及结合预训练优势来超越教师模型。

Conclusion: 发现了三个核心机制可以驱动弱到强的泛化现象，并通过理论分析进行了验证。

Abstract: Weak-to-strong generalization, where a student model trained on imperfect
labels generated by a weaker teacher nonetheless surpasses that teacher, has
been widely observed but the mechanisms that enable it have remained poorly
understood. In this paper, through a theoretical analysis of simple models, we
uncover three core mechanisms that can drive this phenomenon. First, by
analyzing ridge regression, we study the interplay between the teacher and
student regularization and prove that a student can compensate for a teacher's
under-regularization and achieve lower test error. We also analyze the role of
the parameterization regime of the models. Second, by analyzing weighted ridge
regression, we show that a student model with a regularization structure more
aligned to the target, can outperform its teacher. Third, in a nonlinear
multi-index setting, we demonstrate that a student can learn easy,
task-specific features from the teacher while leveraging its own broader
pre-training to learn hard-to-learn features that the teacher cannot capture.

</details>


### [814] [Identifiability of latent causal graphical models without pure children](https://arxiv.org/abs/2505.18410)
*Seunghyun Lee,Yuqi Gu*

Main category: stat.ML

TL;DR: This paper addresses the challenge of identifying causal graphical models with latent variables by proposing a new double triangular graphical condition that ensures identifiability, relaxing previous stringent conditions.


<details>
  <summary>Details</summary>
Motivation: Existing identifiability conditions for causal graphical models with latent variables are too stringent, often requiring multiple pure children per latent variable or restrictions on the latent causal graph.

Method: The authors propose a double triangular graphical condition for identifiability in a general nonparametric measurement model with arbitrary observed variable types and binary latent variables.

Result: Simulation studies show that latent structures satisfying the proposed conditions can be accurately estimated from data.

Conclusion: The double triangular graphical condition significantly relaxes the popular pure children condition and provides valuable insights into the fundamental limits of identifiability.

Abstract: This paper considers a challenging problem of identifying a causal graphical
model under the presence of latent variables. While various identifiability
conditions have been proposed in the literature, they often require multiple
pure children per latent variable or restrictions on the latent causal graph.
Furthermore, it is common for all observed variables to exhibit the same
modality. Consequently, the existing identifiability conditions are often too
stringent for complex real-world data. We consider a general nonparametric
measurement model with arbitrary observed variable types and binary latent
variables, and propose a double triangular graphical condition that guarantees
identifiability of the entire causal graphical model. The proposed condition
significantly relaxes the popular pure children condition. We also establish
necessary conditions for identifiability and provide valuable insights into
fundamental limits of identifiability. Simulation studies verify that latent
structures satisfying our conditions can be accurately estimated from data.

</details>


### [815] [LocalKMeans: Convergence of Lloyd's Algorithm with Distributed Local Iterations](https://arxiv.org/abs/2505.18420)
*Harsh Vardhan,Heng Zhu,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TL;DR: The paper analyzes the classical K-means algorithm for a mixture of Gaussians in a distributed setting, proposing LocalKMeans and studying its cost and requirements.


<details>
  <summary>Details</summary>
Motivation: To understand the performance and cost of running the K-means algorithm in a distributed manner with local iteration steps.

Method: Propose an algorithm named LocalKMeans that performs Lloyd's algorithm in parallel on multiple machines using local data, synchronizing every L steps.

Result: Characterized the cost of local iterations compared to non-distributed settings and found that it requires a higher signal-to-noise ratio.

Conclusion: The analysis of unsupervised learning methods like K-means is more complex than gradient-based methods due to latent variables, and a virtual iterate method was adapted to work with non-convex objectives.

Abstract: In this paper, we analyze the classical $K$-means alternating-minimization
algorithm, also known as Lloyd's algorithm (Lloyd, 1956), for a mixture of
Gaussians in a data-distributed setting that incorporates local iteration
steps. Assuming unlabeled data distributed across multiple machines, we propose
an algorithm, LocalKMeans, that performs Lloyd's algorithm in parallel in the
machines by running its iterations on local data, synchronizing only every $L$
of such local steps. We characterize the cost of these local iterations against
the non-distributed setting, and show that the price paid for the local steps
is a higher required signal-to-noise ratio. While local iterations were
theoretically studied in the past for gradient-based learning methods, the
analysis of unsupervised learning methods is more involved owing to the
presence of latent variables, e.g. cluster identities, than that of an
iterative gradient-based algorithm. To obtain our results, we adapt a virtual
iterate method to work with a non-convex, non-smooth objective function, in
conjunction with a tight statistical analysis of Lloyd steps.

</details>


### [816] [On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts](https://arxiv.org/abs/2505.18455)
*Fanqi Yan,Huy Nguyen,Dung Le,Pedram Akbarian,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

TL;DR: 研究了softmax-contaminated MoE模型中最大似然估计的收敛速度，揭示了参数估计在prompt与预训练模型知识重叠时的问题，并通过数值实验验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: 尽管softmax-contaminated MoE模型在微调大规模预训练模型时被广泛应用，但其理论特性尚未被深入探讨。为了理解统计特性和可能的挑战，需要研究门控和prompt参数的最大似然估计的收敛速度。

Method: 分析了当prompt模型与预训练模型知识重叠时参数估计的影响，提出了区分性的解析概念。在满足区分性条件下，推导出门控和prompt参数的最小最大最优估计率；而在违反区分性条件时，估计率显著变慢，依赖于prompt收敛到预训练模型的速度。

Result: 在区分性条件成立时，成功推导出所有门控和prompt参数的最优估计率。而当条件不成立时，估计率变慢，验证了prompt与预训练模型知识重叠对参数估计的负面影响。并通过数值实验支持了这些理论发现。

Conclusion: 本研究表明，在使用新的prompt进行微调时，prompt与预训练模型的知识重叠会损害参数的可估计性，强调了区分性条件的重要性。

Abstract: The softmax-contaminated mixture of experts (MoE) model is deployed when a
large-scale pre-trained model, which plays the role of a fixed expert, is
fine-tuned for learning downstream tasks by including a new contamination part,
or prompt, functioning as a new, trainable expert. Despite its popularity and
relevance, the theoretical properties of the softmax-contaminated MoE have
remained unexplored in the literature. In the paper, we study the convergence
rates of the maximum likelihood estimator of gating and prompt parameters in
order to gain insights into the statistical properties and potential challenges
of fine-tuning with a new prompt. We find that the estimability of these
parameters is compromised when the prompt acquires overlapping knowledge with
the pre-trained model, in the sense that we make precise by formulating a novel
analytic notion of distinguishability. Under distinguishability of the
pre-trained and prompt models, we derive minimax optimal estimation rates for
all the gating and prompt parameters. By contrast, when the distinguishability
condition is violated, these estimation rates become significantly slower due
to their dependence on the prompt convergence rate to the pre-trained model.
Finally, we empirically corroborate our theoretical findings through several
numerical experiments.

</details>


### [817] [Statistical Inference under Performativity](https://arxiv.org/abs/2505.18493)
*Xiang Li,Yunai Li,Huiying Zhong,Lihua Lei,Zhun Deng*

Main category: stat.ML

TL;DR: 本文首次研究了在预测性影响下的统计推断，建立了中心极限定理，并提出了基于小规模标注数据和大规模机器学习预测数据的预测增强推断（PPI）方法，以提高政策参数估计的精确度和置信区域的质量。


<details>
  <summary>Details</summary>
Motivation: 预测性决策可能影响其预测目标的现象在社会科学和经济学政策制定中广泛存在，因此需要研究在预测性影响下的统计推断方法。

Method: 1. 建立了在预测性影响下的中心极限定理，用于估计和推断。2. 提出了预测增强推断（PPI），结合小规模标注数据和大规模机器学习预测数据进行更精确的估计和改进的置信区域构建。

Result: 通过数值实验验证了所提出框架的有效性，能够获得更精确的估计和改进的置信区域。

Conclusion: 本文是首个在预测性影响下建立统计推断的研究，为政策制定、统计学和机器学习带来了新的挑战和推断场景，具有重要意义。

Abstract: Performativity of predictions refers to the phenomena that
prediction-informed decisions may influence the target they aim to predict,
which is widely observed in policy-making in social sciences and economics. In
this paper, we initiate the study of statistical inference under
performativity. Our contribution is two-fold. First, we build a central limit
theorem for estimation and inference under performativity, which enables
inferential purposes in policy-making such as constructing confidence intervals
or testing hypotheses. Second, we further leverage the derived central limit
theorem to investigate prediction-powered inference (PPI) under performativity,
which is based on a small labeled dataset and a much larger dataset of
machine-learning predictions. This enables us to obtain more precise estimation
and improved confidence regions for the model parameter (i.e., policy) of
interest in performative prediction. We demonstrate the power of our framework
by numerical experiments. To the best of our knowledge, this paper is the first
one to establish statistical inference under performativity, which brings up
new challenges and inference settings that we believe will add significant
values to policy-making, statistics, and machine learning.

</details>


### [818] [Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition](https://arxiv.org/abs/2505.18526)
*Yunqin Zhu,Henry Shaowu Yuchi,Yao Xie*

Main category: stat.ML

TL;DR: 通过神经网络直接表示低秩核函数的可扩展深度核表示，实现线性时间与内存的精确高斯过程推理。


<details>
  <summary>Details</summary>
Motivation: 现有的深度核学习方法受限于基础核的选择、高昂的推理成本和稀疏近似的需求，需要一种更高效且数据驱动的方法来构建灵活的核函数。

Method: 基于Mercer定理，提出了一种完全数据驱动的深度核表示方法，利用神经网络通过少量基函数直接表示低秩核函数，并结合变分推断框架进行可扩展的小批量训练，同时提出了方差校正程序以改善不确定性估计。

Result: 在合成数据和真实数据上的实验表明，该方法在预测准确性、不确定性量化和计算效率方面具有显著优势。

Conclusion: 所提出的深度核高斯过程模型能够在保持高效推理的同时，提升预测性能和不确定性估计质量。

Abstract: Kernels are key to encoding prior beliefs and data structures in Gaussian
process (GP) models. The design of expressive and scalable kernels has garnered
significant research attention. Deep kernel learning enhances kernel
flexibility by feeding inputs through a neural network before applying a
standard parametric form. However, this approach remains limited by the choice
of base kernels, inherits high inference costs, and often demands sparse
approximations. Drawing on Mercer's theorem, we introduce a fully data-driven,
scalable deep kernel representation where a neural network directly represents
a low-rank kernel through a small set of basis functions. This construction
enables highly efficient exact GP inference in linear time and memory without
invoking inducing points. It also supports scalable mini-batch training based
on a principled variational inference framework. We further propose a simple
variance correction procedure to guard against overconfidence in uncertainty
estimates. Experiments on synthetic and real-world data demonstrate the
advantages of our deep kernel GP in terms of predictive accuracy, uncertainty
quantification, and computational efficiency.

</details>


### [819] [Non-Stationary Lipschitz Bandits](https://arxiv.org/abs/2505.18871)
*Nicolas Nguyen,Solenne Gaucher,Claire Vernade*

Main category: stat.ML

TL;DR: The paper addresses non-stationary Lipschitz bandits with infinite actions and changing reward functions, presenting an algorithm that achieves an optimal dynamic regret bound without needing prior knowledge of non-stationarity.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of non-stationary Lipschitz bandits where actions are infinite and reward function can change over time, aiming to achieve minimax-optimal dynamic regret bound.

Method: An algorithm is designed to adaptively track significant shifts in cumulative reward function using hierarchical discretization of action space for detecting reward changes.

Result: Achieves a minimax-optimal dynamic regret bound of $\mathcal{\widetilde{O}}(\tilde{L}^{1/3}T^{2/3})$ without requiring any prior knowledge of the non-stationarity.

Conclusion: This study provides the first optimal guarantee in the setting of non-stationary Lipschitz bandits.

Abstract: We study the problem of non-stationary Lipschitz bandits, where the number of
actions is infinite and the reward function, satisfying a Lipschitz assumption,
can change arbitrarily over time. We design an algorithm that adaptively tracks
the recently introduced notion of significant shifts, defined by large
deviations of the cumulative reward function. To detect such reward changes,
our algorithm leverages a hierarchical discretization of the action space.
Without requiring any prior knowledge of the non-stationarity, our algorithm
achieves a minimax-optimal dynamic regret bound of
$\mathcal{\widetilde{O}}(\tilde{L}^{1/3}T^{2/3})$, where $\tilde{L}$ is the
number of significant shifts and $T$ the horizon. This result provides the
first optimal guarantee in this setting.

</details>


### [820] [Marginal Fairness: Fair Decision-Making under Risk Measures](https://arxiv.org/abs/2505.18895)
*Fei Huang,Silvana M. Pesenti*

Main category: stat.ML

TL;DR: This paper introduces marginal fairness, a new individual fairness notion for equitable decision-making that ensures insensitivity to distributional perturbations in protected attributes. It models business decision-making as a two-step process and utilizes cascade sensitivity to capture the influence of protected attributes.


<details>
  <summary>Details</summary>
Motivation: To create a fair decision-making framework that accounts for protected attributes such as gender, race, and religion, ensuring equitable outcomes under risk-sensitive, regulatory constraints.

Method: Model business decision-making as a two-step process: predictive modeling stage estimating a prediction function based on both protected and non-protected covariates, and decision-making stage applying a generalized distortion risk measure conditional only on non-protected covariates. Modify the risk measure to make decisions insensitive to protected attributes.

Result: The numerical study and empirical implementation using an auto insurance dataset demonstrate the practical application of the marginal fairness framework.

Conclusion: Marginal fairness enforces equitable outcomes by making decisions insensitive to protected attributes, and cascade sensitivity captures how dependencies between covariates propagate the influence of these attributes.

Abstract: This paper introduces marginal fairness, a new individual fairness notion for
equitable decision-making in the presence of protected attributes such as
gender, race, and religion. This criterion ensures that decisions based on
generalized distortion risk measures are insensitive to distributional
perturbations in protected attributes, regardless of whether these attributes
are continuous, discrete, categorical, univariate, or multivariate. To
operationalize this notion and reflect real-world regulatory environments (such
as the EU gender-neutral pricing regulation), we model business decision-making
in highly regulated industries (such as insurance and finance) as a two-step
process: (i) a predictive modeling stage, in which a prediction function for
the target variable (e.g., insurance losses) is estimated based on both
protected and non-protected covariates; and (ii) a decision-making stage, in
which a generalized distortion risk measure is applied to the target variable,
conditional only on non-protected covariates, to determine the decision. In
this second step, we modify the risk measure such that the decision becomes
insensitive to the protected attribute, thus enforcing fairness to ensure
equitable outcomes under risk-sensitive, regulatory constraints. Furthermore,
by utilizing the concept of cascade sensitivity, we extend the marginal
fairness framework to capture how dependencies between covariates propagate the
influence of protected attributes through the modeling pipeline. A numerical
study and an empirical implementation using an auto insurance dataset
demonstrate how the framework can be applied in practice.

</details>


### [821] [On the Role of Label Noise in the Feature Learning Process](https://arxiv.org/abs/2505.18909)
*Andi Han,Wei Huang,Zhanpeng Zhou,Gang Niu,Wuyang Chen,Junchi Yan,Akiko Takeda,Taiji Suzuki*

Main category: stat.ML

TL;DR: This paper explores the effects of label noise on feature learning in deep learning, particularly analyzing the training dynamics of a two-layer convolutional neural network under a signal-noise data distribution.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often encounter challenges when dealing with noisy labels. Understanding how these models learn features and handle label noise is crucial for improving model performance.

Method: The authors consider a signal-noise data distribution where each sample has a label-dependent signal and label-independent noise. They analyze the training dynamics of a two-layer convolutional neural network under this setup, identifying two key stages: Stage I where the model fits clean samples and learns generalizable signals, and Stage II where overfitting occurs due to memorization of noise from noisy samples.

Result: The analysis reveals that in Stage I, the model successfully learns from clean samples without being affected by noisy ones. However, in Stage II, overfitting leads to degradation of the model's generalization ability. The findings also support techniques like early stopping and sample selection as effective strategies for handling label noise.

Conclusion: This work theoretically characterizes the role of label noise in feature learning, providing insights into why certain techniques are effective against label noise and validating these insights through experiments on both synthetic and real-world data.

Abstract: Deep learning with noisy labels presents significant challenges. In this
work, we theoretically characterize the role of label noise from a feature
learning perspective. Specifically, we consider a signal-noise data
distribution, where each sample comprises a label-dependent signal and
label-independent noise, and rigorously analyze the training dynamics of a
two-layer convolutional neural network under this data setup, along with the
presence of label noise. Our analysis identifies two key stages. In Stage I,
the model perfectly fits all the clean samples (i.e., samples without label
noise) while ignoring the noisy ones (i.e., samples with noisy labels). During
this stage, the model learns the signal from the clean samples, which
generalizes well on unseen data. In Stage II, as the training loss converges,
the gradient in the direction of noise surpasses that of the signal, leading to
overfitting on noisy samples. Eventually, the model memorizes the noise present
in the noisy samples and degrades its generalization ability. Furthermore, our
analysis provides a theoretical basis for two widely used techniques for
tackling label noise: early stopping and sample selection. Experiments on both
synthetic and real-world setups validate our theory.

</details>


### [822] [ALPCAHUS: Subspace Clustering for Heteroscedastic Data](https://arxiv.org/abs/2505.18918)
*Javier Salazar Cavazos,Jeffrey A Fessler,Laura Balzano*

Main category: stat.ML

TL;DR: ALPCAHUS是一种新的子空间聚类方法，能够估计样本噪声方差，并利用这些信息改进与数据低秩结构相关联的子空间基估计。该方法扩展了K-Subspaces原则和LR-ALPCAH方法，适用于处理具有异质噪声的联合子空间（UoS）设置下的聚类问题。实验表明，考虑数据异质性可以提高聚类效果。


<details>
  <summary>Details</summary>
Motivation: 当前的一些应用涉及由于噪声特性导致质量不同的异构数据，而现有的PCA扩展方法在处理多子空间数据聚类时未能充分考虑数据质量的差异。因此需要一种能够处理混合数据质量的方法。

Method: ALPCAHUS方法基于K-Subspaces (KSS) 原理，扩展了最近提出的LR-ALPCAH方法。它通过估计样本级别的噪声方差，并利用这些信息改进子空间基的估计，从而适应具有异质噪声的数据。

Result: 仿真和真实数据实验表明，与现有聚类算法相比，考虑数据异质性能够显著提高聚类效果。

Conclusion: ALPCAHUS方法在处理具有异质噪声的多子空间数据聚类问题上表现优异，为实际应用中处理异构数据提供了一种有效工具。

Abstract: Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction. Various methods have been proposed to extend PCA to
the union of subspace (UoS) setting for clustering data that come from multiple
subspaces like K-Subspaces (KSS). However, some applications involve
heterogeneous data that vary in quality due to noise characteristics associated
with each data sample. Heteroscedastic methods aim to deal with such mixed data
quality. This paper develops a heteroscedastic-focused subspace clustering
method, named ALPCAHUS, that can estimate the sample-wise noise variances and
use this information to improve the estimate of the subspace bases associated
with the low-rank structure of the data. This clustering algorithm builds on
K-Subspaces (KSS) principles by extending the recently proposed heteroscedastic
PCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS
setting. Simulations and real-data experiments show the effectiveness of
accounting for data heteroscedasticity compared to existing clustering
algorithms. Code available at https://github.com/javiersc1/ALPCAHUS.

</details>


### [823] [Optimal Conformal Prediction under Epistemic Uncertainty](https://arxiv.org/abs/2505.19033)
*Alireza Javanmardi,Soroush H. Zargarbashi,Santo M. A. R. Thies,Willem Waegeman,Aleksandar Bojchevski,Eyke Hüllermeier*

Main category: stat.ML

TL;DR: The paper explores incorporating second-order predictions into conformal prediction (CP), introducing Bernoulli prediction sets (BPS) for smallest prediction sets ensuring conditional coverage.


<details>
  <summary>Details</summary>
Motivation: There is an open question on how to incorporate second-order predictors, such as credal set predictors or Bayesian models, which represent both aleatoric and epistemic uncertainty, into the Conformal Prediction (CP) framework.

Method: The authors discuss the desiderata for CP with valid second-order predictions and introduce Bernoulli prediction sets (BPS). BPS ensures the smallest prediction sets that guarantee conditional coverage. It reduces to adaptive prediction sets (APS) when given first-order predictions. When validity of second-order predictions is compromised, conformal risk control is applied to maintain marginal coverage while accounting for epistemic uncertainty.

Result: The introduced Bernoulli prediction sets (BPS) produce the smallest prediction sets ensuring conditional coverage in the setting of second-order predictions. The method also provides a marginal coverage guarantee when the validity assumption on second-order predictions is compromised.

Conclusion: This work successfully addresses the challenge of incorporating second-order predictions into the CP framework, offering an effective solution through the introduction of BPS and demonstrating its advantages over traditional methods.

Abstract: Conformal prediction (CP) is a popular frequentist framework for representing
uncertainty by providing prediction sets that guarantee coverage of the true
label with a user-adjustable probability. In most applications, CP operates on
confidence scores coming from a standard (first-order) probabilistic predictor
(e.g., softmax outputs). Second-order predictors, such as credal set predictors
or Bayesian models, are also widely used for uncertainty quantification and are
known for their ability to represent both aleatoric and epistemic uncertainty.
Despite their popularity, there is still an open question on ``how they can be
incorporated into CP''. In this paper, we discuss the desiderata for CP when
valid second-order predictions are available. We then introduce Bernoulli
prediction sets (BPS), which produce the smallest prediction sets that ensure
conditional coverage in this setting. When given first-order predictions, BPS
reduces to the well-known adaptive prediction sets (APS). Furthermore, when the
validity assumption on the second-order predictions is compromised, we apply
conformal risk control to obtain a marginal coverage guarantee while still
accounting for epistemic uncertainty.

</details>


### [824] [When Models Don't Collapse: On the Consistency of Iterative MLE](https://arxiv.org/abs/2505.19046)
*Daniel Barzilai,Ohad Shamir*

Main category: stat.ML

TL;DR: 研究人员研究了在逐渐将合成数据添加到原始数据集的设定下，最大似然估计（MLE）中的模型崩塌问题。他们在标准假设下建立了非渐近界限，表明即使真实数据的比例消失，也可以避免模型崩塌。另一方面，他们证明了一些假设（超越MLE一致性）确实是必要的：没有这些假设，即使原始数据仍在训练集中，模型崩塌也可能迅速发生。


<details>
  <summary>Details</summary>
Motivation: 生成模型的广泛使用导致了一个反馈循环，其中每一代模型都基于部分由其前代产生的数据进行训练。这个过程引发了对模型崩塌的担忧，即由于反复在合成数据上训练而导致性能严重下降。然而，文献中不同的分析对模型崩塌的严重性得出了不同的结论。因此，尚不清楚这一现象有多令人担忧，以及在哪些假设下可以避免。

Method: 研究人员在自然设定下理论研究了最大似然估计（MLE）中的模型崩塌问题，其中合成数据逐渐被添加到原始数据集中。他们基于与证明MLE渐近一致性和正态性相似的标准假设进行了研究。

Result: 他们建立了非渐近界限，表明即使真实数据的比例消失，也可以避免模型崩塌。此外，他们证明了某些假设（超出MLE一致性）是必需的；没有这些假设，模型崩塌可能迅速发生，即使原始数据仍包含在训练集中。

Conclusion: 这是首批严格的迭代生成建模示例之一，展示了累积数据如何迅速导致模型崩塌。结果表明，在特定假设条件下，可以通过控制合成数据的质量和数量来避免模型崩塌。

Abstract: The widespread use of generative models has created a feedback loop, in which
each generation of models is trained on data partially produced by its
predecessors. This process has raised concerns about \emph{model collapse}: A
critical degradation in performance caused by repeated training on synthetic
data. However, different analyses in the literature have reached different
conclusions as to the severity of model collapse. As such, it remains unclear
how concerning this phenomenon is, and under which assumptions it can be
avoided. To address this, we theoretically study model collapse for maximum
likelihood estimation (MLE), in a natural setting where synthetic data is
gradually added to the original data set. Under standard assumptions (similar
to those long used for proving asymptotic consistency and normality of MLE), we
establish non-asymptotic bounds showing that collapse can be avoided even as
the fraction of real data vanishes. On the other hand, we prove that some
assumptions (beyond MLE consistency) are indeed necessary: Without them, model
collapse can occur arbitrarily quickly, even when the original data is still
present in the training set. To the best of our knowledge, these are the first
rigorous examples of iterative generative modeling with accumulating data that
rapidly leads to model collapse.

</details>


### [825] [Statistical inference for Linear Stochastic Approximation with Markovian Noise](https://arxiv.org/abs/2505.19102)
*Sergey Samsonov,Marina Sheshukova,Eric Moulines,Alexey Naumov*

Main category: stat.ML

TL;DR: This paper derives non-asymptotic Berry-Esseen bounds for Polyak-Ruppert averaged iterates of the LSA algorithm with Markovian noise, establishes a multiplier block bootstrap procedure for constructing confidence intervals, and provides non-asymptotic guarantees on the rate of convergence.


<details>
  <summary>Details</summary>
Motivation: To provide non-asymptotic guarantees on the rate of convergence to the Gaussian limit for Polyak-Ruppert averaged iterates of the LSA algorithm under Markovian noise and establish a valid method for constructing confidence intervals.

Method: Derive Berry-Esseen bounds for the LSA algorithm, establish a multiplier block bootstrap procedure for constructing confidence intervals, and analyze the convergence rates for both the main result and the estimation of asymptotic variance.

Result: Achieved a convergence rate of $\mathcal{O}(n^{-1/4})$ to the Gaussian limit in Kolmogorov distance and recovered the classical rate of order $\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the asymptotic variance.

Conclusion: The paper provides the first non-asymptotic guarantees on the rate of convergence of bootstrap-based confidence intervals for stochastic approximation with Markov noise.

Abstract: In this paper we derive non-asymptotic Berry-Esseen bounds for Polyak-Ruppert
averaged iterates of the Linear Stochastic Approximation (LSA) algorithm driven
by the Markovian noise. Our analysis yields $\mathcal{O}(n^{-1/4})$ convergence
rates to the Gaussian limit in the Kolmogorov distance. We further establish
the non-asymptotic validity of a multiplier block bootstrap procedure for
constructing the confidence intervals, guaranteeing consistent inference under
Markovian sampling. Our work provides the first non-asymptotic guarantees on
the rate of convergence of bootstrap-based confidence intervals for stochastic
approximation with Markov noise. Moreover, we recover the classical rate of
order $\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the
asymptotic variance of the iterates of the LSA algorithm.

</details>


### [826] [Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference](https://arxiv.org/abs/2505.19136)
*Frank Shih,Zhenghao Jiang,Faming Liang*

Main category: stat.ML

TL;DR: This paper proposes a novel method within the framework of extended fiducial inference (EFI) for uncertainty quantification in physics-informed neural networks (PINNs). It leverages a narrow-neck hyper-network to learn PINN parameters and quantify their uncertainty based on imputed random errors, overcoming limitations of Bayesian and dropout methods. This advancement enhances PINNs' reliability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty quantification methods for PINNs, such as Bayesian or dropout methods, require additional information to construct honest confidence sets, which limits their applicability.

Method: The proposed method uses extended fiducial inference (EFI) with a narrow-neck hyper-network to learn the parameters of the PINN and quantify their uncertainty based on imputed random errors in the observations.

Result: This approach enables the construction of honest confidence sets based solely on observed data, enhancing the reliability, interpretability, and applicability of PINNs to real-world scientific and engineering challenges.

Conclusion: The paper establishes a new theoretical framework for EFI, extending its application to large-scale models, eliminating the need for sparse hyper-networks, and improving the automaticity and robustness of statistical inference.

Abstract: Uncertainty quantification (UQ) in scientific machine learning is
increasingly critical as neural networks are widely adopted to tackle complex
problems across diverse scientific disciplines. For physics-informed neural
networks (PINNs), a prominent model in scientific machine learning, uncertainty
is typically quantified using Bayesian or dropout methods. However, both
approaches suffer from a fundamental limitation: the prior distribution or
dropout rate required to construct honest confidence sets cannot be determined
without additional information. In this paper, we propose a novel method within
the framework of extended fiducial inference (EFI) to provide rigorous
uncertainty quantification for PINNs. The proposed method leverages a
narrow-neck hyper-network to learn the parameters of the PINN and quantify
their uncertainty based on imputed random errors in the observations. This
approach overcomes the limitations of Bayesian and dropout methods, enabling
the construction of honest confidence sets based solely on observed data. This
advancement represents a significant breakthrough for PINNs, greatly enhancing
their reliability, interpretability, and applicability to real-world scientific
and engineering challenges. Moreover, it establishes a new theoretical
framework for EFI, extending its application to large-scale models, eliminating
the need for sparse hyper-networks, and significantly improving the
automaticity and robustness of statistical inference.

</details>


### [827] [PIGPVAE: Physics-Informed Gaussian Process Variational Autoencoders](https://arxiv.org/abs/2505.19320)
*Michail Spitieris,Massimiliano Ruocco,Abdulmajid Murad,Alessandro Nocente*

Main category: stat.ML

TL;DR: The paper presents a novel generative model, PIGPVAE, which incorporates physical constraints and Gaussian Process to generate synthetic data effectively from limited datasets. It enhances performance in capturing complex temporal dependencies and achieves state-of-the-art results on indoor temperature data.


<details>
  <summary>Details</summary>
Motivation: Existing generative AI methods require large datasets for effective training, which may not always be available.

Method: The method extends the VAE architecture by incorporating physical models and a discrepancy term within a latent Gaussian Process VAE (GPVAE). Regularization is applied to ensure alignment with observed data.

Result: PIGPVAE achieves state-of-the-art performance on indoor temperature data and can produce realistic samples beyond the observed distribution.

Conclusion: PIGPVAE is robust and useful under distribution shifts, offering an effective solution for synthetic data generation from limited datasets.

Abstract: Recent advances in generative AI offer promising solutions for synthetic data
generation but often rely on large datasets for effective training. To address
this limitation, we propose a novel generative model that learns from limited
data by incorporating physical constraints to enhance performance.
Specifically, we extend the VAE architecture by incorporating physical models
in the generative process, enabling it to capture underlying dynamics more
effectively. While physical models provide valuable insights, they struggle to
capture complex temporal dependencies present in real-world data. To bridge
this gap, we introduce a discrepancy term to account for unmodeled dynamics,
represented within a latent Gaussian Process VAE (GPVAE). Furthermore, we apply
regularization to ensure the generated data aligns closely with observed data,
enhancing both the diversity and accuracy of the synthetic samples. The
proposed method is applied to indoor temperature data, achieving
state-of-the-art performance. Additionally, we demonstrate that PIGPVAE can
produce realistic samples beyond the observed distribution, highlighting its
robustness and usefulness under distribution shifts.

</details>


### [828] [Adaptive Diffusion Guidance via Stochastic Optimal Control](https://arxiv.org/abs/2505.19367)
*Iskander Azangulov,Peter Potaptchik,Qinyu Li,Eddie Aamari,George Deligiannidis,Judith Rousseau*

Main category: stat.ML

TL;DR: The paper addresses limitations in guidance scheduling for diffusion models by providing a theoretical formalization of the relationship between guidance strength and classifier confidence, and introducing a stochastic optimal control framework for adaptive optimization of guidance.


<details>
  <summary>Details</summary>
Motivation: Current approaches to guidance scheduling lack a solid theoretical foundation.

Method: Theoretical formalization of the relationship between guidance strength and classifier confidence and introduction of a stochastic optimal control framework.

Result: Establishes a principled foundation for more effective guidance in diffusion models.

Conclusion: This work enhances the understanding and effectiveness of guidance in diffusion models through theoretical analysis and adaptive optimization.

Abstract: Guidance is a cornerstone of modern diffusion models, playing a pivotal role
in conditional generation and enhancing the quality of unconditional samples.
However, current approaches to guidance scheduling--determining the appropriate
guidance weight--are largely heuristic and lack a solid theoretical foundation.
This work addresses these limitations on two fronts. First, we provide a
theoretical formalization that precisely characterizes the relationship between
guidance strength and classifier confidence. Second, building on this insight,
we introduce a stochastic optimal control framework that casts guidance
scheduling as an adaptive optimization problem. In this formulation, guidance
strength is not fixed but dynamically selected based on time, the current
sample, and the conditioning class, either independently or in combination. By
solving the resulting control problem, we establish a principled foundation for
more effective guidance in diffusion models.

</details>


### [829] [Uniform convergence of the smooth calibration error and its relationship with functional gradient](https://arxiv.org/abs/2505.19396)
*Futoshi Futami,Atsushi Nitanda*

Main category: stat.ML

TL;DR: The paper focuses on smooth calibration error, establishes a uniform convergence bound and proves the role of functional gradient in controlling training smooth calibration error. It analyzes three algorithms to derive conditions ensuring both classification and calibration performances.


<details>
  <summary>Details</summary>
Motivation: Calibration is crucial for reliable probabilistic prediction, but theoretical understanding of learning algorithms that achieve high accuracy and good calibration is limited.

Method: The work provides a uniform convergence bound for smooth calibration error and shows it's relationship with training dataset smooth CE and generalization gap. It also proves the effectiveness of functional gradient in controlling training smooth CE. Three algorithms are analyzed under this framework.

Result: Conditions are derived for gradient boosting trees, kernel boosting, and two-layer neural networks where both classification and calibration performances are guaranteed simultaneously.

Conclusion: This study offers new theoretical insights and practical guidance for designing reliable probabilistic models with provable calibration guarantees.

Abstract: Calibration is a critical requirement for reliable probabilistic prediction,
especially in high-risk applications. However, the theoretical understanding of
which learning algorithms can simultaneously achieve high accuracy and good
calibration remains limited, and many existing studies provide empirical
validation or a theoretical guarantee in restrictive settings. To address this
issue, in this work, we focus on the smooth calibration error (CE) and provide
a uniform convergence bound, showing that the smooth CE is bounded by the sum
of the smooth CE over the training dataset and a generalization gap. We further
prove that the functional gradient of the loss function can effectively control
the training smooth CE. Based on this framework, we analyze three
representative algorithms: gradient boosting trees, kernel boosting, and
two-layer neural networks. For each, we derive conditions under which both
classification and calibration performances are simultaneously guaranteed. Our
results offer new theoretical insights and practical guidance for designing
reliable probabilistic models with provable calibration guarantees.

</details>


### [830] [Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables](https://arxiv.org/abs/2505.19470)
*Futoshi Futami,Masahiro Fujisawa*

Main category: stat.ML

TL;DR: This paper extends information-theoretic generalization analysis to VQ-VAEs with discrete latent spaces, providing a new error bound and explaining LVs' role in data generation.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical studies on latent variables have mainly focused on supervised learning. There is a lack of similar analyses for unsupervised models like VAEs, particularly regarding their generalization properties.

Method: The authors introduce a data-dependent prior to analyze the relationship between latent variables, generalization, and data generation in VQ-VAEs. They derive a novel generalization error bound for the reconstruction loss that depends on the complexity of the latent variables and encoder, and also provide an upper bound for the 2-Wasserstein distance.

Result: A novel generalization error bound was derived which is independent of the decoder. An upper bound for the 2-Wasserstein distance was provided, elucidating how regularization of latent variables affects data generation performance.

Conclusion: This work enhances understanding of the role of latent variables in VQ-VAEs and contributes to improving data generation performance through better regulation of these variables.

Abstract: Latent variables (LVs) play a crucial role in encoder-decoder models by
enabling effective data compression, prediction, and generation. Although their
theoretical properties, such as generalization, have been extensively studied
in supervised learning, similar analyses for unsupervised models such as
variational autoencoders (VAEs) remain insufficiently underexplored. In this
work, we extend information-theoretic generalization analysis to
vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel
data-dependent prior to rigorously analyze the relationship among LVs,
generalization, and data generation. We derive a novel generalization error
bound of the reconstruction loss of VQ-VAEs, which depends solely on the
complexity of LVs and the encoder, independent of the decoder. Additionally, we
provide the upper bound of the 2-Wasserstein distance between the distributions
of the true data and the generated data, explaining how the regularization of
the LVs contributes to the data generation performance.

</details>


### [831] [Accelerating Nash Learning from Human Feedback via Mirror Prox](https://arxiv.org/abs/2505.19731)
*Daniil Tiapkin,Daniele Calandriello,Denis Belomestny,Eric Moulines,Alexey Naumov,Kashif Rasul,Michal Valko,Pierre Menard*

Main category: stat.ML

TL;DR: The paper introduces Nash Mirror Prox (Nash-MP), an algorithm for Nash Learning from Human Feedback (NLHF) that achieves fast and stable convergence to the Nash equilibrium, with theoretical guarantees and practical applications.


<details>
  <summary>Details</summary>
Motivation: Traditional Reinforcement Learning from Human Feedback (RLHF) relies on reward models which may not accurately capture the complexities of real human preferences. To address this issue, the authors explore an alternative approach called Nash Learning from Human Feedback (NLHF), which frames the problem as finding a Nash equilibrium based on human preferences.

Method: The authors introduce Nash Mirror Prox (Nash-MP), an online NLHF algorithm leveraging the Mirror Prox optimization scheme. They provide a theoretical analysis proving last-iterate linear convergence towards the β-regularized Nash equilibrium, showing that the KL-divergence to the optimal policy decreases at a specific rate. Additionally, they propose an approximate version of Nash-MP using stochastic policy gradients, making it more applicable in practice. Finally, they detail a practical implementation strategy for fine-tuning large language models.

Result: The theoretical results demonstrate last-iterate linear convergence for Nash-MP, with rates independent of the action space size. The experimental results show competitive performance and compatibility with existing methods when applied to fine-tuning large language models.

Conclusion: Nash Mirror Prox offers a promising approach for Nash Learning from Human Feedback, providing both strong theoretical guarantees and practical applicability, especially for tasks like fine-tuning large language models.

Abstract: Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on
reward models, frequently assuming preference structures like the Bradley-Terry
model, which may not accurately capture the complexities of real human
preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)
offers a more direct alternative by framing the problem as finding a Nash
equilibrium of a game defined by these preferences. In this work, we introduce
Nash Mirror Prox ($\mathtt{Nash-MP}$), an online NLHF algorithm that leverages
the Mirror Prox optimization scheme to achieve fast and stable convergence to
the Nash equilibrium. Our theoretical analysis establishes that Nash-MP
exhibits last-iterate linear convergence towards the $\beta$-regularized Nash
equilibrium. Specifically, we prove that the KL-divergence to the optimal
policy decreases at a rate of order $(1+2\beta)^{-N/2}$, where $N$ is a number
of preference queries. We further demonstrate last-iterate linear convergence
for the exploitability gap and uniformly for the span semi-norm of
log-probabilities, with all these rates being independent of the size of the
action space. Furthermore, we propose and analyze an approximate version of
Nash-MP where proximal steps are estimated using stochastic policy gradients,
making the algorithm closer to applications. Finally, we detail a practical
implementation strategy for fine-tuning large language models and present
experiments that demonstrate its competitive performance and compatibility with
existing methods.

</details>


### [832] [Weighted Leave-One-Out Cross Validation](https://arxiv.org/abs/2505.19737)
*Luc Pronzato,Maria-João Rendas*

Main category: stat.ML

TL;DR: The paper proposes a weighted version of Leave-One-Out (LOO) cross-validation to estimate Integrated Squared Error (ISE) for function approximation, demonstrating higher precision than classical LOO and exploring its application in model selection.


<details>
  <summary>Details</summary>
Motivation: To enhance the precision of estimating ISE when approximating an unknown function using linear predictors, by developing a weighted LOO method that leverages squared LOO residuals.

Method: Constructs the best linear estimator of the squared prediction error at an unsampled site based on squared LOO residuals under the assumption that the function is a realization of a Gaussian Process (GP).

Result: The theoretical analysis shows improved performance of the ISE estimator compared to classical unweighted LOO cross-validation. Robustness with respect to the choice of GP kernel is confirmed both analytically and through numerical examples.

Conclusion: The weighted LOO approach provides a more precise estimation of ISE than traditional methods, with potential applications in model selection demonstrated through examples.

Abstract: We present a weighted version of Leave-One-Out (LOO) cross-validation for
estimating the Integrated Squared Error (ISE) when approximating an unknown
function by a predictor that depends linearly on evaluations of the function
over a finite collection of sites. The method relies on the construction of the
best linear estimator of the squared prediction error at an arbitrary unsampled
site based on squared LOO residuals, assuming that the function is a
realization of a Gaussian Process (GP). A theoretical analysis of performance
of the ISE estimator is presented, and robustness with respect to the choice of
the GP kernel is investigated first analytically, then through numerical
examples. Overall, the estimation of ISE is significantly more precise than
with classical, unweighted, LOO cross validation. Application to model
selection is briefly considered through examples.

</details>


### [833] [Efficient Deconvolution in Populational Inverse Problems](https://arxiv.org/abs/2505.19841)
*Arnaud Vadeboncoeur,Mark Girolami,Andrew M. Stuart*

Main category: stat.ML

TL;DR: 这篇论文提出了一种解决分布反演问题的方法，该方法通过利用从相同物理过程的不同实例中收集的大型数据集，同时进行数据去噪和识别模型参数。此外，还提出了一个新的主动学习方案，以加速计算并实现自动微分。


<details>
  <summary>Details</summary>
Motivation: 在推断导致多组观测结果的参数分布时，盲去卷积是一个主要障碍，特别是在观测噪声分布未知的情况下。然而，当数据来源于物理系统的集合时，可以利用这些信息来进行去卷积。

Method: 提出了一种利用从相同物理过程的不同实例中收集的大型数据集的方法，定义了一个损失函数来表征观测数据与数学模型输出之间的匹配程度，并通过修改的梯度下降算法最小化该函数。此外，还提出了一种基于自适应经验测度的新主动学习方案。

Result: 该方法成功应用于多孔介质流、阻尼弹性动力学和简化的大气动力学模型上，证明了其有效性。

Conclusion: 所提出的方法能够有效地解决分布反演问题，并通过主动学习方案加速计算，为处理复杂物理过程提供了新的工具。

Abstract: This work is focussed on the inversion task of inferring the distribution
over parameters of interest leading to multiple sets of observations. The
potential to solve such distributional inversion problems is driven by
increasing availability of data, but a major roadblock is blind deconvolution,
arising when the observational noise distribution is unknown. However, when
data originates from collections of physical systems, a population, it is
possible to leverage this information to perform deconvolution. To this end, we
propose a methodology leveraging large data sets of observations, collected
from different instantiations of the same physical processes, to simultaneously
deconvolve the data corrupting noise distribution, and to identify the
distribution over model parameters defining the physical processes. A
parameter-dependent mathematical model of the physical process is employed. A
loss function characterizing the match between the observed data and the output
of the mathematical model is defined; it is minimized as a function of the both
the parameter inputs to the model of the physics and the parameterized
observational noise. This coupled problem is addressed with a modified gradient
descent algorithm that leverages specific structure in the noise model.
Furthermore, a new active learning scheme is proposed, based on adaptive
empirical measures, to train a surrogate model to be accurate in parameter
regions of interest; this approach accelerates computation and enables
automatic differentiation of black-box, potentially nondifferentiable, code
computing parameter-to-solution maps. The proposed methodology is demonstrated
on porous medium flow, damped elastodynamics, and simplified models of
atmospheric dynamics.

</details>


### [834] [Linear Bandits with Non-i.i.d. Noise](https://arxiv.org/abs/2505.20017)
*Baptiste Abélès,Eugenio Clerico,Hamish Flynn,Gergely Neu*

Main category: stat.ML

TL;DR: The paper explores linear stochastic bandit problems with interdependent sub-Gaussian noise that decays over time, proposing new confidence sequences and a bandit algorithm based on optimism in the face of uncertainty. Regret bounds are provided, which recover standard rates factoring in mixing time for geometrically mixing observation noise.


<details>
  <summary>Details</summary>
Motivation: The motivation is to relax the restrictive i.i.d. assumption on observation noise in linear stochastic bandit problems, allowing for interdependent noise terms that decay over time.

Method: New confidence sequences were developed using a reduction scheme to sequential probability assignment. A bandit algorithm was derived based on the principle of optimism in the face of uncertainty.

Result: Regret bounds were obtained for the algorithm, expressed in terms of the decay rate of dependence between observations. These bounds recover standard rates up to a factor of the mixing time for geometrically mixing observation noise.

Conclusion: The study successfully addresses the linear stochastic bandit problem with interdependent noise, providing regret bounds that account for the decay of dependence over time.

Abstract: We study the linear stochastic bandit problem, relaxing the standard i.i.d.
assumption on the observation noise. As an alternative to this restrictive
assumption, we allow the noise terms across rounds to be sub-Gaussian but
interdependent, with dependencies that decay over time. To address this
setting, we develop new confidence sequences using a recently introduced
reduction scheme to sequential probability assignment, and use these to derive
a bandit algorithm based on the principle of optimism in the face of
uncertainty. We provide regret bounds for the resulting algorithm, expressed in
terms of the decay rate of the strength of dependence between observations.
Among other results, we show that our bounds recover the standard rates up to a
factor of the mixing time for geometrically mixing observation noise.

</details>


### [835] [No Free Lunch: Non-Asymptotic Analysis of Prediction-Powered Inference](https://arxiv.org/abs/2505.20178)
*Pranav Mani,Peng Xu,Zachary C. Lipton,Michael Oberst*

Main category: stat.ML

TL;DR: Prediction-Powered Inference (PPI++)在特定条件下可以降低估计误差，但并非总是优于仅使用金标准标签。其表现依赖于伪标签与金标准标签之间的相关性以及样本数量。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明PPI++在渐进情况下具有较低的方差，但需要进一步探讨其在有限样本下的性能，特别是是否存在‘免费午餐’。

Method: 作者对PPI++进行了精确的有限样本分析，研究了其在均值估计问题中的估计误差，并给出了PPI++优于仅使用金标准标签的条件。

Result: 结果表明，PPI++仅在伪标签与金标准标签的相关性超过某一阈值时才表现出更好的性能，该阈值与标记样本数$n$有关。例如，对于高斯数据，相关性至少为$1/\sqrt{n - 2}$才能看到改进。

Conclusion: PPI++在有限样本下并不总是优于仅使用金标准标签的方法，其优势取决于伪标签的质量和样本数量。实验验证了理论发现，并提供了关于PPI++变体之间权衡的见解。

Abstract: Prediction-Powered Inference (PPI) is a popular strategy for combining
gold-standard and possibly noisy pseudo-labels to perform statistical
estimation. Prior work has shown an asymptotic "free lunch" for PPI++, an
adaptive form of PPI, showing that the *asymptotic* variance of PPI++ is always
less than or equal to the variance obtained from using gold-standard labels
alone. Notably, this result holds *regardless of the quality of the
pseudo-labels*. In this work, we demystify this result by conducting an exact
finite-sample analysis of the estimation error of PPI++ on the mean estimation
problem. We give a "no free lunch" result, characterizing the settings (and
sample sizes) where PPI++ has provably worse estimation error than using
gold-standard labels alone. Specifically, PPI++ will outperform if and only if
the correlation between pseudo- and gold-standard is above a certain level that
depends on the number of labeled samples ($n$). In some cases our results
simplify considerably: For Gaussian data, the correlation must be at least
$1/\sqrt{n - 2}$ in order to see improvement, and a similar result holds for
binary labels. In experiments, we illustrate that our theoretical findings hold
on real-world datasets, and give insights into trade-offs between single-sample
and sample-splitting variants of PPI++.

</details>


### [836] [Lorentz Local Canonicalization: How to Make Any Network Lorentz-Equivariant](https://arxiv.org/abs/2505.20280)
*Jonas Spinner,Luigi Favaro,Peter Lippmann,Sebastian Pitz,Gerrit Gerhartz,Tilman Plehn,Fred A. Hamprecht*

Main category: stat.ML

TL;DR: Lorentz-equivariant neural networks are revolutionizing high-energy physics, with a new framework called LLoCa that enhances accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To create a more flexible and efficient method for achieving Lorentz-equivariance in neural networks used in high-energy physics.

Method: Introduction of Lorentz Local Canonicalization (LLoCa), which transforms any backbone network into an exactly Lorentz-equivariant one by using equivariantly predicted local reference frames. This includes constructing LLoCa-transformers and graph networks and adapting geometric message passing to the non-compact Lorentz group.

Result: Models built using LLoCa surpass state-of-the-art accuracy on particle physics tasks, while being significantly faster and using fewer FLOPs.

Conclusion: LLoCa provides a general framework for rendering any network Lorentz-equivariant, leading to improved performance and efficiency in high-energy physics applications.

Abstract: Lorentz-equivariant neural networks are becoming the leading architectures
for high-energy physics. Current implementations rely on specialized layers,
limiting architectural choices. We introduce Lorentz Local Canonicalization
(LLoCa), a general framework that renders any backbone network exactly
Lorentz-equivariant. Using equivariantly predicted local reference frames, we
construct LLoCa-transformers and graph networks. We adapt a recent approach to
geometric message passing to the non-compact Lorentz group, allowing
propagation of space-time tensorial features. Data augmentation emerges from
LLoCa as a special choice of reference frame. Our models surpass
state-of-the-art accuracy on relevant particle physics tasks, while being
$4\times$ faster and using $5$-$100\times$ fewer FLOPs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [837] [Towards medical AI misalignment: a preliminary study](https://arxiv.org/abs/2505.18212)
*Barbara Puccio,Federico Castagna,Allan Tucker,Pierangelo Veltri*

Main category: cs.CY

TL;DR: 大型语言模型（LLMs）尽管能力惊人，但仍可能被恶意用户通过角色扮演（如`Goofy Game'）的方式破解安全防护，导致生成不安全内容。本研究初步探讨了无技术知识的恶意用户如何构建角色扮演提示以诱使LLM生成错误或有害的临床建议，揭示了一个特定漏洞场景，为未来的研究提供参考。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多任务中表现出色，但它们仍然容易受到用户的破解尝试。尤其是名为`Goofy Game'的角色扮演方法对当前LLMs的安全防护构成了挑战，可能导致生成不安全内容，尤其是在医疗领域可能会带来危险后果。因此，研究者希望探索恶意用户如何利用角色扮演提示来让LLM生成潜在有害的内容，从而揭示这一漏洞并推动改进。

Method: 研究者通过分析角色扮演提示的方法，展示了即使不了解生成式AI模型的内部架构和参数，恶意用户也可以构建出能够迫使LLM生成错误或有害临床建议的提示。这种方法主要依赖于特定的文本输入策略，诱导模型产生不符合预期的结果。

Result: 研究表明，通过角色扮演提示，恶意用户可以成功地让LLM生成错误或潜在危险的临床建议。这表明当前LLMs在面对此类攻击时存在显著漏洞。

Conclusion: 该研究揭示了一种特定的漏洞场景，即通过角色扮演提示可以让LLM生成错误或有害的内容。这种发现为未来的LLM安全性研究提供了重要参考，并强调了加强模型防护机制的必要性。

Abstract: Despite their staggering capabilities as assistant tools, often exceeding
human performances, Large Language Models (LLMs) are still prone to jailbreak
attempts from malevolent users. Although red teaming practices have already
identified and helped to address several such jailbreak techniques, one
particular sturdy approach involving role-playing (which we named `Goofy Game')
seems effective against most of the current LLMs safeguards. This can result in
the provision of unsafe content, which, although not harmful per se, might lead
to dangerous consequences if delivered in a setting such as the medical domain.
In this preliminary and exploratory study, we provide an initial analysis of
how, even without technical knowledge of the internal architecture and
parameters of generative AI models, a malicious user could construct a
role-playing prompt capable of coercing an LLM into producing incorrect (and
potentially harmful) clinical suggestions. We aim to illustrate a specific
vulnerability scenario, providing insights that can support future advancements
in the field.

</details>


### [838] [AIDRIN 2.0: A Framework to Assess Data Readiness for AI](https://arxiv.org/abs/2505.18213)
*Kaveen Hiniduma,Dylan Ryan,Suren Byna,Jean Luca Bez,Ravi Madduri*

Main category: cs.CY

TL;DR: AIDRIN framework is enhanced with better UI and PPFL integration to evaluate and improve data readiness in AI applications, demonstrated through a case study.


<details>
  <summary>Details</summary>
Motivation: To address critical data readiness dimensions such as quality, bias, fairness, and privacy in AI applications and make the AIDRIN framework more accessible and practical for users with varying technical expertise.

Method: Enhancements include user interface improvements and integration with a privacy-preserving federated learning framework to prioritize data readiness and privacy in decentralized AI pipelines.

Result: A case study using a real-world dataset showed AIDRIN's value in identifying data readiness issues affecting AI model performance.

Conclusion: The improved AIDRIN framework successfully evaluates and improves data preparedness for AI applications while ensuring privacy and accessibility.

Abstract: AI Data Readiness Inspector (AIDRIN) is a framework to evaluate and improve
data preparedness for AI applications. It addresses critical data readiness
dimensions such as data quality, bias, fairness, and privacy. This paper
details enhancements to AIDRIN by focusing on user interface improvements and
integration with a privacy-preserving federated learning (PPFL) framework. By
refining the UI and enabling smooth integration with decentralized AI
pipelines, AIDRIN becomes more accessible and practical for users with varying
technical expertise. Integrating with an existing PPFL framework ensures that
data readiness and privacy are prioritized in federated learning environments.
A case study involving a real-world dataset demonstrates AIDRIN's practical
value in identifying data readiness issues that impact AI model performance.

</details>


### [839] [Navigating Pitfalls: Evaluating LLMs in Machine Learning Programming Education](https://arxiv.org/abs/2505.18220)
*Smitha Kumar,Michael A. Lones,Manuel Maarek,Hind Zantout*

Main category: cs.CY

TL;DR: The study explores the use of LLMs in machine learning education, focusing on their ability to identify pitfalls in code and provide feedback. While LLMs can identify basic pitfalls, they struggle with more complex ones, especially in early ML pipeline stages and model selection. This raises questions about their current use in education and by novices. However, when successful, LLMs offer valuable feedback. The gap between closed and open models is small despite size differences, suggesting potential for smaller, customized models in education.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of LLMs in supporting machine learning education by identifying common errors (pitfalls) in code and providing feedback that can guide learners.

Method: Using a portfolio of code samples, four different LLMs (one closed and three open models) were tested on their ability to identify pitfalls in machine learning code and offer feedback.

Result: LLMs can identify basic pitfalls but struggle with more complex ones, particularly in early ML pipeline stages and model selection. Feedback provided when successful includes advice on how to proceed. The performance gap between closed and open LLMs is relatively small despite large differences in model sizes.

Conclusion: Current LLMs have limitations in supporting machine learning education and novice practitioners due to difficulties in identifying complex pitfalls. However, they show potential in guiding learners when successful. Smaller, customized open LLMs could be deployed in education settings to avoid risks associated with commercial models.

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new avenues
in education. This study examines the use of LLMs in supporting learning in
machine learning education; in particular, it focuses on the ability of LLMs to
identify common errors of practice (pitfalls) in machine learning code, and
their ability to provide feedback that can guide learning. Using a portfolio of
code samples, we consider four different LLMs: one closed model and three open
models. Whilst the most basic pitfalls are readily identified by all models,
many common pitfalls are not. They particularly struggle to identify pitfalls
in the early stages of the ML pipeline, especially those which can lead to
information leaks, a major source of failure within applied ML projects. They
also exhibit limited success at identifying pitfalls around model selection,
which is a concept that students often struggle with when first transitioning
from theory to practice. This questions the use of current LLMs to support
machine learning education, and also raises important questions about their use
by novice practitioners. Nevertheless, when LLMs successfully identify pitfalls
in code, they do provide feedback that includes advice on how to proceed,
emphasising their potential role in guiding learners. We also compare the
capability of closed and open LLM models, and find that the gap is relatively
small given the large difference in model sizes. This presents an opportunity
to deploy, and potentially customise, smaller more efficient LLM models within
education, avoiding risks around cost and data sharing associated with
commercial models.

</details>


### [840] [From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing](https://arxiv.org/abs/2505.18236)
*Natalia Matuszczyk,Craig R. Barnes,Rohit Gupta,Bulent Ozel,Aniket Mitra*

Main category: cs.CY

TL;DR: This paper synthesizes the scattered evidence of bias in GeoAI models and examines how the EU AI Act shapes audit obligations, identifying high-risk GeoAI systems and suggesting routine bias audits before 2027.


<details>
  <summary>Details</summary>
Motivation: To provide a concise overview of bias in GeoAI and examine the impact of the EU AI Act on audit obligations.

Method: Synthesizing fragmented literature, discussing recurring bias mechanisms, mapping them to specific provisions of the EU AI Act, and presenting examples of recent audits along with practical methods for detecting bias.

Result: Demonstrated that widely deployed GeoAI applications qualify as high-risk systems under the EU AI Act's criteria and provided an integration of GeoAI bias evidence into the EU AI Act context.

Conclusion: Even well-curated European datasets should employ routine bias audits before 2027 when the AI Act's high-risk provisions take full effect.

Abstract: Bias in geospatial artificial intelligence (GeoAI) models has been
documented, yet the evidence is scattered across narrowly focused studies. We
synthesize this fragmented literature to provide a concise overview of bias in
GeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes
audit obligations. We discuss recurring bias mechanisms, including
representation, algorithmic and aggregation bias, and map them to specific
provisions of the EU AI Act. By applying the Act's high-risk criteria, we
demonstrate that widely deployed GeoAI applications qualify as high-risk
systems. We then present examples of recent audits along with an outline of
practical methods for detecting bias. As far as we know, this study represents
the first integration of GeoAI bias evidence into the EU AI Act context, by
identifying high-risk GeoAI systems and mapping bias mechanisms to the Act's
Articles. Although the analysis is exploratory, it suggests that even
well-curated European datasets should employ routine bias audits before 2027,
when the AI Act's high-risk provisions take full effect.

</details>


### [841] [Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications](https://arxiv.org/abs/2505.18371)
*Riley Simmons-Edler,Jean Dong,Paul Lushenko,Kanaka Rajan,Ryan P. Badman*

Main category: cs.CY

TL;DR: The development and deployment of AI in military weapon systems have introduced novel risks which threaten military effectiveness and the openness of AI research. Effective regulation must be grounded in the technical behavior of AI models, and AI researchers must be involved throughout the regulatory lifecycle.


<details>
  <summary>Details</summary>
Motivation: To address the sociotechnical impacts of AI on combat systems, military decision-making, and the norms of warfare, especially focusing on a specific subset of lethal autonomous weapon systems (LAWS) that use AI for targeting or battlefield decisions, referred to as AI-powered lethal autonomous weapon systems (AI-LAWS).

Method: Propose a clear, behavior-based definition of AI-LAWS as a foundation for technically grounded regulation, and invite greater participation from the AI research community in military AI policy discussions.

Result: The proposed behavior-based definition of AI-LAWS can serve as a foundation for technically grounded regulation, and several technically-informed policy directions have been proposed.

Conclusion: AI researchers must be involved throughout the regulatory lifecycle to effectively regulate AI-LAWS.

Abstract: Military weapon systems and command-and-control infrastructure augmented by
artificial intelligence (AI) have seen rapid development and deployment in
recent years. However, the sociotechnical impacts of AI on combat systems,
military decision-making, and the norms of warfare have been understudied. We
focus on a specific subset of lethal autonomous weapon systems (LAWS) that use
AI for targeting or battlefield decisions. We refer to this subset as
AI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they
introduce novel risks -- including unanticipated escalation, poor reliability
in unfamiliar environments, and erosion of human oversight -- all of which
threaten both military effectiveness and the openness of AI research. These
risks cannot be addressed by high-level policy alone; effective regulation must
be grounded in the technical behavior of AI models. We argue that AI
researchers must be involved throughout the regulatory lifecycle. Thus, we
propose a clear, behavior-based definition of AI-LAWS -- systems that introduce
unique risks through their use of modern AI -- as a foundation for technically
grounded regulation, given that existing frameworks do not distinguish them
from conventional LAWS. Using this definition, we propose several
technically-informed policy directions and invite greater participation from
the AI research community in military AI policy discussions.

</details>


### [842] [Climate Implications of Diffusion-based Generative Visual AI Systems and their Mass Adoption](https://arxiv.org/abs/2505.18892)
*Vanessa Utz,Steve DiPaola*

Main category: cs.CY

TL;DR: The paper explores the climate impact of diffusion-based AI art systems, highlighting their growing energy consumption and discussing solutions and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked energy consumption of text-prompt based diffusion AI art systems and their potential climate implications as they gain mainstream adoption.

Method: Analyzing the growth, usage patterns, and energy consumption of diffusion-based visual AI systems to estimate their contribution to global energy use.

Result: Mass adoption of these AI tools potentially contributes significantly to global energy consumption, with current estimates showing considerable impact.

Conclusion: Solutions and future research areas are proposed, but difficulties remain, such as lack of publicly available data.

Abstract: Climate implications of rapidly developing digital technologies, such as
blockchains and the associated crypto mining and NFT minting, have been well
documented and their massive GPU energy use has been identified as a cause for
concern. However, we postulate that due to their more mainstream consumer
appeal, the GPU use of text-prompt based diffusion AI art systems also requires
thoughtful considerations. Given the recent explosion in the number of highly
sophisticated generative art systems and their rapid adoption by consumers and
creative professionals, the impact of these systems on the climate needs to be
carefully considered. In this work, we report on the growth of diffusion-based
visual AI systems, their patterns of use, growth and the implications on the
climate. Our estimates show that the mass adoption of these tools potentially
contributes considerably to global energy consumption. We end this paper with
our thoughts on solutions and future areas of inquiry as well as associated
difficulties, including the lack of publicly available data.

</details>


### [843] [Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects](https://arxiv.org/abs/2505.18893)
*Reva Schwartz,Rumman Chowdhury,Akash Kundu,Heather Frase,Marzieh Fadaee,Tom David,Gabriella Waters,Afaf Taik,Morgan Briggs,Patrick Hall,Shomik Jain,Kyra Yee,Spencer Thomas,Sundeep Bhandari,Lee Wan Sie,Qinghua Lu,Matthew Holmes,Theodora Skeadas*

Main category: cs.CY

TL;DR: 当前AI评估方法主要关注技术栈内的因素，难以应对现实世界中人类和社会因素的影响。尽管AI能力评估可以捕捉到直接效果，但其长期影响和间接效果需要新的评估方式。本文提出要扩展静态、单回合的评估方法，结合实际使用场景来全面评估AI的间接和长期影响，并建议构建新生态系统以支持这种评估。


<details>
  <summary>Details</summary>
Motivation: 传统的AI评估方法局限于技术层面，无法充分探索、导航和解决AI在教育、金融、医疗和就业等领域实际部署中的人类和社会因素。

Method: 描述需要数据和方法来促进情境感知，并使对AI次要效应的下游解释和决策成为可能，同时推荐构建一个新生态系统的要求。

Result: 提出了评估AI间接和次要效应的需求，以及实现这一目标的方法论框架。

Conclusion: 为了全面评估AI的间接和长期影响，必须超越静态评估方法，结合实际使用场景，并构建一个新的评估生态系统。

Abstract: Conventional AI evaluation approaches concentrated within the AI stack
exhibit systemic limitations for exploring, navigating and resolving the human
and societal factors that play out in real world deployment such as in
education, finance, healthcare, and employment sectors. AI capability
evaluations can capture detail about first-order effects, such as whether
immediate system outputs are accurate, or contain toxic, biased or
stereotypical content, but AI's second-order effects, i.e. any long-term
outcomes and consequences that may result from AI use in the real world, have
become a significant area of interest as the technology becomes embedded in our
daily lives. These secondary effects can include shifts in user behavior,
societal, cultural and economic ramifications, workforce transformations, and
long-term downstream impacts that may result from a broad and growing set of
risks. This position paper argues that measuring the indirect and secondary
effects of AI will require expansion beyond static, single-turn approaches
conducted in silico to include testing paradigms that can capture what actually
materializes when people use AI technology in context. Specifically, we
describe the need for data and methods that can facilitate contextual awareness
and enable downstream interpretation and decision making about AI's secondary
effects, and recommend requirements for a new ecosystem.

</details>


### [844] [EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding](https://arxiv.org/abs/2505.19558)
*Zhaowei Zhang,Minghua Yi,Mengmeng Wang,Fengshuo Bai,Zilong Zheng,Yipeng Kang,Yaodong Yang*

Main category: cs.CY

TL;DR: The paper introduces EuroCon, a benchmark for evaluating LLMs' ability to reach political consensus using high-quality deliberation records from the European Parliament. Experimental results show that even advanced models struggle with complex tasks like passing resolutions by a two-thirds majority and addressing security issues.


<details>
  <summary>Details</summary>
Motivation: To address the under-studied capabilities of frontier AI systems like LLMs in achieving political consensus, which is crucial for effective social governance.

Method: EuroCon incorporates four factors (specific political issues, political goals, participating parties, and power structures) to build each simulated parliament setting and includes an evaluation framework to simulate real voting outcomes in different parliament settings.

Result: State-of-the-art models remain undersatisfied with complex tasks such as passing resolutions by a two-thirds majority and addressing security issues, but reveal common strategies used by LLMs to find consensus under different power structures.

Conclusion: EuroCon shows promise as an effective platform for studying LLMs' ability to find political consensus.

Abstract: Achieving political consensus is crucial yet challenging for the effective
functioning of social governance. However, although frontier AI systems
represented by large language models (LLMs) have developed rapidly in recent
years, their capabilities on this scope are still understudied. In this paper,
we introduce EuroCon, a novel benchmark constructed from 2,225 high-quality
deliberation records of the European Parliament over 13 years, ranging from
2009 to 2022, to evaluate the ability of LLMs to reach political consensus
among divergent party positions across diverse parliament settings.
Specifically, EuroCon incorporates four factors to build each simulated
parliament setting: specific political issues, political goals, participating
parties, and power structures based on seat distribution. We also develop an
evaluation framework for EuroCon to simulate real voting outcomes in different
parliament settings, assessing whether LLM-generated resolutions meet
predefined political goals. Our experimental results demonstrate that even
state-of-the-art models remain undersatisfied with complex tasks like passing
resolutions by a two-thirds majority and addressing security issues, while
revealing some common strategies LLMs use to find consensus under different
power structures, such as prioritizing the stance of the dominant party,
highlighting EuroCon's promise as an effective platform for studying LLMs'
ability to find political consensus.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [845] [Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights](https://arxiv.org/abs/2505.18385)
*Jeba Rezwana,Corey Ford*

Main category: cs.HC

TL;DR: Effective communication between AI and humans is crucial for successful human-AI co-creation. This paper presents the initial design of FAICO, a framework developed through a systematic review of 107 papers that offers preliminary guidelines for designing human-centered AI communication. A preliminary study with focus groups reveals preferences for a human-AI feedback loop and context-driven understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to address the lack of effective communication in current co-creative AI systems, which limits their potential for collaboration. The authors aim to develop a framework that can improve human-AI communication.

Method: The method involved conducting a systematic review of 107 full-length papers to develop the Framework for AI Communication (FAICO). Additionally, a preliminary study was conducted with two focus groups consisting of skilled individuals in AI, HCI, and design to gather feedback on the framework.

Result: The findings indicate a preference for a human-AI feedback loop over linear communication and highlight the importance of context in promoting mutual understanding. Participants provided valuable insights into refining the framework.

Conclusion: Based on the insights from the focus groups, the authors propose actionable strategies for applying FAICO in practice and outline future directions. This marks the first step toward creating comprehensive guidelines for designing effective human-centered AI communication in co-creation.

Abstract: Effective communication between AI and humans is essential for successful
human-AI co-creation. However, many current co-creative AI systems lack
effective communication, which limits their potential for collaboration. This
paper presents the initial design of the Framework for AI Communication (FAICO)
for co-creative AI, developed through a systematic review of 107 full-length
papers. FAICO presents key aspects of AI communication and their impact on user
experience, offering preliminary guidelines for designing human-centered AI
communication. To improve the framework, we conducted a preliminary study with
two focus groups involving skilled individuals in AI, HCI, and design. These
sessions sought to understand participants' preferences for AI communication,
gather their perceptions of the framework, collect feedback for refinement, and
explore its use in co-creative domains like collaborative writing and design.
Our findings reveal a preference for a human-AI feedback loop over linear
communication and emphasize the importance of context in fostering mutual
understanding. Based on these insights, we propose actionable strategies for
applying FAICO in practice and future directions, marking the first step toward
developing comprehensive guidelines for designing effective human-centered AI
communication in co-creation.

</details>


### [846] [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)
*Ugur Kursuncu,Trilok Padhi,Gaurav Sinha,Abdulkadir Erol,Jaya Krishna Mandivarapu,Christopher R. Larrison*

Main category: cs.HC

TL;DR: The study evaluates the use of LLMs (GPT and Llama) in anxiety support, using posts from r/Anxiety subreddit for prompting and fine-tuning. It uses a mixed-method evaluation framework with criteria including linguistic quality, safety/trustworthiness, and supportiveness. Fine-tuning improved linguistic quality but increased toxicity and bias, while GPT was more supportive overall.


<details>
  <summary>Details</summary>
Motivation: The growing demand for accessible mental health support, compounded by workforce shortages and logistical barriers, has led to increased interest in utilizing Large Language Models (LLMs) for scalable and real-time assistance in sensitive domains such as anxiety support.

Method: The study used real user-generated posts from the r/Anxiety subreddit for both prompting and fine-tuning LLMs (GPT and Llama). A mixed-method evaluation framework incorporating three main categories of criteria: linguistic quality, safety and trustworthiness, and supportiveness was utilized.

Result: Fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic quality but increased toxicity and bias, and diminished emotional responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more supportive overall.

Conclusion: The findings highlight the risks of fine-tuning LLMs on unprocessed social media content without mitigation strategies.

Abstract: The growing demand for accessible mental health support, compounded by
workforce shortages and logistical barriers, has led to increased interest in
utilizing Large Language Models (LLMs) for scalable and real-time assistance.
However, their use in sensitive domains such as anxiety support remains
underexamined. This study presents a systematic evaluation of LLMs (GPT and
Llama) for their potential utility in anxiety support by using real
user-generated posts from the r/Anxiety subreddit for both prompting and
fine-tuning. Our approach utilizes a mixed-method evaluation framework
incorporating three main categories of criteria: (i) linguistic quality, (ii)
safety and trustworthiness, and (iii) supportiveness. Results show that
fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic
quality but increased toxicity and bias, and diminished emotional
responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more
supportive overall. Our findings highlight the risks of fine-tuning LLMs on
unprocessed social media content without mitigation strategies.

</details>


### [847] [It's Not Just Labeling" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features](https://arxiv.org/abs/2505.19419)
*Baichuan Li,Larry Powell,Tracy Hammond*

Main category: cs.HC

TL;DR: A sketch-based annotation approach supported by LLMs is introduced to improve the quality of training data in machine learning applications.


<details>
  <summary>Details</summary>
Motivation: Accurate image labeling often relies on time-consuming, expert-driven methods with limited feedback, which can be a bottleneck for machine learning applications.

Method: The research uses a synthetic dataset to examine the relationship between sketch recognition features and LLM feedback metrics. It also explores the effects of prompting strategies and sketch variations on feedback quality.

Result: The study contributes to improving the reliability and interpretability of LLM-assisted labeling, as well as simplifying annotation for non-experts.

Conclusion: The sketch-based virtual assistant enhances scalability, accessibility, and explainability of LLM-driven labeling tools.

Abstract: The quality of training data is critical to the performance of machine
learning applications in domains like transportation, healthcare, and robotics.
Accurate image labeling, however, often relies on time-consuming, expert-driven
methods with limited feedback. This research introduces a sketch-based
annotation approach supported by large language models (LLMs) to reduce
technical barriers and enhance accessibility. Using a synthetic dataset, we
examine how sketch recognition features relate to LLM feedback metrics, aiming
to improve the reliability and interpretability of LLM-assisted labeling. We
also explore how prompting strategies and sketch variations influence feedback
quality. Our main contribution is a sketch-based virtual assistant that
simplifies annotation for non-experts and advances LLM-driven labeling tools in
terms of scalability, accessibility, and explainability.

</details>


### [848] [Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems](https://arxiv.org/abs/2505.19441)
*Jing Nathan Yan,Junxiong Wang,Jeffrey M. Rzeszotarski,Allison Koenecke*

Main category: cs.HC

TL;DR: The paper explores how industry practitioners perceive and incorporate changing fairness standards in recommendation systems through interviews, highlighting preferences for multi-dimensional debiasing and intuitive metrics, as well as the challenges of balancing fairness with workplace constraints.


<details>
  <summary>Details</summary>
Motivation: The rapid proliferation of recommender systems necessitates robust fairness practices to address inherent biases, but assessing fairness is challenging due to constantly evolving metrics and best practices.

Method: Semi-structured interviews with 11 practitioners from technical teams across a range of large technology companies were conducted to investigate industry implementations of fairness in recommendation system products.

Result: Findings show a preference for multi-dimensional debiasing over traditional demographic methods, reliance on intuitive rather than academic metrics, and difficulties in balancing fairness with both individual and organizational workplace constraints.

Conclusion: Actionable recommendations are offered for the recommender system community and algorithmic fairness practitioners, emphasizing the need to continually refine fairness practices.

Abstract: The rapid proliferation of recommender systems necessitates robust fairness
practices to address inherent biases. Assessing fairness, though, is
challenging due to constantly evolving metrics and best practices. This paper
analyzes how industry practitioners perceive and incorporate these changing
fairness standards in their workflows. Through semi-structured interviews with
11 practitioners from technical teams across a range of large technology
companies, we investigate industry implementations of fairness in
recommendation system products. We focus on current debiasing practices,
applied metrics, collaborative strategies, and integrating academic research
into practice. Findings show a preference for multi-dimensional debiasing over
traditional demographic methods, and a reliance on intuitive rather than
academic metrics. This study also highlights the difficulties in balancing
fairness with both the practitioner's individual (bottom-up) roles and
organizational (top-down) workplace constraints, including the interplay with
legal and compliance experts. Finally, we offer actionable recommendations for
the recommender system community and algorithmic fairness practitioners,
underlining the need to refine fairness practices continually.

</details>


### [849] [On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction](https://arxiv.org/abs/2505.20068)
*Qingyu Liang,Jaime Banks*

Main category: cs.HC

TL;DR: A survey was conducted to explore the perception of shared understanding (PSU) in human-AI interactions, identifying eight dimensions that comprise PSU.


<details>
  <summary>Details</summary>
Motivation: Shared understanding is crucial for effective communication and performance in human-human interactions. As AI becomes more integrated into human contexts, it's important to study PSU in human-AI interactions as this area has been underexplored.

Method: An online survey was used to gather user reflections on their interactions with a large language model, focusing on when the model's understanding was perceived as similar or different from the participant's.

Result: Eight dimensions were identified through inductive thematic analysis that make up PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.

Conclusion: This study enhances our understanding of PSU in human-AI interactions by identifying key dimensions that contribute to it.

Abstract: Shared understanding plays a key role in the effective communication in and
performance of human-human interactions. With the increasingly common
integration of AI into human contexts, the future of personal and workplace
interactions will likely see human-AI interaction (HAII) in which the
perception of shared understanding is important. Existing literature has
addressed the processes and effects of PSU in human-human interactions, but the
construal remains underexplored in HAII. To better understand PSU in HAII, we
conducted an online survey to collect user reflections on interactions with a
large language model when it sunderstanding of a situation was thought to be
similar to or different from the participant's. Through inductive thematic
analysis, we identified eight dimensions comprising PSU in human-AI
interactions: Fluency, aligned operation, fluidity, outcome satisfaction,
contextual awareness, lack of humanlike abilities, computational limits, and
suspicion.

</details>


### [850] [Explanation User Interfaces: A Systematic Literature Review](https://arxiv.org/abs/2505.20085)
*Eleonora Cappuccio,Andrea Esposito,Francesco Greco,Giuseppe Desolda,Rosa Lanzilotti,Salvatore Rinzivillo*

Main category: cs.HC

TL;DR: This paper conducts a systematic literature review on Explanation User Interfaces (XUIs) and presents the HERMES framework to guide practitioners and academics in designing and evaluating XUIs.


<details>
  <summary>Details</summary>
Motivation: Artificial Intelligence models are often unintelligible, leading to the use of eXplainable Artificial Intelligence (XAI) techniques. However, presenting explanations to users effectively is often overlooked, making AI systems less useful to end-users.

Method: The authors performed a Systematic Literature Review on Explanation User Interfaces (XUIs) to understand the solutions and design guidelines used in academic literature. They also introduced the HERMES framework.

Result: The review provided insights into current practices for presenting explanations to users and identified gaps that need addressing. The HERMES framework was presented as a tool to guide the design and evaluation of XUIs.

Conclusion: Effective presentation of explanations to users is crucial for creating useful AI systems. The HERMES framework aims to improve the contribution and real-world impact of XUIs.

Abstract: Artificial Intelligence (AI) is one of the major technological advancements
of this century, bearing incredible potential for users through AI-powered
applications and tools in numerous domains. Being often black-box (i.e., its
decision-making process is unintelligible), developers typically resort to
eXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour
of AI models to produce systems that are transparent, fair, reliable, and
trustworthy. However, presenting explanations to the user is not trivial and is
often left as a secondary aspect of the system's design process, leading to AI
systems that are not useful to end-users. This paper presents a Systematic
Literature Review on Explanation User Interfaces (XUIs) to gain a deeper
understanding of the solutions and design guidelines employed in the academic
literature to effectively present explanations to users. To improve the
contribution and real-world impact of this survey, we also present a framework
for Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide
practitioners and academics in the design and evaluation of XUIs.

</details>
