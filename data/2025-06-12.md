<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.CR](#cs.CR) [Total: 19]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [eess.SP](#eess.SP) [Total: 5]
- [stat.ML](#stat.ML) [Total: 7]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.CL](#cs.CL) [Total: 30]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CV](#cs.CV) [Total: 47]
- [eess.IV](#eess.IV) [Total: 3]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.GR](#cs.GR) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.AI

TL;DR: The paper introduces AIM, a robot-gated IIL algorithm that reduces cognitive demands on human supervisors by learning when to request human demonstrations. It uses a proxy Q-function to mimic human intervention rules and shows significant improvements over baselines in reducing expert monitoring efforts and enhancing learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Interactive Imitation Learning methods place high cognitive demands on human supervisors due to frequent intervention requests.

Method: AIM is proposed which utilizes a proxy Q-function to mimic the human intervention rule. The proxy Q-function assigns high values when the agent deviates from the expert and decreases these as proficiency increases, allowing adaptive intervention requests.

Result: Experiments show AIM significantly reduces expert monitoring efforts in both continuous and discrete control tasks, with a 40% improvement over Thrifty-DAgger in human take-over cost and learning efficiency. It also identifies safety-critical states effectively.

Conclusion: AIM reduces the burden on human supervisors, improves learning efficiency, and collects higher-quality demonstrations while needing less expert data and interaction.

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [2] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus,A. Lawsen*

Main category: cs.AI

TL;DR: Shojaee et al. reported that LRMs show 'accuracy collapse' on complex planning puzzles, but this paper argues their findings are due to experimental design flaws rather than reasoning failures. Key issues include model output limits being exceeded, an evaluation framework conflating reasoning and practical constraints, and benchmarks including unsolvable problems. When controlling for these factors, models demonstrate high accuracy on previously failed tasks.


<details>
  <summary>Details</summary>
Motivation: To re-evaluate the claim by Shojaee et al. that LRMs experience 'accuracy collapse' on complex planning puzzles, and determine whether this is due to actual reasoning failures or flaws in the experimental design.

Method: Analyze the experimental setup of Shojaee et al.'s study, identifying three critical issues: model output token limits being exceeded, evaluation framework misclassifications, and inclusion of unsolvable benchmark problems. Conduct preliminary experiments using generating functions instead of exhaustive move lists to assess model accuracy on Tower of Hanoi instances.

Result: Preliminary experiments indicate high accuracy on Tower of Hanoi instances when controlling for experimental artifacts, suggesting that the original findings were largely due to design limitations rather than fundamental reasoning failures.

Conclusion: Careful experimental design is crucial when evaluating AI reasoning capabilities, as flawed designs can lead to incorrect conclusions about model performance.

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [3] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: The paper introduces Ming-Omni, a unified multimodal model that processes images, text, audio, and video with dedicated encoders and routers. It supports speech and image generation via advanced decoders and performs diverse tasks without needing separate models or fine-tuning. Experimental results show its effectiveness across modalities, being the first open-source model on par with GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To create a single, efficient model capable of handling multiple modalities (images, text, audio, video) for both perception and generation tasks without requiring separate models or task-specific fine-tuning.

Method: Ming-Omni uses dedicated encoders to extract tokens from different modalities, processed by Ling, an MoE architecture with modality-specific routers. It integrates an advanced audio decoder for speech generation and Ming-Lite-Uni for image generation, enabling context-aware chatting, text-to-speech conversion, and versatile image editing.

Result: Ming-Omni effectively handles multimodal inputs and generation tasks, matching the capabilities of GPT-4o in modality support. The model's code and weights are released openly to promote further research.

Conclusion: Ming-Omni is a powerful unified model for multimodal perception and generation, being the first open-source model comparable to GPT-4o in modality support.

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [4] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/abs/2506.09390)
*Kehan Zheng,Jinfeng Zhou,Hongning Wang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在战略决策中应用日益广泛，但其行为与人类一样常偏离完全理性。本研究通过行为博弈论实验范式，比较了LLMs和人类在‘石头剪刀布’与‘囚徒困境’中的表现，发现LLMs虽能重现人类熟悉的启发式策略，但规则应用更僵化且对游戏环境动态变化的敏感性较低。分析表明，当前LLMs仅部分具备类似人类的有限理性，强调需要改进训练方法以促进灵活对手建模和更强的情境意识。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被广泛应用于战略决策场景，但其行为可能并非完全理性。因此，研究者希望了解LLMs在战略游戏中是否表现出类似于人类的有限理性行为，以及这些行为的具体特点是什么。

Method: 研究者使用改编自行为博弈论研究的实验范式，将LLMs置于与人类相同的实验条件下，通过‘石头剪刀布’和‘囚徒困境’两个经典战略游戏，评估LLMs的行为是否展现出类似于人类的有限理性特征。

Result: LLMs能够重现人类熟悉的启发式策略，如基于结果的战略切换和在可能产生未来互动时增加合作。然而，LLMs应用这些规则更为僵化，并对游戏环境的动态变化表现出较弱的敏感性。此外，模型级别的分析揭示了战略行为中的独特架构特征，甚至推理模型在适应性情况下有时也难以找到有效策略。

Conclusion: 当前LLMs仅部分具备类似人类的有限理性，这表明需要开发新的训练方法，以鼓励更灵活的对手建模和更强的情境意识。

Abstract: Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [5] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: Recent improvements in LLMs have driven research towards fully autonomous AI agents. However, this paper questions the approach due to issues with reliability, transparency, and understanding human needs. Instead, it proposes LLM-based Human-Agent Systems (LLM-HAS) where AI collaborates with humans for better trustworthiness and adaptability. Examples from healthcare, finance, and software development illustrate how human-AI collaboration outperforms solo AI in complex tasks. The paper also addresses challenges in building collaborative systems and provides solutions. It argues that AI progress should be measured by how well it works with humans, not by its independence. The most promising AI future lies in enhancing human capabilities through partnership.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to question the current trend of focusing on building fully autonomous AI agents due to concerns over their reliability, transparency, and ability to understand human requirements. It seeks to explore a different path where AI works alongside humans rather than replacing them.

Method: The method proposed in this paper is the creation of LLM-based Human-Agent Systems (LLM-HAS), which involve keeping humans in the loop to provide guidance, answer questions, and maintain control over AI systems. This approach aims to make AI systems more trustworthy and adaptable.

Result: The result of implementing LLM-HAS shows that human-AI teamwork can handle complex tasks better than AI working alone, as demonstrated through examples in healthcare, finance, and software development. Additionally, the paper discusses the challenges and offers practical solutions for building these collaborative systems.

Conclusion: The conclusion drawn from this paper is that the progress of AI should not be measured by its independence but by its ability to work effectively with humans. The most promising future for AI lies in enhancing human capabilities through meaningful partnerships rather than replacing human roles.

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [6] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)
*Jaesik Yoon,Hyeonseo Cho,Yoshua Bengio,Sungjin Ahn*

Main category: cs.AI

TL;DR: Fast-MCTD is a more efficient variant of MCTD that significantly improves speed and scalability while maintaining or improving planning performance.


<details>
  <summary>Details</summary>
Motivation: To address the substantial computational overhead incurred by MCTD due to the sequential nature of tree search and the cost of iterative denoising.

Method: Propose Fast-MCTD, which integrates Parallel MCTD (enabling parallel rollouts via delayed tree updates and redundancy-aware selection) and Sparse MCTD (reducing rollout length through trajectory coarsening).

Result: Experiments show that Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or improving planning performance. It even outperforms Diffuser in inference speed on some tasks.

Conclusion: Fast-MCTD is positioned as a practical and scalable solution for diffusion-based inference-time reasoning.

Abstract: Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [7] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu,Jiajun Chai,Sicheng Li,Yuqian Fu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.AI

TL;DR: The paper presents DipLLM, a fine-tuned LLM-based agent for the game Diplomacy that learns equilibrium policies and surpasses state-of-the-art performance using significantly less data.


<details>
  <summary>Details</summary>
Motivation: Diplomacy is a complex multiplayer game requiring both cooperation and competition, which poses challenges for AI systems. Traditional methods rely on equilibrium search demanding substantial computational resources, while applying LLMs to Diplomacy is challenging due to exponential action combinations and intricate strategic interactions.

Method: Propose DipLLM, a fine-tuned LLM-based agent that learns equilibrium policies for Diplomacy. It employs an autoregressive factorization framework to simplify multi-unit action assignment into unit-level decisions and defines an equilibrium policy within this framework as the learning objective.

Result: DipLLM achieves better performance than the Cicero model while using only 1.5% of the data required by Cicero.

Conclusion: The study demonstrates the potential of fine-tuned LLMs in handling complex strategic decision-making in multiplayer games like Diplomacy.

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [8] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/abs/2506.09656)
*Wei Zeng,Hengshu Zhu,Chuan Qin,Han Wu,Yihang Cheng,Sirui Zhang,Xiaowei Jin,Yinuo Shen,Zhenxing Wang,Feimin Zhong,Hui Xiong*

Main category: cs.AI

TL;DR: The paper reviews value alignment in agent systems, covering principles, application scenarios, evaluation methods, and multi-agent coordination within the context of Agentic AI and large language models.


<details>
  <summary>Details</summary>
Motivation: With the evolution of AI paradigms into the Agentic AI stage, there is a need to address the increasing situational and systemic risks associated with diverse and complex applications of Large Language Models (LLMs). This drives the attention towards ensuring value alignment for AI agents.

Method: The paper adopts a comprehensive review approach, organizing value principles hierarchically (macro, meso, micro levels), categorizing agent system application scenarios from general-to-specific, examining datasets and methods for value alignment evaluation, and exploring value coordination among multiple agents.

Result: The review provides insights into how value alignment can be achieved in agent systems, highlighting the integration of advancements in AI with social governance demands. It also identifies datasets and methods relevant to value alignment evaluation.

Conclusion: The paper concludes by proposing potential research directions in the field of value alignment for AI agents, emphasizing the importance of this area in the context of Agentic AI and complex multi-agent systems.

Abstract: The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [9] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: IFG is a method to increase sample diversity of LLMs by factorising the sampling process into two stages, improving performance in various tasks while maintaining generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for increasing diversity in LLMs often only operate at the token-level, leading to poor exploration on reasoning problems and repetitive conversational agents.

Method: Propose Intent Factored Generation (IFG), which factorises the sampling process into two stages: first sample a semantically dense intent, then sample the final response conditioning on both the original prompt and the intent from the first stage. Also, prompting the model to explicitly state its intent for each step of the chain-of-thought before generating the step.

Result: Improved pass@k and Reinforcement Learning from Verifier Feedback on maths and code tasks; increased conversational diversity without sacrificing reward for instruction-tuning; higher diversity while maintaining generation quality on a general language modelling task.

Conclusion: Presented a simple method of increasing the sample diversity of LLMs while maintaining performance, which can be easily integrated into many algorithms.

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [10] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/abs/2506.09977)
*Stylianos Loukas Vasileiou,Antonio Rago,Maria Vanina Martinez,William Yeoh*

Main category: cs.AI

TL;DR: 人们在面对冲突信息时，更倾向于基于解释的信念修订，而非经典理论所描述的方式。此研究通过三个全面的用户实验展示了这一现象，并对AI系统的设计有重要启示。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何根据新信息修订信念，对于开发能有效模拟并与人类推理一致的AI系统至关重要。然而，认知心理学的经验数据表明，人们在面对冲突信息时可能遵循不同的模式。

Method: 进行了三项全面的用户研究，探讨了人们在面对不一致性时，无论是提供解释还是自行推导解释的情况下，如何修订其信念。

Result: 研究表明，人们始终偏好基于解释的修订，这些修订可能导致对其信念系统的改变，而这些改变不一定被经典信念变更理论所涵盖。

Conclusion: 研究结果表明，为了更好地与人类认知过程一致，旨在模拟人类推理或与人类互动的AI系统应包含基于解释的、可能是非最小化的信念修订操作符。

Abstract: Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [11] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: The paper presents V-JEPA 2, a self-supervised model pretrained on large-scale internet video data that excels in motion understanding and human action anticipation. After alignment with a large language model, it achieves state-of-the-art results in video question-answering tasks. A variant, V-JEPA 2-AC, enables zero-shot robotic planning using minimal unlabeled robot video data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of AI learning to understand the world and act through observation, the paper explores combining massive internet video data with limited interaction data (robot trajectories) for developing models capable of understanding, predicting, and planning in physical environments.

Method: The method involves pretraining an action-free joint-embedding-predictive architecture (V-JEPA 2) on over 1 million hours of internet video. This model is then aligned with a large language model for video question-answering tasks. Additionally, a latent action-conditioned world model (V-JEPA 2-AC) is post-trained using less than 62 hours of unlabeled robot videos for robotic planning tasks.

Result: V-JEPA 2 shows strong performance in motion understanding and human action anticipation, surpassing task-specific models. After alignment with a language model, it achieves state-of-the-art results in video question-answering tasks. V-JEPA 2-AC successfully enables zero-shot robotic planning without any task-specific training or reward.

Conclusion: This work demonstrates that self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of effective planning in physical environments.

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [12] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 大型多模态模型（LMMs）通常依赖于上下文学习（ICL）来以最少的监督执行新任务。然而，ICL性能在较小的LMMs中表现不稳定。本文提出了一种元学习方法，通过从任务相关的图像特征中提取固定的软提示，并可以在测试时使用少量示例进行适应，从而为LMMs引入少样本能力。实验表明该方法在VL-ICL Bench上始终优于ICL和相关提示调整方法。


<details>
  <summary>Details</summary>
Motivation: LMMs在执行新任务时依赖于ICL，但其性能不稳定，尤其是在较小的LMMs中。这可能是因为LMM被图像嵌入中与下游任务无关的额外信息所淹没。

Method: 提出了一种元学习方法，使用从任务相关图像特征中提取的固定软提示，并可在测试时使用少量示例进行适应。还引入了一个注意力映射模块，可以轻松集成到流行的LLaVA v1.5架构中，并与软提示联合学习，使LMMs在低数据条件下通过少量梯度步骤实现任务适应。

Result: 在VL-ICL Bench上的评估显示，该方法始终优于ICL和相关提示调整方法，即使在图像扰动下也能提高任务归纳和推理能力。

Conclusion: 提出的元学习方法有效提高了LMMs在少样本条件下的任务适应能力，特别是在视觉问答任务中表现出色。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain,Ehsan Saghapour,Kevin Song,Jake Y. Chen*

Main category: cs.LG

TL;DR: The paper introduces LlamaAffinity, a new model using an open-source Llama 3 backbone and OAS data to predict antibody-antigen binding affinity with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve the prediction of antibody-antigen binding affinity using AI methods, reducing time and cost compared to traditional experimental methods.

Method: Leveraging a large language model (LLM) from Llama 3 and antibody sequence data from OAS database to create LlamaAffinity model for predicting binding affinity.

Result: Achieved accuracy of 0.9640, F1-score of 0.9643, precision of 0.9702, recall of 0.9586, and AUC-ROC of 0.9936; also demonstrated higher computational efficiency with a five-fold average cumulative training time of 0.46 hours.

Conclusion: LlamaAffinity shows significant improvement over existing SOTA methods in both performance metrics and computational efficiency.

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [14] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen,Mingxi Zou,Zhuo Wang,Qifan Wang,Dongning Sun,Chi Zhang,Zenglin Xu*

Main category: cs.LG

TL;DR: Financial decision-making is challenging for language models. This paper proposes FinHEAR, a multi-agent framework that incorporates expert-guided retrieval, confidence-adjusted position sizing, and outcome-based refinement to improve interpretability and robustness in financial tasks.


<details>
  <summary>Details</summary>
Motivation: Current large language models fail to capture behavioral patterns central to human financial decisions such as expert reliance under information asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment.

Method: FinHEAR orchestrates specialized LLM-based agents to analyze historical trends, interpret current events, and retrieve expert-informed precedents within an event-centric pipeline. It is grounded in behavioral economics and incorporates expert-guided retrieval, confidence-adjusted position sizing, and outcome-based refinement.

Result: Empirical results on curated financial datasets show that FinHEAR consistently outperforms strong baselines across trend prediction and trading tasks, achieving higher accuracy and better risk-adjusted returns.

Conclusion: FinHEAR presents a promising approach to enhance the capabilities of language models in financial decision-making.

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [15] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang,Liang Wu,Yanjie Fu*

Main category: cs.LG

TL;DR: 本研究提出了一种基于奖励的微调方法PageLLM，利用用户反馈作为监督信号优化整页推荐系统，通过结合页面级和项目级奖励机制，在公开和工业数据集上验证了其有效性，并在在线A/B测试中实现了GMV 0.44%的增长。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型（LLMs）在生成连贯且上下文相关的内容方面表现出色，但在如整页优化（WPO）等复杂任务上的微调面临挑战，例如需要大量人工标注数据以解决幻觉和模型不稳定等问题，这在与数百万项内容交互的大规模系统中尤其昂贵。

Method: 提出了PageLLM方法，使用用户反馈作为监督信号，设计了结合页面级和项目级奖励的混合粒度奖励机制，其中页面级奖励评估整体质量和连贯性，项目级奖励关注关键推荐的准确性和相关性，从而优化整体呈现和个体组件。

Result: 在公开和工业数据集上的实验表明，PageLLM优于基线方法，并在包含超过1000万用户的在线A/B测试中实现了GMV 0.44%的增长，证明了其实际应用价值。

Conclusion: PageLLM通过利用用户反馈和混合粒度奖励机制，有效解决了LLMs在WPO任务中的微调挑战，提升了用户体验和商业价值，适用于大规模推荐系统。

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [16] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang,Haoyue Bai,Nanxu Gong,Wangyang Ying,Sixun Dong,Xiquan Cui,Yanjie Fu*

Main category: cs.LG

TL;DR: 通过结合LLMs的符号生成能力和ML的梯度优化，提出了一种新的特征转换框架，该框架在下游任务性能上提高了5%，同时将错误案例减少了近一半。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在特征转换任务中面临稳定性和有效性两大挑战，现有的传统机器学习方法和大语言模型各有所短，无法同时解决这两个问题。

Method: 提出的框架包含四个步骤：1) 利用教师LLM生成高质量样本；2) 在潜在空间中搜索潜在的优质嵌入；3) 从教师LLM中提取知识以进行特征转换；4) 结合ML和学生LLM的概率以实现有效且稳定的生成。

Result: 实验结果表明，该团队策略在多个数据集上的下游任务性能提高了5%，并且错误案例减少了近一半，同时展示了LLM理解原始数据的能力。

Conclusion: 所提出的团队框架不仅提高了特征转换的效果和稳定性，还展示了LLM对原始数据的理解能力，证明了其效率和鲁棒性。

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [17] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard,Giulia Mezzadri,Patricia Reynaud-Bouret,Etienne Tanré*

Main category: cs.LG

TL;DR: 提出了一种结合学习机制的生物合理性脉冲神经网络(SNN)模型，将认知模型与生物模型相结合，并通过多变量霍克斯过程建模神经元活动。研究发现DDM可由脉冲泊松神经元近似，特定的DDM可通过霍克斯神经网络推导，从而为理解神经活动和行为的关系提供了重要进展。


<details>
  <summary>Details</summary>
Motivation: 当前的认知模型如DDM和Poisson counter model缺乏学习机制，仅适用于参与者事先了解类别的任务。为了弥合认知和生物模型之间的差距，需要一种新的模型来整合学习机制并解释神经活动与行为的关系。

Method: 1. 提出了一种具有学习机制的SNN模型，其神经元活动由多变量Hawkes过程建模。
2. 证明了DDM与Poisson counter model之间的耦合关系，表明两者在分类和反应时间上具有相似性。
3. 推导出特定带有相关噪声的DDM可从受局部学习规则控制的Hawkes网络中得出。
4. 设计了一个在线分类任务以评估模型预测能力。

Result: 1. DDM可以被脉冲Poisson神经元近似。
2. 特定的DDM可以通过Hawkes网络中的脉冲神经元推导。
3. SNN模型成功地集成了学习机制，并在在线分类任务中表现良好。

Conclusion: 该研究通过引入具有学习机制的SNN模型，显著推动了将生物相关的神经机制整合到认知模型中的进程，加深了对神经活动与行为之间关系的理解。

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [18] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan,Nuria Gomez Blas*

Main category: cs.LG

TL;DR: This paper presents an enhanced asynchronous AdaBoost framework for federated learning (FL) that improves communication efficiency, scalability, convergence, and robustness in five distinct domains.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of synchronization frequency and communication overhead in federated learning while preserving or improving model accuracy.

Method: The proposed algorithm incorporates adaptive communication scheduling and delayed weight compensation mechanisms.

Result: Empirical results show training time reductions on the order of 20-35% and communication overhead reductions of 30-40% compared to baseline AdaBoost, with convergence achieved in significantly fewer boosting rounds.

Conclusion: The enhanced AdaBoost exhibits markedly improved efficiency and robustness across diverse FL scenarios, suggesting broad applicability of the approach.

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [19] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson,Igor Oliveira,Amenah Al-Najafi,Fode Zhang,Hon Keung Tony Ng*

Main category: cs.LG

TL;DR: 本研究提出了一种基于耦合自由能的变分推理优化框架，通过引入耦合概率和分布改进了模型的准确性和鲁棒性。应用到CVAE中，相比VAE在CelebA图像重建上提高了3%。


<details>
  <summary>Details</summary>
Motivation: 现有的变分推理技术没有充分考虑耦合指数族的弯曲几何特性，这限制了对重尾分布（如广义帕累托分布和学生t分布）的有效建模能力。

Method: 提出基于耦合自由能的优化框架，利用耦合证据下界（ELBO）来扩展变分推理技术，涵盖耦合指数族的弯曲几何特性。同时，设计了耦合变分自编码器（CVAE），通过耦合分布和成本函数进行采样，使重建度量仍为均方平均损失但具有修改后的常数。

Result: 实验表明，在训练5个周期后，使用CVAE重建CelebA图像的Wasserstein-2或Fréchet Inception Distance相较于VAE提高了3%。此外，该方法能够减少训练样本中的异常值数量。

Conclusion: 提出的基于耦合自由能的优化框架可以有效提升变分推理技术对重尾分布的建模能力，特别是在图像重建任务中表现出色，同时增强了模型的准确性和鲁棒性。

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [20] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen,Jiace Zhu,Qi Fan,Yehan Ma,An Zou*

Main category: cs.LG

TL;DR: The paper explores using Large Language Models (LLMs) with a novel framework, Feature Search and Reinforcement (FSR), to generate and optimize CUDA programs for GPUs. This approach not only ensures correct code but also improves performance tailored to GPU architecture.


<details>
  <summary>Details</summary>
Motivation: Generating hardware-specific, architecture-aware, and performance-critical code for massively parallel GPUs is challenging despite the general-purpose code generation capabilities of LLMs.

Method: The authors propose FSR, a framework that optimizes compilation, functional correctness, and runtime performance of CUDA programs through extensive test cases and actual kernel execution latency measurements on target GPUs.

Result: FSR guarantees correctness rates in generated CUDA kernels and achieves up to 179 times faster execution speeds compared to general human-written code for representative AI workloads and computational intensive algorithms.

Conclusion: The combination of LLMs with performance reinforcement shows promise in automating GPU programming for applications requiring specific hardware considerations and high performance.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [21] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang,Hongkang Li,Changlong Shi,Guowei Rong,He Zhao,Dongsheng Wang,Dandan Guo,Meng Wang*

Main category: cs.LG

TL;DR: This paper proposes LwPTV (Layer-wise Pruning Task Vector) to improve the performance of multi-task learning models on out-of-domain tasks while preserving in-domain task abilities.


<details>
  <summary>Details</summary>
Motivation: Current model merging approaches for multi-task learning focus on enhancing performance within in-domain datasets, but often neglect their efficacy on out-of-domain datasets.

Method: The authors propose LwPTV, which builds a saliency score to measure parameter redundancy in task vectors. This method achieves mask vectors for each task and performs layer-wise pruning on the task vectors, only keeping pre-trained model parameters at corresponding layers in merged models.

Result: Extensive experiments show that applying LwPTV leads to significant improvements in out-of-domain performance while maintaining the ability on in-domain tasks.

Conclusion: LwPTV is flexible and can be integrated with most existing model merging methods to enhance their performance on out-of-domain tasks.

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [22] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou,Shunyu Liu,Zengmao Wang,Di Wang,Rong-Cheng Tu,Bo Du,Dacheng Tao*

Main category: cs.LG

TL;DR: Reward models are critical for improving large language models (LLMs). Current reward modeling struggles to identify specific components within a response trajectory that truly correlate with the scores. In this paper, we propose leveraging generation probabilities to establish reward consistency between processes in the response trajectory.


<details>
  <summary>Details</summary>
Motivation: Current reward modeling typically relies on scores of overall responses to learn the outcome rewards for the responses. However, since the response-level scores are coarse-grained supervision signals, the reward model struggles to identify the specific components within a response trajectory that truly correlate with the scores, leading to poor generalization on unseen responses.

Method: The authors propose to leverage generation probabilities to establish reward consistency between processes in the response trajectory, which allows the response-level supervisory signal to propagate across processes, thereby providing additional fine-grained signals for reward learning. They develop an intra-trajectory consistency regularization to enforce that adjacent processes with higher next-token generation probability maintain more consistent rewards.

Result: The proposed regularization improves the performance of the advanced outcome reward model on RewardBench. It also induces better DPO-aligned policies and achieves better best-of-N (BON) inference-time verification results.

Conclusion: Leveraging generation probabilities to establish reward consistency between processes in the response trajectory can improve the performance of reward models, leading to better generalization on unseen responses.

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [23] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: In this paper, researchers explore the relationship between memorization and generalization in large language models by pre-training capacity-limited Transformer models on synthetic tasks. They observe a trade-off where smaller models generalize better but cannot memorize, larger models memorize but do not generalize well, and no model can achieve both when trained jointly.


<details>
  <summary>Details</summary>
Motivation: To understand the interplay between memorization and generalization in large language models (LLMs), which is crucial for their design and deployment.

Method: Pre-training a series of capacity-limited Transformer models from scratch on two synthetic character-level tasks designed to separately probe generalization (via arithmetic extrapolation) and memorization (via factual recall).

Result: A consistent trade-off was observed: small models generalize but fail to memorize; larger models memorize but fail to generalize; and no model succeeds at extrapolation when trained on both tasks jointly.

Conclusion: The study suggests that pre-training may favor one learning mode over the other and provides insights into how model capacity shapes learning behavior, offering implications for the design and deployment of small language models.

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [24] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: 研究人员提出了一种统一的威胁模型，用于评估和比较针对安全调优的大语言模型（LLM）的越狱攻击方法。该模型基于一个在1万亿个令牌上训练的N-gram语言模型，允许对攻击进行非参数化、可解释的评估。通过适应流行攻击以符合此威胁模型，研究发现现代安全调优模型对攻击的成功率低于之前报告，并且基于离散优化的攻击显著优于最近基于LLM的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的越狱攻击方法虽然在原始设置中大多成功，但它们在流畅性和计算努力方面差异很大，缺乏一个统一的标准来比较这些方法的有效性。因此，需要一种系统的方法来评估这些攻击的实际影响和可行性。

Method: 研究人员构建了一个统一的威胁模型，利用一个基于1万亿个令牌的N-gram语言模型来检查给定的越狱攻击是否可能出现在文本分布中。这种模型不依赖于特定的LLM，提供了一种非参数化且本质上可解释的评价方式。随后，他们将流行的攻击方法适配到这个威胁模型中进行基准测试。

Result: 使用该威胁模型进行广泛比较后，研究人员发现现代安全调优模型对攻击的成功率低于之前的报告。此外，基于离散优化的攻击方法明显优于最近基于LLM的攻击方法。有效的攻击往往利用和滥用在真实文本中很少出现的双词组（bigrams）。

Conclusion: 提出的统一威胁模型为全面分析和比较越狱攻击提供了一种有效且可解释的方法，揭示了现有攻击方法的局限性，并为进一步提高大语言模型的安全性提供了方向。

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [25] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés,Daniel Mas Montserrat,Kapal Dev,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 在处理医疗、生物医学、社会经济、金融、调查和多传感器数据等应用中，特征偏移是一个常见问题。本文提出了一种名为Feature Shift Localization Network（FSL-Net）的神经网络，可以快速准确地定位大规模高维数据集中的特征偏移。该网络通过学习数据集的统计特性，能够从未见过的数据集中定位特征偏移，而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，数据源之间存在特征偏移的问题，这可能导致下游分析性能下降。因此，需要一种方法来准确定位这些特征偏移并进行纠正。然而，现有的技术要么不准确，要么无法扩展到大规模和高维数据集。

Method: 提出了一种名为Feature Shift Localization Network (FSL-Net) 的神经网络。该网络通过大量数据集进行训练，学习提取数据集的统计特性，并能对以前未见过的数据集和特征偏移进行定位，而无需重新训练。

Result: 实验结果表明，FSL-Net能够在大型和高维数据集中快速且准确地定位特征偏移。

Conclusion: FSL-Net为解决大规模高维数据集中的特征偏移定位问题提供了一种有效的方法。代码和预训练模型已公开发布。

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [26] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee,Seungjae Shin,Vinnam Kim,Jaeseong You,An Chen*

Main category: cs.LG

TL;DR: In order to deploy large language models on resource-constrained devices, the paper proposes Unified Progressive Quantization (UPQ), a novel framework for INT2 instruction-tuned LLM quantization. UPQ can quantize open-source instruction-tuned LLMs to INT2 without relying on proprietary post-training data, achieving state-of-the-art performances.


<details>
  <summary>Details</summary>
Motivation: The rapid scaling of large language models poses significant challenges for deployment on resource-constrained devices. There is growing interest in extremely low-bit quantization, such as 2-bit, but advancements have been limited to pre-trained LLMs and not extended to instruction-tuned models.

Method: The paper proposes Unified Progressive Quantization (UPQ), a progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2) that unifies block-wise post-training quantization (PTQ) with distillation-based quantization-aware training (Distill-QAT) for INT2 instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned models to INT4 using block-wise PTQ, then applies Distill-QAT to minimize the generalized Jensen-Shannon divergence (JSD) between the two models.

Result: UPQ can quantize open-source instruction-tuned LLMs to INT2 without relying on proprietary post-training data, while achieving state-of-the-art performances on MMLU and IFEval benchmarks.

Conclusion: The authors demonstrate that UPQ can successfully quantize instruction-tuned LLMs to INT2, providing a solution for deploying large language models on resource-constrained devices.

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [27] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres,Pranav Deshpande,Archan Ray,Mattia J. Villani,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: MetaTT is a unified Tensor Train adapter framework for global low-rank fine-tuning of pre-trained transformers. It uses a single shared TT to factorize all transformer sub-modules and significantly compresses the final adapter.


<details>
  <summary>Details</summary>
Motivation: The motivation behind MetaTT is to provide an efficient alternative to LoRA by using a single shared Tensor Train (TT) to factorize all transformer sub-modules, thus reducing the number of added parameters while maintaining or even improving accuracy.

Method: MetaTT uses a single shared Tensor Train (TT) to factorize all transformer sub-modules including query, key, value, projection, and feed-forward layers. It indexes structural axes such as layer and matrix type, and optionally heads and tasks. This method leads to a significantly compressed final adapter compared to LoRA.

Result: In benchmarks comparing MetaTT with LoRA and other recent state-of-the-art methods, MetaTT was observed to lead to the most reduction in parameters while maintaining similar accuracy to LoRA and outperforming other tensor-based methods.

Conclusion: MetaTT provides an effective solution for global low-rank fine-tuning of pre-trained transformers by significantly reducing parameters and maintaining or improving accuracy. It benefits from mature optimization routines and naturally extends to shared adapters across many tasks.

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [28] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM is a family of sensor-language foundation models that enable wearable sensor data understanding with natural language, using a hierarchical caption generation pipeline to create the largest sensor-language dataset and extending multimodal pretraining architectures for superior performance in human activity analysis and healthcare.


<details>
  <summary>Details</summary>
Motivation: Aligning and interpreting sensor data with language is challenging due to the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data.

Method: Introduced a hierarchical caption generation pipeline designed to capture statistical, structural, and semantic information from sensor data, curated the largest sensor-language dataset to date, and extended prominent multimodal pretraining architectures.

Result: Extensive experiments verified the superior performance of SensorLM over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval. It also demonstrated intriguing capabilities including scaling behaviors, label efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

Conclusion: SensorLM enables wearable sensor data understanding with natural language and outperforms current methods in various real-world tasks.

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [29] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma,Feng Wu,Qika Lin,Yucheng Xing,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: The paper introduces CodeBrain, an efficient Electroencephalography (EEG) foundation model that addresses limitations in traditional models by improving representation capacity and capturing multi-scale brain dependencies. It uses a TFDual-Tokenizer for tokenizing temporal and frequency components and an EEGSSM architecture to model dependencies. Experiments on 10 public datasets show its generalizability.


<details>
  <summary>Details</summary>
Motivation: Current EEG foundation models have limited heterogeneous representation capacity and are inefficient in capturing multi-scale brain dependencies. The authors aim to develop a more effective and biologically informed model.

Method: CodeBrain is trained in two stages: (1) A TFDual-Tokenizer independently tokenizes temporal and frequency components, expanding the discrete representation space and offering interpretability. (2) An EEGSSM architecture combines global convolution and sliding window attention to model sparse long-range and local dependencies, reflecting the brain's small-world topology.

Result: Comprehensive experiments on 10 public EEG datasets demonstrate CodeBrain's generalizability with linear probing.

Conclusion: CodeBrain provides an efficient and interpretable EEG modeling approach, laying the foundation for future neuroscience research. The code and pretraining weights will be released in the future.

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [30] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Main category: cs.LG

TL;DR: TRACE是一种通用的多模态检索器，通过将时间序列嵌入与对齐的文本上下文结合，支持灵活的跨模态检索模式，并在下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 动态数据在天气、医疗和能源等领域普遍存在，需要有效解释和检索时间序列数据。然而，现有方法缺乏语义基础，难以对齐异构模态，处理多通道信号的能力有限。

Method: 提出TRACE模型，通过硬负例挖掘实现细粒度通道级对齐，支持Text-to-Timeseries和Timeseries-to-Text等跨模态检索模式，将语言描述与复杂的时间模式连接起来。

Result: TRACE通过检索语义相关对，为下游模型提供信息丰富的上下文，提高了预测准确性和可解释性，在多个领域的实验中展示了其作为编码器和检索器的双重效用。

Conclusion: TRACE不仅是一个静态检索引擎，还是一个强大的独立编码器，经过轻量级的任务特定微调后，可以生成保持强跨模态对齐的情境感知表示。

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [31] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren,Yue Xing,Yingqian Cui,Charu C. Aggarwal,Hui Liu*

Main category: cs.LG

TL;DR: Large language model unlearning is crucial in machine learning for removing specific training data influence without retraining. This SoK paper presents a taxonomy based on the intention of unlearning, makes three key contributions, revisits removal methods, surveys evaluation strategies, and highlights practical challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive framework for understanding and advancing unlearning in generative AI, focusing on the underlying intention of unlearning.

Method: Proposing a new taxonomy based on the intention-oriented perspective, revisiting recent findings, surveying existing evaluation strategies, and highlighting practical challenges.

Result: Offered insights into whether true removal is necessary or achievable, identified limitations in current metrics and benchmarks, and pointed out challenges hindering broader deployment of unlearning methods.

Conclusion: This work aims to support future research and guide policy decisions around data removal and privacy in generative AI.

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [32] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson,Jhonathan Navott,Piotr Grynfelder,Mengyan Zhang,Makkunda Sharma,Elizaveta Semenova,Seth Flaxman*

Main category: cs.LG

TL;DR: The paper introduces BSA-TNP, a new architecture for Neural Processes that matches or exceeds accuracy of existing models while improving scalability and supporting high dimensional fixed effects.


<details>
  <summary>Details</summary>
Motivation: Current Neural Process architectures compromise accuracy for scalability in complex applications such as geology, epidemiology, climate, and robotics.

Method: Proposes BSA-TNP with Kernel Regression Blocks (KRBlocks), group-invariant attention biases, and memory-efficient Biased Scan Attention (BSA).

Result: BSA-TNP matches or exceeds the accuracy of the best models while training faster, exhibits translation invariance, models processes evolving in space and time, supports high dimensional fixed effects, and scales gracefully to run inference with over 1M test points.

Conclusion: The tradeoff between accuracy and scalability is often unnecessary when modeling fully or partially translation invariant processes.

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [33] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: A novel LLM agent framework enhancing planning capabilities through in-context learning, atomic fact augmentation, and recursive lookahead search is introduced. This approach improves understanding and decision-making without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods that struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning.

Method: The agent learns to extract task-critical 'atomic facts' from its interaction trajectories. These facts augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search where the LLM simulates potential trajectories and evaluates their outcomes.

Result: Empirically, the agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience.

Conclusion: This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates.

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [34] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: The paper presents Stained Glass Transform, a method for preserving privacy in LLMs by transforming word embeddings while maintaining model utility.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of data privacy when deploying large language models (LLMs) on shared or multi-tenant compute infrastructure.

Method: Introduction of the Stained Glass Transform, a learned, stochastic, and sequence dependent transformation of the word embeddings of an LLM which theoretically provides privacy to the input while preserving utility.

Result: Theoretical connection to mutual information of Gaussian Mixture Models and verification of privacy and utility through token level metrics and standard LLM performance benchmarks.

Conclusion: Stained Glass Transform offers a solution for ensuring data privacy in LLM deployments without sacrificing model performance.

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [35] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: The paper introduces MultiNet, an open-source benchmark and software ecosystem to evaluate models across vision, language, and action domains. It includes standardized evaluation protocols, a composite dataset with over 1.3 trillion tokens, and tools for downloading data, models, and evaluations.


<details>
  <summary>Details</summary>
Motivation: Recent innovations in multimodal action models show promise for developing general-purpose agentic systems that combine visual understanding, language comprehension, and action generation.

Method: Introduced MultiNet which consists of a benchmark, framework, toolkit, and evaluation harness. Standardized evaluation protocols for VLMs and VLAs are established, and an open source composite dataset with various tasks is provided.

Result: MultiNet has been utilized in downstream research to study the limitations of VLA generalization.

Conclusion: MultiNet provides a comprehensive ecosystem for evaluating and adapting models across vision, language, and action domains.

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [36] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper,Rohan Wadhawan,John Michael Giorgi,Chenhao Tan,Davis Liang*

Main category: cs.LG

TL;DR: CuriosiTree是一种基于启发式的策略，用于在大型语言模型中进行零样本信息获取。它通过贪婪树搜索来估计每个动作的预期信息增益，并根据预期信息增益和相关成本的平衡来选择动作。实验证明，CuriosiTree能够在临床诊断模拟中有效地整合异构信息源，优于基线动作选择策略。


<details>
  <summary>Details</summary>
Motivation: 决策者通常缺乏做出自信决策所需的信息，因此需要采取行动获取必要信息。然而，不同的信息获取手段成本不同，因此需要一种方法来选择既具有信息量又经济有效的动作。

Method: 提出了CuriosiTree，这是一种基于启发式的测试时策略。它使用贪婪树搜索来估算每个动作的预期信息增益，并根据预期的信息增益和成本之间的平衡来战略性地选择动作。

Result: 在临床诊断模拟中的实证验证表明，CuriosiTree能够以经济有效的方式整合异构信息源，并且在选择能够实现准确诊断的动作序列方面优于基准动作选择策略。

Conclusion: CuriosiTree为大型语言模型提供了一种有效的零样本信息获取策略，能够在保持成本效益的同时提高决策准确性。

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [37] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu,Dan Wu,Yixin Zhu,Ying Nian Wu*

Main category: cs.LG

TL;DR: 提出FNF和DBD用于时空建模，无需辅助技术即可达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多变量长时间序列预测面临同时捕捉变量内时间依赖性和变量间空间相关性的挑战，现有方法未能充分解决时间序列的独特属性（如周期性）。

Method: 引入FNF作为骨干网络，DBD作为架构，分别提供优秀的学习能力和最优的学习路径。FNF统一了局部时域和全局频域信息处理，并自然扩展到空间建模；DBD依据信息瓶颈理论提供了优越的梯度流和表示能力。

Result: 在11个公开基准数据集上进行实证评估，在五个领域（能源、气象、交通、环境和自然）中表现出一致的超参数设置下的SOTA性能，且无需任何辅助技术。

Conclusion: 适当设计的神经架构可以捕捉时间序列的固有属性，可能改变科学和工业应用中的时间序列建模方式。

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [38] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu,Devin White,Evelyn Rose,Vernon Lawhern,Nicholas R Waytowich,Yongcan Cao*

Main category: cs.LG

TL;DR: The paper proposes a new reinforcement learning method that mimics human decision-making by considering multiple tasks, using learnable weights to balance classification and regression models. Experiments show it outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Current RLHF approaches simplify human reasoning through isolated tasks like classification or regression, not fully capturing the complexity of human decision-making.

Method: A novel RL method is proposed that uses human ratings in reward-free environments to infer a reward function with learnable weights balancing classification and regression models.

Result: The proposed method outperforms existing rating-based RL methods and sometimes surpasses traditional RL approaches in experiments using synthetic human ratings.

Conclusion: The new RL method effectively mimics human decision-making by jointly considering multiple tasks and adaptively emphasizing different strategies.

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [39] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang,Karthik Duraisamy*

Main category: cs.LG

TL;DR: LaDCast is the first global latent-diffusion framework for medium-range ensemble forecasting, which can generate accurate and efficient probabilistic weather forecasts. It incorporates GeoRoPE, dual-stream attention mechanism and sinusoidal temporal embeddings to capture various patterns. LaDCast achieves skill close to that of the European Centre for Medium-Range Forecast IFS-ENS, especially superior in tracking rare extreme events. By operating in latent space, it reduces storage and compute significantly.


<details>
  <summary>Details</summary>
Motivation: Accurate probabilistic weather forecasting requires both high accuracy and efficient uncertainty quantification, but current methods face challenges.

Method: An autoencoder compresses high-dimensional ERA5 reanalysis fields into a compact representation, and a transformer-based diffusion model produces sequential latent updates with arbitrary hour initialization. The model incorporates GeoRoPE, dual-stream attention mechanism and sinusoidal temporal embeddings.

Result: LaDCast achieves deterministic and probabilistic skill close to that of the European Centre for Medium-Range Forecast IFS-ENS, without any explicit perturbations. It demonstrates superior performance in tracking rare extreme events such as cyclones.

Conclusion: LaDCast provides a practical path toward forecasting at kilometer-scale resolution in real time by reducing storage and compute significantly.

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [40] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He,Chaozhuo Li,Peng Tang,Litian Zhang,Sen Su*

Main category: cs.LG

TL;DR: Graph neural networks (GNNs) have been successful in graph representation learning but raise privacy concerns when handling sensitive personal information. Locally private graph learning protocols, combining local differential privacy (LDP) and GNN's message-passing, provide privacy guarantees while maintaining utility. However, these protocols are vulnerable to data poisoning attacks, where fake users compromise the utility of private graph learning. This paper presents the first such attack and demonstrates its effectiveness theoretically and empirically, also exploring defense strategies.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in graph learning using GNNs and ensure robustness and security against potential threats like data poisoning attacks.

Method: Introducing a data poisoning attack that targets locally private graph learning protocols by injecting fake users, establishing links with genuine users, and sending crafted data to the server.

Result: The attack is shown to be effective both theoretically and empirically, highlighting vulnerabilities in current locally private graph learning protocols. Explored defense strategies show limited effectiveness.

Conclusion: Locally private graph learning protocols are vulnerable to data poisoning attacks, necessitating more robust defenses for ensuring secure and privacy-preserving graph learning.

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [41] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Main category: cs.LG

TL;DR: FLoRIST是一种新的联邦微调框架，通过在本地适配器堆栈上分别执行奇异值分解，实现数学上精确的聚合，避免了高通信或计算开销，同时引入可调奇异值阈值以优化服务器端的秩选择，从而构建所有客户端共享的一对全局低秩适配器。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦LoRA方法在平衡通信效率、模型准确性和计算成本方面存在显著挑战，尤其是在异构客户端中。这些方法要么依赖简单的局部适配器平均化，引入聚合噪声；要么需要传输大量的局部适配器，导致通信效率低下；或者需要重建内存密集型的全局权重更新矩阵并进行昂贵的分解操作来设计特定于客户端的低秩适配器。

Method: FLoRIST不需在服务器端构建完整的全局权重更新矩阵，而是通过对堆叠的局部适配器分别执行有效的分解流水线（即奇异值分解）来操作。该方法在一个紧凑的中间空间内表示来自局部LoRAs的信息累积，并引入可调的奇异值阈值进行服务器端最优秩选择，以构建一对由所有客户端共享的全局低秩适配器。

Result: 广泛的实证评估表明，FLoRIST在多个数据集和大语言模型上，无论是在同质还是异质设置中，都能在保持优越的通信效率的同时，达到具有竞争力的性能。

Conclusion: FLoRIST为参数高效的大型语言模型联邦微调提供了一种有前途的解决方案，在通信效率和性能之间取得了最佳平衡。

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [42] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo,David B. Emerson,Amandeep Singh,Veronica Chatrath,Marcelo Lotif,Ravi Theja,Alex Cheung,Izuki Matsubi*

Main category: cs.LG

TL;DR: FedRAG is a framework for fine-tuning RAG systems across centralized and federated architectures, providing a simple interface and seamless conversion from centralized to federated training tasks.


<details>
  <summary>Details</summary>
Motivation: To address the drawbacks of solely relying on the parametric memory of large language models, and to improve RAG systems by fine-tuning their retriever and generator models.

Method: Introduction of FedRAG, a framework that supports state-of-the-art fine-tuning methods for RAG systems, allowing for both centralized and federated architectures.

Result: FedRAG offers an intuitive interface and seamless transition between centralized and federated training, integrating deeply with the modern RAG ecosystem.

Conclusion: FedRAG fills a critical gap in available tools for fine-tuning RAG systems.

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [43] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu,Xinqi Wang,Simon Shaolei Du*

Main category: cs.LG

TL;DR: This paper introduces a new task of clustering trajectories from offline RL datasets and proposes two methods, PG-Kmeans and CAAE, to solve it. Theoretically, the finite-step convergence of PG-Kmeans is proven, and experimentally, both methods are validated on D4RL dataset and GridWorld environments.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to introduce a novel task of clustering trajectories from offline reinforcement learning datasets and develop effective methods to solve it.

Method: The authors propose two methods, Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted Autoencoder (CAAE), for trajectory clustering. PG-Kmeans iteratively trains behavior cloning policies and assigns trajectories based on policy generation probabilities, while CAAE guides the latent representations of trajectories toward the vicinity of specific codebook entries to achieve clustering.

Result: Both PG-Kmeans and CAAE effectively partition trajectories into meaningful clusters, as validated on the D4RL dataset and custom GridWorld environments.

Conclusion: The proposed methods offer a promising framework for policy-based trajectory clustering with broad applications in offline RL and beyond.

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [44] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson,Kevin Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: An improved version of LaSDI, called mLaSDI, is proposed to enhance the accuracy and efficiency of ROMs by using multiple autoencoders trained in stages.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of LaSDI where the autoencoder struggles to accurately reconstruct training data while satisfying imposed dynamics in latent space, especially in complex or high-frequency regimes.

Method: mLaSDI uses several autoencoders trained sequentially in stages. Each autoencoder learns to correct the error of the previous stages.

Result: mLaSDI with small autoencoders results in lower prediction and reconstruction errors, and also reduces training time compared to LaSDI.

Conclusion: mLaSDI enhances the performance of ROMs by improving both the accuracy and computational efficiency.

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [45] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

Main category: cs.LG

TL;DR: 研究了用于总结变压器嵌入模型输出的池化方法设计，特别是由强化学习和视觉应用驱动的。提出了一种基于注意力的自适应池化方法，可以近似在任何信噪比（SNR）下信号最优的向量量化器，并在合成数据集和实际任务中验证了其优越的稳健性。


<details>
  <summary>Details</summary>
Motivation: 调查变压器嵌入模型输出的池化方法设计，主要受强化学习和视觉应用的推动。解决输入向量子集中包含下游任务所需信息（信号），而其余部分为干扰项（噪声）的问题。

Method: 将池化视为向量量化，目标是最小化信号损失。展示标准聚合方法（如AvgPool、MaxPool和ClsToken）在输入信噪比（SNR）波动时容易出现性能崩溃。提出一种基于注意力的自适应池化方法，该方法可以在任何SNR下近似信号最优的向量量化器。

Result: 理论结果首先通过一个专门设计的合成数据集进行监督实验验证，然后推广到标准关系推理、多智能体强化学习和具有噪声观测的视觉基准测试中。在这些任务中，使用自适应池化的变压器表现出更强的鲁棒性。

Conclusion: 基于注意力的自适应池化方法能够更好地应对输入信噪比变化带来的挑战，在多个任务中显示出优越的稳健性。这表明该方法可能成为处理复杂输入数据的有效工具。

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [46] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark,Daniel Strömbergsson,Chang Liu,Marcus Liwicki,Fredrik Sandin*

Main category: cs.LG

TL;DR: Condition monitoring (CM) is vital in process industry. Current systems have uncertainty and high false alarm rates. This work integrates LLM-based reasoning agents with CM workflows to reduce false alarms, enhance fault severity estimation, improve decision support, and offer explainable interfaces.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current computerised maintenance systems which still largely depend on human expert analysis for tasks like fault severity estimation and maintenance decisions, exhibiting considerable uncertainty and high false alarm rates.

Method: Integrating large language model (LLM)-based reasoning agents with CM workflows. Proposing MindRAG, a modular framework combining multimodal retrieval-augmented generation (RAG) with novel vector store structures designed specifically for CM data.

Result: Preliminary results indicate that MindRAG provides meaningful decision support for more efficient management of alarms, improving the interpretability of CM systems.

Conclusion: The primary contributions include an approach for structuring CM data into a semi-structured multimodal vector store compatible with LLM-driven workflows, developing multimodal RAG techniques tailored for CM data, developing practical reasoning agents, and presenting an experimental framework for integration and evaluation.

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [47] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus,Michael U. Gutmann*

Main category: cs.LG

TL;DR: This paper proposes CFMI, a new method for imputing missing data that combines continuous normalising flows, flow-matching, and shared conditional modelling. It outperforms other methods across various metrics and is scalable to high-dimensional settings.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional multiple imputation methods, especially in dealing with missing data in complex scenarios.

Method: The method uses conditional flow matching for imputation (CFMI), which integrates continuous normalising flows, flow-matching, and shared conditional modelling.

Result: CFMI matches or outperforms both traditional and modern techniques across a wide range of metrics on 24 data sets. It also performs well in zero-shot imputation of time-series data, showing higher computational efficiency compared to related methods.

Conclusion: CFMI is a versatile imputation method that performs at least as well as traditional methods on lower-dimensional data while remaining scalable to high-dimensional settings.

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [48] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis,Sebastian Lee,Claudia Clopath,Will Dabney*

Main category: cs.LG

TL;DR: The paper proposes using epistemic uncertainty estimation to guide the prioritization of transitions in experience replay for deep reinforcement learning models, which is demonstrated to be effective through experiments on tabular toy models and the Atari suite.


<details>
  <summary>Details</summary>
Motivation: Prioritized experience replay can sometimes favor noisy transitions over genuinely informative ones, leading to inefficiencies in learning. This issue resembles the 'noisy TV problem' where agents mistake noise for novelty.

Method: The authors propose using epistemic uncertainty estimation to prioritize transitions in the replay buffer. Epistemic uncertainty quantifies reducible uncertainty through learning, helping to avoid sampling transitions caused by unpredictable random processes.

Result: The proposed method is first validated on two tabular toy models (a multi-arm bandit task and a noisy gridworld), showing its benefits. It then outperforms quantile regression deep Q-learning benchmarks when evaluated on the Atari suite.

Conclusion: Using epistemic uncertainty prioritized replay can improve the efficiency of value-based deep reinforcement learning models, paving the way for more effective use of uncertainty in reinforcement learning.

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [49] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt,Max Ruiz Luyten,Antonin Berthon,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: G-Sim is a hybrid framework that combines LLM-driven structural design with empirical calibration to automate simulator construction, enabling robust decision-making in complex systems.


<details>
  <summary>Details</summary>
Motivation: Existing methods for constructing simulators either fail to generalize beyond historical data or suffer from inaccuracies when using Large Language Models (LLMs). There is a need for a more reliable approach to simulator construction.

Method: G-Sim employs an LLM iteratively to propose and refine a simulator's core components and causal relationships guided by domain knowledge. It uses flexible calibration techniques, including likelihood-free and gradient-free methods, to ground the structure in reality by estimating parameters.

Result: G-Sim produces reliable, causally-informed simulators that mitigate data-inefficiency and enable robust system-level interventions for complex decision-making.

Conclusion: By integrating domain priors with empirical evidence, G-Sim provides a solution to the challenges faced by current simulator construction methods.

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [50] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella,Joshua B. Evans,Özgür Şimşek,Anders Jonsson*

Main category: cs.LG

TL;DR: This paper proposes a state representation framework for MDPs that learns minimum action distance (MAD) from state trajectories without reward signals or actions, which enables goal-conditioned reinforcement learning and reward shaping.


<details>
  <summary>Details</summary>
Motivation: To create a state representation framework for MDPs that can be learned solely from state trajectories without needing reward signals or actions executed by the agent.

Method: Propose learning the minimum action distance (MAD), defined as the minimum number of actions required to transition between states. The self-supervised learning approach constructs an embedding space where distances correspond to MAD values, accommodating both symmetric and asymmetric approximations.

Result: Empirical results show that the proposed method efficiently learns accurate MAD representations across diverse environments with deterministic and stochastic dynamics, discrete and continuous state spaces, and noisy observations.

Conclusion: The proposed framework significantly outperforms existing state representation methods in terms of representation quality.

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [51] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen,Yiyang Liu,Mattia Prosperi,Krishna Vaddiparti,Robert L Cook,Jiang Bian,Yi Guo,Yonghui Wu*

Main category: cs.LG

TL;DR: 本文使用自然语言处理方法分析了大量电子健康记录中的HIV相关污名和社会行为情况，发现了一系列主题，并提出这种方法能更高效地评估患者状况，改善患者结果。


<details>
  <summary>Details</summary>
Motivation: 通过自然语言处理方法从电子健康记录中提取与HIV相关的污名、社会和行为情况，以克服传统问卷的局限性并提高患者结果。

Method: 识别9,140名HIV感染者，使用LDA主题建模分析，结合关键词列表和滚雪球策略扩展术语，进行主题变化分析等。

Result: 发现了多个主题，如心理健康问题与污名、社会支持与参与、医疗保健获取受限与严重疾病、治疗拒绝与孤立等，并揭示了不同年龄亚组之间的差异。

Conclusion: 从EHR临床笔记中提取和理解HIV相关污名维度、社会和相关行为情况，可以实现可扩展且高效的时间评估，克服传统问卷的限制并改善患者结果。

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [52] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani,Kseniya Solovyeva,David Danks,Vince Calhoun,Sergey Plis*

Main category: cs.LG

TL;DR: This paper addresses the challenge of learning graphical causal structures from time series data with mismatched measurement frequencies by incorporating sub-sampling effects, using answer set programming for optimization, and demonstrating superior performance and robustness in experiments.


<details>
  <summary>Details</summary>
Motivation: Learning accurate graphical causal structures from time series data is difficult when measurement frequency doesn't align with the causal timescale, leading to information loss from sub-sampling.

Method: The method uses constraint optimization via answer set programming (ASP) to find the optimal set of causal graphs. ASP identifies the most probable graph and provides an equivalence class of possible graphs. Graph theory is leveraged to prune solutions, enhancing accuracy and speed.

Result: Validated on simulated and empirical data, the approach outperforms established methods with a 12% improvement in F1 score on average. It achieves state-of-the-art precision and recall and shows robustness to varying sub-sampling rates.

Conclusion: The proposed method effectively incorporates sub-sampling effects, offering a superior and robust solution for reconstructing causal graphs from sub-sampled time series data.

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [53] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro,Zhenyu Zhang,Souvik Kundu,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: This paper proposes Dual-state Linear Attention (DSLA) and an adaptive distillation framework called SERVE to optimize large language models for lengthy inputs, improving inference speed without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with compute and memory costs when processing long inputs. Existing sub-quadratic methods like linear attention reduce these costs but often degrade accuracy by focusing too much on recent tokens.

Method: The authors introduce DSLA, which maintains two hidden states to balance historical context and recency. They also propose SERVE, an adaptive distillation framework that progressively replaces Transformer layers with DSLA layers during inference, guided by sensitivity-based layer ordering and using chained fine-tuning to maintain consistency.

Result: Experiments show that SERVE achieves 2.3x faster inference than Llama2-7B and 3.0x faster than Zamba-7B while maintaining comparable performance on tasks such as commonsense reasoning, long-context QA, and text summarization. Ablation studies confirm that DSLA's dual states effectively capture both global and local dependencies.

Conclusion: DSLA and SERVE provide a promising approach to enhance the efficiency of large language models on lengthy inputs without compromising accuracy.

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [54] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [55] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao,Hanlin Gu,Xin Yang,Bingjun Wei,Haoyang Liang,Xiangkun Wang,Tianrui Li*

Main category: cs.LG

TL;DR: In this paper, the authors propose ErrorEraser, a plugin for Continual Learning (CL) that intentionally forgets erroneous memories caused by data biases, improving performance on both new and old tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of data biases in CL methods, which lead to spurious correlations that transfer and amplify across tasks, thus reducing CL's ability to retain and transfer knowledge effectively.

Method: ErrorEraser consists of two modules: Error Identification and Error Erasure. The Error Identification module learns the probability density distribution of task data in the feature space without prior knowledge to identify potentially biased samples. The Error Erasure module ensures only erroneous knowledge is erased by shifting the decision space of representative outlier samples. Additionally, an incremental feature distribution learning strategy is designed to reduce resource overhead during error identification in downstream tasks.

Result: Extensive experimental results demonstrate that ErrorEraser significantly mitigates the negative impact of data biases, achieving higher accuracy and lower forgetting rates across three types of CL methods.

Conclusion: ErrorEraser is a universal plugin that removes erroneous memories caused by biases in CL, enhancing performance in both new and old tasks.

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [56] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

Main category: cs.LG

TL;DR: 本文研究了对抗训练中代理风险最小化序列收敛到最优对抗分类风险的速率，并提供了相应的代理风险界限，还推导了标准学习设定下的分布相关代理风险界限。


<details>
  <summary>Details</summary>
Motivation: 目前的研究已经探讨了对抗替代风险最小化序列与二元分类中的对抗分类风险最小化序列之间的关系，但尚未涉及该序列收敛到最优对抗分类风险的速率问题。

Method: 作者通过提供代理风险界限来量化对抗分类风险收敛到其最优值的速率，并在标准（非对抗）学习设定下推导出分布相关的代理风险界限。

Result: 得到了代理风险界限以量化对抗分类风险的收敛速率，并提出了标准学习设定下的分布相关代理风险界限。

Conclusion: 这些结果有助于理解对抗训练中代理风险最小化的收敛特性，并为标准学习设定提供了可能具有独立兴趣的分布相关代理风险界限。

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [57] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu,Jing Liu,Chengfang Li,Rui Xi,Wenchao Li,Liang Cao,Jin Wang,Laurence T. Yang,Junsong Yuan,Wei Zhou*

Main category: cs.LG

TL;DR: Anomaly detection (AD) is crucial in various fields. Recent advancements in deep learning, particularly diffusion models (DMs), have provided a robust framework for unsupervised AD. This survey reviews anomaly detection and generation with diffusion models (ADGDM), focusing on their synergistic relationship, categorizing methods, discussing challenges, and outlining future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive review of anomaly detection and generation using diffusion models, highlighting the synergistic relationship between the two processes and guiding researchers in leveraging DMs for innovative AD solutions.

Method: Theoretical foundations and practical implementations of ADGDM are analyzed across different data types (images, videos, time series, tabular, multimodal). Methods are categorized based on anomaly scoring mechanisms, conditioning strategies, and architectural designs.

Result: A detailed taxonomy of ADGDM methods is presented, along with an analysis of their strengths and limitations. The survey also discusses key challenges such as scalability and computational efficiency, and outlines promising future research directions.

Conclusion: This survey synthesizes recent advances in ADGDM and outlines open research questions, aiming to guide researchers and practitioners in developing innovative AD solutions using DMs across diverse applications.

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [58] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: The paper presents Location Preference Optimization (LPO), a new method that uses locational data to improve interaction preferences in GUIs. LPO introduces an entropy-based approach for predicting interaction positions and a dynamic location reward function based on physical distance. With the help of GRPO, LPO enhances exploration and interaction precision in GUI environments, outperforming existing methods in offline and real-world evaluations.


<details>
  <summary>Details</summary>
Motivation: Current methods for achieving spatial localization in GUI agents, like Supervised Fine-Tuning (SFT), struggle with accurately perceiving positional data. Reinforcement learning strategies also fail to effectively assess positional accuracy, motivating the need for a more efficient approach.

Method: LPO leverages locational data to optimize interaction preferences using information entropy for predicting interaction positions and a dynamic location reward function based on physical distance to reflect the importance of interaction positions. It is supported by Group Relative Preference Optimization (GRPO) for extensive exploration and improved interaction precision.

Result: LPO demonstrates superior performance compared to existing methods, achieving state-of-the-art results in both offline benchmarks and real-world online evaluations.

Conclusion: Location Preference Optimization (LPO) represents a significant advancement in optimizing interaction preferences for GUI agents, offering enhanced exploration and interaction precision through its innovative use of locational data.

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [59] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng,Tianming Yang*

Main category: cs.LG

TL;DR: This paper reconsiders diffusion distillation and proposes that diffusion training can be seen as a form of generative pre-training, which can be efficiently fine-tuned with GAN to achieve one-step generation.


<details>
  <summary>Details</summary>
Motivation: Diffusion distillation often requires extensive training and may degrade student model performance. Recent studies show that incorporating a GAN objective may alleviate these issues, but the underlying mechanism remains unclear.

Method: The authors identify the limitation of distillation due to mismatched step sizes and parameter numbers between teacher and student models. They demonstrate that a standalone GAN objective overcomes this limitation and is sufficient to convert diffusion models into efficient one-step generators. The authors propose that diffusion training may be viewed as a form of generative pre-training.

Result: By fine-tuning a pre-trained model with 85% of parameters frozen, the authors create a one-step generation model that achieves strong performance with only 0.2M images and near-SOTA results with 5M images.

Conclusion: The work provides a new perspective for diffusion training, highlighting its role as a powerful generative pre-training process that can be the basis for building efficient one-step generation models.

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [60] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu,Yuchao Lin,Xuan Zhang,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: An efficient network QHNetV2 is proposed for predicting Hamiltonian matrices by leveraging SO(2) local frames and operations, achieving global SO(3) equivariance without costly tensor products. It shows superior performance on QH9 and MD17 datasets.


<details>
  <summary>Details</summary>
Motivation: The inherent relationship between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local frame inspires the development of a more efficient model for predicting Hamiltonian matrices in electronic structure calculations.

Method: QHNetV2 introduces new SO(2)-equivariant operations and performs all off-diagonal feature updates and message passing within SO(2) local frames. A continuous SO(2) tensor product is used at each node to fuse node features.

Result: Extensive experiments on QH9 and MD17 datasets demonstrate superior performance across a wide range of molecular structures and trajectories.

Conclusion: QHNetV2 offers a promising direction for scalable and symmetry-aware learning of electronic structures.

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [61] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: EAM融合了DRL的学习效率与GAs的全局搜索能力，通过生成解决方案并结合遗传操作来增强探索，加速收敛，并在多个基准问题上显著提升了解决方案的质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题因其离散结构和巨大的解空间而具有挑战性，尽管DRL可以从数据中学习启发式算法，但其探索有限且容易陷入局部最优；而进化算法虽然具有强大的全局探索能力，但样本效率低且计算成本高。

Method: 提出了一种名为Evolutionary Augmentation Mechanism (EAM)的框架，该框架通过从学习策略生成解决方案，并通过特定领域的遗传操作（如交叉和变异）对其进行改进，然后选择性地将这些演化后的解决方案重新注入策略训练循环中以增强探索和加速收敛。此外，还提供了理论分析，建立了演化解分布与策略分布之间的KL散度上限，以确保稳定有效的策略更新。

Result: 在基准问题（例如TSP、CVRP、PCTSP和OP）上的广泛结果表明，EAM显著提高了解决方案质量和训练效率，优于竞争基线。

Conclusion: EAM是一个通用且即插即用的框架，可以与最先进的DRL求解器无缝集成，并在组合优化问题上表现出色。

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [62] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui,Shuiwang Ji*

Main category: cs.LG

TL;DR: Large language models (LLMs) often fail on out-of-distribution samples due to spurious correlations. This paper proposes causality-aware post-training (CAPT) to mitigate these correlations by decomposing biased predictions into unbiased steps, improving LLMs' generalization ability.


<details>
  <summary>Details</summary>
Motivation: To address the issue of large language models failing on out-of-distribution samples due to spurious correlations acquired during pre-training.

Method: Causality-aware post-training (CAPT) decomposes a biased prediction into two unbiased steps: event estimation and event intervention, reducing pre-training biases without introducing additional fine-tuning biases.

Result: Experiments on CLadder and PrOntoQA show that 3B-scale language models fine-tuned with CAPT can outperform traditional SFT and larger LLMs on in-distribution and out-of-distribution tasks using only 100 in-distribution fine-tuning samples.

Conclusion: CAPT is effective in mitigating spurious correlations in LLMs and enhancing their generalization ability.

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [63] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Main category: cs.LG

TL;DR: The paper explores generalization errors in decentralized learning, considering data heterogeneity, model initialization, stochastic gradient noise, and Byzantine attacks.


<details>
  <summary>Details</summary>
Motivation: Decentralized learning has gained significant attention but the understanding of generalization errors remains under-explored. This limits the ability to predict real-world performance of models trained through decentralized means.

Method: Fine-grained analysis of generalization errors for decentralized learning algorithms is performed under conditions of data heterogeneity and presence of Byzantine attacks. The study contrasts with prior works by not assuming homogeneous data or stringent bounded stochastic gradient assumptions.

Result: The analysis reveals the impacts of data heterogeneity, model initialization, and stochastic gradient noise on generalization error. It also shows that Byzantine attacks significantly affect generalization error in a manner tied to data heterogeneity but independent of sample size.

Conclusion: This work provides insights into factors affecting generalization errors in decentralized learning, which is crucial for improving real-world applicability. The findings are validated through numerical experiments on both convex and non-convex tasks.

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [64] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao,Quanchao Lu,Yanfu Zhang*

Main category: cs.LG

TL;DR: This paper proposes a safe screening rule for the Group SLOPE model to identify inactive predictor groups, reducing computational costs and memory usage in high-dimensional learning while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Group SLOPE is effective for adaptive selection of predictor groups but suffers from significant computational costs and memory usage due to block non-separable group effects in practical high-dimensional scenarios.

Method: The authors introduce a safe screening rule specifically designed for the Group SLOPE model. This rule efficiently identifies inactive groups with zero coefficients by addressing the block non-separable group effects, allowing these groups to be excluded during training.

Result: The proposed screening rule significantly improves computational efficiency and reduces memory usage without affecting the accuracy of the results. It can be integrated into existing solvers for both batch and stochastic algorithms and works safely with optimization algorithms.

Conclusion: The safe screening rule for Group SLOPE effectively detects inactive feature groups, leading to substantial gains in computational efficiency and reduced resource consumption while preserving the same level of accuracy.

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [65] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu,Defu Lian,Xiaolong Chen,Xu Huang,Jin Chen,Enhong Chen*

Main category: cs.LG

TL;DR: 提出RG²和RGˣ损失函数以解决Softmax损失在大规模对象空间应用中的计算开销和可扩展性限制问题，同时结合ALS优化方法，在保证泛化性能的基础上加速收敛。


<details>
  <summary>Details</summary>
Motivation: Softmax损失虽然能够自然处理列表排序并通过全局负比较灵活适用于不同场景，但在大规模对象空间中存在显著的计算开销和可扩展性限制。

Method: 通过Softmax损失的泰勒展开推导出与排序指标直接对齐的新型损失公式：Ranking-Generalizable squared（RG²）损失和Ranking-Generalizable interactive（RGˣ）损失，并将其与高效的交替最小二乘（ALS）优化方法集成，提供泛化保证和收敛率分析。

Result: 在真实数据集上的实证评估表明，该方法在实现与Softmax损失相当或更优的排序性能的同时，显著加速了收敛。

Conclusion: 所提出的框架为相似性学习社区提供了理论见解和实用高效的工具，适用于需要平衡排序质量和计算效率的广泛任务。

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [66] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang,Rémi Munos*

Main category: cs.LG

TL;DR: 在强化学习训练大语言模型时，通过KL散度进行梯度估计存在一些实现上的陷阱。许多开源项目和论文中错误地将KL估计作为损失函数来最小化KL散度，这无法产生正确的KL梯度。此外，一些实现忽略了估计问题的顺序性，导致最多只能得到部分梯度。本文通过表格和大语言模型实验展示了这些问题的影响，并展示了正确实现KL梯度的方法。


<details>
  <summary>Details</summary>
Motivation: 许多开源项目和论文在强化学习训练大语言模型时，使用了不正确的KL散度梯度估计方法，这可能导致训练效果不佳。因此，需要指出这些错误并提供正确的实现方法。

Method: 1. 分析并指出将KL估计作为损失函数来最小化KL散度的错误。
2. 强调考虑估计问题顺序性的重要性。
3. 通过表格和大语言模型实验展示错误实现的影响。
4. 提供正确实现KL梯度的方法。

Result: 展示了错误实现对训练效果的影响，并验证了正确实现方法的有效性。

Conclusion: 在RL训练LLM时，应避免直接通过KL估计作为损失函数进行优化，并且需要考虑估计问题的顺序性以获得完整的梯度。

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [67] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong,Haotian Lu,Wenzhuo Zheng,Fan Zhang,Shuangjia Zheng,Ning Liu*

Main category: cs.LG

TL;DR: EnerBridge-DPO是一种新的蛋白质逆向折叠框架，通过整合Markov Bridges与Direct Preference Optimization（DPO），并引入显式能量约束损失，生成低能量、高稳定性的蛋白质序列。该模型在保持序列恢复率的同时，能设计出能量更低的蛋白质复合物序列，并准确预测不同序列间的$\Delta \Delta G$值。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法主要通过最大化序列恢复率来训练，常常忽视生成序列的能量，导致在蛋白质逆向折叠中难以设计出具有最佳能量稳定性的蛋白序列。

Method: 提出EnerBridge-DPO框架，核心创新包括：1) 将Markov Bridges与DPO结合，使用基于能量的偏好微调Markov Bridge模型；2) 引入显式能量约束损失，增强DPO的能量驱动特性，使模型能够从先验知识中学习能量表示并直接预测序列能量值。

Result: EnerBridge-DPO可以设计出能量更低的蛋白质复合物序列，同时保持与最先进模型相当的序列恢复率，并能准确预测不同序列间的$\Delta \Delta G$值。

Conclusion: EnerBridge-DPO克服了现有方法的局限性，成功生成低能量、高稳定性的蛋白质序列，在蛋白质逆向折叠领域展现了显著优势。

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [68] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom,Paul R. Schrater*

Main category: cs.LG

TL;DR: The paper introduces Option Kernel Bellman Equations (OKBEs) for a new reward-free Markov Decision Process that constructs and optimizes state-time option kernels (STOKs), which are compositional, modular, and interpretable. OKBEs support verifiable long-horizon planning and intrinsic motivation in high-dimensional world-models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of reward-maximization in reinforcement learning, particularly its conflict with compositionality, modularity, and interpretability, the authors propose a new framework based on Option Kernel Bellman Equations (OKBEs).

Method: OKBEs directly construct and optimize a predictive map called a state-time option kernel (STOK) to maximize the probability of completing a goal while avoiding constraint violations. STOKs can be composed using Chapman-Kolmogorov equations, represented efficiently in a factorized form, and record probabilities of goal-success and constraint-violation events.

Result: This approach leads to highly flexible agents that can rapidly synthesize meta-policies, reuse planning representations across many tasks, and justify goals using empowerment, an intrinsic motivation function.

Conclusion: OKBEs facilitate compositionality, modularity, and interpretability, supporting verifiable long-horizon planning and scalable intrinsic motivation in dynamic high-dimensional world-models.

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [69] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen,Reda Ouhamma,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 研究了基于人类反馈的强化学习，提出了一种元算法和改进算法以解决轨迹级偏好比较中的奖励识别问题。实验表明该方法在少量偏好查询下与基于奖励的强化学习具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 在一般的马尔可夫决策过程中，通过轨迹级偏好比较进行强化学习的研究，旨在设计能有效识别潜在奖励的算法并确保理论保障。

Method: 提出了一种基于随机探索的元算法，避免计算挑战且保持可行性；同时引入改进算法，通过收集轨迹对批次和应用最优实验设计来选择信息量大的比较查询，并支持偏好查询的并行化。

Result: 建立了后悔值和最后一次迭代的保证，改进算法减少了查询复杂度，实证结果表明该方法在少量偏好查询下与基于奖励的强化学习有竞争力。

Conclusion: 提出的基于随机探索的元算法及其改进版本能够有效减少计算复杂度和查询复杂度，在理论和实践上均表现出色。

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [70] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho,Minju Jo,Kookjin Lee,Noseong Park*

Main category: cs.LG

TL;DR: The paper proposes a new network architecture to improve generalization and extrapolation performance for signals with periodic properties.


<details>
  <summary>Details</summary>
Motivation: Coordinate-based MLPs often face issues of overfitting and limited generalizability beyond the training region, resulting in subpar extrapolation performance.

Method: A novel network architecture is proposed, which extracts periodic patterns from measurements and leverages this information to represent the signal.

Result: Comprehensive experiments demonstrate the efficacy of the proposed method, including learning periodic solutions for differential equations and time series imputation and forecasting on real-world datasets.

Conclusion: The proposed network architecture enhances generalization and improves extrapolation performance for signals with periodic properties.

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [71] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: The paper introduces Athena-PRM, a multimodal process reward model that evaluates reasoning steps efficiently and effectively. It uses prediction consistency between weak and strong completers for labels, ORM initialization, and up-sampling for negative data to enhance PRM performance. Athena-PRM sets new state-of-the-art results in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of developing high-performance process reward models (PRMs) which usually require significant time and financial investment due to step-level annotations, as well as the noise and computational costs from conventional automated labeling methods like Monte Carlo estimation.

Method: Athena-PRM leverages prediction consistency between weak and strong completers as a criterion for identifying reliable process labels, reducing the need for extensive manual annotations. Additionally, two strategies are developed: ORM initialization and up-sampling for negative data, to further improve PRM performance. The model is validated in three scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning.

Result: Athena-PRM achieves superior performance across multiple benchmarks and scenarios. Specifically, it enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling when using Qwen2.5-VL-7B as the policy model. It also sets the state-of-the-art results in VisualProcessBench, outperforming previous SoTA by 3.9 F1-score. When used as a reward model, Athena-PRM significantly improves performance on five benchmarks with Athena-7B.

Conclusion: Athena-PRM demonstrates outstanding effectiveness in evaluating reasoning steps with limited samples, while its innovative strategies contribute to enhanced PRM performance. The model establishes new state-of-the-art results in several benchmarks, proving its robust capability to assess reasoning step correctness.

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [72] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang,Du Yin,Hao Xue,Flora Salim*

Main category: cs.LG

TL;DR: The paper proposes STOAT, a novel framework for probabilistic forecasting in spatial-temporal causal time series (STC-TS). It incorporates causal inference and spatial relation matrices to improve predictions, especially in regions with strong spatial dependencies. Experiments show it outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for modeling spatial-temporal data often treat spatial and temporal dynamics independently and do not adequately address causality-driven probabilistic forecasting, which limits their predictive accuracy.

Method: The authors introduce STOAT, which uses a spatial relation matrix to encode interregional dependencies and deep probabilistic models to estimate distribution parameters, allowing for calibrated uncertainty modeling. Multiple output distributions are explored to capture region-specific variability.

Result: Experiments on COVID-19 data from six countries demonstrate that STOAT surpasses state-of-the-art probabilistic forecasting models in key metrics, particularly in areas with strong spatial dependencies.

Conclusion: STOAT provides a generalizable framework for complex spatial-temporal tasks by integrating causal inference with geospatial probabilistic forecasting, offering potential applications in fields like epidemic management.

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [73] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary,Wassim Uddin Mondal,Laxmidhar Behera*

Main category: cs.LG

TL;DR: The paper proposes Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that combines offline and online RL to enhance exploration, improve policy performance, and generalize better. It introduces a meta-policy for seamless adaptation between offline and online trajectories, achieving strong performance on benchmarks with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Sample efficiency and exploration are critical challenges in DRL, particularly in complex domains. While offline RL is promising, it faces issues like out-of-distribution actions that limit policy performance and generalization. This motivates the development of MOORL, which aims to overcome these limitations by unifying offline and online RL.

Method: MOORL introduces a meta-policy that adapts across offline and online trajectories, enabling robust initialization from offline data and efficient exploration through online interactions. The method avoids extensive design components and added computational complexity, learning a stable Q-function effectively.

Result: Extensive experiments on 28 tasks from D4RL and V-D4RL benchmarks show consistent improvements over state-of-the-art offline and hybrid RL baselines. MOORL achieves strong performance with minimal computational overhead.

Conclusion: MOORL demonstrates the potential for practical applications in real-world scenarios by efficiently combining the strengths of offline and online data, enhancing exploration, and improving policy performance without significant computational costs.

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [74] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler,Lukas Kuhn,Florian Buettner*

Main category: cs.LG

TL;DR: Reliable uncertainty calibration is crucial for deploying deep neural networks in high-stakes applications. Despite foundation models showing improvements in predictive performance, their calibration properties are not well understood. This paper investigates the calibration behavior of foundation models, revealing that they tend to be underconfident in in-distribution predictions and more calibrated under distribution shifts. Post-hoc calibration techniques can mitigate underconfidence bias in in-distribution settings but become less reliable under severe distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To explore the calibration properties of foundation models such as ConvNeXt, EVA and BEiT, which have shown significant improvements in predictive performance but remain underexplored in terms of calibration behavior.

Method: Conduct a comprehensive investigation into the calibration behavior of foundation models, analyzing both in-distribution and out-of-distribution predictions, and assessing the effectiveness of post-hoc calibration techniques.

Result: Foundation models tend to be underconfident in in-distribution predictions leading to higher calibration errors, while showing improved calibration under distribution shifts. Post-hoc calibration methods effectively reduce underconfidence bias in in-distribution settings but lose reliability under severe distribution shifts.

Conclusion: The calibration behavior of foundation models is complex and does not follow a continuous improvement trend. Architectural and training innovations have non-monotonic effects on calibration, challenging existing paradigms.

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [75] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin,Hailin Wang,Jingyao Hou,Jianjun Wang*

Main category: cs.LG

TL;DR: 为了解决现有张量恢复方法无法识别张量尺度变化对其结构特征影响的问题，以及处理大规模高阶张量数据时面临高昂的计算成本问题，本文设计了两种快速准确的随机化算法用于低秩张量逼近问题，并建立了近似误差估计的理论界。然后，提出了一种新的广义非凸建模框架，适用于大规模张量恢复，并利用新的正则化范式实现对大规模张量的先验表示。在此基础上，进一步研究了统一的非凸模型和高效的优化算法，分别适用于未量化和量化情况下的典型高阶张量恢复任务。为了使所提出的算法在处理大规模张量数据时实用且高效，将所提出的随机LRTA方案集成到其核心和耗时的计算中。最后，通过在各种大规模张量上进行广泛的实验，结果表明所提出的方法在与一些最先进的方法相比具有实用性、有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的张量恢复方法无法识别张量尺度变化对其结构特征的影响，并且在处理大规模高阶张量数据时面临高昂的计算成本。这促使了对更快速、更准确的张量恢复方法的需求。

Method: 1. 设计了两种快速准确的随机化算法用于低秩张量逼近问题。
2. 提出了一个新的广义非凸建模框架，适用于大规模张量恢复。
3. 利用新的正则化范式实现对大规模张量的先验表示。
4. 研究了统一的非凸模型和高效的优化算法，分别适用于未量化和量化情况下的典型高阶张量恢复任务。
5. 将随机LRTA方案集成到核心和耗时的计算中以提高效率。

Result: 广泛的实验结果表明，所提出的方法在处理各种大规模张量时，相较于一些最先进的方法，具有更高的实用性、有效性和优越性。

Conclusion: 本文提出的方法能够有效地解决现有张量恢复方法存在的问题，包括无法识别张量尺度变化对其结构特征的影响和处理大规模高阶张量数据时的高昂计算成本。通过结合Krylov子空间迭代、块Lanczos双对角化过程和随机投影策略，提出了快速准确的随机化算法和新的建模框架，展示了在大规模张量恢复任务中的优势。

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [76] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo,Huan Wang*

Main category: cs.LG

TL;DR: SparseSSM是一种新的训练无关剪枝框架，扩展了经典的OBS框架到状态空间架构，通过层析算法实现无微调情况下修剪SSM权重50%而不损失零样本准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的单次剪枝方法适用于注意力块，但未能考虑到选择性状态空间模块（SSM）中的时间共享和离散化状态转换矩阵，这促使了针对状态空间模型的更有效剪枝方法的需求。

Method: 引入SparseSSM框架，该框架包含：(i) 推导出一个近似的二阶显著性分数，整合跨时间步长的Hessian-trace信息；(ii) 进行组件敏感性分析以指导前馈网络（FFN）剪枝；(iii) 可轻松扩展到半结构化和结构化稀疏性。

Result: 在无需微调的情况下，成功修剪了50%的SSM权重，并且没有观察到零样本准确性的损失，达到了目前基于Mamba的大型语言模型的最佳剪枝算法。

Conclusion: SparseSSM为状态空间模型提供了一种有效的剪枝解决方案，能够在不损失性能的情况下大幅减少模型参数，从而促进其部署和应用。

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [77] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina,Dmitry Shirokov*

Main category: cs.LG

TL;DR: The paper introduces GLGENN, a new architecture for equivariant neural networks based on geometric algebras, which performs well with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To create an improved architecture of equivariant neural networks that can handle pseudo-orthogonal transformations and reduce the tendency to overfitting compared to baseline models.

Method: Developed Generalized Lipschitz Group Equivariant Neural Networks (GLGENN) using weight-sharing parametrization technique considering fundamental structures and operations of geometric algebras.

Result: GLGENN outperforms or matches competitors in benchmarking tasks like equivariant function estimation and convex hull experiment while using significantly fewer optimizable parameters.

Conclusion: GLGENN is parameter-efficient and shows better or equal performance compared to other equivariant models.

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [78] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens,Alberto Gutierrez,Jordi Torres,Josep. Ll Berral,Anisa Halimi,Kieran Fraser*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) can generate synthetic tabular data through in-context learning (ICL), but statistical biases within in-context examples can propagate to the distribution of synthetic tabular data, even leading to adversarial scenarios where malicious contributors compromise the fairness of downstream classifiers.


<details>
  <summary>Details</summary>
Motivation: To systematically study how statistical biases within in-context examples propagate to the distribution of synthetic tabular data and introduce an adversarial scenario where a malicious contributor can inject bias into the synthetic dataset via a subset of in-context examples.

Method: Systematically studying the propagation of statistical biases within in-context examples to the distribution of synthetic tabular data and introducing an adversarial scenario for analysis.

Result: Even mild in-context biases lead to global statistical distortions and a malicious contributor can compromise the fairness of downstream classifiers for a targeted and protected subgroup.

Conclusion: There is a new vulnerability associated with LLM-based data generation pipelines that rely on in-context prompts with in sensitive domains.

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [79] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: FedVLMBench，首个针对视觉-语言模型（VLMs）联邦微调的系统性基准测试平台被提出。它集成了主流VLM架构、微调策略、FL算法及多模态数据集，通过广泛实验揭示了VLM架构、微调策略、数据异构性和多任务联邦优化之间的相互作用关键见解。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）在跨模态理解和生成方面表现出色，但大多数方法依赖集中式训练，在隐私敏感领域（如医疗保健）部署存在挑战。尽管已有研究将联邦学习引入VLM微调以解决隐私问题，但缺乏全面评估联邦微调策略、模型架构和任务泛化的基准。

Method: 构建FedVLMBench基准测试平台，整合两种主流VLM架构（基于编码器和无编码器）、四种微调策略、五种FL算法、六个涵盖多种跨域单任务和多任务场景的多模态数据集。执行广泛实验以探索VLM架构、微调策略、数据异构性和多任务联邦优化之间的关系。

Result: 发现基于编码器的VLMs在联邦学习中，使用2层MLP连接器并同时调整连接器和LLM是最佳配置。此外，现有FL方法在视觉为中心的任务上对数据异构性的敏感度显著高于文本为中心的任务。

Conclusion: FedVLMBench提供了必要的工具、数据集和实证指导，为研究社区提供了一个标准化平台，以推动保护隐私的多模态基础模型联邦训练的发展。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [80] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: In the abstract, the authors present SyncFed, a new Federated Learning framework that uses time-aware mechanisms to improve model reliability and convergence in large, distributed systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges of maintaining consistency in Federated Learning as it moves towards larger, more distributed environments. Issues such as network-induced delays, clock unsynchronicity, and variability in client updates can lead to misaligned contributions that affect model reliability and convergence.

Method: The proposed method, SyncFed, is a time-aware FL framework that employs explicit synchronization and timestamping to establish a common temporal reference across the system. It quantifies staleness numerically using timestamps exchanged under the Network Time Protocol (NTP), allowing the server to assess the relative freshness of client updates and apply appropriate weighting during aggregation.

Result: Empirical evaluation on a geographically distributed testbed demonstrates that SyncFed ensures the global model evolves within a stable temporal context, leading to improved accuracy and information freshness compared to round-based baselines without temporal semantics.

Conclusion: SyncFed provides a mechanism to quantify staleness and improve the reliability and convergence of models in latency-sensitive and cross-regional deployments through its time-aware approach.

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [81] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi,Davide Leo,Davide Carbone*

Main category: cs.LG

TL;DR: WAFFLE is a detection algorithm for Federated Learning that identifies anomalous clients using compressed representations from Wavelet Scattering Transform or Fourier Transform, improving detection accuracy and classification performance.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges in detecting anomalous or corrupted clients without accessing raw data, which can degrade model performance.

Method: Proposes WAFFLE, an algorithm that uses locally computed compressed representations (Wavelet Scattering Transform or Fourier Transform) to label malicious clients before training. A lightweight detector trained on a public dataset performs the labeling with low overhead.

Result: Experiments show improved detection accuracy and downstream classification performance compared to existing anomaly detection algorithms in Federated Learning.

Conclusion: WAFFLE provides an effective pre-training alternative to online detection strategies, with Wavelet Scattering Transform offering theoretical advantages in federated scenarios.

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [82] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta,Pietro Liò*

Main category: cs.LG

TL;DR: The paper introduces Wasserstein Hypergraph Neural Network that uses Sliced Wasserstein Pooling to aggregate information, preserving geometric properties and improving node classification tasks.


<details>
  <summary>Details</summary>
Motivation: To advance hypergraph neural networks by developing a method that goes beyond basic pooling operations and captures more complex geometric properties in the data.

Method: Treat nodes and hyperedge neighbourhood as distributions and use Sliced Wasserstein Pooling for aggregation, which differs from conventional methods by preserving geometric properties like shape and spread of distributions.

Result: Experimental results show significant benefits for node classification tasks and top performance on several real-world datasets.

Conclusion: Wasserstein Hypergraph Neural Network with Sliced Wasserstein Pooling improves node classification by reflecting how easily one hyperedge distribution can be transformed into another.

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [83] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura,Karim Tit,Laura Bussi,Eleonora Giunchiglia,Maxime Cordy*

Main category: cs.LG

TL;DR: TRIDENT是一种通用的推理时算法，可以确保模型输出满足LTLf约束，无需重新训练。它通过将LTLf公式编译为DFA，并用其引导受约束的波束搜索来实现。此方法在两个任务中都达到了完美的约束满足，并且相较于现有技术显示出更高的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型和其他神经架构在生成和分类任务中取得了令人印象深刻的结果，但它们本质上无法确保输出满足时间约束（例如LTLf）。

Method: TRIDENT将LTLf公式编译为确定性有限自动机（DFA），并使用它来指导受约束的波束搜索。在每个解码步骤中，屏蔽可能导致约束违反的转换，并根据模型的概率和DFA的接受结构动态重新排名剩余路径。

Result: TRIDENT在两个不同任务中均实现了完美的约束满足，同时与现有技术相比，展示了改进的效率和高质量的标准度量。

Conclusion: TRIDENT提供了一种通用且模型无关的方法，保证了输出符合给定的LTLf约束，同时提高了输出质量。

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [84] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.LG

TL;DR: ACNs replace short residual connections with long feedforward connections, enabling 'auto-compression' which enhances early layer representations, reduces redundancy in deeper layers, and offers benefits like noise robustness, low-data performance, transfer learning, and mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often face computational redundancy when increasing depth does not improve representation quality.

Method: Introduce ACNs with long feedforward connections to the output from each layer instead of short residual connections, leading to auto-compression where information is pushed into early layers during training.

Result: Showed up to 18% reduction in catastrophic forgetting and 30-80% architectural compression while maintaining accuracy across various architectures. Also demonstrated better sparsity-performance trade-offs when combined with pruning techniques.

Conclusion: ACNs provide a practical approach for efficient neural architectures that adapt their computational footprint to task complexity while learning robust representations.

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [85] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: The paper presents AtmosMJ, a deep convolutional network that achieves stable long-range weather forecasts on a standard latitude-longitude grid without using non-standard spatial domains. It uses a Gated Residual Fusion (GRF) mechanism to prevent error accumulation and shows competitive forecast accuracy with significantly lower training costs.


<details>
  <summary>Details</summary>
Motivation: To investigate if comparable long-range performance in weather forecasting can be achieved using the standard latitude-longitude grid rather than relying on non-standard spatial domains like spherical harmonics or HEALPix meshes.

Method: Introduction of AtmosMJ, a deep convolutional network operating directly on ERA5 data without spherical remapping. Stability is ensured via a novel Gated Residual Fusion (GRF) mechanism which moderates feature updates to avoid error accumulation over recursive simulations.

Result: AtmosMJ produces stable and physically plausible forecasts for about 500 days and achieves competitive 10-day forecast accuracy against other models while requiring a low training budget of 5.7 days on a V100 GPU.

Conclusion: Efficient architectural design, as demonstrated by AtmosMJ, can be more crucial than non-standard data representation for achieving stable and computationally efficient long-range weather prediction.

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [86] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang,Zeyang Zhang,Linxin Xiao,Haibo Chen,Chendi Ge,Wenwu Zhu*

Main category: cs.LG

TL;DR: The paper explores the potential of Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across diverse multi-modal graph data and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal graph learning methods fail to generalize across various multi-modal graph data and tasks as they are typically trained from scratch for specific graph data and tasks.

Method: The authors propose a unified framework of multi-modal graph data, task, and model, discovering the inherent multi-granularity and multi-scale characteristics in multi-modal graphs. They present five key desired characteristics for MG-LLM.

Result: The paper elaborates on the key challenges, reviews related works, and highlights promising future research directions towards realizing these characteristics.

Conclusion: This paper can contribute to the ongoing advancement of the research towards MG-LLM for generalization across multi-modal graph data and tasks.

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [87] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca,Peini Liu,Jordi Guitart,Rodrigo M Carrillo-Larco,Ajay Dholakia,David Ellison*

Main category: cs.LG

TL;DR: A cognitive architecture for ML model monitoring uses feature engineering principles with LLMs to enhance interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional ML monitoring approaches produce outputs that are verbose and difficult to interpret, hindering effective decision-making.

Method: Proposes a Decision Procedure module with three steps - Refactor, Break Down, and Compile - that simulates feature engineering using LLM-based agents. This reduces noise in data, allows for detailed analysis of complex information, and integrates insights into clear outputs.

Result: Experiments with multiple LLMs show higher accuracy compared to various baselines across several domains.

Conclusion: The combination of feature engineering-driven planning and selective LLM utilization creates a robust decision support system providing interpretable and actionable insights.

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [88] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma,Takayuki Nishio*

Main category: cs.LG

TL;DR: This paper proposes Load-aware Tram-FL, which reduces training time in decentralized federated learning by accounting for computational and communication loads.


<details>
  <summary>Details</summary>
Motivation: To minimize total training time in decentralized federated learning considering both computational and communication loads.

Method: The scheduling problem is formulated as a global optimization task and decomposed into node-wise subproblems. A variance constraint is introduced to promote balanced data utilization under non-IID distributions.

Result: Simulation results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly reduces training time and accelerates convergence compared to baseline methods.

Conclusion: Load-aware Tram-FL effectively minimizes overall training latency and improves data utilization balance.

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [89] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee,Sehee Lim,Kibok Lee,Jy-yong Sohn*

Main category: cs.LG

TL;DR: The paper proposes a unified framework for understanding contrastive learning (CL) by analyzing cosine similarity between embeddings of positive and negative pairs. It highlights the unattainability of perfect alignment in full-batch settings, the impact of smaller batch sizes on mini-batch CL, and introduces an auxiliary loss term to reduce variance in similarities of negative pairs.


<details>
  <summary>Details</summary>
Motivation: Prior works lack a comprehensive framework that systematically explains a broad class of contrastive loss objectives.

Method: A unified framework is presented which analyzes the cosine similarity between embeddings of positive and negative pairs in both full-batch and mini-batch settings. An auxiliary loss term is introduced to reduce variance in similarities of negative pairs in mini-batch CL.

Result: Empirical results show that incorporating the proposed loss consistently improves the performance of CL methods in small-batch training.

Conclusion: This work provides insights into the limitations of current CL methods and demonstrates the effectiveness of the proposed auxiliary loss in enhancing CL performance.

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [90] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov,Alexander Yuhay,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 提出了一种新的对比自监督学习框架Dependent TS2Vec，适用于连续相关数据，通过引入依赖感知损失函数来捕捉时空依赖性。在多个基准测试中超越了现有方法，特别是在UEA和UCR基准上分别提高了4.17%和2.08%的准确率，并在干旱分类任务中ROC-AUC得分提高了7%。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习（SSL）在处理相关数据（如时间序列和时空数据）方面存在不足，传统方法假设样本之间语义独立，不适用于具有复杂相关性的数据。因此需要一种新的方法来解决这一问题。

Method: 提出了一个针对连续相关数据的理论框架，定义了两种对象之间的相似度量（硬接近和软接近），并推导出一个估计相似矩阵的解析形式以适应样本间的不同接近类型，进而设计了依赖感知的损失函数。最终形成的方法称为Dependent TS2Vec。

Result: 在时间和时空下游任务中验证了该方法的有效性，相比现代方法表现更优。具体来说，在UEA和UCR基准测试中分别提升了4.17%和2.08%的准确率；在复杂的干旱分类任务中，ROC-AUC得分提高了7%。

Conclusion: 所提出的依赖感知损失函数在捕捉时空依赖性方面非常有效，为相关数据的自监督学习提供了一种新途径。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [91] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong,Alfred Hero*

Main category: cs.LG

TL;DR: Self-supervised contrastive learning (SSCL) is powerful for representation learning. However, supervised contrastive (SupCon) approaches have received little attention. This paper introduces ProjNCE, a generalization of the InfoNCE loss that unifies supervised and self-supervised contrastive objectives. ProjNCE constitutes a valid mutual information (MI) bound and offers flexibility in selecting projection strategies for class embeddings. Extensive experiments demonstrate that ProjNCE consistently outperforms both SupCon and standard cross-entropy training.


<details>
  <summary>Details</summary>
Motivation: Despite the popularity of SSCL, there has been less focus on supervised contrastive approaches, particularly the relationship between SupCon and mutual information (MI).

Method: The authors introduce ProjNCE, which incorporates projection functions and an adjustment term for negative pairs to unify supervised and self-supervised contrastive objectives. They prove that ProjNCE forms a valid MI bound and explore various projection methods for centroid-based class embeddings in SupCon.

Result: ProjNCE consistently outperforms both SupCon and standard cross-entropy training across multiple datasets and settings.

Conclusion: This work refines SupCon by providing a mutual information interpretation and exploring projection design, offering improvements whenever SupCon is used as the foundational contrastive objective.

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [92] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia,Benjamin Schiffer,Serena Wang,Shirley Zhang*

Main category: cs.LG

TL;DR: The paper uses social choice theory to formalize two notions of representation for selecting a subset of evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: A common problem in LLM evaluation is how to choose a subset of metrics from a full suite of possible metrics. Subset selection is usually done for efficiency or interpretability reasons, and the goal is often to select a ``representative'' subset of metrics.

Method: The authors introduce positional representation and positional proportionality, which guarantee every alternative is sufficiently represented at every position cutoff and no alternative is proportionally over- or under-represented by more than a small error at any position respectively.

Result: They prove upper and lower bounds on the smallest number of metrics needed to guarantee either of these properties in the worst case. They also study a generalized form of each property that allows for additional input on groups of metrics that must be represented.

Conclusion: Finally, they tie theory to practice through real-world case studies on both LLM evaluation and hospital quality evaluation.

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [93] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo,Sören Becker,Niki Kilbertus*

Main category: cs.LG

TL;DR: Sparse linear ODEs are unidentifiable with a positive probability in practically relevant sparsity regimes.


<details>
  <summary>Details</summary>
Motivation: Dynamical systems modeling is crucial for scientific inquiry across natural and life sciences. Identifiability of dynamical system models learned from data is paramount, but the sparse regime of linear ordinary differential equations (ODE) remains underexplored despite its practical relevance.

Method: The authors characterize the identifiability of sparse linear ODEs, showing that sparse systems are unidentifiable with a positive probability in practically relevant sparsity regimes. They provide lower bounds for this probability and study empirically how this theoretical unidentifiability manifests in state-of-the-art methods to estimate linear ODEs from data.

Result: Sparse linear ODEs are theoretically and practically unidentifiable. Theoretical limitations cannot be resolved through inductive biases or optimization dynamics.

Conclusion: The findings call for rethinking what can be expected from data-driven dynamical system modeling and allow for quantitative assessments of how much to trust a learned linear ODE.

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [94] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani,Sonia Ben Mokhtar,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Diana Nurbakova*

Main category: cs.LG

TL;DR: 本研究提出了Worker Label Alignement Loss (WoLA)，一种在异构联邦学习环境中对齐诚实工作者梯度的加权损失方法，有效识别拜占庭梯度并显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 联邦学习尽管保护隐私，但从安全角度来看存在威胁，因为拜占庭参与者可能提供有害梯度或模型参数影响模型收敛。现有的拜占庭弹性联邦学习策略难以在异构设置中区分诚实和拜占庭梯度。

Method: 引入了Worker Label Alignement Loss (WoLA)，一种加权损失方法，用于在数据异构情况下对齐诚实工作者梯度，从而帮助识别拜占庭梯度。

Result: WoLA方法在异构设置中显著优于现有技术，并通过理论分析和实证证据证明其有效性。

Conclusion: WoLA为解决异构联邦学习中的拜占庭问题提供了新途径，并展示了其优越性能。

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [95] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals,Vasilis Belis,Elias F. Combarro,Eduard Alarcón,Sofia Vallecorsa,Michele Grossi*

Main category: cs.LG

TL;DR: The paper introduces Guided Graph Compression (GGC), a framework that uses a graph autoencoder to compress large graphs for use in quantum computing, specifically Quantum Graph Neural Networks (QGNNs). It enhances performance on a downstream classification task and is evaluated on the Jet Tagging task in high energy physics. GGC outperforms alternatives and facilitates testing of QGNNs on realistic datasets.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNNs) struggle with large graphs due to memory constraints and inefficient operations on GPUs. Quantum Computing offers potential solutions but current hardware limits data dimensions effectively encodable. Existing methods either manually simplify datasets or use artificial ones.

Method: GGC framework uses a graph autoencoder to reduce nodes and node feature dimensionality. The compression is guided by enhancing performance of a downstream classification task, compatible with both quantum and classical classifiers.

Result: GGC outperforms using the autoencoder as a standalone preprocessing step and a baseline classical GNN classifier. It also enables testing novel QGNN ansatzes on realistic datasets.

Conclusion: GGC provides an effective solution for compressing large graphs, enhancing performance on classification tasks and enabling further exploration of QGNNs.

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [96] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey,Wasim Arif,Rakhesh Singh Kshetrimayum*

Main category: cs.LG

TL;DR: The paper proposes a machine learning-based methodology for oil sample classification using a microwave resonant sensor, achieving 99.41% accuracy with random forest classifier.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and reliable method for oil classification based on their dielectric properties.

Method: Using microwave resonant sensor to capture variations in resonant frequency and amplitude response of oils, then applying multiple machine learning classifiers on the extracted features.

Result: Achieved a high classification accuracy of 99.41% with the random forest classifier.

Conclusion: The proposed system is viable for fast and reliable oil characterization in industrial environments due to its compact form factor, efficiency, and high performance.

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [97] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 在联邦学习中，确保对拜占庭客户端的弹性同时保护客户端数据隐私是一个基本挑战。本文提出了一种多阶段方法，结合可验证的秘密共享、安全聚合和定制的对称私有信息检索方案，在数据异构情况下实现信息理论隐私保证和拜占庭弹性。此外，还研究了零阶估计方法与安全聚合的相互作用，以降低通信成本并使私有聚合可扩展。


<details>
  <summary>Details</summary>
Motivation: 尽管在客户端数据同质的情况下，已有从信息论角度研究的适当对策来确保梯度聚合的鲁棒性，但在数据异构时这些对策失效。虽然最近提出的预处理技术（如最近邻混合）可以增强异构设置下的性能，但它们无法与引入的隐私保护机制结合使用。

Method: 提出了一种多阶段方法，包括精心设计的可验证秘密共享、安全聚合和定制的对称私有信息检索方案，以实现信息理论隐私保证和拜占庭弹性。还研究了零阶估计方法与安全聚合的相互作用，以降低通信成本并提高私有聚合的可扩展性。

Result: 通过在多种攻击场景下评估该方案的有效性，结果表明其性能优于已知的其他技术。

Conclusion: 所提出的方法成功地解决了在数据异构情况下，同时确保隐私和拜占庭弹性的挑战，并且通过结合零阶估计方法，显著降低了通信开销，提高了私有聚合的可扩展性。

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [98] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi,Hugo Koubbi,Theodor Misiakiewicz,Nathan Srebro*

Main category: cs.LG

TL;DR: The paper explores the problem of learning single-index models using spherical harmonics as a natural basis, providing new insights into the complexity of this task and proposing two families of estimators.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of the complexity involved in learning single-index models and to offer improved methods for recovering the unknown one-dimensional projection vector $\boldsymbol{w}_*$ under arbitrary spherically symmetric input distributions.

Method: Propose the use of spherical harmonics instead of Hermite polynomials as the natural basis for analyzing the problem. Introduce two families of estimators based on tensor unfolding and online SGD that achieve either optimal sample complexity or optimal runtime.

Result: Characterize the complexity of learning single-index models under arbitrary spherically symmetric input distributions. Show that estimators achieving both optimal sample complexity and optimal runtime may not exist in general. For Gaussian inputs, the theory recovers and clarifies existing results while revealing previously overlooked phenomena.

Conclusion: Spherical harmonics provide a more intrinsic perspective for studying single-index models due to their capture of rotational symmetry. The proposed estimators offer significant advancements in terms of either sample efficiency or computational speed.

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [99] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman,Ilija Trajkovic,Julia Kaltenborn,Francis Pelletier,Alex Archibald,Yaniv Gurwicz,Peer Nowack,David Rolnick,Julien Boussard*

Main category: cs.LG

TL;DR: The paper presents a new interpretable climate model emulator based on causal representation learning, incorporating physics-informed causal relationships to accurately emulate climate dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional climate change models are computationally expensive and current machine learning approaches cannot incorporate physics-informed causal relationships.

Method: Developed an interpretable climate model emulator using causal representation learning, included a Bayesian filter for stable long-term autoregressive emulation.

Result: The emulator successfully learns accurate climate dynamics and the importance of each component is demonstrated on a realistic synthetic dataset and data from two widely used climate models.

Conclusion: This new emulator can potentially improve predictions of climate change and analyses of its causes and effects.

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [100] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso,Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.LG

TL;DR: The paper explores how VQVAE can suppress adversarial attacks on high-SNR radio-frequency data-points, demonstrating its effectiveness through various evaluations and comparisons.


<details>
  <summary>Details</summary>
Motivation: To understand the attack suppressing properties of VQVAE when an adversarial attack is performed on high-SNR radio-frequency (RF) data-points.

Method: 1. Design a VQVAE that maps digital radio waveforms into discrete latent space.
2. Create adversarial attacks targeting amplitude modulations while preserving the phase between in-phase and quadrature components.
3. Compare these attacks with those where phase is not preserved.
4. Test classification accuracy on adversarial examples using a classifier trained on original data.
5. Evaluate classifier accuracy on VQVAE reconstructions of adversarial data points.
6. Compare I/Q plane diagrams of attacked data, reconstructions, and original data.
7. Compare probability distributions of VQVAE latent space with and without attacks.

Result: VQVAE substantially decreases the effectiveness of adversarial attacks as shown by improved classifier accuracy on reconstructions. Interesting properties of the discrete latent space are observed when varying attack strength, which may help detect such attacks.

Conclusion: VQVAE exhibits strong potential in suppressing adversarial attacks on RF data, providing insights into detecting such attacks through analysis of the latent space.

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [101] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise,Vijeth Hebbar,Riya Shah,Cedric Langbort*

Main category: cs.LG

TL;DR: This paper discusses an approach named Diverse Near-Optimal Alternatives (DNA) for explainable Reinforcement Learning. DNA generates a set of diverse policies to provide different trajectory options in Euclidean space, using reward shaping and Q-learning methods. It is applicable to value function-based policies on Markov decision processes with continuous trajectories.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to improve the explainability of reinforcement learning by generating a set of diverse and reasonable options that an agent could take, allowing human users to understand and choose from these options.

Method: The method used in this paper is called Diverse Near-Optimal Alternatives (DNA). This method applies reward shaping in local, modified Q-learning problems to solve for distinct policies with guaranteed epsilon-optimality. The policies generated are qualitatively diverse and constitute meaningfully different 'options' for trajectory planning.

Result: The result shows that DNA successfully returns qualitatively different policies that can be considered as meaningfully different 'options' in simulation. These policies were also compared to related approaches in the stochastic optimization field of Quality Diversity.

Conclusion: The conclusion drawn from this work is that the DNA approach not only enhances the explainability of reinforcement learning but also opens new possibilities for exploration and adaptive planning in RL.

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [102] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang,James Joshi,Ashish Kundu*

Main category: cs.LG

TL;DR: An abstract about a novel privacy attack, Apollo, towards Machine Unlearning (MU). Apollo can infer if a data sample has been unlearned with high precision under a strict threat model where the adversary only accesses the label-output of the unlearned model.


<details>
  <summary>Details</summary>
Motivation: Machine Unlearning (MU) is aimed to update ML models without retraining from scratch when removing training samples. However, MU could increase the attack surface of the model. Existing privacy inference attacks on MU have limitations because they assume the attacker has access to both the unlearned model and the original model.

Method: Propose Apollo, A Posteriori Label-Only Membership Inference Attack towards MU. This attack infers whether a data sample has been unlearned under a strict threat model where the adversary only has access to the label-output of the unlearned model.

Result: Apollo requires less access to the target model compared to previous attacks and can achieve relatively high precision on the membership status of the unlearned samples.

Conclusion: Apollo presents a new privacy risk for Machine Unlearning methods under more realistic attack scenarios.

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [103] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu,Xiangxiang Weng*

Main category: cs.LG

TL;DR: This paper explores the application of Probabilistic Matrix Factorization (PMF) in recommendation systems, utilizing Bayesian inference methods MCMC and VI to approximate posterior distributions. Experiments on MovieLens dataset reveal that VI converges faster while MCMC yields more accurate posterior estimates.


<details>
  <summary>Details</summary>
Motivation: To address the intractability of computing posterior distribution in PMF due to high-dimensional integral, thus enhancing the performance and accuracy in recommendation systems.

Method: Employ two Bayesian inference methods, MCMC and VI, to approximate posterior distribution in PMF for recommendation systems.

Result: VI offers faster convergence, whereas MCMC provides more accurate posterior estimates when evaluated on MovieLens dataset.

Conclusion: Both MCMC and VI have their strengths in approximating posterior distribution in PMF; VI is preferable for faster convergence, while MCMC is better for obtaining more accurate posterior estimates.

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [104] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu,Rui Ai,Han Zhong,Xiaoyu Chen,Liwei Wang,Zhaoran Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: The paper explores the use of non-i.i.d. actions for learning about confounders in multi-agent systems with information asymmetry, while addressing knowledge transfer challenges in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Information asymmetry and knowledge transportability are significant challenges in multi-agent systems, particularly in economics and social sciences, where agents act strategically based on private information.

Method: A sample-efficient algorithm is introduced to accurately identify system dynamics under information asymmetry and address knowledge transfer issues in reinforcement learning through an online strategic interaction model.

Result: The method achieves learning of an $\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

Conclusion: This work demonstrates the feasibility of using non-i.i.d. actions to learn about confounders in environments with information asymmetry and requiring knowledge transfer.

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [105] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: Conditional diffusion models (CDMs) have the ability to model full data distribution but entangle class-defining features with irrelevant context. Canonical LAtent Representations (CLAReps) are identified as latent codes preserving essential categorical information and discarding non-discriminative signals. A novel diffusion-based feature-distillation paradigm, CaDistill, is developed exploiting CLAReps.


<details>
  <summary>Details</summary>
Motivation: To extract robust and interpretable representations from CDMs by disentangling class-defining features from irrelevant context.

Method: Identify CLAReps that preserve essential categorical information and discard non-discriminative signals. Develop CaDistill, a diffusion-based feature-distillation paradigm, where the CDM as teacher transfers core class knowledge only via CLAReps.

Result: The student model achieves strong adversarial robustness and generalization ability, focusing more on class signals instead of spurious background cues.

Conclusion: CDMs can serve as compact, interpretable teachers driving robust representation learning.

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [106] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang,Yuwei An,Hongyi Liu,Tianqi Chen,Beidi Chen*

Main category: cs.LG

TL;DR: Multiverse is a new generative model enabling parallel generation via a MapReduce paradigm. It consists of three stages: Map, Process, and Reduce. The researchers built a real-world Multiverse reasoning model with co-design of data, algorithm, and system, creating structured training data using an automated LLM-assisted pipeline. Algorithmically, they designed Multiverse Attention for efficient training. Systematically, they implemented Multiverse Engine for parallel inference. After fine-tuning, Multiverse-32B outperforms leading AR-LLMs in efficiency while maintaining comparable performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the implicit parallelism in sequential generation exhibited by Autoregressive Large Language Models (AR-LLMs) and develop a model that enables natively parallel generation.

Method: The method involves introducing the Multiverse model which incorporates a MapReduce paradigm with three stages: Map for adaptive task decomposition, Process for parallel subtask execution, and Reduce for lossless result synthesis. They also build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, create structured training data using an automated LLM-assisted pipeline, design Multiverse Attention for separating parallel reasoning steps, and implement Multiverse Engine for parallel inference.

Result: After a 3-hour fine-tuning with 1K examples, Multiverse-32B achieves performance on par with leading AR-LLMs of the same scale as evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Additionally, it exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length, and achieves up to 2x speedup across varying batch sizes.

Conclusion: Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs while offering practical efficiency gains. The entire Multiverse ecosystem has been open-sourced, including data, model weights, engine, supporting tools, data curation prompts, and detailed training and evaluation recipes.

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


### [107] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: Large language models (LLMs) have difficulty generating reliable samples despite accurately describing probability distributions. This paper introduces Verbalized Rejection Sampling (VRS), a method that leverages natural language to improve sampling from Bernoulli distributions, demonstrating reduced bias and improved reliability without needing access to model internals.


<details>
  <summary>Details</summary>
Motivation: LLMs can describe probability distributions well but struggle to generate faithful samples, which limits their applicability in tasks requiring stochasticity like Monte Carlo methods and simulations.

Method: The paper proposes Verbalized Rejection Sampling (VRS), a technique based on classical rejection sampling, where LLMs use natural language to reason about and accept or reject proposed samples from Bernoulli distributions.

Result: VRS significantly reduces sampling bias across different models and improves over direct sampling due to the algorithm's design and prompt structure.

Conclusion: This study highlights how probabilistic tools can be integrated into LLM workflows through natural language, enhancing reliability without heavy prompt engineering or access to model internals.

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [108] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: 提出了一种名为动态上下文扰动（DCP）的新颖对抗文本攻击方案，通过结合预训练语言模型和迭代优化过程，生成语义一致且难以察觉的对抗样本，挑战了最先进的NLP系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击方法主要集中在词级或局部文本片段的修改上，忽略了更广泛的上下文信息，导致生成的对抗样本容易被检测到或语义不一致。

Method: DCP利用预训练语言模型的能力，通过一个对抗目标函数来平衡诱导模型误分类和保持文本自然性的双重目标，动态地在句子、段落和文档级别生成上下文感知的扰动。该方法包括迭代细化扰动的过程，以确保语义保真度和流畅度。

Result: 实验结果表明，DCP在多个NLP模型和数据集上有效地挑战了最先进的NLP系统的鲁棒性，显著增强了对抗攻击的隐蔽性和影响力。

Conclusion: 本研究强调了上下文在对抗攻击中的关键作用，并为构建能够抵御复杂对抗策略的更鲁棒的NLP系统奠定了基础。

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


### [109] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Main category: cs.CR

TL;DR: 尽管生成轨迹数据的深度学习模型很有前景，但加入差分隐私（DP）会显著影响效用。本文研究了DP-SGD对生成模型的影响，并提出了一种新的有条件生成的DP机制，发现不同模型在有无DP条件下的表现各异，强调了DP轨迹生成任务的挑战性及对大数据集的需求。


<details>
  <summary>Details</summary>
Motivation: 位置轨迹提供了有价值的信息，但也可能泄露敏感个人信息，因此需要在效用和隐私之间找到平衡。现有的生成模型虽然有效，但缺乏正式的隐私保护。

Method: 评估DP-SGD对生成模型效用的影响；提出一种新的有条件生成的DP机制并分析其效用；比较扩散模型、VAE和GAN在不同隐私条件下的效用-隐私权衡。

Result: DP-SGD显著影响模型性能，但在足够大的数据集上仍保留一定效用；新提出的DP机制提高了训练稳定性，特别是在小数据集和不稳定模型（如GAN）中；扩散模型在无隐私保证下效用最佳，而GAN在加入DP-SGD后表现最优。

Conclusion: 在轨迹生成中实现差分隐私仍然是一个挑战，正式的隐私保证目前仅适用于大数据集和受限使用场景。

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [110] [DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353)
*Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.CR

TL;DR: The paper presents DAVSP, a method using Visual Safety Prompt and Deep Alignment to enhance the resistance of LVLMs to malicious queries while maintaining utility on benign inputs.


<details>
  <summary>Details</summary>
Motivation: LVLMs are vulnerable to malicious queries that exploit the visual modality. Existing alignment approaches fail to effectively resist malicious queries while preserving utility on benign ones.

Method: DAVSP is built upon two key innovations: 1) Visual Safety Prompt which appends a trainable padding region around the input image to preserve visual features and expand the optimization space; 2) Deep Alignment which trains the visual safety prompt through supervision in the model's activation space to enhance the inherent ability of LVLMs to perceive malicious queries.

Result: Extensive experiments across five benchmarks on two representative LVLMs demonstrate that DAVSP effectively resists malicious queries while preserving benign input utility. DAVSP also exhibits great cross-model generation ability.

Conclusion: Both the Visual Safety Prompt and Deep Alignment are essential components of DAVSP, jointly contributing to its overall effectiveness.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress across
various applications but remain vulnerable to malicious queries that exploit
the visual modality. Existing alignment approaches typically fail to resist
malicious queries while preserving utility on benign ones effectively. To
address these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),
which is built upon two key innovations. First, we introduce the Visual Safety
Prompt, which appends a trainable padding region around the input image. It
preserves visual features and expands the optimization space. Second, we
propose Deep Alignment, a novel approach to train the visual safety prompt
through supervision in the model's activation space. It enhances the inherent
ability of LVLMs to perceive malicious queries, achieving deeper alignment than
prior works. Extensive experiments across five benchmarks on two representative
LVLMs demonstrate that DAVSP effectively resists malicious queries while
preserving benign input utility. Furthermore, DAVSP exhibits great cross-model
generation ability. Ablation studies further reveal that both the Visual Safety
Prompt and Deep Alignment are essential components, jointly contributing to its
overall effectiveness. The code is publicly available at
https://github.com/zhangyitonggg/DAVSP.

</details>


### [111] [ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)](https://arxiv.org/abs/2506.09365)
*Ronal Singh,Mohan Baruwal Chhetri,Surya Nepal,Cecile Paris*

Main category: cs.CR

TL;DR: ContextBuddy是一个AI助手，通过学习分析师先前的调查来帮助他们确定新警报的最相关上下文。在实验和用户研究中，ContextBuddy提高了分类准确性和决策信心，减少了错误分类，并缩短了警报验证时间。


<details>
  <summary>Details</summary>
Motivation: 现代安全运营中心(SOCs)集成了多种工具，提供了丰富的上下文数据，但分析师仍需手动确定哪些上下文线索与特定警报最相关。

Method: 将上下文选择建模为顺序决策问题，并应用模仿学习(IL)捕捉分析师的策略。使用两个入侵检测数据集(HIKARI-2021, UNSW-NB15)进行评估。

Result: 在模拟实验中，ContextBuddy提高了分类准确性（HIKARI提高2.5%，UNSW提高9%），减少了假阴性，并保持假阳性低于1%。在用户研究中，非专家使用ContextBuddy时，分类准确性提高了21.1%，警报验证时间减少了24%。

Conclusion: 通过学习分析师的上下文选择模式，ContextBuddy可以显著提高调查的有效性和效率。

Abstract: Modern Security Operations Centres (SOCs) integrate diverse tools, such as
SIEM, IDS, and XDR systems, offering rich contextual data, including alert
enrichments, flow features, and similar case histories. Yet, analysts must
still manually determine which of these contextual cues are most relevant when
validating specific alerts. We introduce ContextBuddy, an AI assistant that
learns from analysts' prior investigations to help them identify the most
relevant context for new alerts. Rather than providing enrichments,
ContextBuddy models how analysts have previously selected context and suggests
tailored cues based on the characteristics of each alert. We formulate context
selection as a sequential decision-making problem and apply imitation learning
(IL) to capture analysts' strategies, evaluating multiple IL approaches.
Through staged evaluation, we validate ContextBuddy using two intrusion
detection datasets (HIKARI-2021, UNSW-NB15). In simulation-based experiments,
ContextBuddy helped simulated reinforcement learning analysts improve
classification accuracy (p < 0.001) (increasing F1 by 2.5% for HIKARI and 9%
for UNSW), reducing false negatives (1.5% for HIKARI and 10% for UNSW), and
keeping false positives below 1%. Decision confidence among agents also
improved by 2-3% (p < 0.001). In a within-subject user study (N=13; power =
0.8), non-experts using ContextBuddy improved classification accuracy by 21.1%
(p = 0.008) and reduced alert validation time by 24% (p = 0.01). These results
demonstrate that by learning context-selection patterns from analysts,
ContextBuddy can yield notable improvements in investigation effectiveness and
efficiency.

</details>


### [112] [Epass: Efficient and Privacy-Preserving Asynchronous Payment on Blockchain](https://arxiv.org/abs/2506.09387)
*Weijie Wang,Jinwen Liang,Chuan Zhang,Ximeng Liu,Liehuang Zhu,Song Guo*

Main category: cs.CR

TL;DR: 提出了一种有效且保护隐私的区块链异步支付方案(Epass)，解决了BNPL服务中的隐私和时间开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有的BNPL平台存在隐私泄露风险和时间开销大的问题，需要一种新的支付方案来解决这些问题。

Method: Epass利用本地可验证签名和时间释放加密技术，构建了一个隐私保护的异步支付方案，并通过修改延迟支付交易减少时间开销。

Result: 广泛的比较和实验分析表明，Epass实现了KB级别的通信成本，并将时间开销减少了四倍以上。

Conclusion: Epass在保护消费者交易隐私的同时，提高了BNPL服务的可扩展性。

Abstract: Buy Now Pay Later (BNPL) is a rapidly proliferating e-commerce model,
offering consumers to get the product immediately and defer payments.
Meanwhile, emerging blockchain technologies endow BNPL platforms with digital
currency transactions, allowing BNPL platforms to integrate with digital
wallets. However, the transparency of transactions causes critical privacy
concerns because malicious participants may derive consumers' financial
statuses from on-chain asynchronous payments. Furthermore, the newly created
transactions for deferred payments introduce additional time overheads, which
weaken the scalability of BNPL services. To address these issues, we propose an
efficient and privacy-preserving blockchain-based asynchronous payment scheme
(Epass), which has promising scalability while protecting the privacy of
on-chain consumer transactions. Specifically, Epass leverages locally
verifiable signatures to guarantee the privacy of consumer transactions against
malicious acts. Then, a privacy-preserving asynchronous payment scheme can be
further constructed by leveraging time-release encryption to control trapdoors
of redactable blockchain, reducing time overheads by modifying transactions for
deferred payment. We give formal definitions and security models, generic
structures, and formal proofs for Epass. Extensive comparisons and experimental
analysis show that \textsf{Epass} achieves KB-level communication costs, and
reduces time overhead by more than four times in comparisons with locally
verifiable signatures and Go-Ethereum private test networks.

</details>


### [113] [Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G](https://arxiv.org/abs/2506.09418)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.CR

TL;DR: O-RAN brings modularity and flexibility to 5G but also new security challenges. This review synthesizes recent research on vulnerabilities, cryptographic tools, and emerging testbeds in O-RAN, concluding with future research directions.


<details>
  <summary>Details</summary>
Motivation: To examine the novel security challenges brought by O-RAN in 5G deployments and evaluate cryptographic tools and emerging technologies for addressing these challenges.

Method: Literature review synthesizing research from thirteen academic and industry sources on vulnerabilities, cryptographic tools, and emerging testbeds in O-RAN.

Result: Identification of key vulnerabilities such as cipher bidding-down attacks, partial encryption exposure, and performance trade-offs. Evaluation of cryptographic tools like SNOW-V, AES-256, and ZUC-256 for throughput, side-channel resilience, and adaptability. Emphasis on AI-driven controllers for dynamic orchestration and anomaly detection.

Conclusion: Future research should focus on hardware offloading, cross-layer cipher adaptation, and alignment with 3GPP and O-RAN security mandates, pointing towards integrated, zero-trust architectures in 6G.

Abstract: The advent of Open Radio Access Networks (O-RAN) introduces modularity and
flexibility into 5G deployments but also surfaces novel security challenges
across disaggregated interfaces. This literature review synthesizes recent
research across thirteen academic and industry sources, examining
vulnerabilities such as cipher bidding-down attacks, partial encryption
exposure on control/user planes, and performance trade-offs in securing O-RAN
interfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,
AES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,
and adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is
placed on emerging testbeds and AI-driven controllers that facilitate dynamic
orchestration, anomaly detection, and secure configuration. We conclude by
outlining future research directions, including hardware offloading,
cross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN
Alliance security mandates, all of which point toward the need for integrated,
zero-trust architectures in 6G.

</details>


### [114] [LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge](https://arxiv.org/abs/2506.09443)
*Songze Li,Chuokun Xu,Jiaying Wang,Xueluan Gong,Chen Chen,Jirui Zhang,Jun Wang,Kwok-Yan Lam,Shouling Ji*

Main category: cs.CR

TL;DR: This paper introduces RobustJudge, a framework to evaluate the robustness of LLM-as-a-Judge systems. It explores attack methods, defense strategies, prompt templates, and model selections, revealing vulnerabilities in current systems and providing solutions.


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-Judge systems are increasingly used but are susceptible to adversarial attacks, which affects their trustworthiness. Existing evaluation methods lack comprehensiveness and a unified framework.

Method: The paper proposes RobustJudge, an automated and scalable framework that investigates the impact of attack methods and defense strategies, the influence of prompt template and model selection, and the robustness of real-world LLM-as-a-Judge applications.

Result: 1) LLM-as-a-Judge systems are vulnerable to adversarial attacks but can be better protected by certain defense mechanisms; 2) Robustness is highly sensitive to prompt template and judge model choices, with proposed optimizations improving performance; 3) Applying RobustJudge to Alibaba's PAI platform revealed new vulnerabilities.

Conclusion: RobustJudge provides a systematic way to assess and improve the robustness of LLM-as-a-Judge systems, highlighting areas for improvement and offering practical solutions.

Abstract: Large Language Models (LLMs) have demonstrated remarkable intelligence across
various tasks, which has inspired the development and widespread adoption of
LLM-as-a-Judge systems for automated model testing, such as red teaming and
benchmarking. However, these systems are susceptible to adversarial attacks
that can manipulate evaluation outcomes, raising concerns about their
robustness and, consequently, their trustworthiness. Existing evaluation
methods adopted by LLM-based judges are often piecemeal and lack a unified
framework for comprehensive assessment. Furthermore, prompt template and model
selections for improving judge robustness have been rarely explored, and their
performance in real-world settings remains largely unverified. To address these
gaps, we introduce RobustJudge, a fully automated and scalable framework
designed to systematically evaluate the robustness of LLM-as-a-Judge systems.
RobustJudge investigates the impact of attack methods and defense strategies
(RQ1), explores the influence of prompt template and model selection (RQ2), and
assesses the robustness of real-world LLM-as-a-Judge applications (RQ3).Our
main findings are: (1) LLM-as-a-Judge systems are still vulnerable to a range
of adversarial attacks, including Combined Attack and PAIR, while defense
mechanisms such as Re-tokenization and LLM-based Detectors offer improved
protection; (2) Robustness is highly sensitive to the choice of prompt template
and judge models. Our proposed prompt template optimization method can improve
robustness, and JudgeLM-13B demonstrates strong performance as a robust
open-source judge; (3) Applying RobustJudge to Alibaba's PAI platform reveals
previously unreported vulnerabilities. The source code of RobustJudge is
provided at https://github.com/S3IC-Lab/RobustJudge.

</details>


### [115] [Efficient Modular Multiplier over GF (2^m) for ECPM](https://arxiv.org/abs/2506.09464)
*Ruby Kumari,Gaurav Purohit,Abhijit Karmakar*

Main category: cs.CR

TL;DR: This paper introduces a hybrid multiplication technique for modular multiplication over binary field GF(2m) in ECC systems, optimizing the combination of conventional and Karatsuba multiplication methods. It targets NIST parameters (B-163, 233, 283, 571) and demonstrates significant improvements in speed, hardware efficiency, and resource utilization.


<details>
  <summary>Details</summary>
Motivation: Elliptic curve cryptography (ECC) has become dominant in public-key protocols, with NIST standardizing parameters for binary field ECC systems. There is a need for more efficient hardware implementations to enhance performance in terms of speed and resource utilization.

Method: The paper proposes a hybrid multiplication technique that uses conventional multiplication for smaller operands (up to 41 bits for m=163) and Karatsuba multiplication for larger ones. This method aims to optimize elliptic curve point multiplication by reducing computational complexity.

Result: The hybrid design significantly reduces LUT usage and improves delay performance compared to conventional methods. For example, it achieves a 39.82% reduction in LUTs for m=163 and maintains lower delays. The Area-Delay Product also shows substantial improvements, demonstrating enhanced efficiency.

Conclusion: The hybrid multiplication technique successfully improves speed, hardware efficiency, and resource utilization for ECC cryptographic systems, making it a promising approach for optimizing ECC implementations.

Abstract: Elliptic curve cryptography (ECC) has emerged as the dominant public-key
protocol, with NIST standardizing parameters for binary field GF(2^m) ECC
systems. This work presents a hardware implementation of a Hybrid
Multiplication technique for modular multiplication over binary field GF(2m),
targeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the
combination of conventional multiplication (CM) and Karatsuba multiplication
(KM) to enhance elliptic curve point multiplication (ECPM). The key innovation
uses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones,
reducing computational complexity and enhancing efficiency. The design is
evaluated in three areas: Resource Utilization For m=163, the hybrid design
uses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For
m=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and
bit-parallel implementations. Delay Performance For m=163, achieves 13.31ns
delay, improving by 37.60% over bit-parallel implementations. For m=233,
maintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860,
outperforming bit-parallel (75,337) and digit-serial (43,179) implementations.
For m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over
bit-parallel designs. Results show the hybrid technique significantly improves
speed, hardware efficiency, and resource utilization for ECC cryptographic
systems.

</details>


### [116] [The Secure Overview and Analysis OF 3GPP MAC CE](https://arxiv.org/abs/2506.09502)
*Jin Cao,Yuanyuan Yang,Ruhui Ma,Sheng Li,Hui Li*

Main category: cs.CR

TL;DR: This paper explores the security threats to MAC CE and corresponding protection mechanisms, aiming to enhance the security and reliability of the communication system.


<details>
  <summary>Details</summary>
Motivation: MAC CE lacks encryption and integrity protection mechanisms provided by PDCP, making it vulnerable to interception or tampering during resource scheduling and allocation. There may be other potential security vulnerabilities in protocol procedures beyond LTM.

Method: The paper analyzes the security risks associated with MAC CE, identifies the vulnerabilities, and proposes corresponding protection mechanisms.

Result: The research provides insights into the security threats to MAC CE and suggests ways to protect against them, contributing to the enhancement of the entire communication system's security.

Conclusion: This study is expected to support 3GPP's research on MAC CE and integrate with the security analysis of lower-layer protocols, improving overall network security.

Abstract: To more effectively control and allocate network resources, MAC CE has been
introduced into the network protocol, which is a type of control signaling
located in the MAC layer. Since MAC CE lacks encryption and integrity
protection mechanisms provided by PDCP, the control signaling carried by MAC CE
is vulnerable to interception or tampering by attackers during resource
scheduling and allocation. Currently, the 3GPP has analyzed the security risks
of Layer 1/Layer 2 Triggered Mobility (LTM), where handover signaling sent to
the UE via MAC CE by the network can lead to privacy leaks and network attacks.
However, in addition to LTM, there may be other potential security
vulnerabilities in other protocol procedures. Therefore, this paper explores
the security threats to MAC CE and the corresponding protection mechanisms. The
research is expected to support the 3GPP's study of MAC CE and be integrated
with the security research of lower-layer protocols, thereby enhancing the
security and reliability of the entire communication system.

</details>


### [117] [Beyond Personalization: Federated Recommendation with Calibration via Low-rank Decomposition](https://arxiv.org/abs/2506.09525)
*Jundong Chen,Honglei Zhang,Haoxuan Li,Chunxu Zhang,Zhiwei Li,Yidong Li*

Main category: cs.CR

TL;DR: In federated recommendation (FR), globally aggregated item embeddings can cause user embedding skew, leading to suboptimal performance. To address this, the authors propose PFedCLR, which uses a dual-function mechanism with a buffer matrix for calibration and personalization while ensuring efficiency and privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to solve the issue of user embedding skew in federated recommendation systems, which arises from globally aggregated item embeddings and leads to suboptimal performance.

Method: The method proposed is called Personalized Federated recommendation with Calibration via Low-Rank decomposition (PFedCLR). It introduces an integrated dual-function mechanism using a buffer matrix to calibrate local user embeddings and personalize global item embeddings. A low-rank decomposition of the buffer matrix reduces model overhead, enhancing efficiency. Privacy is ensured by training and uploading the local model before personalization, preventing the server from accessing sensitive information.

Result: Extensive experiments show that PFedCLR effectively mitigates user embedding skew and achieves a balance among performance, efficiency, and privacy, outperforming state-of-the-art methods.

Conclusion: PFedCLR addresses the user embedding skew issue in federated recommendation, providing improved performance while maintaining efficiency and privacy.

Abstract: Federated recommendation (FR) is a promising paradigm to protect user privacy
in recommender systems. Distinct from general federated scenarios, FR
inherently needs to preserve client-specific parameters, i.e., user embeddings,
for privacy and personalization. However, we empirically find that globally
aggregated item embeddings can induce skew in user embeddings, resulting in
suboptimal performance. To this end, we theoretically analyze the user
embedding skew issue and propose Personalized Federated recommendation with
Calibration via Low-Rank decomposition (PFedCLR). Specifically, PFedCLR
introduces an integrated dual-function mechanism, implemented with a buffer
matrix, to jointly calibrate local user embedding and personalize global item
embeddings. To ensure efficiency, we employ a low-rank decomposition of the
buffer matrix to reduce the model overhead. Furthermore, for privacy, we train
and upload the local model before personalization, preventing the server from
accessing sensitive information. Extensive experiments demonstrate that PFedCLR
effectively mitigates user embedding skew and achieves a desirable trade-off
among performance, efficiency, and privacy, outperforming state-of-the-art
(SOTA) methods.

</details>


### [118] [Identity and Access Management for the Computing Continuum](https://arxiv.org/abs/2506.09559)
*Chalima Dimitra Nassar Kyriakidou,Athanasia Maria Papathanasiou,Vasilios A. Siris,Nikos Fotiou,George C. Polyzos,Eduardo Cánovas Martínez,Antonio Skarmeta*

Main category: cs.CR

TL;DR: The paper proposes a Zero-Trust access control solution using DIDs, VCs, and ReBAC for the computing continuum.


<details>
  <summary>Details</summary>
Motivation: The dynamic, distributed, and heterogeneous nature of the computing continuum presents new challenges for access control.

Method: Proposes a Zero-Trust access control solution that uses decentralized identification and authentication mechanisms based on Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). Also employs Relationship-Based Access Control (ReBAC) to define policies capturing evolving trust relationships.

Result: Through proof-of-concept implementation, demonstrates the feasibility and efficiency of the proposed solution.

Conclusion: The proposed solution has the potential to enhance security and trust in decentralized environments.

Abstract: The computing continuum introduces new challenges for access control due to
its dynamic, distributed, and heterogeneous nature. In this paper, we propose a
Zero-Trust (ZT) access control solution that leverages decentralized
identification and authentication mechanisms based on Decentralized Identifiers
(DIDs) and Verifiable Credentials (VCs). Additionally, we employ
Relationship-Based Access Control (ReBAC) to define policies that capture the
evolving trust relationships inherent in the continuum. Through a
proof-of-concept implementation, we demonstrate the feasibility and efficiency
of our solution, highlighting its potential to enhance security and trust in
decentralized environments.

</details>


### [119] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Main category: cs.CR

TL;DR: In this paper, the authors introduce TooBadRL, a framework to optimize triggers for backdoor attacks in deep reinforcement learning (DRL). It enhances attack success rates while maintaining normal task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is that most existing backdoor attacks on DRL systems rely on simplistic and heuristic trigger configurations. This paper aims to explore the potential efficacy of trigger optimization in DRL backdoor attacks.

Method: TooBadRL optimizes DRL backdoor triggers along three axes - temporal, spatial, and magnitude. Specifically, it introduces a performance-aware adaptive freezing mechanism for injection timing, formulates dimension selection as a cooperative game using Shapley value analysis, and proposes a gradient-based adversarial procedure for optimizing injection magnitude under environment constraints.

Result: Evaluations on three mainstream DRL algorithms and nine benchmark tasks show that TooBadRL significantly improves attack success rates while ensuring minimal degradation of normal task performance.

Conclusion: This study highlights the importance of principled trigger optimization in DRL backdoor attacks.

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


### [120] [The Rabin cryptosystem over number fields](https://arxiv.org/abs/2506.09569)
*Alessandro Cobbe,Andreas Nickel,Akay Schuster*

Main category: cs.CR

TL;DR: An extension of Rabin's cryptosystem to general number fields is presented, proving decryption as hard as integer factorization with a carefully chosen modulus. Performance is compared with classical Rabin and Gaussian integer versions.


<details>
  <summary>Details</summary>
Motivation: To enhance the security and efficiency of Rabin's cryptosystem by extending it to general number fields for broader applications.

Method: Extended Rabin's cryptosystem to general number fields and analyzed its decryption hardness in relation to integer factorization problem with a carefully selected modulus.

Result: The decryption difficulty matches the integer factorization problem, ensuring robust security. The new cryptosystem performs comparably to existing versions while offering extended applicability.

Conclusion: The generalized Rabin cryptosystem over number fields provides a secure alternative with performance on par with established methods.

Abstract: We extend Rabin's cryptosystem to general number fields. We show that
decryption of a random plaintext is as hard as the integer factorisation
problem, provided the modulus in our scheme has been chosen carefully. We
investigate the performance of our new cryptosystem in comparison with the
classical Rabin scheme and a more recent version over the Gaussian integers.

</details>


### [121] [The Everyday Security of Living with Conflict](https://arxiv.org/abs/2506.09580)
*Jessica McClearn,Reem Talhouk,Rikke Bjerg Jensen*

Main category: cs.CR

TL;DR: This paper shifts focus from the technological aspects of war and security to everyday experiences of communities living with and fleeing from war, using ethnographic research in Colombia, Lebanon, and Sweden.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a different perspective on understanding security by highlighting mundane experiences instead of focusing solely on the technological and spectacular aspects of war and conflict.

Method: The method involves presenting three vignettes based on field research conducted in Colombia, Lebanon, and Sweden to demonstrate the significance of ethnography in security research.

Result: The result is an emphasis on the importance of considering lived experiences of communities in designing security technology for global conflict and disaster regions.

Conclusion: The conclusion is a call to action for security researchers and practitioners to incorporate such lived experiences when developing security technology for communities affected by war.

Abstract: When `cyber' is used as a prefix, attention is typically drawn to the
technological and spectacular aspects of war and conflict -- and, by extension,
security. We offer a different approach to engaging with and understanding
security in such contexts, by foregrounding the everyday -- mundane --
experiences of security within communities living with and fleeing from war. We
do so through three vignettes from our field research in Colombia, Lebanon and
Sweden, respectively, and by highlighting the significance of ethnography for
security research with communities living in regions afflicted by war. We
conclude by setting out a call to action for security researchers and
practitioners to consider such lived experiences in the design of security
technology that aims to cater to the needs of communities in `global conflict
and disaster regions'.

</details>


### [122] [Empirical Quantification of Spurious Correlations in Malware Detection](https://arxiv.org/abs/2506.09662)
*Bianca Perasso,Ludovico Lozza,Andrea Ponte,Luca Demetrio,Luca Oneto,Fabio Roli*

Main category: cs.CR

TL;DR: 本研究通过分析小规模平衡数据集，探讨了深度学习在恶意软件检测中对编译器留下的空白空间的依赖程度，量化了这种虚假相关性对决策的影响，并对两种端到端模型进行了排名，以确定哪种更适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究所指出深度神经网络在恶意软件检测中主要关注元数据，但没有进一步研究这些虚假相关性的具体影响及其量化作用。

Method: 研究者分析了一个小规模平衡数据集，评估了模型对编译器留下的空格的依赖程度，并对两个端到端模型进行了排名。

Result: 研究表明，深度学习模型在恶意软件检测中高度依赖编译器留下的空白空间，而非代码本身的相关性。同时，研究者成功对两种模型进行了排序，揭示了其在生产环境中的适用性差异。

Conclusion: 虚假相关性显著影响了深度学习模型在恶意软件检测中的表现，研究结果为选择更合适的模型提供了依据。

Abstract: End-to-end deep learning exhibits unmatched performance for detecting
malware, but such an achievement is reached by exploiting spurious correlations
-- features with high relevance at inference time, but known to be useless
through domain knowledge. While previous work highlighted that deep networks
mainly focus on metadata, none investigated the phenomenon further, without
quantifying their impact on the decision. In this work, we deepen our
understanding of how spurious correlation affects deep learning for malware
detection by highlighting how much models rely on empty spaces left by the
compiler, which diminishes the relevance of the compiled code. Through our
seminal analysis on a small-scale balanced dataset, we introduce a ranking of
two end-to-end models to better understand which is more suitable to be put in
production.

</details>


### [123] [On the Virtues of Information Security in the UK Climate Movement](https://arxiv.org/abs/2506.09719)
*Mikaela Brough,Rikke Bjerg Jensen,Martin R. Albrecht*

Main category: cs.CR

TL;DR: An ethnographic study in the UK climate movement reveals social complexities in information-security research among activists, including tensions between openness and secrecy, autonomy vs. collective interdependence, conflicting ideals, and pressures from various social gazes.


<details>
  <summary>Details</summary>
Motivation: To understand the social complexities of information-security research within activist settings, specifically focusing on the climate movement in the UK.

Method: Ethnographic study involving participant observation and interviews conducted at protests and various activist settings.

Result: Members of the UK climate movement faced four main challenges: a fundamental tension between openness and secrecy, tensions between autonomy and collective interdependence in information-security decision-making, conflicting activist ideals shaping security discourses, and pressures from different social gazes.

Conclusion: The findings highlight the intricate social dynamics involved in information-security among activists and raise methodological questions for designing programs aimed at supporting activists.

Abstract: We report on an ethnographic study with members of the climate movement in
the United Kingdom (UK). We conducted participant observation and interviews at
protests and in various activist settings. Reporting on the findings as they
relate to information security, we show that members of the UK climate movement
wrestled with (i) a fundamental tension between openness and secrecy; (ii)
tensions between autonomy and collective interdependence in
information-security decision-making; (iii) conflicting activist ideals that
shape security discourses; and (iv) pressures from different social gazes --
from each other, from people outside the movement and from their adversaries.
Overall, our findings shed light on the social complexities of
information-security research in activist settings and provoke methodological
questions about programmes that aim to design for activists.

</details>


### [124] [Physical Layer-Based Device Fingerprinting for Wireless Security: From Theory to Practice](https://arxiv.org/abs/2506.09807)
*Junqing Zhang,Francesco Ardizzon,Mattia Piana,Guanxiong Shen,Stefano Tomasin*

Main category: cs.CR

TL;DR: The paper surveys physical layer-based device fingerprinting as an emerging authentication method for wireless security in IoT, focusing on hardware impairment and channel features based authentication, reviewing their features, methodologies, scenarios, challenges, and suggesting future work.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient and low-cost authentication mechanisms in IoT due to the limitations of conventional cryptography-based methods which are computationally expensive for resource-constrained devices.

Method: The paper conducts a comprehensive survey on physical layer-based device fingerprinting techniques, particularly examining hardware impairment-based identity authentication and channel features-based authentication.

Result: The survey identifies the intrinsic hardware and channel features used, outlines algorithm design methodologies, describes application scenarios, and highlights key research questions. Remaining research challenges are also discussed.

Conclusion: Physical layer-based device fingerprinting is a promising passive technique for IoT device authentication. The paper suggests future research directions to enhance this approach.

Abstract: The identification of the devices from which a message is received is part of
security mechanisms to ensure authentication in wireless communications.
Conventional authentication approaches are cryptography-based, which, however,
are usually computationally expensive and not adequate in the Internet of
Things (IoT), where devices tend to be low-cost and with limited resources.
This paper provides a comprehensive survey of physical layer-based device
fingerprinting, which is an emerging device authentication for wireless
security. In particular, this article focuses on hardware impairment-based
identity authentication and channel features-based authentication. They are
passive techniques that are readily applicable to legacy IoT devices. Their
intrinsic hardware and channel features, algorithm design methodologies,
application scenarios, and key research questions are extensively reviewed
here. The remaining research challenges are discussed, and future work is
suggested that can further enhance the physical layer-based device
fingerprinting.

</details>


### [125] [Oracle-Based Multistep Strategy for Solving Polynomial Systems Over Finite Fields and Algebraic Cryptanalysis of the Aradi Cipher](https://arxiv.org/abs/2506.09950)
*La Scala Roberto,Sharwan Kumar Tiwari*

Main category: cs.CR

TL;DR: An improved multistep solving strategy based on Depth-First Search and oracle function is proposed for algebraic cryptanalysis. This method unifies previous approaches and is successfully applied to break the full rounds of Aradi cipher, questioning its security.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and applicability of the multistep solving strategy in algebraic cryptanalysis, particularly for ciphers like Trivium and the newly introduced Aradi cipher.

Method: The method involves a divide-and-conquer approach where one variable is assigned over the elements of the base finite field, recursively simplifying the polynomial system. A new implementation using Depth-First Search and an 'oracle function' for predicting necessary variable evaluations is introduced, unifying previous strategies.

Result: The multistep solving strategy was successfully applied to perform the first full round algebraic attack on the Aradi cipher, indicating potential security issues related to its key length.

Conclusion: The proposed multistep solving strategy with Depth-First Search and oracle function provides a unified framework for algebraic cryptanalysis, demonstrating effectiveness in breaking the Aradi cipher and raising concerns about its security.

Abstract: The multistep solving strategy consists in a divide-and-conquer approach:
when a multivariate polynomial system is computationally infeasible to solve
directly, one variable is assigned over the elements of the base finite field,
and the procedure is recursively applied to the resulting simplified systems.
In a previous work by the same authors (among others), this approach proved
effective in the algebraic cryptanalysis of the Trivium cipher. In this paper,
we present a new implementation of the corresponding algorithm based on a
Depth-First Search strategy, along with a novel complexity analysis leveraging
tree structures. We further introduce the notion of an "oracle function" as a
general predictive tool for deciding whether the evaluation of a new variable
is necessary to simplify the current polynomial system. This notion allows us
to unify all previously proposed variants of the multistep strategy, including
the classical hybrid approach, by appropriately selecting the oracle function.
Finally, we apply the multistep solving strategy to the cryptanalysis of the
low-latency block cipher Aradi, recently introduced by the NSA. We present the
first full round algebraic attack, raising concerns about the cipher's actual
security with respect to its key length.

</details>


### [126] [LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)
*Sahar Abdelnabi,Aideen Fay,Ahmed Salem,Egor Zverev,Kai-Chieh Liao,Chi-Huang Liu,Chun-Chih Kuo,Jannis Weigend,Danyael Manlangit,Alex Apostolov,Haris Umair,João Donato,Masayuki Kawakita,Athar Mahboob,Tran Huu Bach,Tsun-Han Chiang,Myeongjin Cho,Hajin Choi,Byeonghyeon Kim,Hyeonjin Lee,Benjamin Pannell,Conor McCauley,Mark Russinovich,Andrew Paverd,Giovanni Cherubin*

Main category: cs.CR

TL;DR: A public challenge named LLMail-Inject was conducted to evaluate the defense against indirect prompt injection attacks on Large Language Models (LLMs). It resulted in 208,095 unique attack submissions. The dataset and analysis are released to aid future research.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate defenses against indirect prompt injection attacks which exploit the limitation of LLMs to distinguish between instructions and data, potentially leading to security and privacy issues.

Method: The method involved conducting a public challenge called LLMail-Inject where participants adaptively attempted to inject malicious instructions into emails for an LLM-based email assistant. This spanned multiple defense strategies, LLM architectures, and retrieval configurations.

Result: The challenge generated a dataset of 208,095 unique attack submissions from 839 participants, providing insights into the instruction-data separation problem.

Conclusion: The authors hope that the released dataset and analysis will serve as a foundation for future research towards practical structural solutions to prompt injection.

Abstract: Indirect Prompt Injection attacks exploit the inherent limitation of Large
Language Models (LLMs) to distinguish between instructions and data in their
inputs. Despite numerous defense proposals, the systematic evaluation against
adaptive adversaries remains limited, even when successful attacks can have
wide security and privacy implications, and many real-world LLM-based
applications remain vulnerable. We present the results of LLMail-Inject, a
public challenge simulating a realistic scenario in which participants
adaptively attempted to inject malicious instructions into emails in order to
trigger unauthorized tool calls in an LLM-based email assistant. The challenge
spanned multiple defense strategies, LLM architectures, and retrieval
configurations, resulting in a dataset of 208,095 unique attack submissions
from 839 participants. We release the challenge code, the full dataset of
submissions, and our analysis demonstrating how this data can provide new
insights into the instruction-data separation problem. We hope this will serve
as a foundation for future research towards practical structural solutions to
prompt injection.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [127] [A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications](https://arxiv.org/abs/2506.09512)
*Donglin Wang,Anjie Qiu,Qiuheng Zhou,Hans D. Schotten*

Main category: eess.SY

TL;DR: The paper surveys recent advances in AI and ML models applied to 6G-V2X communication, focusing on DL, RL, GL, and FL techniques. It analyzes their roles in applications like intelligent resource allocation and traffic management while exploring technical challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a systematic summary of recent research efforts in applying AI and ML to 6G-V2X communication and provide valuable insights for researchers, engineers, and policymakers.

Method: Comprehensive review of state-of-the-art AI and ML techniques (DL, RL, GL, FL) applied to 6G-V2X systems over the past two years, analyzing their roles in various applications and identifying technical challenges.

Result: AI and ML, especially GL, have shown remarkable progress in enhancing the performance, adaptability, and intelligence of 6G-V2X systems. However, challenges such as computational complexity, data privacy, and real-time decision-making constraints remain.

Conclusion: This study provides valuable insights for advancing AI-driven 6G-V2X development, aiming to realize intelligent, AI-powered V2X ecosystems in 6G communication.

Abstract: The rapid advancement of Vehicle-to-Everything (V2X) communication is
transforming Intelligent Transportation Systems (ITS), with 6G networks
expected to provide ultra-reliable, low-latency, and high-capacity connectivity
for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and
Machine Learning (ML) have emerged as key enablers in optimizing V2X
communication by enhancing network management, predictive analytics, security,
and cooperative driving due to their outstanding performance across various
domains, such as natural language processing and computer vision. This survey
comprehensively reviews recent advances in AI and ML models applied to 6G-V2X
communication. It focuses on state-of-the-art techniques, including Deep
Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and
Federated Learning (FL), with particular emphasis on developments from the past
two years. Notably, AI, especially GL, has shown remarkable progress and
emerging potential in enhancing the performance, adaptability, and intelligence
of 6G-V2X systems. Despite these advances, a systematic summary of recent
research efforts in this area remains lacking, which this survey aims to
address. We analyze their roles in 6G-V2X applications, such as intelligent
resource allocation, beamforming, intelligent traffic management, and security
management. Furthermore, we explore the technical challenges, including
computational complexity, data privacy, and real-time decision-making
constraints, while identifying future research directions for AI-driven 6G-V2X
development. This study aims to provide valuable insights for researchers,
engineers, and policymakers working towards realizing intelligent, AI-powered
V2X ecosystems in 6G communication.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [128] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: This paper proposes SimClass, a synthesized classroom noise and speech dataset created via game engines to overcome the scarcity of large-scale classroom speech data for AI-driven education models.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the lack of large-scale classroom speech data and dedicated classroom noise corpus, which hinders the development of robust AI-driven speech models for educational purposes.

Method: The method involves using game engines to synthesize classroom noise and creating SimClass, a dataset that includes both a synthesized classroom noise corpus and a simulated classroom speech dataset. The speech data is generated by combining a public children's speech corpus with YouTube lecture videos.

Result: Experiments on clean and noisy speech show that SimClass closely approximates real classroom speech.

Conclusion: SimClass serves as a valuable resource for developing robust speech recognition and enhancement models in educational settings.

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [129] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Main category: cs.SD

TL;DR: This paper introduces BemaGANv2, an advanced GAN-based vocoder for high-fidelity and long-term audio generation. It replaces traditional ResBlocks with AMP modules in the generator and integrates MED in the discriminator framework. The paper evaluates various discriminator configurations and provides a tutorial on the model architecture, training methodology, and implementation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to present an improved version of the BemaGAN vocoder (BemaGANv2) that can generate high-fidelity and long-term audio by incorporating architectural innovations such as the AMP module and MED architecture.

Method: The method involves using the BemaGANv2 architecture which includes replacing traditional ResBlocks in the generator with the AMP module that applies the Snake activation function. In the discriminator, the Multi-Envelope Discriminator (MED) is integrated along with the Multi-Resolution Discriminator (MRD) to extract rich temporal envelope features and model long-range dependencies in audio.

Result: The paper systematically evaluates various discriminator configurations using objective metrics (FAD, SSIM, PLCC, MCD) and subjective evaluations (MOS, SMOS), demonstrating the effectiveness of the proposed architecture.

Conclusion: The conclusion highlights the improvements in audio generation achieved by BemaGANv2 through its innovative architecture. The paper also promotes reproducibility by providing a comprehensive tutorial on the model architecture, training methodology, and implementation.

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [130] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: A training-free modification for kNN-VC pipeline, named Factorized MKL-VC, which performs high quality any-to-any cross-lingual voice conversion with only 5 second of reference audio.


<details>
  <summary>Details</summary>
Motivation: The original kNN-VC pipeline has limitations in handling short reference audio and non-uniform variance across dimensions. To address these issues, the authors propose a new algorithm.

Method: Replace kNN regression with a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution. This factorization addresses non-uniform variance across dimensions, ensuring effective feature transformation.

Result: Experiments on LibriSpeech and FLEURS datasets show that MKL-VC significantly improves content preservation and robustness with short reference audio, outperforming kNN-VC. It achieves performance comparable to FACodec, especially in cross-lingual voice conversion domain.

Conclusion: Factorized MKL-VC is a successful training-free modification for kNN-VC pipeline, providing high quality any-to-any cross-lingual voice conversion with very short reference audio.

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [131] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: This paper explores the use of pre-trained speech-language models and pre-trained language models to improve audio-visual target speaker extraction without adding computational cost during inference, leading to consistent improvements in speech quality and intelligibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the performance of audio-visual target speaker extraction by leveraging linguistic knowledge from pre-trained models, inspired by how humans use syntax and semantics for speech perception.

Method: Incorporate linguistic constraints from pre-trained speech-language models or pre-trained language models as additional supervision signals for the audio-visual target speaker extraction model.

Result: The proposed approach improves speech quality and intelligibility consistently without extra computational cost during inference. It also shows robust performance gains in multi-language settings and visual cue-impaired scenarios.

Conclusion: Using pre-trained speech-language models and pre-trained language models as auxiliary knowledge sources can effectively boost the performance of audio-visual target speaker extraction models.

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [132] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/abs/2506.09874)
*Neta Glazer,Aviv Navon,Yael Segal,Aviv Shamsian,Hilit Segev,Asaf Buchnick,Menachem Pirchi,Gil Hetz,Joseph Keshet*

Main category: cs.SD

TL;DR: UmbraTTS is a Text-to-Speech model that generates speech and environmental audio together, with fine-grained control over background volume and context-awareness.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-Speech models produce natural speech but struggle to integrate it with complex background environments. The lack of paired data with aligned speech and background audio in natural contexts poses an additional challenge.

Method: UmbraTTS uses a flow-matching based TTS model which jointly generates speech and environmental audio, conditioned on text and acoustic context. A self-supervised framework is proposed to extract speech, background audio, and transcripts from unannotated recordings to address the lack of paired training data.

Result: Extensive evaluations show UmbraTTS outperforms existing baselines, producing high-quality audios that are natural and environmentally aware.

Conclusion: UmbraTTS successfully integrates speech with complex background environments, offering control over background volume and generating coherent, context-aware audio scenes.

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [133] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: 本研究使用2018年至2023年的历史USD/BDT汇率数据，开发了LSTM神经网络模型进行准确预测，并采用Gradient Boosting Classifier进行方向性预测。LSTM模型表现优于传统ARIMA方法，而方向性交易策略尽管盈利率为40.82%，但最终仍导致净亏损。


<details>
  <summary>Details</summary>
Motivation: 外汇汇率预测在全球金融市场中具有重要作用，影响贸易、投资和经济稳定。因此，开发更精确的预测模型具有重要意义。

Method: 利用Yahoo Finance提供的2018至2023年USD/BDT汇率数据，采用LSTM神经网络进行汇率预测，并使用Gradient Boosting Classifier进行方向性预测。通过回测评估交易策略的表现。

Result: LSTM模型实现了99.449%的高精度，RMSE为0.9858，优于ARIMA方法（RMSE 1.342）。 Gradient Boosting Classifier的方向性预测在回测中显示40.82%的盈利交易率，但49次交易后产生20,653.25美元的净亏损。

Conclusion: 深度学习方法如LSTM在外汇预测中有巨大潜力，可为交易者和政策制定者提供强大的工具以降低风险。未来可通过整合情绪分析和实时经济指标进一步提高模型适应性。

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [134] [Abstraction-Based Proof Production in Formal Verification of Neural Networks](https://arxiv.org/abs/2506.09455)
*Yizhak Yisrael Elboher,Omri Isac,Guy Katz,Tobias Ladner,Haoze Wu*

Main category: cs.LO

TL;DR: This paper introduces a novel framework for proof-producing abstraction-based DNN verification, separating the task into proving the correctness of an abstract network and proving the soundness of the abstraction with respect to the original DNN.


<details>
  <summary>Details</summary>
Motivation: Modern verification tools for deep neural networks increasingly rely on abstraction to scale to realistic architectures. Proof production is becoming a critical requirement for increasing the reliability of DNN verification results.

Method: The approach modularly separates the verification task into two components: (i) proving the correctness of an abstract network, and (ii) proving the soundness of the abstraction with respect to the original DNN.

Result: This preliminary work aims to enable scalable and trustworthy verification by supporting common abstraction techniques within a formal proof framework.

Conclusion: The novel framework addresses the gap between scalability and provable guarantees in DNN verification.

Abstract: Modern verification tools for deep neural networks (DNNs) increasingly rely
on abstraction to scale to realistic architectures. In parallel, proof
production is becoming a critical requirement for increasing the reliability of
DNN verification results. However, current proofproducing verifiers do not
support abstraction-based reasoning, creating a gap between scalability and
provable guarantees. We address this gap by introducing a novel framework for
proof-producing abstraction-based DNN verification. Our approach modularly
separates the verification task into two components: (i) proving the
correctness of an abstract network, and (ii) proving the soundness of the
abstraction with respect to the original DNN. The former can be handled by
existing proof-producing verifiers, whereas we propose the first method for
generating formal proofs for the latter. This preliminary work aims to enable
scalable and trustworthy verification by supporting common abstraction
techniques within a formal proof framework.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [135] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)
*Xia Li*

Main category: cs.HC

TL;DR: In this paper, the authors discuss the development of a teaching program for Oral Expression in Teaching Chinese as a Foreign Language. The teacher designs communicative tasks based on conflicts and uses ChatGPT to assist in finalizing the program. The article presents the key characteristics of the interactions between the teacher and ChatGPT, and examines the use and impacts of ChatGPT in this context.


<details>
  <summary>Details</summary>
Motivation: To encourage learners to engage in interactive dynamics and develop their oral interaction skills in the course of Oral Expression in Teaching Chinese as a Foreign Language.

Method: Design communicative tasks based on conflicts and use ChatGPT to assist in finalizing the teaching program.

Result: The article presents the key characteristics of the interactions between the teacher and ChatGPT during the program development process, and provides insights into the use and impacts of ChatGPT in this specific context.

Conclusion: ChatGPT can be effectively used to assist in the development of teaching programs in Oral Expression in Teaching Chinese as a Foreign Language.

Abstract: In developing the teaching program for a course in Oral Expression in
Teaching Chinese as a Foreign Language at the university level, the teacher
designs communicative tasks based on conflicts to encourage learners to engage
in interactive dynamics and develop their oral interaction skills. During the
design of these tasks, the teacher uses ChatGPT to assist in finalizing the
program. This article aims to present the key characteristics of the
interactions between the teacher and ChatGPT during this program development
process, as well as to examine the use of ChatGPT and its impacts in this
specific context.

</details>


### [136] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)
*Kellie Yu Hui Sim,Roy Ka-Wei Lee,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: The paper explores an AI-supported system with LLMs to enhance peer support interactions for mental health. Studies with peer supporters and experts reveal the system's potential in training and improving interaction quality, while highlighting limitations in current peer support training.


<details>
  <summary>Details</summary>
Motivation: To address the growing global concern of mental health and explore AI-driven solutions to expand access to psychosocial support, particularly through enhancing peer support interactions using Large Language Models (LLMs).

Method: Developed and evaluated an AI-supported system featuring LLM-simulated distressed clients, context-sensitive suggestions, and real-time emotion visualizations. Conducted 2 mixed-methods studies involving 12 peer supporters and 5 mental health professionals to examine the system's effectiveness and implications.

Result: Both groups recognized the system's potential to improve training and interaction quality. However, a key tension emerged where peer supporters engaged meaningfully but experts identified critical issues such as missed distress cues and premature advice-giving in peer supporter responses.

Conclusion: There is a need for standardized, psychologically grounded training for peer supporters. LLM-supported systems can assist in this development if carefully designed with expert oversight, contributing to discussions on responsible AI integration in mental health.

Abstract: Mental health is a growing global concern, prompting interest in AI-driven
solutions to expand access to psychosocial support. Peer support, grounded in
lived experience, offers a valuable complement to professional care. However,
variability in training, effectiveness, and definitions raises concerns about
quality, consistency, and safety. Large Language Models (LLMs) present new
opportunities to enhance peer support interactions, particularly in real-time,
text-based interactions. We present and evaluate an AI-supported system with an
LLM-simulated distressed client, context-sensitive LLM-generated suggestions,
and real-time emotion visualisations. 2 mixed-methods studies with 12 peer
supporters and 5 mental health professionals (i.e., experts) examined the
system's effectiveness and implications for practice. Both groups recognised
its potential to enhance training and improve interaction quality. However, we
found a key tension emerged: while peer supporters engaged meaningfully,
experts consistently flagged critical issues in peer supporter responses, such
as missed distress cues and premature advice-giving. This misalignment
highlights potential limitations in current peer support training, especially
in emotionally charged contexts where safety and fidelity to best practices are
essential. Our findings underscore the need for standardised, psychologically
grounded training, especially as peer support scales globally. They also
demonstrate how LLM-supported systems can scaffold this development--if
designed with care and guided by expert oversight. This work contributes to
emerging conversations on responsible AI integration in mental health and the
evolving role of LLMs in augmenting peer-delivered care.

</details>


### [137] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)
*Kellie Yu Hui Sim,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: Peer support is crucial for mental health care access. This study explores the experiences of 20 peer supporters in Singapore through interviews, focusing on motivations, emotional labour, and sociocultural factors. It provides design directions for culturally responsive digital tools that support relational care and discusses how AI might responsibly augment peer support.


<details>
  <summary>Details</summary>
Motivation: To understand the experiences, challenges, and practices of peer supporters in Singapore, particularly focusing on their motivations, emotional labour, and sociocultural dimensions shaping their work across diverse environments.

Method: Conducted an interview study with 20 peer supporters operating in online, offline, and hybrid environments in Singapore. Thematic analysis was used to analyze the data.

Result: Unpacked how peer supporters start, conduct, and sustain support, revealing insights into their motivations, emotional labour, and sociocultural influences. Identified design directions for digital tools that support relational care without replacing it.

Conclusion: This research highlights the importance of culturally responsive and context-sensitive AI in mental health support. It proposes design implications for trustworthy AI systems that can responsibly augment peer support.

Abstract: Peer support plays a vital role in expanding access to mental health care by
providing empathetic, community-based support outside formal clinical systems.
As digital platforms increasingly mediate such support, the design and impact
of these technologies remain under-examined, particularly in Asian contexts.
This paper presents findings from an interview study with 20 peer supporters in
Singapore, who operate across diverse online, offline, and hybrid environments.
Through a thematic analysis, we unpack how participants start, conduct, and
sustain peer support, highlighting their motivations, emotional labour, and the
sociocultural dimensions shaping their practices. Building on this grounded
understanding, we surface design directions for culturally responsive digital
tools that scaffold rather than supplant relational care. Drawing insights from
qualitative accounts, we offer a situated perspective on how AI might
responsibly augment peer support. This research contributes to human-centred
computing by articulating the lived realities of peer supporters and proposing
design implications for trustworthy and context-sensitive AI in mental health.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [138] [On the Impossibility of a Perfect Hypervisor](https://arxiv.org/abs/2506.09825)
*Mordechai Guri*

Main category: cs.OS

TL;DR: 本研究探讨了完美虚拟机监控器（hypervisor）的概念，定义为能够完全保留任何程序在裸机上的所有可观察行为，并且不增加任何时间和资源开销。然而，通过建立两个定理：不可检测性定理和不可能性定理，证明了在有限计算资源的机器上，这样的完美虚拟机监控器无法存在。此结果适用于各种虚拟化技术，如仿真器、沙盒、容器或运行时插桩框架，为未来关于虚拟化原则和限制的研究提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 完美虚拟机监控器（hypervisor）的理想概念引发了研究兴趣，即是否存在一种虚拟化层，能够在不改变程序任何可观察行为的同时，也完全不引入额外的时间或资源开销。

Method: 作者构建了一个理论模型来分析完美虚拟机监控器的可能性，并在该模型中证明了两个关键定理：(1) 不可检测性定理：如果完美虚拟机监控器存在，则没有任何测试方法可以将其与原生执行区分开来；(2) 不可能性定理：即使从理论上讲这种虚拟机监控器不可被检测，但由于有限的计算资源，它实际上无法实现。

Result: 证明了完美虚拟机监控器在有限计算资源的机器上是不可能存在的，这一结论对所有虚拟化技术均适用，包括但不限于仿真器、沙盒、容器及运行时插桩框架。

Conclusion: 完美虚拟机监控器在实际系统中无法实现，但其理论分析为理解虚拟化的基本原理和限制提供了一个重要的基础框架。这将有助于指导未来虚拟化技术的设计和发展。

Abstract: We establish a fundamental impossibility result for a `perfect hypervisor',
one that (1) preserves every observable behavior of any program exactly as on
bare metal and (2) adds zero timing or resource overhead.
  Within this model we prove two theorems. (1) Indetectability Theorem. If such
a hypervisor existed, no guest-level program, measurement, or timing test could
distinguish it from native execution; all traces, outputs, and timings would be
identical.
  (2) Impossibility Theorem. Despite that theoretical indetectability, a
perfect hypervisor cannot exist on any machine with finite computational
resources.
  These results are architecture-agnostic and extend beyond hypervisors to any
virtualization layer emulators, sandboxes, containers, or
runtime-instrumentation frameworks. Together they provide a formal foundation
for future work on the principles and limits of virtualization.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [139] [A theoretical basis for model collapse in recursive training](https://arxiv.org/abs/2506.09401)
*Vivek Shripad Borkar*

Main category: math.PR

TL;DR: Recursive training from generative models can cause 'collapse' of the simulated probability distribution, but there are two different asymptotic behaviors depending on whether an external source also contributes samples.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of recursive training from generative models on the simulated probability distribution and identify any changes in asymptotic behavior when an external source contributes samples.

Method: Investigate the asymptotic behaviors of simulated probability distributions under recursive training with and without contributions from an external source.

Result: Two distinct asymptotic behaviors were observed depending on the presence or absence of an external sample-contributing source.

Conclusion: Recursive training leads to collapse of the simulated probability distribution, but the nature of this collapse changes if an external source is also contributing samples.

Abstract: It is known that recursive training from generative models can lead to the so
called `collapse' of the simulated probability distribution. This note shows
that one in fact gets two different asymptotic behaviours depending on whether
an external source, howsoever minor, is also contributing samples.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [140] [Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy](https://arxiv.org/abs/2506.09805)
*Tonghe Wang,Yining Feng,Xiaofeng Yang*

Main category: physics.med-ph

TL;DR: The paper explores the use of reinforcement learning (RL) in high-dose-rate (HDR) prostate brachytherapy to optimize needle placement and dwell times, showing potential to standardize treatment planning and improve patient outcomes.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on physician experience and decrease procedure time while ensuring consistent plan quality in HDR prostate brachytherapy.

Method: Training a RL agent to adjust needle positions and dwell times based on patient anatomy during pre-planning stage. The agent observes the environment, adjusts one needle at a time until all are adjusted, and repeats multiple rounds. Plan data from 11 prostate HDR boost patients were used for training and testing.

Result: RL plans showed very similar prostate coverage and Rectum D2cc compared to clinical plans, but with less prostate hotspot and Urethra D20%, and an average of 2 fewer needles used than clinical plans.

Conclusion: This study demonstrates the feasibility of using RL to autonomously generate clinically practical HDR prostate brachytherapy plans, achieving equal or improved plan quality with fewer needles.

Abstract: Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the
pattern of needle placement solely relies on physician experience. We
investigated the feasibility of using reinforcement learning (RL) to provide
needle positions and dwell times based on patient anatomy during pre-planning
stage. This approach would reduce procedure time and ensure consistent plan
quality. Materials and Methods: We train a RL agent to adjust the position of
one selected needle and all the dwell times on it to maximize a pre-defined
reward function after observing the environment. After adjusting, the RL agent
then moves on to the next needle, until all needles are adjusted. Multiple
rounds are played by the agent until the maximum number of rounds is reached.
Plan data from 11 prostate HDR boost patients (1 for training, and 10 for
testing) treated in our clinic were included in this study. The dosimetric
metrics and the number of used needles of RL plan were compared to those of the
clinical results (ground truth). Results: On average, RL plans and clinical
plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no
statistical significance), while RL plans have less prostate hotspot (Prostate
V150) and Urethra D20% plans with statistical significance. Moreover, RL plans
use 2 less needles than clinical plan on average. Conclusion: We present the
first study demonstrating the feasibility of using reinforcement learning to
autonomously generate clinically practical HDR prostate brachytherapy plans.
This RL-based method achieved equal or improved plan quality compared to
conventional clinical approaches while requiring fewer needles. With minimal
data requirements and strong generalizability, this approach has substantial
potential to standardize brachytherapy planning, reduce clinical variability,
and enhance patient outcomes.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [141] [Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features](https://arxiv.org/abs/2506.09313)
*Angel Yanguas-Gil,Jeffrey W. Elam*

Main category: cond-mat.mtrl-sci

TL;DR: This paper explores surrogate models for optimizing plasma enhanced atomic layer deposition (PEALD) in high aspect ratio features using machine learning, achieving accurate predictions of saturation times and dominance of surface recombination with minimal experimental data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unfeasibly long exposure times required to achieve full conformality inside nanostructures like high aspect ratio vias during plasma-based processes such as PEALD and atomic layer etching.

Method: Training artificial neural networks on a synthetic dataset based on simulations of PEALD to predict saturation times from cross section thickness data under partially coated conditions.

Result: Two experiments in undersaturated conditions provide sufficient information to predict saturation times within 10% accuracy. A surrogate model trained to detect if surface recombination dominates plasma-surface interactions achieves 99% accuracy.

Conclusion: Machine learning offers a promising approach to accelerate the optimization of PEALD processes in fields like microelectronics and can be extended to other areas such as atomic layer etching.

Abstract: In this work we explore surrogate models to optimize plasma enhanced atomic
layer deposition (PEALD) in high aspect ratio features. In plasma-based
processes such as PEALD and atomic layer etching, surface recombination can
dominate the reactivity of plasma species with the surface, which can lead to
unfeasibly long exposure times to achieve full conformality inside
nanostructures like high aspect ratio vias. Using a synthetic dataset based on
simulations of PEALD, we train artificial neural networks to predict saturation
times based on cross section thickness data obtained for partially coated
conditions. The results obtained show that just two experiments in
undersaturated conditions contain enough information to predict saturation
times within 10% of the ground truth. A surrogate model trained to determine
whether surface recombination dominates the plasma-surface interactions in a
PEALD process achieves 99% accuracy. This demonstrates that machine learning
can provide a new pathway to accelerate the optimization of PEALD processes in
areas such as microelectronics. Our approach can be easily extended to atomic
layer etching and more complex structures.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [142] [Large Language Models for Design Structure Matrix Optimization](https://arxiv.org/abs/2506.09749)
*Shuo Jiang,Min Xie,Jianxi Luo*

Main category: cs.CE

TL;DR: 在复杂工程系统中，设计结构矩阵（DSM）建模和分析组件或开发活动之间的相互依赖关系。随着问题规模增大和依赖网络变得更加复杂，传统优化方法难以提供有效解决方案。本文探讨了大型语言模型（LLMs）在解决此类组合优化（CO）问题中的潜力，并提出了一个基于LLM的框架，该框架结合网络拓扑与领域知识进行DSM元素排序的迭代优化。实验表明，该方法比随机和确定性基线更快收敛且具有更优解质量，同时发现融入领域知识显著提升优化性能，无论使用何种LLM骨干架构。这些结果强调了LLMs通过结合语义和数学推理解决复杂工程CO问题的潜力，为基于LLM的工程设计优化开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 复杂工程系统中的DSM元素重组以减少反馈回路、提高模块化或过程效率是一个困难的组合优化问题。随着问题规模和依赖网络复杂性的增加，仅依赖数学启发式的传统优化方法难以捕捉上下文细微差别并提供有效解决方案。因此，需要探索新的方法来解决这一挑战。

Method: 提出了一种基于LLM的框架，将网络拓扑与领域知识相结合，用于DSM元素排序的迭代优化。利用LLM的高级推理和上下文理解能力，以解决复杂的CO问题。该框架通过整合上下文领域知识来增强优化性能。

Result: 实验表明，所提出的方法在各种DSM案例中均表现出更快的收敛速度和更高的解质量，优于随机和确定性基线。此外，无论使用何种LLM骨干架构，加入上下文领域知识都能显著提升优化性能。

Conclusion: 研究证明了LLMs在解决复杂工程CO问题中的潜力，通过结合语义和数学推理可以实现更有效的解决方案。这一成果为基于LLM的工程设计优化提供了新的范式。

Abstract: In complex engineering systems, the interdependencies among components or
development activities are often modeled and analyzed using Design Structure
Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and
enhance modularity or process efficiency constitutes a challenging
combinatorial optimization (CO) problem in engineering design and operations.
As problem sizes increase and dependency networks become more intricate,
traditional optimization methods that solely use mathematical heuristics often
fail to capture the contextual nuances and struggle to deliver effective
solutions. In this study, we explore the potential of Large Language Models
(LLMs) for helping solve such CO problems by leveraging their capabilities for
advanced reasoning and contextual understanding. We propose a novel LLM-based
framework that integrates network topology with contextual domain knowledge for
iterative optimization of DSM element sequencing - a common CO problem.
Experiments on various DSM cases show that our method consistently achieves
faster convergence and superior solution quality compared to both stochastic
and deterministic baselines. Notably, we find that incorporating contextual
domain knowledge significantly enhances optimization performance regardless of
the chosen LLM backbone. These findings highlight the potential of LLMs to
solve complex engineering CO problems by combining semantic and mathematical
reasoning. This approach paves the way towards a new paradigm in LLM-based
engineering design optimization.

</details>


### [143] [Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era](https://arxiv.org/abs/2506.09755)
*Shuo Jiang,Min Xie,Frank Youhua Chen,Jian Ma,Jianxi Luo*

Main category: cs.CE

TL;DR: The paper introduces Intelligent Design 4.0 (ID 4.0) as a new paradigm empowered by agentic AI systems, reviews the historical evolution of ID through four stages, proposes a conceptual framework for ID 4.0 and discusses its potential to support end-to-end automation in engineering design processes.


<details>
  <summary>Details</summary>
Motivation: To explore how Foundation Models, particularly Large Language Models, can transform engineering design through a new paradigm called Intelligent Design 4.0.

Method: Reviewing the historical development of Intelligent Design across four stages and proposing a conceptual framework for ID 4.0 that involves multi-agent collaboration for end-to-end automation of engineering design processes.

Result: Discussion on the potential of ID 4.0 to handle complex design scenarios, practical implementations, novel agent coordination mechanisms, and autonomous design goal-setting aligned with human values.

Conclusion: These insights provide a basis for advancing Intelligent Design towards greater adaptivity, autonomy, and effectiveness in tackling increasingly complex design challenges.

Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced
engineering innovation, efficiency, quality, and productivity over recent
decades, fundamentally reshaping how engineering designers think, behave, and
interact with design processes. The recent emergence of Foundation Models
(FMs), particularly Large Language Models (LLMs), has demonstrated general
knowledge-based reasoning capabilities, and open new paths and avenues for
further transformation in engineering design. In this context, this paper
introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by
agentic AI systems. We review the historical evolution of ID across four
distinct stages: rule-based expert systems, task-specific machine learning
models, large-scale foundation AI models, and the recent emerging paradigm of
multi-agent collaboration. We propose a conceptual framework for ID 4.0 and
discuss its potential to support end-to-end automation of engineering design
processes through coordinated, autonomous multi-agent-based systems.
Furthermore, we discuss future perspectives to enhance and fully realize ID
4.0's potential, including more complex design scenarios, more practical design
implementations, novel agent coordination mechanisms, and autonomous design
goal-setting with better human value alignment. In sum, these insights lay a
foundation for advancing Intelligent Design toward greater adaptivity,
autonomy, and effectiveness in addressing increasingly complex design
challenges.

</details>


### [144] [Superstudent intelligence in thermodynamics](https://arxiv.org/abs/2506.09822)
*Rebecca Loubet,Pascal Zittlau,Marco Hoffmann,Luisa Vollmer,Sophie Fellenz,Heike Leitte,Fabian Jirasek,Johannes Lenhard,Hans Hasse*

Main category: cs.CE

TL;DR: In this paper, OpenAI's large language model o3 outperformed all students in a challenging university thermodynamics exam, showcasing a turning point where machines excel in complex tasks traditionally seen as proof of human intellectual capabilities.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of large language models in solving complex problems that require deep understanding and creative combination of principles, rather than pattern learning.

Method: Administered the latest thermodynamics exam to both students and OpenAI's powerful reasoning model o3, evaluating o3's answers using the same criteria as the students' in zero-shot mode.

Result: The model o3 solved all problems correctly, surpassing all students' performances; its overall score ranked among the best scores observed in over 10,000 exams since 1985.

Conclusion: This event marks a significant turning point where machines demonstrate superior performance in tasks typically associated with human intellectual capabilities, raising important considerations for engineering work and education.

Abstract: In this short note, we report and analyze a striking event: OpenAI's large
language model o3 has outwitted all students in a university exam on
thermodynamics. The thermodynamics exam is a difficult hurdle for most
students, where they must show that they have mastered the fundamentals of this
important topic. Consequently, the failure rates are very high, A-grades are
rare - and they are considered proof of the students' exceptional intellectual
abilities. This is because pattern learning does not help in the exam. The
problems can only be solved by knowledgeably and creatively combining
principles of thermodynamics. We have given our latest thermodynamics exam not
only to the students but also to OpenAI's most powerful reasoning model, o3,
and have assessed the answers of o3 exactly the same way as those of the
students. In zero-shot mode, the model o3 solved all problems correctly, better
than all students who took the exam; its overall score was in the range of the
best scores we have seen in more than 10,000 similar exams since 1985. This is
a turning point: machines now excel in complex tasks, usually taken as proof of
human intellectual capabilities. We discuss the consequences this has for the
work of engineers and the education of future engineers.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [145] [BF-Max: an Efficient Bit Flipping Decoder with Predictable Decoding Failure Rate](https://arxiv.org/abs/2506.09689)
*Alessio Baldelli,Marco Baldi,Franco Chiaraluce,Paolo Santini*

Main category: cs.IT

TL;DR: The paper introduces BF-Max, a new Bit-Flipping decoder variant for Moderate Density Parity Check codes used in post-quantum cryptographic schemes. It flips only the least reliable bit per iteration and allows theoretical DFR characterization matching simulations. BF-Max is efficient, low complexity, constant time, and achieves lower DFR values than other methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring negligible Decoding Failure Rate (DFR) in post-quantum cryptographic schemes based on Moderate Density Parity Check codes. Existing methods either rely heavily on simulations or lack accurate theoretical modeling.

Method: Introduced BF-Max, which flips only one least reliable bit per iteration. Developed a theoretical characterization of DFR when iterations equal errors to be corrected. Implemented BF-Max efficiently to ensure low complexity and constant time operation.

Result: Theoretical DFR characterization tightly matches numerical simulations. Achieved significantly lower DFR values compared to other approaches. Demonstrated efficiency and low complexity of BF-Max implementation.

Conclusion: BF-Max provides an efficient, low-complexity solution for decoding in post-quantum cryptographic schemes with accurately predictable and remarkably lower DFR values.

Abstract: The Bit-Flipping (BF) decoder, thanks to its very low computational
complexity, is widely employed in post-quantum cryptographic schemes based on
Moderate Density Parity Check codes in which, ultimately, decryption boils down
to syndrome decoding. In such a setting, for security concerns, one must
guarantee that the Decoding Failure Rate (DFR) is negligible. Such a condition,
however, is very difficult to guarantee, because simulations are of little help
and the decoder performance is difficult to model theoretically. In this paper,
we introduce a new version of the BF decoder, that we call BF-Max,
characterized by the fact that in each iteration only one bit (the least
reliable) is flipped. When the number of iterations is equal to the number of
errors to be corrected, we are able to develop a theoretical characterization
of the DFR that tightly matches with numerical simulations. We also show how
BF-Max can be implemented efficiently, achieving low complexity and making it
inherently constant time. With our modeling, we are able to accurately predict
values of DFR that are remarkably lower than those estimated by applying other
approaches.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [146] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: Task-oriented LLM-based agents face challenges in adhering to strict policies while maintaining natural interactions. This paper proposes a novel threat model and introduces CRAFT, a multi-agent red-teaming system that uses policy-aware persuasive strategies to test agent resilience. Additionally, it presents tau-break, a benchmark for assessing agent robustness against manipulative user behavior, and evaluates several defense strategies.


<details>
  <summary>Details</summary>
Motivation: To ensure task-oriented LLM-based agents consistently adhere to strict policies while maintaining helpful and natural interactions, and to develop tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior.

Method: Propose a novel threat model focusing on adversarial users aiming to exploit policy-adherent agents. Present CRAFT, a multi-agent red-teaming system leveraging policy-aware persuasive strategies. Introduce tau-break, a complementary benchmark designed to rigorously assess agent robustness. Evaluate straightforward defense strategies.

Result: CRAFT outperforms conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Defense strategies provide some protection but fall short, indicating the need for stronger safeguards.

Conclusion: There is a need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks.

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


### [147] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/abs/2506.09335)
*Moshi Wei,Sparks Li*

Main category: cs.MA

TL;DR: ISEK establishes a decentralized network combining human and artificial intelligence, built on Web3 infrastructure with economic incentives and NFT-based identity management.


<details>
  <summary>Details</summary>
Motivation: To create a self-organizing cognitive ecosystem that facilitates emergent intelligence beyond centralized constraints.

Method: Implementing a six-phase workflow coordination protocol on a decentralized multi-agent architecture with symbiotic AI-human collaboration and distributed consensus mechanisms.

Result: Synthesis of blockchain technology, artificial intelligence, and incentive engineering creates an infrastructure for emergent intelligence.

Conclusion: ISEK represents a paradigm shift enabling large-scale, decentralized cognitive systems where autonomous agents collectively evolve.

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [148] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Main category: cs.MA

TL;DR: 在多智能体任务分配问题中，研究了异构团队优于同质团队的条件及奖励机制。通过理论分析与实验验证，提出了一种基于梯度的算法（HED），揭示了行为多样性带来的可量化优势。


<details>
  <summary>Details</summary>
Motivation: 尽管团队成功往往依赖于劳动分工和多样性，但尚缺乏对其优于同质团队的原理性解释。本文旨在从奖励设计角度探讨何种目标最适合异构团队，并研究促使异构性出现的激励机制。

Method: 1. 在瞬时、非空间设置下，使用两个广义聚合算子构建全局奖励模型，分析其曲率对异构团队奖励的影响。
2. 提出Heterogeneous Environment Design (HED)算法，优化未完全指定的多智能体强化学习环境参数空间，寻找异构性有利的情景。
3. 通过矩阵博弈和Multi-Goal-Capture环境中的实验验证理论预测和HED的有效性。

Result: 1. 理论证明了奖励函数的曲率决定了异构性是否能增加奖励，且对于广泛使用的奖励函数族，这一判断可简化为凸性测试。
2. 实验结果表明HED能够重新发现最大化异构性优势的奖励机制，验证了理论预测并与多智能体强化学习中的奖励设计建立了联系。

Conclusion: 本文的研究成果帮助理解了行为多样性何时能带来可测量的优势，为多智能体系统中异构团队的设计提供了理论依据和实践指导。

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [149] [Estimating Visceral Adiposity from Wrist-Worn Accelerometry](https://arxiv.org/abs/2506.09167)
*James R. Williamson,Andrew Alini,Brian A. Telfer,Adam W. Potter,Karl E. Friedl*

Main category: eess.SP

TL;DR: The paper demonstrates a strong relationship between physical activity (PA) and visceral adipose tissue (VAT), using NHANES data and two approaches to estimate VAT from activity. The best performance was obtained by combining both approaches, resulting in VAT estimates with correlations of r=0.86.


<details>
  <summary>Details</summary>
Motivation: To find a method to measure VAT directly from physical activity (PA) without sophisticated imaging technologies, given that VAT is a key marker of metabolic health and habitual PA.

Method: Two approaches were used for estimating VAT from activity: one using engineered features based on movements during gait and sleep and ridge regression; the other using deep neural networks trained on 24 hours of continuous accelerometry. Both methods incorporate covariate information about subject demographics and body measurements for the most accurate estimates.

Result: Both approaches resulted in high correlations when estimating VAT from PA data, with the best performance (correlations of r=0.86) obtained by combining the two approaches.

Conclusion: There is a strong relationship between PA and VAT, and thus between PA and metabolic health risks.

Abstract: Visceral adipose tissue (VAT) is a key marker of both metabolic health and
habitual physical activity (PA). Excess VAT is highly correlated with type 2
diabetes and insulin resistance. The mechanistic basis for this pathophysiology
relates to overloading the liver with fatty acids. VAT is also a highly labile
fat depot, with increased turnover stimulated by catecholamines during
exercise. VAT can be measured with sophisticated imaging technologies, but can
also be inferred directly from PA. We tested this relationship using National
Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for
individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;
2,427 women) [1]. Two approaches were used for estimating VAT from activity.
The first used engineered features based on movements during gait and sleep,
and then ridge regression to map summary statistics of these features into a
VAT estimate. The second approach used deep neural networks trained on 24 hours
of continuous accelerometry. A foundation model first mapped each 10s frame
into a high-dimensional feature vector. A transformer model then mapped each
day's feature vector time series into a VAT estimate, which were averaged over
multiple days. For both approaches, the most accurate estimates were obtained
with the addition of covariate information about subject demographics and body
measurements. The best performance was obtained by combining the two
approaches, resulting in VAT estimates with correlations of r=0.86. These
findings demonstrate a strong relationship between PA and VAT and, by
extension, between PA and metabolic health risks.

</details>


### [150] [Integration of Contrastive Predictive Coding and Spiking Neural Networks](https://arxiv.org/abs/2506.09194)
*Emirhan Bilgiç,Neslihan Serap Şengör,Namık Berk Yalabık,Yavuz Selim İşler,Aykut Görkem Gelen,Rahmi Elibol*

Main category: eess.SP

TL;DR: This study integrates Contrastive Predictive Coding (CPC) with Spiking Neural Networks (SNN), tests it on MNIST, and shows effectiveness in classification tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a predictive coding model that processes inputs and outputs in a spike-based system for greater biological plausibility.

Method: Combining Contrastive Predictive Coding (CPC) with Spiking Neural Networks (SNN) and testing the model on the MNIST dataset.

Result: Achieved a high classification rate in distinguishing positive sequential samples from non-sequential negative samples.

Conclusion: CPC can be effectively combined with SNN, demonstrating that an SNN trained for classification tasks can also function as an encoding mechanism.

Abstract: This study examines the integration of Contrastive Predictive Coding (CPC)
with Spiking Neural Networks (SNN). While CPC learns the predictive structure
of data to generate meaningful representations, SNN mimics the computational
processes of biological neural systems over time. In this study, the goal is to
develop a predictive coding model with greater biological plausibility by
processing inputs and outputs in a spike-based system. The proposed model was
tested on the MNIST dataset and achieved a high classification rate in
distinguishing positive sequential samples from non-sequential negative
samples. The study demonstrates that CPC can be effectively combined with SNN,
showing that an SNN trained for classification tasks can also function as an
encoding mechanism. Project codes and detailed results can be accessed on our
GitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN

</details>


### [151] [Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms](https://arxiv.org/abs/2506.09195)
*Haoran Peng,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: This research focuses on optimizing multi-UAV systems with dual objectives using a Graph Attention-based Decentralized Actor-Critic (GADC). GADC leverages a graph attention network and an actor-double-critic network to manage dual policies for joint objective optimization. It uses a KL divergence factor to balance coverage performance and battery lifetime. Extensive testing shows GADC's superior performance.


<details>
  <summary>Details</summary>
Motivation: To optimize multi-UAV systems with the primary goal of maximizing service coverage while also extending battery lifetime as a secondary objective.

Method: Propose a Graph Attention-based Decentralized Actor-Critic (GADC) that includes a graph attention network for processing UAVs' limited local observation, reducing environment state dimensions, and an actor-double-critic network for managing dual policies for joint objective optimization. A Kullback-Leibler (KL) divergence factor is used to balance the tradeoff between coverage performance and battery lifetime.

Result: GADC demonstrates superior performance in both ideal settings and realistic ray tracing environments provided by NVIDIA Sionna through extensive benchmarking against state-of-the-art methods.

Conclusion: The proposed GADC approach successfully optimizes multi-UAV systems for both maximizing service coverage and extending battery lifetime, showing strong scalability and efficiency.

Abstract: This research focuses on optimizing multi-UAV systems with dual objectives:
maximizing service coverage as the primary goal while extending battery
lifetime as the secondary objective. We propose a Graph Attention-based
Decentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed
approach leverages a graph attention network to process UAVs' limited local
observation and reduce the dimension of the environment states. Subsequently,
an actor-double-critic network is developed to manage dual policies for joint
objective optimization. The proposed GADC uses a Kullback-Leibler (KL)
divergence factor to balance the tradeoff between coverage performance and
battery lifetime in the multi-UAV system. We assess the scalability and
efficiency of GADC through comprehensive benchmarking against state-of-the-art
methods, considering both theory and experimental aspects. Extensive testing in
both ideal settings and NVIDIA Sionna's realistic ray tracing environment
demonstrates GADC's superior performance.

</details>


### [152] [AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization](https://arxiv.org/abs/2506.09255)
*Saeed Hashemi,Genchang Peng,Mehrdad Nourani,Omar Nofal,Jay Harvey*

Main category: eess.SP

TL;DR: The paper proposes a machine learning approach to rank impactful SEEG channels by combining clinician's selection and computational findings, using XGBoost for feature learning and SHAP scoring for channel ranking, with a channel extension strategy to identify suspicious epileptogenic zones.


<details>
  <summary>Details</summary>
Motivation: To reduce the time consuming and inefficient process of visually inspecting signals recorded from hundreds of channels in Stereo-electroencephalography (SEEG) data.

Method: A classification model using XGBoost is trained to learn discriminative features of each channel during ictal periods. SHapley Additive exPlanations (SHAP) scoring is used to rank SEEG channels based on their contribution to seizures. A channel extension strategy is incorporated to expand the search space.

Result: Analysis of SEEG data for five patients showed promising results in terms of accuracy, consistency, and explainability.

Conclusion: The proposed machine learning approach can effectively rank impactful SEEG channels and identify suspicious epileptogenic zones.

Abstract: Stereo-electroencephalography (SEEG) is an invasive technique to implant
depth electrodes and collect data for pre-surgery evaluation. Visual inspection
of signals recorded from hundreds of channels is time consuming and
inefficient. We propose a machine learning approach to rank the impactful
channels by incorporating clinician's selection and computational finding. A
classification model using XGBoost is trained to learn the discriminative
features of each channel during ictal periods. Then, the SHapley Additive
exPlanations (SHAP) scoring is utilized to rank SEEG channels based on their
contribution to seizures. A channel extension strategy is also incorporated to
expand the search space and identify suspicious epileptogenic zones beyond
those selected by clinicians. For validation, SEEG data for five patients were
analyzed showing promising results in terms of accuracy, consistency, and
explainability.

</details>


### [153] [Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces](https://arxiv.org/abs/2506.09773)
*Taulant Koka,Manolis C. Tsakiris,Benjamín Béjar Haro,Michael Muma*

Main category: eess.SP

TL;DR: The paper extends cross-channel unlabeled sensing to signals in a union of subspaces, improving bounds for unique reconstruction and demonstrating its application in whole-brain calcium imaging.


<details>
  <summary>Details</summary>
Motivation: To address the problem of recovering multi-channel signals from shuffled measurements across channels, especially for more complex signal structures.

Method: Expands the cross-channel unlabeled sensing framework to signals in a union of subspaces, deriving tighter bounds on the required number of samples for unique reconstruction while supporting more general signal types.

Result: Validated through an application in whole-brain calcium imaging, achieving accurate signal reconstruction despite imprecise sample-channel associations.

Conclusion: The extended framework can handle more complex signal structures and is applicable to real-world settings with mismatched sample-channel associations.

Abstract: Cross-channel unlabeled sensing addresses the problem of recovering a
multi-channel signal from measurements that were shuffled across channels. This
work expands the cross-channel unlabeled sensing framework to signals that lie
in a union of subspaces. The extension allows for handling more complex signal
structures and broadens the framework to tasks like compressed sensing. These
mismatches between samples and channels often arise in applications such as
whole-brain calcium imaging of freely moving organisms or multi-target
tracking. We improve over previous models by deriving tighter bounds on the
required number of samples for unique reconstruction, while supporting more
general signal types. The approach is validated through an application in
whole-brain calcium imaging, where organism movements disrupt sample-to-neuron
mappings. This demonstrates the utility of our framework in real-world settings
with imprecise sample-channel associations, achieving accurate signal
reconstruction.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [154] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park,Kristjan Greenewald,Kaveh Alim,Hao Wang,Navid Azizan*

Main category: stat.ML

TL;DR: In this paper, the authors propose a calibration approach using quantile regression to adjust process reward model (PRM) outputs for better alignment with true success probabilities. They introduce an instance-adaptive scaling (IAS) framework that dynamically adjusts inference budgets based on calibrated PRMs. Experiments show reduced inference costs and maintained accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of poorly calibrated state-of-the-art PRMs that often overestimate success probabilities in guiding inference-time scaling algorithms for large language models.

Method: The method involves a calibration approach performed via quantile regression to adjust PRM outputs. Using these calibrated success estimates and their associated confidence bounds, they introduce an instance-adaptive scaling (IAS) framework that dynamically adjusts the inference budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer.

Result: Experiments on mathematical reasoning benchmarks demonstrate that: (i) the PRM calibration method successfully achieves small calibration error, outperforming baseline methods; (ii) calibration is crucial for enabling effective adaptive scaling; (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy.

Conclusion: The conclusion is that the proposed calibration method improves PRM performance, the IAS framework effectively reduces inference costs while maintaining accuracy, and calibration is essential for successful adaptive scaling.

Abstract: Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [155] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra,Philippe Roudot*

Main category: stat.ML

TL;DR: A hybrid tracking framework combining transformers and Bayesian filtering for multiple particle tracking in noisy scenes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of tracking multiple particles in noisy and cluttered scenes, where the number of trajectory hypotheses increases super-exponentially with the number of particles and frames.

Method: Introduce a hybrid framework using transformer encoder for inferring soft associations between detections across frames to prune hypothesis set, followed by Bayesian filtering for reliable and interpretable tracking.

Result: Improved tracking accuracy and robustness against spurious detections in high clutter scenarios.

Conclusion: The hybrid approach effectively combines the strengths of self-attention and Bayesian filtering, providing a solution for multiple particle tracking in challenging environments.

Abstract: Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [156] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan,Jinchi Lv,Ao Sun,Yurou Wang*

Main category: stat.ML

TL;DR: The paper proposes LLM-CPI, a method leveraging large language models and high-frequency online text data for improved Consumer Price Index prediction.


<details>
  <summary>Details</summary>
Motivation: Most existing approaches for forecasting CPI rely on low-frequency, survey-based data. With the recent advances of large language models, there is potential to leverage high-frequency online text data for improved CPI prediction.

Method: LLM-CPI incorporates online text time series using LLMs such as ChatGPT and trained BERT models to construct continuous inflation labels for posts related to inflation. Online text embeddings are extracted via LDA and BERT. A joint time series framework combines monthly CPI data with LLM-generated daily CPI surrogates.

Result: The asymptotic properties of the method are established and two forms of constructed prediction intervals are provided. The finite-sample performance and practical advantages of LLM-CPI are demonstrated through both simulation and real data examples.

Conclusion: LLM-CPI shows potential in improving CPI prediction by incorporating high-frequency online text data processed by large language models.

Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [157] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: The paper introduces a methodology for designing optimal evasion attacks against Bayesian predictive models, investigating two adversarial objectives and proposing gradient-based attacks.


<details>
  <summary>Details</summary>
Motivation: There is an increasing interest in analyzing the behavior of machine learning systems against adversarial attacks, but the susceptibility of Bayesian predictive models to attacks remains underexplored.

Method: The paper proposes a general methodology for designing optimal evasion attacks against Bayesian predictive models. It investigates two adversarial objectives: perturbing specific point predictions and altering the entire posterior predictive distribution. Novel gradient-based attacks are proposed for both scenarios.

Result: The proposed gradient-based attacks are studied in various computational setups, showing their implementation feasibility and properties.

Conclusion: This research provides insights into the vulnerabilities of Bayesian predictive models to adversarial attacks and offers a methodology for designing optimal evasion attacks.

Abstract: There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [158] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso,Simone Rossi,Giulio Franzese,Markus Heinonen,Maurizio Filippone*

Main category: stat.ML

TL;DR: The paper explores scaling laws in predictive uncertainties within deep learning models, providing evidence that even with large datasets, epistemic uncertainty remains significant.


<details>
  <summary>Details</summary>
Motivation: Inspired by the discovery of scaling laws in deep learning model performance based on dataset and model sizes, the authors investigate whether similar scaling laws govern predictive uncertainties in deep learning.

Method: The authors conduct empirical studies on vision and language tasks using identifiable parametric models and over-parameterized models. They examine in- and out-of-distribution predictive uncertainty through popular approximate Bayesian inference and ensemble methods.

Result: They find scaling laws associated with various measures of predictive uncertainty with respect to dataset and model sizes. Even with large datasets, epistemic uncertainty does not become negligible.

Conclusion: This work highlights the importance of Bayesian approaches in deep learning despite large amounts of data, showing that scaling laws can be used to extrapolate uncertainties.

Abstract: Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [159] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan,Elen Vardanyan,Arnak Dalalyan*

Main category: stat.ML

TL;DR: Denoising diffusion probabilistic models (DDPMs) are robust to constant-variance noise in score evaluations and achieve faster convergence rates.


<details>
  <summary>Details</summary>
Motivation: To explore the robustness of DDPMs to noisy score estimates and establish finite-sample guarantees with faster convergence rates.

Method: Provide empirical evidence for DDPMs' robustness to constant-variance noise, establish finite-sample guarantees in Wasserstein-2 distance that quantify this robustness and show faster convergence rates.

Result: Achieved finite-sample guarantees with faster convergence rates than previously known results, matching those in the Gaussian case.

Conclusion: DDPMs exhibit robustness to noisy score estimates and achieve optimal convergence rates.

Abstract: Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [160] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon,Julien Straubhaar,Philippe Renard*

Main category: stat.ML

TL;DR: This paper proposes a new method using graph generative models to simulate discrete karst networks, which consists of two steps: utilizing GraphRNN for topological distribution learning and G-DDPM for node feature learning. The approach is tested on real-world data and allows stochastic simulation of karst networks.


<details>
  <summary>Details</summary>
Motivation: The complexity of simulating discrete karst networks due to the intricate physicochemical processes in various geological and hydrogeological contexts motivates the exploration of a novel approach that can capture the intricacies of karst environments.

Method: The method represents karst networks as graphs with nodes retaining spatial information and properties, and edges signifying connections between nodes. It uses GraphRNN to learn the topological distribution by decomposing graph simulation into sequential generation of nodes and edges, and G-DDPM to learn node features by sampling from the derived probability distribution.

Result: The methodology was tested using real-world karst networks, and generated subgraphs were compared with actual subgraphs from the database using geometry and topology metrics. The results show that the approach enables stochastic simulation of discrete karst networks across various types of formations.

Conclusion: The proposed methodology provides a useful tool for studying physical processes such as flow and transport in karst networks, allowing stochastic simulation of these complex systems.

Abstract: The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [161] [Covert Entanglement Generation over Bosonic Channels](https://arxiv.org/abs/2506.09474)
*Evan J. D. Anderson,Michael S. Bullock,Ohad Kimelfeld,Christopher K. Eyre,Filip Rozpędek,Uzi Pereg,Boulat A. Bash*

Main category: quant-ph

TL;DR: 在损失热噪声玻色信道上探索了隐蔽的纠缠生成，该信道是许多实际设置（包括光学、微波和射频信道）的量子力学模型。我们展示了与经典相似的隐蔽纠缠生成的平方根定律：可以通过n次使用玻色信道隐蔽且可靠地生成$L_{\rm EG}\sqrt{n}$个纠缠比特（ebits）。我们报告了最优$L_{\rm EG}$的单字母表达式以及一种可实现的方法。此外，我们还分析了使用单轨和双轨光子比特进行隐蔽纠缠生成的性能，这可能对物理实现更为实用。


<details>
  <summary>Details</summary>
Motivation: 研究隐蔽通信中的纠缠生成问题，特别是在存在损失和热噪声的量子信道中，如何确保传输不被对手检测到，同时生成可靠的纠缠资源。

Method: 1. 探索了损失热噪声玻色信道上的隐蔽纠缠生成。2. 提出了类似于经典的平方根定律（SRL）的隐蔽纠缠生成理论。3. 推导出最优$L_{\rm EG}$的单字母表达式，并提出了一种可实现的方法。4. 分析了单轨和双轨光子比特在隐蔽纠缠生成中的性能。

Result: 证明了通过n次使用玻色信道可以隐蔽且可靠地生成$L_{\rm EG}\sqrt{n}$个纠缠比特。给出了最优$L_{\rm EG}$的单字母表达式，并验证了其可行性。此外，评估了单轨和双轨光子比特在实际应用中的性能。

Conclusion: 隐蔽纠缠生成在量子通信中有重要意义，本研究提出的平方根定律为隐蔽纠缠生成提供了理论基础，而单轨和双轨光子比特的分析则为实际应用提供了指导。

Abstract: We explore covert entanglement generation over the lossy thermal-noise
bosonic channel, which is a quantum-mechanical model of many practical
settings, including optical, microwave, and radio-frequency (RF) channels.
Covert communication ensures that an adversary is unable to detect the presence
of transmissions, which are concealed in channel noise. We show that a
$\textit{square root law}$ (SRL) for covert entanglement generation similar to
that for classical: $L_{\rm EG}\sqrt{n}$ entangled bits (ebits) can be
generated covertly and reliably over $n$ uses of a bosonic channel. We report a
single-letter expression for optimal $L_{\rm EG}$ as well as an achievable
method. We additionally analyze the performance of covert entanglement
generation using single- and dual-rail photonic qubits, which may be more
practical for physical implementation.

</details>


### [162] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: This paper presents a hybrid quantum-classical model for Devanagari handwritten digit recognition, achieving state-of-the-art accuracy with fewer parameters compared to classical models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of recognizing handwritten digits in complex regional scripts like Devanagari, which has limited annotated datasets and poses difficulties for conventional models.

Method: The method combines a convolutional neural network (CNN) for spatial feature extraction with a 10-qubit variational quantum circuit (VQC) for classification. It is trained and evaluated on the Devanagari Handwritten Character Dataset (DHCD).

Result: The proposed model achieves a test accuracy of 99.80%, a test loss of 0.2893, and an average per-class F1-score of 0.9980. It demonstrates superior accuracy with significantly fewer parameters compared to equivalent classical CNNs.

Conclusion: This work establishes a new benchmark for regional script recognition using quantum machine learning, showcasing its potential in real-world, low-resource language settings.

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [163] [Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation](https://arxiv.org/abs/2506.09102)
*Mihaela van der Schaar,Richard Peck,Eoin McKinney,Jim Weatherall,Stuart Bailey,Justine Rochon,Chris Anagnostopoulos,Pierre Marquet,Anthony Wood,Nicky Best,Harry Amad,Julianna Piskorz,Krzysztof Kacprzyk,Rafik Salama,Christina Gunther,Francesca Frau,Antoine Pugeat,Ramon Hernandez*

Main category: cs.CY

TL;DR: This paper presents a collaborative vision for transforming clinical trials using causal inference and digital twins within existing regulatory frameworks.


<details>
  <summary>Details</summary>
Motivation: To revolutionize clinical research and redefine the gold standard for clinical trials by integrating AI technologies in a way that is compliant with current regulations.

Method: Focus on actionable integration of causal inference and digital twins within the existing regulatory frameworks to transform clinical trials.

Result: Proposes a roadmap to deliver faster, safer, and more personalized outcomes for patients through the use of AI technologies.

Conclusion: AI technologies such as causal inference and digital twins can significantly enhance clinical trials when properly integrated into the current regulatory environment.

Abstract: This manifesto represents a collaborative vision forged by leaders in
pharmaceuticals, consulting firms, clinical research, and AI. It outlines a
roadmap for two AI technologies - causal inference and digital twins - to
transform clinical trials, delivering faster, safer, and more personalized
outcomes for patients. By focusing on actionable integration within existing
regulatory frameworks, we propose a way forward to revolutionize clinical
research and redefine the gold standard for clinical trials using AI.

</details>


### [164] [FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines](https://arxiv.org/abs/2506.09107)
*Athena Vakali,Ilias Dimitriadis*

Main category: cs.CY

TL;DR: AI模型已发展为活跃的决策者，常在无人监督下运行。鉴于AI不公平现象频发，亟需通过代理技术引入谨慎、及时且持续的公平监控方案。本文提出FAIRTOPIA框架，旨在通过多角色代理嵌入端到端的人工智能流程，以实现设计中的公平性，并通过稳健的多代理工作流覆盖整个AI管道阶段，推动以人为本、系统化、跨学科的社会技术原则下的新公平研究假设、启发式方法和方法论。


<details>
  <summary>Details</summary>
Motivation: 当前AI技术快速发展，但忽视人类原则，主要集中在数据（前）、模型（中）和部署（后）处理阶段的计算偏差探索，导致有害事件频发，AI不公平问题备受批评。因此，需要一种新的方法来确保AI系统的公平性。

Method: 利用代理技术，将多角色代理嵌入端到端（从人类到AI）协同方案中，提出了一种基于设计的公平性方法（fairness-by-design）。构建了名为FAIRTOPIA的三层次架构框架，该框架包含AI管道，并通过代理守护和基于知识的自我优化分层方案实现公平性监控。

Result: FAIRTOPIA框架能够适应每个AI决策场景的要求和目标，通过稳健的多代理工作流，在AI管道的所有阶段实施公平监控，从而激发新的公平研究假设、启发式方法和方法论。

Conclusion: 通过设计中的公平性方法和FAIRTOPIA框架，可以创建适应性强且现实的AI公平框架，推动以人为本、系统化、跨学科的社会技术原则下的公平性研究。

Abstract: AI models have become active decision makers, often acting without human
supervision. The rapid advancement of AI technology has already caused harmful
incidents that have hurt individuals and societies and AI unfairness in heavily
criticized. It is urgent to disrupt AI pipelines which largely neglect human
principles and focus on computational biases exploration at the data (pre),
model(in), and deployment (post) processing stages. We claim that by exploiting
the advances of agents technology, we will introduce cautious, prompt, and
ongoing fairness watch schemes, under realistic, systematic, and human-centric
fairness expectations. We envision agents as fairness guardians, since agents
learn from their environment, adapt to new information, and solve complex
problems by interacting with external tools and other systems. To set the
proper fairness guardrails in the overall AI pipeline, we introduce a
fairness-by-design approach which embeds multi-role agents in an end-to-end
(human to AI) synergetic scheme. Our position is that we may design adaptive
and realistic AI fairness frameworks, and we introduce a generalized algorithm
which can be customized to the requirements and goals of each AI decision
making scenario. Our proposed, so called FAIRTOPIA framework, is structured
over a three-layered architecture, which encapsulates the AI pipeline inside an
agentic guardian and a knowledge-based, self-refining layered scheme. Based on
our proposition, we enact fairness watch in all of the AI pipeline stages,
under robust multi-agent workflows, which will inspire new fairness research
hypothesis, heuristics, and methods grounded in human-centric, systematic,
interdisciplinary, socio-technical principles.

</details>


### [165] [Understanding Human-AI Trust in Education](https://arxiv.org/abs/2506.09160)
*Griffin Pitts,Sanaz Motamedi*

Main category: cs.CY

TL;DR: 随着AI聊天机器人在教育中的集成度越来越高，学生越来越多地向这些系统寻求指导、反馈和信息。然而，这些聊天机器人的拟人化特征导致了学生是否基于人际信任还是技术信任来发展对它们的信任的模糊性。通过部分最小二乘结构方程建模，我们发现人类相似的信任和系统相似的信任显著影响学生的感知，且具有不同的效果。因此，我们提出学生与AI聊天机器人之间存在一种独特的人工智能信任形式（human-AI trust）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决学生对AI聊天机器人的信任是基于人际信任还是技术信任的模糊性问题，这种模糊性带来了理论挑战，因为人际信任模型可能不恰当地将人类意图和道德归因于AI，而技术信任模型是为非社交技术开发的，其适用性尚不清楚。

Method: 通过部分最小二乘结构方程建模（partial least squares structural equation modeling）来研究人类相似的信任和系统相似的信任如何比较性地影响学生对AI聊天机器人的感知享受、信任意图、使用行为意图和感知实用性等因素。

Result: 研究发现人类相似的信任和系统相似的信任显著影响学生的感知，且具有不同的效果。人类相似的信任更强烈地预测信任意图，而系统相似的信任更好地预测使用行为意图和感知实用性。两者对感知享受有类似的影响。

Conclusion: 学生与AI聊天机器人之间发展出一种独特的人工智能信任形式（human-AI trust），这种信任形式不同于人类-人类和人类-技术信任模型。研究结果强调了建立特定于人类-AI信任的新理论框架的必要性，并提供了培养适当校准信任的实际见解，这对于有效采用和实现教育中AI的教育影响至关重要。

Abstract: As AI chatbots become increasingly integrated in education, students are
turning to these systems for guidance, feedback, and information. However, the
anthropomorphic characteristics of these chatbots create ambiguity regarding
whether students develop trust toward them as they would a human peer or
instructor, based in interpersonal trust, or as they would any other piece of
technology, based in technology trust. This ambiguity presents theoretical
challenges, as interpersonal trust models may inappropriately ascribe human
intentionality and morality to AI, while technology trust models were developed
for non-social technologies, leaving their applicability to anthropomorphic
systems unclear. To address this gap, we investigate how human-like and
system-like trusting beliefs comparatively influence students' perceived
enjoyment, trusting intention, behavioral intention to use, and perceived
usefulness of an AI chatbot - factors associated with students' engagement and
learning outcomes. Through partial least squares structural equation modeling,
we found that human-like and system-like trust significantly influenced student
perceptions, with varied effects. Human-like trust more strongly predicted
trusting intention, while system-like trust better predicted behavioral
intention and perceived usefulness. Both had similar effects on perceived
enjoyment. Given the partial explanatory power of each type of trust, we
propose that students develop a distinct form of trust with AI chatbots
(human-AI trust) that differs from human-human and human-technology models of
trust. Our findings highlight the need for new theoretical frameworks specific
to human-AI trust and offer practical insights for fostering appropriately
calibrated trust, which is critical for the effective adoption and pedagogical
impact of AI in education.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [166] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Main category: cs.NI

TL;DR: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures are proposed for expanding coverage and improving capacity, with a focus on energy efficiency. A novel online optimisation framework based on multi-armed bandit formulation is developed, which adaptively optimises system parameters including bandwidth allocation, user equipment association, and macro base station shutdown. The framework significantly reduces unsatisfied UEs during peak hours and achieves throughput gains and energy savings.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of non-terrestrial networks (NTNs) in supporting more sustainable network operations by alleviating terrestrial network load and enabling energy-efficient operation, addressing concerns related to the densification of terrestrial deployments.

Method: A novel online optimisation framework for integrated TN-NTN architectures is proposed, built on a multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback Constrained Online Mirror Descent (BCOMD) algorithm. This approach adaptively optimises key system parameters such as bandwidth allocation, user equipment (UE) association, and macro base station (MBS) shutdown.

Result: Extensive system-level simulations over a 24-hour period demonstrate that the framework significantly reduces the proportion of unsatisfied UEs during peak hours, achieves up to 19% throughput gains, and 5% energy savings in low-traffic periods, outperforming standard network settings following 3GPP recommendations.

Conclusion: The proposed online optimisation framework for integrated TN-NTN architectures effectively balances network capacity and energy efficiency in real time, highlighting the potential of NTNs in supporting more sustainable network operations.

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [167] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Main category: cs.NI

TL;DR: The paper proposes a generative model for real-time network traffic forecasting with missing data, turning the task into a tensor completion problem and optimizing latent representations to simplify the process and enable fast forecasting.


<details>
  <summary>Details</summary>
Motivation: Real-world network traffic data often suffers from incompleteness due to various factors, while existing forecasting methods assume fully observed data.

Method: Model network traffic forecasting as a tensor completion problem, use a pre-trained generative model to capture low-rank structure, optimize latent representation instead of high-dimensional tensor directly.

Result: Achieves accurate network traffic forecasting within 100 ms on real-world datasets, with a mean absolute error below 0.002 on the Abilene dataset.

Conclusion: The proposed generative model approach offers a simplified optimization process and real-time forecasting capability with theoretical recovery guarantee.

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [168] [Mapping NVD Records to Their VFCs: How Hard is it?](https://arxiv.org/abs/2506.09702)
*Huu Hung Nguyen,Duc Manh Tran,Yiran Cheng,Thanh Le-Cong,Hong Jin Kang,Ratnadira Widyasari,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 映射国家漏洞数据库（NVD）记录到修复漏洞的提交（VFCs）对于漏洞分析至关重要，但具有挑战性。本研究通过实证方法探讨了这种映射的可行性，构建了一个自动化管道，并结合外部安全数据库和GitHub仓库，成功映射了一部分NVD记录，但也揭示了没有Git链接时的巨大困难。


<details>
  <summary>Details</summary>
Motivation: 将NVD记录映射到对应的漏洞修复提交（VFCs）是进行漏洞分析的关键步骤，然而由于NVD参考文献中显式链接的稀缺性，这一任务充满挑战。因此，需要探索一种可行的方法来实现这种映射。

Method: 通过对NVD参考文献的手动分析，发现Git引用能够使映射成功率超过86%，而非Git引用的成功率不足14%。基于此，构建了一个自动化的管道，从20,360个NVD记录中提取出31,942个VFCs，精度达到87%，主要来自Git引用。为了填补空白，还挖掘了六个外部安全数据库，获得了29,254个VFCs，覆盖18,985个记录，精度为88.4%；同时，从GitHub仓库中额外获取了3,686个VFCs，覆盖2,795个记录，精度为73%。

Result: 最终，通过结合上述方法，成功映射了26,710个独特的NVD记录，占总记录的11.3%，涉及7,634个项目。尽管在处理含有Git引用的记录时取得了较高的成功率，但仍有88.7%的记录未能被映射，凸显了在缺乏Git链接情况下的巨大挑战。

Conclusion: 本研究提供了关于如何改进漏洞数据集以及指导未来自动化安全研究的重要见解，特别是在提升NVD记录与实际修复代码之间的关联性方面。

Abstract: Mapping National Vulnerability Database (NVD) records to vulnerability-fixing
commits (VFCs) is crucial for vulnerability analysis but challenging due to
sparse explicit links in NVD references.This study explores this mapping's
feasibility through an empirical approach. Manual analysis of NVD references
showed Git references enable over 86% success, while non-Git references achieve
under 14%. Using these findings, we built an automated pipeline extracting
31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,
mainly from Git references. To fill gaps, we mined six external security
databases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,
and GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%
precision. Combining these, we mapped 26,710 unique records (11.3% coverage)
from 7,634 projects, with overlap between NVD and external databases, plus
unique GitHub contributions. Despite success with Git references, 88.7% of
records remain unmapped, highlighting the difficulty without Git links. This
study offers insights for enhancing vulnerability datasets and guiding future
automated security research.

</details>


### [169] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: This position paper proposes to treat reasoning depth as a controllable resource in designing code generation models and optimize reasoning budgets across the entire model lifecycle.


<details>
  <summary>Details</summary>
Motivation: The authors observe that there is a trade-off between rapid, direct answers ('fast thinking') and elaborate, chain-of-thought deliberation ('slow thinking') in code generation models, which should be explicitly managed rather than being an incidental byproduct of prompting.

Method: Optimizing reasoning budgets across the entire model lifecycle - from synthetic data creation and benchmarking to real-world deployment. This involves enriching supervision signals, motivating new multi-dimensional benchmarks, and informing cost-aware, security-conscious deployment policies.

Result: Envision coding agents that can think deep when necessary and act fast when possible by viewing fast and slow thinking as complementary modes to be scheduled.

Conclusion: Treating reasoning depth as a controllable resource can unlock superior trade-offs among accuracy, latency, and cost.

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [170] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: This paper explores the extent to which established stakeholder involvement (SHI) practices contribute to responsible AI (rAI) efforts, identifying a disconnect between commercial priorities and rAI goals.


<details>
  <summary>Details</summary>
Motivation: To understand how current SHI practices in commercial software development can be leveraged or adapted for rAI initiatives.

Method: Analysis of 56 rAI guidance documents followed by an online survey (n=130) and semi-structured interviews (n=10) with AI practitioners.

Result: Findings indicate that SHI in practice is mainly driven by commercial interests rather than rAI goals, suggesting a significant disconnect. Established SHI practices largely do not contribute to rAI efforts.

Conclusion: The authors propose interventions and research opportunities to bridge the gap between current SHI practices and rAI goals, aiming to shift industry practice towards more responsible AI development.

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [171] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: The paper presents RuleReasoner, a reinforced rule-based reasoning method for small reasoning models (SRMs), which uses domain-aware dynamic sampling to improve generalization across tasks and domains. It outperforms large reasoning models on both in-distribution and out-of-distribution benchmarks while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To determine if small reasoning models can effectively learn rule-based reasoning with robust generalization across diverse tasks and domains, addressing the challenges posed by deviations in rule formats, types, and complexity in real-world applications.

Method: RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards using a novel domain-aware dynamic sampling approach. This facilitates domain augmentation and flexible online learning schedules for reinforcement learning without requiring pre-hoc human-engineered mix-training recipes.

Result: RuleReasoner significantly outperforms large reasoning models, achieving an average improvement of 4.1% on eight in-distribution tasks and 10.4% on three out-of-distribution tasks over OpenAI-o1. It also exhibits higher computational efficiency compared to prior dynamic sampling methods for reinforcement learning.

Conclusion: RuleReasoner demonstrates that small reasoning models can effectively learn rule-based reasoning with robust generalization. The method's performance and efficiency make it a promising approach for real-world applications.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [172] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: The paper proposes LLM-as-a-qualitative-judge, an approach to evaluate NLG systems by identifying common issue types through open-ended per-instance analysis and clustering. It performs well compared to human annotators.


<details>
  <summary>Details</summary>
Motivation: To provide developers with meaningful insights on improving NLG systems beyond numerical scores.

Method: LLM-as-a-qualitative-judge consists of open-ended per-instance issue analysis and clustering of discovered issues using a cumulative algorithm.

Result: Correctly recognizes instance-specific issues in 2/3 cases and produces error type reports similar to those by human annotators.

Conclusion: This approach offers valuable insights for enhancing NLG systems and is publicly available.

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [173] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: The paper proposes a phrase dictionary biasing method to enhance phrase translation in speech translation tasks, showing significant improvements.


<details>
  <summary>Details</summary>
Motivation: Phrases are crucial for understanding core concepts in conversations but are challenging to translate correctly due to their rarity in training data.

Method: Propose a phrase dictionary biasing method that uses pairs of phrases mapping from the source language to the target language, applied to both a transducer-based streaming speech translation model and a multimodal large language model.

Result: For the streaming speech translation model, the phrase dictionary biasing method outperforms phrase list biasing by 21% relatively. For multimodal large language models, it achieves an 85% relative improvement in phrase recall.

Conclusion: Phrase dictionary biasing is an effective approach for leveraging external phrase information in both streaming speech translation and multimodal large language models.

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [174] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: Transformer语言模型在自然语言领域展示了令人印象深刻的泛化能力，但对这些泛化是如何产生的缺乏细致的理解。本文通过任务关联的视角研究长度泛化能力，并发现相关任务之间可以转移长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前对于Transformer语言模型如何产生泛化能力缺乏深入理解，特别是在处理更长输入时的泛化能力（长度泛化）尚未被充分研究。

Method: 通过任务关联的视角研究长度泛化能力，即从较短输入泛化到较长输入的能力。实验包括在不同算法任务上训练模型，如算术运算、字符串转换和迷宫导航等，观察是否可以通过在更长的相关辅助任务上训练模型，使其在目标任务上泛化到未见过的更长输入。

Result: 结果表明，Transformer模型可以从联合训练的相似任务中继承泛化能力。预训练语言模型也表现出类似的转移效果，说明预训练为模型提供了可重用的计算框架，有助于下游任务中的外推。此外，初步证据显示，长度泛化转移与任务间相同的注意力头重用有关。

Conclusion: 本研究加深了对Transformer如何泛化到分布外输入的理解，并强调了跨任务归纳结构的组合重用的重要性。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [175] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 研究人员开发了一种新的自我锚定注意力模型（SAAM），用于从游戏中发现和分类亲社会行为。该模型在低资源环境下表现良好，相比现有最佳技术提升了7.9%。此研究首次实现了自动分类游戏内亲社会行为的系统，并在《使命召唤：现代战争II》中得到了应用。


<details>
  <summary>Details</summary>
Motivation: 尽管过去的研究主要集中在检测游戏内的毒性内容，但近期研究表明，识别和促进亲社会行为同样重要。然而，这方面的数据集和模型资源非常有限。

Method: 研究结合了无监督学习方法与游戏领域专家的合作，使用整个训练集作为“锚”来增强模型性能。提出了一个名为Self-Anchored Attention Model (SAAM)的新模型，以应对标注数据稀缺的问题。

Result: SAAM模型相较于现有最佳技术提升了7.9%，并成功应用于《使命召唤：现代战争II》的游戏聊天数据，证明了其有效性和可行性。

Conclusion: 本研究为游戏内的亲社会行为识别提供了一个新方法和自动化系统，有助于将管理重点从惩罚毒性行为转向鼓励积极互动。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [176] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: The paper introduces an improved framework called $(RSA)^2$ which better interprets figurative language by focusing on rhetorical strategies rather than speaker's motivations, achieving advanced performance in irony interpretation when combined with LLMs.


<details>
  <summary>Details</summary>
Motivation: Current models either cannot handle figurative expressions or need to model specific motivations for using them, which is setting-specific and complex.

Method: Introduce the Rhetorical-Strategy-Aware RSA $(RSA)^2$ framework that interprets non-literal utterances by considering the speaker's rhetorical strategy instead of their implicit motivations.

Result: $(RSA)^2$ allows human-compatible interpretations of non-literal language and achieves state-of-the-art results on irony interpretation in the PragMega+ dataset when combined with LLMs.

Conclusion: $(RSA)^2$ provides a more effective way to interpret figurative language without needing to model specific motivations.

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [177] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: The paper presents an improved method for detecting Alzheimer's dementia (AD) using a fine-tuned large language model (LLM), enhancing accuracy and providing clear, interpretable decision boundaries.


<details>
  <summary>Details</summary>
Motivation: To enhance the detection of Alzheimer's dementia through advancements in language models, specifically by improving upon the paired perplexity approach.

Method: Utilizing the instruction-following version of Mistral-7B LLM to extend the paired perplexity approach for AD detection, demonstrating its effectiveness with interpretable decision boundaries.

Result: Achieved an average improvement of 3.33% over the best current paired perplexity method and 6.35% over the top-ranked ADReSS 2020 challenge benchmark method.

Conclusion: The proposed method effectively detects AD with interpretable decision-making and shows potential for new approaches in model interpretation and data augmentation.

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [178] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: The paper explores whether Large Language Models (LLMs) possess a form of theory of mind, enabling them to model and reason about the intentions of others, through cooperative multi-agent reinforcement learning (MARL).


<details>
  <summary>Details</summary>
Motivation: Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises the question of whether LLMs can model and reason about the intentions of others.

Method: Investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions.

Result: Aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners by leveraging LLM-based agents capable of natural language interaction.

Conclusion: This work moves towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [179] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: RePO is a new method enhancing policy optimization for LLMs by using diverse replay strategies, achieving better performance with slightly increased computational cost.


<details>
  <summary>Details</summary>
Motivation: Current RL methods like GRPO have high computational costs and low data efficiency when optimizing LLMs.

Method: RePO leverages diverse replay strategies to retrieve off-policy samples from a replay buffer, enabling broader and more diverse sample sets for each prompt.

Result: Experiments show RePO achieves significant performance gains (e.g., +18.4 points for Qwen2.5-Math-1.5B) compared to GRPO, while increasing computational cost by only 15% and boosting effective optimization steps by 48%.

Conclusion: RePO provides an efficient way to optimize LLMs through reinforcement learning, improving performance with manageable computational overhead.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [180] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 研究了小语言模型中的潜在多头注意力（MLA），发现了效率与质量之间的有趣权衡。


<details>
  <summary>Details</summary>
Motivation: 揭示小语言模型中潜在多头注意力（MLA）的效率与质量之间的权衡。

Method: 训练30M参数的GPT模型在10万合成故事上，比较标准多头注意力（MHA）、MLA和带有旋转位置编码的MLA（MLA+RoPE）三种架构变体。

Result: MLA+RoPE在半秩潜在维度（r = d/2）时，KV缓存内存减少45%，验证损失仅增加0.3%；RoPE对小模型中的MLA至关重要，无RoPE时MLA表现较差，有RoPE时超越传统注意力机制；推理测试显示r=d/2的MLA比全秩MLA快1.4倍且保持内存节省；GPT-4评估显示该方法在语法、创造力和一致性方面得分最高（7.4/10）。

Conclusion: MLA+RoPE在小模型中提供了帕累托改进，在内存受限部署中有优势，且代码和模型将在接受后发布。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [181] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: This paper presents COGENT, a framework that generates grade-appropriate educational content by incorporating curriculum components, controlling readability, and adopting a 'wonder-based' approach. It is evaluated through LLM-as-a-judge and human expert analysis, showing comparable or superior results to human references.


<details>
  <summary>Details</summary>
Motivation: Generative AI faces challenges in educational contexts, such as aligning with curriculum standards and maintaining grade-appropriate reading levels. There's also a difficulty in balancing scientific explanations with everyday language in STEM education.

Method: The proposed framework COGENT incorporates three curriculum components (science concepts, core ideas, learning objectives), controls readability via length, vocabulary, and sentence complexity, and adopts a 'wonder-based' approach for student engagement.

Result: Experimental results indicate that COGENT consistently produces passages that are appropriate for the intended grade level and perform comparably or better than human-generated content.

Conclusion: COGENT offers a promising method for creating scalable, adaptive, and high-quality educational resources.

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [182] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在多项选择题回答上表现出色，但对输入扰动很脆弱。本文介绍并评估了标记约束解码（TCD），一种简单有效的推理时算法，通过强制标记级预测的一致性来增强在噪声环境下的稳健性。实验表明，TCD与提示工程结合可显著恢复因输入噪声而降低的性能，尤其对于较弱的模型如Gemma3 1B有高达39%的绝对增益。此外，TCD对过度自信的输出有隐式正则化作用，不同模型需要不同的惩罚计划以最大化弹性。研究结果表明，TCD是一种实用的、与模型无关的方法，可提高LLMs在现实世界不完美条件下的推理稳定性，为在安全关键或用户交互应用中更可靠的部署铺平道路。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多项选择题回答基准测试中表现出色，但它们对微小的输入扰动仍然非常脆弱。为了解决这一问题，提高模型在噪声环境下的稳健性，引入了一种新的推理时算法——标记约束解码（TCD）。

Method: 提出了一种名为标记约束解码（TCD）的推理时算法，该算法通过强制标记级预测之间的一致性来增强在噪声环境下的稳健性。并且，TCD可以与提示工程（PE）结合使用，进一步提升效果。通过广泛的实验，包括CommonsenseQA、MMLU和MMLU-Pro等数据集，评估TCD的效果。

Result: 实验结果显示，TCD与提示工程结合使用时，可以显著恢复因输入噪声而降低的性能，特别是在较弱的模型如Gemma3 1B上，绝对增益高达39%。此外，惩罚扫描分析表明，TCD对过度自信的输出有隐式正则化作用，不同模型需要不同的惩罚计划以最大化弹性。

Conclusion: TCD被证明是一种实用的、与模型无关的方法，能够提高大型语言模型在现实世界不完美条件下的推理稳定性，从而为在安全关键或用户交互应用中更可靠的部署铺平道路。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [183] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: 提出了一种新的监督微调(SFT)方法，无需使用原始SFT数据，即可有效减少灾难性遗忘的风险。通过重建基础模型的可能SFT指令分布，并筛选最优数据与新数据混合进行SFT，该方法在提升特定任务性能的同时保留了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调(SFT)方法虽然提升了大型语言模型(LLMs)遵循指令和适应特定领域任务的能力，但往往会削弱其一般能力。此外，由于无法访问原始预训练数据，第三方实践者在开源模型上实施SFT时，灾难性遗忘的问题往往会被加剧。

Method: 首先重建基础模型的可能SFT指令分布，然后通过多模型筛选过程选择最优数据，最后将这些数据与新数据混合进行SFT。

Result: 实验结果表明，该方法在改进特定任务性能的同时，保留了模型在一般领域的泛化能力。

Conclusion: 所提出的方法在不使用原始SFT数据的情况下，有效地减少了灾难性遗忘的风险，同时保持了模型的一般能力和提高了特定任务的性能。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [184] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: This paper presents GigaChat, a family of Russian LLMs. It reports on their architecture, pre-training, and performance evaluation in Russian and English benchmarks. The top-performing models are available via API, Telegram bot, and Web interface. Three open GigaChat models have been released to promote NLP research for the Russian language.


<details>
  <summary>Details</summary>
Motivation: To address the lack of foundational models specifically tailored to the Russian language due to significant computational resources required.

Method: Introduced GigaChat family of Russian LLMs with various sizes including base models and instruction-tuned versions. Detailed report on model architecture, pre-training process, and experiments to guide design choices.

Result: Evaluated performance on Russian and English benchmarks and compared GigaChat with multilingual analogs. Top-performing models demonstrated via API, Telegram bot, and Web interface.

Conclusion: Released three open GigaChat models in open-source to expand NLP research opportunities and support industrial solutions development for the Russian language.

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [185] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: Theory of Mind (ToM) is difficult for large language models (LLMs). This paper introduces UniToMBench, a benchmark that combines SimToM and TOMBENCH to improve and assess ToM capabilities in LLMs. It uses over 1,000 hand-written scenarios and diverse evaluation metrics. Models like GPT-4o perform well in emotional and belief-related tasks but vary in knowledge-based tasks.


<details>
  <summary>Details</summary>
Motivation: To systematically enhance and evaluate the Theory of Mind (ToM) capabilities in large language models (LLMs), which currently struggle with accurately predicting human mental states.

Method: Developed UniToMBench by integrating strengths of SimToM and TOMBENCH, incorporating multi-interaction task designs and evolving story scenarios. Utilized a custom dataset of over 1,000 hand-written scenarios and combined perspective-taking techniques with diverse evaluation metrics.

Result: Models such as GPT-4o and GPT-4o Mini exhibit high accuracy (>80%) in emotional and belief-related tasks, yet show significant variability in knowledge-based tasks.

Conclusion: UniToMBench effectively highlights both the strengths and limitations of current LLMs in ToM-related tasks, proving valuable for future development.

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [186] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: 在论文中，作者提出了一种统一的旋转位置编码（\ourRoPE）方法，解决了Transformer和状态空间模型（SSM）之间的位置编码不兼容问题。基于此，他们引入了\model混合架构，在4K序列长度下，训练和推理速度分别提高了42.3%和29.5%，并且在语言建模基准上比Transformer基线高出4%以上。此外，\model在扩展性方面表现更优，13亿参数版本相较于3.2亿参数版本平均准确率提升了7.22%。


<details>
  <summary>Details</summary>
Motivation: Transformer擅长捕捉长距离依赖关系，而状态空间模型（SSM）能够实现线性时间序列建模。然而，将这两种架构结合时存在一个主要挑战：它们各自的位置编码机制不同，导致性能下降和不连续性。Transformer使用显式的旋转位置嵌入（RoPE），而SSM通过卷积利用隐式位置表示。这种差异阻碍了混合模型的有效构建。

Method: 为了解决这一问题，作者提出了统一旋转位置编码（\ourRoPE）方法，创建了一个适用于自注意力和状态空间组件的一致位置编码框架。基于此方法，他们开发了\model混合架构，将Transformer层和SSM层在一个统一的位置编码方案下进行连贯集成。

Result: 实验结果表明，\model在4K序列长度下表现出显著的优势：训练速度提高了42.3%，推理速度提高了29.5%，同时在语言建模基准测试中比标准Transformer模型高出超过4%的准确率。此外，\model在扩展性方面也更为出色，13亿参数版本相较于3.2亿参数版本平均准确率提升了7.22%，优于同等规模的Transformer或SSM模型。

Conclusion: 统一位置编码成功地解决了混合模型中的位置不兼容问题，使得高效、高性能的长上下文建模成为可能。

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [187] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: ReasonMed is a large medical reasoning dataset used to train ReasonMed-7B, a sub-10B model that outperforms previous models in medical question answering.


<details>
  <summary>Details</summary>
Motivation: To explore and improve the capabilities of reasoning-based LLMs in knowledge-intensive medical question answering.

Method: Constructing the ReasonMed dataset through a multi-agent verification and refinement process, designing an Error Refiner to enhance reasoning paths, and systematically investigating best practices for training medical reasoning models.

Result: Training ReasonMed-7B using a strategy that combines detailed Chain-of-Thought reasoning with concise answer summaries, which sets a new benchmark for sub-10B models and outperforms prior models by 4.17% and exceeds LLaMA3.1-70B on PubMedQA by 4.60%.

Conclusion: The combination of detailed CoT reasoning with concise answer summaries is the most effective fine-tuning strategy for medical reasoning models, as demonstrated by the superior performance of ReasonMed-7B.

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [188] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: This paper surveys the integration of Knowledge Graphs (KGs) into Large Language Models (LLMs), categorizes existing methods, identifies gaps, and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: To systematically examine the synergy between Knowledge Graphs (KGs) and Large Language Models (LLMs), emphasizing scalability, computational efficiency, and data quality.

Method: Categorizing approaches into KG-enhanced LLMs and LLM-augmented KGs, analyzing their capabilities and mutual benefits.

Result: Identification of critical gaps in current integration methods and emphasis on the importance of scalability, efficiency, and quality in combining structured knowledge with language models.

Conclusion: Future research should focus on neuro-symbolic integration, dynamic KG updating, data reliability, and ethical considerations for more intelligent systems.

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [189] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: The paper addresses issues in Unstructured Knowledge Editing (UKE) for LLMs by constructing new datasets for locality evaluation, identifying factors affecting fine-tuning methods, and providing a training recipe. FT-UKE outperforms SOTA methods.


<details>
  <summary>Details</summary>
Motivation: There is a lack of locality evaluation for UKE and fine-tuning based methods abnormally fail sometimes.

Method: Constructed two datasets (UnKEBench-Loc and AKEW-Loc (CF)) for locality evaluation, identified four factors affecting fine-tuning methods, and provided a training recipe for UKE task.

Result: FT-UKE with optimal setting outperforms existing SOTA methods; advantage increases with batch size from +6.78% to +10.80%.

Conclusion: FT-based method (FT-UKE) is surprisingly strong and provides a systematic approach for future research in UKE.

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [190] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: This paper presents CoRT, a post-training framework that teaches Large Reasoning Models (LRMs) to effectively and efficiently use Code Interpreter (CI). It introduces Hint-Engineering to optimize LRM-CI interaction by synthesizing code-integrated reasoning data. Experimental results show significant improvements in mathematical reasoning tasks with fewer tokens used.


<details>
  <summary>Details</summary>
Motivation: LRMs like o1 and DeepSeek-R1 are powerful for natural language reasoning but struggle with complex mathematical operations. Combining LRMs with computational tools such as CI can address these limitations, but the direct combination is inefficient due to external knowledge introduced by CI.

Method: The authors developed CoRT, which uses Hint-Engineering to synthesize code-integrated reasoning data. They manually created 30 high-quality samples and post-trained models with parameters ranging from 1.5B to 32B using supervised fine-tuning, rejection fine-tuning, and reinforcement learning.

Result: Hint-Engineering models achieved 4% and 8% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively across five challenging mathematical reasoning datasets. The models also used fewer tokens compared to natural language models.

Conclusion: CoRT successfully teaches LRMs to leverage CI effectively and efficiently, improving performance in mathematical reasoning tasks while reducing token usage.

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [191] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: The paper introduces EmoNet-Voice, including a large-scale pre-training dataset and a benchmark dataset with human expert annotations, designed to evaluate SER models on a fine-grained spectrum of emotions. It also presents Empathic Insight Voice models that perform well in speech emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Current speech emotion recognition datasets have limitations such as lack of emotional granularity, privacy issues, or reliance on acted portrayals.

Method: Created EmoNet-Voice with EmoNet-Voice Big (a large-scale pre-training dataset) and EmoNet-Voice Bench (a benchmark dataset with human expert annotations). Used state-of-the-art voice generation to curate synthetic audio snippets and conducted rigorous validation by psychology experts.

Result: Empathic Insight Voice models show high agreement with human experts in speech emotion recognition. High-arousal emotions are easier to detect than low-arousal states.

Conclusion: EmoNet-Voice provides a robust resource for evaluating the emotional understanding capabilities of AI systems.

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [192] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: Out-of-context and misattributed imagery is a major issue in misinformation. This paper introduces the News Media Provenance Dataset with two tasks (LOR and DTOR) to address this issue, providing baseline results using six large language models.


<details>
  <summary>Details</summary>
Motivation: The existing methods for detecting out-of-context or misattributed imagery often only consider whether the semantics of the imagery corresponds to the text narrative, missing manipulations if objects or scenes somewhat correspond to the narrative.

Method: Introduced News Media Provenance Dataset with provenance-tagged images. Two tasks were formulated: location of origin relevance (LOR) and date and time of origin relevance (DTOR). Baseline results on six large language models (LLMs) were presented.

Result: Zero-shot performance on LOR task was promising, but performance on DTOR task hindered, suggesting room for specialized architectures and future work.

Conclusion: While there is promising potential in detecting out-of-context imagery through the LOR task, challenges remain in accurately assessing date and time relevance, indicating a need for further advancements in this area.

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [193] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: The paper proposes a causal framework to enhance Chain-of-Thought (CoT) reasoning in large language models by addressing the challenges of sufficiency and necessity, leading to improved reasoning efficiency and cost-effectiveness without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought prompting is crucial for complex reasoning in large language models but faces challenges related to sufficiency and necessity of inference steps.

Method: A causal framework is introduced that uses causal Probability of Sufficiency and Necessity to identify and quantify essential inference steps, allowing for automated addition of missing steps and removal of redundant ones.

Result: Experiments on various benchmarks show significant improvements in reasoning efficiency and reduced token usage while maintaining accuracy.

Conclusion: This work offers a promising approach to improve reasoning performance and cost-effectiveness in large language models.

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [194] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: A new method for detecting hallucinations in LLMs by analyzing the probabilistic divergence between prompt and response hidden-state distributions is presented. It outperforms existing baselines and offers a robust, scalable solution.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs often arise from superficial rephrasing rather than substantive reasoning, and there is a need for a model-intrinsic detection method that eliminates the need for external knowledge or auxiliary models.

Method: Propose using distributional distances as principled hallucination scores and employing deep learnable kernels to enhance sensitivity to capture nuanced geometric differences between distributions.

Result: The approach outperforms existing baselines and demonstrates state-of-the-art performance on several benchmarks. It remains competitive even without kernel training.

Conclusion: This novel approach provides a robust and scalable solution for hallucination detection in LLMs.

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [195] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）逐渐形成一个核心的语言无关参数空间，支持抽象思维的出现。我们发现共享神经元在这一过程中起到关键作用，并提出了针对LLMs不同发展阶段的神经元特定训练策略。


<details>
  <summary>Details</summary>
Motivation: 尽管初步研究表明LLMs可能以英语为'思考'基础，但其多语言性能的提升对这一假设提出了挑战。本研究旨在探索LLMs是否发展出一种超越具体语言系统的抽象思维能力。

Method: 识别与语言相关的神经元，将其分类为共享神经元（跨多种语言激活）和独占神经元（特定于一种语言）。通过观察LLMs的发展过程，分析共享神经元和独占神经元的比例及功能重要性变化。提出基于神经元特定的训练策略，适应LLMs在不同发展阶段的语言无关水平。

Result: 发现LLMs逐渐形成一个核心的语言无关参数空间，其中共享神经元占据关键地位，其比例和功能重要性随时间增加，而独占神经元的影响逐渐减弱。实验验证了提出的训练策略的有效性。

Conclusion: LLMs通过发展共享神经元构建了一个核心语言无关参数空间，支持超越具体语言的抽象思维能力。提出的神经元特定训练策略可有效提升LLMs的多语言性能。

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [196] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: POET, a new training approach, improves Direct Alignment Algorithms (DAAs) by addressing the reward-generation gap, leading to better performance in aligning LLMs with human preferences.


<details>
  <summary>Details</summary>
Motivation: Direct Alignment Algorithms (DAAs) have limitations due to the 'reward-generation gap', which is caused by a mismatch between the importance of prefix tokens during LLM generation and their reflection in DAAs' implicit reward functions.

Method: Prefix-Oriented Equal-length Training (POET) truncates both preferred and dispreferred responses to match the shorter one's length, resulting in diverse truncated lengths across samples. This constrains the optimization of DAAs' objective to converge across all positions, focusing more on prefix tokens.

Result: Experiments with DPO and SimPO show POET improves over standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements in downstream tasks.

Conclusion: The study emphasizes the significance of resolving the misalignment between reward optimization and generation performance in DAAs.

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [197] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）虽然推动了对话式人工智能助手的发展，但评估这些助手在任务执行中适应个人用户偏好的能力仍具挑战性。现有的个性化基准主要集中在闲聊、非对话任务或狭窄领域，无法涵盖个性化任务导向协助的复杂性。为了解决这个问题，我们引入了PersonaLens，这是一个全面评估任务导向型AI助手个性化的基准。该基准包括多样的用户画像，丰富的偏好和交互历史，以及两个专门的基于LLM的代理：一个用户代理与AI助手进行实际的任务导向对话，一个法官代理根据LLM-as-a-Judge范式来评估个性化、响应质量和任务成功。通过广泛的实验，我们发现当前LLM助手在个性化能力上存在显著差异，这为推进对话式AI系统提供了关键见解。


<details>
  <summary>Details</summary>
Motivation: 作者指出目前缺乏一个能够系统性评估对话式AI助手在执行任务时个性化能力的工具或标准。现有的个性化基准测试大多集中于闲聊、非对话任务或者特定狭窄领域，无法充分反映任务导向型个性化协助的复杂性。因此，需要一个更全面的基准来评估AI助手如何根据用户的偏好调整其行为，特别是在完成任务的过程中。

Method: 研究人员提出了名为PersonaLens的全新基准测试框架。这个框架具有以下特点：包含多样化的用户画像，每个画像都配备了详细的偏好和交互历史；设计了两个基于LLM的特殊代理——用户代理和法官代理。用户代理可以与AI助手进行真实的任务导向型对话，而法官代理则使用LLM-as-a-Judge方法对个性化程度、回复质量及任务完成情况进行评价。

Result: 通过对多个现有LLM助手在不同任务上的广泛实验，研究团队发现了这些助手在个性化能力方面的显著差异。部分助手能够较好地根据用户偏好调整回应，而另一些则表现平平。这一结果表明，尽管当前的LLM技术已经取得了一定进展，但在实现真正个性化的任务导向型对话方面仍有很大的提升空间。

Conclusion: 本研究通过开发PersonaLens基准测试，填补了系统性评估任务导向型AI助手个性化能力的空白。实验结果揭示了当前LLM助手在个性化方面的不足，并为未来改进对话式AI系统提供了重要参考。研究人员呼吁业界关注个性化能力的提升，并利用类似PersonaLens这样的工具来指导未来的研发工作。

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [198] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 该研究通过分析YouTube上181个经历致命尝试的用户频道和134个对照频道，结合自底向上、混合和自顶向下的方法，揭示了自杀行为在数字平台上的表现形式及其与专家知识的差异。研究表明，心理健康斗争和YouTube参与度是与自杀企图相关的显著主题，并且个人分享动机在不同阶段存在显著变化。


<details>
  <summary>Details</summary>
Motivation: 鉴于自杀仍然是西方国家的主要死亡原因之一，同时社交媒体在日常生活中占据重要地位，研究希望通过分析数字足迹来揭示自杀行为的表现形式，从而为预防和干预提供新视角。

Method: 研究使用了三种互补方法：基于LLM的主题建模（自底向上）、临床专家审查（混合方法）以及心理学评估（自顶向下）。数据集包括181个经历过致命尝试的YouTube用户频道和134个对照频道。通过主题建模识别行为指标，专家审查标记自杀相关主题，并对叙述进行心理评估。

Result: 发现了五个与自杀企图相关的主题，其中心理健康斗争和YouTube参与度显示出与自杀企图相关的时间变化。混合方法中，专家标记了19个自杀相关主题，但未发现额外的时间效应。心理学评估显示，在上传期间尝试自杀的个体更倾向于将经历视为个人康复的一部分，而在上传前尝试的个体更希望帮助他人。

Conclusion: 通过整合自底向上、混合和自顶向下的方法，研究提供了关于自杀行为的细致理解，强调了数字行为与临床见解之间的联系。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [199] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: The paper presents VerIF, a verification method for reinforcement learning in instruction following that combines rule-based code verification with LLM-based verification. Using the VerInstruct dataset, they train two models which show significant improvements and reach state-of-the-art performance without affecting general capabilities. The resources are publicly available.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored best practices for reinforcement learning in instruction following and to improve the verification challenge in this context.

Method: Propose VerIF, a verification method combining rule-based code verification with LLM-based verification from a large reasoning model. Construct a high-quality instruction-following dataset called VerInstruct with about 22,000 instances. Apply RL training with VerIF to two models.

Result: Achieve significant improvements across several representative instruction-following benchmarks. Reach state-of-the-art performance among comparable models and generalize well to unseen constraints without affecting general capabilities.

Conclusion: RL with VerIF can be integrated into existing RL recipes to enhance overall model performance.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [200] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: 提出了一种新的框架RAPL，用于知识图谱问答中的高效和有效的图检索，通过三个方面的改进，在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在跨领域的归纳推理能力方面表现出色，但其可靠性受到过时知识和幻觉的影响。现有的检索增强生成（RAG）管道依赖于非结构化文本，限制了可解释性和结构化推理。知识图谱提供了一种更结构化和紧凑的替代方案，但仍面临泛化能力的挑战。

Method: RAPL通过三个方面改进：(1) 两阶段标记策略，结合启发式信号与参数模型以提供因果监督；(2) 模型无关的图转换方法，捕捉三元组内和三元组间的交互，增强表示能力；(3) 基于路径的推理策略，利用注入的理性知识学习，并通过结构化输入支持下游推理器。

Result: 实验上，RAPL比现有最先进方法高出2.66%-20.34%，显著缩小了小型和更强大的LLM推理器之间的性能差距，以及跨数据集设置下的差距，突显了其优越的检索能力和泛化性。

Conclusion: RAPL作为一种新颖的框架，通过改进的知识图谱检索方法，在知识图谱问答任务中展现了更高的效率、效果和泛化能力。

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [201] [Revisiting Graph Projections for Effective Complementary Product Recommendation](https://arxiv.org/abs/2506.09209)
*Leandro Anghinoni,Pablo Zivic,Jorge Adrian Sanchez*

Main category: cs.IR

TL;DR: The paper proposes a method for predicting complementary products using a directed weighted graph derived from user-item interactions, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Complementary product recommendation can enhance customer experience and sales, but it's challenging due to the noisy and sparse nature of user-item interactions.

Method: Propose a method based on the structure of a directed weighted graph projected from the user-item bipartite graph to predict complementary products given a query item.

Result: Achieved an average improvement of +43% over sequential recommenders and +38% over graph-based recommenders across different benchmarks.

Conclusion: The proposed method is simple yet effective in predicting complementary products, outperforming recent methods.

Abstract: Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [202] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: Cryo-EM uses computational methods to infer 3D molecular structures from noisy 2D images. A new framework, CryoSPIRE, with a hierarchical Gaussian mixture model addresses challenges of particle flexibility and variation, revealing biologically meaningful structures and setting a new benchmark standard.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the modeling of 3D molecular structures in cryo-electron microscopy, especially when particles exhibit non-rigid conformational flexibility and compositional variation.

Method: A novel 3D reconstruction framework called CryoSPIRE is introduced, which employs a hierarchical Gaussian mixture model partly inspired by Gaussian Splatting for 4D scene reconstruction. It involves an initial process that infers part-based segmentation of the particle.

Result: CryoSPIRE reveals biologically meaningful structures on complex experimental datasets and sets a new state-of-the-art on the CryoBench benchmark for cryo-EM heterogeneity methods.

Conclusion: CryoSPIRE effectively handles both conformational and compositional variability in cryo-EM data, providing a significant advancement in the field.

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


### [203] [Detecting malignant dynamics on very few blood sample using signature coefficients](https://arxiv.org/abs/2506.09097)
*Rémi Vaucher,Stéphane Chrétien*

Main category: q-bio.QM

TL;DR: An approach combining continuous time Markov modelling and Signature theory is proposed to detect aggressive cancer tumors based on blood sample analysis, addressing data scarcity and demonstrating efficiency through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Recent discoveries indicate that using circulating tumor DNA (ctDNA) levels in blood samples provides accurate cancer monitoring with minimal patient burden. Monitoring ctDNA dynamics might enable early multi-cancer detection.

Method: The method combines continuous time Markov modelling for the dynamics of ctDNA levels with Signature theory for feature extraction from irregularly sampled signals, such as blood samples.

Result: The proposed pipeline successfully overcomes data scarcity issues due to limited blood samples per patient and shows efficiency confirmed by extensive numerical experiments.

Conclusion: The combination of continuous time Markov modelling and Signature theory presents a promising approach for detecting aggressive cancer tumors based on blood sample analysis.

Abstract: Recent discoveries have suggested that the promising avenue of using
circulating tumor DNA (ctDNA) levels in blood samples provides reasonable
accuracy for cancer monitoring, with extremely low burden on the patient's
side. It is known that the presence of ctDNA can result from various mechanisms
leading to DNA release from cells, such as apoptosis, necrosis or active
secretion. One key idea in recent cancer monitoring studies is that monitoring
the dynamics of ctDNA levels might be sufficient for early multi-cancer
detection. This interesting idea has been turned into commercial products, e.g.
in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for
detecting aggressive cancer tumors based on the analysis of blood samples. Our
approach combines tools from continuous time Markov modelling for the dynamics
of ctDNA levels in the blood, with Signature theory for building efficient
testing procedures. Signature theory is a topic of growing interest in the
Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now
recognised as a powerful feature extraction tool for irregularly sampled
signals. The method proposed in the present paper is shown to correctly address
the challenging problem of overcoming the inherent data scarsity due to the
extremely small number of blood samples per patient. The relevance of our
approach is illustrated with extensive numerical experiments that confirm the
efficiency of the proposed pipeline.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [204] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: ReStNet是一种可重用和可拼接的网络，通过动态组合两个预训练模型来适应不同的资源限制。它通过层间相似性计算选择最佳拼接点，并仅微调拼接层以快速适应变化的预算。ReStNet支持同质和异质模型拼接，在多个基准测试中表现出灵活的准确性和效率权衡，同时显著降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，越来越多的预训练模型被公开，但在实际物联网应用中部署这些固定模型具有挑战性，因为不同设备具有异构的计算和内存资源。传统压缩方法虽然可以提高效率，但一旦应用便变得不灵活，无法适应变化的资源约束。

Method: 提出了一种名为ReStNet的可重用和可拼接网络，该方法通过将两个预训练模型拼接在一起动态构建混合网络。具体来说，通过Centered Kernel Alignment (CKA)计算层间相似性来确定最佳拼接点，保留大容量模型的早期层并附加小模型的深层。为了高效部署，仅对拼接层进行微调。此外，ReStNet支持同质（如CNN-CNN）和异质（如CNN-Transformer）模型的拼接。

Result: 广泛的实验表明，ReStNet能够在运行时实现灵活的准确性与效率权衡，同时显著减少训练成本。

Conclusion: ReStNet为适应不同资源限制提供了一种灵活的解决方案，能够动态构建混合网络并快速适应变化的预算。

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [205] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: The paper presents a novel inference-time defense strategy for generative medical vision-language models (Med-VLMs) to mitigate harmful queries and defend against visual and textual jailbreak attacks. This is demonstrated using diverse medical imaging datasets, with findings indicating that synthetic clinical demonstrations enhance model safety without significantly affecting performance. Additionally, a mixed demonstration strategy is proposed to balance security and performance under budget constraints.


<details>
  <summary>Details</summary>
Motivation: Generative medical vision-language models (Med-VLMs) are vulnerable to harmful queries and jailbreak attacks, yet there is a lack of exploration into their security vulnerabilities. There is also the risk of over-defense which may degrade general performance by rejecting benign clinical queries.

Method: The authors propose an inference-time defense strategy based on synthetic clinical demonstrations to enhance the safety of Med-VLMs without significantly compromising their performance. They use diverse medical imaging datasets from nine modalities to test this strategy. Furthermore, they introduce a mixed demonstration strategy as a trade-off solution for balancing security and performance when there are few-shot demonstration budget constraints.

Result: The defense strategy based on synthetic clinical demonstrations successfully enhances model safety without significantly compromising performance. Increasing the demonstration budget alleviates the over-defense issue, where safety mechanisms might otherwise degrade general performance. The mixed demonstration strategy provides a balance between security and performance under budget constraints.

Conclusion: This research demonstrates the effectiveness of an inference-time defense strategy in enhancing the safety of Med-VLMs while maintaining performance. It also highlights the benefits of using synthetic clinical demonstrations and the potential of a mixed demonstration strategy for balancing security and performance.

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [206] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: The paper proposes SAGE, a method for safe text-to-image generation using diffusion models. It introduces semantic-augment erasing and global-local collaborative retention mechanism to efficiently erase unsafe content and retain irrelevant concepts.


<details>
  <summary>Details</summary>
Motivation: To address safety risks in diffusion models such as unsafe content generation and copyright infringement by improving concept erasing techniques beyond fixed word erasure.

Method: SAGE uses semantic-augment erasing which transforms concept word erasure into concept domain erasure through cyclic self-check and self-erasure, exploring the boundary representation of concept domain via semantic spatial relationships. It also employs a global-local collaborative retention mechanism to mitigate degradation of irrelevant concepts.

Result: Extensive experiments show that SAGE has comprehensive superiority compared to other methods in safely generating content with diffusion models.

Conclusion: SAGE is an effective solution for safe text-to-image generation in diffusion models, and its code and weights will be open-sourced.

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [207] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: In this paper, a model named SAAF is proposed to achieve automatic segmentation of building facade walls and windows. It uses multimodal semantic guidance to improve the accuracy and generalization ability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to improve the efficiency of building information models and computer-aided design through the automatic segmentation of walls and windows in architecture.

Method: SAAF has a multimodal semantic collaborative feature extraction mechanism that fuses text description semantics with image features using natural language processing technology. An end-to-end training framework was developed to allow the model to learn the mapping relationship from text descriptions to image segmentation autonomously.

Result: Extensive experiments on multiple facade datasets showed that SAAF outperformed existing methods in the mIoU metric, indicating high-precision segmentation ability.

Conclusion: The SAAF model improves the accuracy and generalization ability of wall and window segmentation. It provides a reference for architectural computer vision technology and explores new ideas for multimodal learning applications in architecture.

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [208] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: Recent advancements in multimodal large language models have successfully extended the Reason-Then-Respond paradigm to image-based reasoning, yet video-based reasoning remains an underdeveloped frontier. To bridge this gap, we introduce DarkEventInfer and MixVidQA, two novel datasets specifically designed to stimulate the model's advanced video understanding and reasoning abilities. Leveraging these carefully curated training samples together with reinforcement learning guided by diverse reward functions, we develop VersaVid-R1, the first versatile video understanding and reasoning model under the Reason-Then-Respond paradigm capable of handling multiple-choice and open-ended question answering, as well as video captioning tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to address the limitations in current video-based reasoning capabilities of multimodal large language models. This is primarily due to the scarcity of high-quality reasoning-oriented data and effective training methodologies for videos.

Method: The researchers introduced two novel datasets: DarkEventInfer and MixVidQA. DarkEventInfer presents videos with masked event segments, requiring models to infer the obscured content based on contextual video cues. MixVidQA presents interleaved video sequences composed of two distinct clips, challenging models to isolate and reason about one while disregarding the other. Using these datasets and reinforcement learning guided by diverse reward functions, they developed VersaVid-R1.

Result: Extensive experiments demonstrate that VersaVid-R1 significantly outperforms existing models across a broad spectrum of benchmarks covering video general understanding, cognitive reasoning, and captioning tasks.

Conclusion: VersaVid-R1 is the first versatile video understanding and reasoning model under the Reason-Then-Respond paradigm capable of handling multiple-choice and open-ended question answering, as well as video captioning tasks.

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [209] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: The paper introduces FlagEvalMM, an open-source evaluation framework for multimodal models that separates model inference from evaluation to allow flexible resource allocation and easy integration of new tasks and models. It leverages advanced tools for faster evaluation and provides accurate insights into model capabilities.


<details>
  <summary>Details</summary>
Motivation: There is a need for a comprehensive and efficient evaluation framework for multimodal models that can handle a variety of vision-language understanding and generation tasks.

Method: FlagEvalMM decouples model inference from evaluation using an independent evaluation service, allowing for flexible resource allocation and easy addition of new tasks and models. It also uses advanced inference acceleration tools and asynchronous data loading to improve evaluation efficiency.

Result: Extensive experiments demonstrate that FlagEvalMM provides accurate and efficient evaluations of model strengths and limitations.

Conclusion: FlagEvalMM is a valuable tool for advancing multimodal research and is publicly available on GitHub.

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [210] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: AVA-Bench is a new benchmark that separates 14 Atomic Visual Abilities to evaluate Vision Foundation Models more accurately.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for Vision Foundation Models (VFMs) have blind spots such as misalignment between instruction tuning data and VQA test distributions, and the inability to isolate specific visual abilities that lead to errors.

Method: Introduced AVA-Bench, a benchmark that disentangles 14 Atomic Visual Abilities to pinpoint VFMs' strengths and weaknesses by matching training and test distributions within each ability.

Result: Using AVA-Bench on leading VFMs reveals distinctive 'ability fingerprints', aiding in more precise VFM selection. Also, a smaller LLM (0.5B) can yield similar VFM rankings as a larger one (7B) while significantly reducing GPU hours.

Conclusion: AVA-Bench provides a comprehensive and transparent way to evaluate VFMs, potentially guiding the development of the next generation of these models.

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [211] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: BakuFlow is a semi-automatic label generation tool with features like live adjustable magnifier, interactive data augmentation, label propagation and an automatic labeling module based on modified YOLOE framework. It significantly reduces the workload of labeling for object detection and tracking.


<details>
  <summary>Details</summary>
Motivation: Accurately labeling data is still a bottleneck in computer vision, especially for large-scale tasks where manual labeling is time-consuming and error-prone.

Method: Key features include (1) a live adjustable magnifier for pixel-precise manual corrections; (2) an interactive data augmentation module to diversify training datasets; (3) label propagation for rapidly copying labeled objects between consecutive frames; (4) an automatic labeling module powered by a modified YOLOE framework.

Result: Substantially reduces labeling workload and improves efficiency in practical computer vision and industrial scenarios.

Conclusion: BakuFlow makes object detection and tracking more effective, reducing labeling efforts.

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [212] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为自回归对抗性后训练（AAPT）的新方法，将预训练的潜在视频扩散模型转换为实时、交互式的视频生成器。该方法通过单次神经函数评估（1NFE）自回归地生成潜在帧，利用对抗性训练范式进行自回归生成，使模型在长视频生成过程中减少误差累积，同时提高了实时性和交互性。实验表明，8B参数规模的模型可以在单一H100上以736x416分辨率24fps实时生成长达一分钟的视频。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视频生成模型计算量过大，限制了其在实时和交互应用中的使用。为了克服这一限制，研究者希望开发一种更高效的模型架构，能够在保持高质量视频生成的同时实现实时性和交互性。

Method: 采用自回归对抗性后训练（AAPT）方法，对预训练的潜在视频扩散模型进行转换。具体来说，模型通过单次神经函数评估（1NFE）逐帧生成潜在帧，并利用KV缓存优化一步生成效率。此外，模型采用学生强制训练方式，有效减少了长视频生成过程中的误差累积。

Result: 实验结果表明，8B参数规模的模型能够实现实时、24fps的视频流生成，分辨率为736x416时仅需单一H100 GPU，而分辨率为1280x720时则需要8个H100 GPU。生成视频长度可达一分钟（1440帧）。

Conclusion: AAPT方法成功将预训练的潜在视频扩散模型转化为高效、实时、交互式的视频生成器，在降低计算复杂度的同时，保证了生成视频的质量，适用于实际应用场景。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [213] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

TL;DR: 提出了一种使用姿态迁移生成合成人体动作视频数据的方法，该方法在动作识别任务上表现出色，并能有效扩展少样本数据集。


<details>
  <summary>Details</summary>
Motivation: 在视频理解任务中，特别是涉及人类运动的任务，合成数据生成通常存在令人不安的特征，从而削弱了其训练效果。因此，诸如手语翻译、手势识别和自动驾驶中的人类运动理解等任务无法充分利用合成数据的潜力。

Method: 提出了一种使用姿态迁移（具体来说是可控的3D高斯化身模型）生成合成人体动作视频数据的方法。并在Toyota Smarthome和NTU RGB+D数据集上评估了该方法。

Result: 该方法提高了动作识别任务的性能，能够有效扩展少样本数据集，弥补了真实训练数据中代表性不足的群体，并增加了多样化的背景。

Conclusion: 作者开源了该方法以及RANDOM People数据集，该数据集包含从互联网众包的新的人体身份的视频和化身，用于姿态迁移。

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [214] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: Recent advancements in Large Multimodal Models (LMMs) have improved multimodal understanding and generation, but they still struggle to generate tightly interleaved image-text outputs. To address this, the authors introduce InterSyn, a large-scale multimodal dataset constructed using their Self-Evaluation with Iterative Refinement (SEIR) method, and SynJudge, an automatic evaluation model.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is the limitation of current training datasets for LMMs which leads to struggles in generating tightly interleaved image-text outputs.

Method: The method involves introducing InterSyn, a large-scale multimodal dataset constructed using the SEIR method, and SynJudge, an automatic evaluation model designed to assess multimodal outputs along four dimensions.

Result: Experimental studies show that the SEIR method improves dataset quality and LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics.

Conclusion: InterSyn and SynJudge are effective solutions for advancing the capabilities of LMMs in generating high-quality, interleaved image-text outputs.

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [215] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TL;DR: The paper presents TOGA, a vision-language model for temporally grounded open-ended video QA with weak supervision. It jointly generates answers and temporal grounding, achieving state-of-the-art performance on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of video question answering with temporal grounding in a weakly supervised setup without any temporal annotations.

Method: Propose TOGA, a vision-language model that jointly generates the answer and the temporal grounding. Generate pseudo labels for temporal grounding and ensure their validity by imposing a consistency constraint.

Result: Achieves state-of-the-art performance on both grounded QA and open-ended QA tasks on several benchmarks.

Conclusion: TOGA effectively addresses the problem of video QA with temporal grounding under weak supervision, improving performance on both QA and grounding.

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [216] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: ReVisiT is a decoding method for LVLMs that uses vision tokens to guide text generation, enhancing visual grounding with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Current decoding strategies of LVLMs often fail to successfully utilize visual information, leading to visually ungrounded responses. Existing solutions usually require additional training or complex inference procedures.

Method: ReVisiT leverages semantic information in vision tokens by projecting them into the text token distribution space and dynamically selecting the most relevant vision token at each decoding step through constrained divergence minimization. The selected vision token refines the output distribution to better incorporate visual semantics.

Result: Experiments on three LVLM hallucination benchmarks show that ReVisiT consistently enhances visual grounding with minimal computational overhead, achieving competitive or superior results compared to state-of-the-art baselines while reducing computational costs up to 2 times.

Conclusion: ReVisiT is a simple and effective decoding method that improves visual grounding in LVLMs without significant computational burden.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [217] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 提出了一种新的方法Symbolic Graph Ranker (SGR)，通过利用大型语言模型（LLMs）的能力，结合文本和图结构信息进行会话搜索。实验结果表明该方法在两个基准数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前的会话搜索策略通常侧重于序列建模以实现深度语义理解，但忽略了交互中的图结构；而一些关注捕捉结构信息的方法则使用了泛化的文档表示，忽视了词级别的语义建模。

Method: 提出了Symbolic Graph Ranker (SGR)方法，首先引入一套符号语法规则将会话图转换为文本，使会话历史、交互过程和任务指令能够无缝整合为LLMs的输入。此外，设计了一系列自监督的符号学习任务（如链接预测、节点内容生成和生成对比学习），以增强LLMs对图结构的捕获能力。

Result: 在AOL和Tiangong-ST两个基准数据集上的实验结果和综合分析验证了该方法的优越性。

Conclusion: 该方法提供了一种新颖且有效的方式，弥合了传统搜索策略与现代LLMs之间的差距。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [218] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Main category: cs.CV

TL;DR: The paper presents AD^2-Bench, a new benchmark for evaluating Chain-of-Thought reasoning in Multi-Modal Large Models designed for autonomous driving under adverse weather and complex traffic conditions. It provides comprehensive data coverage, fine-grained annotations, and a dedicated evaluation framework, revealing current model limitations and advancing research.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks do not adequately evaluate Chain-of-Thought reasoning processes in challenging scenarios such as autonomous driving in adverse weather and complex traffic environments.

Method: Introduced AD^2-Bench, the first Chain-of-Thought benchmark for autonomous driving with adverse weather and complex scenes. It includes over 5.4k high-quality manually annotated instances with explicit ground truth for each intermediate reasoning step.

Result: Evaluation of state-of-the-art MLLMs on AD^2-Bench showed accuracy below 60%, indicating the difficulty of the benchmark and the need for more robust models.

Conclusion: AD^2-Bench serves as a standardized evaluation platform to enhance Multi-Modal Large Models' reasoning capabilities in autonomous driving, promoting further research and development.

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [219] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: 本文提出了BG-HOP生成先验模型，解决了双手机动交互数据不足的问题，并展示其生成双手机动和合成抓取的能力，最后发布了相关代码和模型。


<details>
  <summary>Details</summary>
Motivation: 双手机动物体交互数据有限的问题。

Method: 通过扩展现有的单手生成先验，提出了一种新的生成先验BG-HOP，用于在3D中建模双手与物体的交互。

Result: 实验展示了该模型生成双手机动和为给定物体合成抓取的能力。

Conclusion: 代码和模型公开发布。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [220] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Main category: cs.CV

TL;DR: The paper introduces HSENet, a framework using enriched 3D medical visual cues for accurate vision-language understanding. It employs dual-3D vision encoders and Spatial Packer to achieve state-of-the-art performance in 3D language-visual retrieval, medical report generation, and visual question answering.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing methods that mainly focus on 2D medical images, leading to misinterpretation of subtle pathologies and causing diagnostic hallucinations.

Method: HSENet uses dual-3D vision encoders to perceive global volumetric contexts and fine-grained anatomical details, pre-trained by dual-stage alignment with diagnostic reports. It also proposes Spatial Packer, an efficient multimodal projector that condenses high-resolution 3D spatial regions into informative visual tokens via centroid-based compression.

Result: Achieves state-of-the-art performance in 3D language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering (73.60% of Major Class Accuracy, +1.99% gain).

Conclusion: HSENet effectively exploits enriched 3D medical visual cues for accurate and robust vision-language understanding.

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [221] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: Autoencoders are improved by compressing pixels into latent space through visual tokenization. DGAE uses a diffusion model to guide the decoder, reducing performance degradation and achieving better results with smaller latent space.


<details>
  <summary>Details</summary>
Motivation: To address training instability caused by GAN in autoencoders and minimize latent space dimensionality for more efficient representations.

Method: Propose DGAE which employs a diffusion model to guide the decoder in recovering informative signals not fully decoded from the latent representation.

Result: Effectively mitigates performance degradation under high spatial compression rates and achieves state-of-the-art performance with a 2x smaller latent space. Shows competitive performance on image generation and faster convergence of diffusion models when integrated.

Conclusion: DGAE improves decoder expressiveness, reduces latent space size, and enhances performance in image generation tasks.

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [222] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: Recent advances in reasoning-centric models are explored for their robustness against misleading user input. A systematic evaluation of three state-of-the-art reasoning models across three multimodal benchmarks reveals significant accuracy drops following gaslighting negation prompts. A new diagnostic benchmark, GaslightingBench-R, is introduced to further probe this vulnerability, inducing even more dramatic failures.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the ability of reasoning-centric models to withstand misleading user input and reveal potential vulnerabilities.

Method: Conduct a systematic evaluation of three state-of-the-art reasoning models across three multimodal benchmarks using gaslighting negation prompts. Introduce GaslightingBench-R, a new diagnostic benchmark constructed by filtering and curating challenging samples from existing benchmarks.

Result: Significant accuracy drops (25-29% on average) were observed following gaslighting negation prompts. GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average.

Conclusion: The findings reveal fundamental limitations in the robustness of reasoning models, highlighting the gap between step-by-step reasoning and belief persistence.

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [223] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: 提出了一种新的混合神经架构FasterSNN，结合了生物启发的LIF神经元、区域自适应卷积和多尺度脉冲注意力，用于阿尔茨海默病的早期诊断。实验表明，该方法在保持诊断准确性的同时，显著提高了效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前阿尔茨海默病（AD）的早期诊断受到主观评估和高昂成本的限制，而现有的深度学习方法虽然提供自动化选择，但能源效率低下且计算需求高。尖峰神经网络（SNNs）作为一种脑启发范式，适合建模AD中稀疏、事件驱动的神经退化模式，但在复杂医疗任务中的有效性受限于表达能力弱和训练不稳定。

Method: 提出了FasterSNN，一种混合神经架构，整合了生物启发的LIF神经元、区域自适应卷积和多尺度脉冲注意力，以实现对3D MRI的稀疏、高效处理，同时保持诊断准确性。

Result: 在基准数据集上的实验表明，FasterSNN实现了具有竞争力的性能，并大幅提高了效率和稳定性，支持其在实际AD筛查中的潜力。

Conclusion: FasterSNN为解决现有SNNs在复杂医疗任务中的局限性提供了有效方案，展示了其在实际AD筛查中的潜在应用价值。

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [224] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: The paper explores bias in unconditional generative AI models, defining it as the difference in attribute probability between observed and ideal distributions. Through experiments, they find small attribute shifts which are sensitive to the classifiers used, especially for spectrum attributes. This highlights the need for better labeling practices and scrutiny of evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: There is growing concern about representational harm and discriminatory outcomes from generative AI models. However, the mechanisms by which bias emerges, particularly in unconditional generation, remain unclear.

Method: The authors define bias as a probabilistic difference, train unconditional image generative models, and use a common bias evaluation framework to study the shift in attributes between training and generated data distributions.

Result: Attribute shifts detected were generally small but sensitive to the choice of attribute classifier, especially when dealing with attributes on a spectrum rather than binary ones.

Conclusion: Improved representative labeling practices are needed along with a more thorough scrutiny of evaluation frameworks to address bias in generative AI models.

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [225] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Main category: cs.CV

TL;DR: This paper introduces LADH, the first long-term rPPG dataset with 240 synchronized RGB and IR facial videos from 21 participants in personal care scenarios. Combining RGB and IR video inputs improves non-contact physiological monitoring accuracy with a MAE of 4.99 BPM in heart rate estimation.


<details>
  <summary>Details</summary>
Motivation: rPPG is promising for daily health monitoring but faces challenges such as ambient lighting variations, occlusions, and dynamic facial postures in long-term personal care scenarios like mirror-facing routines in high-altitude environments.

Method: Created LADH dataset containing synchronized RGB and IR facial videos with ground-truth PPG, respiration, and blood oxygen signals. Conducted experiments combining RGB and IR video inputs using multi-task learning to enhance performance across multiple physiological indicators.

Result: Achieved a mean absolute error (MAE) of 4.99 BPM in heart rate estimation when combining RGB and IR video inputs. Multi-task learning improved performance across multiple physiological indicators simultaneously.

Conclusion: Combining RGB and IR video inputs along with multi-task learning significantly improves the accuracy and robustness of non-contact physiological monitoring in challenging long-term personal care scenarios.

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [226] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. This paper proposes a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from an interesting finding that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This indicates that current MLLMs may fail to effectively integrate visual information during reasoning.

Method: The method introduces three targeted perturbations into existing post-training pipelines: distractor concatenation, dominance-preserving mixup, and random rotation. These are designed to enhance perceptual robustness without needing algorithmic modifications or extra training data.

Result: Through extensive experiments across multiple datasets, consistent improvements in mathematical reasoning performance were demonstrated, with gains comparable to those achieved through algorithmic changes. Competitive performance was also achieved among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation.

Conclusion: The findings highlight the critical role of visual perturbation in multimodal mathematical reasoning, emphasizing that better reasoning begins with better seeing.

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [227] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: This paper presents PatchGuard, an adversarially robust Anomaly Detection (AD) and Anomaly Localization (AL) method that uses pseudo anomalies with localization masks within a Vision Transformer-based architecture. It significantly outperforms previous methods in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: Current AD and AL approaches are often susceptible to adversarial attacks due to limitations in training data which typically include only normal, unlabeled samples.

Method: PatchGuard incorporates pseudo anomalies with localization masks into a Vision Transformer (ViT)-based architecture. The approach leverages Foreground-Aware Pseudo-Anomalies and uses adversarial training guided by a novel loss function designed to improve model robustness.

Result: PatchGuard achieves performance gains of 53.2% in AD and 68.5% in AL in adversarial settings, while maintaining competitive accuracy in non-adversarial settings.

Conclusion: PatchGuard significantly outperforms previous methods in adversarial settings.

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [228] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: Diffusion models are great at image generation and encode text-image alignment info, but misalignment occurs in certain cases. This paper proposes ELBO-T2IAlign, a training-free method to calibrate pixel-text alignment.


<details>
  <summary>Details</summary>
Motivation: Current methods assume perfect text-image alignment in diffusion models, which is not always accurate, especially for small sized, occluded, or rare object classes.

Method: Propose using zero-shot referring image segmentation as a proxy task to evaluate alignment and introduce ELBO-T2IAlign, a training-free method based on the evidence lower bound (ELBO) of likelihood to calibrate pixel-text alignment.

Result: Extensive experiments verify the effectiveness of ELBO-T2IAlign across various diffusion model architectures.

Conclusion: ELBO-T2IAlign is a simple yet effective method to address pixel-text misalignment without additional training.

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [229] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: UFM is a new model that unifies flow and matching tasks, achieving better accuracy, less error, and faster performance than current state-of-the-art methods in both optical flow estimation and wide-baseline matching.


<details>
  <summary>Details</summary>
Motivation: Dense image correspondence is crucial for many applications, but it has traditionally been handled separately for wide-baseline scenarios and optical flow estimation. The authors aim to develop a unified approach that can handle both types of tasks effectively.

Method: The Unified Flow & Matching (UFM) model uses a transformer architecture to regress (u,v) flow directly. It is trained on unified data where pixels are co-visible in both source and target images, making it easier to train and more accurate for large flows compared to previous coarse-to-fine cost volume methods.

Result: UFM is 28% more accurate than the state-of-the-art flow method (Unimatch), has 62% less error, and is 6.7x faster than dense wide-baseline matchers (RoMa). It outperforms specialized approaches across both domains of optical flow and wide-baseline matching.

Conclusion: UFM demonstrates that unified training can surpass specialized methods in both optical flow estimation and wide-baseline matching. This advancement enables faster, general-purpose dense image correspondence and opens up possibilities for multi-modal, long-range, and real-time applications.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [230] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Main category: cs.CV

TL;DR: The paper proposes DarkerBB, a method to reconstruct facial images using only similarity scores from black-box recognition models via zero-order optimization in eigenface space. It achieves high verification accuracies and competitive query efficiency.


<details>
  <summary>Details</summary>
Motivation: Reconstructing facial images from black-box models highlights a privacy threat. The authors aim to address the challenge of model inversion using only similarity scores instead of embeddings.

Method: DarkerBB performs zero-order optimization within a PCA-derived eigenface space to reconstruct color faces.

Result: DarkerBB achieves state-of-the-art verification accuracies on LFW, AgeDB-30, and CFP-FP benchmarks with competitive query efficiency.

Conclusion: DarkerBB successfully reconstructs facial images using limited information (similarity scores), showing its potential as a significant privacy threat.

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [231] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: This paper presents a lightweight and energy-efficient object detection solution for aerial imagery captured during emergency response situations.


<details>
  <summary>Details</summary>
Motivation: The authors observed the lack of publicly available drone-view emergency imagery datasets, which motivated them to create their own dataset consisting of 10,820 annotated images covering critical emergency scenarios. They also wanted to provide an efficient object detection model suitable for real-time emergency detection on low-power edge devices.

Method: The authors deployed the YOLOv4-Tiny model, a compact convolutional neural network, optimized through post-training quantization to INT8 precision. The model was trained on their custom-curated aerial emergency dataset and evaluated against YOLOv5-small across multiple metrics, including mean Average Precision (mAP), F1 score, inference time, and model size.

Result: Experimental results show that the quantized YOLOv4-Tiny achieves comparable detection performance while reducing the model size from 22.5 MB to 6.4 MB and improving inference speed by 44%.

Conclusion: With a significant reduction in model size and improvement in inference speed, the quantized YOLOv4-Tiny model is highly suitable for real-time emergency detection on low-power edge devices.

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [232] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Main category: cs.CV

TL;DR: The paper introduces Q-SAM2, an accurate low-bit quantization method for efficient SAM2. It includes linear layer calibration and a Quantization-Aware Training (QAT) pipeline. Experiments show Q-SAM2 surpasses state-of-the-art quantization schemes.


<details>
  <summary>Details</summary>
Motivation: SAM2's high computational and memory consumption poses challenges in resource-constrained scenarios, motivating the need for an efficient quantization method.

Method: Proposes Q-SAM2 with two key contributions: 1) linear layer calibration for low-bit initialization to reposition weight distributions; 2) a QAT pipeline with clipping to suppress outliers and adapt to quantization thresholds during training.

Result: Q-SAM2 achieves highly accurate inference while improving efficiency, outperforming existing general quantization schemes particularly in ultra-low 2-bit quantization. The calibration technique also enhances post-training quantization accuracy by up to 66% mIoU.

Conclusion: Q-SAM2 offers an effective solution for efficient SAM2 deployment in resource-constrained environments through accurate low-bit quantization.

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [233] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat extends Gaussian Splatting to dynamic scenes via dynamic-static separation and hierarchical motion modeling, achieving superior accuracy and efficiency in dynamic scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Reconstructing complex, changing environments is a key challenge in computer vision that current methods struggle with due to real-world dynamics complexity.

Method: Classify scene elements as static or dynamic using deformation offset statistics and 2D motion flow consistency. Apply hierarchical motion modeling for both global and local movements. Integrate physically-based opacity estimation for coherent reconstructions.

Result: Extensive experiments show DynaSplat outperforms state-of-the-art methods in accuracy and realism while providing a more compact and efficient solution.

Conclusion: DynaSplat successfully reconstructs dynamic scenes with high accuracy and efficiency, offering an improved approach over existing techniques.

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [234] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Main category: cs.CV

TL;DR: This paper aims to develop generalist navigation agents with the ability to follow free-form instructions through OctoNav-Bench and OctoNav-R1, utilizing a Hybrid Training Paradigm inspired by thinking-before-action models.


<details>
  <summary>Details</summary>
Motivation: Previous navigation research is divided into different tasks/capabilities such as ObjNav, ImgNav, and VLN. The authors aim to move towards generalist navigation agents that can handle multi-modal and multi-capability instructions.

Method: The authors propose OctoNav-Bench, a large-scale benchmark featuring continuous environments and diverse free-form instructions. They also introduce the Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench. For the model, OctoNav-R1 is built upon MLLMs adapted to a VLA-type model producing low-level actions based on 2D visual observations. A Hybrid Training Paradigm (HTP) consisting of three stages (Action-/TBA-SFT, Nav-GPRO, and Online RL) is designed, inspired by OpenAI-o1 and DeepSeek-R1.

Result: OctoNav-R1 demonstrates superior performance compared to previous methods in embodied navigation.

Conclusion: The development of OctoNav-Bench and OctoNav-R1 represents a significant step towards creating generalist navigation agents with enhanced reasoning abilities.

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [235] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Main category: cs.CV

TL;DR: This paper proposes a new loss function for handwritten text recognition that uses Wasserstein distance to align character frequency distributions, improving model accuracy and robustness. It also shows that this alignment can be used during inference without retraining, enhancing existing models.


<details>
  <summary>Details</summary>
Motivation: Handwritten text recognition is challenging due to evolving and context-dependent nature of handwriting, leading to underperformance of models trained on diverse corpora when applied to specific subsets.

Method: A novel loss function incorporating Wasserstein distance between the character frequency distribution of predicted text and target distribution derived from training data is proposed. Character distribution alignment is also integrated as a scoring function in a guided decoding scheme for inference time improvements.

Result: Experimental results across multiple datasets and architectures confirm the effectiveness of the method in boosting generalization and performance.

Conclusion: The proposed approach enhances both accuracy and robustness of handwritten text recognition models under temporal and contextual intra-dataset shifts. Code is open sourced.

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [236] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Main category: cs.CV

TL;DR: This paper introduces Geometric Distillation, a framework enhancing pretrained Vision-Language Models (VLMs) with geometric cues for better 3D spatial understanding without added annotations or architectural changes. It outperforms prior methods in 3D vision-language reasoning and perception benchmarks with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models show excellent performance on visual and linguistic tasks but lack deep understanding of 3D spatial structures. The authors aim to enhance these models' geometric awareness.

Method: The proposed method, Geometric Distillation, uses a fine-tuning framework that incorporates human-inspired geometric cues into pretrained VLMs by distilling sparse correspondences, relative depth relations, and dense cost volumes from existing 3D foundation models without modifying the original model architecture.

Result: Geometric Distillation consistently outperforms previous approaches on 3D vision-language reasoning and perception benchmarks, offering improved 3D spatial reasoning at a significantly lower computational cost.

Conclusion: The work presents an efficient way to bridge 2D-trained VLMs with 3D understanding, making multimodal tasks involving spatial grounding more accessible.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [237] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: Diffusion models are great for image generation but require a lot of resources. Post-Training Quantization (PTQ) can help by reducing bitwidth, but it has limitations. This paper introduces HadaNorm, which improves PTQ by normalizing activations before applying transformations, allowing for better quantization and reduced error.


<details>
  <summary>Details</summary>
Motivation: To address the resource constraints faced when deploying diffusion models on devices with limited capabilities, this work seeks to enhance Post-Training Quantization methods to allow for more efficient model deployment without significant performance loss.

Method: The method involves proposing a new linear transformation called HadaNorm. This technique normalizes activation feature channels prior to applying Hadamard transformations, thereby mitigating outliers and enabling more aggressive activation quantization.

Result: HadaNorm consistently decreases quantization error across different components of transformer blocks and provides better efficiency-performance trade-offs compared to current state-of-the-art techniques.

Conclusion: HadaNorm represents an advancement in the field of model quantization for diffusion models, offering a way to significantly reduce memory and computational requirements while maintaining or improving performance.

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [238] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Main category: cs.CV

TL;DR: An abstract about a new benchmark dataset called CausalVQA for video question answering that focuses on causality in the physical world.


<details>
  <summary>Details</summary>
Motivation: The existing VQA benchmarks either focus on surface perceptual understanding or narrow physical reasoning questions. There is a need for a benchmark that challenges models' understanding of causality in real-world scenarios.

Method: Introduced CausalVQA, a benchmark dataset composed of question-answer pairs probing models' understanding of causality through five question types: counterfactual, hypothetical, anticipation, planning and descriptive. Also designed quality control mechanisms to prevent models from exploiting trivial shortcuts.

Result: Current frontier multimodal models perform substantially below human performance on the benchmark, particularly on anticipation and hypothetical questions.

Conclusion: CausalVQA presents challenging questions grounded in real-world scenarios, focusing on models' ability to predict likely outcomes of different actions and events, thus highlighting the challenge for current systems in spatial-temporal reasoning, understanding physical principles, and comprehending possible alternatives.

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [239] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper introduces UniPre3D, a unified pre-training method for point cloud data of any scale and 3D models of any architecture. It predicts Gaussian primitives, uses differentiable Gaussian splatting for rendering images, and integrates 2D features from pre-trained image models.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of unified 3D models and pre-training methods that are equally effective for both object- and scene-level point clouds.

Method: The method involves predicting Gaussian primitives as the pre-training task and using differentiable Gaussian splatting to render images. This enables precise pixel-level supervision and end-to-end optimization. Additionally, 2D features from pre-trained image models are integrated to regulate the complexity of the pre-training task and direct the model's focus toward geometric structures.

Result: The universal effectiveness of the proposed method is validated through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones.

Conclusion: UniPre3D is the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture.

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [240] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: The paper introduces a new dataset for visually grounded dialogues in videos that require external knowledge, along with baselines and future challenges.


<details>
  <summary>Details</summary>
Motivation: To advance the task of visually grounded dialogues based on videos by incorporating external knowledge not visually present.

Method: Created a dataset with 2,017 videos and 5,986 human-annotated dialogues requiring models to identify relevant video parts and leverage external knowledge for conversation.

Result: Provided several baselines evaluated on the new dataset and highlighted future challenges for this task.

Conclusion: The dataset offers a new direction for research in visually grounded dialogues with external knowledge, and the associated challenges are presented.

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [241] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper provides a comprehensive overview of vision generalist models, discussing their characteristics, capabilities, design frameworks, enhancing techniques, related domains, real-world applications, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the increasing interest in applying generalist models, originally successful in natural language processing, to computer vision tasks despite the diversity and difficulty in unifying vision task representations.

Method: The paper reviews the background (datasets, tasks, benchmarks), explores the design of existing frameworks and performance-enhancing techniques, examines related domains, and discusses real-world applications and challenges.

Result: Offers a detailed understanding of vision generalist models, their interconnections with other domains, and insights into practical applications and research challenges.

Conclusion: The conclusion highlights real-world application scenarios, persistent challenges in the field, and suggests possible directions for future research on vision generalist models.

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [242] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Main category: cs.CV

TL;DR: The paper proposes a new paradigm called 'drawing to reason in space' for enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs) through elementary drawing operations, which significantly improves spatial reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing multimodal reasoning methods that primarily focus on text-centric approaches and lack precise geometric understanding and continuous spatial tracking.

Method: Proposes equipping LVLMs with basic drawing operations such as annotating bounding boxes and drawing auxiliary lines, enabling them to express and analyze spatial relationships through direct visual manipulation. Introduces a three-stage training framework including cold-start training with synthetic data, reflective rejection sampling, and reinforcement learning.

Result: Extensive experiments show that the proposed model VILASR outperforms existing methods across various spatial reasoning benchmarks with an average improvement of 18.4%.

Conclusion: The novel paradigm of 'drawing to reason in space' effectively enhances the spatial reasoning capabilities of LVLMs, avoiding the performance ceiling imposed by specialized perception tools.

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [243] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Main category: cs.CV

TL;DR: A novel framework for end-to-end human animation with multi-modal conditions is proposed, enabling precise control of multiple concepts including humans and objects.


<details>
  <summary>Details</summary>
Motivation: Existing methods for end-to-end human animation can only animate a single subject and inject conditions in a global manner, ignoring rich interactions and hindering applications.

Method: The framework enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. It uses a mask predictor to infer layout information and injects local audio condition into its corresponding region iteratively.

Result: Empirical results and ablation studies validate the effectiveness of explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.

Conclusion: This design enables the high-quality generation of controllable multi-concept human-centric videos.

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [244] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: This paper pioneers textual reference-guided human action segmentation in multi-person settings, introduces RHAS133 dataset and proposes HopaDIFF framework achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for action segmentation mainly focus on single-person activities with fixed action sequences, neglecting multi-person scenarios. The authors aim to address this gap by introducing a new task of textual reference-guided human action segmentation in multi-person settings.

Method: The authors introduce RHAS133, the first dataset for Referring Human Action Segmentation, built from 133 movies with 33 hours of video data and annotated with fine-grained actions and textual descriptions. They benchmark existing action recognition methods on RHAS133 using VLM-based feature extractors and find limited performance. To improve this, they propose HopaDIFF, a holistic-partial aware Fourier-conditioned diffusion framework that uses a novel cross-input gate attentional xLSTM and a novel Fourier condition to enhance reasoning and control.

Result: HopaDIFF achieves state-of-the-art results on RHAS133 in diverse evaluation settings.

Conclusion: The authors have introduced a new task and dataset for textual reference-guided human action segmentation in multi-person settings and proposed a novel framework, HopaDIFF, which outperforms existing methods.

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [245] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: The paper presents EditInspector, a benchmark for evaluating text-guided image edits, revealing current model limitations and proposing two new methods to improve artifact detection and difference caption generation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of a comprehensive framework to verify and assess the quality of text-guided image edits as this technology becomes more widespread.

Method: EditInspector benchmark uses human annotations for edit verification across multiple dimensions (accuracy, artifact detection, visual quality, etc.). Two novel methods are proposed to enhance artifact detection and difference caption generation beyond current SoTA models.

Result: Current SoTA models struggle with comprehensive edit evaluation and often hallucinate when describing changes. The proposed methods show improvement in artifact detection and difference caption generation.

Conclusion: A benchmark called EditInspector has been introduced to evaluate text-guided image edits, exposing the limitations of current models and demonstrating the effectiveness of two newly proposed methods.

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [246] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: The paper presents Text-Aware Image Restoration (TAIR) to address text-image hallucination in image restoration, introduces SA-Text benchmark and TeReDiff framework, achieving superior performance in restoring both visual content and textual fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based image restoration methods struggle with faithfully reconstructing textual regions in degraded images, often generating incorrect text-like patterns (text-image hallucination).

Method: Introduced TAIR task requiring simultaneous recovery of visual contents and textual fidelity. Created SA-Text, a large-scale benchmark for scene images annotated with diverse texts. Proposed TeReDiff, a multi-task diffusion framework integrating internal features from diffusion models into a text-spotting module for joint training.

Result: Extensive experiments show that the approach outperforms state-of-the-art restoration methods, providing significant improvements in text recognition accuracy.

Conclusion: The novel TAIR task, SA-Text benchmark, and TeReDiff framework effectively enhance image restoration by preserving textual fidelity alongside visual content.

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [247] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: CINeMA is a novel framework for creating high-resolution, spatio-temporal, multimodal brain atlases that operates in latent space and enables flexible conditioning on anatomical features.


<details>
  <summary>Details</summary>
Motivation: Studying the critical stage of the developing human brain requires accurate brain models with high spatial and temporal resolution. Traditional methods and deep learning-based approaches rely on large datasets which pose challenges for studying brains with pathologies where data is scarce.

Method: CINeMA operates in latent space to avoid compute-intensive image registration and reduce atlas construction times from days to minutes. It supports flexible conditioning on anatomical features like GA, birth age, and pathologies such as ventriculomegaly and agenesis of the corpus callosum.

Result: CINeMA surpasses state-of-the-art methods in accuracy, efficiency, and versatility. It supports downstream tasks like tissue segmentation and age prediction and enables synthetic data creation and anatomically informed data augmentation.

Conclusion: CINeMA represents a powerful tool for advancing brain research and its code and atlases are released at https://github.com/m-dannecker/CINeMA.

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [248] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: 通过在推理阶段增加简单结构的方法，可以有效提升双编码器视觉-语言模型的组合能力，特别是在属性-对象绑定方面。该方法将图像分为更小的部分，提取文本片段，并使用VLM找到与文本片段更好的对齐的图像裁剪部分以获得匹配，最后聚合这些匹配的相似性来计算最终的图像-文本相似度。此方法无需任何训练即可持续提高VLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的双编码器视觉-语言模型（如CLIP）在图像-文本检索任务中表现良好，但在组合性方面存在困难，表现出类似词袋的行为，这限制了它们的检索性能。尽管已提出多种训练方法来改善这种组合能力，但推理时技术却较少受到关注。

Method: 提出了一种在推理时添加简单结构的方法：1) 将图像分成不同的较小裁剪部分；2) 提取捕获对象、属性和关系的文本片段；3) 使用VLM找到与文本片段更好地对齐的图像裁剪部分以获得匹配；4) 聚合这些匹配的个体相似性来计算最终的图像-文本相似度。

Result: 该方法在受控和自然数据集上评估了多种流行的双编码器VLMs，结果表明其能持续提高VLMs的性能而无需任何训练。特别地，在受控数据集中，该方法在属性-对象绑定方面的表现尤为出色。进一步分析表明，处理图像裁剪对于观察到的性能提升至关重要，并识别出了一些可进一步改进推理时方法的具体领域。

Conclusion: 本文提出的推理时方法能够显著提升双编码器VLMs的视觉-语言组合能力，尤其是在属性-对象绑定方面。同时，研究强调了推理时技术的潜力，并指出了未来改进的方向。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [249] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: The paper introduces Kvasir-VQA-x1, an expanded large-scale dataset for gastrointestinal endoscopy visual question answering. It incorporates 159,549 new question-answer pairs designed to test deeper clinical reasoning and features visual augmentations that mimic common imaging artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for medical visual question answering lack clinical complexity and visual diversity, limiting the progress in developing clinical decision support systems.

Method: The authors developed a systematic method using large language models to generate 159,549 new question-answer pairs stratified by complexity. They also introduced visual augmentations that mimic common imaging artifacts to better assess model robustness.

Result: Kvasir-VQA-x1 is structured to support two main evaluation tracks: standard VQA performance and model robustness against visual perturbations, providing a more challenging and clinically relevant benchmark.

Conclusion: Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for clinical use. The dataset is fully accessible and adheres to FAIR data principles.

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [250] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: 为了更准确地评估视频语言模型的空间时间理解和推理能力，本文提出了Minimal Video Pairs (MVP)基准。该基准包含55K高质量的多项选择视频问答示例，并通过最小变化对来减少浅层视觉或文本线索的影响。人类在MVP上的表现为92.9%，而最好的开源视频语言模型仅达到40.2%。


<details>
  <summary>Details</summary>
Motivation: 现有的用于评估视频语言模型空间时间理解和推理能力的基准容易受到基于浅层视觉或文本线索的捷径解决方案的影响，导致评分膨胀。

Method: 引入了Minimal Video Pairs (MVP)基准，一个简单的、注重捷径意识的视频问答基准，以评估视频语言模型的物理理解能力。MVP由55K高质量的多选视频问答实例组成，这些实例来自九个视频数据源。每个样本都有一个最小变化对，即视觉上相似的视频，伴有相同的问题但答案相反。

Result: 人类在MVP上的正确率为92.9%，而最先进的开源视频语言模型的正确率仅为40.2%，相比之下随机猜测的正确率为25%。

Conclusion: MVP基准能有效减少基于浅层线索的捷径解决方案的影响，提供了一种更准确评估视频语言模型性能的方法。

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [251] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: The paper introduces an AI-powered assistive technology for diagnosing and managing ASD, utilizing transfer learning and image transforms from eye gaze variables. It aims to provide timely, accessible diagnosis while protecting privacy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the rapid increase in ASD prevalence and the time-intensive nature of current diagnostic techniques, leading to high social and economic costs.

Method: Integration of transfer learning with image transforms derived from eye gaze variables for ASD diagnosis.

Result: Facilitates in-home periodical diagnosis, reduces stress for individuals and caregivers, preserves user privacy, and improves communication between guardians and therapists.

Conclusion: The proposed approach ensures timely, accessible diagnosis while protecting privacy, ultimately improving outcomes for individuals with ASD.

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [252] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Main category: eess.IV

TL;DR: Foundation models (FMs) in medical image analysis learn from large unlabeled datasets, providing adaptable visual features for clinical tasks. This review explores FM development and application in pathology, radiology, ophthalmology, their core components, usage strategies, and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of foundation models (FMs) in transforming medical image analysis by leveraging large unlabeled datasets to learn general-purpose visual features that can be adapted to specific clinical tasks with minimal supervision.

Method: Reviewing over 150 studies on FMs in pathology, radiology, and ophthalmology. Examining core components such as model architectures, self-supervised learning methods, and downstream adaptation strategies.

Result: FMs show promise in various medical imaging domains, offering adaptable visual features learned from large unlabeled datasets.

Conclusion: FMs are reshaping medical image analysis by reducing reliance on manually annotated data and enabling efficient adaptation to clinical tasks. However, key challenges remain and warrant further research.

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [253] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: LoREIN is a new unsupervised framework for 3D MP-qMRI reconstruction that integrates dual priors, improving imaging efficiency and clinical diagnosis accuracy.


<details>
  <summary>Details</summary>
Motivation: Current qMRI reconstruction methods struggle with highly undersampled, high-dimensional data due to reliance on single prior or physics-informed models, leading to suboptimal results.

Method: LoREIN incorporates low-rank and continuity priors through low-rank representation and implicit neural representation to enhance reconstruction fidelity. It uses INR for optimal spatial bases estimation within the low-rank subspace.

Result: High-fidelity reconstruction of weighted images and enhanced accuracy in quantitative parameter maps are achieved. The method also introduces a zero-shot learning paradigm.

Conclusion: LoREIN offers an advanced approach for accelerated 3D MP-qMRI reconstruction, contributing significantly to medical imaging field.

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [254] [Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization](https://arxiv.org/abs/2506.09730)
*Pierre Vernimmen,François Glineur*

Main category: math.OC

TL;DR: This paper evaluates the robustness of different first-order optimization methods under relative inexactness in gradient computations, both empirically and theoretically. It analyzes three families of methods and introduces a shortening factor to improve theoretical guarantees. Testing shows that accelerated methods are more robust than expected and the shortening factor significantly aids long-step methods.


<details>
  <summary>Details</summary>
Motivation: To understand how different first-order optimization methods perform when there is relative inexactness in their gradient computations, which can occur due to compression techniques used in large-scale problems on GPUs.

Method: Theoretical analysis using the performance estimation methodology on three major families of optimization methods: constant step gradient descent, long-step methods, and accelerated methods. Introduction of a semi-heuristic shortening factor for long-step and accelerated methods. Empirical testing on a concrete inexact problem with two types of relative inexactness.

Result: Accelerated methods show more robustness than expected under inexactness. The shortening factor significantly improves the performance of long-step methods. All shortened methods appear promising even in an inexact setting.

Conclusion: Different first-order optimization methods have varying levels of robustness to relative inexactness in gradient computations. Introducing a shortening factor can enhance the theoretical guarantees and practical performance of some methods.

Abstract: This work assesses both empirically and theoretically, using the performance
estimation methodology, how robust different first-order optimization methods
are when subject to relative inexactness in their gradient computations.
Relative inexactness occurs, for example, when compressing the gradient using
fewer bits of information, which happens when dealing with large-scale problems
on GPUs. Three major families of methods are analyzed: constant step gradient
descent, long-step methods, and accelerated methods. The latter two are first
shown to be theoretically not robust to inexactness. Then, a semi-heuristic
shortening factor is introduced to improve their theoretical guarantees. All
methods are subsequently tested on a concrete inexact problem, with two
different types of relative inexactness, and it is observed that both
accelerated methods are much more robust than expected, and that the shortening
factor significantly helps the long-step methods. In the end, all shortened
methods appear to be promising, even in this inexact setting.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [255] [How attention simplifies mental representations for planning](https://arxiv.org/abs/2506.09520)
*Jason da Silva Castanheira,Nicholas Shea,Stephen M. Fleming*

Main category: q-bio.NC

TL;DR: 人类规划高效且灵活，通过构建环境的简化心理表征来平衡任务复杂性和实用性。本文研究了空间注意力如何控制任务表征进入主观意识并用于规划，发现空间接近性及任务相关的信息遵循自然注意力轮廓时，人们能更易构建简化和有用的迷宫表征。个体间注意力影响差异显著，解释了人们任务表征和行为的不同。本文将视空间注意力效果融入现有的价值导向构想计算模型中，连接感知与决策制定的计算视角，以理解个体如何为规划表示环境。


<details>
  <summary>Details</summary>
Motivation: 探讨人类规划过程中感知与规划之间的相互作用机制，特别是揭示空间注意力对任务表征的影响以及其如何塑造规划过程。

Method: 利用虚拟迷宫导航实验，分析空间注意力如何控制任务表征进入主观意识并可用于规划。研究注意力对构建简化而有用的迷宫表征的影响，并考察个体间的注意力差异。

Result: 空间接近性决定了哪些迷宫方面可用于规划；当任务相关信息遵循自然注意力轮廓时，人们更容易构建简化和有用的迷宫表征。个体间注意力影响差异显著，解释了不同个体的任务表征和行为差异。

Conclusion: 本文通过将视空间注意力的效果融入现有计算模型，连接了感知与决策制定的计算视角，从而更好地理解个体如何为规划表示环境。

Abstract: Human planning is efficient -- it frugally deploys limited cognitive
resources to accomplish difficult tasks -- and flexible -- adapting to novel
problems and environments. Computational approaches suggest that people
construct simplified mental representations of their environment, balancing the
complexity of a task representation with its utility. These models imply a
nested optimisation in which planning shapes perception, and perception shapes
planning -- but the perceptual and attentional mechanisms governing how this
interaction unfolds remain unknown. Here, we harness virtual maze navigation to
characterise how spatial attention controls which aspects of a task
representation enter subjective awareness and are available for planning. We
find that spatial proximity governs which aspects of a maze are available for
planning, and that when task-relevant information follows natural (lateralised)
contours of attention, people can more easily construct simplified and useful
maze representations. This influence of attention varies considerably across
individuals, explaining differences in people's task representations and
behaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the
effects of visuospatial attention into existing computational accounts of
value-guided construal. Together, our work bridges computational perspectives
on perception and decision-making to better understand how individuals
represent their environments in aid of planning.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [256] [A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project](https://arxiv.org/abs/2506.09204)
*Xiaotian Chen,Hongyun Liu,Seyed Sahand Mohammadi Ziabari*

Main category: cs.NE

TL;DR: This paper explores the potential of motif-based optimization to further improve the SET algorithm applied on sparse MLPs, aiming for more than 40% efficiency gains with less than 4% performance decline.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the increasing demand for reducing computational costs and memory overheads in complex DNN models while maintaining accuracy.

Method: Investigate the application of motif-based optimization as a structural optimization method on SET-MLP.

Result: The research finds that structural optimization of SET-MLP can indeed enhance performance with significant efficiency gains.

Conclusion: Motif-based optimization shows promise in improving SET-MLP, achieving over 40% efficiency gains with minimal performance loss.

Abstract: Deep Neural Networks (DNNs) have been proven to be exceptionally effective
and have been applied across diverse domains within deep learning. However, as
DNN models increase in complexity, the demand for reduced computational costs
and memory overheads has become increasingly urgent. Sparsity has emerged as a
leading approach in this area. The robustness of sparse Multi-layer Perceptrons
(MLPs) for supervised feature selection, along with the application of Sparse
Evolutionary Training (SET), illustrates the feasibility of reducing
computational costs without compromising accuracy. Moreover, it is believed
that the SET algorithm can still be improved through a structural optimization
method called motif-based optimization, with potential efficiency gains
exceeding 40% and a performance decline of under 4%. This research investigates
whether the structural optimization of Sparse Evolutionary Training applied to
Multi-layer Perceptrons (SET-MLP) can enhance performance and to what extent
this improvement can be achieved.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [257] [A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models](https://arxiv.org/abs/2506.09076)
*Haley Stone,Jing Du,Hao Xue,Matthew Scotch,David Heslop,Andreas Züfle,Chandini Raina MacIntyre,Flora Salim*

Main category: q-bio.GN

TL;DR: 该研究提出了一种概率框架，用于推断未测序病例与已知序列之间的遗传距离，通过时间感知的进化距离建模，估计成对分歧，支持基因组数据集的可扩展、不确定性感知增强，并改进时空建模工作流。


<details>
  <summary>Details</summary>
Motivation: 病原体基因组数据为时空模型提供了有价值的结构，但其效用受到不完整测序覆盖率的限制。

Method: 提出一种概率框架，利用时间感知的进化距离建模，估计采集日期和观测遗传距离之间的成对分歧，实现基于观测分歧模式的生物合理性插补，无需序列比对或已知传播链。

Result: 应用于美国野生鸟类中高致病性禽流感A/H5病例，该方法支持基因组数据集的可扩展、不确定性感知增强，并改进了进化信息在时空建模工作流中的整合。

Conclusion: 此概率框架能够有效增强基因组数据集并改进时空建模，即使在测序覆盖率不完整的情况下也能提供有价值的信息。

Abstract: Pathogen genome data offers valuable structure for spatial models, but its
utility is limited by incomplete sequencing coverage. We propose a
probabilistic framework for inferring genetic distances between unsequenced
cases and known sequences within defined transmission chains, using time-aware
evolutionary distance modeling. The method estimates pairwise divergence from
collection dates and observed genetic distances, enabling biologically
plausible imputation grounded in observed divergence patterns, without
requiring sequence alignment or known transmission chains. Applied to highly
pathogenic avian influenza A/H5 cases in wild birds in the United States, this
approach supports scalable, uncertainty-aware augmentation of genomic datasets
and enhances the integration of evolutionary information into spatiotemporal
modeling workflows.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [258] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: This paper explores regularization methods for training ASR models with learnable feature extraction front-ends, proposing masking in the STFT-domain and audio perturbation methods to close the performance gap between traditional and learnable features.


<details>
  <summary>Details</summary>
Motivation: Neural front-ends for ASR systems can be trained to fit the acoustic model but often underperform compared to classical methods due to overfitting issues.

Method: The authors examine audio perturbation methods and propose masking in the STFT-domain as modifications to SpecAugment for regularizing learnable feature extraction front-ends in ASR models.

Result: Both proposed regularization approaches effectively close the performance gap between traditional and learnable features in ASR models.

Conclusion: Regularization techniques such as audio perturbation and STFT-domain masking are crucial for improving the performance of learnable feature extraction front-ends in ASR models.

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [259] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: An unsupervised method,UAD,is introduced to distill affordance knowledge from foundation models into a task-conditioned affordance model without manual annotations.It shows promising generalization in robotic scenes and human activities.An imitation learning policy using UAD's affordance also demonstrates good generalization with few demonstrations.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing visual affordance prediction methods that rely on manually annotated data or predefined tasks,thus introducing a new method that doesn't require manual annotations.

Method: UAD leverages large vision models and vision-language models to automatically annotate a dataset with <instruction,visual affordance> pairs.Trains a lightweight task-conditioned decoder on rendered objects in simulation.

Result: UAD exhibits notable generalization to real-world robotic scenes and human activities.The imitation learning policy using UAD's affordance generalizes well to unseen object instances,object categories,and variations in task instructions after training on as few as 10 demonstrations.

Conclusion: UAD successfully distills affordance knowledge without manual annotations and shows strong generalization capabilities.The approach may enable robots to better manipulate objects in unstructured environments.

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [260] [Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations](https://arxiv.org/abs/2506.09383)
*Chengtian Ma,Yunyue Wei,Chenhui Zuo,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: This paper explores a hierarchical control pipeline for simulating human balance using a whole-body musculoskeletal system, providing muscle-level insights into balance dynamics and demonstrating improvements in balance maintenance with simulated hip exoskeleton assistance.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding of static balance and falling mechanisms, which have been less studied compared to dynamic balance during locomotion.

Method: Development of a hierarchical control pipeline that simulates human balance through a comprehensive whole-body musculoskeletal system. This includes identifying spatiotemporal dynamics of balancing, analyzing the impact of muscle injury on balance behavior, generating fall contact patterns, and simulating hip exoskeleton assistance.

Result: The study successfully identified balancing dynamics during stable standing, showed how muscle injury affects balance, generated fall patterns matching clinical data, and demonstrated that hip exoskeleton assistance improves balance maintenance and reduces muscle effort under perturbation.

Conclusion: This work provides deep muscle-level insights into human balance dynamics, which are difficult to obtain experimentally. It lays the groundwork for creating targeted interventions for people with balance issues and supports advancements in humanoid robotic systems.

Abstract: Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.

</details>


### [261] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: 提出Adv-BMT框架，通过逆向交通运动预测生成多样且真实的对抗性交互场景，减少碰撞率20%。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中长尾、安全关键场景稀缺，限制了自动驾驶系统的情景测试效果。

Method: Adv-BMT框架包含双向运动变换模型（BMT），进行逆向交通运动预测，输入情景最后一时间步的代理信息，反向重建交通直至初始时间步；采用两阶段管道：先对抗初始化再逆向运动预测，无需碰撞数据预训练即可生成真实多样的碰撞交互。

Result: 实验结果表明，使用Adv-BMT生成的增强数据集训练可使碰撞率降低20%。

Conclusion: Adv-BMT框架能够有效生成真实且多样的碰撞场景，提升自动驾驶系统的测试和性能。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [262] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: This paper presents a tightly coupled LiDAR-IMU-leg odometry with an online learning-based leg kinematics model for robust performance in challenging environments.


<details>
  <summary>Details</summary>
Motivation: To develop a method that is robust to featureless environments and deformable terrains using sensor fusion of LiDAR, IMU, and leg kinematics.

Method: An online learning-based leg kinematics model named the neural leg kinematics model was developed. It incorporates tactile information (foot reaction force) to express nonlinear dynamics between robot feet and the ground. Online training enhances adaptability to weight load changes and terrain conditions.

Result: The proposed method was verified through real experiments using a quadruped robot in challenging situations such as sandy beach and campus environment. Results showed that the odometry estimation incorporating the neural leg kinematics model outperforms state-of-the-art works.

Conclusion: The tightly coupled LiDAR-IMU-leg odometry with the neural leg kinematics model provides robust performance in challenging environments.

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [263] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Main category: cs.RO

TL;DR: Humanoid robots have great potential in daily tasks. Recent methods, though effective, lack versatility and scalability due to task-specific tuning. This paper introduces SkillBlender, a hierarchical reinforcement learning framework for versatile humanoid loco-manipulation, which pretrains primitive skills and dynamically blends them for complex tasks with minimal reward engineering. The paper also introduces SkillBench, a simulated benchmark for evaluation. Experiments show superior performance over baselines with more accurate and feasible movements.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current methods requiring tedious task-specific tuning for satisfactory behaviors in diverse daily scenarios, thus limiting versatility and scalability.

Method: SkillBlender is a hierarchical reinforcement learning framework that first pretrains goal-conditioned task-agnostic primitive skills, then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. SkillBench is a parallel, cross-embodiment, and diverse simulated benchmark introduced for evaluation.

Result: Extensive simulated experiments demonstrate that SkillBlender significantly outperforms all baselines while naturally regularizing behaviors to avoid reward hacking, leading to more accurate and feasible movements for various loco-manipulation tasks.

Conclusion: SkillBlender offers a promising solution for versatile humanoid loco-manipulation by reducing the need for task-specific tuning. The introduction of SkillBench provides a valuable tool for evaluating such systems. The code and benchmark will be open-sourced to facilitate future research.

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [264] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Main category: cs.RO

TL;DR: The paper proposes a framework for quadruped robots to collect objects using leg-based manipulation without additional actuators.


<details>
  <summary>Details</summary>
Motivation: To expand the capabilities of quadruped robots beyond locomotion and relatively static tasks by enabling them to perform dynamic object manipulation using their legs.

Method: Attach a scoop-like add-on to one leg of the robot, use a hierarchical policy structure with two expert policies (for scooping/tossing and approaching object positions) and a meta-policy for switching between them. The expert policies are trained separately followed by meta-policy training for coordinated multi-object collection.

Result: Demonstrates the effective utilization of quadruped legs for dynamic object manipulation, allowing the robot to collect objects without additional actuators.

Conclusion: Quadruped robots can leverage their legs for dynamic object manipulation tasks, expanding their role beyond just locomotion.

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [265] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Main category: cs.RO

TL;DR: To address the limitations of existing diffusion-based policy methods in robotic manipulation, this paper introduces Time-Unified Diffusion Policy (TUDP). It builds a time-unified velocity field and proposes an action-wise training method to improve action generation efficiency and accuracy. TUDP achieves state-of-the-art performance on RLBench with success rates of 82.6% and 83.8% in multi-view and single-view setups respectively.


<details>
  <summary>Details</summary>
Motivation: Diffusion models perform well in imitation learning for robotic manipulation but suffer from high time requirements for iterative denoising and increased temporal complexity which affects model training and action accuracy.

Method: The TUDP method constructs a time-unified velocity field in action space incorporating additional action discrimination information, unifying all timesteps of action denoising. Additionally, an action-wise training method is proposed, introducing an action discrimination branch to provide extra action discrimination information.

Result: TUDP achieves state-of-the-art performance on RLBench with success rates of 82.6% in a multi-view setup and 83.8% in a single-view setup. Notably, when using fewer denoising iterations, TUDP shows a more significant improvement in success rate and can accurately produce actions for various real-world tasks.

Conclusion: TUDP significantly enhances the efficiency and accuracy of robot action generation through its time-unified velocity field and action-wise training approach, outperforming existing methods.

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [266] [SAFE: Multitask Failure Detection for Vision-Language-Action Models](https://arxiv.org/abs/2506.09937)
*Qiao Gu,Yuanliang Ju,Shengxiang Sun,Igor Gilitschenski,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: SAFE is a failure detector for vision-language-action (VLA) models that predicts task failure likelihood, generalizing well to unseen tasks and environments.


<details>
  <summary>Details</summary>
Motivation: Vision-language-action (VLA) models show promise in robotic behaviors but have limited success rates on novel tasks. A failure detector is needed to allow safe interaction with environments, especially one that can generalize across different tasks and environments.

Method: The authors analyze the VLA feature space and find sufficient high-level knowledge about task success/failure that's generic across tasks. They design SAFE to learn from VLA internal features and predict a scalar indicating task failure likelihood. SAFE is trained on both successful and failed rollouts, compatible with different policy architectures.

Result: SAFE achieves state-of-the-art performance in failure detection and offers the best trade-off between accuracy and detection time using conformal prediction. It outperforms diverse baselines and is tested extensively in both simulated and real-world environments.

Conclusion: SAFE addresses the multitask failure detection problem for generalist robot policies like VLAs, providing timely alerts for task failures and enabling safer interactions in unseen tasks and environments.

Abstract: While vision-language-action models (VLAs) have shown promising robotic
behaviors across a diverse set of manipulation tasks, they achieve limited
success rates when deployed on novel tasks out-of-the-box. To allow these
policies to safely interact with their environments, we need a failure detector
that gives a timely alert such that the robot can stop, backtrack, or ask for
help. However, existing failure detectors are trained and tested only on one or
a few specific tasks, while VLAs require the detector to generalize and detect
failures also in unseen tasks and novel environments. In this paper, we
introduce the multitask failure detection problem and propose SAFE, a failure
detector for generalist robot policies such as VLAs. We analyze the VLA feature
space and find that VLAs have sufficient high-level knowledge about task
success and failure, which is generic across different tasks. Based on this
insight, we design SAFE to learn from VLA internal features and predict a
single scalar indicating the likelihood of task failure. SAFE is trained on
both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is
compatible with different policy architectures. We test it on OpenVLA, $\pi_0$,
and $\pi_0$-FAST in both simulated and real-world environments extensively. We
compare SAFE with diverse baselines and show that SAFE achieves
state-of-the-art failure detection performance and the best trade-off between
accuracy and detection time using conformal prediction. More qualitative
results can be found at https://vla-safe.github.io/.

</details>


### [267] [eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures](https://arxiv.org/abs/2506.09994)
*Venkatesh Pattabiraman,Zizhou Huang,Daniele Panozzo,Denis Zorin,Lerrel Pinto,Raunaq Bhirangi*

Main category: cs.RO

TL;DR: eFlesh是一种低成本、易制造且高度可定制的磁性触觉传感器，能够显著提高机器人在非结构化环境中的操作能力。


<details>
  <summary>Details</summary>
Motivation: 当前机器人在非结构化环境中操作时，缺乏一种通用、易获取且可高度定制的触觉传感器，导致解决方案碎片化或无传感器的力感知方法。

Method: 通过使用四组件（3D打印机、现成磁铁、CAD模型和磁强计电路板）构建基于磁性的触觉传感器eFlesh，并利用参数化微结构调整传感器几何形状和机械响应，同时提供开源设计工具以实现定制化。

Result: eFlesh实现了接触定位均方根误差0.5毫米，法向力预测均方根误差0.27牛顿，切向力预测均方根误差0.12牛顿；学习到的滑动检测模型对未见物体的准确率达到95%，并在四个需要亚毫米精度的任务中将操作成功率提高40%，达到91%的成功率。

Conclusion: eFlesh为机器人提供了有效的触觉感知能力，降低了成本并提高了定制化程度，其性能在多种任务中得到了验证。所有设计文件、代码和转换工具均已开源。

Abstract: If human experience is any guide, operating effectively in unstructured
environments -- like homes and offices -- requires robots to sense the forces
during physical interaction. Yet, the lack of a versatile, accessible, and
easily customizable tactile sensor has led to fragmented, sensor-specific
solutions in robotic manipulation -- and in many cases, to force-unaware,
sensorless approaches. With eFlesh, we bridge this gap by introducing a
magnetic tactile sensor that is low-cost, easy to fabricate, and highly
customizable. Building an eFlesh sensor requires only four components: a
hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired
shape, and a magnetometer circuit board. The sensor is constructed from tiled,
parameterized microstructures, which allow for tuning the sensor's geometry and
its mechanical response. We provide an open-source design tool that converts
convex OBJ/STL files into 3D-printable STLs for fabrication. This modular
design framework enables users to create application-specific sensors, and to
adjust sensitivity depending on the task. Our sensor characterization
experiments demonstrate the capabilities of eFlesh: contact localization RMSE
of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for
shear force. We also present a learned slip detection model that generalizes to
unseen objects with 95% accuracy, and visuotactile control policies that
improve manipulation performance by 40% over vision-only baselines -- achieving
91% average success rate for four precise tasks that require sub-mm accuracy
for successful completion. All design files, code and the CAD-to-eFlesh STL
conversion tool are open-sourced and available on https://e-flesh.com.

</details>


### [268] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Main category: cs.RO

TL;DR: The paper proposes an ML-based framework for optimizing sampled picks in warehouse automation, achieving a 20% reduction in pick failure rates compared to heuristic methods.


<details>
  <summary>Details</summary>
Motivation: Warehouse automation needs enhancement in operational efficiency, cost minimization and resilience. Prior research focused on predicting success probabilities using heuristics but less on data-driven optimization for better performance.

Method: An ML-based framework was developed that predicts transform adjustments and improves suction cup selection for multi-suction end effectors, integrated and tested in workcells similar to Amazon Robotics' operations.

Result: Evaluated on over 2 million picks, the method achieved a 20% reduction in pick failure rates compared to a heuristic-based baseline.

Conclusion: The proposed ML framework is effective in improving pick success rates in large-scale warehouse automation scenarios.

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [269] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: Chain-of-Action (CoA) is a new paradigm for visuo-motor policy that uses backward reasoning and task-specific goals to generate entire trajectories, achieving state-of-the-art performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: To create a visuo-motor policy paradigm that can achieve strong spatial generalization capabilities while maintaining flexibility and simplicity.

Method: CoA generates an entire trajectory by explicit backward reasoning with task-specific goals through an action-level Chain-of-Thought (CoT) process, unified within a single autoregressive structure. It incorporates four complementary designs: continuous action token representation; dynamic stopping for variable-length trajectory generation; reverse temporal ensemble; and multi-token prediction.

Result: CoA achieves the state-of-the-art performance across 60 RLBench tasks and 8 real-world manipulation tasks.

Conclusion: CoA provides a novel approach to visuo-motor policy with strong spatial generalization capabilities, flexibility, and simplicity.

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [270] [Alice and the Caterpillar: A more descriptive null model for assessing data mining results](https://arxiv.org/abs/2506.09764)
*Giulia Preti,Gianmarco De Francisci Morales,Matteo Riondato*

Main category: cs.SI

TL;DR: The paper presents Alice, a new method using Markov chain Monte Carlo algorithms to sample datasets from novel null models for binary transactional and sequence datasets, which preserves more dataset properties and finds different significant results.


<details>
  <summary>Details</summary>
Motivation: To introduce more accurate null models for assessing results of binary transactional and sequence datasets by preserving more properties than existing models.

Method: Alice is a suite of Markov chain Monte Carlo algorithms that samples datasets from the null models, based on a carefully defined set of states and efficient operations to move between them. It maintains the Bipartite Joint Degree Matrix, preserving paths of length three and other properties.

Result: Experimental evaluation shows that Alice mixes fast and scales well, discovering different significant results compared to previous models.

Conclusion: The novel null models with Alice provide a better way to assess binary transactional and sequence datasets through statistical hypothesis testing.

Abstract: We introduce novel null models for assessing the results obtained from
observed binary transactional and sequence datasets, using statistical
hypothesis testing. Our null models maintain more properties of the observed
dataset than existing ones. Specifically, they preserve the Bipartite Joint
Degree Matrix of the bipartite (multi-)graph corresponding to the dataset,
which ensures that the number of caterpillars, i.e., paths of length three, is
preserved, in addition to other properties considered by other models. We
describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling
datasets from our null models, based on a carefully defined set of states and
efficient operations to move between them. The results of our experimental
evaluation show that Alice mixes fast and scales well, and that our null model
finds different significant results than ones previously considered in the
literature.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [271] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Main category: cs.DC

TL;DR: The paper presents EdgeProfiler, a framework for evaluating lightweight LLMs on edge systems using quantization and memory constraints, showing significant reductions in memory usage, improved inference speeds, and reduced energy consumption while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying large language models on resource-constrained edge devices by providing a systematic methodology to assess their performance.

Method: Using aggressive quantization techniques and strict memory constraints to profile compact LLMs, including TinyLLaMA, Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B. Analytical modeling is used to estimate latency, FLOPs, and energy consumption.

Result: 4-bit quantization reduces model memory usage by 60-70% with an accuracy drop of only 2-5% compared to full-precision baselines. Inference speeds improve by 2-3x compared to FP16 baselines across various edge devices. Power modeling estimates a 35-50% reduction in energy consumption for INT4 configurations.

Conclusion: Efficient profiling tailored to lightweight LLMs in edge environments is crucial for balancing accuracy, energy efficiency, and computational feasibility.

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [272] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Main category: cs.DC

TL;DR: This paper proposes SLED, a method using speculative decoding to enable efficient inferencing of large language models at the edge without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Efficient inferencing of large language models at the edge is challenging due to limited device memory and power constraints. Current strategies either trade accuracy for efficiency or incur significant costs.

Method: SLED allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single shared edge server batches and verifies the tokens using a more precise target model. This approach supports device heterogeneity and reduces server-side memory footprint.

Result: Experiments with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server showed significantly reduced latency, improved energy efficiency, and increased concurrent inference sessions without sacrificing model accuracy.

Conclusion: SLED presents a promising approach for efficient inferencing of large language models at the edge by leveraging speculative decoding and orchestrating computation across heterogeneous devices.

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [273] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Main category: cs.DC

TL;DR: TTrace is a system designed to detect and localize silent bugs in distributed training by collecting intermediate tensors and comparing them with a trusted single-device reference implementation, effectively finding bugs in Megatron-LM framework.


<details>
  <summary>Details</summary>
Motivation: Distributed training programs for large neural network models are complex and prone to silent bugs which can lead to incorrect training outcomes. Current debugging practices are inefficient and ineffective in detecting these bugs.

Method: TTrace collects intermediate tensors from distributed training in a fine-grained manner and compares them against those from a trusted single-device reference implementation. It uses novel mathematical analysis to set thresholds that distinguish bug-induced errors from floating-point round-off errors.

Result: TTrace successfully detected 11 existing bugs and 3 new bugs in the Megatron-LM framework, requiring minimal code changes (less than 10 lines). It is effective across various training recipes, including low-precision ones involving BF16 and FP8.

Conclusion: TTrace provides an efficient solution for detecting and localizing silent bugs in distributed training of large neural networks.

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [274] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: Hyperdimensional Computing (HDC) is a computing paradigm that uses high-dimensional vectors. While robust and parallel, traditional HDC has low accuracy. Recent methods improve this with iterative training but focus on specialized hardware. This paper introduces ScalableHD, a solution for efficient HDC inference on multi-core CPUs, achieving up to 10x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of HDC inference specifically on general-purpose multi-core CPUs, which has been largely overlooked in favor of specialized hardware like FPGAs and GPUs.

Method: ScalableHD employs a two-stage pipelined execution model with parallelization across cores. It processes chunks of base and class hypervectors, using a producer-consumer mechanism for intermediate results. Memory tiling and NUMA-aware worker-to-core binding are integrated for performance maximization. Two execution variants are tailored for different batch sizes to exploit compute parallelism while mitigating memory-bound constraints.

Result: ScalableHD achieves up to 10x speedup in throughput compared to state-of-the-art baselines such as TorchHD, across various tasks including human activity recognition and image classification, without compromising task accuracy. It demonstrates strong scalability with near-proportional throughput improvements when increasing the number of cores.

Conclusion: ScalableHD provides an effective solution for scalable and high-throughput HDC inference on multi-core CPUs, significantly improving upon existing methods.

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [275] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: STREAMINGGS is a new design that improves the speed and energy efficiency of 3D Gaussian Splatting on mobile devices by changing the rendering approach, offering significant performance enhancements over existing solutions.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) is efficient but does not meet the real-time requirement of 90 FPS on mobile devices, only achieving 2 to 9 FPS. Current accelerators focus on compute efficiency but ignore memory efficiency, leading to redundant DRAM traffic.

Method: The paper introduces STREAMINGGS, which is a fully streaming 3DGS algorithm-architecture co-design. It achieves fine-grained pipelining and reduces DRAM traffic by transforming from tile-centric rendering to memory-centric rendering.

Result: Compared with mobile Ampere GPUs, STREAMINGGS achieves up to 45.7 times speedup and 62.9 times energy savings.

Conclusion: STREAMINGGS significantly enhances the performance and energy efficiency of 3DGS on mobile devices.

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [276] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: This paper presents a simple Transformer-based framework for motion in-betweening that uses a single Transformer encoder to generate realistic motions, highlighting the importance of data modeling choices over model complexity.


<details>
  <summary>Details</summary>
Motivation: Current solutions for motion in-betweening rely on complex models with skeleton-aware architectures or multiple modules and training steps, which can be cumbersome. There is a need for a simpler yet effective solution.

Method: The authors introduce a Transformer-based framework using a single Transformer encoder to synthesize realistic motions for motion in-betweening tasks. They focus on data modeling choices such as increasing data volume, selecting appropriate pose representation, and incorporating velocity input features.

Result: The proposed method achieves high-quality motion transitions comparable to more complex models. The findings indicate that data modeling choices are crucial and can sometimes yield better results than increasing model complexity.

Conclusion: A simple Transformer-based framework can effectively handle motion in-betweening tasks. The research challenges the assumption that model complexity is the primary factor in animation quality and suggests focusing on data-centric approaches.

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [277] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: The paper presents DGS-LRM, a feed-forward model for reconstructing dynamic scenes from monocular posed videos using deformable 3D Gaussian splats.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward scene reconstruction models are limited to static scenes and fail to reconstruct the motion of moving objects. There is a need for a model that can handle dynamic scenes effectively.

Method: The method uses an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision. It employs a per-pixel deformable 3D Gaussian representation and a large transformer network for real-time dynamic scene reconstruction.

Result: DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods and significantly outperforms the state-of-the-art predictive dynamic reconstruction method on real-world examples. The predicted 3D deformation is accurate and adaptable for long-range 3D tracking tasks.

Conclusion: DGS-LRM is a successful feed-forward approach for dynamic scene reconstruction, offering high-quality results and effective long-range 3D tracking capabilities.

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>
