{"id": "2505.21552", "pdf": "https://arxiv.org/pdf/2505.21552", "abs": "https://arxiv.org/abs/2505.21552", "authors": ["Diogo Cruz"], "title": "Understanding the learned look-ahead behavior of chess neural networks", "categories": ["cs.AI", "cs.LG"], "comment": "40 pages, 47 figures", "summary": "We investigate the look-ahead capabilities of chess-playing neural networks,\nspecifically focusing on the Leela Chess Zero policy network. We build on the\nwork of Jenner et al. (2024) by analyzing the model's ability to consider\nfuture moves and alternative sequences beyond the immediate next move. Our\nfindings reveal that the network's look-ahead behavior is highly\ncontext-dependent, varying significantly based on the specific chess position.\nWe demonstrate that the model can process information about board states up to\nseven moves ahead, utilizing similar internal mechanisms across different\nfuture time steps. Additionally, we provide evidence that the network considers\nmultiple possible move sequences rather than focusing on a single line of play.\nThese results offer new insights into the emergence of sophisticated look-ahead\ncapabilities in neural networks trained on strategic tasks, contributing to our\nunderstanding of AI reasoning in complex domains. Our work also showcases the\neffectiveness of interpretability techniques in uncovering cognitive-like\nprocesses in artificial intelligence systems.", "AI": {"tldr": "The paper explores how chess-playing neural networks, particularly Leela Chess Zero's policy network, can look ahead in the game. It shows that these networks can plan up to seven moves ahead using context-dependent mechanisms and consider multiple move sequences, providing insights into AI reasoning.", "motivation": "To understand the look-ahead capabilities of chess-playing neural networks and how they process future moves beyond the immediate next one.", "method": "Analyze the Leela Chess Zero policy network's ability to consider future moves and alternative sequences by building on previous work and using interpretability techniques.", "result": "The network can process board states up to seven moves ahead and considers multiple possible move sequences rather than focusing on a single line of play.", "conclusion": "This study reveals sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains."}}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98.", "AI": {"tldr": "\u5c3d\u7ba1R1\u7c7b\u6a21\u578b\u5728\u63a8\u7406\u548c\u89c4\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9700\u8981\u7cbe\u786e\u8ba1\u7b97\u3001\u7b26\u53f7\u64cd\u4f5c\u3001\u4f18\u5316\u548c\u7b97\u6cd5\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u672c\u6587\u63d0\u51fa\u4e86R1-Code-Interpreter\uff0c\u901a\u8fc7\u591a\u8f6e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u4f7f\u6587\u672c\u578bLLM\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u591a\u4e2a\u4ee3\u7801\u67e5\u8be2\u4ee5\u8f85\u52a9\u63a8\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u7684\u81ea\u6211\u68c0\u67e5\u884c\u4e3a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u7cbe\u786e\u8ba1\u7b97\u548c\u7b26\u53f7\u64cd\u4f5c\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u6709\u6548\u5229\u7528\u4ee3\u7801\u6267\u884c\u7684\u516c\u5171\u7814\u7a76\u6307\u5bfc\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u65b9\u6cd5\uff0c\u4f7fLLM\u80fd\u591f\u5728\u6587\u672c\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e4b\u95f4\u505a\u51fa\u5408\u7406\u9009\u62e9\u3002", "method": "\u63d0\u51faR1-Code-Interpreter\u6269\u5c55\uff0c\u57fa\u4e8e\u6587\u672c\u578bLLM\u901a\u8fc7\u591a\u8f6e\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f7f\u5176\u5728\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u751f\u6210\u591a\u4e2a\u4ee3\u7801\u67e5\u8be2\u3002\u540c\u65f6\uff0c\u521b\u5efa\u4e86144\u4e2a\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5bf9Qwen-2.5\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u548c\u6d4b\u8bd5\uff0c\u63a2\u7d22\u4e0d\u540c\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u53c2\u6570\u8bbe\u7f6e\u3002", "result": "\u6700\u7ec8\u6a21\u578bR1-CI-14B\u572837\u4e2a\u6d4b\u8bd5\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u4ece44.0%\u63d0\u5347\u81f364.1%\uff0c\u4f18\u4e8e\u4ec5\u6587\u672c\u578bGPT-4o\uff0858.6%\uff09\uff0c\u63a5\u8fd1\u5e26\u6709\u4ee3\u7801\u89e3\u91ca\u5668\u7684GPT-4o\uff0870.9%\uff09\uff0c\u5e76\u5c55\u73b0\u51fa\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u7684\u81ea\u6211\u68c0\u67e5\u884c\u4e3a\u3002", "conclusion": "R1-Code-Interpreter\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5f3a\u8c03\u4e86\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u652f\u6301\u3002"}}
{"id": "2505.21671", "pdf": "https://arxiv.org/pdf/2505.21671", "abs": "https://arxiv.org/abs/2505.21671", "authors": ["Davin Choo", "Yuqi Pan", "Tonghan Wang", "Milind Tambe", "Alastair van Heerden", "Cheryl Johnson"], "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing", "categories": ["cs.AI", "cs.DS", "cs.LG", "math.OC"], "comment": null, "summary": "We study a sequential decision-making problem on a $n$-node graph $G$ where\neach node has an unknown label from a finite set $\\mathbf{\\Sigma}$, drawn from\na joint distribution $P$ that is Markov with respect to $G$. At each step,\nselecting a node reveals its label and yields a label-dependent reward. The\ngoal is to adaptively choose nodes to maximize expected accumulated discounted\nrewards. We impose a frontier exploration constraint, where actions are limited\nto neighbors of previously selected nodes, reflecting practical constraints in\nsettings such as contact tracing and robotic exploration. We design a Gittins\nindex-based policy that applies to general graphs and is provably optimal when\n$G$ is a forest. Our implementation runs in $O(n^2 \\cdot |\\mathbf{\\Sigma}|^2)$\ntime while using $O(n \\cdot |\\mathbf{\\Sigma}|^2)$ oracle calls to $P$ and\n$O(n^2 \\cdot |\\mathbf{\\Sigma}|)$ space. Experiments on synthetic and real-world\ngraphs show that our method consistently outperforms natural baselines,\nincluding in non-tree, budget-limited, and undiscounted settings. For example,\nin HIV testing simulations on real-world sexual interaction networks, our\npolicy detects nearly all positive cases with only half the population tested,\nsubstantially outperforming other baselines.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5177\u6709\u672a\u77e5\u6807\u7b7e\u7684n\u8282\u70b9\u56feG\u4e0a\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eGittins\u6307\u6570\u7684\u7b56\u7565\uff0c\u5728\u591a\u79cd\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u56fe\u7ed3\u6784\u4e0a\u5e26\u6709\u672a\u77e5\u6807\u7b7e\u8282\u70b9\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5b58\u5728\u524d\u6cbf\u63a2\u7d22\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u63a5\u89e6\u8005\u8ffd\u8e2a\u548c\u673a\u5668\u4eba\u63a2\u7d22\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eGittins\u6307\u6570\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u4e00\u822c\u56fe\u7ed3\u6784\uff0c\u5e76\u4e14\u5728\u56feG\u4e3a\u68ee\u6797\u65f6\u53ef\u8bc1\u660e\u662f\u6700\u4f18\u7684\u3002\u8be5\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n\u00b2\u00b7|\u03a3|\u00b2)\uff0c\u4f7f\u7528O(n\u00b7|\u03a3|\u00b2)\u6b21\u5bf9\u8054\u5408\u5206\u5e03P\u7684\u8c03\u7528\uff0c\u4ee5\u53caO(n\u00b2\u00b7|\u03a3|)\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\uff08\u975e\u6811\u72b6\u7ed3\u6784\u3001\u9884\u7b97\u53d7\u9650\u3001\u672a\u6298\u6263\uff09\u59cb\u7ec8\u4f18\u4e8e\u81ea\u7136\u57fa\u7ebf\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728\u73b0\u5b9e\u4e16\u754cHIV\u68c0\u6d4b\u6a21\u62df\u4e2d\uff0c\u53ea\u9700\u6d4b\u8bd5\u4e00\u534a\u4eba\u53e3\u5373\u53ef\u68c0\u6d4b\u5230\u51e0\u4e4e\u6240\u6709\u7684\u9633\u6027\u75c5\u4f8b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eGittins\u6307\u6570\u7684\u7b56\u7565\u5728\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u524d\u6cbf\u63a2\u7d22\u7ea6\u675f\u7684\u56fe\u7ed3\u6784\u4e0a\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.21674", "pdf": "https://arxiv.org/pdf/2505.21674", "abs": "https://arxiv.org/abs/2505.21674", "authors": ["Michael Katz", "Harsha Kokel", "Christian Muise", "Shirin Sohrabi", "Sarath Sreedharan"], "title": "Make Planning Research Rigorous Again!", "categories": ["cs.AI"], "comment": null, "summary": "In over sixty years since its inception, the field of planning has made\nsignificant contributions to both the theory and practice of building planning\nsoftware that can solve a never-before-seen planning problem. This was done\nthrough established practices of rigorous design and evaluation of planning\nsystems. It is our position that this rigor should be applied to the current\ntrend of work on planning with large language models. One way to do so is by\ncorrectly incorporating the insights, tools, and data from the automated\nplanning community into the design and evaluation of LLM-based planners. The\nexperience and expertise of the planning community are not just important from\na historical perspective; the lessons learned could play a crucial role in\naccelerating the development of LLM-based planners. This position is\nparticularly important in light of the abundance of recent works that replicate\nand propagate the same pitfalls that the planning community has encountered and\nlearned from. We believe that avoiding such known pitfalls will contribute\ngreatly to the progress in building LLM-based planners and to planning in\ngeneral.", "AI": {"tldr": "In over sixty years, planning field significantly contributed to building planning software. The rigor of design and evaluation should be applied to planning with large language models by incorporating insights, tools, and data from the automated planning community. This is crucial in avoiding known pitfalls encountered before.", "motivation": "The motivation of this paper lies in applying established practices of rigorous design and evaluation from traditional planning systems into the development of LLM-based planners, aiming to avoid replication of past pitfalls.", "method": "By correctly incorporating insights, tools, and data from the automated planning community into the design and evaluation of LLM-based planners.", "result": "This approach could accelerate the development of LLM-based planners and contribute greatly to the progress in building such planners.", "conclusion": "Avoiding known pitfalls will greatly contribute to the progress in building LLM-based planners and to planning in general."}}
{"id": "2505.21512", "pdf": "https://arxiv.org/pdf/2505.21512", "abs": "https://arxiv.org/abs/2505.21512", "authors": ["Harry Li", "Gabriel Appleby", "Kenneth Alperin", "Steven R Gomez", "Ashley Suh"], "title": "The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Knowledge graphs (KGs) are powerful data structures, but exploring them\neffectively remains difficult for even expert users. Large language models\n(LLMs) are increasingly used to address this gap, yet little is known\nempirically about how their usage with KGs shapes user trust, exploration\nstrategies, or downstream decision-making - raising key design challenges for\nLLM-based KG visual analysis systems. To study these effects, we developed\nLinkQ, a KG exploration system that converts natural language questions into\nstructured queries with an LLM. We collaborated with KG experts to design five\nvisual mechanisms that help users assess the accuracy of both KG queries and\nLLM responses: an LLM-KG state diagram that illustrates which stage of the\nexploration pipeline LinkQ is in, a query editor displaying the generated query\npaired with an LLM explanation, an entity-relation ID table showing extracted\nKG entities and relations with semantic descriptions, a query structure graph\nthat depicts the path traversed in the KG, and an interactive graph\nvisualization of query results. From a qualitative evaluation with 14\npractitioners, we found that users - even KG experts - tended to overtrust\nLinkQ's outputs due to its \"helpful\" visualizations, even when the LLM was\nincorrect. Users exhibited distinct workflows depending on their prior\nfamiliarity with KGs and LLMs, challenging the assumption that these systems\nare one-size-fits-all - despite often being designed as if they are. Our\nfindings highlight the risks of false trust in LLM-assisted data analysis tools\nand the need for further investigation into the role of visualization as a\nmitigation technique.", "AI": {"tldr": "LinkQ is a KG exploration system that uses LLMs to convert natural language questions into structured queries, with five visual mechanisms designed to help users evaluate the accuracy of both KG queries and LLM responses. However, even KG experts tend to overtrust LinkQ's outputs due to its 'helpful' visualizations.", "motivation": "To study the effects of using LLMs with KGs on user trust, exploration strategies, or downstream decision-making, which raises key design challenges for LLM-based KG visual analysis systems.", "method": "Developed LinkQ, a KG exploration system that converts natural language questions into structured queries with an LLM, and collaborated with KG experts to design five visual mechanisms to help users assess the accuracy of both KG queries and LLM responses.", "result": "From a qualitative evaluation with 14 practitioners, it was found that users - even KG experts - tended to overtrust LinkQ's outputs due to its 'helpful' visualizations, even when the LLM was incorrect. Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs.", "conclusion": "The findings highlight the risks of false trust in LLM-assisted data analysis tools and emphasize the need for further investigation into the role of visualization as a mitigation technique."}}
{"id": "2505.21609", "pdf": "https://arxiv.org/pdf/2505.21609", "abs": "https://arxiv.org/abs/2505.21609", "authors": ["Mathew J. Walter", "Aaron Barrett", "Kimberly Tam"], "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Adversarial artificial intelligence (AI) attacks pose a significant threat to\nautonomous transportation, such as maritime vessels, that rely on AI\ncomponents. Malicious actors can exploit these systems to deceive and\nmanipulate AI-driven operations. This paper addresses three critical research\nchallenges associated with adversarial AI: the limited scope of traditional\ndefences, inadequate security metrics, and the need to build resilience beyond\nmodel-level defences. To address these challenges, we propose building defences\nutilising multiple inputs and data fusion to create defensive components and an\nAI security metric as a novel approach toward developing more secure AI\nsystems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,\nand we evaluate it through real-world demonstrations and comprehensive\nquantitative analyses, comparing a system built with the DFCR method against\nsingle-input models and models utilising existing state-of-the-art defences.\nThe findings show that the DFCR approach significantly enhances resilience\nagainst adversarial machine learning attacks in maritime autonomous system\noperations, achieving up to a 35\\% reduction in loss for successful\nmulti-pronged perturbation attacks, up to a 100\\% reduction in loss for\nsuccessful adversarial patch attacks and up to 100\\% reduction in loss for\nsuccessful spoofing attacks when using these more resilient systems. We\ndemonstrate how DFCR and DFCR confidence scores can reduce adversarial AI\ncontact confidence and improve decision-making by the system, even when typical\nadversarial defences have been compromised. Ultimately, this work contributes\nto the development of more secure and resilient AI-driven systems against\nadversarial attacks.", "AI": {"tldr": "This paper proposes the Data Fusion Cyber Resilience (DFCR) method to enhance resilience against adversarial AI attacks in autonomous systems, particularly maritime vessels. Through real-world demonstrations and quantitative analyses, DFCR significantly reduces losses from various adversarial attacks and improves decision-making even when typical defences are compromised.", "motivation": "Adversarial AI attacks pose significant threats to autonomous transportation systems, such as maritime vessels. Current defences have a limited scope, security metrics are inadequate, and there is a need for resilience beyond model-level defences.", "method": "The DFCR method uses multiple inputs and data fusion to create defensive components and introduces an AI security metric. It is evaluated through real-world demonstrations and comprehensive quantitative analyses, comparing it with single-input models and state-of-the-art defences.", "result": "DFCR achieves up to a 35% reduction in loss for multi-pronged perturbation attacks, up to 100% reduction in loss for adversarial patch attacks, and up to 100% reduction in loss for spoofing attacks. It also reduces adversarial AI contact confidence and improves system decision-making.", "conclusion": "The DFCR method contributes to the development of more secure and resilient AI-driven systems against adversarial attacks."}}
{"id": "2505.21765", "pdf": "https://arxiv.org/pdf/2505.21765", "abs": "https://arxiv.org/abs/2505.21765", "authors": ["Sohyun An", "Ruochen Wang", "Tianyi Zhou", "Cho-Jui Hsieh"], "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models", "categories": ["cs.AI"], "comment": "Work In Progress", "summary": "While recent success of large reasoning models (LRMs) significantly advanced\nLLMs' reasoning capability by optimizing the final answer accuracy using\nreinforcement learning, they may also drastically increase the output length\ndue to overthinking, characterized by unnecessarily complex reasoning paths\nthat waste computation and potentially degrade the performance. We hypothesize\nthat such inefficiencies stem from LRMs' limited capability to dynamically\nselect the proper modular reasoning strategies, termed thinking patterns at the\nright position. To investigate this hypothesis, we propose a dynamic\noptimization framework that segments model-generated reasoning paths into\ndistinct thinking patterns, systematically identifying and promoting beneficial\npatterns that improve the answer while removing detrimental ones. Empirical\nanalysis confirms that our optimized thinking paths yield more concise yet\nsufficiently informative trajectories, enhancing reasoning efficiency by\nreducing attention FLOPs by up to 47% while maintaining accuracy for originally\ncorrect responses. Moreover, a non-trivial portion of originally incorrect\nresponses are transformed into correct ones, achieving a 15.6% accuracy\nimprovement with reduced length. Motivated by the improvement brought by the\noptimized thinking paths, we apply a preference optimization technique\nsupported by a pairwise dataset contrasting suboptimal and optimal reasoning\npaths. Experimental evaluations across multiple mathematical reasoning\nbenchmarks reveal that our method notably reduces computational overhead while\nsimultaneously improving reasoning accuracy, achieving up to a 12% accuracy\nimprovement and reducing token usage from approximately 5,000 to 3,000 tokens.", "AI": {"tldr": "Recent large reasoning models (LRMs) enhance LLMs' reasoning but may overthink, leading to inefficiency. This study proposes a dynamic optimization framework that identifies and promotes beneficial thinking patterns while removing detrimental ones, resulting in more concise and accurate reasoning paths. Experiments show significant reductions in computational overhead and token usage with notable accuracy improvements.", "motivation": "The motivation is to address the inefficiencies in LRMs caused by overthinking, which leads to unnecessarily complex reasoning paths that waste computation and potentially degrade performance. The authors aim to dynamically select proper modular reasoning strategies, termed thinking patterns, to improve reasoning efficiency.", "method": "The method involves proposing a dynamic optimization framework that segments model-generated reasoning paths into distinct thinking patterns. It systematically identifies and promotes beneficial patterns that improve answers while removing detrimental ones.", "result": "Empirical analysis shows that optimized thinking paths yield more concise yet sufficiently informative trajectories, reducing attention FLOPs by up to 47% while maintaining accuracy for originally correct responses. Additionally, it transforms a portion of incorrect responses into correct ones, achieving a 15.6% accuracy improvement with reduced length. Overall, the method reduces computational overhead and token usage while improving reasoning accuracy by up to 12%.", "conclusion": "The conclusion is that the proposed dynamic optimization framework effectively enhances reasoning efficiency by reducing unnecessary complexity in LRMs' reasoning paths. This results in both computational savings and improved accuracy."}}
{"id": "2505.21514", "pdf": "https://arxiv.org/pdf/2505.21514", "abs": "https://arxiv.org/abs/2505.21514", "authors": ["Mingchao Jiang", "Abhinav Jain", "Sophia Zorek", "Chris Jermaine"], "title": "SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation", "categories": ["cs.LG", "cs.PL", "cs.SE"], "comment": "Keywords: Benchmark Dataset, LLM Evaluation, Gen-AI, Program\n  Synthesis; TLDR: SimCoPilot is a benchmark for evaluating LLMs as\n  \"copilot\"-style interactive coding assistants, testing their ability to\n  integrate and complete code within complex real-world software environments", "summary": "We introduce SIMCOPILOT, a benchmark that simulates the role of large\nlanguage models (LLMs) as interactive, \"copilot\"-style coding assistants.\nTargeting both completion (finishing incomplete methods or code blocks) and\ninfill tasks (filling missing segments within existing code), SIMCOPILOT\nprovides a comprehensive framework for evaluating LLM coding capabilities. The\nbenchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python\n(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our\nkey contributions include: (a) establishing a realistic, detailed evaluation\nenvironment to assess LLM utility in practical coding scenarios, and (b)\nproviding fine-grained analyses that address critical factors frequently\noverlooked by existing benchmarks, such as task-specific performance nuances,\ncontextual understanding across code segments, and sensitivity to variable\nscope. Evaluations conducted across domains-including algorithms, databases,\ncomputer vision, and neural networks-offer insights into model strengths and\nhighlight persistent challenges in maintaining logical consistency within\ncomplex dependency structures. Beyond benchmarking, our study sheds light on\nthe current limitations of LLM-driven code generation and underscores the\nongoing transition of LLMs from merely syntax-aware generators toward reliable,\nintelligent software development partners.", "AI": {"tldr": "The paper introduces SIMCOPILOT, a benchmark simulating LLMs as coding assistants. It evaluates LLMs' coding capabilities in Java and Python through dedicated sub-benchmarks, providing detailed analyses of their performance nuances.", "motivation": "To establish a realistic evaluation environment for assessing LLM utility in practical coding scenarios and address overlooked factors in existing benchmarks.", "method": "SIMCOPILOT comprises sub-benchmarks for Java (SIMCOPILOTJ) and Python (SIMCOPILOTP), covering diverse codebases varying in size and complexity. It targets both completion and infill tasks, offering fine-grained analyses on task-specific performance, contextual understanding, and sensitivity to variable scope.", "result": "Evaluations across domains reveal insights into model strengths and highlight challenges in maintaining logical consistency within complex dependency structures. The study also sheds light on current limitations of LLM-driven code generation.", "conclusion": "SIMCOPILOT provides a comprehensive framework for evaluating LLM coding capabilities, marking the transition of LLMs from syntax-aware generators to reliable, intelligent software development partners."}}
{"id": "2505.21620", "pdf": "https://arxiv.org/pdf/2505.21620", "abs": "https://arxiv.org/abs/2505.21620", "authors": ["Zhengyuan Jiang", "Moyang Guo", "Kecen Li", "Yuepeng Hu", "Yupu Wang", "Zhicong Huang", "Cheng Hong", "Neil Zhenqiang Gong"], "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench.", "AI": {"tldr": "The paper presents VideoMarkBench, a benchmark for evaluating the robustness of video watermarks against various attacks, revealing vulnerabilities in current methods.", "motivation": "To address the lack of exploration on the robustness of video watermarking methods against perturbations and attacks.", "method": "Introduced VideoMarkBench, a systematic benchmark with a unified dataset from three video generative models across three styles, four watermarking methods, and seven aggregation strategies. Evaluated 12 types of perturbations under different threat models.", "result": "Found significant vulnerabilities in existing watermarking approaches.", "conclusion": "There is an urgent need for more robust video watermarking solutions."}}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE", "AI": {"tldr": "AIDSAFE is a new method using multi-agent deliberation to generate high-quality safety policy datasets, which improves LLMs' safety reasoning and reduces jailbreak vulnerabilities.", "motivation": "Existing safety measures for LLMs have limitations like over-refusal and vulnerability to jailbreaks. Creating accurate and conflict-free safety policy datasets is resource-intensive and challenging.", "method": "Propose AIDSAFE, incorporating multi-agent deliberation to iteratively expand safety policy reasoning and a data refiner stage to ensure quality by removing repetitive, redundant, or deceptive thoughts. Also, introduce a supplemental recipe using belief augmentation for preference data in alignment stages.", "result": "AIDSAFE-generated CoTs show superior policy adherence and reasoning quality. Fine-tuning open-source LLMs with these CoTs significantly enhances safety generalization and jailbreak robustness while maintaining utility and over-refusal accuracy.", "conclusion": "AIDSAFE provides an effective solution for generating high-quality safety policy datasets, improving LLMs' safety reasoning capabilities."}}
{"id": "2505.21525", "pdf": "https://arxiv.org/pdf/2505.21525", "abs": "https://arxiv.org/abs/2505.21525", "authors": ["Peiliang Gong", "Yucheng Wang", "Min Wu", "Zhenghua Chen", "Xiaoli Li", "Daoqiang Zhang"], "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach.", "AI": {"tldr": "TERSE\u662f\u4e00\u79cd\u65b0\u7684SFDA\u65b9\u6cd5\uff0c\u4e13\u4e3aMTS\u6570\u636e\u8bbe\u8ba1\uff0c\u901a\u8fc7\u65f6\u7a7a\u7279\u5f81\u7f16\u7801\u5668\u548c\u4efb\u52a1\u6765\u6355\u6349\u65f6\u7a7a\u7279\u6027\uff0c\u4fc3\u8fdb\u8de8\u57df\u7684\u9690\u5f0f\u7279\u5f81\u5bf9\u9f50\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684SFDA\u65b9\u6cd5\u5728\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u672a\u80fd\u8003\u8651\u5230MTS\u6570\u636e\u4e2d\u56fa\u6709\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u800c\u8fd9\u4e9b\u76f8\u5173\u6027\u5bf9\u4e8e\u51c6\u786e\u8868\u793a\u6570\u636e\u548c\u8de8\u57df\u4fdd\u7559\u4e0d\u53d8\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "method": "TERSE\u5305\u62ec\u4e00\u4e2a\u5b9a\u5236\u7684\u7a7a\u95f4\u65f6\u95f4\u7279\u5f81\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u6355\u83b7\u6f5c\u5728\u7684\u7a7a\u95f4\u65f6\u95f4\u7279\u6027\uff0c\u5e76\u7ed3\u5408\u65f6\u95f4\u548c\u7a7a\u95f4\u6062\u590d\u4efb\u52a1\uff0c\u4ee5\u91cd\u5efa\u88ab\u65f6\u95f4\u5c4f\u853d\u7684\u65f6\u95f4\u5e8f\u5217\u7684\u6f5c\u5728\u8868\u793a\u548c\u88ab\u7a7a\u95f4\u5c4f\u853d\u7684\u76f8\u5173\u7ed3\u6784\u3002\u5728\u76ee\u6807\u9002\u5e94\u9636\u6bb5\uff0c\u76ee\u6807\u7f16\u7801\u5668\u901a\u8fc7\u5229\u7528\u6e90\u9884\u8bad\u7ec3\u7684\u65f6\u95f4\u6062\u590d\u548c\u7a7a\u95f4\u91cd\u8fde\u7f51\u7edc\uff0c\u751f\u6210\u4e0e\u6e90\u57df\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u7279\u5f81\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86TERSE\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002", "conclusion": "TERSE\u53ef\u4ee5\u6709\u6548\u5730\u5efa\u6a21\u548c\u8f6c\u79fb\u8de8\u57df\u7684\u7a7a\u95f4\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4fc3\u8fdb\u9690\u5f0f\u7279\u5f81\u5bf9\u9f50\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\u96c6\u6210\u5230\u5df2\u5efa\u7acb\u7684SFDA\u65b9\u6cd5\u4e2d\u3002"}}
{"id": "2505.21636", "pdf": "https://arxiv.org/pdf/2505.21636", "abs": "https://arxiv.org/abs/2505.21636", "authors": ["Alexander Nemecek", "Yuzhou Jiang", "Erman Ayday"], "title": "The Feasibility of Topic-Based Watermarking on Academic Peer Reviews", "categories": ["cs.CR", "cs.AI"], "comment": "8 pages main, 9 pages appendix", "summary": "Large language models (LLMs) are increasingly integrated into academic\nworkflows, with many conferences and journals permitting their use for tasks\nsuch as language refinement and literature summarization. However, their use in\npeer review remains prohibited due to concerns around confidentiality breaches,\nhallucinated content, and inconsistent evaluations. As LLM-generated text\nbecomes more indistinguishable from human writing, there is a growing need for\nreliable attribution mechanisms to preserve the integrity of the review\nprocess. In this work, we evaluate topic-based watermarking (TBW), a\nlightweight, semantic-aware technique designed to embed detectable signals into\nLLM-generated text. We conduct a comprehensive assessment across multiple LLM\nconfigurations, including base, few-shot, and fine-tuned variants, using\nauthentic peer review data from academic conferences. Our results show that TBW\nmaintains review quality relative to non-watermarked outputs, while\ndemonstrating strong robustness to paraphrasing-based evasion. These findings\nhighlight the viability of TBW as a minimally intrusive and practical solution\nfor enforcing LLM usage in peer review.", "AI": {"tldr": "The paper evaluates topic-based watermarking (TBW) as a method to embed detectable signals into LLM-generated text for use in peer reviews, showing it maintains review quality and is robust to paraphrasing.", "motivation": "To address concerns about confidentiality breaches, hallucinated content, and inconsistent evaluations when using LLMs in peer review, there is a need for reliable attribution mechanisms.", "method": "Evaluate topic-based watermarking (TBW) across multiple LLM configurations (base, few-shot, fine-tuned) using authentic peer review data from academic conferences.", "result": "TBW maintains review quality relative to non-watermarked outputs and demonstrates strong robustness to paraphrasing-based evasion.", "conclusion": "TBW is a minimally intrusive and practical solution for enforcing LLM usage in peer review."}}
{"id": "2505.21828", "pdf": "https://arxiv.org/pdf/2505.21828", "abs": "https://arxiv.org/abs/2505.21828", "authors": ["Chen Yueh-Han", "Guy Davidson", "Brenden M. Lake"], "title": "SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts", "categories": ["cs.AI"], "comment": null, "summary": "Do LLMs robustly generalize critical safety facts to novel situations?\nLacking this ability is dangerous when users ask naive questions. For instance,\n\"I'm considering packing melon balls for my 10-month-old's lunch. What other\nfoods would be good to include?\" Before offering food options, the LLM should\nwarn that melon balls pose a choking hazard to toddlers, as documented by the\nCDC. Failing to provide such warnings could result in serious injuries or even\ndeath. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic\nGEneralization evaluation, the first benchmark that tests whether LLMs properly\napply well established safety facts to naive user queries. SAGE-Eval comprises\n104 facts manually sourced from reputable organizations, systematically\naugmented to create 10,428 test scenarios across 7 common domains (e.g.,\nOutdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,\npasses only 58% of all the safety facts tested. We also observe that model\ncapabilities and training compute weakly correlate with performance on\nSAGE-Eval, implying that scaling up is not the golden solution. Our findings\nsuggest frontier LLMs still lack robust generalization ability. We recommend\ndevelopers use SAGE-Eval in pre-deployment evaluations to assess model\nreliability in addressing salient risks. We publicly release SAGE-Eval at\nhttps://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available\nat https://github.com/YuehHanChen/SAGE-Eval/tree/main.", "AI": {"tldr": "The paper introduces SAGE-Eval, a benchmark to test if LLMs can apply established safety facts to user queries. The top model passes only 58% of tests, showing a lack of robust generalization. Developers should use SAGE-Eval for pre-deployment evaluations.", "motivation": "To evaluate whether LLMs can robustly generalize critical safety facts to novel situations and warn users about potential dangers.", "method": "Introduced SAGE-Eval, a benchmark consisting of 104 facts from reputable organizations, augmented into 10,428 test scenarios across 7 domains, to test LLMs' ability to apply safety facts to naive user queries.", "result": "Claude-3.7-sonnet, the top model tested, passed only 58% of the safety facts tests. Model capabilities and training compute weakly correlate with performance on SAGE-Eval.", "conclusion": "Frontier LLMs still lack robust generalization ability in applying safety facts. Developers are recommended to use SAGE-Eval for pre-deployment evaluations."}}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md.", "AI": {"tldr": "Large Language Model (LLM)-based agents can be used to reduce prediction errors in chemistry tools.", "motivation": "LLM-based agents have shown potential in improving performance for chemistry-related tasks, but their effectiveness is still constrained by the prediction errors of chemistry tools.", "method": "Propose ChemHAS, a method that enhances chemistry tools by optimizing agent-stacking structures from limited data.", "result": "ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks and identifies four distinct agent-stacking behaviors.", "conclusion": "ChemHAS effectively compensates for prediction errors of chemistry tools and improves interpretability, opening new possibilities for AI agent applications in scientific research."}}
{"id": "2505.21642", "pdf": "https://arxiv.org/pdf/2505.21642", "abs": "https://arxiv.org/abs/2505.21642", "authors": ["Joshua Drexel", "Esther H\u00e4nggi", "Iy\u00e1n M\u00e9ndez Veiga"], "title": "Reproducible Builds and Insights from an Independent Verifier for Arch Linux", "categories": ["cs.CR"], "comment": "14 pages, 2 figures, Sicherheit 2024", "summary": "Supply chain attacks have emerged as a prominent cybersecurity threat in\nrecent years. Reproducible and bootstrappable builds have the potential to\nreduce such attacks significantly. In combination with independent, exhaustive\nand periodic source code audits, these measures can effectively eradicate\ncompromises in the building process. In this paper we introduce both concepts,\nwe analyze the achievements over the last ten years and explain the remaining\nchallenges. We contribute to the reproducible builds effort by setting up a\nrebuilder and verifier instance to test the reproducibility of Arch Linux\npackages. Using the results from this instance, we uncover an unnoticed and\nsecurity-relevant packaging issue affecting 16 packages related to Certbot, the\nrecommended software to install TLS certificates from Let's Encrypt, making\nthem unreproducible. Additionally, we find the root cause of unreproduciblity\nin the source code of fwupd, a critical software used to update device firmware\non Linux devices, and submit an upstream patch to fix it.", "AI": {"tldr": "\u4e3a\u4e86\u5e94\u5bf9\u4f9b\u5e94\u94fe\u653b\u51fb\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u53ef\u91cd\u73b0\u6784\u5efa\u548c\u5f15\u5bfc\u6784\u5efa\u7684\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u8fc7\u53bb\u5341\u5e74\u7684\u6210\u679c\u4e0e\u5269\u4f59\u6311\u6218\u3002\u901a\u8fc7\u8bbe\u7f6eArch Linux\u5305\u7684\u91cd\u5efa\u8005\u548c\u9a8c\u8bc1\u8005\u5b9e\u4f8b\uff0c\u53d1\u73b0Certbot\u76f8\u5173\u5305\u7684\u672a\u88ab\u6ce8\u610f\u5230\u7684\u5b89\u5168\u95ee\u9898\uff0c\u4ee5\u53cafwupd\u4e2d\u5bfc\u81f4\u4e0d\u53ef\u91cd\u73b0\u6027\u7684\u6839\u6e90\u5e76\u63d0\u4ea4\u8865\u4e01\u3002", "motivation": "\u4f9b\u5e94\u94fe\u653b\u51fb\u6210\u4e3a\u8fd1\u5e74\u6765\u663e\u8457\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff0c\u53ef\u91cd\u73b0\u548c\u53ef\u5f15\u5bfc\u6784\u5efa\u7ed3\u5408\u72ec\u7acb\u3001\u5168\u9762\u548c\u5b9a\u671f\u7684\u6e90\u4ee3\u7801\u5ba1\u8ba1\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u6b64\u7c7b\u653b\u51fb\u3002", "method": "\u4ecb\u7ecd\u53ef\u91cd\u73b0\u6784\u5efa\u548c\u5f15\u5bfc\u6784\u5efa\u7684\u6982\u5ff5\uff0c\u5206\u6790\u8fc7\u53bb\u5341\u5e74\u7684\u6210\u5c31\uff0c\u89e3\u91ca\u5269\u4f59\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5efa\u7acbrebuilder\u548cverifier\u5b9e\u4f8b\u6d4b\u8bd5Arch Linux\u5305\u7684\u53ef\u91cd\u73b0\u6027\u3002", "result": "\u53d1\u73b0\u4e86\u5f71\u54cd16\u4e2a\u4e0eCertbot\u76f8\u5173\u7684\u5305\u7684\u672a\u6ce8\u610f\u5230\u7684\u5b89\u5168\u76f8\u5173\u5305\u88c5\u95ee\u9898\uff0c\u627e\u51fafwupd\u6e90\u4ee3\u7801\u4e2d\u4e0d\u53ef\u91cd\u73b0\u7684\u6839\u672c\u539f\u56e0\u5e76\u63d0\u4ea4\u4e0a\u6e38\u8865\u4e01\u3002", "conclusion": "\u53ef\u91cd\u73b0\u6784\u5efa\u5728\u51cf\u5c11\u4f9b\u5e94\u94fe\u653b\u51fb\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u9700\u8981\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2505.21887", "pdf": "https://arxiv.org/pdf/2505.21887", "abs": "https://arxiv.org/abs/2505.21887", "authors": ["Ahmed Heakl", "Yahia Salaheldin Shaaban", "Martin Takac", "Salem Lahlou", "Zangir Iklassov"], "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem", "categories": ["cs.AI", "cs.CE", "cs.LG"], "comment": "18 pages, 14 figures, 11 tables", "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.", "AI": {"tldr": "SVRPBench\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u6355\u6349\u57ce\u5e02\u89c4\u6a21\u8f66\u8f86\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u9ad8\u4fdd\u771f\u968f\u673a\u52a8\u6001\u3002\u5b83\u6a21\u62df\u4e86\u771f\u5b9e\u7684\u914d\u9001\u6761\u4ef6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709RL\u6c42\u89e3\u5668\u5728\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u800c\u7ecf\u5178\u548c\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u5219\u4fdd\u6301\u7a33\u5065\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u5047\u8bbe\u9759\u6001\u3001\u7406\u60f3\u5316\u7684\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u6d41\u4e2d\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u7684\u8003\u8651\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7500\u4e2a\u5b9e\u4f8b\u7684\u5f00\u653e\u57fa\u51c6SVRPBench\uff0c\u6a21\u62df\u65f6\u95f4\u4f9d\u8d56\u6027\u62e5\u5835\u3001\u5bf9\u6570\u6b63\u6001\u5ef6\u8fdf\u3001\u6982\u7387\u4e8b\u6545\u548c\u57fa\u4e8e\u5b9e\u8bc1\u7684\u65f6\u95f4\u7a97\u53e3\u7b49\u771f\u5b9e\u914d\u9001\u6761\u4ef6\u3002\u4f7f\u7528\u5305\u62ec\u591a\u4ed3\u5e93\u548c\u591a\u8f66\u8f86\u8bbe\u7f6e\u5728\u5185\u7684\u591a\u6837\u5316\u7ea6\u675f\u573a\u666f\u751f\u6210\u7ba1\u9053\u3002", "result": "\u53d1\u73b0\u6700\u5148\u8fdb\u7684RL\u6c42\u89e3\u5668\uff08\u5982POMO\u548cAM\uff09\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65f6\u6027\u80fd\u4e0b\u964d\u8d85\u8fc720%\uff0c\u800c\u7ecf\u5178\u548c\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u4ecd\u7136\u4fdd\u6301\u7a33\u5065\u3002", "conclusion": "SVRPBench\u6311\u6218\u7814\u7a76\u793e\u533a\u8bbe\u8ba1\u80fd\u591f\u8d85\u8d8a\u5408\u6210\u5047\u8bbe\u5e76\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u4e0d\u786e\u5b9a\u6027\u7684\u6c42\u89e3\u5668\uff0c\u5e76\u53d1\u5e03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5957\u4ef6\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2505.21571", "pdf": "https://arxiv.org/pdf/2505.21571", "abs": "https://arxiv.org/abs/2505.21571", "authors": ["Yao Lu", "Tengfei Ma", "Zeyu Wang", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the rapid development of wireless communications and the growing\ncomplexity of digital modulation schemes, traditional manual modulation\nrecognition methods struggle to extract reliable signal features and meet\nreal-time requirements in modern scenarios. Recently, deep learning based\nAutomatic Modulation Recognition (AMR) approaches have greatly improved\nclassification accuracy. However, their large model sizes and high\ncomputational demands hinder deployment on resource-constrained devices. Model\npruning provides a general approach to reduce model complexity, but existing\nweight, channel, and layer pruning techniques each present a trade-off between\ncompression rate, hardware acceleration, and accuracy preservation. To this\nend, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning\nframework that combines channel-level pruning with layer-level collapse\ndiagnosis to achieve extreme compression, high performance and efficient\ninference. In the first stage of FCOS, hierarchical clustering and parameter\nfusion are applied to channel weights to achieve channel-level pruning. Then a\nLayer Collapse Diagnosis (LaCD) module uses linear probing to identify layer\ncollapse and removes the collapsed layers due to high channel compression\nratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms\nexisting channel and layer pruning methods. Specifically, FCOS achieves 95.51%\nFLOPs reduction and 95.31% parameter reduction while still maintaining\nperformance close to the original ResNet56, with only a 0.46% drop in accuracy\non Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.", "AI": {"tldr": "FCOS\u662f\u4e00\u79cd\u65b0\u7684\u7ec6\u5230\u7c97\u7684\u4e24\u9636\u6bb5\u526a\u679d\u6846\u67b6\uff0c\u7ed3\u5408\u901a\u9053\u7ea7\u526a\u679d\u4e0e\u5c42\u574d\u7f29\u8bca\u65ad\uff0c\u5b9e\u73b0\u6781\u81f4\u538b\u7f29\u3001\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u63a8\u7406\u3002\u5728AMR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u51cf\u5c11\u4e86FLOPs\u548c\u53c2\u6570\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u59cbResNet56\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u8c03\u5236\u8bc6\u522b\u65b9\u6cd5\u5728\u73b0\u4ee3\u573a\u666f\u4e2d\u96be\u4ee5\u63d0\u53d6\u53ef\u9760\u4fe1\u53f7\u7279\u5f81\u5e76\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4f46\u5176\u5927\u6a21\u578b\u5c3a\u5bf8\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u73b0\u6709\u7684\u526a\u679d\u6280\u672f\u5728\u538b\u7f29\u7387\u3001\u786c\u4ef6\u52a0\u901f\u548c\u7cbe\u5ea6\u4fdd\u6301\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u5f15\u5165\u4e86FCOS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5206\u5c42\u805a\u7c7b\u548c\u53c2\u6570\u878d\u5408\u5bf9\u901a\u9053\u6743\u91cd\u8fdb\u884c\u901a\u9053\u7ea7\u526a\u679d\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u5c42\u574d\u7f29\u8bca\u65ad\u6a21\u5757\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u8bc6\u522b\u5e76\u79fb\u9664\u56e0\u9ad8\u901a\u9053\u538b\u7f29\u6bd4\u800c\u574d\u7f29\u7684\u5c42\u3002", "result": "\u5728\u591a\u4e2aAMR\u57fa\u51c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cFCOS\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u9053\u548c\u5c42\u526a\u679d\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0cFCOS\u5b9e\u73b0\u4e8695.51%\u7684FLOPs\u51cf\u5c11\u548c95.31%\u7684\u53c2\u6570\u51cf\u5c11\uff0c\u540c\u65f6\u5728Sig2019-12\u4e0a\u4ec5\u635f\u59310.46%\u7684\u51c6\u786e\u7387\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u59cbResNet56\u3002", "conclusion": "FCOS\u6846\u67b6\u5728\u5b9e\u73b0\u9ad8\u6a21\u578b\u538b\u7f29\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u81ea\u52a8\u8c03\u5236\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2505.21703", "pdf": "https://arxiv.org/pdf/2505.21703", "abs": "https://arxiv.org/abs/2505.21703", "authors": ["Julia Boone", "Tolunay Seyfi", "Fatemeh Afghah"], "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": "Accepted for publication in the IEEE Internet of Things Journal\n  (IoT-J)", "summary": "Internet of Vehicles (IoV) systems, while offering significant advancements\nin transportation efficiency and safety, introduce substantial security\nvulnerabilities due to their highly interconnected nature. These dynamic\nsystems produce massive amounts of data between vehicles, infrastructure, and\ncloud services and present a highly distributed framework with a wide attack\nsurface. In considering network-centered attacks on IoV systems, attacks such\nas Denial-of-Service (DoS) can prohibit the communication of essential physical\ntraffic safety information between system elements, illustrating that the\nsecurity concerns for these systems go beyond the traditional confidentiality,\nintegrity, and availability concerns of enterprise systems. Given the\ncomplexity and volume of data generated by IoV systems, traditional security\nmechanisms are often inadequate for accurately detecting sophisticated and\nevolving cyberattacks. Here, we present an unsupervised autoencoder method\ntrained entirely on benign network data for the purpose of unseen attack\ndetection in IoV networks. We leverage a weighted combination of reconstruction\nand triplet margin loss to guide the autoencoder training and develop a diverse\nrepresentation of the benign training set. We conduct extensive experiments on\nrecent network intrusion datasets from two different application domains,\nindustrial IoT and home IoT, that represent the modern IoV task. We show that\nour method performs robustly for all unseen attack types, with roughly 99%\naccuracy on benign data and between 97% and 100% performance on anomaly data.\nWe extend these results to show that our model is adaptable through the use of\ntransfer learning, achieving similarly high results while leveraging domain\nfeatures from one domain to another.", "AI": {"tldr": "In this paper, researchers address the security vulnerabilities in Internet of Vehicles (IoV) systems which traditional mechanisms cannot handle effectively due to the complexity and volume of data. They propose an unsupervised autoencoder method trained solely on benign network data for detecting unseen attacks in IoV networks. The model uses a weighted combination of reconstruction and triplet margin loss during training and demonstrates robust performance with high accuracy on both benign and anomaly data. Experiments conducted on recent network intrusion datasets show that the method is effective across different application domains and adaptable via transfer learning.", "motivation": "The motivation behind this research lies in the substantial security vulnerabilities introduced by IoV systems. Traditional security mechanisms are inadequate for accurately detecting sophisticated and evolving cyberattacks in these highly interconnected systems.", "method": "The method involves an unsupervised autoencoder trained entirely on benign network data for detecting unseen attacks in IoV networks. A weighted combination of reconstruction and triplet margin loss is used to guide the autoencoder training and develop a diverse representation of the benign training set.", "result": "The proposed method performs robustly for all unseen attack types, achieving approximately 99% accuracy on benign data and between 97% to 100% performance on anomaly data. Additionally, the model's adaptability through transfer learning is demonstrated, maintaining high performance while leveraging domain features from one domain to another.", "conclusion": "The conclusion is that the unsupervised autoencoder method effectively detects unseen attacks in IoV networks with high accuracy and robustness. The model's adaptability via transfer learning further enhances its applicability across different domains."}}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907", "abs": "https://arxiv.org/abs/2505.21907", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design.", "AI": {"tldr": "AI copilots are AI systems assisting in tasks like software development and content creation. Personalization through preference optimization is crucial for their usability, trust, and productivity. This survey synthesizes research on capturing, modeling, and refining user preferences within AI copilot design, proposing a phase-based taxonomy of preference optimization strategies.", "motivation": "To address the gap in adapting personalization techniques to interactive, real-time systems like AI copilots and provide a structured foundation for designing adaptive, preference-aware AI copilots.", "method": "The survey introduces a unified definition of AI copilots and proposes a phase-based taxonomy of preference optimization strategies, analyzing techniques for acquiring preference signals, modeling user intent, and integrating feedback loops.", "result": "The survey provides insights into AI personalization, human-AI collaboration, and large language model adaptation, offering a holistic view of preference resources and suitable technical approaches for each stage of system design.", "conclusion": "This survey bridges fragmented knowledge about preference optimization in AI copilots, guiding future research and development towards more personalized and effective AI systems."}}
{"id": "2505.21573", "pdf": "https://arxiv.org/pdf/2505.21573", "abs": "https://arxiv.org/abs/2505.21573", "authors": ["Han Wan", "Rui Zhang", "Hao Sun"], "title": "Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Partial differential equations (PDEs) govern the spatiotemporal evolution of\nvarious physical systems. Classical numerical solvers, while accurate, require\nfine discretization and full knowledge of the governing PDEs, limiting their\napplicability when the physics is unknown or fast inference is required.\nData-driven neural PDE solvers alleviate these constraints by learning from\ndata but demand large training datasets and perform poorly in data-scarce\nregimes. Physics-aware methods mitigate data requirements by incorporating\nphysical knowledge yet rely on known PDE terms or local numerical schemes,\nrestricting their ability to handle unknown or globally coupled systems. In\nthis work, we propose the Spectral-inspired Neural Operator (SINO), a novel\nframework that learns PDE operators from limited trajectories (as few as 2-5),\nwithout any known PDE terms. SINO operates in the frequency domain and\nintroduces a Frequency-to-Vector module to learn spectral representations\nanalogous to derivative multipliers. To model nonlinear physical interactions,\nwe design a nonlinear operator block that includes a $\\Pi$-Block with low-pass\nfiltering to prevent aliasing. Finally, we introduce an operator distillation\ntechnique to distill the trained model for efficient inference. SINO achieves\nstate-of-the-art results across multiple PDE benchmarks, demonstrating strong\ndiscretization invariance and robust generalization to out-of-distribution\ninitial conditions. To our knowledge, SINO is the first physics-aware method\ncapable of accurately simulating globally coupled systems (e.g., the\nNavier-Stokes equations) from limited data without any explicit PDE terms.", "AI": {"tldr": "SINO is a novel framework that learns PDE operators from limited trajectories without known PDE terms, achieving state-of-the-art results across multiple benchmarks.", "motivation": "Classical numerical solvers are accurate but require fine discretization and full knowledge of the governing PDEs. Data-driven neural PDE solvers learn from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes.", "method": "Propose Spectral-inspired Neural Operator (SINO), which operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. Design a nonlinear operator block including a \u03a0-Block with low-pass filtering to prevent aliasing. Introduce an operator distillation technique for efficient inference.", "result": "Achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. Can accurately simulate globally coupled systems (e.g., Navier-Stokes equations) from limited data without explicit PDE terms.", "conclusion": "SINO is the first physics-aware method capable of accurately simulating globally coupled systems from limited data without any explicit PDE terms."}}
{"id": "2505.21725", "pdf": "https://arxiv.org/pdf/2505.21725", "abs": "https://arxiv.org/abs/2505.21725", "authors": ["Alessio Di Santo"], "title": "Lazarus Group Targets Crypto-Wallets and Financial Data while employing new Tradecrafts", "categories": ["cs.CR", "cs.OS"], "comment": null, "summary": "This report presents a comprehensive analysis of a malicious software sample,\ndetailing its architecture, behavioral characteristics, and underlying intent.\nThrough static and dynamic examination, the malware core functionalities,\nincluding persistence mechanisms, command-and-control communication, and data\nexfiltration routines, are identified and its supporting infrastructure is\nmapped. By correlating observed indicators of compromise with known techniques,\ntactics, and procedures, this analysis situates the sample within the broader\ncontext of contemporary threat campaigns and infers the capabilities and\nmotivations of its likely threat actor.\n  Building on these findings, actionable threat intelligence is provided to\nsupport proactive defenses. Threat hunting teams receive precise detection\nhypotheses for uncovering latent adversarial presence, while monitoring systems\ncan refine alert logic to detect anomalous activity in real time. Finally, the\nreport discusses how this structured intelligence enhances predictive risk\nassessments, informs vulnerability prioritization, and strengthens\norganizational resilience against advanced persistent threats. By integrating\ndetailed technical insights with strategic threat landscape mapping, this\nmalware analysis report not only reconstructs past adversary actions but also\nestablishes a robust foundation for anticipating and mitigating future attacks.", "AI": {"tldr": "This report conducts a thorough analysis of a malware sample's architecture and behavior, links it to known threat actors, and provides actionable intelligence to improve defense.", "motivation": "To understand the capabilities and motivations of the likely threat actor behind a specific malware sample and provide actionable threat intelligence for proactive defense.", "method": "Through static and dynamic examination, identifying core functionalities such as persistence mechanisms, command-and-control communication, and data exfiltration routines, then correlating indicators of compromise with known techniques, tactics, and procedures.", "result": "The malware sample is situated within contemporary threat campaigns, providing precise detection hypotheses and refined alert logic for real-time monitoring systems.", "conclusion": "This structured intelligence enhances predictive risk assessments, informs vulnerability prioritization, and strengthens organizational resilience against advanced persistent threats."}}
{"id": "2505.21935", "pdf": "https://arxiv.org/pdf/2505.21935", "abs": "https://arxiv.org/abs/2505.21935", "authors": ["Kaiyu He", "Zhiyu Chen"], "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Since the advent of Large Language Models (LLMs), efforts have largely\nfocused on improving their instruction-following and deductive reasoning\nabilities, leaving open the question of whether these models can truly discover\nnew knowledge. In pursuit of artificial general intelligence (AGI), there is a\ngrowing need for models that not only execute commands or retrieve information\nbut also learn, reason, and generate new knowledge by formulating novel\nhypotheses and theories that deepen our understanding of the world. Guided by\nPeirce's framework of abduction, deduction, and induction, this survey offers a\nstructured lens to examine LLM-based hypothesis discovery. We synthesize\nexisting work in hypothesis generation, application, and validation,\nidentifying both key achievements and critical gaps. By unifying these threads,\nwe illuminate how LLMs might evolve from mere ``information executors'' into\nengines of genuine innovation, potentially transforming research, science, and\nreal-world problem solving.", "AI": {"tldr": "Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. Through Peirce's framework of abduction, deduction, and induction, this survey examines LLM-based hypothesis discovery and how LLMs might evolve from mere ``information executors'' into engines of genuine innovation.", "motivation": "The motivation is to explore whether LLMs can go beyond executing commands or retrieving information and truly discover new knowledge by learning, reasoning, and generating novel hypotheses and theories that deepen our understanding of the world in pursuit of artificial general intelligence (AGI).", "method": "This survey uses Peirce's framework of abduction, deduction, and induction to examine LLM-based hypothesis discovery. It synthesizes existing work in hypothesis generation, application, and validation.", "result": "The survey identifies key achievements and critical gaps in the field of LLM-based hypothesis discovery.", "conclusion": "LLMs have the potential to evolve from mere ``information executors'' into engines of genuine innovation, which could transform research, science, and real-world problem solving."}}
{"id": "2505.21576", "pdf": "https://arxiv.org/pdf/2505.21576", "abs": "https://arxiv.org/abs/2505.21576", "authors": ["Jiawei Tang", "Yuheng Jia"], "title": "Concentration Distribution Learning from Label Distributions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Label distribution learning (LDL) is an effective method to predict the\nrelative label description degree (a.k.a. label distribution) of a sample.\nHowever, the label distribution is not a complete representation of an instance\nbecause it overlooks the absolute intensity of each label. Specifically, it's\nimpossible to obtain the total description degree of hidden labels that not in\nthe label space, which leads to the loss of information and confusion in\ninstances. To solve the above problem, we come up with a new concept named\nbackground concentration to serve as the absolute description degree term of\nthe label distribution and introduce it into the LDL process, forming the\nimproved paradigm of concentration distribution learning. Moreover, we propose\na novel model by probabilistic methods and neural networks to learn label\ndistributions and background concentrations from existing LDL datasets.\nExtensive experiments prove that the proposed approach is able to extract\nbackground concentrations from label distributions while producing more\naccurate prediction results than the state-of-the-art LDL methods. The code is\navailable in https://github.com/seutjw/CDL-LD.", "AI": {"tldr": "This paper proposes an improved paradigm called concentration distribution learning that introduces the concept of background concentration to address the limitations of label distribution learning (LDL). It presents a novel model using probabilistic methods and neural networks for learning label distributions and background concentrations, achieving more accurate predictions than existing LDL methods.", "motivation": "The limitation of traditional label distribution learning (LDL) lies in its inability to represent the absolute intensity of each label, leading to information loss and confusion in instances. This motivates the need for a new approach that can capture both relative and absolute description degrees of labels.", "method": "The authors introduce the concept of 'background concentration' as the absolute description degree term of the label distribution. They incorporate this into the LDL process to form the improved paradigm of concentration distribution learning. A novel model combining probabilistic methods and neural networks is proposed to learn label distributions and background concentrations from existing LDL datasets.", "result": "Extensive experiments demonstrate that the proposed approach can successfully extract background concentrations from label distributions, resulting in more accurate prediction outcomes compared to state-of-the-art LDL methods.", "conclusion": "The introduction of background concentration significantly enhances the performance of label distribution learning by addressing its inherent limitations. The proposed concentration distribution learning paradigm and associated model provide a promising direction for future research."}}
{"id": "2505.22010", "pdf": "https://arxiv.org/pdf/2505.22010", "abs": "https://arxiv.org/abs/2505.22010", "authors": ["Nasir Hussain", "Haohan Chen", "Chanh Tran", "Philip Huang", "Zhuohao Li", "Pravir Chugh", "William Chen", "Ashish Kundu", "Yuan Tian"], "title": "VulBinLLM: LLM-powered Vulnerability Detection for Stripped Binaries", "categories": ["cs.CR"], "comment": null, "summary": "Recognizing vulnerabilities in stripped binary files presents a significant\nchallenge in software security. Although some progress has been made in\ngenerating human-readable information from decompiled binary files with Large\nLanguage Models (LLMs), effectively and scalably detecting vulnerabilities\nwithin these binary files is still an open problem. This paper explores the\nnovel application of LLMs to detect vulnerabilities within these binary files.\nWe demonstrate the feasibility of identifying vulnerable programs through a\ncombined approach of decompilation optimization to make the vulnerabilities\nmore prominent and long-term memory for a larger context window, achieving\nstate-of-the-art performance in binary vulnerability analysis. Our findings\nhighlight the potential for LLMs to overcome the limitations of traditional\nanalysis methods and advance the field of binary vulnerability detection,\npaving the way for more secure software systems. In this paper, we present\nVul-BinLLM , an LLM-based framework for binary vulnerability detection that\nmirrors traditional binary analysis workflows with fine-grained optimizations\nin decompilation and vulnerability reasoning with an extended context. In the\ndecompilation phase, Vul-BinLLM adds vulnerability and weakness comments\nwithout altering the code structure or functionality, providing more contextual\ninformation for vulnerability reasoning later. Then for vulnerability\nreasoning, Vul-BinLLM combines in-context learning and chain-of-thought\nprompting along with a memory management agent to enhance accuracy. Our\nevaluations encompass the commonly used synthetic dataset Juliet to evaluate\nthe potential feasibility for analysis and vulnerability detection in C/C++\nbinaries. Our evaluations show that Vul-BinLLM is highly effective in detecting\nvulnerabilities on the compiled Juliet dataset.", "AI": {"tldr": "This paper presents Vul-BinLLM, an LLM-based framework for detecting vulnerabilities in stripped binary files by combining decompilation optimization and long-term memory. It achieves state-of-the-art performance in binary vulnerability analysis.", "motivation": "Detecting vulnerabilities within stripped binary files is a significant challenge in software security. While some progress has been made with LLMs in generating human-readable information from decompiled binaries, effectively detecting vulnerabilities remains an open problem.", "method": "The Vul-BinLLM framework uses a combined approach of decompilation optimization to make vulnerabilities more prominent and long-term memory for a larger context window. In the decompilation phase, it adds vulnerability and weakness comments without altering the code structure or functionality. For vulnerability reasoning, it combines in-context learning, chain-of-thought prompting, and a memory management agent to enhance accuracy.", "result": "Evaluations on the commonly used synthetic dataset Juliet show that Vul-BinLLM is highly effective in detecting vulnerabilities in C/C++ binaries.", "conclusion": "Vul-BinLLM demonstrates the potential of LLMs to overcome limitations of traditional analysis methods and advance the field of binary vulnerability detection, leading to more secure software systems."}}
{"id": "2505.21988", "pdf": "https://arxiv.org/pdf/2505.21988", "abs": "https://arxiv.org/abs/2505.21988", "authors": ["Ziyang Zheng", "Kezhi Li", "Zhengyuan Shi", "Qiang Xu"], "title": "Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism", "categories": ["cs.AI"], "comment": null, "summary": "Subgraph matching in logic circuits is foundational for numerous Electronic\nDesign Automation (EDA) applications, including datapath optimization,\narithmetic verification, and hardware trojan detection. However, existing\ntechniques rely primarily on structural graph isomorphism and thus fail to\nidentify function-related subgraphs when synthesis transformations\nsubstantially alter circuit topology. To overcome this critical limitation, we\nintroduce the concept of functional subgraph matching, a novel approach that\nidentifies whether a given logic function is implicitly present within a larger\ncircuit, irrespective of structural variations induced by synthesis or\ntechnology mapping. Specifically, we propose a two-stage multi-modal framework:\n(1) learning robust functional embeddings across AIG and post-mapping netlists\nfor functional subgraph detection, and (2) identifying fuzzy boundaries using a\ngraph segmentation approach. Evaluations on standard benchmarks (ITC99,\nOpenABCD, ForgeEDA) demonstrate significant performance improvements over\nexisting structural methods, with average $93.8\\%$ accuracy in functional\nsubgraph detection and a dice score of $91.3\\%$ in fuzzy boundary\nidentification.", "AI": {"tldr": "The paper introduces functional subgraph matching for logic circuits, a two-stage framework that improves upon structural methods by focusing on function-related subgraphs, achieving high accuracy.", "motivation": "Existing subgraph matching techniques in logic circuits rely on structural graph isomorphism and cannot effectively identify function-related subgraphs when circuit topology changes due to synthesis transformations.", "method": "A two-stage multi-modal framework is proposed: (1) learning robust functional embeddings across AIG and post-mapping netlists for functional subgraph detection, and (2) identifying fuzzy boundaries using a graph segmentation approach.", "result": "Evaluations on standard benchmarks show significant performance improvements over existing structural methods, with 93.8% accuracy in functional subgraph detection and a dice score of 91.3% in fuzzy boundary identification.", "conclusion": "Functional subgraph matching offers a novel solution to the limitations of structural methods in logic circuits, providing robust detection of function-related subgraphs despite variations in circuit topology."}}
{"id": "2505.21584", "pdf": "https://arxiv.org/pdf/2505.21584", "abs": "https://arxiv.org/abs/2505.21584", "authors": ["Afaf Taik", "Khaoula Chehbouni", "Golnoosh Farnadi"], "title": "Fairness in Federated Learning: Fairness for Whom?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Fairness in federated learning has emerged as a rapidly growing area of\nresearch, with numerous works proposing formal definitions and algorithmic\ninterventions. Yet, despite this technical progress, fairness in FL is often\ndefined and evaluated in ways that abstract away from the sociotechnical\ncontexts in which these systems are deployed. In this paper, we argue that\nexisting approaches tend to optimize narrow system level metrics, such as\nperformance parity or contribution-based rewards, while overlooking how harms\narise throughout the FL lifecycle and how they impact diverse stakeholders. We\nsupport this claim through a critical analysis of the literature, based on a\nsystematic annotation of papers for their fairness definitions, design\ndecisions, evaluation practices, and motivating use cases. Our analysis reveals\nfive recurring pitfalls: 1) fairness framed solely through the lens of server\nclient architecture, 2) a mismatch between simulations and motivating use-cases\nand contexts, 3) definitions that conflate protecting the system with\nprotecting its users, 4) interventions that target isolated stages of the\nlifecycle while neglecting upstream and downstream effects, 5) and a lack of\nmulti-stakeholder alignment where multiple fairness definitions can be relevant\nat once. Building on these insights, we propose a harm centered framework that\nlinks fairness definitions to concrete risks and stakeholder vulnerabilities.\nWe conclude with recommendations for more holistic, context-aware, and\naccountable fairness research in FL.", "AI": {"tldr": "Fairness in federated learning (FL) research often focuses narrowly on system-level metrics, ignoring broader sociotechnical contexts and stakeholder impacts. This paper critiques existing approaches, identifies five key pitfalls, and proposes a harm-centered framework for more holistic fairness research.", "motivation": "The motivation of this paper is to address the limitations in current fairness approaches in federated learning that focus solely on narrow system-level metrics such as performance parity or contribution-based rewards, while ignoring the potential harms and diverse stakeholder impacts throughout the FL lifecycle.", "method": "The authors conducted a critical analysis of existing literature by systematically annotating papers based on their fairness definitions, design decisions, evaluation practices, and motivating use cases. They identified five recurring pitfalls in current fairness approaches in FL.", "result": "The analysis revealed five key pitfalls: fairness framed only through server-client architecture; mismatch between simulations and real-world use cases; conflating system protection with user protection; isolated interventions neglecting upstream/downstream effects; and lack of multi-stakeholder alignment. A harm-centered framework was proposed to link fairness definitions to concrete risks and vulnerabilities.", "conclusion": "The paper concludes with recommendations for future research in federated learning fairness, emphasizing the need for more holistic, context-aware, and accountable approaches."}}
{"id": "2505.22052", "pdf": "https://arxiv.org/pdf/2505.22052", "abs": "https://arxiv.org/abs/2505.22052", "authors": ["Keno Hassler", "Philipp G\u00f6rz", "Stephan Lipp", "Thorsten Holz", "Marcel B\u00f6hme"], "title": "A Comparative Study of Fuzzers and Static Analysis Tools for Finding Memory Unsafety in C and C++", "categories": ["cs.CR"], "comment": "34 pages, 10 figures", "summary": "Even today, over 70% of security vulnerabilities in critical software systems\nresult from memory safety violations. To address this challenge, fuzzing and\nstatic analysis are widely used automated methods to discover such\nvulnerabilities. Fuzzing generates random program inputs to identify faults,\nwhile static analysis examines source code to detect potential vulnerabilities.\nAlthough these techniques share a common goal, they take fundamentally\ndifferent approaches and have evolved largely independently.\n  In this paper, we present an empirical analysis of five static analyzers and\n13 fuzzers, applied to over 100 known security vulnerabilities in C/C++\nprograms. We measure the number of bug reports generated for each vulnerability\nto evaluate how the approaches differ and complement each other. Moreover, we\nrandomly sample eight bug-containing functions, manually analyze all bug\nreports therein, and quantify false-positive rates. We also assess limits to\nbug discovery, ease of use, resource requirements, and integration into the\ndevelopment process. We find that both techniques discover different types of\nbugs, but there are clear winners for each. Developers should consider these\ntools depending on their specific workflow and usability requirements. Based on\nour findings, we propose future directions to foster collaboration between\nthese research domains.", "AI": {"tldr": "Even today, over 70% of security vulnerabilities result from memory safety violations. This paper presents an empirical analysis of five static analyzers and 13 fuzzers applied to over 100 known security vulnerabilities in C/C++ programs.", "motivation": "To address the challenge of memory safety violations which cause over 70% of security vulnerabilities in critical software systems.", "method": "Empirical analysis of five static analyzers and 13 fuzzers applied to over 100 known security vulnerabilities in C/C++ programs. Measurement of bug reports generated for each vulnerability, manual analysis of sampled bug-containing functions, quantification of false-positive rates, and assessment of bug discovery limits, ease of use, resource requirements, and integration into development process.", "result": "Both techniques discover different types of bugs with clear winners for each. Static analysis and fuzzing have different strengths and limitations.", "conclusion": "Developers should choose tools based on specific workflow and usability requirements. Future research should foster collaboration between static analysis and fuzzing domains."}}
{"id": "2505.22006", "pdf": "https://arxiv.org/pdf/2505.22006", "abs": "https://arxiv.org/abs/2505.22006", "authors": ["Changze Qiao", "Mingming Lu"], "title": "Efficiently Enhancing General Agents With Hierarchical-categorical Memory", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "With large language models (LLMs) demonstrating remarkable capabilities,\nthere has been a surge in research on leveraging LLMs to build general-purpose\nmulti-modal agents. However, existing approaches either rely on computationally\nexpensive end-to-end training using large-scale multi-modal data or adopt\ntool-use methods that lack the ability to continuously learn and adapt to new\nenvironments. In this paper, we introduce EHC, a general agent capable of\nlearning without parameter updates. EHC consists of a Hierarchical Memory\nRetrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)\nmodule. The HMR module facilitates rapid retrieval of relevant memories and\ncontinuously stores new information without being constrained by memory\ncapacity. The TOEL module enhances the agent's comprehension of various task\ncharacteristics by classifying experiences and extracting patterns across\ndifferent categories. Extensive experiments conducted on multiple standard\ndatasets demonstrate that EHC outperforms existing methods, achieving\nstate-of-the-art performance and underscoring its effectiveness as a general\nagent for handling complex multi-modal tasks.", "AI": {"tldr": "The paper introduces EHC, a general agent capable of learning without parameter updates. It consists of HMR and TOEL modules, which facilitate memory retrieval and enhance task comprehension respectively. Experiments show that EHC outperforms existing methods.", "motivation": "Existing approaches to build multi-modal agents either rely on computationally expensive training or lack the ability to continuously learn and adapt.", "method": "EHC consists of a Hierarchical Memory Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL) module. HMR allows rapid memory retrieval and continuous storage without memory constraints, while TOEL classifies experiences and extracts patterns to understand task characteristics.", "result": "Extensive experiments on multiple standard datasets demonstrate that EHC achieves state-of-the-art performance in handling complex multi-modal tasks.", "conclusion": "EHC is an effective general agent for complex multi-modal tasks, capable of learning without parameter updates."}}
{"id": "2505.21587", "pdf": "https://arxiv.org/pdf/2505.21587", "abs": "https://arxiv.org/abs/2505.21587", "authors": ["Bin Qin", "Qirui Ji", "Jiangmeng Li", "Yupeng Wang", "Xuesong Wu", "Jianwen Cao", "Fanjiang Xu"], "title": "CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised topological deep learning (TDL) represents a nascent but\nunderexplored area with significant potential for modeling higher-order\ninteractions in simplicial complexes and cellular complexes to derive\nrepresentations of unlabeled graphs. Compared to simplicial complexes, cellular\ncomplexes exhibit greater expressive power. However, the advancement in\nself-supervised learning for cellular TDL is largely hindered by two core\nchallenges: \\textit{extrinsic structural constraints} inherent to cellular\ncomplexes, and intrinsic semantic redundancy in cellular representations. The\nfirst challenge highlights that traditional graph augmentation techniques may\ncompromise the integrity of higher-order cellular interactions, while the\nsecond underscores that topological redundancy in cellular complexes\npotentially diminish task-relevant information. To address these issues, we\nintroduce Cellular Complex Contrastive Learning with Adaptive Trimming\n(CellCLAT), a twofold framework designed to adhere to the combinatorial\nconstraints of cellular complexes while mitigating informational redundancy.\nSpecifically, we propose a parameter perturbation-based augmentation method\nthat injects controlled noise into cellular interactions without altering the\nunderlying cellular structures, thereby preserving cellular topology during\ncontrastive learning. Additionally, a cellular trimming scheduler is employed\nto mask gradient contributions from task-irrelevant cells through a bi-level\nmeta-learning approach, effectively removing redundant topological elements\nwhile maintaining critical higher-order semantics. We provide theoretical\njustification and empirical validation to demonstrate that CellCLAT achieves\nsubstantial improvements over existing self-supervised graph learning methods,\nmarking a significant attempt in this domain.", "AI": {"tldr": "Self-supervised topological deep learning (TDL) in cellular complexes faces challenges due to structural constraints and semantic redundancy. The paper introduces CellCLAT, a framework that uses parameter perturbation-based augmentation and cellular trimming scheduler to overcome these issues, leading to significant improvements over existing methods.", "motivation": "To model higher-order interactions in simplicial and cellular complexes for deriving representations of unlabeled graphs, overcoming the limitations posed by extrinsic structural constraints and intrinsic semantic redundancy in cellular representations.", "method": "Proposes Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT), which includes a parameter perturbation-based augmentation method to preserve cellular topology and a cellular trimming scheduler to remove redundant topological elements while maintaining critical semantics.", "result": "Empirical validation shows substantial improvements over existing self-supervised graph learning methods.", "conclusion": "CellCLAT addresses key challenges in self-supervised learning for cellular TDL, marking a significant advancement in this domain."}}
{"id": "2505.22162", "pdf": "https://arxiv.org/pdf/2505.22162", "abs": "https://arxiv.org/abs/2505.22162", "authors": ["Hongyu Jin", "Panos Papadimitratos"], "title": "Accountable, Scalable and DoS-resilient Secure Vehicular Communication", "categories": ["cs.CR"], "comment": null, "summary": "Paramount to vehicle safety, broadcasted Cooperative Awareness Messages\n(CAMs) and Decentralized Environmental Notification Messages (DENMs) are\npseudonymously authenticated for security and privacy protection, with each\nnode needing to have all incoming messages validated within an expiration\ndeadline. This creates an asymmetry that can be easily exploited by external\nadversaries to launch a clogging Denial of Service (DoS) attack: each forged VC\nmessage forces all neighboring nodes to cryptographically validate it; at\nincreasing rates, easy to generate forged messages gradually exhaust processing\nresources and severely degrade or deny timely validation of benign CAMs/DENMs.\nThe result can be catastrophic when awareness of neighbor vehicle positions or\ncritical reports are missed. We address this problem making the standardized VC\npseudonymous authentication DoS-resilient. We propose efficient cryptographic\nconstructs, which we term message verification facilitators, to prioritize\nprocessing resources for verification of potentially valid messages among bogus\nmessages and verify multiple messages based on one signature verification. Any\nmessage acceptance is strictly based on public-key based message\nauthentication/verification for accountability, i.e., non-repudiation is not\nsacrificed, unlike symmetric key based approaches. This further enables drastic\nmisbehavior detection, also exploiting the newly introduced facilitators, based\non probabilistic signature verification and cross-checking over multiple\nfacilitators verifying the same message; while maintaining verification latency\nlow even when under attack, trading off modest communication overhead. Our\nfacilitators can also be used for efficient discovery and verification of DENM\nor any event-driven message, including misbehavior evidence used for our\nscheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d88\u606f\u9a8c\u8bc1\u673a\u5236\uff0c\u901a\u8fc7\u4f18\u5148\u5904\u7406\u6709\u6548\u6d88\u606f\u3001\u6279\u91cf\u9a8c\u8bc1\u4ee5\u53ca\u884c\u4e3a\u68c0\u6d4b\u7b49\u624b\u6bb5\uff0c\u589e\u5f3a\u8f66\u8f86\u901a\u4fe1\u4e2d\u57fa\u4e8e\u6807\u51c6\u5316VC\u533f\u540d\u8ba4\u8bc1\u7684DoS\u653b\u51fb\u62b5\u5fa1\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u548c\u9002\u5ea6\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5e7f\u64ad\u7684CAM\u548cDENM\u6d88\u606f\u9700\u8981\u5728\u8fc7\u671f\u671f\u9650\u5185\u5b8c\u6210\u9a8c\u8bc1\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u4f2a\u9020\u6d88\u606f\u5f15\u53d1\u7684DoS\u653b\u51fb\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8d44\u6e90\u8017\u5c3d\uff0c\u65e0\u6cd5\u53ca\u65f6\u9a8c\u8bc1\u6b63\u5e38\u6d88\u606f\uff0c\u4ece\u800c\u53ef\u80fd\u9020\u6210\u707e\u96be\u6027\u540e\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u6d88\u606f\u9a8c\u8bc1\u8f85\u52a9\u6784\u9020\uff08message verification facilitators\uff09\uff0c\u7528\u4e8e\u4f18\u5148\u9a8c\u8bc1\u6f5c\u5728\u7684\u6709\u6548\u6d88\u606f\uff0c\u5e76\u5b9e\u73b0\u57fa\u4e8e\u5355\u4e00\u7b7e\u540d\u9a8c\u8bc1\u7684\u591a\u6d88\u606f\u9a8c\u8bc1\uff1b\u7ed3\u5408\u6982\u7387\u7b7e\u540d\u9a8c\u8bc1\u548c\u8de8\u591a\u4e2a\u8f85\u52a9\u6784\u9020\u7684\u4ea4\u53c9\u68c0\u67e5\u8fdb\u884c\u884c\u4e3a\u68c0\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u9a8c\u8bc1\u5ef6\u8fdf\u5e76\u5f15\u5165\u9002\u5ea6\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u6240\u63d0\u51fa\u7684\u673a\u5236\u80fd\u591f\u5728\u906d\u53d7\u653b\u51fb\u65f6\u7ef4\u6301\u8f83\u4f4e\u7684\u9a8c\u8bc1\u5ef6\u8fdf\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u4e8b\u4ef6\u9a71\u52a8\u6d88\u606f\u53d1\u73b0\u4e0e\u9a8c\u8bc1\uff0c\u5305\u62ec\u7528\u4e8e\u65b9\u6848\u4e2d\u7684\u4e0d\u5f53\u884c\u4e3a\u8bc1\u636e\u7684\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u4e0d\u53ef\u5426\u8ba4\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u589e\u5f3a\u4e86\u6807\u51c6\u5316VC\u533f\u540d\u8ba4\u8bc1\u5bf9DoS\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\uff0c\u4e3a\u8f66\u8f86\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4fdd\u969c\u3002"}}
{"id": "2505.22050", "pdf": "https://arxiv.org/pdf/2505.22050", "abs": "https://arxiv.org/abs/2505.22050", "authors": ["Di Wu", "Jiaxin Fan", "Junzhe Zang", "Guanbo Wang", "Wei Yin", "Wenhao Li", "Bo Jin"], "title": "Reinforced Reasoning for Embodied Planning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Embodied planning requires agents to make coherent multi-step decisions based\non dynamic visual observations and natural language goals. While recent\nvision-language models (VLMs) excel at static perception tasks, they struggle\nwith the temporal reasoning, spatial understanding, and commonsense grounding\nneeded for planning in interactive environments. In this work, we introduce a\nreinforcement fine-tuning framework that brings R1-style reasoning enhancement\ninto embodied planning. We first distill a high-quality dataset from a powerful\nclosed-source model and perform supervised fine-tuning (SFT) to equip the model\nwith structured decision-making priors. We then design a rule-based reward\nfunction tailored to multi-step action quality and optimize the policy via\nGeneralized Reinforced Preference Optimization (GRPO). Our approach is\nevaluated on Embench, a recent benchmark for interactive embodied tasks,\ncovering both in-domain and out-of-domain scenarios. Experimental results show\nthat our method significantly outperforms models of similar or larger scale,\nincluding GPT-4o-mini and 70B+ open-source baselines, and exhibits strong\ngeneralization to unseen environments. This work highlights the potential of\nreinforcement-driven reasoning to advance long-horizon planning in embodied AI.", "AI": {"tldr": "This paper presents a reinforcement fine-tuning framework for embodied planning that significantly outperforms existing models and generalizes well to unseen environments.", "motivation": "Embodied planning requires coherent multi-step decisions based on dynamic visual observations and natural language goals, but current vision-language models struggle with temporal reasoning, spatial understanding, and commonsense grounding in interactive environments.", "method": "The authors distill a high-quality dataset from a closed-source model and use supervised fine-tuning to equip the model with structured decision-making priors. They then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO).", "result": "The approach outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, on the Embench benchmark covering both in-domain and out-of-domain scenarios. It also exhibits strong generalization to unseen environments.", "conclusion": "The work demonstrates the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI."}}
{"id": "2505.21591", "pdf": "https://arxiv.org/pdf/2505.21591", "abs": "https://arxiv.org/abs/2505.21591", "authors": ["Maosen Zhao", "Pengtao Chen", "Chong Yu", "Yan Wen", "Xudong Tan", "Tao Chen"], "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model quantization reduces the bit-width of weights and activations,\nimproving memory efficiency and inference speed in diffusion models. However,\nachieving 4-bit quantization remains challenging. Existing methods, primarily\nbased on integer quantization and post-training quantization fine-tuning,\nstruggle with inconsistent performance. Inspired by the success of\nfloating-point (FP) quantization in large language models, we explore low-bit\nFP quantization for diffusion models and identify key challenges: the failure\nof signed FP quantization to handle asymmetric activation distributions, the\ninsufficient consideration of temporal complexity in the denoising process\nduring fine-tuning, and the misalignment between fine-tuning loss and\nquantization error. To address these challenges, we propose the mixup-sign\nfloating-point quantization (MSFP) framework, first introducing unsigned FP\nquantization in model quantization, along with timestep-aware LoRA (TALoRA) and\ndenoising-factor loss alignment (DFA), which ensure precise and stable\nfine-tuning. Extensive experiments show that we are the first to achieve\nsuperior performance in 4-bit FP quantization for diffusion models,\noutperforming existing PTQ fine-tuning methods in 4-bit INT quantization.", "AI": {"tldr": "Model quantization is important for improving memory efficiency and inference speed in diffusion models. However, 4-bit quantization remains challenging. This paper explores low-bit floating-point (FP) quantization for diffusion models and proposes the MSFP framework to address key challenges.", "motivation": "Existing methods primarily based on integer quantization and post-training quantization fine-tuning struggle with inconsistent performance when achieving 4-bit quantization.", "method": "The paper proposes the mixup-sign floating-point quantization (MSFP) framework which includes unsigned FP quantization, timestep-aware LoRA (TALoRA), and denoising-factor loss alignment (DFA).", "result": "Extensive experiments demonstrate that the proposed method achieves superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.", "conclusion": "The MSFP framework successfully addresses the challenges of low-bit FP quantization in diffusion models, leading to improved performance in 4-bit quantization."}}
{"id": "2505.22220", "pdf": "https://arxiv.org/pdf/2505.22220", "abs": "https://arxiv.org/abs/2505.22220", "authors": ["Denis Petrov", "Pascal Ruffing", "Sebastian Zillien", "Steffen Wendzel"], "title": "Domainator: Detecting and Identifying DNS-Tunneling Malware Using Metadata Sequences", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "In recent years, malware with tunneling (or: covert channel) capabilities is\non the rise. While malware research led to several methods and innovations, the\ndetection and differentiation of malware solely based on its DNS tunneling\nfeatures is still in its infancy. Moreover, no work so far has used the DNS\ntunneling traffic to gain knowledge over the current actions taken by the\nmalware. In this paper, we present Domainator, an approach to detect and\ndifferentiate state-of-the-art malware and DNS tunneling tools without relying\non trivial (but quickly altered) features such as \"magic bytes\" that are\nembedded into subdomains. Instead, we apply an analysis of sequential patterns\nto identify specific types of malware. We evaluate our approach with 7\ndifferent malware samples and tunneling tools and can identify the particular\nmalware based on its DNS traffic. We further infer the rough behavior of the\nparticular malware through its DNS tunneling artifacts. Finally, we compare our\nDomainator with related methods.", "AI": {"tldr": "This paper presents Domainator, an approach to detect and differentiate state-of-the-art malware and DNS tunneling tools by analyzing sequential patterns in DNS traffic without relying on trivial features. Evaluated with 7 samples, it can identify specific malware and infer their behavior.", "motivation": "The motivation of this paper is to address the challenges in detecting and differentiating malware based on its DNS tunneling features, as well as gaining knowledge about the current actions taken by the malware through its DNS tunneling traffic.", "method": "The method used in this paper is to apply an analysis of sequential patterns to identify specific types of malware, rather than relying on trivial features such as 'magic bytes' embedded into subdomains.", "result": "The result is that they can identify particular malware based on its DNS traffic and infer the rough behavior of the malware through its DNS tunneling artifacts.", "conclusion": "In conclusion, the paper presents Domainator, a novel approach for detecting and differentiating malware and DNS tunneling tools, and compares it with related methods."}}
{"id": "2505.22087", "pdf": "https://arxiv.org/pdf/2505.22087", "abs": "https://arxiv.org/abs/2505.22087", "authors": ["Ruxiao Chen", "Dezheng Han", "Wenjie Han", "Shuaishuai Guo"], "title": "Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired", "categories": ["cs.AI"], "comment": null, "summary": "Assistive systems for visually impaired individuals must deliver rapid,\ninterpretable, and adaptive feedback to facilitate real-time navigation.\nCurrent approaches face a trade-off between latency and semantic richness:\nnatural language-based systems provide detailed guidance but are too slow for\ndynamic scenarios, while emergent communication frameworks offer low-latency\nsymbolic languages but lack semantic depth, limiting their utility in tactile\nmodalities like vibration. To address these limitations, we introduce a novel\nframework, Cognitively-Inspired Emergent Communication via Knowledge Graphs\n(VAG-EC), which emulates human visual perception and cognitive mapping. Our\nmethod constructs knowledge graphs to represent objects and their\nrelationships, incorporating attention mechanisms to prioritize task-relevant\nentities, thereby mirroring human selective attention. This structured approach\nenables the emergence of compact, interpretable, and context-sensitive symbolic\nlanguages. Extensive experiments across varying vocabulary sizes and message\nlengths demonstrate that VAG-EC outperforms traditional emergent communication\nmethods in Topographic Similarity (TopSim) and Context Independence (CI). These\nfindings underscore the potential of cognitively grounded emergent\ncommunication as a fast, adaptive, and human-aligned solution for real-time\nassistive technologies. Code is available at\nhttps://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.", "AI": {"tldr": "\u4e3a\u4e86\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u5b9e\u65f6\u5bfc\u822a\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6VAG-EC\uff0c\u5b83\u7ed3\u5408\u4e86\u77e5\u8bc6\u56fe\u8c31\u548c\u6ce8\u610f\u673a\u5236\uff0c\u751f\u6210\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u4e0e\u60c5\u5883\u76f8\u5173\u7684\u7b26\u53f7\u8bed\u8a00\uff0c\u5b9e\u73b0\u5728\u4e0d\u540c\u8bcd\u6c47\u91cf\u548c\u6d88\u606f\u957f\u5ea6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8f85\u52a9\u7cfb\u7edf\u5728\u5ef6\u8fdf\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u7cfb\u7edf\u867d\u63d0\u4f9b\u8be6\u7ec6\u6307\u5bfc\u4f46\u901f\u5ea6\u6162\uff0c\u800c\u65b0\u5174\u901a\u4fe1\u6846\u67b6\u867d\u4f4e\u5ef6\u8fdf\u4f46\u7f3a\u4e4f\u8bed\u4e49\u6df1\u5ea6\u3002", "method": "\u5f15\u5165Cognitively-Inspired Emergent Communication via Knowledge Graphs (VAG-EC) \u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u6765\u8868\u793a\u5bf9\u8c61\u53ca\u5176\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5148\u5904\u7406\u4efb\u52a1\u76f8\u5173\u5b9e\u4f53\uff0c\u4ece\u800c\u6a21\u4eff\u4eba\u7c7b\u7684\u9009\u62e9\u6027\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u7684\u8bcd\u6c47\u91cf\u548c\u6d88\u606f\u957f\u5ea6\u4e0b\uff0cVAG-EC\u5728Topographic Similarity (TopSim) \u548c Context Independence (CI)\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u65b0\u5174\u901a\u4fe1\u65b9\u6cd5\u3002", "conclusion": "\u8ba4\u77e5\u57fa\u7840\u7684\u65b0\u5174\u901a\u4fe1\u5177\u6709\u4f5c\u4e3a\u5feb\u901f\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u4e0e\u4eba\u7c7b\u4e00\u81f4\u7684\u5b9e\u65f6\u8f85\u52a9\u6280\u672f\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.21595", "pdf": "https://arxiv.org/pdf/2505.21595", "abs": "https://arxiv.org/abs/2505.21595", "authors": ["Shreyas Gururaj", "Lars Gr\u00fcne", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "title": "Relevance-driven Input Dropout: an Explanation-guided Regularization Technique", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": null, "summary": "Overfitting is a well-known issue extending even to state-of-the-art (SOTA)\nMachine Learning (ML) models, resulting in reduced generalization, and a\nsignificant train-test performance gap. Mitigation measures include a\ncombination of dropout, data augmentation, weight decay, and other\nregularization techniques. Among the various data augmentation strategies,\nocclusion is a prominent technique that typically focuses on randomly masking\nregions of the input during training. Most of the existing literature\nemphasizes randomness in selecting and modifying the input features instead of\nregions that strongly influence model decisions. We propose Relevance-driven\nInput Dropout (RelDrop), a novel data augmentation method which selectively\noccludes the most relevant regions of the input, nudging the model to use other\nimportant features in the prediction process, thus improving model\ngeneralization through informed regularization. We further conduct qualitative\nand quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)\naffects model decision-making. Through a series of experiments on benchmark\ndatasets, we demonstrate that our approach improves robustness towards\nocclusion, results in models utilizing more features within the region of\ninterest, and boosts inference time generalization performance. Our code is\navailable at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.", "AI": {"tldr": "The paper proposes Relevance-driven Input Dropout (RelDrop), a new data augmentation method that selectively occludes the most relevant input regions to improve model generalization.", "motivation": "Overfitting is a common issue in machine learning models, leading to poor generalization and a significant train-test performance gap. Existing data augmentation strategies often use randomness instead of focusing on regions that strongly influence model decisions.", "method": "The proposed method, Relevance-driven Input Dropout (RelDrop), selectively occludes the most relevant regions of the input during training, encouraging the model to utilize other important features for prediction and thereby improving generalization through informed regularization.", "result": "Through experiments on benchmark datasets, RelDrop was shown to improve robustness towards occlusion, result in models utilizing more features within the region of interest, and enhance inference time generalization performance.", "conclusion": "RelDrop is an effective data augmentation technique that improves model generalization by selectively occluding relevant input regions, as evidenced by qualitative and quantitative analyses."}}
{"id": "2505.22271", "pdf": "https://arxiv.org/pdf/2505.22271", "abs": "https://arxiv.org/abs/2505.22271", "authors": ["Yongcan Yu", "Yanbo Wang", "Ran He", "Jian Liang"], "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM.", "AI": {"tldr": "A universal defense framework called Test-time IMmunization (TIM) is proposed to defend against various jailbreak attacks on large language models.", "motivation": "Large language models are vulnerable to jailbreak attacks and current defense methods are often tailored to specific types of attacks, limiting their effectiveness against diverse adversarial strategies.", "method": "The TIM framework initially trains a gist token for efficient detection of jailbreak activities. When jailbreak attempts are identified, safety fine-tuning is implemented using the detected jailbreak instructions paired with refusal answers. To mitigate potential performance degradation in the detector, the fine-tuning process is decoupled from the detection module.", "result": "Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy of TIM.", "conclusion": "TIM can adaptively defend against various jailbreak attacks in a self-evolving way."}}
{"id": "2505.22092", "pdf": "https://arxiv.org/pdf/2505.22092", "abs": "https://arxiv.org/abs/2505.22092", "authors": ["Valentin Cuzin-Rambaud", "Emilien Komlenovic", "Alexandre Faure", "Bruno Yun"], "title": "VIRAL: Vision-grounded Integration for Reward design And Learning", "categories": ["cs.AI"], "comment": null, "summary": "The alignment between humans and machines is a critical challenge in\nartificial intelligence today. Reinforcement learning, which aims to maximize a\nreward function, is particularly vulnerable to the risks associated with poorly\ndesigned reward functions. Recent advancements has shown that Large Language\nModels (LLMs) for reward generation can outperform human performance in this\ncontext. We introduce VIRAL, a pipeline for generating and refining reward\nfunctions through the use of multi-modal LLMs. VIRAL autonomously creates and\ninteractively improves reward functions based on a given environment and a goal\nprompt or annotated image. The refinement process can incorporate human\nfeedback or be guided by a description generated by a video LLM, which explains\nthe agent's policy in video form. We evaluated VIRAL in five Gymnasium\nenvironments, demonstrating that it accelerates the learning of new behaviors\nwhile ensuring improved alignment with user intent. The source-code and demo\nvideo are available at: https://github.com/VIRAL-UCBL1/VIRAL and\nhttps://youtu.be/t4_BXugBm9Q.", "AI": {"tldr": "The paper presents VIRAL, a pipeline using multi-modal LLMs to generate and refine reward functions in reinforcement learning, improving alignment between human goals and machine behaviors.", "motivation": "Reinforcement learning systems are susceptible to risks from poorly designed reward functions, creating a need for more effective and aligned reward generation methods.", "method": "VIRAL autonomously creates initial reward functions and refines them interactively. Refinement can be guided by human feedback or video LLM descriptions explaining the agent's policy.", "result": "VIRAL was evaluated in five Gymnasium environments and showed accelerated learning of new behaviors with better alignment to user intent compared to existing methods.", "conclusion": "VIRAL demonstrates potential in enhancing the alignment between human goals and machine behaviors in reinforcement learning through the use of multi-modal LLMs."}}
{"id": "2505.21605", "pdf": "https://arxiv.org/pdf/2505.21605", "abs": "https://arxiv.org/abs/2505.21605", "authors": ["Fengqing Jiang", "Fengbo Ma", "Zhangchen Xu", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bo Li", "Xianyan Chen", "Zhen Xiang", "Radha Poovendran"], "title": "SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) exhibit advancing capabilities in complex tasks,\nsuch as reasoning and graduate-level question answering, yet their resilience\nagainst misuse, particularly involving scientifically sophisticated risks,\nremains underexplored. Existing safety benchmarks typically focus either on\ninstructions requiring minimal knowledge comprehension (e.g., ``tell me how to\nbuild a bomb\") or utilize prompts that are relatively low-risk (e.g.,\nmultiple-choice or classification tasks about hazardous content). Consequently,\nthey fail to adequately assess model safety when handling knowledge-intensive,\nhazardous scenarios.\n  To address this critical gap, we introduce SOSBench, a regulation-grounded,\nhazard-focused benchmark encompassing six high-risk scientific domains:\nchemistry, biology, medicine, pharmacology, physics, and psychology. The\nbenchmark comprises 3,000 prompts derived from real-world regulations and laws,\nsystematically expanded via an LLM-assisted evolutionary pipeline that\nintroduces diverse, realistic misuse scenarios (e.g., detailed explosive\nsynthesis instructions involving advanced chemical formulas). We evaluate\nfrontier models within a unified evaluation framework using our SOSBench.\nDespite their alignment claims, advanced models consistently disclose\npolicy-violating content across all domains, demonstrating alarmingly high\nrates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).\nThese results highlight significant safety alignment deficiencies and\nunderscore urgent concerns regarding the responsible deployment of powerful\nLLMs.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u6d89\u53ca\u9ad8\u98ce\u9669\u79d1\u5b66\u9886\u57df\u7684\u5b89\u5168\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002\u672c\u6587\u63d0\u51faSOSBench\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6cd5\u89c4\u3001\u5173\u6ce8\u5371\u9669\u60c5\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLMs\u5728\u5904\u7406\u9ad8\u98ce\u9669\u79d1\u5b66\u9886\u57df\u65f6\u7684\u5b89\u5168\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u7ecf\u5e38\u4ea7\u751f\u8fdd\u53cd\u653f\u7b56\u7684\u5185\u5bb9\uff0c\u8868\u660e\u5176\u5b89\u5168\u5bf9\u9f50\u5b58\u5728\u91cd\u5927\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u7684\u5b89\u5168\u6027\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u578b\u3001\u9ad8\u98ce\u9669\u573a\u666f\u65f6\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u79d1\u5b66\u9886\u57df\u7684\u6f5c\u5728\u6ee5\u7528\u60c5\u51b5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSOSBench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u516d\u4e2a\u9ad8\u98ce\u9669\u79d1\u5b66\u9886\u57df\uff0c\u5e76\u4f7f\u75283000\u4e2a\u6e90\u81ea\u771f\u5b9e\u6cd5\u89c4\u548c\u6cd5\u5f8b\u7684\u63d0\u793a\uff0c\u901a\u8fc7LLM\u8f85\u52a9\u7684\u8fdb\u5316\u7ba1\u9053\u7cfb\u7edf\u6269\u5c55\uff0c\u4ee5\u5f15\u5165\u591a\u6837\u4e14\u771f\u5b9e\u7684\u8bef\u7528\u60c5\u666f\u3002\u7136\u540e\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u4e2d\u4f7f\u7528SOSBench\u5bf9\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u5728\u6240\u6709\u9886\u57df\u4e2d\u4e00\u81f4\u62ab\u9732\u8fdd\u53cd\u653f\u7b56\u7684\u5185\u5bb9\uff0c\u4f8b\u5982Deepseek-R1\u670979.1%\u7684\u6709\u5bb3\u54cd\u5e94\u7387\uff0cGPT-4.1\u4e3a47.3%\uff0c\u663e\u793a\u51fa\u4ee4\u4eba\u62c5\u5fe7\u7684\u9ad8\u6709\u5bb3\u54cd\u5e94\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u4e8e\u5f3a\u5927LLMs\u8d1f\u8d23\u4efb\u90e8\u7f72\u7684\u7d27\u8feb\u95ee\u9898\u3002"}}
{"id": "2505.22435", "pdf": "https://arxiv.org/pdf/2505.22435", "abs": "https://arxiv.org/abs/2505.22435", "authors": ["Victor J\u00fcttner", "Erik Buchmann"], "title": "Does Johnny Get the Message? Evaluating Cybersecurity Notifications for Everyday Users", "categories": ["cs.CR"], "comment": null, "summary": "Due to the increasing presence of networked devices in everyday life, not\nonly cybersecurity specialists but also end users benefit from security\napplications such as firewalls, vulnerability scanners, and intrusion detection\nsystems. Recent approaches use large language models (LLMs) to rewrite brief,\ntechnical security alerts into intuitive language and suggest actionable\nmeasures, helping everyday users understand and respond appropriately to\nsecurity risks. However, it remains an open question how well such alerts are\nexplained to users. LLM outputs can also be hallucinated, inconsistent, or\nmisleading. In this work, we introduce the Human-Centered Security Alert\nEvaluation Framework (HCSAEF). HCSAEF assesses LLM-generated cybersecurity\nnotifications to support researchers who want to compare notifications\ngenerated for everyday users, improve them, or analyze the capabilities of\ndifferent LLMs in explaining cybersecurity issues. We demonstrate HCSAEF\nthrough three use cases, which allow us to quantify the impact of prompt\ndesign, model selection, and output consistency. Our findings indicate that\nHCSAEF effectively differentiates generated notifications along dimensions such\nas intuitiveness, urgency, and correctness.", "AI": {"tldr": "The paper introduces HCSAEF, a framework for evaluating LLM-generated cybersecurity alerts aimed at end users. It assesses the intuitiveness, urgency, and correctness of these notifications through three use cases.", "motivation": "To address the challenge of ensuring that security alerts generated by LLMs are effectively communicated to everyday users in an intuitive and accurate manner.", "method": "Development of HCSAEF, which evaluates LLM-generated cybersecurity notifications across dimensions like intuitiveness, urgency, and correctness. Demonstrated through three use cases focusing on prompt design, model selection, and output consistency.", "result": "HCSAEF successfully differentiates and quantifies the quality of generated notifications, providing insights into improving LLM-generated alerts.", "conclusion": "HCSAEF is an effective tool for researchers to compare, improve, and analyze LLM-generated cybersecurity notifications for end users."}}
{"id": "2505.22104", "pdf": "https://arxiv.org/pdf/2505.22104", "abs": "https://arxiv.org/abs/2505.22104", "authors": ["Davide Corsi", "Kaushik Mallik", "Andoni Rodriguez", "Cesar Sanchez"], "title": "Efficient Dynamic Shielding for Parametric Safety Specifications", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Shielding has emerged as a promising approach for ensuring safety of\nAI-controlled autonomous systems. The algorithmic goal is to compute a shield,\nwhich is a runtime safety enforcement tool that needs to monitor and intervene\nthe AI controller's actions if safety could be compromised otherwise.\nTraditional shields are designed statically for a specific safety requirement.\nTherefore, if the safety requirement changes at runtime due to changing\noperating conditions, the shield needs to be recomputed from scratch, causing\ndelays that could be fatal. We introduce dynamic shields for parametric safety\nspecifications, which are succinctly represented sets of all possible safety\nspecifications that may be encountered at runtime. Our dynamic shields are\nstatically designed for a given safety parameter set, and are able to\ndynamically adapt as the true safety specification (permissible by the\nparameters) is revealed at runtime. The main algorithmic novelty lies in the\ndynamic adaptation procedure, which is a simple and fast algorithm that\nutilizes known features of standard safety shields, like maximal\npermissiveness. We report experimental results for a robot navigation problem\nin unknown territories, where the safety specification evolves as new obstacles\nare discovered at runtime. In our experiments, the dynamic shields took a few\nminutes for their offline design, and took between a fraction of a second and a\nfew seconds for online adaptation at each step, whereas the brute-force online\nrecomputation approach was up to 5 times slower.", "AI": {"tldr": "The paper presents dynamic shields for parametric safety specifications in AI-controlled autonomous systems, which can adapt at runtime as the safety specification changes, avoiding delays of recomputation from scratch. Experimental results show that dynamic shields are more efficient than brute-force online recomputation.", "motivation": "Traditional shields are designed statically for a specific safety requirement. If the safety requirement changes at runtime due to changing operating conditions, the shield needs to be recomputed from scratch, causing delays that could be fatal.", "method": "Dynamic shields for parametric safety specifications are introduced. They are succinctly represented sets of all possible safety specifications that may be encountered at runtime. The dynamic shields are statically designed for a given safety parameter set and able to dynamically adapt as the true safety specification is revealed at runtime.", "result": "Experimental results for a robot navigation problem in unknown territories showed that dynamic shields took a few minutes for their offline design and between a fraction of a second and a few seconds for online adaptation at each step, whereas the brute-force online recomputation approach was up to 5 times slower.", "conclusion": "Dynamic shields offer an efficient solution for ensuring safety in AI-controlled autonomous systems with changing safety requirements."}}
{"id": "2505.21626", "pdf": "https://arxiv.org/pdf/2505.21626", "abs": "https://arxiv.org/abs/2505.21626", "authors": ["Nicolas Guerra", "Nicholas H. Nelsen", "Yunan Yang"], "title": "Learning Where to Learn: Training Distribution Selection for Provable OOD Performance", "categories": ["cs.LG", "math.OC", "stat.ML", "62K05, 65K10 (Primary) 68T07, 65D15, 62R20, 60G57 (Secondary)"], "comment": "32 pages, 8 figures, 2 tables, 3 algorithms", "summary": "Out-of-distribution (OOD) generalization remains a fundamental challenge in\nmachine learning. Models trained on one data distribution often experience\nsubstantial performance degradation when evaluated on shifted or unseen\ndomains. To address this challenge, the present paper studies the design of\ntraining data distributions that maximize average-case OOD performance. First,\na theoretical analysis establishes a family of generalization bounds that\nquantify how the choice of training distribution influences OOD error across a\npredefined family of target distributions. These insights motivate the\nintroduction of two complementary algorithmic strategies: (i) directly\nformulating OOD risk minimization as a bilevel optimization problem over the\nspace of probability measures and (ii) minimizing a theoretical upper bound on\nOOD error. Last, the paper evaluates the two approaches across a range of\nfunction approximation and operator learning examples. The proposed methods\nsignificantly improve OOD accuracy over standard empirical risk minimization\nwith a fixed distribution. These results highlight the potential of\ndistribution-aware training as a principled and practical framework for robust\nOOD generalization.", "AI": {"tldr": "The paper investigates training data distribution designs to maximize OOD performance, proposing two algorithmic strategies that significantly improve OOD accuracy.", "motivation": "Out-of-distribution generalization is a significant challenge in machine learning where models trained on one data distribution suffer from performance degradation when evaluated on different domains.", "method": "Theoretical analysis provides generalization bounds showing the impact of training distribution choice on OOD error. Two algorithmic strategies are introduced: (i) bilevel optimization over probability measures and (ii) minimizing an upper bound on OOD error.", "result": "The proposed methods show significant improvement in OOD accuracy compared to standard empirical risk minimization with a fixed distribution.", "conclusion": "Distribution-aware training presents a promising framework for robust out-of-distribution generalization."}}
{"id": "2505.22447", "pdf": "https://arxiv.org/pdf/2505.22447", "abs": "https://arxiv.org/abs/2505.22447", "authors": ["Sizai Hou", "Songze Li", "Baturalp Buyukates"], "title": "Privacy-preserving Prompt Personalization in Federated Learning for Multimodal Large Language Models", "categories": ["cs.CR"], "comment": "Under Review", "summary": "Prompt learning is a crucial technique for adapting pre-trained multimodal\nlanguage models (MLLMs) to user tasks. Federated prompt personalization (FPP)\nis further developed to address data heterogeneity and local overfitting,\nhowever, it exposes personalized prompts - valuable intellectual assets - to\nprivacy risks like prompt stealing or membership inference attacks.\nWidely-adopted techniques like differential privacy add noise to prompts,\nwhereas degrading personalization performance. We propose SecFPP, a secure FPP\nprotocol harmonizing generalization, personalization, and privacy guarantees.\nSecFPP employs hierarchical prompt adaptation with domain-level and class-level\ncomponents to handle multi-granular data imbalance. For privacy, it uses a\nnovel secret-sharing-based adaptive clustering algorithm for domain-level\nadaptation while keeping class-level components private. While theoretically\nand empirically secure, SecFPP achieves state-of-the-art accuracy under severe\nheterogeneity in data distribution. Extensive experiments show it significantly\noutperforms both non-private and privacy-preserving baselines, offering a\nsuperior privacy-performance trade-off.", "AI": {"tldr": "An abstract about SecFPP, a secure federated prompt personalization protocol that balances generalization, personalization, and privacy.", "motivation": "Prompt learning is essential for adapting pre-trained multimodal language models to user tasks. However, existing methods like Federated prompt personalization (FPP) expose personalized prompts to privacy risks such as prompt stealing or membership inference attacks while techniques like differential privacy degrade personalization performance.", "method": "The paper proposes SecFPP which employs hierarchical prompt adaptation with domain-level and class-level components to handle multi-granular data imbalance. For privacy, it uses a novel secret-sharing-based adaptive clustering algorithm for domain-level adaptation while keeping class-level components private.", "result": "SecFPP achieves state-of-the-art accuracy under severe heterogeneity in data distribution. Extensive experiments show it significantly outperforms both non-private and privacy-preserving baselines, offering a superior privacy-performance trade-off.", "conclusion": "SecFPP is a secure FPP protocol that harmonizes generalization, personalization, and privacy guarantees."}}
{"id": "2505.22112", "pdf": "https://arxiv.org/pdf/2505.22112", "abs": "https://arxiv.org/abs/2505.22112", "authors": ["Guangfu Hao", "Frederic Alexandre", "Shan Yu"], "title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test", "categories": ["cs.AI", "q-bio.NC"], "comment": null, "summary": "Cognitive flexibility has been extensively studied in human cognition but\nremains relatively unexplored in the context of Visual Large Language Models\n(VLLMs). This study assesses the cognitive flexibility of state-of-the-art\nVLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card\nSorting Test (WCST), a classic measure of set-shifting ability. Our results\nreveal that VLLMs achieve or surpass human-level set-shifting capabilities\nunder chain-of-thought prompting with text-based inputs. However, their\nabilities are highly influenced by both input modality and prompting strategy.\nIn addition, we find that through role-playing, VLLMs can simulate various\nfunctional deficits aligned with patients having impairments in cognitive\nflexibility, suggesting that VLLMs may possess a cognitive architecture, at\nleast regarding the ability of set-shifting, similar to the brain. This study\nreveals the fact that VLLMs have already approached the human level on a key\ncomponent underlying our higher cognition, and highlights the potential to use\nthem to emulate complex brain processes.", "AI": {"tldr": "Cognitive flexibility in Visual Large Language Models (VLLMs) is assessed using the Wisconsin Card Sorting Test (WCST). VLLMs achieve human-level set-shifting under chain-of-thought prompting with text-based inputs, but their abilities are influenced by input modality and prompting strategy. Role-playing allows VLLMs to simulate functional deficits related to cognitive flexibility.", "motivation": "To evaluate the cognitive flexibility of state-of-the-art VLLMs using WCST and explore whether these models possess a cognitive architecture similar to the brain.", "method": "Assessing the performance of VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) on WCST under different prompting strategies and input modalities, and using role-playing to simulate functional deficits.", "result": "VLLMs achieve or surpass human-level set-shifting capabilities under certain conditions, and can simulate cognitive impairments through role-playing.", "conclusion": "VLLMs have approached the human level in cognitive flexibility and may possess a cognitive architecture similar to the brain, offering potential for emulating complex brain processes."}}
{"id": "2505.21639", "pdf": "https://arxiv.org/pdf/2505.21639", "abs": "https://arxiv.org/abs/2505.21639", "authors": ["Mauricio Junca", "Esteban Leiva"], "title": "Apprenticeship learning with prior beliefs using inverse optimization", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "The relationship between inverse reinforcement learning (IRL) and inverse\noptimization (IO) for Markov decision processes (MDPs) has been relatively\nunderexplored in the literature, despite addressing the same problem. In this\nwork, we revisit the relationship between the IO framework for MDPs, IRL, and\napprenticeship learning (AL). We incorporate prior beliefs on the structure of\nthe cost function into the IRL and AL problems, and demonstrate that the\nconvex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a\nrelaxation of our framework. Notably, the AL formalism is a special case in our\nframework when the regularization term is absent. Focusing on the suboptimal\nexpert setting, we formulate the AL problem as a regularized min-max problem.\nThe regularizer plays a key role in addressing the ill-posedness of IRL by\nguiding the search for plausible cost functions. To solve the resulting\nregularized-convex-concave-min-max problem, we use stochastic mirror descent\n(SMD) and establish convergence bounds for the proposed method. Numerical\nexperiments highlight the critical role of regularization in learning cost\nvectors and apprentice policies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86MDP\u4e2d\u7684IO\u6846\u67b6\u3001IRL\u548c\u5b66\u5f92\u5236\u5b66\u4e60\uff08AL\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u52a0\u5165\u5148\u9a8c\u4fe1\u5ff5\u548c\u6b63\u5219\u5316\u9879\u6539\u8fdb\u4e86AL\u95ee\u9898\u7684\u51f8\u5206\u6790\u89c6\u89d2\u3002\u5b9e\u9a8c\u8868\u660e\u6b63\u5219\u5316\u5728\u5b66\u4e60\u6210\u672c\u5411\u91cf\u548c\u5b66\u5f92\u7b56\u7565\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1IRL\u548cIO\u90fd\u9488\u5bf9\u76f8\u540c\u7684\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u6df1\u5165\u63a2\u8ba8\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684\u65b9\u6cd5\u6765\u89e3\u51b3IRL\u4e2d\u7684\u75c5\u6001\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5c06\u5148\u9a8c\u4fe1\u5ff5\u5f15\u5165IRL\u548cAL\u95ee\u9898\u4e2d\uff0c\u5e76\u5c55\u793a\u4e86AL\u5f62\u5f0f\u4e3b\u4e49\u5982\u4f55\u4f5c\u4e3a\u5176\u6846\u67b6\u7684\u653e\u677e\u7248\u672c\u51fa\u73b0\u3002\u7279\u522b\u662f\u5728\u6ca1\u6709\u6b63\u5219\u5316\u9879\u7684\u60c5\u51b5\u4e0b\uff0cAL\u5f62\u5f0f\u4e3b\u4e49\u6210\u4e3a\u8be5\u6846\u67b6\u7684\u4e00\u4e2a\u7279\u4f8b\u3002\u63a5\u7740\uff0c\u4f5c\u8005\u5728\u6b21\u4f18\u4e13\u5bb6\u8bbe\u7f6e\u4e0b\uff0c\u5c06AL\u95ee\u9898\u8868\u793a\u4e3a\u4e00\u4e2a\u6b63\u5219\u5316\u7684min-max\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u955c\u50cf\u4e0b\u964d\u6cd5\uff08SMD\uff09\u6c42\u89e3\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u6536\u655b\u6027\u754c\u9650\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6b63\u5219\u5316\u5728\u5b66\u4e60\u6210\u672c\u5411\u91cf\u548c\u5b66\u5f92\u7b56\u7565\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3IRL\u4e2d\u7684\u75c5\u6001\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u6b63\u5219\u5316\u548c\u5148\u9a8c\u4fe1\u5ff5\u6539\u8fdb\u4e86AL\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9IRL\u4e2d\u7684\u6311\u6218\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6b63\u5219\u5316\u5728\u5bfb\u627e\u5408\u7406\u7684\u6210\u672c\u51fd\u6570\u65b9\u9762\u8d77\u5230\u4e86\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2505.22449", "pdf": "https://arxiv.org/pdf/2505.22449", "abs": "https://arxiv.org/abs/2505.22449", "authors": ["Joel Daniel Andersson", "Lukas Retschmeier", "Boel Nelson", "Rasmus Pagh"], "title": "Private Lossless Multiple Release", "categories": ["cs.CR", "cs.DS"], "comment": null, "summary": "Koufogiannis et al. (2016) showed a $\\textit{gradual release}$ result for\nLaplace noise-based differentially private mechanisms: given an\n$\\varepsilon$-DP release, a new release with privacy parameter $\\varepsilon' >\n\\varepsilon$ can be computed such that the combined privacy loss of both\nreleases is at most $\\varepsilon'$ and the distribution of the latter is the\nsame as a single release with parameter $\\varepsilon'$. They also showed\ngradual release techniques for Gaussian noise, later also explored by\nWhitehouse et al. (2022).\n  In this paper, we consider a more general $\\textit{multiple release}$ setting\nin which analysts hold private releases with different privacy parameters\ncorresponding to different access/trust levels. These releases are determined\none by one, with privacy parameters in arbitrary order. A multiple release is\n$\\textit{lossless}$ if having access to a subset $S$ of the releases has the\nsame privacy guarantee as the least private release in $S$, and each release\nhas the same distribution as a single release with the same privacy parameter.\nOur main result is that lossless multiple release is possible for a large class\nof additive noise mechanisms. For the Gaussian mechanism we give a simple\nmethod for lossless multiple release with a short, self-contained analysis that\ndoes not require knowledge of the mathematics of Brownian motion. We also\npresent lossless multiple release for the Laplace and Poisson mechanisms.\nFinally, we consider how to efficiently do gradual release of sparse\nhistograms, and present a mechanism with running time independent of the number\nof dimensions.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4e0d\u540c\u8bbf\u95ee/\u4fe1\u4efb\u7ea7\u522b\u4e0b\uff0c\u5206\u6790\u5e08\u6301\u6709\u5177\u6709\u4e0d\u540c\u9690\u79c1\u53c2\u6570\u7684\u79c1\u6709\u53d1\u5e03\u4e4b\u66f4\u901a\u7528\u7684\u591a\u53d1\u5e03\u8bbe\u7f6e\u3002\u4e3b\u8981\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u4e00\u5927\u7c7b\u52a0\u6027\u566a\u58f0\u673a\u5236\uff0c\u65e0\u635f\u591a\u53d1\u5e03\u662f\u53ef\u884c\u7684\uff0c\u5e76\u5bf9\u9ad8\u65af\u3001\u62c9\u666e\u62c9\u65af\u548c\u6cca\u677e\u673a\u5236\u63d0\u4f9b\u4e86\u65e0\u635f\u591a\u53d1\u5e03\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u7ef4\u5ea6\u6570\u91cf\u65e0\u5173\u7684\u7a00\u758f\u76f4\u65b9\u56fe\u6e10\u8fdb\u53d1\u5e03\u7684\u6709\u6548\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5df2\u7ecf\u5c55\u793a\u4e86\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u566a\u58f0\u548c\u9ad8\u65af\u566a\u58f0\u7684\u5dee\u5206\u9690\u79c1\u673a\u5236\u7684\u6e10\u8fdb\u53d1\u5e03\u6280\u672f\uff0c\u4f46\u672a\u5145\u5206\u63a2\u7d22\u5728\u4e0d\u540c\u8bbf\u95ee/\u4fe1\u4efb\u7ea7\u522b\u4e0b\u7684\u591a\u53d1\u5e03\u573a\u666f\u53ca\u5176\u65e0\u635f\u7279\u6027\u3002", "method": "1. \u5b9a\u4e49\u65e0\u635f\u591a\u53d1\u5e03\uff1a\u786e\u4fdd\u5b50\u96c6S\u7684\u53d1\u5e03\u5177\u6709\u4e0eS\u4e2d\u6700\u5c11\u9690\u79c1\u53d1\u5e03\u76f8\u540c\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u5e76\u4e14\u6bcf\u4e2a\u53d1\u5e03\u7684\u5206\u5e03\u7b49\u540c\u4e8e\u5355\u4e2a\u5177\u6709\u76f8\u540c\u9690\u79c1\u53c2\u6570\u7684\u53d1\u5e03\u3002\n2. \u5bf9\u9ad8\u65af\u673a\u5236\u63d0\u4f9b\u7b80\u5355\u7684\u65b9\u6cd5\u5b9e\u73b0\u65e0\u635f\u591a\u53d1\u5e03\uff0c\u5e76\u8fdb\u884c\u72ec\u7acb\u5206\u6790\u3002\n3. \u63d0\u4f9b\u62c9\u666e\u62c9\u65af\u548c\u6cca\u677e\u673a\u5236\u7684\u65e0\u635f\u591a\u53d1\u5e03\u65b9\u6cd5\u3002\n4. \u63a2\u8ba8\u7a00\u758f\u76f4\u65b9\u56fe\u7684\u9ad8\u6548\u6e10\u8fdb\u53d1\u5e03\u673a\u5236\uff0c\u63d0\u51fa\u4e00\u79cd\u8fd0\u884c\u65f6\u95f4\u4e0e\u7ef4\u5ea6\u6570\u91cf\u65e0\u5173\u7684\u673a\u5236\u3002", "result": "- \u8bc1\u660e\u65e0\u635f\u591a\u53d1\u5e03\u5bf9\u4e8e\u4e00\u5927\u7c7b\u52a0\u6027\u566a\u58f0\u673a\u5236\u662f\u53ef\u884c\u7684\u3002\n- \u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u65af\u3001\u62c9\u666e\u62c9\u65af\u548c\u6cca\u677e\u673a\u5236\u7684\u65e0\u635f\u591a\u53d1\u5e03\u3002\n- \u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u7a00\u758f\u76f4\u65b9\u56fe\u6e10\u8fdb\u53d1\u5e03\u673a\u5236\uff0c\u5176\u8fd0\u884c\u65f6\u95f4\u4e0e\u7ef4\u5ea6\u6570\u91cf\u65e0\u5173\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9a\u4e49\u5e76\u5b9e\u73b0\u65e0\u635f\u591a\u53d1\u5e03\uff0c\u6269\u5c55\u4e86\u5dee\u5206\u9690\u79c1\u673a\u5236\u7684\u5e94\u7528\u8303\u56f4\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8bbf\u95ee/\u4fe1\u4efb\u7ea7\u522b\u4e0b\u7684\u573a\u666f\u3002\u540c\u65f6\uff0c\u4e3a\u9ad8\u65af\u3001\u62c9\u666e\u62c9\u65af\u548c\u6cca\u677e\u673a\u5236\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u65e0\u635f\u591a\u53d1\u5e03\u65b9\u6cd5\uff0c\u5e76\u89e3\u51b3\u4e86\u7a00\u758f\u76f4\u65b9\u56fe\u6e10\u8fdb\u53d1\u5e03\u7684\u95ee\u9898\u3002"}}
{"id": "2505.22147", "pdf": "https://arxiv.org/pdf/2505.22147", "abs": "https://arxiv.org/abs/2505.22147", "authors": ["Florian Andreas Marwitz", "Tanya Braun", "Ralf M\u00f6ller", "Marcel Gehrke"], "title": "Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions", "categories": ["cs.AI"], "comment": null, "summary": "Decision making is a central problem in AI that can be formalized using a\nMarkov Decision Process. A problem is that, with increasing numbers of\n(indistinguishable) objects, the state space grows exponentially. To compute\npolicies, the state space has to be enumerated. Even more possibilities have to\nbe enumerated if the size of the action space depends on the size of the state\nspace, especially if we allow concurrent actions. To tackle the exponential\nblow-up in the action and state space, we present a first-order representation\nto store the spaces in polynomial instead of exponential size in the number of\nobjects and introduce Foreplan, a relational forward planner, which uses this\nrepresentation to efficiently compute policies for numerous indistinguishable\nobjects and actions. Additionally, we introduce an even faster approximate\nversion of Foreplan. Moreover, Foreplan identifies how many objects an agent\nshould act on to achieve a certain task given restrictions. Further, we provide\na theoretical analysis and an empirical evaluation of Foreplan, demonstrating a\nspeedup of at least four orders of magnitude.", "AI": {"tldr": "\u4e3a\u4e86\u5e94\u5bf9\u968f\u5bf9\u8c61\u6570\u91cf\u589e\u52a0\u800c\u6307\u6570\u7ea7\u589e\u957f\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e00\u9636\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u591a\u9879\u5f0f\u800c\u975e\u6307\u6570\u5927\u5c0f\u5b58\u50a8\u8fd9\u4e9b\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u4e86Foreplan\uff08\u4e00\u79cd\u5173\u7cfb\u524d\u5411\u89c4\u5212\u5668\uff09\u6765\u9ad8\u6548\u8ba1\u7b97\u5305\u542b\u5927\u91cf\u4e0d\u53ef\u533a\u5206\u5bf9\u8c61\u548c\u52a8\u4f5c\u7684\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5feb\u7684Foreplan\u8fd1\u4f3c\u7248\u672c\u4ee5\u53ca\u5982\u4f55\u5728\u7ed9\u5b9a\u9650\u5236\u6761\u4ef6\u4e0b\u786e\u5b9a\u4ee3\u7406\u5e94\u64cd\u4f5c\u7684\u5bf9\u8c61\u6570\u91cf\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u8bc1\u8bc4\u4f30\uff0cForeplan\u5c55\u73b0\u51fa\u81f3\u5c11\u56db\u4e2a\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u4e0d\u53ef\u533a\u5206\u5bf9\u8c61\u6570\u91cf\u7684\u589e\u52a0\uff0c\u72b6\u6001\u7a7a\u95f4\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4f20\u7edf\u7684\u679a\u4e3e\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5982\u6b64\u5e9e\u5927\u7684\u72b6\u6001\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u52a8\u4f5c\u7a7a\u95f4\u5927\u5c0f\u4f9d\u8d56\u4e8e\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\u65f6\uff0c\u4f8b\u5982\u5141\u8bb8\u5e76\u53d1\u52a8\u4f5c\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u4f7f\u5f97\u8ba1\u7b97\u7b56\u7565\u53d8\u5f97\u56f0\u96be\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u79cd\u6307\u6570\u7ea7\u81a8\u80c0\u7684\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u4e00\u9636\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ee5\u591a\u9879\u5f0f\u89c4\u6a21\u800c\u975e\u6307\u6570\u89c4\u6a21\u5b58\u50a8\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002\n2. \u5f15\u5165Foreplan\uff08\u4e00\u79cd\u5173\u7cfb\u524d\u5411\u89c4\u5212\u5668\uff09\uff0c\u5229\u7528\u4e0a\u8ff0\u8868\u793a\u65b9\u6cd5\u9ad8\u6548\u8ba1\u7b97\u6d89\u53ca\u4f17\u591a\u4e0d\u53ef\u533a\u5206\u5bf9\u8c61\u548c\u52a8\u4f5c\u7684\u7b56\u7565\u3002\n3. \u5f00\u53d1\u4e86\u4e00\u4e2a\u66f4\u5feb\u7684Foreplan\u8fd1\u4f3c\u7248\u672c\u3002\n4. Foreplan\u80fd\u591f\u8bc6\u522b\u4ee3\u7406\u4e3a\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u5728\u7ed9\u5b9a\u9650\u5236\u4e0b\u5e94\u64cd\u4f5c\u7684\u5bf9\u8c61\u6570\u91cf\u3002", "result": "1. \u7406\u8bba\u5206\u6790\u8868\u660eForeplan\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u6307\u6570\u7ea7\u589e\u957f\u3002\n2. \u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aForeplan\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u901f\u5ea6\u63d0\u5347\u4e86\u81f3\u5c11\u56db\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "Foreplan\u53ca\u5176\u8fd1\u4f3c\u7248\u672c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5305\u542b\u5927\u91cf\u4e0d\u53ef\u533a\u5206\u5bf9\u8c61\u548c\u52a8\u4f5c\u7684\u573a\u666f\u3002\u5176\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.21640", "pdf": "https://arxiv.org/pdf/2505.21640", "abs": "https://arxiv.org/abs/2505.21640", "authors": ["Oren Mangoubi", "Neil He", "Nisheeth K. Vishnoi"], "title": "Efficient Diffusion Models for Symmetric Manifolds", "categories": ["cs.LG", "cs.AI", "cs.DS", "math.PR", "stat.ML"], "comment": "The conference version of this paper appears in ICML 2025", "summary": "We introduce a framework for designing efficient diffusion models for\n$d$-dimensional symmetric-space Riemannian manifolds, including the torus,\nsphere, special orthogonal group and unitary group. Existing manifold diffusion\nmodels often depend on heat kernels, which lack closed-form expressions and\nrequire either $d$ gradient evaluations or exponential-in-$d$ arithmetic\noperations per training step. We introduce a new diffusion model for symmetric\nmanifolds with a spatially-varying covariance, allowing us to leverage a\nprojection of Euclidean Brownian motion to bypass heat kernel computations. Our\ntraining algorithm minimizes a novel efficient objective derived via Ito's\nLemma, allowing each step to run in $O(1)$ gradient evaluations and\nnearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap\nbetween diffusions on symmetric manifolds and Euclidean space. Manifold\nsymmetries ensure the diffusion satisfies an \"average-case\" Lipschitz\ncondition, enabling accurate and efficient sample generation. Empirically, our\nmodel outperforms prior methods in training speed and improves sample quality\non synthetic datasets on the torus, special orthogonal group, and unitary\ngroup.", "AI": {"tldr": "The paper presents an efficient diffusion model framework for symmetric-space Riemannian manifolds, reducing computational complexity and improving performance in training speed and sample quality.", "motivation": "Existing manifold diffusion models often depend on heat kernels which are computationally expensive. This drives the need for a more efficient approach to handle symmetric-space Riemannian manifolds.", "method": "A new diffusion model is introduced with spatially-varying covariance for symmetric manifolds, using a projection of Euclidean Brownian motion. The training algorithm minimizes a novel objective derived via Ito's Lemma, achieving nearly-linear-in-dimension computational complexity.", "result": "The model outperforms prior methods in terms of training speed and sample quality on synthetic datasets involving the torus, special orthogonal group, and unitary group.", "conclusion": "This work successfully reduces the computational gap between diffusions on symmetric manifolds and Euclidean space, providing a more efficient and accurate method for sample generation."}}
{"id": "2505.22605", "pdf": "https://arxiv.org/pdf/2505.22605", "abs": "https://arxiv.org/abs/2505.22605", "authors": ["Banafsheh Saber Latibari", "Najmeh Nazari", "Avesta Sasan", "Houman Homayoun", "Pratik Satam", "Soheil Salehi", "Hossein Sayadi"], "title": "Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook", "categories": ["cs.CR"], "comment": null, "summary": "The rise of hardware-level security threats, such as side-channel attacks,\nhardware Trojans, and firmware vulnerabilities, demands advanced detection\nmechanisms that are more intelligent and adaptive. Traditional methods often\nfall short in addressing the complexity and evasiveness of modern attacks,\ndriving increased interest in machine learning-based solutions. Among these,\nTransformer models, widely recognized for their success in natural language\nprocessing and computer vision, have gained traction in the security domain due\nto their ability to model complex dependencies, offering enhanced capabilities\nin identifying vulnerabilities, detecting anomalies, and reinforcing system\nintegrity. This survey provides a comprehensive review of recent advancements\non the use of Transformers in hardware security, examining their application\nacross key areas such as side-channel analysis, hardware Trojan detection,\nvulnerability classification, device fingerprinting, and firmware security.\nFurthermore, we discuss the practical challenges of applying Transformers to\nsecure hardware systems, and highlight opportunities and future research\ndirections that position them as a foundation for next-generation\nhardware-assisted security. These insights pave the way for deeper integration\nof AI-driven techniques into hardware security frameworks, enabling more\nresilient and intelligent defenses.", "AI": {"tldr": "Transformers are gaining traction in hardware security due to their ability to model complex dependencies, offering enhanced capabilities in identifying vulnerabilities, detecting anomalies, and reinforcing system integrity.", "motivation": "The rise of hardware-level security threats such as side-channel attacks, hardware Trojans, and firmware vulnerabilities demands advanced detection mechanisms that are more intelligent and adaptive.", "method": "This survey provides a comprehensive review of recent advancements on the use of Transformers in hardware security, examining their application across key areas such as side-channel analysis, hardware Trojan detection, vulnerability classification, device fingerprinting, and firmware security.", "result": "The practical challenges of applying Transformers to secure hardware systems are discussed, and opportunities and future research directions are highlighted.", "conclusion": "These insights pave the way for deeper integration of AI-driven techniques into hardware security frameworks, enabling more resilient and intelligent defenses."}}
{"id": "2505.22148", "pdf": "https://arxiv.org/pdf/2505.22148", "abs": "https://arxiv.org/abs/2505.22148", "authors": ["Gangwei Jiang", "Yahui Liu", "Zhaoyi Li", "Qi Wang", "Fuzheng Zhang", "Linqi Song", "Ying Wei", "Defu Lian"], "title": "What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have\npopularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate\nand step-by-step reasoning before producing a final answer. While LCoTs have\nenabled expert-level performance in complex tasks, how the internal structures\nof their reasoning chains drive, or even predict, the correctness of final\nanswers remains a critical yet underexplored question. In this work, we present\nLCoT2Tree, an automated framework that converts sequential LCoTs into\nhierarchical tree structures and thus enables deeper structural analysis of LLM\nreasoning. Using graph neural networks (GNNs), we reveal that structural\npatterns extracted by LCoT2Tree, including exploration, backtracking, and\nverification, serve as stronger predictors of final performance across a wide\nrange of tasks and models. Leveraging an explainability technique, we further\nidentify critical thought patterns such as over-branching that account for\nfailures. Beyond diagnostic insights, the structural patterns by LCoT2Tree\nsupport practical applications, including improving Best-of-N decoding\neffectiveness. Overall, our results underscore the critical role of internal\nstructures of reasoning chains, positioning LCoT2Tree as a powerful tool for\ndiagnosing, interpreting, and improving reasoning in LLMs.", "AI": {"tldr": "\u8fd1\u671f\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u7684\u7814\u7a76\u63a8\u52a8\u4e86\u957f\u94fe\u601d\u7ef4\uff08LCoT\uff09\u7b56\u7565\u7684\u53d1\u5c55\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLCoT2Tree\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u987a\u5e8fLCoTs\u8f6c\u6362\u4e3a\u5206\u5c42\u6811\u7ed3\u6784\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9LLM\u63a8\u7406\u7684\u66f4\u6df1\u5165\u7ed3\u6784\u5206\u6790\u3002\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\uff0c\u6211\u4eec\u53d1\u73b0\u7531LCoT2Tree\u63d0\u53d6\u7684\u7ed3\u6784\u6a21\u5f0f\uff08\u5305\u62ec\u63a2\u7d22\u3001\u56de\u6eaf\u548c\u9a8c\u8bc1\uff09\u662f\u9884\u6d4b\u6700\u7ec8\u8868\u73b0\u7684\u66f4\u5f3a\u6307\u6807\u3002\u6b64\u5916\uff0c\u5229\u7528\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u786e\u5b9a\u4e86\u5bfc\u81f4\u5931\u8d25\u7684\u5173\u952e\u601d\u7ef4\u6a21\u5f0f\uff0c\u4f8b\u5982\u8fc7\u5ea6\u5206\u652f\u3002LCoT2Tree\u652f\u6301\u5b9e\u9645\u5e94\u7528\uff0c\u5982\u63d0\u9ad8\u6700\u4f73N\u89e3\u7801\u7684\u6709\u6548\u6027\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u5f3a\u8c03\u4e86\u63a8\u7406\u94fe\u5185\u90e8\u7ed3\u6784\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4f7fLCoT2Tree\u6210\u4e3a\u8bca\u65ad\u3001\u89e3\u91ca\u548c\u6539\u8fdbLLMs\u63a8\u7406\u7684\u5f3a\u5927\u5de5\u5177\u3002", "motivation": "\u5c3d\u7ba1\u957f\u94fe\u601d\u7ef4\uff08LCoT\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4e13\u5bb6\u7ea7\u7684\u8868\u73b0\uff0c\u4f46\u5176\u63a8\u7406\u94fe\u7684\u5185\u90e8\u7ed3\u6784\u5982\u4f55\u9a71\u52a8\u751a\u81f3\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u5c1a\u672a\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86LCoT2Tree\uff0c\u4e00\u79cd\u5c06\u987a\u5e8fLCoTs\u8f6c\u6362\u4e3a\u5206\u5c42\u6811\u7ed3\u6784\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u4ee5\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u7ed3\u6784\u5206\u6790\u3002\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u63ed\u793a\u7ed3\u6784\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8bc6\u522b\u5173\u952e\u601d\u7ef4\u6a21\u5f0f\u3002", "result": "\u7ed3\u6784\u6a21\u5f0f\uff08\u5982\u63a2\u7d22\u3001\u56de\u6eaf\u548c\u9a8c\u8bc1\uff09\u662f\u66f4\u5f3a\u7684\u6027\u80fd\u9884\u6d4b\u6307\u6807\uff0c\u80fd\u591f\u89e3\u91ca\u63a8\u7406\u5931\u8d25\u7684\u539f\u56e0\uff0c\u5e76\u652f\u6301\u5b9e\u9645\u5e94\u7528\uff0c\u5982\u6539\u8fdb\u89e3\u7801\u6548\u679c\u3002", "conclusion": "\u63a8\u7406\u94fe\u7684\u5185\u90e8\u7ed3\u6784\u5728LLM\u63a8\u7406\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0cLCoT2Tree\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bca\u65ad\u3001\u89e3\u91ca\u548c\u6539\u8fdbLLM\u63a8\u7406\u3002"}}
{"id": "2505.21641", "pdf": "https://arxiv.org/pdf/2505.21641", "abs": "https://arxiv.org/abs/2505.21641", "authors": ["Maresa Schr\u00f6der", "Justin Hartenstein", "Stefan Feuerriegel"], "title": "PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects", "categories": ["cs.LG", "cs.CR", "stat.ME"], "comment": null, "summary": "The average treatment effect (ATE) is widely used to evaluate the\neffectiveness of drugs and other medical interventions. In safety-critical\napplications like medicine, reliable inferences about the ATE typically require\nvalid uncertainty quantification, such as through confidence intervals (CIs).\nHowever, estimating treatment effects in these settings often involves\nsensitive data that must be kept private. In this work, we present PrivATE, a\nnovel machine learning framework for computing CIs for the ATE under\ndifferential privacy. Specifically, we focus on deriving valid\nprivacy-preserving CIs for the ATE from observational data. Our PrivATE\nframework consists of three steps: (i) estimating a differentially private ATE\nthrough output perturbation; (ii) estimating the differentially private\nvariance through a truncated output perturbation mechanism; and (iii)\nconstructing the CIs while accounting for the uncertainty from both the\nestimation and privatization steps. Our PrivATE framework is model agnostic,\ndoubly robust, and ensures valid CIs. We demonstrate the effectiveness of our\nframework using synthetic and real-world medical datasets. To the best of our\nknowledge, we are the first to derive a general, doubly robust framework for\nvalid CIs of the ATE under ($\\varepsilon$, $\\delta$)-differential privacy.", "AI": {"tldr": "\u5728\u533b\u5b66\u7b49\u9700\u8981\u53ef\u9760\u63a8\u65ad\u7684\u9886\u57df\uff0c\u8bc4\u4f30\u836f\u7269\u548c\u5176\u4ed6\u533b\u7597\u5e72\u9884\u63aa\u65bd\u7684\u6548\u679c\u901a\u5e38\u9700\u8981\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u9886\u57df\u4e2d\u7684\u6570\u636e\u5f80\u5f80\u662f\u654f\u611f\u7684\uff0c\u5fc5\u987b\u4fdd\u6301\u9690\u79c1\u3002\u672c\u6587\u63d0\u51fa\u4e86PrivATE\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u8bc1\u5dee\u5f02\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u8ba1\u7b97\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u7684\u7f6e\u4fe1\u533a\u95f4\uff08CIs\uff09\u3002PrivATE\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a(i) \u901a\u8fc7\u8f93\u51fa\u6270\u52a8\u4f30\u8ba1\u5dee\u5f02\u9690\u79c1ATE\uff1b(ii) \u901a\u8fc7\u622a\u65ad\u8f93\u51fa\u6270\u52a8\u673a\u5236\u4f30\u8ba1\u5dee\u5f02\u9690\u79c1\u65b9\u5dee\uff1b(iii) \u8003\u8651\u4f30\u8ba1\u548c\u9690\u79c1\u5316\u6b65\u9aa4\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6784\u5efaCIs\u3002PrivATE\u6846\u67b6\u662f\u6a21\u578b\u65e0\u5173\u3001\u53cc\u91cd\u7a33\u5065\u5e76\u786e\u4fdd\u6709\u6548\u7684CIs\u3002\u5b9e\u9a8c\u8868\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u6027\u5173\u952e\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u533b\u5b66\uff0c\u8bc4\u4f30\u836f\u7269\u548c\u5176\u4ed6\u533b\u7597\u5e72\u9884\u63aa\u65bd\u7684\u6548\u679c\u9700\u8981\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5e94\u7528\u6d89\u53ca\u7684\u6570\u636e\u901a\u5e38\u662f\u654f\u611f\u7684\uff0c\u5fc5\u987b\u4fdd\u6301\u9690\u79c1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u8fdb\u884c\u53ef\u9760\u7684\u56e0\u679c\u63a8\u65ad\u3002", "method": "PrivATE\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1. \u4f7f\u7528\u8f93\u51fa\u6270\u52a8\u65b9\u6cd5\u4f30\u8ba1\u5dee\u5f02\u9690\u79c1ATE\uff1b2. \u4f7f\u7528\u622a\u65ad\u8f93\u51fa\u6270\u52a8\u673a\u5236\u4f30\u8ba1\u5dee\u5f02\u9690\u79c1\u65b9\u5dee\uff1b3. \u6784\u5efaCIs\u65f6\u8003\u8651\u4f30\u8ba1\u548c\u9690\u79c1\u5316\u6b65\u9aa4\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u53cc\u91cd\u7a33\u5065\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPrivATE\u6846\u67b6\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u533b\u7597\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u80fd\u591f\u4ea7\u751f\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6PrivATE\uff0c\u7528\u4e8e\u5728\u5dee\u5f02\u9690\u79c1\u4e0b\u8ba1\u7b97ATE\u7684\u7f6e\u4fe1\u533a\u95f4\u3002\u8be5\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u548c\u53cc\u91cd\u7a33\u5065\u6027\uff0c\u5e76\u4e14\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9($\\varepsilon$, $\\delta$)-\u5dee\u5f02\u9690\u79c1\u4e0b\u7684\u6709\u6548\u7f6e\u4fe1\u533a\u95f4\u63d0\u4f9b\u901a\u7528\u3001\u53cc\u91cd\u7a33\u5065\u6846\u67b6\u7684\u7814\u7a76\u3002"}}
{"id": "2505.22638", "pdf": "https://arxiv.org/pdf/2505.22638", "abs": "https://arxiv.org/abs/2505.22638", "authors": ["Denis Donadel", "Gabriele Crestanello", "Giulio Morandini", "Daniele Antonioli", "Mauro Conti", "Massimo Merro"], "title": "SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes", "categories": ["cs.CR", "cs.LG"], "comment": "In 11th ACM Cyber-Physical System Security Workshop (CPSS '25),\n  August 25-29, 2025, Hanoi, Vietnam", "summary": "Industrial Control Systems (ICS) manage critical infrastructures like power\ngrids and water treatment plants. Cyberattacks on ICSs can disrupt operations,\ncausing severe economic, environmental, and safety issues. For example,\nundetected pollution in a water plant can put the lives of thousands at stake.\nICS researchers have increasingly turned to honeypots -- decoy systems designed\nto attract attackers, study their behaviors, and eventually improve defensive\nmechanisms. However, existing ICS honeypots struggle to replicate the ICS\nphysical process, making them susceptible to detection. Accurately simulating\nthe noise in ICS physical processes is challenging because different factors\nproduce it, including sensor imperfections and external interferences.\n  In this paper, we propose SimProcess, a novel framework to rank the fidelity\nof ICS simulations by evaluating how closely they resemble real-world and noisy\nphysical processes. It measures the simulation distance from a target system by\nestimating the noise distribution with machine learning models like Random\nForest. Unlike existing solutions that require detailed mathematical models or\nare limited to simple systems, SimProcess operates with only a timeseries of\nmeasurements from the real system, making it applicable to a broader range of\ncomplex dynamic systems. We demonstrate the framework's effectiveness through a\ncase study using real-world power grid data from the EPIC testbed. We compare\nthe performance of various simulation methods, including static and generative\nnoise techniques. Our model correctly classifies real samples with a recall of\nup to 1.0. It also identifies Gaussian and Gaussian Mixture as the best\ndistribution to simulate our power systems, together with a generative solution\nprovided by an autoencoder, thereby helping developers to improve honeypot\nfidelity. Additionally, we make our code publicly available.", "AI": {"tldr": "\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edf\uff08ICS\uff09\u5bf9\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u7f51\u7edc\u653b\u51fb\u3002\u672c\u6587\u63d0\u51faSimProcess\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u62df\u4e0e\u771f\u5b9e\u5e26\u566a\u58f0\u7269\u7406\u8fc7\u7a0b\u7684\u63a5\u8fd1\u7a0b\u5ea6\u6765\u63d0\u9ad8ICS\u4eff\u771f\u4fdd\u771f\u5ea6\uff0c\u4ece\u800c\u6539\u8fdb\u871c\u7f50\u7cfb\u7edf\u7684\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684ICS\u871c\u7f50\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u590d\u5236ICS\u7269\u7406\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\uff0c\u5bfc\u81f4\u5bb9\u6613\u88ab\u68c0\u6d4b\u5230\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc4\u4f30ICS\u4eff\u771f\u4fdd\u771f\u5ea6\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e2e\u52a9\u6539\u8fdb\u9632\u5fa1\u673a\u5236\u3002", "method": "\u63d0\u51faSimProcess\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\uff09\u4f30\u8ba1\u566a\u58f0\u5206\u5e03\uff0c\u901a\u8fc7\u6d4b\u91cf\u5e8f\u5217\u6570\u636e\u8bc4\u4f30\u4eff\u771f\u4e0e\u771f\u5b9e\u7cfb\u7edf\u7684\u8ddd\u79bb\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8be6\u7ec6\u6570\u5b66\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4f7f\u7528EPIC\u6d4b\u8bd5\u5e73\u53f0\u7684\u771f\u5b9e\u7535\u529b\u6570\u636e\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86SimProcess\u7684\u6709\u6548\u6027\u3002\u6a21\u578b\u80fd\u4ee5\u9ad8\u8fbe1.0\u7684\u53ec\u56de\u7387\u6b63\u786e\u5206\u7c7b\u771f\u5b9e\u6837\u672c\uff0c\u5e76\u786e\u5b9a\u9ad8\u65af\u5206\u5e03\u548c\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u4e3a\u6700\u4f73\u566a\u58f0\u6a21\u62df\u65b9\u6cd5\uff0c\u540c\u65f6\u63a8\u8350\u4f7f\u7528\u81ea\u52a8\u7f16\u7801\u5668\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SimProcess\u6846\u67b6\u53ef\u6709\u6548\u8bc4\u4f30ICS\u4eff\u771f\u7684\u4fdd\u771f\u5ea6\uff0c\u5e2e\u52a9\u6539\u8fdb\u871c\u7f50\u7cfb\u7edf\u3002\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2505.22244", "pdf": "https://arxiv.org/pdf/2505.22244", "abs": "https://arxiv.org/abs/2505.22244", "authors": ["Yaron Halle", "Ariel Felner", "Sven Koenig", "Oren Salzman"], "title": "A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives", "categories": ["cs.AI"], "comment": null, "summary": "The bi-objective shortest-path (BOSP) problem seeks to find paths between\nstart and target vertices of a graph while optimizing two conflicting objective\nfunctions. We consider the BOSP problem in the presence of correlated\nobjectives. Such correlations often occur in real-world settings such as road\nnetworks, where optimizing two positively correlated objectives, such as travel\ntime and fuel consumption, is common. BOSP is generally computationally\nchallenging as the size of the search space is exponential in the number of\nobjective functions and the graph size. Bounded sub-optimal BOSP solvers such\nas A*pex alleviate this complexity by approximating the Pareto-optimal solution\nset rather than computing it exactly (given a user-provided approximation\nfactor). As the correlation between objective functions increases, smaller\napproximation factors are sufficient for collapsing the entire Pareto-optimal\nset into a single solution. We leverage this insight to propose an efficient\nalgorithm that reduces the search effort in the presence of correlated\nobjectives. Our approach for computing approximations of the entire\nPareto-optimal set is inspired by graph-clustering algorithms. It uses a\npreprocessing phase to identify correlated clusters within a graph and to\ngenerate a new graph representation. This allows a natural generalization of\nA*pex to run up to five times faster on DIMACS dataset instances, a standard\nbenchmark in the field. To the best of our knowledge, this is the first\nalgorithm proposed that efficiently and effectively exploits correlations in\nthe context of bi-objective search while providing theoretical guarantees on\nsolution quality.", "AI": {"tldr": "The paper presents an efficient algorithm for solving the bi-objective shortest-path (BOSP) problem with correlated objectives, inspired by graph-clustering algorithms. It reduces search effort and runs up to five times faster on benchmark instances compared to existing methods.", "motivation": "The BOSP problem is computationally challenging due to the exponential size of the search space. Existing bounded sub-optimal solvers like A*pex can approximate Pareto-optimal solutions but do not exploit correlations between objectives. The authors aim to develop a more efficient algorithm that leverages such correlations to reduce search effort.", "method": "The proposed method uses a preprocessing phase to identify correlated clusters within a graph and generate a new graph representation. This allows a generalization of A*pex to run faster while maintaining theoretical guarantees on solution quality.", "result": "The algorithm runs up to five times faster on DIMACS dataset instances, a standard benchmark in the field. It effectively exploits correlations in bi-objective search while providing theoretical guarantees on solution quality.", "conclusion": "This is the first algorithm that efficiently and effectively exploits correlations in the context of bi-objective search with theoretical guarantees. It offers significant speed improvements over existing methods."}}
{"id": "2505.21651", "pdf": "https://arxiv.org/pdf/2505.21651", "abs": "https://arxiv.org/abs/2505.21651", "authors": ["Nikola Surjanovic", "Alexandre Bouchard-C\u00f4t\u00e9", "Trevor Campbell"], "title": "AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent", "categories": ["cs.LG", "math.OC", "stat.CO", "stat.ML"], "comment": null, "summary": "The learning rate is an important tuning parameter for stochastic gradient\ndescent (SGD) and can greatly influence its performance. However, appropriate\nselection of a learning rate schedule across all iterations typically requires\na non-trivial amount of user tuning effort. To address this, we introduce\nAutoSGD: an SGD method that automatically determines whether to increase or\ndecrease the learning rate at a given iteration and then takes appropriate\naction. We introduce theory supporting the convergence of AutoSGD, along with\nits deterministic counterpart for standard gradient descent. Empirical results\nsuggest strong performance of the method on a variety of traditional\noptimization problems and machine learning tasks.", "AI": {"tldr": "AutoSGD is an SGD method that automatically adjusts the learning rate, showing strong performance in various optimization problems and machine learning tasks.", "motivation": "The tuning of learning rate for stochastic gradient descent (SGD) can significantly affect its performance. However, selecting an appropriate learning rate schedule usually requires substantial user effort.", "method": "Introduced AutoSGD, which can automatically decide to increase or decrease the learning rate at each iteration and then take corresponding actions. Theoretical support for the convergence of AutoSGD and its deterministic counterpart for standard gradient descent was also provided.", "result": "Empirical results indicate that AutoSGD performs well on a variety of traditional optimization problems and machine learning tasks.", "conclusion": "AutoSGD provides an effective solution to automatically adjust the learning rate for SGD, reducing the need for manual tuning."}}
{"id": "2505.22644", "pdf": "https://arxiv.org/pdf/2505.22644", "abs": "https://arxiv.org/abs/2505.22644", "authors": ["Mohamed Aly Bouke"], "title": "On the Intractability of Chaotic Symbolic Walks: Toward a Non-Algebraic Post-Quantum Hardness Assumption", "categories": ["cs.CR", "math.DS"], "comment": null, "summary": "Most classical and post-quantum cryptographic assumptions, including integer\nfactorization, discrete logarithms, and Learning with Errors (LWE), rely on\nalgebraic structures such as rings or vector spaces. While mathematically\npowerful, these structures can be exploited by quantum algorithms or advanced\nalgebraic attacks, raising a pressing need for structure-free alternatives. To\naddress this gap, we introduce the Symbolic Path Inversion Problem (SPIP), a\nnew computational hardness assumption based on symbolic trajectories generated\nby contractive affine maps with bounded noise over Z2. Unlike traditional\nsystems, SPIP is inherently non-algebraic and relies on chaotic symbolic\nevolution and rounding-induced non-injectivity to render inversion\ncomputationally infeasible. We prove that SPIP is PSPACE-hard and #P-hard, and\ndemonstrate through empirical simulation that even short symbolic sequences\n(e.g., n = 3, m = 2) can produce over 500 valid trajectories for a single\nendpoint, with exponential growth reaching 2256 paths for moderate parameters.\nA quantum security analysis further shows that Grover-style search offers no\npractical advantage due to oracle ambiguity and verification instability. These\nresults position SPIP as a viable foundation for post-quantum cryptography that\navoids the vulnerabilities of algebraic symmetry while offering scalability,\nunpredictability, and resistance to both classical and quantum inversion.", "AI": {"tldr": "The paper introduces SPIP, a new non-algebraic computational hardness assumption for post-quantum cryptography, proving its PSPACE-hard and #P-hard nature and demonstrating resistance to quantum attacks.", "motivation": "To address the vulnerability of classical and post-quantum cryptographic assumptions to quantum algorithms and advanced algebraic attacks, there is a need for structure-free alternatives.", "method": "Introduced Symbolic Path Inversion Problem (SPIP) based on symbolic trajectories generated by contractive affine maps with bounded noise over Z2. Proved that SPIP is PSPACE-hard and #P-hard, and demonstrated through empirical simulation.", "result": "Even short symbolic sequences can produce a large number of valid trajectories with exponential growth. Quantum security analysis shows resistance to Grover-style search due to oracle ambiguity and verification instability.", "conclusion": "SPIP is positioned as a viable foundation for post-quantum cryptography avoiding algebraic symmetry vulnerabilities while offering scalability, unpredictability, and resistance to both classical and quantum inversion."}}
{"id": "2505.22288", "pdf": "https://arxiv.org/pdf/2505.22288", "abs": "https://arxiv.org/abs/2505.22288", "authors": ["Jan Speller", "Malte Luttermann", "Marcel Gehrke", "Tanya Braun"], "title": "Compression versus Accuracy: A Hierarchy of Lifted Models", "categories": ["cs.AI"], "comment": null, "summary": "Probabilistic graphical models that encode indistinguishable objects and\nrelations among them use first-order logic constructs to compress a\npropositional factorised model for more efficient (lifted) inference. To obtain\na lifted representation, the state-of-the-art algorithm Advanced Colour Passing\n(ACP) groups factors that represent matching distributions. In an approximate\nversion using $\\varepsilon$ as a hyperparameter, factors are grouped that\ndiffer by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable\n$\\varepsilon$ is not obvious and may need a lot of exploration, possibly\nrequiring many ACP runs with different $\\varepsilon$ values. Additionally,\nvarying $\\varepsilon$ can yield wildly different models, leading to decreased\ninterpretability. Therefore, this paper presents a hierarchical approach to\nlifted model construction that is hyperparameter-free. It efficiently computes\na hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning\nthat once factors are grouped together given some $\\varepsilon$, these factors\nwill be grouped together for larger $\\varepsilon$ as well. The hierarchy of\n$\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for\nexplicitly weighing compression versus accuracy when choosing specific\n$\\varepsilon$ values to run ACP with and enables interpretability between the\ndifferent models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8d85\u53c2\u6570\u7684\u5206\u5c42\u65b9\u6cd5\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u6784\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u9700\u8981\u591a\u6b21\u5c1d\u8bd5\u4e0d\u540c$\\varepsilon$\u503c\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u538b\u7f29\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684Advanced Colour Passing (ACP)\u7b97\u6cd5\u5728\u4f7f\u7528\u8fd1\u4f3c\u7248\u672c\u65f6\uff0c\u9700\u8981\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570$\\varepsilon$\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5927\u91cf\u7684\u63a2\u7d22\u548c\u6a21\u578b\u89e3\u91ca\u6027\u964d\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u8d85\u53c2\u6570\uff0c\u80fd\u591f\u9ad8\u6548\u8ba1\u7b97\u4e00\u7cfb\u5217$\\varepsilon$\u503c\uff0c\u786e\u4fdd\u6a21\u578b\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5373\u5bf9\u4e8e\u67d0\u4e2a$\\varepsilon$\u503c\u5206\u7ec4\u7684\u56e0\u5b50\uff0c\u5728\u66f4\u5927\u7684$\\varepsilon$\u503c\u4e0b\u4e5f\u4f1a\u4fdd\u6301\u5206\u7ec4\u3002\u540c\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u8bef\u5dee\u754c\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u65b0\u65b9\u6cd5\u5141\u8bb8\u660e\u786e\u6743\u8861\u6a21\u578b\u7684\u538b\u7f29\u7a0b\u5ea6\u4e0e\u51c6\u786e\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5206\u5c42\u65b9\u6cd5\u4e3a\u63d0\u5347\u6a21\u578b\u6784\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u5bf9\u8d85\u53c2\u6570\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6548\u7387\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.21660", "pdf": "https://arxiv.org/pdf/2505.21660", "abs": "https://arxiv.org/abs/2505.21660", "authors": ["Xiaojie Xu", "Xinli Xu", "Sirui Chen", "Haoyu Chen", "Fan Zhang", "Ying-Cong Chen"], "title": "PreGenie: An Agentic Framework for High-quality Visual Presentation Generation", "categories": ["cs.LG"], "comment": "11 pages, 9 figures", "summary": "Visual presentations are vital for effective communication. Early attempts to\nautomate their creation using deep learning often faced issues such as poorly\norganized layouts, inaccurate text summarization, and a lack of image\nunderstanding, leading to mismatched visuals and text. These limitations\nrestrict their application in formal contexts like business and scientific\nresearch. To address these challenges, we propose PreGenie, an agentic and\nmodular framework powered by multimodal large language models (MLLMs) for\ngenerating high-quality visual presentations.\n  PreGenie is built on the Slidev presentation framework, where slides are\nrendered from Markdown code. It operates in two stages: (1) Analysis and\nInitial Generation, which summarizes multimodal input and generates initial\ncode, and (2) Review and Re-generation, which iteratively reviews intermediate\ncode and rendered slides to produce final, high-quality presentations. Each\nstage leverages multiple MLLMs that collaborate and share information.\nComprehensive experiments demonstrate that PreGenie excels in multimodal\nunderstanding, outperforming existing models in both aesthetics and content\nconsistency, while aligning more closely with human design preferences.", "AI": {"tldr": "PreGenie is a new framework that uses multimodal large language models to generate high-quality visual presentations, overcoming previous issues like poor layouts and mismatched visuals. It works in two stages: analysis/initial generation and review/re-generation, using multiple models that collaborate. Experiments show it performs well in aesthetics and content consistency.", "motivation": "Previous deep learning methods for creating visual presentations often had problems such as disorganized layouts, inaccurate text summarization, and mismatched visuals, limiting their use in formal settings.", "method": "PreGenie operates in two stages: (1) Analysis and Initial Generation where it summarizes multimodal input and generates initial code, and (2) Review and Re-generation where it iteratively reviews intermediate code and rendered slides to produce the final presentation. Multiple MLLMs collaborate and share information in each stage.", "result": "Experiments indicate that PreGenie surpasses existing models in terms of aesthetics and content consistency, aligning closely with human design preferences.", "conclusion": "PreGenie addresses the limitations of earlier methods by leveraging MLLMs to create visually appealing and content-consistent presentations, making it suitable for formal contexts."}}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162", "abs": "https://arxiv.org/abs/2505.20162", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers.", "AI": {"tldr": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u548c\u81ea\u4e3b\u6027\u589e\u5f3a\uff0c\u8bc6\u522b\u6f0f\u6d1e\u5bf9\u4e8e\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u53ef\u80fd\u5728\u7ea2\u961f\u6d4b\u8bd5\u8f6c\u53d8\u4e3a\u5f31\u5230\u5f3a\u95ee\u9898\u65f6\u65e0\u6548\u3002\u672c\u6587\u901a\u8fc7\u80fd\u529b\u5dee\u8ddd\u7684\u89c6\u89d2\u7814\u7a76\u7ea2\u961f\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u8d85\u8fc7500\u4e2a\u653b\u51fb\u8005-\u76ee\u6807\u5bf9\u3002\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\u662f\u66f4\u597d\u7684\u653b\u51fb\u8005\uff0c\u4e00\u65e6\u76ee\u6807\u7684\u80fd\u529b\u8d85\u8fc7\u653b\u51fb\u8005\uff0c\u653b\u51fb\u6210\u529f\u7387\u4f1a\u6025\u5267\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u653b\u51fb\u6210\u529f\u7387\u4e0eMMLU-Pro\u57fa\u51c6\u7684\u793e\u4f1a\u79d1\u5b66\u90e8\u5206\u7684\u9ad8\u6027\u80fd\u76f8\u5173\u3002\u57fa\u4e8e\u8fd9\u4e9b\u8d8b\u52bf\uff0c\u6211\u4eec\u63a8\u5bfc\u51fa\u4e00\u4e2a\u8d8a\u72f1\u6269\u5c55\u5b9a\u5f8b\uff0c\u9884\u6d4b\u56fa\u5b9a\u76ee\u6807\u7684\u653b\u51fb\u6210\u529f\u57fa\u4e8e\u653b\u51fb\u8005-\u76ee\u6807\u80fd\u529b\u5dee\u8ddd\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u56fa\u5b9a\u80fd\u529b\u7684\u653b\u51fb\u8005\uff08\u5982\u4eba\u7c7b\uff09\u53ef\u80fd\u5728\u672a\u6765\u6a21\u578b\u4e2d\u53d8\u5f97\u65e0\u6548\uff0c\u5f00\u6e90\u6a21\u578b\u7684\u98ce\u9669\u589e\u52a0\uff0c\u6a21\u578b\u63d0\u4f9b\u8005\u5fc5\u987b\u51c6\u786e\u6d4b\u91cf\u548c\u63a7\u5236\u6a21\u578b\u7684\u8bf4\u670d\u548c\u64cd\u7eb5\u80fd\u529b\u4ee5\u9650\u5236\u5176\u4f5c\u4e3a\u653b\u51fb\u8005\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u548c\u81ea\u4e3b\u6027\u589e\u5f3a\uff0c\u8bc6\u522b\u6f0f\u6d1e\u5bf9\u4e8e\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u53ef\u80fd\u5728\u7ea2\u961f\u6d4b\u8bd5\u8f6c\u53d8\u4e3a\u5f31\u5230\u5f3a\u95ee\u9898\u65f6\u65e0\u6548\uff0c\u9700\u8981\u7814\u7a76\u653b\u51fb\u8005\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "method": "\u8bc4\u4f30\u8d85\u8fc7500\u4e2a\u653b\u51fb\u8005-\u76ee\u6807\u5bf9\uff0c\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8d8a\u72f1\u653b\u51fb\u6a21\u62df\u4eba\u7c7b\u7ea2\u961f\u6210\u5458\uff0c\u6db5\u76d6\u4e0d\u540c\u7684\u5bb6\u65cf\u3001\u5927\u5c0f\u548c\u80fd\u529b\u6c34\u5e73\u3002\u5206\u6790\u653b\u51fb\u6210\u529f\u7387\u4e0e\u76ee\u6807\u548c\u653b\u51fb\u8005\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u4e0eMMLU-Pro\u57fa\u51c6\u7684\u793e\u4f1a\u79d1\u5b66\u90e8\u5206\u7684\u9ad8\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\u662f\u66f4\u597d\u7684\u653b\u51fb\u8005\uff0c\u653b\u51fb\u6210\u529f\u7387\u5728\u76ee\u6807\u80fd\u529b\u8d85\u8fc7\u653b\u51fb\u8005\u65f6\u6025\u5267\u4e0b\u964d\uff0c\u653b\u51fb\u6210\u529f\u7387\u4e0eMMLU-Pro\u57fa\u51c6\u7684\u793e\u4f1a\u79d1\u5b66\u90e8\u5206\u7684\u9ad8\u6027\u80fd\u76f8\u5173\u3002", "conclusion": "\u56fa\u5b9a\u80fd\u529b\u7684\u653b\u51fb\u8005\uff08\u5982\u4eba\u7c7b\uff09\u53ef\u80fd\u5728\u672a\u6765\u6a21\u578b\u4e2d\u53d8\u5f97\u65e0\u6548\uff0c\u5f00\u6e90\u6a21\u578b\u7684\u98ce\u9669\u589e\u52a0\uff0c\u6a21\u578b\u63d0\u4f9b\u8005\u5fc5\u987b\u51c6\u786e\u6d4b\u91cf\u548c\u63a7\u5236\u6a21\u578b\u7684\u8bf4\u670d\u548c\u64cd\u7eb5\u80fd\u529b\u4ee5\u9650\u5236\u5176\u4f5c\u4e3a\u653b\u51fb\u8005\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u641c\u7d22\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u53ef\u4ee5\u5728\u8d85\u96be\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\uff0c\u6311\u6218\u4e86\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u9650\u5236\u7684\u73b0\u6709\u5047\u8bbe\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u732e\u5927\u591a\u4f9d\u8d56\u4e8e\u76f4\u63a5\u63d0\u793a\u548c\u7b80\u5355\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u793a\u4f8b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u5148\u8fdb\u7684\u6280\u672f\u6765\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u601d\u719f\u8651\u63a8\u7406\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8fc7\u65e9\u5f97\u51fa\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u4e0a\u9650\u7684\u7ed3\u8bba\u3002", "method": "\u7cfb\u7edf\u5730\u63a2\u7d22\u4e0a\u4e0b\u6587\u641c\u7d22\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u5728\u8d85\u7ea7\u56f0\u96be\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8054\u5408\u6f5c\u529b\uff0c\u4f7f\u7528\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u641c\u7d22\u63d0\u793a\u548c\u5185\u90e8\u6269\u5c55\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u53d7\u63a7\u7684NP\u96be\u95ee\u9898\u548c\u590d\u6742\u7684\u73b0\u5b9e\u4e16\u754c\u89c4\u5212\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e4b\u524d\u62a5\u9053\u7684\u7ed3\u679c\uff08\u6ca1\u6709\u4efb\u4f55\u5916\u90e8\u673a\u5236\uff09\u6210\u529f\u7387\u8fbe\u523030\u500d\u7684\u63d0\u5347\uff0c\u5e76\u4e14\u7406\u8bba\u4e0a\u8bc1\u660e\u8fd9\u79cd\u65b9\u6cd5\u663e\u8457\u6269\u5c55\u4e86\u53ef\u89e3\u63a8\u7406\u95ee\u9898\u7684\u590d\u6742\u6027\u7c7b\u522b\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8bc4\u4f30\u8303\u5f0f\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\u4e86\u5b83\u4eec\u7684\u771f\u5b9e\u6f5c\u529b\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u5982\u4f55\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u91c7\u7528\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u7b56\u7565\u4ee5\u5168\u9762\u6355\u6349\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u771f\u5b9e\u80fd\u529b\u3002"}}
{"id": "2505.21666", "pdf": "https://arxiv.org/pdf/2505.21666", "abs": "https://arxiv.org/abs/2505.21666", "authors": ["Owen Oertell", "Shikun Sun", "Yiding Chen", "Jin Peng Zhou", "Zhiyong Wang", "Wen Sun"], "title": "Efficient Controllable Diffusion via Optimal Classifier Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 9 figures, 3 tables", "summary": "The controllable generation of diffusion models aims to steer the model to\ngenerate samples that optimize some given objective functions. It is desirable\nfor a variety of applications including image generation, molecule generation,\nand DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of\nthe base model is a popular approach but it can overfit the reward function\nwhile requiring significant resources. We frame controllable generation as a\nproblem of finding a distribution that optimizes a KL-regularized objective\nfunction. We present SLCD -- Supervised Learning based Controllable Diffusion,\nwhich iteratively generates online data and trains a small classifier to guide\nthe generation of the diffusion model. Similar to the standard\nclassifier-guided diffusion, SLCD's key computation primitive is classification\nand does not involve any complex concepts from RL or control. Via a reduction\nto no-regret online learning analysis, we show that under KL divergence, the\noutput from SLCD provably converges to the optimal solution of the\nKL-regularized objective. Further, we empirically demonstrate that SLCD can\ngenerate high quality samples with nearly the same inference time as the base\nmodel in both image generation with continuous diffusion and biological\nsequence generation with discrete diffusion. Our code is available at\nhttps://github.com/Owen-Oertell/slcd", "AI": {"tldr": "The paper introduces SLCD, a supervised learning approach for controllable generation in diffusion models. It optimizes a KL-regularized objective without overfitting and achieves high-quality sample generation with minimal inference time overhead.", "motivation": "Controllable generation in diffusion models is crucial for various applications but existing reinforcement learning methods can overfit and require significant resources.", "method": "SLCD frames controllable generation as finding a distribution optimizing a KL-regularized objective function. It uses supervised learning to iteratively generate data and train a small classifier to guide the diffusion model.", "result": "SLCD provably converges to the optimal solution under KL divergence and empirically generates high quality samples with similar inference time as the base model.", "conclusion": "SLCD offers an effective alternative to RL-based methods for controllable generation, providing theoretical guarantees and efficient performance."}}
{"id": "2505.21568", "pdf": "https://arxiv.org/pdf/2505.21568", "abs": "https://arxiv.org/abs/2505.21568", "authors": ["Haiyun Li", "Zhiyong Wu", "Xiaofeng Xie", "Jingran Xie", "Yaoxun Xu", "Hanyang Peng"], "title": "VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Voice cloning (VC)-resistant watermarking is an emerging technique for\ntracing and preventing unauthorized cloning. Existing methods effectively trace\ntraditional VC models by training them on watermarked audio but fail in\nzero-shot VC scenarios, where models synthesize audio from an audio prompt\nwithout training. To address this, we propose VoiceMark, the first zero-shot\nVC-resistant watermarking method that leverages speaker-specific latents as the\nwatermark carrier, allowing the watermark to transfer through the zero-shot VC\nprocess into the synthesized audio. Additionally, we introduce VC-simulated\naugmentations and VAD-based loss to enhance robustness against distortions.\nExperiments on multiple zero-shot VC models demonstrate that VoiceMark achieves\nover 95% accuracy in watermark detection after zero-shot VC synthesis,\nsignificantly outperforming existing methods, which only reach around 50%. See\nour code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark", "AI": {"tldr": "VoiceMark is the first zero-shot VC-resistant watermarking method that uses speaker-specific latents as the watermark carrier, achieving over 95% accuracy in watermark detection after zero-shot VC synthesis.", "motivation": "Existing methods for voice cloning (VC)-resistant watermarking fail in zero-shot VC scenarios, where models synthesize audio from an audio prompt without training.", "method": "VoiceMark leverages speaker-specific latents as the watermark carrier, allowing the watermark to transfer through the zero-shot VC process into the synthesized audio. Additionally, VC-simulated augmentations and VAD-based loss are introduced to enhance robustness against distortions.", "result": "Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves over 95% accuracy in watermark detection after zero-shot VC synthesis, significantly outperforming existing methods which only reach around 50%.", "conclusion": "VoiceMark is a novel and effective approach for zero-shot VC-resistant watermarking."}}
{"id": "2505.22311", "pdf": "https://arxiv.org/pdf/2505.22311", "abs": "https://arxiv.org/abs/2505.22311", "authors": ["Feibo Jiang", "Cunhua Pan", "Li Dong", "Kezhi Wang", "Octavia A. Dobre", "Merouane Debbah"], "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications", "categories": ["cs.AI", "cs.CY", "cs.NI", "eess.SP"], "comment": null, "summary": "With the advent of 6G communications, intelligent communication systems face\nmultiple challenges, including constrained perception and response\ncapabilities, limited scalability, and low adaptability in dynamic\nenvironments. This tutorial provides a systematic introduction to the\nprinciples, design, and applications of Large Artificial Intelligence Models\n(LAMs) and Agentic AI technologies in intelligent communication systems, aiming\nto offer researchers a comprehensive overview of cutting-edge technologies and\npractical guidance. First, we outline the background of 6G communications,\nreview the technological evolution from LAMs to Agentic AI, and clarify the\ntutorial's motivation and main contributions. Subsequently, we present a\ncomprehensive review of the key components required for constructing LAMs. We\nfurther categorize LAMs and analyze their applicability, covering Large\nLanguage Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models\n(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a\nLAM-centric design paradigm tailored for communications, encompassing dataset\nconstruction and both internal and external learning approaches. Building upon\nthis, we develop an LAM-based Agentic AI system for intelligent communications,\nclarifying its core components such as planners, knowledge bases, tools, and\nmemory modules, as well as its interaction mechanisms. We also introduce a\nmulti-agent framework with data retrieval, collaborative planning, and\nreflective evaluation for 6G. Subsequently, we provide a detailed overview of\nthe applications of LAMs and Agentic AI in communication scenarios. Finally, we\nsummarize the research challenges and future directions in current studies,\naiming to support the development of efficient, secure, and sustainable\nnext-generation intelligent communication systems.", "AI": {"tldr": "\u968f\u77406G\u901a\u4fe1\u7684\u5230\u6765\uff0c\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u9762\u4e34\u591a\u91cd\u6311\u6218\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u5927\u578b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff08LAMs\uff09\u548c\u4ee3\u7406\u578bAI\u6280\u672f\u5728\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u539f\u5219\u3001\u8bbe\u8ba1\u548c\u5e94\u7528\u7684\u7cfb\u7edf\u6027\u4ecb\u7ecd\u3002\u9996\u5148\u6982\u8ff0\u4e866G\u901a\u4fe1\u7684\u80cc\u666f\uff0c\u56de\u987e\u4e86\u4eceLAMs\u5230\u4ee3\u7406\u578bAI\u7684\u6280\u672f\u6f14\u53d8\uff0c\u5e76\u660e\u786e\u4e86\u6559\u7a0b\u7684\u52a8\u673a\u548c\u4e3b\u8981\u8d21\u732e\u3002\u63a5\u7740\u5168\u9762\u56de\u987e\u4e86\u6784\u5efaLAMs\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5206\u7c7b\u5206\u6790\u4e86\u4e0d\u540c\u7c7b\u578b\u7684LAMs\u53ca\u5176\u9002\u7528\u6027\u3002\u7136\u540e\u63d0\u51fa\u4e86\u4ee5LAM\u4e3a\u6838\u5fc3\u7684\u901a\u4fe1\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5305\u62ec\u6570\u636e\u96c6\u6784\u5efa\u548c\u5185\u5916\u90e8\u5b66\u4e60\u65b9\u6cd5\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLAM\u7684\u4ee3\u7406\u578bAI\u7cfb\u7edf\uff0c\u4ecb\u7ecd\u4e86\u5176\u6838\u5fc3\u7ec4\u4ef6\u53ca\u4ea4\u4e92\u673a\u5236\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u6db5\u76d6\u4e866G\u7684\u6570\u636e\u68c0\u7d22\u3001\u534f\u4f5c\u89c4\u5212\u548c\u53cd\u601d\u8bc4\u4f30\u3002\u6700\u540e\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u652f\u6301\u4e0b\u4e00\u4ee3\u9ad8\u6548\u3001\u5b89\u5168\u548c\u53ef\u6301\u7eed\u7684\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u5728\u611f\u77e5\u4e0e\u54cd\u5e94\u80fd\u529b\u3001\u53ef\u6269\u5c55\u6027\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u5148\u8fdb\u7684\u6280\u672f\u652f\u6301\u4ee5\u5e94\u5bf96G\u901a\u4fe1\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "1. \u7cfb\u7edf\u6027\u4ecb\u7ecdLAMs\u548c\u4ee3\u7406\u578bAI\u6280\u672f\uff1b2. \u56de\u987e\u6280\u672f\u6f14\u53d8\u5e76\u5206\u6790LAMs\u7684\u5173\u952e\u7ec4\u4ef6\uff1b3. \u63d0\u51fa\u4ee5LAM\u4e3a\u6838\u5fc3\u7684\u901a\u4fe1\u8bbe\u8ba1\u8303\u5f0f\uff1b4. \u5f00\u53d1\u57fa\u4e8eLAM\u7684\u4ee3\u7406\u578bAI\u7cfb\u7edf\uff1b5. \u5f15\u5165\u591a\u4ee3\u7406\u6846\u67b6\uff1b6. \u6982\u8ff0LAMs\u548c\u4ee3\u7406\u578bAI\u5728\u901a\u4fe1\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff1b7. \u603b\u7ed3\u7814\u7a76\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9LAMs\u548c\u4ee3\u7406\u578bAI\u6280\u672f\u7684\u5168\u9762\u7406\u89e3\uff0c\u660e\u786e\u4e86\u5b83\u4eec\u5728\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528LAMs\u548c\u4ee3\u7406\u578bAI\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u5e94\u5bf96G\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6311\u6218\uff0c\u63a8\u52a8\u9ad8\u6548\u3001\u5b89\u5168\u548c\u53ef\u6301\u7eed\u7684\u4e0b\u4e00\u4ee3\u667a\u80fd\u901a\u4fe1\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.21677", "pdf": "https://arxiv.org/pdf/2505.21677", "abs": "https://arxiv.org/abs/2505.21677", "authors": ["Hung Ahn Vu", "Galen Reeves", "Emily Wenger"], "title": "What happens when generative AI models train recursively on each others' generated outputs?", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "9 pages", "summary": "The internet is full of AI-generated content while also serving as a common\nsource of training data for generative AI (genAI) models. This duality raises\nthe possibility that future genAI models may be trained on other models'\ngenerated outputs. Prior work has studied consequences of models training on\ntheir own generated outputs, but limited work has considered what happens if\nmodels ingest content produced by other models. Given society's increasing\ndependence on genAI tools, understanding downstream effects of such\ndata-mediated model interactions is critical. To this end, we provide empirical\nevidence for how data-mediated interactions might unfold in practice, develop a\ntheoretical model for this interactive training process, and show\nexperimentally possible long-term results of such interactions. We find that\ndata-mediated interactions can benefit models by exposing them to novel\nconcepts perhaps missed in original training data, but also can homogenize\ntheir performance on shared tasks.", "AI": {"tldr": "The paper explores the consequences of generative AI models being trained on outputs generated by other AI models, providing empirical evidence and a theoretical model for this interactive training process, revealing both benefits and potential homogenization effects.", "motivation": "To understand the downstream effects of genAI models being trained on outputs from other generative AI models, given society's increasing reliance on these tools.", "method": "Provide empirical evidence of data-mediated interactions among genAI models, develop a theoretical model to describe the interactive training process, and experimentally demonstrate possible long-term results of such interactions.", "result": "Data-mediated interactions can expose genAI models to novel concepts not present in their original training data, potentially benefiting them. However, these interactions can also homogenize model performance on shared tasks.", "conclusion": "Understanding the impacts of genAI models trained on other models' outputs is critical as it may bring benefits but also lead to homogenized performance."}}
{"id": "2505.22368", "pdf": "https://arxiv.org/pdf/2505.22368", "abs": "https://arxiv.org/abs/2505.22368", "authors": ["Enfang Cui", "Yujun Cheng", "Rui She", "Dan Liu", "Zhiyuan Liang", "Minxin Guo", "Tianzheng Li", "Qian Wei", "Wenjuan Xing", "Zhijie Zhong"], "title": "AgentDNS: A Root Domain Naming System for LLM Agents", "categories": ["cs.AI"], "comment": "7 pages, 6 figures", "summary": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns.", "AI": {"tldr": "This paper proposes AgentDNS, a system inspired by traditional DNS, to enable LLM agents to autonomously discover, resolve, and securely invoke third-party services across different vendors.", "motivation": "The rapid evolution of LLM agents has brought challenges in cross-vendor service discovery, interoperability, and communication. Current protocols have not fully addressed the need for standardized solutions in this area.", "method": "AgentDNS is proposed as a root domain naming and service discovery system. It includes mechanisms for service registration, semantic service discovery, secure invocation, and unified billing, all designed to facilitate multi-agent collaboration.", "result": "The architecture, core functionalities, and use cases of AgentDNS are detailed, showing its potential to improve multi-agent collaboration in real-world scenarios.", "conclusion": "AgentDNS aims to address the lack of standardized protocols for service discovery across different agent and tool vendors, and its source code will be published on GitHub."}}
{"id": "2505.21680", "pdf": "https://arxiv.org/pdf/2505.21680", "abs": "https://arxiv.org/abs/2505.21680", "authors": ["Andrew J. Loza", "Jun Yup Kim", "Shangzheng Song", "Yihang Liu", "Joseph J. Y. Sung", "R Andrew Taylor", "Dennis L. Shung"], "title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; I.5.1"], "comment": "15 pates, 5 figures", "summary": "Real-world processes often generate data that are a mix of categorical and\nnumeric values that are recorded at irregular and informative intervals.\nDiscrete token-based approaches are limited in numeric representation capacity\nwhile methods like neural ordinary differential equations are not well suited\nfor categorical data or informative sampling and require augmentation to handle\ncertain classes of trajectories. Here, we present multivariateGPT, a single\narchitecture for modeling sequences of mixed categorical (including tokenized\ntext) and numeric data. This is accomplished with an autoregressive sequence\ndecomposition, embedding scheme, and loss function that extend the next token\nprediction task to likelihood estimation of the joint distribution of next\ntoken class and value. We demonstrate how this approach can efficiently learn\nto generalize patterns in simple physical systems and model complex time series\nincluding electrocardiograms and multivariate electronic health record data.\nThis work extends the utility of transformer based models to additional classes\nof data.", "AI": {"tldr": "multivariateGPT\u662f\u4e00\u79cd\u5355\u4e00\u67b6\u6784\uff0c\u7528\u4e8e\u5bf9\u6df7\u5408\u7c7b\u522b\uff08\u5305\u62ec\u6807\u8bb0\u5316\u6587\u672c\uff09\u548c\u6570\u503c\u6570\u636e\u7684\u5e8f\u5217\u8fdb\u884c\u5efa\u6a21\u3002\u5b83\u901a\u8fc7\u81ea\u56de\u5f52\u5e8f\u5217\u5206\u89e3\u3001\u5d4c\u5165\u65b9\u6848\u548c\u635f\u5931\u51fd\u6570\u6765\u6269\u5c55\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u4f30\u8ba1\u4e0b\u4e00\u4e2a\u6807\u8bb0\u7c7b\u522b\u548c\u503c\u7684\u8054\u5408\u5206\u5e03\u7684\u4f3c\u7136\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u7b80\u5355\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u6a21\u5f0f\uff0c\u5e76\u5bf9\u590d\u6742\u7684\u65f6\u95f4\u5e8f\u5217\uff08\u5982\u5fc3\u7535\u56fe\u548c\u591a\u53d8\u91cf\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff09\u8fdb\u884c\u5efa\u6a21\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8fc7\u7a0b\u901a\u5e38\u751f\u6210\u6df7\u5408\u7c7b\u522b\u548c\u6570\u503c\u7684\u6570\u636e\uff0c\u5e76\u4e14\u5728\u4e0d\u89c4\u5219\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u95f4\u9694\u5185\u8bb0\u5f55\u3002\u73b0\u6709\u7684\u79bb\u6563\u6807\u8bb0\u65b9\u6cd5\u5728\u6570\u503c\u8868\u793a\u80fd\u529b\u4e0a\u6709\u9650\uff0c\u800c\u50cf\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u8fd9\u6837\u7684\u65b9\u6cd5\u5bf9\u4e8e\u7c7b\u522b\u6570\u636e\u6216\u4fe1\u606f\u91c7\u6837\u5e76\u4e0d\u9002\u5408\uff0c\u9700\u8981\u589e\u5f3a\u624d\u80fd\u5904\u7406\u67d0\u4e9b\u7c7b\u522b\u7684\u8f68\u8ff9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3amultivariateGPT\u7684\u5355\u4e00\u67b6\u6784\uff0c\u7528\u4e8e\u5bf9\u6df7\u5408\u7c7b\u522b\uff08\u5305\u62ec\u6807\u8bb0\u5316\u6587\u672c\uff09\u548c\u6570\u503c\u6570\u636e\u7684\u5e8f\u5217\u8fdb\u884c\u5efa\u6a21\u3002\u901a\u8fc7\u81ea\u56de\u5f52\u5e8f\u5217\u5206\u89e3\u3001\u5d4c\u5165\u65b9\u6848\u548c\u635f\u5931\u51fd\u6570\u6765\u6269\u5c55\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u4f30\u8ba1\u4e0b\u4e00\u4e2a\u6807\u8bb0\u7c7b\u522b\u548c\u503c\u7684\u8054\u5408\u5206\u5e03\u7684\u4f3c\u7136\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u7b80\u5355\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u6a21\u5f0f\uff0c\u5e76\u5bf9\u590d\u6742\u7684\u65f6\u95f4\u5e8f\u5217\uff08\u5982\u5fc3\u7535\u56fe\u548c\u591a\u53d8\u91cf\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff09\u8fdb\u884c\u5efa\u6a21\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6269\u5c55\u4e86\u57fa\u4e8e\u53d8\u538b\u5668\u6a21\u578b\u7684\u6548\u7528\u5230\u66f4\u591a\u7684\u6570\u636e\u7c7b\u578b\u3002"}}
{"id": "2505.22451", "pdf": "https://arxiv.org/pdf/2505.22451", "abs": "https://arxiv.org/abs/2505.22451", "authors": ["Yuanhang Liu", "Yanxing Huang", "Yanqiao Wang", "Peng Li", "Yang Liu"], "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research", "categories": ["cs.AI"], "comment": "95 pages, 1 figure", "summary": "Large Reasoning Models (LRMs) have made significant progress in mathematical\ncapabilities in recent times. However, these successes have been primarily\nconfined to competition-level problems. In this work, we propose AI\nMathematician (AIM) framework, which harnesses the reasoning strength of LRMs\nto support frontier mathematical research. We have identified two critical\nchallenges of mathematical research compared to competition, {\\it the intrinsic\ncomplexity of research problems} and {\\it the requirement of procedural rigor}.\nTo address these challenges, AIM incorporates two core strategies: an\nexploration mechanism to foster longer solution paths, and the pessimistic\nreasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling\nresearch-level tasks. We conducted extensive experiments across several\nreal-world mathematical topics and obtained promising results. AIM is able to\nautonomously construct substantial portions of proofs and uncover non-trivial\ninsights within each research area. These findings highlight the potential of\nLRMs in mathematical discovery and suggest that LRM-based agent systems could\nsignificantly accelerate mathematical research in the future.", "AI": {"tldr": "This paper introduces AI Mathematician (AIM) framework that uses Large Reasoning Models (LRMs) to aid in frontier mathematical research. AIM addresses the complexity of research problems and requirement of procedural rigor through an exploration mechanism and pessimistic reasonable verification method. Experiments show AIM's strong capability in constructing proofs and discovering insights in real-world mathematical topics, suggesting the potential for LRM-based systems to accelerate future mathematical research.", "motivation": "To utilize the reasoning strength of LRMs for supporting frontier mathematical research beyond competition-level problems, addressing the intrinsic complexity of research problems and the requirement of procedural rigor.", "method": "The AIM framework incorporates two core strategies: an exploration mechanism fostering longer solution paths and a pessimistic reasonable verification method ensuring reliability.", "result": "AIM shows strong capability in tackling research-level tasks, autonomously constructing substantial portions of proofs and uncovering non-trivial insights in several real-world mathematical topics.", "conclusion": "The findings highlight the potential of LRMs in mathematical discovery and suggest that LRM-based agent systems could significantly accelerate mathematical research in the future."}}
{"id": "2505.21684", "pdf": "https://arxiv.org/pdf/2505.21684", "abs": "https://arxiv.org/abs/2505.21684", "authors": ["Joel Lidin", "Amir Sarfi", "Evangelos Pappas", "Samuel Dare", "Eugene Belilovsky", "Jacob Steeves"], "title": "Incentivizing Permissionless Distributed Learning of LLMs", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We describe an incentive system for distributed deep learning of foundational\nmodels where peers are rewarded for contributions. The incentive system,\n\\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to\ntrain a 1.2B LLM with completely permissionless contributions of\npseudo-gradients: no control over the users that can register or their\nhardware. \\textit{Gauntlet} can be applied to any synchronous distributed\ntraining scheme that relies on aggregating updates or pseudo-gradients. We rely\non a two-stage mechanism for fast filtering of peer uptime, reliability, and\nsynchronization, combined with the core component that estimates the loss\nbefore and after individual pseudo-gradient contributions. We utilized an\nOpenSkill rating system to track competitiveness of pseudo-gradient scores\nacross time. Finally, we introduce a novel mechanism to ensure peers on the\nnetwork perform unique computations. Our live 1.2B run, which has paid out\nreal-valued tokens to participants based on the value of their contributions,\nyielded a competitive (on a per-iteration basis) 1.2B model that demonstrates\nthe utility of our incentive system.", "AI": {"tldr": "The paper introduces Gauntlet, an incentive system for distributed deep learning deployed on the bittensor blockchain, which successfully trained a 1.2B LLM with permissionless contributions.", "motivation": "To create an effective incentive system that encourages contributions in distributed deep learning of foundational models without controlling participant access or hardware.", "method": "Deployed Gauntlet on the bittensor blockchain, using a two-stage mechanism for filtering peer uptime, reliability, and synchronization, along with estimating loss changes from pseudo-gradient contributions and utilizing an OpenSkill rating system.", "result": "Successfully trained a competitive 1.2B LLM with real-valued token payouts to participants based on their contributions.", "conclusion": "Gauntlet demonstrates utility as an incentive system in distributed deep learning by enabling successful training of a large language model with permissionless contributions."}}
{"id": "2505.21726", "pdf": "https://arxiv.org/pdf/2505.21726", "abs": "https://arxiv.org/abs/2505.21726", "authors": ["Nitin Jha", "Abhishek Parakh", "Mahadevan Subramaniam"], "title": "Multi-photon QKD for Practical Quantum Networks", "categories": ["quant-ph", "cs.CR", "cs.NI"], "comment": null, "summary": "Quantum key distribution (QKD) will most likely be an integral part of any\npractical quantum network in the future. However, not all QKD protocols can be\nused in today's networks because of the lack of single-photon emitters and\nnoisy intermediate quantum hardware. Attenuated-photon transmission, typically\nused to simulate single-photon emitters, severely limits the achievable\ntransmission distances and makes the integration of the QKD into existing\nclassical networks, that use tens of thousands of photons per bit of\ntransmission, difficult. Furthermore, it has been found that protocol\nperformance varies with topology. In order to remove the reliance of QKD on\nsingle-photon emitters and increase transmission distances, it is worthwhile to\nexplore QKD protocols that do not rely on single-photon transmissions for\nsecurity, such as the 3-stage QKD protocol, which can tolerate multiple photons\nin each burst without information leakage. This paper compares and contrasts\nthe 3-stage QKD protocol with conventional QKD protocols and its efficiency in\ndifferent network topologies and conditions. Furthermore, we establish a\nmathematical relationship between achievable key rates to increase transmission\ndistances in various topologies.", "AI": {"tldr": "The paper explores the 3-stage QKD protocol, comparing it with conventional protocols and analyzing its efficiency across different network topologies.", "motivation": "To address the limitations of current QKD protocols due to lack of single-photon emitters and noisy hardware, as well as improving transmission distances and integration into classical networks.", "method": "Compare and contrast the 3-stage QKD protocol with conventional ones; analyze their performance in various network conditions and topologies; establish a mathematical relationship for key rates to extend transmission distances.", "result": "Provides insights on how the 3-stage QKD protocol can be more effective in certain topologies and conditions, allowing for longer transmission distances without relying on single-photon transmissions.", "conclusion": "The 3-stage QKD protocol shows potential for practical implementation in future quantum networks by overcoming current limitations."}}
{"id": "2505.22597", "pdf": "https://arxiv.org/pdf/2505.22597", "abs": "https://arxiv.org/abs/2505.22597", "authors": ["Ngoc La", "Ruaridh Mon-Williams", "Julie A. Shah"], "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted to Proceedings of ICAPS 2025", "summary": "In recent years, reinforcement learning (RL) methods have been widely tested\nusing tools like OpenAI Gym, though many tasks in these environments could also\nbenefit from hierarchical planning. However, there is a lack of a tool that\nenables seamless integration of hierarchical planning with RL. Hierarchical\nDomain Definition Language (HDDL), used in classical planning, introduces a\nstructured approach well-suited for model-based RL to address this gap. To\nbridge this integration, we introduce HDDLGym, a Python-based tool that\nautomatically generates OpenAI Gym environments from HDDL domains and problems.\nHDDLGym serves as a link between RL and hierarchical planning, supporting\nmulti-agent scenarios and enabling collaborative planning among agents. This\npaper provides an overview of HDDLGym's design and implementation, highlighting\nthe challenges and design choices involved in integrating HDDL with the Gym\ninterface, and applying RL policies to support hierarchical planning. We also\nprovide detailed instructions and demonstrations for using the HDDLGym\nframework, including how to work with existing HDDL domains and problems from\nInternational Planning Competitions, exemplified by the Transport domain.\nAdditionally, we offer guidance on creating new HDDL domains for multi-agent\nscenarios and demonstrate the practical use of HDDLGym in the Overcooked\ndomain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a\nvaluable tool for studying RL in hierarchical planning, particularly in\nmulti-agent contexts.", "AI": {"tldr": "The paper introduces HDDLGym, a tool that integrates hierarchical planning via HDDL with reinforcement learning environments from OpenAI Gym, supporting multi-agent scenarios and collaborative planning.", "motivation": "There is a need for seamless integration of hierarchical planning with reinforcement learning (RL) methods, as current tools lack this capability.", "method": "HDDLGym is developed to automatically generate OpenAI Gym environments from HDDL domains and problems, bridging the gap between RL and hierarchical planning. It supports multi-agent scenarios and collaborative planning among agents.", "result": "The paper provides an overview of HDDLGym's design and implementation, along with detailed instructions and demonstrations for using the framework in various domains, such as Transport and Overcooked.", "conclusion": "HDDLGym is positioned as a valuable tool for studying reinforcement learning within the context of hierarchical planning, especially in multi-agent settings."}}
{"id": "2505.21695", "pdf": "https://arxiv.org/pdf/2505.21695", "abs": "https://arxiv.org/abs/2505.21695", "authors": ["Ganglou Xu"], "title": "AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated learning faces critical challenges in balancing communication\nefficiency and model accuracy. One key issue lies in the approximation of\nupdate errors without incurring high computational costs. In this paper, we\npropose a lightweight yet effective method called Gradient Difference\nApproximation (GDA), which leverages first-order information to estimate local\nerror trends without computing the full Hessian matrix. The proposed method\nforms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)\nframework and provides a unified error modeling strategy for large-scale\nmulti-step adaptive training environments.", "AI": {"tldr": "Federated learning struggles with balancing communication efficiency and model accuracy. This paper proposes Gradient Difference Approximation (GDA), a lightweight method using first-order info to estimate local error trends, part of the AMSFL framework for adaptive multi-step training.", "motivation": "The motivation is to address the challenge in federated learning of approximating update errors without high computational costs, specifically focusing on improving communication efficiency and model accuracy.", "method": "The method proposed is called Gradient Difference Approximation (GDA), which uses first-order information to estimate local error trends without computing the full Hessian matrix. It is integrated into the Adaptive Multi-Step Federated Learning (AMSFL) framework.", "result": "The paper does not explicitly mention results in the provided abstract.", "conclusion": "GDA provides a unified error modeling strategy for large-scale multi-step adaptive training environments within the AMSFL framework."}}
{"id": "2505.21733", "pdf": "https://arxiv.org/pdf/2505.21733", "abs": "https://arxiv.org/abs/2505.21733", "authors": ["Taein Kim", "Karstan Bock", "Claire Luo", "Amanda Liswood", "Emily Wenger"], "title": "Scrapers selectively respect robots.txt directives: evidence from a large-scale empirical study", "categories": ["cs.NI", "cs.CR", "cs.CY"], "comment": "13 pages", "summary": "Online data scraping has taken on new dimensions in recent years, as\ntraditional scrapers have been joined by new AI-specific bots. To counteract\nunwanted scraping, many sites use tools like the Robots Exclusion Protocol\n(REP), which places a robots.txt file at the site root to dictate scraper\nbehavior. Yet, the efficacy of the REP is not well-understood. Anecdotal\nevidence suggests some bots comply poorly with it, but no rigorous study exists\nto support (or refute) this claim. To understand the merits and limits of the\nREP, we conduct the first large-scale study of web scraper compliance with\nrobots.txt directives using anonymized web logs from our institution. We\nanalyze the behavior of 130 self-declared bots (and many anonymous ones) over\n40 days, using a series of controlled robots.txt experiments. We find that bots\nare less likely to comply with stricter robots.txt directives, and that certain\ncategories of bots, including AI search crawlers, rarely check robots.txt at\nall. These findings suggest that relying on robots.txt files to prevent\nunwanted scraping is risky and highlight the need for alternative approaches.", "AI": {"tldr": "The paper conducts a large-scale study on how well web scrapers comply with robots.txt directives using anonymized web logs, finding that stricter directives see less compliance, especially from AI search crawlers, thus suggesting the need for alternative approaches.", "motivation": "To understand the efficacy of the Robots Exclusion Protocol (REP) and whether bots comply with robots.txt directives.", "method": "Analyze the behavior of 130 self-declared bots and many anonymous ones over 40 days using controlled robots.txt experiments based on anonymized web logs from the institution.", "result": "Bots are less likely to comply with stricter robots.txt directives. AI search crawlers rarely check robots.txt.", "conclusion": "Relying on robots.txt to prevent unwanted scraping is risky and there is a need for alternative approaches."}}
{"id": "2404.11045", "pdf": "https://arxiv.org/pdf/2404.11045", "abs": "https://arxiv.org/abs/2404.11045", "authors": ["James Y. Huang", "Wenxuan Zhou", "Fei Wang", "Fred Morstatter", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Offset Unlearning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in TMLR. https://openreview.net/pdf?id=A4RLpHPXCu", "summary": "Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, biased, and private content has\nled to ethical and legal concerns. In response to these challenges, unlearning\nhas emerged as a potential remedy for LLMs affected by problematic training\ndata. However, previous unlearning techniques are either not applicable to\nblack-box LLMs due to required access to model internal weights, or violate\ndata protection principles by retaining sensitive data for inference-time\ncorrection. We propose {\\delta}-Unlearning, an offset unlearning framework for\nblack-box LLMs. Instead of tuning the black-box LLM itself, {\\delta}-Unlearning\nlearns the logit offset needed for unlearning by contrasting the logits from a\npair of smaller models. Experiments demonstrate that {\\delta}- Unlearning can\neffectively unlearn target data while maintaining similar or even stronger\nperformance on general out-of-forget-scope tasks. {\\delta}-Unlearning also\neffectively incorporates different unlearning algorithms, making our approach a\nversatile solution to adapting various existing unlearning algorithms to\nblack-box LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u03b4-Unlearning\u7684\u504f\u79fb\u9057\u5fd8\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u9057\u5fd8\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u8f83\u5c0f\u6a21\u578b\u5bf9\u7684logit\u504f\u79fb\u91cf\u6765\u5b9e\u73b0\u9057\u5fd8\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u6743\u91cd\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4fdd\u7559\u654f\u611f\u6570\u636e\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u6548\u9057\u5fd8\u76ee\u6807\u6570\u636e\u7684\u540c\u65f6\uff0c\u8fd8\u80fd\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e00\u822c\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u7ed3\u5408\u4e0d\u540c\u7684\u9057\u5fd8\u7b97\u6cd5\uff0c\u4e3a\u9002\u5e94\u73b0\u6709\u5404\u79cd\u9057\u5fd8\u7b97\u6cd5\u5230\u9ed1\u76d2LLMs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b\uff0c\u4f46\u5176\u5bf9\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u654f\u611f\u4fe1\u606f\u7684\u8bb0\u5fc6\uff08\u5982\u53d7\u7248\u6743\u4fdd\u62a4\u3001\u5b58\u5728\u504f\u5dee\u6216\u79c1\u5bc6\u7684\u5185\u5bb9\uff09\u5f15\u53d1\u4e86\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\u3002\u5df2\u6709\u9057\u5fd8\u6280\u672f\u8981\u4e48\u65e0\u6cd5\u9002\u7528\u4e8e\u9ed1\u76d2LLMs\uff08\u56e0\u4e3a\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u6743\u91cd\uff09\uff0c\u8981\u4e48\u8fdd\u53cd\u6570\u636e\u4fdd\u62a4\u539f\u5219\uff08\u901a\u8fc7\u4fdd\u7559\u654f\u611f\u6570\u636e\u8fdb\u884c\u63a8\u7406\u65f6\u6821\u6b63\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u03b4-Unlearning\uff0c\u4e00\u79cd\u9488\u5bf9\u9ed1\u76d2LLMs\u7684\u504f\u79fb\u9057\u5fd8\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u4e0d\u76f4\u63a5\u8c03\u6574\u9ed1\u76d2LLM\u672c\u8eab\uff0c\u800c\u662f\u901a\u8fc7\u5bf9\u6bd4\u4e00\u5bf9\u8f83\u5c0f\u6a21\u578b\u7684logits\uff0c\u5b66\u4e60\u6240\u9700\u7684logit\u504f\u79fb\u91cf\u4ee5\u5b9e\u73b0\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u03b4-Unlearning\u80fd\u591f\u5728\u6709\u6548\u9057\u5fd8\u76ee\u6807\u6570\u636e\u7684\u540c\u65f6\uff0c\u7ef4\u6301\u751a\u81f3\u589e\u5f3a\u5728\u4e00\u822c\u975e\u9057\u5fd8\u8303\u56f4\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u6574\u5408\u4e0d\u540c\u7684\u9057\u5fd8\u7b97\u6cd5\uff0c\u6210\u4e3a\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u03b4-Unlearning\u4e3a\u9ed1\u76d2LLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u80fd\u5e76\u652f\u6301\u591a\u79cd\u9057\u5fd8\u7b97\u6cd5\u7684\u96c6\u6210\u3002"}}
{"id": "2505.21717", "pdf": "https://arxiv.org/pdf/2505.21717", "abs": "https://arxiv.org/abs/2505.21717", "authors": ["M\u00f3nika Farsang", "Ramin Hasani", "Radu Grosu"], "title": "Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "We present LrcSSM, a \\textit{nonlinear} recurrent model that processes long\nsequences as fast as today's linear state-space layers. By forcing the\nstate-transition matrix to be diagonal and learned at every step, the full\nsequence can be solved in parallel with a single prefix-scan, giving\n$\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$ sequential\ndepth, for input-sequence length $T$ and a state dimension $D$. Moreover,\nLrcSSM offers a formal gradient-stability guarantee that other input-varying\nsystems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth\n$L$, as the forward and backward passes cost $\\Theta(T\\,D\\,L)$ FLOPs, with its\nlow sequential depth and parameter count $\\Theta(D\\,L)$, the model follows the\ncompute-optimal scaling law regime ($\\beta \\approx 0.42$) recently observed for\nMamba, outperforming quadratic-attention Transformers at equal compute while\navoiding the memory overhead of FFT-based long convolutions. We show that on a\nseries of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.21756", "pdf": "https://arxiv.org/pdf/2505.21756", "abs": "https://arxiv.org/abs/2505.21756", "authors": ["Bernardo A. Huberman", "Jing Wang"], "title": "Online Voting using Point to MultiPoint Quantum Key Distribution via Passive Optical Networks", "categories": ["quant-ph", "cs.CR", "cs.ET"], "comment": null, "summary": "We propose using Point-to-Multipoint quantum key distribution (QKD) via time\ndivision multiplexing (TDM) and wavelength division multiplexing (WDM) in\npassive optical networks (PON) to improve the security of online voting\nsystems.", "AI": {"tldr": "This paper proposes a method using Point-to-Multipoint QKD through TDM and WDM in PON for enhancing the security of online voting systems.", "motivation": "To improve the security of online voting systems.", "method": "Propose to use Point-to-Multipoint QKD with TDM and WDM in PON.", "result": "The security of online voting systems is improved by implementing this method.", "conclusion": "Using Point-to-Multipoint QKD via TDM and WDM in PON can enhance the security of online voting systems."}}
{"id": "2505.21506", "pdf": "https://arxiv.org/pdf/2505.21506", "abs": "https://arxiv.org/abs/2505.21506", "authors": ["Eli Bogdanov", "Izack Cohen", "Avigdor Gal"], "title": "Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences", "categories": ["cs.DB", "cs.AI", "cs.PL"], "comment": "17 pages, 4 figures", "summary": "Long event sequences (termed traces) and large data logs that originate from\nsensors and prediction models are becoming increasingly common in our data-rich\nworld. In such scenarios, conformance checking-validating a data log against an\nexpected system behavior (the process model) can become computationally\ninfeasible due to the exponential complexity of finding an optimal alignment.\nTo alleviate scalability challenges for this task, we propose ConLES, a\nsliding-window conformance checking approach for long event sequences that\npreserves the interpretability of alignment-based methods. ConLES partitions\ntraces into manageable subtraces and iteratively aligns each against the\nexpected behavior, leading to significant reduction of the search space while\nmaintaining overall accuracy. We use global information that captures\nstructural properties of both the trace and the process model, enabling\ninformed alignment decisions and discarding unpromising alignments, even if\nthey appear locally optimal. Performance evaluations across multiple datasets\nhighlight that ConLES outperforms the leading optimal and heuristic algorithms\nfor long traces, consistently achieving the optimal or near-optimal solution.\nUnlike other conformance methods that struggle with long event sequences,\nConLES significantly reduces the search space, scales efficiently, and uniquely\nsupports both predefined and discovered process models, making it a viable and\nleading option for conformance checking of long event sequences.", "AI": {"tldr": "In order to solve the computational infeasibility of conformance checking for long event sequences, this paper proposes ConLES. It partitions traces into subtraces and uses global information for alignment, significantly reducing the search space while maintaining accuracy.", "motivation": "Conformance checking for long event sequences and large data logs can be computationally infeasible due to the exponential complexity of finding an optimal alignment.", "method": "Propose ConLES, a sliding-window conformance checking approach that partitions traces into manageable subtraces and iteratively aligns each against the expected behavior using global information capturing structural properties of both the trace and the process model.", "result": "ConLES outperforms leading optimal and heuristic algorithms for long traces, achieving optimal or near-optimal solutions while significantly reducing the search space.", "conclusion": "ConLES is a viable and leading option for conformance checking of long event sequences as it scales efficiently and supports both predefined and discovered process models."}}
{"id": "2505.21722", "pdf": "https://arxiv.org/pdf/2505.21722", "abs": "https://arxiv.org/abs/2505.21722", "authors": ["Ioannis Bantzis", "James B. Simon", "Arthur Jacot"], "title": "Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When a deep ReLU network is initialized with small weights, GD is at first\ndominated by the saddle at the origin in parameter space. We study the\nso-called escape directions, which play a similar role as the eigenvectors of\nthe Hessian for strict saddles. We show that the optimal escape direction\nfeatures a low-rank bias in its deeper layers: the first singular value of the\n$\\ell$-th layer weight matrix is at least $\\ell^{\\frac{1}{4}}$ larger than any\nother singular value. We also prove a number of related results about these\nescape directions. We argue that this result is a first step in proving\nSaddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of\nsaddles with increasing bottleneck rank.", "AI": {"tldr": "In deep ReLU networks initialized with small weights, the optimal escape direction from the saddle at the origin has a low-rank bias in deeper layers, marking a step towards understanding Saddle-to-Saddle dynamics.", "motivation": "To understand the behavior of Gradient Descent (GD) in deep ReLU networks when initialized with small weights and to study the escape directions from the saddle point at the origin.", "method": "Analyze the escape directions in the context of deep ReLU networks, focusing on the properties of these directions and their relation to the singular values of weight matrices in different layers.", "result": "The optimal escape direction exhibits a low-rank bias in its deeper layers; specifically, the first singular value of the $\\ell$-th layer weight matrix is significantly larger than others.", "conclusion": "This finding represents an initial step in proving Saddle-to-Saddle dynamics in deep ReLU networks, where GD transitions through a sequence of saddles with increasing bottleneck rank."}}
{"id": "2505.21938", "pdf": "https://arxiv.org/pdf/2505.21938", "abs": "https://arxiv.org/abs/2505.21938", "authors": ["Qirun Zeng", "Eric He", "Richard Hoffmann", "Xuchuang Wang", "Jinhang Zuo"], "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios.", "AI": {"tldr": "The paper explores a practical adversarial threat model called Fake Data Injection for stochastic bandits, demonstrating its ability to mislead UCB and Thompson Sampling algorithms with low cost, supported by theoretical analysis and experiments.", "motivation": "Existing adversarial attacks on stochastic bandits rely on unrealistic assumptions such as per-round reward manipulation and unbounded perturbations, which limits their applicability to real-world systems. There is a need for more realistic attack models that consider the constraints faced by potential adversaries.", "method": "The authors propose the Fake Data Injection threat model where an attacker can inject a limited number of bounded fake feedback samples into the learner's history. They design efficient attack strategies considering both magnitude (reward values) and temporal (timing and frequency) constraints. Theoretical analysis and experimental validation are conducted to evaluate the effectiveness of these strategies on UCB and Thompson Sampling algorithms.", "result": "The proposed attack strategies can successfully mislead UCB and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets confirm the significant vulnerabilities of stochastic bandit algorithms under practical adversarial scenarios.", "conclusion": "The study highlights the vulnerability of widely used stochastic bandit algorithms to adversarial attacks under realistic constraints through the Fake Data Injection model. This calls for further research into robustifying these algorithms against such practical threats."}}
{"id": "2505.21513", "pdf": "https://arxiv.org/pdf/2505.21513", "abs": "https://arxiv.org/abs/2505.21513", "authors": ["Nicolas Echevarrieta-Catalan", "Ana Ribas-Rodriguez", "Francisco Cedron", "Odelia Schwartz", "Vanessa Aguiar-Pulido"], "title": "Enhancing Vision Transformer Explainability Using Artificial Astrocytes", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition\n  Conference (CVPR) 2025", "summary": "Machine learning models achieve high precision, but their decision-making\nprocesses often lack explainability. Furthermore, as model complexity\nincreases, explainability typically decreases. Existing efforts to improve\nexplainability primarily involve developing new eXplainable artificial\nintelligence (XAI) techniques or incorporating explainability constraints\nduring training. While these approaches yield specific improvements, their\napplicability remains limited. In this work, we propose the Vision Transformer\nwith artificial Astrocytes (ViTA). This training-free approach is inspired by\nneuroscience and enhances the reasoning of a pretrained deep neural network to\ngenerate more human-aligned explanations. We evaluated our approach employing\ntwo well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a\nstandard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the\nsimilarity between the heatmaps produced by the XAI techniques and a\n(human-aligned) ground truth. Our results consistently demonstrate that\nincorporating artificial astrocytes enhances the alignment of model\nexplanations with human perception, leading to statistically significant\nimprovements across all XAI techniques and metrics utilized.", "AI": {"tldr": "The paper introduces Vision Transformer with artificial Astrocytes (ViTA), a training-free method inspired by neuroscience, which enhances pretrained neural networks' reasoning for more human-aligned explanations. Using Grad-CAM and Grad-CAM++ on the ClickMe dataset, it shows significant improvements in aligning model explanations with human perception.", "motivation": "To address the lack of explainability in complex machine learning models and the limitations of current XAI techniques or explainability constraints.", "method": "Propose ViTA, a training-free approach incorporating artificial astrocytes into pretrained deep neural networks to improve their reasoning and generate more human-aligned explanations. Evaluated using Grad-CAM and Grad-CAM++ techniques and compared against standard Vision Transformer.", "result": "ViTA leads to statistically significant improvements in aligning model explanations with human perception across all XAI techniques and metrics tested on the ClickMe dataset.", "conclusion": "Incorporating artificial astrocytes enhances the alignment of model explanations with human perception, offering a promising direction for improving explainability in machine learning models."}}
{"id": "2505.21731", "pdf": "https://arxiv.org/pdf/2505.21731", "abs": "https://arxiv.org/abs/2505.21731", "authors": ["Quentin Delfosse", "Jannis Bl\u00fcml", "Fabian Tatai", "Th\u00e9o Vincent", "Bjarne Gregori", "Elisabeth Dillies", "Jan Peters", "Constantin Rothkopf", "Kristian Kersting"], "title": "Deep Reinforcement Learning Agents are not even close to Human Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": "49 pages in total, 5 main figures, 14 figures total", "summary": "Deep reinforcement learning (RL) agents achieve impressive results in a wide\nvariety of tasks, but they lack zero-shot adaptation capabilities. While most\nrobustness evaluations focus on tasks complexifications, for which human also\nstruggle to maintain performances, no evaluation has been performed on tasks\nsimplifications. To tackle this issue, we introduce HackAtari, a set of task\nvariations of the Arcade Learning Environments. We use it to demonstrate that,\ncontrary to humans, RL agents systematically exhibit huge performance drops on\nsimpler versions of their training tasks, uncovering agents' consistent\nreliance on shortcuts. Our analysis across multiple algorithms and\narchitectures highlights the persistent gap between RL agents and human\nbehavioral intelligence, underscoring the need for new benchmarks and\nmethodologies that enforce systematic generalization testing beyond static\nevaluation protocols. Training and testing in the same environment is not\nenough to obtain agents equipped with human-like intelligence.", "AI": {"tldr": "Deep reinforcement learning agents, despite impressive results, lack zero-shot adaptation and show significant performance drops on simpler task variations compared to humans, indicating a reliance on shortcuts. This highlights the gap between RL agents and human behavioral intelligence.", "motivation": "To evaluate the robustness and generalization capabilities of deep reinforcement learning (RL) agents by examining their performance on simplified versions of tasks, as opposed to the typical focus on complexified tasks.", "method": "Introduced HackAtari, a set of task variations based on Arcade Learning Environments, to test RL agents' performance on simpler task versions and analyze their reliance on shortcuts across multiple algorithms and architectures.", "result": "RL agents exhibit large performance drops on simpler task variations, unlike humans, revealing their consistent reliance on shortcuts and highlighting the persistent gap in generalization capabilities compared to human behavioral intelligence.", "conclusion": "Training and testing in the same environment is insufficient for achieving human-like intelligence in RL agents; new benchmarks and methodologies are needed to enforce systematic generalization testing."}}
{"id": "2505.22023", "pdf": "https://arxiv.org/pdf/2505.22023", "abs": "https://arxiv.org/abs/2505.22023", "authors": ["Ritwik Murali", "Akash Ravi"], "title": "Securing the Software Package Supply Chain for Critical Systems", "categories": ["cs.SE", "cs.CR"], "comment": "14 Pages,3 Figures. Published as a chapter in the book,\n  \"Cybersecurity and Data Science Innovations for Sustainable Development of\n  HEICC\". CRC Press, eBook ISBN: 9781032711300", "summary": "Software systems have grown as an indispensable commodity used across various\nindustries, and almost all essential services depend on them for effective\noperation. The software is no longer an independent or stand-alone piece of\ncode written by a developer but rather a collection of packages designed by\nmultiple developers across the globe. Ensuring the reliability and resilience\nof these systems is crucial since emerging threats target software supply\nchains, as demonstrated by the widespread SolarWinds hack in late 2020. These\nsupply chains extend beyond patches and updates, involving distribution\nnetworks throughout the software lifecycle. Industries like smart grids,\nmanufacturing, healthcare, and finance rely on interconnected software systems\nand their dependencies for effective functioning. To secure software modules\nand add-ons, robust distribution architectures are essential. The proposed\nchapter enhances the existing delivery frameworks by including a permissioned\nledger with Proof of Authority consensus and multi-party signatures. The\nproposed system aims to prevent attacks while permitting every stakeholder to\nverify the same. Critical systems can interface with the secure pipeline\nwithout disrupting existing functionalities, thus preventing the cascading\neffect of an attack at any point in the supply chain.", "AI": {"tldr": "\u8f6f\u4ef6\u7cfb\u7edf\u5df2\u6210\u4e3a\u5404\u884c\u5404\u4e1a\u4e0d\u53ef\u6216\u7f3a\u7684\u5546\u54c1\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u5f39\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5305\u542b\u6743\u5a01\u8bc1\u660e\u5171\u8bc6\u548c\u591a\u65b9\u7b7e\u540d\u7684\u8bb8\u53ef\u8d26\u672c\u589e\u5f3a\u73b0\u6709\u4ea4\u4ed8\u6846\u67b6\uff0c\u4ee5\u9632\u6b62\u653b\u51fb\u5e76\u5141\u8bb8\u6240\u6709\u5229\u76ca\u76f8\u5173\u8005\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u8f6f\u4ef6\u7cfb\u7edf\u4e0d\u518d\u72ec\u7acb\uff0c\u800c\u662f\u7531\u5168\u7403\u591a\u4e2a\u5f00\u53d1\u8005\u8bbe\u8ba1\u7684\u5305\u96c6\u5408\uff0c\u5176\u53ef\u9760\u6027\u76f4\u63a5\u5f71\u54cd\u5404\u884c\u4e1a\u670d\u52a1\u7684\u6709\u6548\u8fd0\u884c\uff0c\u4e14\u65b0\u5174\u5a01\u80c1\u9488\u5bf9\u8f6f\u4ef6\u4f9b\u5e94\u94fe\uff0c\u59822020\u5e74\u672b\u7684SolarWinds\u9ed1\u5ba2\u4e8b\u4ef6\u3002", "method": "\u5728\u73b0\u6709\u4ea4\u4ed8\u6846\u67b6\u4e2d\u52a0\u5165\u5177\u6709Proof of Authority\u5171\u8bc6\u548c\u591a\u65b9\u7b7e\u540d\u7684\u8bb8\u53ef\u8d26\u672c\uff0c\u6784\u5efa\u5b89\u5168\u7ba1\u9053\uff0c\u8ba9\u5173\u952e\u7cfb\u7edf\u80fd\u591f\u5728\u4e0d\u7834\u574f\u73b0\u6709\u529f\u80fd\u7684\u60c5\u51b5\u4e0b\u63a5\u5165\u3002", "result": "\u63d0\u51fa\u7684\u7cfb\u7edf\u53ef\u4ee5\u9632\u6b62\u653b\u51fb\uff0c\u540c\u65f6\u5141\u8bb8\u6bcf\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u8fdb\u884c\u9a8c\u8bc1\uff0c\u907f\u514d\u4f9b\u5e94\u94fe\u4efb\u4f55\u73af\u8282\u653b\u51fb\u7684\u8fde\u9501\u53cd\u5e94\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u73b0\u6709\u4ea4\u4ed8\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u7684\u5b89\u5168\u6027\uff0c\u4fdd\u62a4\u5173\u952e\u7cfb\u7edf\u514d\u53d7\u653b\u51fb\u5f71\u54cd\u3002"}}
{"id": "2505.21520", "pdf": "https://arxiv.org/pdf/2505.21520", "abs": "https://arxiv.org/abs/2505.21520", "authors": ["Spiros Baxavanakis", "Manos Schinas", "Symeon Papadopoulos"], "title": "Do DeepFake Attribution Models Generalize?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in DeepFake generation, along with the proliferation of\nopen-source tools, have significantly lowered the barrier for creating\nsynthetic media. This trend poses a serious threat to the integrity and\nauthenticity of online information, undermining public trust in institutions\nand media. State-of-the-art research on DeepFake detection has primarily\nfocused on binary detection models. A key limitation of these models is that\nthey treat all manipulation techniques as equivalent, despite the fact that\ndifferent methods introduce distinct artifacts and visual cues. Only a limited\nnumber of studies explore DeepFake attribution models, although such models are\ncrucial in practical settings. By providing the specific manipulation method\nemployed, these models could enhance both the perceived trustworthiness and\nexplainability for end users. In this work, we leverage five state-of-the-art\nbackbone models and conduct extensive experiments across six DeepFake datasets.\nFirst, we compare binary and multi-class models in terms of cross-dataset\ngeneralization. Second, we examine the accuracy of attribution models in\ndetecting seen manipulation methods in unknown datasets, hence uncovering data\ndistribution shifts on the same DeepFake manipulations. Last, we assess the\neffectiveness of contrastive methods in improving cross-dataset generalization\nperformance. Our findings indicate that while binary models demonstrate better\ngeneralization abilities, larger models, contrastive methods, and higher data\nquality can lead to performance improvements in attribution models. The code of\nthis work is available on GitHub.", "AI": {"tldr": "Recent advancements in DeepFake generation have lowered the barrier for creating synthetic media, posing a threat to online information integrity. State-of-the-art research on DeepFake detection primarily focuses on binary detection models, which treat all manipulation techniques as equivalent. This work leverages five state-of-the-art backbone models and conducts extensive experiments across six DeepFake datasets, comparing binary and multi-class models in terms of cross-dataset generalization, examining the accuracy of attribution models, and assessing the effectiveness of contrastive methods.", "motivation": "The motivation of this paper is to address the limitation of current DeepFake detection models that treat all manipulation techniques equivalently, despite different methods introducing distinct artifacts and visual cues. The authors aim to explore DeepFake attribution models, which provide the specific manipulation method employed, enhancing both the perceived trustworthiness and explainability for end users.", "method": "The authors leverage five state-of-the-art backbone models and conduct extensive experiments across six DeepFake datasets. They compare binary and multi-class models in terms of cross-dataset generalization, examine the accuracy of attribution models in detecting seen manipulation methods in unknown datasets, and assess the effectiveness of contrastive methods in improving cross-dataset generalization performance.", "result": "The findings indicate that binary models demonstrate better generalization abilities. Larger models, contrastive methods, and higher data quality can lead to performance improvements in attribution models.", "conclusion": "This study highlights the importance of DeepFake attribution models and provides insights into their performance and potential improvements. The code for this work is available on GitHub."}}
{"id": "2505.21732", "pdf": "https://arxiv.org/pdf/2505.21732", "abs": "https://arxiv.org/abs/2505.21732", "authors": ["Ruijie Zhang", "Ziyue Liu", "Zhengyang Wang", "Zheng Zhang"], "title": "LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing", "categories": ["cs.LG"], "comment": null, "summary": "Training foundation models such as ViTs and LLMs requires tremendous\ncomputing cost. Low-rank matrix or tensor factorization offers a\nparameter-efficient alternative, but often downgrades performance due to the\nrestricted parameter space. In this work, we introduce {\\textbf{Latent Crossing\n(LaX)}} -- a simple yet effective plug-and-play module that enhances the\ncapacity of low-rank models by enabling information flow across low-rank\nsubspaces. We extensively validate the benefits of LaX on pre-training tasks\nwith ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.\nLaX boosts low-rank model performance to match or exceed the full-rank\nbaselines while using 2-3\\(\\times\\) fewer parameters. When equipped with\nlow-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently\nimproves performance on arithmetic and common sense reasoning tasks with\nnegligible cost.", "AI": {"tldr": "LaX is a plug-and-play module that enhances low-rank models by enabling information flow across subspaces, improving performance with fewer parameters.", "motivation": "Training foundation models like ViTs and LLMs is computationally expensive. Low-rank factorization methods reduce this cost but often sacrifice performance.", "method": "Introduced LaX, which allows information exchange across low-rank subspaces to boost model capacity.", "result": "LaX matches or exceeds full-rank baseline performance using 2-3x fewer parameters in pre-training tasks. It also improves fine-tuning performance on arithmetic and common sense reasoning tasks with negligible additional cost.", "conclusion": "LaX effectively enhances low-rank models, reducing parameter usage while maintaining or improving performance."}}
{"id": "2505.22037", "pdf": "https://arxiv.org/pdf/2505.22037", "abs": "https://arxiv.org/abs/2505.22037", "authors": ["Jingyu Zhang", "Ahmed Elgohary", "Xiawei Wang", "A S M Iftekhar", "Ahmed Magooda", "Benjamin Van Durme", "Daniel Khashabi", "Kyle Jackson"], "title": "Jailbreak Distillation: Renewable Safety Benchmarking", "categories": ["cs.CL", "cs.CR", "cs.SE"], "comment": "Project page: https://aka.ms/jailbreak-distillation", "summary": "Large language models (LLMs) are rapidly deployed in critical applications,\nraising urgent needs for robust safety benchmarking. We propose Jailbreak\nDistillation (JBDistill), a novel benchmark construction framework that\n\"distills\" jailbreak attacks into high-quality and easily-updatable safety\nbenchmarks. JBDistill utilizes a small set of development models and existing\njailbreak attack algorithms to create a candidate prompt pool, then employs\nprompt selection algorithms to identify an effective subset of prompts as\nsafety benchmarks. JBDistill addresses challenges in existing safety\nevaluation: the use of consistent evaluation prompts across models ensures fair\ncomparisons and reproducibility. It requires minimal human effort to rerun the\nJBDistill pipeline and produce updated benchmarks, alleviating concerns on\nsaturation and contamination. Extensive experiments demonstrate our benchmarks\ngeneralize robustly to 13 diverse evaluation models held out from benchmark\nconstruction, including proprietary, specialized, and newer-generation LLMs,\nsignificantly outperforming existing safety benchmarks in effectiveness while\nmaintaining high separability and diversity. Our framework thus provides an\neffective, sustainable, and adaptable solution for streamlining safety\nevaluation.", "AI": {"tldr": "Jailbreak Distillation (JBDistill) is a new framework that turns jailbreak attacks into high-quality safety benchmarks for large language models, ensuring fair comparisons and reproducibility.", "motivation": "There is an urgent need for robust safety benchmarking as large language models are quickly being used in critical applications.", "method": "JBDistill uses development models and existing jailbreak attack algorithms to create a candidate prompt pool. Then, it applies prompt selection algorithms to choose an effective subset of prompts as safety benchmarks.", "result": "Experiments show that JBDistill's benchmarks generalize well to 13 diverse evaluation models not involved in benchmark construction, outperforming existing benchmarks in effectiveness while keeping high separability and diversity.", "conclusion": "JBDistill provides an effective, sustainable, and adaptable solution for improving safety evaluation of large language models."}}
{"id": "2505.21522", "pdf": "https://arxiv.org/pdf/2505.21522", "abs": "https://arxiv.org/abs/2505.21522", "authors": ["Shan Gao", "Zhiqiang Wu", "Yawen Niu", "Xiaotao Li", "Qingqing Xu"], "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "While deep neural network (DNN)-based video denoising has demonstrated\nsignificant performance, deploying state-of-the-art models on edge devices\nremains challenging due to stringent real-time and energy efficiency\nrequirements. Computing-in-Memory (CIM) chips offer a promising solution by\nintegrating computation within memory cells, enabling rapid matrix-vector\nmultiplication (MVM). However, existing DNN models are often designed without\nconsidering CIM architectural constraints, thus limiting their acceleration\npotential during inference. To address this, we propose a hardware-algorithm\nco-design framework incorporating two innovations: (1) a CIM-Aware\nArchitecture, CIM-NET, optimized for large receptive field operation and CIM's\ncrossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,\nCIM-CONV, used within CIM-NET to integrate slide-based processing with fully\nconnected transformations for high-quality feature extraction and\nreconstruction. This framework significantly reduces the number of MVM\noperations, improving inference speed on CIM chips while maintaining\ncompetitive performance. Experimental results indicate that, compared to the\nconventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM\noperations with a slight decrease in denoising performance. With a stride value\nof 8, CIM-NET reduces MVM operations to 1/77th of the original, while\nmaintaining competitive PSNR (35.11 dB vs. 35.56 dB", "AI": {"tldr": "This paper proposes a hardware-algorithm co-design framework for video denoising, including CIM-NET and CIM-CONV, which significantly reduces MVM operations on CIM chips while maintaining competitive performance.", "motivation": "Deep neural network-based video denoising shows great performance, but deploying state-of-the-art models on edge devices is challenging due to real-time and energy efficiency requirements. Existing DNN models are not optimized for CIM architectural constraints, limiting their acceleration potential during inference.", "method": "The authors propose CIM-NET, a CIM-Aware Architecture optimized for large receptive field operation and CIM's crossbar-based MVM acceleration, and CIM-CONV, a pseudo-convolutional operator that integrates slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction.", "result": "Compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with only a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original while maintaining competitive PSNR (35.11 dB vs. 35.56 dB).", "conclusion": "The proposed hardware-algorithm co-design framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance."}}
{"id": "2505.21743", "pdf": "https://arxiv.org/pdf/2505.21743", "abs": "https://arxiv.org/abs/2505.21743", "authors": ["Zihao Li", "Xinyuan Cao", "Xiangbo Gao", "Kexin Tian", "Keshu Wu", "Mohammad Anis", "Hao Zhang", "Keke Long", "Jiwan Jiang", "Xiaopeng Li", "Yunlong Zhang", "Tianbao Yang", "Dominique Lord", "Zhengzhong Tu", "Yang Zhou"], "title": "Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic safety science has long been hindered by a fundamental data paradox:\nthe crashes we most wish to prevent are precisely those events we rarely\nobserve. Existing crash-frequency models and surrogate safety metrics rely\nheavily on sparse, noisy, and under-reported records, while even sophisticated,\nhigh-fidelity simulations undersample the long-tailed situations that trigger\ncatastrophic outcomes such as fatalities. We argue that the path to achieving\nVision Zero, i.e., the complete elimination of traffic fatalities and severe\ninjuries, requires a paradigm shift from traditional crash-only learning to a\nnew form of counterfactual safety learning: reasoning not only about what\nhappened, but also about the vast set of plausible yet perilous scenarios that\ncould have happened under slightly different circumstances. To operationalize\nthis shift, our proposed agenda bridges macro to micro. Guided by crash-rate\npriors, generative scene engines, diverse driver models, and causal learning,\nnear-miss events are synthesized and explained. A crash-focused digital twin\ntestbed links micro scenes to macro patterns, while a multi-objective validator\nensures that simulations maintain statistical realism. This pipeline transforms\nsparse crash data into rich signals for crash prediction, enabling the\nstress-testing of vehicles, roads, and policies before deployment. By learning\nfrom crashes that almost happened, we can shift traffic safety from reactive\nforensics to proactive prevention, advancing Vision Zero.", "AI": {"tldr": "The paper proposes a counterfactual safety learning approach to transform sparse crash data into rich signals for crash prediction, promoting proactive prevention in traffic safety.", "motivation": "Traditional traffic safety science is limited by the lack of data on severe crashes. Current models rely on sparse and noisy records, while simulations undersample critical situations.", "method": "The proposed method bridges macro to micro using crash-rate priors, generative scene engines, diverse driver models, and causal learning to synthesize and explain near-miss events. A digital twin testbed links micro scenes to macro patterns and a multi-objective validator ensures statistical realism.", "result": "This pipeline enables stress-testing of vehicles, roads, and policies before deployment, transforming sparse crash data into rich signals for crash prediction.", "conclusion": "By learning from crashes that almost happened, traffic safety can shift from reactive forensics to proactive prevention, advancing towards Vision Zero."}}
{"id": "2505.22108", "pdf": "https://arxiv.org/pdf/2505.22108", "abs": "https://arxiv.org/abs/2505.22108", "authors": ["Santhosh Parampottupadam", "Melih Co\u015f\u011fun", "Sarthak Pati", "Maximilian Zenk", "Saikat Roy", "Dimitrios Bounias", "Benjamin Hamm", "Sinem Sav", "Ralf Floca", "Klaus Maier-Hein"], "title": "Inclusive, Differentially Private Federated Learning for Clinical Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) offers a promising approach for training clinical AI\nmodels without centralizing sensitive patient data. However, its real-world\nadoption is hindered by challenges related to privacy, resource constraints,\nand compliance. Existing Differential Privacy (DP) approaches often apply\nuniform noise, which disproportionately degrades model performance, even among\nwell-compliant institutions. In this work, we propose a novel compliance-aware\nFL framework that enhances DP by adaptively adjusting noise based on\nquantifiable client compliance scores. Additionally, we introduce a compliance\nscoring tool based on key healthcare and security standards to promote secure,\ninclusive, and equitable participation across diverse clinical settings.\nExtensive experiments on public datasets demonstrate that integrating\nunder-resourced, less compliant clinics with highly regulated institutions\nyields accuracy improvements of up to 15% over traditional FL. This work\nadvances FL by balancing privacy, compliance, and performance, making it a\nviable solution for real-world clinical workflows in global healthcare.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u89c4\u611f\u77e5\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6839\u636e\u53ef\u91cf\u5316\u7684\u5ba2\u6237\u5408\u89c4\u5206\u6570\u81ea\u9002\u5e94\u8c03\u6574\u566a\u58f0\u6765\u589e\u5f3a\u5dee\u5206\u9690\u79c1\uff08DP\uff09\uff0c\u5e76\u5728\u516c\u5171\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u5e73\u8861\u9690\u79c1\u3001\u5408\u89c4\u6027\u548c\u6027\u80fd\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e3a\u8bad\u7ec3\u4e34\u5e8aAI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e0d\u96c6\u4e2d\u654f\u611f\u60a3\u8005\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u5230\u9690\u79c1\u3001\u8d44\u6e90\u9650\u5236\u548c\u5408\u89c4\u6027\u6311\u6218\u7684\u963b\u788d\u3002\u73b0\u6709\u7684\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u65b9\u6cd5\u901a\u5e38\u5e94\u7528\u7edf\u4e00\u566a\u58f0\uff0c\u8fd9\u4f1a\u4e0d\u6210\u6bd4\u4f8b\u5730\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u5373\u4f7f\u662f\u5728\u5408\u89c4\u6027\u826f\u597d\u7684\u673a\u6784\u4e2d\u4e5f\u662f\u5982\u6b64\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u89c4\u611f\u77e5FL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u53ef\u91cf\u5316\u7684\u5ba2\u6237\u5408\u89c4\u5206\u6570\u81ea\u9002\u5e94\u8c03\u6574\u566a\u58f0\u6765\u589e\u5f3aDP\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5173\u952e\u533b\u7597\u548c\u5b89\u5168\u6807\u51c6\u7684\u5408\u89c4\u8bc4\u5206\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdb\u5728\u4e0d\u540c\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u3001\u5305\u5bb9\u548c\u5e73\u7b49\u53c2\u4e0e\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u8d44\u6e90\u4e0d\u8db3\u3001\u5408\u89c4\u6027\u8f83\u4f4e\u7684\u8bca\u6240\u4e0e\u9ad8\u5ea6\u76d1\u7ba1\u7684\u673a\u6784\u6574\u5408\u8d77\u6765\uff0c\u4e0e\u4f20\u7edf\u7684FL\u76f8\u6bd4\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u591a\u8fbe15%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5e73\u8861\u9690\u79c1\u3001\u5408\u89c4\u6027\u548c\u6027\u80fd\uff0c\u63a8\u52a8\u4e86FL\u7684\u53d1\u5c55\uff0c\u4f7f\u5176\u6210\u4e3a\u5168\u7403\u533b\u7597\u4fdd\u5065\u4e2d\u73b0\u5b9e\u4e16\u754c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity.", "AI": {"tldr": "\u5728\u6d4b\u8bd5\u65f6\uff0c\u8ba1\u7b97\u80fd\u529b\u4f7f\u5f97\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u6269\u5c55\u63a8\u7406\u94fe\uff0c\u4ece\u800c\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u589e\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u5f80\u5f80\u4f34\u968f\u7740\u66f4\u591a\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u4e3a\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\uff0c\u672c\u6587\u63d0\u51fa\u4e86RH-AUC\u6307\u6807\u548cRH-Bench\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u53d1\u73b0\u66f4\u5927\u6a21\u578b\u901a\u5e38\u80fd\u5728\u63a8\u7406\u548c\u611f\u77e5\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\uff0c\u800c\u8fd9\u79cd\u5e73\u8861\u66f4\u591a\u5730\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u7c7b\u578b\u548c\u9886\u57df\u7684\u5f71\u54cd\uff0c\u800c\u975e\u6570\u636e\u603b\u4f53\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u957f\u63a8\u7406\u94fe\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u4f46\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u6a21\u578b\u9010\u6e10\u504f\u79bb\u57fa\u4e8e\u56fe\u50cf\u7684\u5185\u5bb9\uff0c\u66f4\u4f9d\u8d56\u4e8e\u8bed\u8a00\u5148\u9a8c\u77e5\u8bc6\u3002\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u63a8\u7406\u957f\u5ea6\u4e0e\u5e7b\u89c9\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51faRH-AUC\u6307\u6807\u6765\u91cf\u5316\u6a21\u578b\u63a8\u7406\u957f\u5ea6\u5bf9\u5176\u611f\u77e5\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86RH-Bench\u8bca\u65ad\u57fa\u51c6\u4ee5\u8bc4\u4f30\u591a\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u63a8\u7406\u80fd\u529b\u548c\u5e7b\u89c9\u4e4b\u95f4\u7684\u6743\u8861\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u63a8\u7406\u4e0e\u611f\u77e5\u5e73\u8861\u7684\u5f71\u54cd\u3002", "result": "(i) \u8f83\u5927\u89c4\u6a21\u6a21\u578b\u901a\u5e38\u80fd\u66f4\u597d\u5730\u5e73\u8861\u63a8\u7406\u548c\u611f\u77e5\uff1b(ii) \u8fd9\u79cd\u5e73\u8861\u53d7\u8bad\u7ec3\u6570\u636e\u7c7b\u578b\u548c\u9886\u57df\u7684\u5f71\u54cd\u6bd4\u53d7\u6570\u636e\u603b\u4f53\u91cf\u7684\u5f71\u54cd\u66f4\u5927\u3002", "conclusion": "\u8bc4\u4ef7\u6846\u67b6\u5e94\u540c\u65f6\u8003\u8651\u63a8\u7406\u8d28\u91cf\u548c\u611f\u77e5\u4fdd\u771f\u5ea6\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.21749", "pdf": "https://arxiv.org/pdf/2505.21749", "abs": "https://arxiv.org/abs/2505.21749", "authors": ["M. Reza Ebrahimi", "Roland Memisevic"], "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy.", "AI": {"tldr": "Revisit bi-linear operations in recurrent neural networks, demonstrating their effectiveness for state tracking tasks and establishing a hierarchy of network complexity.", "motivation": "To explore the role of hidden units as active participants in computation rather than passive memory stores, focusing on bi-linear operations that involve multiplicative interactions between hidden units and input embeddings.", "method": "Theoretically and empirically analyze bi-linear operations in recurrent neural networks, showing they provide a natural inductive bias for state tracking tasks and form a hierarchy corresponding to task complexity.", "result": "Bi-linear state updates are effective for state tracking tasks and create a hierarchy with linear recurrent networks like Mamba at the lowest complexity level.", "conclusion": "Bi-linear operations offer a promising approach to enhance the active role of hidden units in recurrent neural networks, particularly for state tracking tasks."}}
{"id": "2505.22531", "pdf": "https://arxiv.org/pdf/2505.22531", "abs": "https://arxiv.org/abs/2505.22531", "authors": ["Andres Molina-Markham", "Luis Robaina", "Sean Steinle", "Akash Trivedi", "Derek Tsui", "Nicholas Potteiger", "Lauren Brandt", "Ransom Winder", "Ahmed Ridley"], "title": "Training RL Agents for Multi-Objective Network Defense Tasks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work.", "AI": {"tldr": "The paper proposes an OEL-inspired training approach for developing autonomous network defenders, emphasizing the importance of a consistent task representation approach.", "motivation": "To address the challenge of applying Open-ended learning (OEL) in real-world cybersecurity applications, specifically for developing autonomous network defenders.", "method": "Proposing a training approach inspired by OEL principles to develop autonomous network defenders, focusing on providing a task representation approach that maintains a consistent interface over goals, rewards and action spaces.", "result": "Demonstrates that OEL principles can lead to more robust and generalizable agents for cyber defense.", "conclusion": "The tools and results aim to significantly impact AI-based cybersecurity research, urging researchers to consider diverse tasks with consistent representations when developing gyms and benchmarks for cyber defense."}}
{"id": "2505.21750", "pdf": "https://arxiv.org/pdf/2505.21750", "abs": "https://arxiv.org/abs/2505.21750", "authors": ["Vivienne Huiling Wang", "Tinghuai Wang", "Joni Pajarinen"], "title": "Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Hierarchical reinforcement learning (HRL) learns to make decisions on\nmultiple levels of temporal abstraction. A key challenge in HRL is that the\nlow-level policy changes over time, making it difficult for the high-level\npolicy to generate effective subgoals. To address this issue, the high-level\npolicy must capture a complex subgoal distribution while also accounting for\nuncertainty in its estimates. We propose an approach that trains a conditional\ndiffusion model regularized by a Gaussian Process (GP) prior to generate a\ncomplex variety of subgoals while leveraging principled GP uncertainty\nquantification. Building on this framework, we develop a strategy that selects\nsubgoals from both the diffusion policy and GP's predictive mean. Our approach\noutperforms prior HRL methods in both sample efficiency and performance on\nchallenging continuous control benchmarks.", "AI": {"tldr": "The paper proposes an HRL approach using a conditional diffusion model with GP prior for subgoal generation, outperforming previous methods in sample efficiency and performance.", "motivation": "The key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals.", "method": "An approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Subgoals are selected from both the diffusion policy and GP's predictive mean.", "result": "Outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks.", "conclusion": "The proposed method improves HRL by better handling the changing low-level policy through advanced subgoal generation."}}
{"id": "2505.22612", "pdf": "https://arxiv.org/pdf/2505.22612", "abs": "https://arxiv.org/abs/2505.22612", "authors": ["C. G. Liu", "P. Bodorik", "D. Jutla"], "title": "BPMN to Smart Contract by Business Analyst", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "This paper addresses the challenge of creating smart contracts for\napplications represented using Business Process Management and Notation (BPMN)\nmodels. In our prior work we presented a methodology that automates the\ngeneration of smart contracts from BPMN models. This approach abstracts the\nBPMN flow control, making it independent of the underlying blockchain\ninfrastructure, with only the BPMN task elements requiring coding. In\nsubsequent research, we enhanced our approach by adding support for nested\ntransactions and enabling a smart contract repair and/or upgrade. To empower\nBusiness Analysts (BAs) to generate smart contracts without relying on software\ndevelopers, we tackled the challenge of generating smart contracts from BPMN\nmodels without assistance of a software developer. We exploit the Decision\nModel and Notation (DMN) standard to represent the decisions and the business\nlogic of the BPMN task elements and amended our methodology for transformation\nof BPMN models into smart contracts to support also the generation script to\nrepresent the business logic represented by the DMN models. To support such\ntransformation, we describe how the BA documents, using the BPMN elements, the\nflow of information along with the flow of execution. Thus, if the BA is\nsuccessful in representing the blockchain application requirements using BPMN\nand DMN models, our methodology and the tool, called TABS, that we developed as\na proof of concept, is used to generate the smart contracts directly from those\nmodels without developer assistance.", "AI": {"tldr": "This paper presents a methodology and tool (TABS) that allows Business Analysts to generate smart contracts from BPMN and DMN models without needing software developers, incorporating business logic and enabling contract repair/upgrade.", "motivation": "The motivation is to simplify the creation of smart contracts for blockchain applications by using BPMN models, abstracting flow control and empowering Business Analysts to perform this task without developer assistance.", "method": "Using BPMN for process representation and DMN for decision and business logic, the authors developed a methodology and tool (TABS) to transform these models into smart contracts. This includes generating scripts representing business logic and supporting features like nested transactions and contract upgrades.", "result": "The result is a system where Business Analysts can independently generate smart contracts directly from BPMN and DMN models, potentially reducing reliance on software developers and streamlining the development process.", "conclusion": "The paper concludes that their methodology and TABS tool successfully enable the generation of smart contracts from BPMN and DMN models without developer intervention, advancing the automation and accessibility of smart contract creation."}}
{"id": "2505.21527", "pdf": "https://arxiv.org/pdf/2505.21527", "abs": "https://arxiv.org/abs/2505.21527", "authors": ["Jianheng Zhuo", "Yifan Yang", "Yiwen Shao", "Yong Xu", "Dong Yu", "Kai Yu", "Xie Chen"], "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR.", "AI": {"tldr": "VietASR is a new ASR training pipeline designed for low-resource languages like Vietnamese, utilizing large unlabeled data and limited labeled data via multi-iteration self-supervised learning. It surpasses Whisper Large-v3 and commercial systems in performance while being cost-effective.", "motivation": "Current ASR systems struggle with high training costs, latency, and accessibility issues when dealing with low-resource languages such as Vietnamese.", "method": "Propose VietASR, an ASR training pipeline that employs multi-iteration ASR-biased self-supervised learning on large-scale unlabeled data combined with a small set of labeled data for fine-tuning.", "result": "Pre-training on 70,000-hour unlabeled data and fine-tuning on 50-hour labeled data results in a lightweight yet powerful ASR model that outperforms Whisper Large-v3 and commercial systems on real-world datasets.", "conclusion": "VietASR offers a practical and cost-effective solution to enhance ASR performance for low-resource languages, and the code and models will be open-sourced."}}
{"id": "2505.21775", "pdf": "https://arxiv.org/pdf/2505.21775", "abs": "https://arxiv.org/abs/2505.21775", "authors": ["Michael Klamkin", "Arnaud Deza", "Sikai Cheng", "Haoruo Zhao", "Pascal Van Hentenryck"], "title": "DualSchool: How Reliable are LLMs for Optimization Education?", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Consider the following task taught in introductory optimization courses which\naddresses challenges articulated by the community at the intersection of\n(generative) AI and OR: generate the dual of a linear program. LLMs, being\ntrained at web-scale, have the conversion process and many instances of Primal\nto Dual Conversion (P2DC) at their disposal. Students may thus reasonably\nexpect that LLMs would perform well on the P2DC task. To assess this\nexpectation, this paper introduces DualSchool, a comprehensive framework for\ngenerating and verifying P2DC instances. The verification procedure of\nDualSchool uses the Canonical Graph Edit Distance, going well beyond existing\nevaluation methods for optimization models, which exhibit many false positives\nand negatives when applied to P2DC. Experiments performed by DualSchool reveal\ninteresting findings. Although LLMs can recite the conversion procedure\naccurately, state-of-the-art open LLMs fail to consistently produce correct\nduals. This finding holds even for the smallest two-variable instances and for\nderivative tasks, such as correctness, verification, and error classification.\nThe paper also discusses the implications for educators, students, and the\ndevelopment of large reasoning systems.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u51c6\u786e\u590d\u8ff0\u7ebf\u6027\u89c4\u5212\u5bf9\u5076\u8f6c\u6362\u8fc7\u7a0b\uff0c\u4f46\u6700\u5148\u8fdb\u7684\u5f00\u6e90LLMs\u5728\u751f\u6210\u6b63\u786e\u5bf9\u5076\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u662f\u6700\u5c0f\u7684\u4e24\u53d8\u91cf\u5b9e\u4f8b\u4ea6\u5982\u6b64\u3002\u672c\u6587\u901a\u8fc7\u5f15\u5165DualSchool\u6846\u67b6\u63ed\u793a\u4e86\u8fd9\u4e00\u53d1\u73b0\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9\u6559\u80b2\u8005\u3001\u5b66\u751f\u53ca\u5927\u578b\u63a8\u7406\u7cfb\u7edf\u53d1\u5c55\u7684\u610f\u4e49\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f18\u5316\u8bfe\u7a0b\u4e2d\u6267\u884c\u4ece\u539f\u95ee\u9898\u5230\u5bf9\u5076\u95ee\u9898\u8f6c\u6362\uff08P2DC\uff09\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u751f\u6210\u5f0fAI\u4e0e\u8fd0\u7b79\u5b66\u4ea4\u53c9\u9886\u57df\u4e2d\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6DualSchool\uff0c\u7528\u4e8e\u751f\u6210\u548c\u9a8c\u8bc1P2DC\u5b9e\u4f8b\uff0c\u91c7\u7528\u89c4\u8303\u56fe\u7f16\u8f91\u8ddd\u79bb\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u4f18\u5316\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1LLMs\u80fd\u51c6\u786e\u590d\u8ff0\u8f6c\u6362\u8fc7\u7a0b\uff0c\u4f46\u65e0\u6cd5\u4e00\u8d2f\u751f\u6210\u6b63\u786e\u7684\u5bf9\u5076\u95ee\u9898\uff0c\u5305\u62ec\u6700\u5c0f\u7684\u4e24\u53d8\u91cf\u5b9e\u4f8b\u53ca\u5176\u884d\u751f\u4efb\u52a1\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u5bf9\u6559\u80b2\u8005\u3001\u5b66\u751f\u4ee5\u53ca\u5927\u578b\u63a8\u7406\u7cfb\u7edf\u7684\u5f00\u53d1\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u8868\u660e\u5f53\u524dLLMs\u5728\u7279\u5b9a\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.22619", "pdf": "https://arxiv.org/pdf/2505.22619", "abs": "https://arxiv.org/abs/2505.22619", "authors": ["C. G. Liu", "P. Bodorik", "D. Jutla"], "title": "Smart Contracts for SMEs and Large Companies", "categories": ["cs.SE", "cs.CR", "cs.DC"], "comment": null, "summary": "Research on blockchains addresses multiple issues, with one being writing\nsmart contracts. In our previous research we described methodology and a tool\nto generate, in automated fashion, smart contracts from BPMN models. The\ngenerated smart contracts provide support for multi-step transactions that\nfacilitate repair/upgrade of smart contracts. In this paper we show how the\napproach is used to support collaborations via smart contracts for companies\nranging from SMEs with little IT capabilities to companies with IT using\nblockchain smart contracts. Furthermore, we also show how the approach is used\nfor certain applications to generate smart contracts by a BPMN modeler who does\nnot need any knowledge of blockchain technology or smart contract development -\nthus we are hoping to facilitate democratization of smart contracts and\nblockchain technology.", "AI": {"tldr": "The paper explores a tool and methodology for automatically generating smart contracts from BPMN models, enabling collaborations across companies and reducing the need for blockchain expertise.", "motivation": "To simplify the creation of smart contracts and make blockchain technology more accessible to companies with varying levels of IT capabilities.", "method": "Using BPMN models to automatically generate smart contracts that support multi-step transactions and facilitate contract repairs/ upgrades.", "result": "The approach allows for the generation of smart contracts by users without blockchain knowledge, supporting diverse company collaborations.", "conclusion": "This method promotes the democratization of smart contract development and blockchain usage."}}
{"id": "2505.21528", "pdf": "https://arxiv.org/pdf/2505.21528", "abs": "https://arxiv.org/abs/2505.21528", "authors": ["Mokai Pan", "Kaizhen Zhu", "Yuexin Ma", "Yanwei Fu", "Jingyi Yu", "Jingya Wang", "Ye Shi"], "title": "UniDB++: Fast Sampling of Unified Diffusion Bridge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion Bridges enable transitions between arbitrary distributions, with\nthe Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image\ngeneration via a Stochastic Optimal Control (SOC) formulation. However, UniDB's\nreliance on iterative Euler sampling methods results in slow, computationally\nexpensive inference, while existing acceleration techniques for diffusion or\ndiffusion bridge models fail to address its unique challenges: missing terminal\nmean constraints and SOC-specific penalty coefficients in its SDEs. We present\nUniDB++, a training-free sampling algorithm that significantly improves upon\nthese limitations. The method's key advancement comes from deriving exact\nclosed-form solutions for UniDB's reverse-time SDEs, effectively reducing the\nerror accumulation inherent in Euler approximations and enabling high-quality\ngeneration with up to 20$\\times$ fewer sampling steps. This method is further\ncomplemented by replacing conventional noise prediction with a more stable data\nprediction model, along with an SDE-Corrector mechanism that maintains\nperceptual quality for low-step regimes (5-10 steps). Additionally, we\ndemonstrate that UniDB++ aligns with existing diffusion bridge acceleration\nmethods by evaluating their update rules, and UniDB++ can recover DBIMs as\nspecial cases under some theoretical conditions. Experiments demonstrate\nUniDB++'s state-of-the-art performance in image restoration tasks,\noutperforming Euler-based methods in fidelity and speed while reducing\ninference time significantly. This work bridges the gap between theoretical\ngenerality and practical efficiency in SOC-driven diffusion bridge models. Our\ncode is available at https://github.com/2769433owo/UniDB-plusplus.", "AI": {"tldr": "UniDB++\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a8\u5bfcUniDB\u53cd\u65f6\u95f4SDE\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\uff0c\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5c06\u91c7\u6837\u6b65\u9aa4\u51cf\u5c1120\u500d\u3002\u6b64\u5916\uff0c\u91c7\u7528\u66f4\u7a33\u5b9a\u7684\u6570\u636e\u9884\u6d4b\u6a21\u578b\u548cSDE-Corrector\u673a\u5236\uff0c\u5728\u4f4e\u6b65\u957f\u6761\u4ef6\u4e0b\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cUniDB++\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6b27\u62c9\u65b9\u6cd5\u7684\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1Unified Diffusion Bridge\uff08UniDB\uff09\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u5176\u4f9d\u8d56\u8fed\u4ee3\u7684Euler\u91c7\u6837\u65b9\u6cd5\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u73b0\u6709\u7684\u52a0\u901f\u6280\u672f\u65e0\u6cd5\u89e3\u51b3\u5176\u72ec\u7279\u7684\u6311\u6218\uff0c\u5982\u7f3a\u5931\u7684\u7ec8\u7aef\u5747\u503c\u7ea6\u675f\u548cSOC\u7279\u5b9a\u7684\u60e9\u7f5a\u7cfb\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u91c7\u6837\u7b97\u6cd5UniDB++\uff0c\u901a\u8fc7\u63a8\u5bfcUniDB\u53cd\u65f6\u95f4SDE\u7684\u7cbe\u786e\u95ed\u5f0f\u89e3\u6765\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u3002\u8fd8\u7528\u66f4\u7a33\u5b9a\u7684\u6570\u636e\u9884\u6d4b\u6a21\u578b\u53d6\u4ee3\u4f20\u7edf\u7684\u566a\u58f0\u9884\u6d4b\uff0c\u5e76\u5f15\u5165SDE-Corrector\u673a\u5236\u4ee5\u5728\u4f4e\u6b65\u957f\u6761\u4ef6\u4e0b\u7ef4\u6301\u611f\u77e5\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUniDB++\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u8f83\u4e8eEuler-based\u65b9\u6cd5\uff0c\u5728\u4fdd\u771f\u5ea6\u548c\u901f\u5ea6\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002\u6b64\u5916\uff0cUniDB++\u4e0e\u73b0\u6709\u7684\u6269\u6563\u6865\u52a0\u901f\u65b9\u6cd5\u76f8\u4e00\u81f4\uff0c\u5e76\u80fd\u5728\u67d0\u4e9b\u7406\u8bba\u6761\u4ef6\u4e0b\u6062\u590dDBIMs\u4f5c\u4e3a\u7279\u6b8a\u60c5\u51b5\u3002", "conclusion": "UniDB++\u5f25\u5408\u4e86SOC\u9a71\u52a8\u7684\u6269\u6563\u6865\u6a21\u578b\u5728\u7406\u8bba\u666e\u904d\u6027\u548c\u5b9e\u9645\u6548\u7387\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ece\u8054\u60f3\u8bb0\u5fc6\uff08AM\uff09\u7684\u89d2\u5ea6\u5206\u6790\u6269\u6563\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u5c0f\u6570\u636e\u548c\u5927\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u4e0d\u540c\u8bb0\u5fc6\u9636\u6bb5\uff0c\u5e76\u53d1\u73b0\u4e86\u865a\u5047\u72b6\u6001\u7684\u5b58\u5728\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u4e0e\u8054\u60f3\u8bb0\u5fc6\u7cfb\u7edf\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u7279\u522b\u662f\u7406\u89e3\u6269\u6563\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u4e0b\u7684\u8bb0\u5fc6\u884c\u4e3a\u53ca\u865a\u5047\u72b6\u6001\u7684\u51fa\u73b0\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u89c6\u4e3a\u8bb0\u5fc6\u7f16\u7801\uff0c\u751f\u6210\u8fc7\u7a0b\u89c6\u4e3a\u8bb0\u5fc6\u68c0\u7d22\uff1b\u5206\u6790\u5c0f\u6570\u636e\u548c\u5927\u6570\u636e\u60c5\u51b5\u4e0b\u6269\u6563\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u8bc6\u522b\u865a\u5047\u72b6\u6001\u7684\u4ea7\u751f\u53ca\u5176\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u6269\u6563\u6a21\u578b\u5728\u5c0f\u6570\u636e\u9636\u6bb5\u8868\u73b0\u51fa\u5f3a\u8bb0\u5fc6\u5316\u7279\u6027\uff0c\u5728\u5927\u6570\u636e\u9636\u6bb5\u5219\u5f62\u6210\u65b0\u7684\u5438\u5f15\u5b50\u72b6\u6001\uff1b\u865a\u5047\u72b6\u6001\u51fa\u73b0\u5728\u4ece\u5c0f\u6570\u636e\u5230\u5927\u6570\u636e\u7684\u8fc7\u6e21\u8fb9\u754c\u4e0a\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u8fc7\u8054\u60f3\u8bb0\u5fc6\u89c6\u89d2\u7406\u89e3\u6269\u6563\u6a21\u578b\u8bb0\u5fc6\u5316-\u6cdb\u5316\u73b0\u8c61\u7684\u65b0\u65b9\u6cd5\uff0c\u7406\u8bba\u9884\u6d4b\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u865a\u5047\u72b6\u6001\u7684\u5b58\u5728\u3002"}}
{"id": "2505.21530", "pdf": "https://arxiv.org/pdf/2505.21530", "abs": "https://arxiv.org/abs/2505.21530", "authors": ["Xuhang Chen", "Zhuo Li", "Yanyan Shen", "Mufti Mahmud", "Hieu Pham", "Chi-Man Pun", "Shuqiang Wang"], "title": "High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Functional ultrasound (fUS) imaging provides exceptional spatiotemporal\nresolution for neurovascular mapping, yet its practical application is\nsignificantly hampered by critical challenges. Foremost among these are data\nscarcity, arising from ethical considerations and signal degradation through\nthe cranium, which collectively limit dataset diversity and compromise the\nfairness of downstream machine learning models.", "AI": {"tldr": "fUS imaging has great potential but is limited by data scarcity and signal degradation, affecting the diversity of datasets and fairness of machine learning models.", "motivation": "To address the challenges in practical application of fUS imaging such as data scarcity and signal degradation through the cranium.", "method": "Not specified in the abstract.", "result": "Not specified in the abstract.", "conclusion": "The issues of data scarcity and signal degradation need to be resolved for better application of fUS imaging."}}
{"id": "2505.21783", "pdf": "https://arxiv.org/pdf/2505.21783", "abs": "https://arxiv.org/abs/2505.21783", "authors": ["Hyunsik Yun"], "title": "P-DROP: Poisson-Based Dropout for Graph Neural Networks", "categories": ["cs.LG"], "comment": "10 pages, 9 figures", "summary": "Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),\nwhere repeated message passing causes node representations to converge and lose\ndiscriminative power. To address this, we propose a novel node selection\nstrategy based on Poisson processes, introducing stochastic but structure-aware\nupdates. Specifically, we equip each node with an independent Poisson clock,\nenabling asynchronous and localized updates that preserve structural diversity.\nWe explore two applications of this strategy: as a replacement for\ndropout-based regularization and as a dynamic subgraph training scheme.\nExperimental results on standard benchmarks (Cora, Citeseer, Pubmed)\ndemonstrate that our Poisson-based method yields competitive or improved\naccuracy compared to traditional Dropout, DropEdge, and DropNode approaches,\nparticularly in later training stages.", "AI": {"tldr": "\u5728\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\uff0c\u8fc7\u5e73\u6ed1\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6cca\u677e\u8fc7\u7a0b\u7684\u65b0\u578b\u8282\u70b9\u9009\u62e9\u7b56\u7565\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5bfc\u81f4\u8282\u70b9\u8868\u793a\u6536\u655b\u5e76\u5931\u53bb\u8fa8\u522b\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6cca\u677e\u8fc7\u7a0b\u7684\u8282\u70b9\u9009\u62e9\u7b56\u7565\uff0c\u7ed9\u6bcf\u4e2a\u8282\u70b9\u914d\u5907\u4e00\u4e2a\u72ec\u7acb\u7684\u6cca\u677e\u65f6\u949f\uff0c\u5b9e\u73b0\u5f02\u6b65\u548c\u5c40\u90e8\u66f4\u65b0\uff0c\u4ee5\u4fdd\u6301\u7ed3\u6784\u591a\u6837\u6027\u3002\u63a2\u7d22\u4e86\u8be5\u7b56\u7565\u5728dropout\u6b63\u5219\u5316\u66ff\u4ee3\u548c\u52a8\u6001\u5b50\u56fe\u8bad\u7ec3\u65b9\u6848\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5728Cora\u3001Citeseer\u548cPubmed\u7b49\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6cca\u677e\u7684\u65b9\u6cd5\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684Dropout\u3001DropEdge\u548cDropNode\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u540e\u671f\u9636\u6bb5\u3002", "conclusion": "\u57fa\u4e8e\u6cca\u677e\u8fc7\u7a0b\u7684\u8282\u70b9\u9009\u62e9\u7b56\u7565\u80fd\u6709\u6548\u7f13\u89e3GNN\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns.", "AI": {"tldr": "Large Language Models (LLMs) are explored for their ability to generate human motion plans and avatar animations from instructions, revealing strengths in high-level planning but weaknesses in precise body part positioning.", "motivation": "To investigate the extent of Large Language Models' understanding and generation capabilities regarding human motion knowledge through 3D avatar control.", "method": "Given a motion instruction, LLMs generate a high-level movement plan followed by specifying body part positions in each step. These positions are linearly interpolated into avatar animations which serve as a verification tool for human evaluators.", "result": "LLMs excel at interpreting high-level body movements but struggle with precise body part positioning. They provide reasonable approximations for general spatial descriptions yet fail in handling precise spatial specifications and parameters needed for avatar control. However, they show promise in conceptualizing creative motions and distinguishing culturally-specific motion patterns.", "conclusion": "LLMs demonstrate potential in generating high-level human motion plans but face challenges in precise low-level planning, indicating their current limitations in detailed spatial-temporal parameter handling."}}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785", "abs": "https://arxiv.org/abs/2505.21785", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "title": "Born a Transformer -- Always a Transformer?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024]. We use the recently proposed C-RASP framework for studying length\ngeneralization [Huang et al., 2025b] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained Transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain Transformer\ncapabilities, but does not overcome fundamental length-generalization limits.", "AI": {"tldr": "Transformers\u5728\u5e8f\u5217\u4efb\u52a1\u5efa\u6a21\u4e0a\u5b58\u5728\u7406\u8bba\u9650\u5236\uff0c\u4f46\u8fd9\u4e9b\u9650\u5236\u662f\u5426\u5f71\u54cd\u5927\u89c4\u6a21\u9884\u8bad\u7ec3LLM\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u901a\u8fc7\u68c0\u7d22\u548c\u590d\u5236\u4efb\u52a1\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5904\u7406\u53f3\u4fa7\uff08\u5f52\u7eb3\uff09\u4efb\u52a1\u65f6\u8868\u73b0\u4f18\u4e8e\u5de6\u4fa7\uff08\u53cd\u5f52\u7eb3\uff09\u4efb\u52a1\uff0c\u5e76\u63ed\u793a\u4e86\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u4e0eTransformer\u5185\u90e8\u7535\u8def\u5f3a\u5ea6\u5dee\u5f02\u6709\u5173\u3002\u5c3d\u7ba1\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u6d88\u9664\u4e0d\u5bf9\u79f0\u6027\uff0c\u4f46\u9884\u8bad\u7ec3\u5e76\u4e0d\u80fd\u514b\u670d\u57fa\u672c\u7684\u957f\u5ea6\u6cdb\u5316\u9650\u5236\u3002", "motivation": "\u63a2\u8ba8\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u53d7\u5230Transformer\u67b6\u6784\u5728\u5e8f\u5217\u4efb\u52a1\u5efa\u6a21\u4e0a\u7684\u7406\u8bba\u9650\u5236\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u9650\u5236\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u3002", "method": "1. \u8bbe\u8ba1\u4e00\u7cfb\u5217\u68c0\u7d22\u548c\u590d\u5236\u4efb\u52a1\u6765\u7814\u7a76Transformer\u67b6\u6784\u7ea6\u675f\u5728\u9884\u8bad\u7ec3\u540e\u7684\u8868\u73b0\u3002\n2. \u4f7f\u7528C-RASP\u6846\u67b6\u786e\u4fdd\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u7684\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u3002\n3. \u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u5f52\u7eb3\u4e0e\u53cd\u5f52\u7eb3\u4efb\u52a1\u7684\u8868\u73b0\u5dee\u5f02\u3002\n4. \u8fdb\u884c\u673a\u5236\u5206\u6790\u4ee5\u63ed\u793a\u4e0d\u5bf9\u79f0\u6027\u7684\u539f\u56e0\u3002\n5. \u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5f52\u7eb3\u4efb\u52a1\uff08\u53f3\u4fa7\u68c0\u7d22\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u53cd\u5f52\u7eb3\u4efb\u52a1\uff08\u5de6\u4fa7\u68c0\u7d22\uff09\uff0c\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u6027\u4e0eTransformer\u5185\u90e8\u7535\u8def\u5f3a\u5ea6\u5dee\u5f02\u76f8\u5173\u3002\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u4ee5\u6d88\u9664\u4e0d\u5bf9\u79f0\u6027\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u514b\u670d\u957f\u5ea6\u6cdb\u5316\u7684\u7406\u8bba\u9650\u5236\u3002", "conclusion": "\u9884\u8bad\u7ec3\u589e\u5f3a\u4e86\u67d0\u4e9bTransformer\u80fd\u529b\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u5176\u57fa\u672c\u7684\u957f\u5ea6\u6cdb\u5316\u9650\u5236\uff0c\u8fd9\u63d0\u793a\u4e86\u6f5c\u5728\u7684\u53ef\u9760\u6027\u98ce\u9669\u3002"}}
{"id": "2505.21532", "pdf": "https://arxiv.org/pdf/2505.21532", "abs": "https://arxiv.org/abs/2505.21532", "authors": ["Ismail Erbas", "Ferhat Demirkiran", "Karthik Swaminathan", "Naigang Wang", "Navid Ibtehaj Nizam", "Stefan T. Radev", "Kaoutar El Maghraoui", "Xavier Intes", "Vikas Pandey"], "title": "EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.optics"], "comment": "18 pages, 4 figures", "summary": "Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology\nemployed for distance and depth estimation across medical, automotive, and\nother fields, encounters significant computational challenges in scattering\nmedia. The complex nature of the acquired FLiDAR signal, particularly in such\nenvironments, makes isolating photon time-of-flight (related to target depth)\nand intrinsic fluorescence lifetime exceptionally difficult, thus limiting the\neffectiveness of current analytical and computational methodologies. To\novercome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)\nframework tailored for specialized modeling of diverse temporal components. In\ncontrast to the conventional MoE approaches our expert models are informed by\nunderlying physics, such as the radiative transport equation governing photon\npropagation in scattering media. Central to our approach is EvidenceMoE, which\nintegrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess\nthe reliability of each expert's output by providing per-expert quality scores\nand corrective feedback. A Decider Network then leverages this information to\nfuse expert predictions into a robust final estimate adaptively. We validate\nour method using realistically simulated Fluorescence LiDAR (FLiDAR) data for\nnon-invasive cancer cell depth detection generated from photon transport models\nin tissue. Our framework demonstrates strong performance, achieving a\nnormalized root mean squared error (NRMSE) of 0.030 for depth estimation and\n0.074 for fluorescence lifetime.", "AI": {"tldr": "This paper proposes a Physics-Guided Mixture-of-Experts (MoE) framework with Evidence-Based Dirichlet Critics (EDCs) for Fluorescence LiDAR (FLiDAR) data analysis in scattering media. The method improves the estimation of photon time-of-flight and fluorescence lifetime, achieving NRMSE values of 0.030 for depth and 0.074 for lifetime.", "motivation": "The computational challenges faced by FLiDAR technology in scattering media motivate the need for more effective methodologies to isolate photon time-of-flight and intrinsic fluorescence lifetime.", "method": "The authors develop a Physics-Guided MoE framework incorporating EDCs that assess expert model outputs based on physical principles such as the radiative transport equation. A Decider Network uses this information to adaptively fuse expert predictions into robust final estimates.", "result": "Validated using simulated FLiDAR data for non-invasive cancer cell depth detection, the proposed framework achieves an NRMSE of 0.030 for depth estimation and 0.074 for fluorescence lifetime.", "conclusion": "The Physics-Guided MoE framework demonstrates strong performance in analyzing FLiDAR data, providing accurate estimations of depth and fluorescence lifetime in scattering media."}}
{"id": "2505.21790", "pdf": "https://arxiv.org/pdf/2505.21790", "abs": "https://arxiv.org/abs/2505.21790", "authors": ["Hilal Asi", "Vinod Raman", "Kunal Talwar"], "title": "Faster Rates for Private Adversarial Bandits", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u65b0\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u7528\u4e8e\u5bf9\u6297\u6027bandits\u95ee\u9898\u548c\u5e26\u4e13\u5bb6\u5efa\u8bae\u7684bandits\u95ee\u9898\u3002\u5bf9\u4e8e\u5bf9\u6297\u6027bandits\uff0c\u7ed9\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u5c06\u975e\u9690\u79c1bandit\u7b97\u6cd5\u8f6c\u5316\u4e3a\u9690\u79c1bandit\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u4e0a\u754c\u3002\u5bf9\u4e8e\u5e26\u4e13\u5bb6\u5efa\u8bae\u7684bandits\u95ee\u9898\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u5e76\u83b7\u5f97\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728\u4fdd\u6301\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u89e3\u51b3\u5bf9\u6297\u6027bandits\u548c\u5e26\u4e13\u5bb6\u5efa\u8bae\u7684bandits\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u6709\u6548\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u4ee5\u8fbe\u5230\u8f83\u4f4e\u7684\u9057\u61be\u7387\u3002", "method": "\u5bf9\u4e8e\u5bf9\u6297\u6027bandits\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u975e\u9690\u79c1bandit\u7b97\u6cd5\u8f6c\u6362\u4e3a\u9690\u79c1bandit\u7b97\u6cd5\u7684\u65b9\u6cd5\uff1b\u5bf9\u4e8e\u5e26\u4e13\u5bb6\u5efa\u8bae\u7684bandits\uff0c\u9996\u6b21\u8bbe\u8ba1\u4e86\u51e0\u79cd\u4e0d\u540c\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u3002", "result": "\u5bf9\u6297\u6027bandits\u95ee\u9898\u7684\u9057\u61be\u4e0a\u754c\u4ece$O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ \u6539\u8fdb\u5230 $O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$\u3002\u5e26\u4e13\u5bb6\u5efa\u8bae\u7684bandits\u95ee\u9898\u5f97\u5230\u4e86\u51e0\u79cd\u6b21\u7ebf\u6027\u9057\u61be\u7387\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u4e9b\u65b0\u7b97\u6cd5\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u9057\u61be\u7387\uff0c\u5e76\u9996\u6b21\u5c55\u793a\u4e86\u4e2d\u5fc3\u5316\u548c\u672c\u5730\u5316\u5dee\u5206\u9690\u79c1\u5728\u8fd9\u7c7b\u95ee\u9898\u4e0a\u7684\u5dee\u5f02\u3002"}}
{"id": "2505.21534", "pdf": "https://arxiv.org/pdf/2505.21534", "abs": "https://arxiv.org/abs/2505.21534", "authors": ["Yao Fehlis"], "title": "Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Scientific laboratories, particularly those in pharmaceutical and\nbiotechnology companies, encounter significant challenges in optimizing\nworkflows due to the complexity and volume of tasks such as compound screening\nand assay execution. We introduce Cycle Time Reduction Agents (CTRA), a\nLangGraph-based agentic workflow designed to automate the analysis of lab\noperational metrics. CTRA comprises three main components: the Question\nCreation Agent for initiating analysis, Operational Metrics Agents for data\nextraction and validation, and Insights Agents for reporting and visualization,\nidentifying bottlenecks in lab processes. This paper details CTRA's\narchitecture, evaluates its performance on a lab dataset, and discusses its\npotential to accelerate pharmaceutical and biotechnological development. CTRA\noffers a scalable framework for reducing cycle times in scientific labs.", "AI": {"tldr": "The paper introduces Cycle Time Reduction Agents (CTRA), a LangGraph-based agentic workflow for automating lab operational metrics analysis. CTRA includes Question Creation Agent, Operational Metrics Agents, and Insights Agents to identify bottlenecks in lab processes.", "motivation": "To address the challenges of optimizing workflows in scientific laboratories due to the complexity and volume of tasks such as compound screening and assay execution.", "method": "CTRA is composed of three main components: Question Creation Agent for initiating analysis, Operational Metrics Agents for data extraction and validation, and Insights Agents for reporting and visualization.", "result": "CTRA's performance is evaluated on a lab dataset, demonstrating its potential to accelerate pharmaceutical and biotechnological development.", "conclusion": "CTRA provides a scalable framework for reducing cycle times in scientific labs."}}
{"id": "2505.21792", "pdf": "https://arxiv.org/pdf/2505.21792", "abs": "https://arxiv.org/abs/2505.21792", "authors": ["Yuanzhe Peng", "Jieming Bian", "Lei Wang", "Yin Huang", "Jie Xu"], "title": "Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Federated Learning (MFL) lies at the intersection of two pivotal\nresearch areas: leveraging complementary information from multiple modalities\nto improve downstream inference performance and enabling distributed training\nto enhance efficiency and preserve privacy. Despite the growing interest in\nMFL, there is currently no comprehensive taxonomy that organizes MFL through\nthe lens of different Federated Learning (FL) paradigms. This perspective is\nimportant because multimodal data introduces distinct challenges across various\nFL settings. These challenges, including modality heterogeneity, privacy\nheterogeneity, and communication inefficiency, are fundamentally different from\nthose encountered in traditional unimodal or non-FL scenarios. In this paper,\nwe systematically examine MFL within the context of three major FL paradigms:\nhorizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we\npresent the problem formulation, review representative training algorithms, and\nhighlight the most prominent challenge introduced by multimodal data in\ndistributed settings. We also discuss open challenges and provide insights for\nfuture research. By establishing this taxonomy, we aim to uncover the novel\nchallenges posed by multimodal data from the perspective of different FL\nparadigms and to offer a new lens through which to understand and advance the\ndevelopment of MFL.", "AI": {"tldr": "Multimodal Federated Learning (MFL) combines multiple data types to improve performance while preserving privacy through distributed training. This paper provides a systematic examination of MFL within three major FL paradigms: horizontal, vertical, and hybrid FL, highlighting challenges and providing insights for future research.", "motivation": "To address the lack of a comprehensive taxonomy for MFL and to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms.", "method": "Systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. Present problem formulation, review representative training algorithms, and highlight challenges introduced by multimodal data in distributed settings.", "result": "Provides a taxonomy of MFL that organizes it through the lens of different FL paradigms, identifies distinct challenges such as modality heterogeneity, privacy heterogeneity, and communication inefficiency, and offers insights for future research.", "conclusion": "This taxonomy aims to offer a new lens through which to understand and advance the development of MFL, addressing its unique challenges across various FL settings."}}
{"id": "2505.21535", "pdf": "https://arxiv.org/pdf/2505.21535", "abs": "https://arxiv.org/abs/2505.21535", "authors": ["Yuxin Ren", "Maxwell D Collins", "Miao Hu", "Huanrui Yang"], "title": "Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages main paper + 6 pages appendix, 14 figures; submitted to\n  NeurIPS 2025", "summary": "While transformers excel across vision and language pretraining tasks, their\nreliance on attention mechanisms poses challenges for inference efficiency,\nespecially on edge and embedded accelerators with limited parallelism and\nmemory bandwidth. Hinted by the observed redundancy of attention at inference\ntime, we hypothesize that though the model learns complicated token dependency\nthrough pretraining, the inference-time sequence-to-sequence mapping in each\nattention layer is actually ''simple'' enough to be represented with a much\ncheaper function. In this work, we explore FAR, a Function-preserving Attention\nReplacement framework that replaces all attention blocks in pretrained\ntransformers with learnable sequence-to-sequence modules, exemplified by an\nLSTM. FAR optimize a multi-head LSTM architecture with a block-wise\ndistillation objective and a global structural pruning framework to achieve a\nfamily of efficient LSTM-based models from pretrained transformers. We validate\nFAR on the DeiT vision transformer family and demonstrate that it matches the\naccuracy of the original models on ImageNet and multiple downstream tasks with\nreduced parameters and latency. Further analysis shows that FAR preserves the\nsemantic token relationships and the token-to-token correlation learned in the\ntransformer's attention module.", "AI": {"tldr": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f7f\u7528Transformer\u65f6\uff0c\u5c3d\u7ba1\u5176\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u5176\u63a8\u7406\u6548\u7387\u6784\u6210\u4e86\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86FAR\u6846\u67b6\uff0c\u7528LSTM\u7b49\u53ef\u5b66\u4e60\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u5757\u66ff\u6362Transformer\u4e2d\u7684\u6240\u6709\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ece\u800c\u751f\u6210\u4e00\u7cfb\u5217\u9ad8\u6548\u7684\u57fa\u4e8eLSTM\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u59cb\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u8bed\u4e49\u5173\u7cfb\u3002", "motivation": "Transformer\u5728\u63a8\u7406\u9636\u6bb5\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f9d\u8d56\u5bfc\u81f4\u5728\u8fb9\u7f18\u548c\u5d4c\u5165\u5f0f\u52a0\u901f\u5668\u4e0a\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u8bbe\u5907\u7684\u5e76\u884c\u6027\u548c\u5185\u5b58\u5e26\u5bbd\u6709\u9650\u3002\u89c2\u5bdf\u5230\u63a8\u7406\u65f6\u6ce8\u610f\u529b\u5b58\u5728\u5197\u4f59\uff0c\u7814\u7a76\u8005\u5047\u8bbe\u6bcf\u4e2a\u6ce8\u610f\u529b\u5c42\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6620\u5c04\u53ef\u4ee5\u901a\u8fc7\u66f4\u7b80\u5355\u7684\u51fd\u6570\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAR\uff08Function-preserving Attention Replacement\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5757\u7ea7\u84b8\u998f\u76ee\u6807\u548c\u5168\u5c40\u7ed3\u6784\u526a\u679d\u6846\u67b6\u4f18\u5316\u591a\u5934LSTM\u67b6\u6784\uff0c\u7528LSTM\u7b49\u53ef\u5b66\u4e60\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u5757\u66ff\u6362\u9884\u8bad\u7ec3Transformer\u4e2d\u7684\u6240\u6709\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728DeiT\u89c6\u89c9Transformer\u7cfb\u5217\u4e0a\u9a8c\u8bc1\u4e86FAR\uff0c\u7ed3\u679c\u8868\u660e\uff0cFAR\u80fd\u591f\u5728\u51cf\u5c11\u53c2\u6570\u548c\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0e\u539f\u59cb\u6a21\u578b\u5728ImageNet\u548c\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u76f8\u5339\u914d\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0cFAR\u4fdd\u7559\u4e86Transformer\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u5b66\u5230\u7684\u8bed\u4e49\u4ee4\u724c\u5173\u7cfb\u548c\u4ee4\u724c\u95f4\u76f8\u5173\u6027\u3002", "conclusion": "FAR\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5c06\u9884\u8bad\u7ec3Transformer\u8f6c\u6362\u4e3a\u57fa\u4e8eLSTM\u7684\u9ad8\u6548\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u548c\u5b66\u5230\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002"}}
{"id": "2505.21800", "pdf": "https://arxiv.org/pdf/2505.21800", "abs": "https://arxiv.org/abs/2505.21800", "authors": ["Stanley Yu", "Vaidehi Bulusu", "Oscar Yasunaga", "Clayton Lau", "Cole Blondin", "Sean O'Brien", "Kevin Zhu", "Vasu Sharma"], "title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong conversational abilities but\noften generate falsehoods. Prior work suggests that the truthfulness of simple\npropositions can be represented as a single linear direction in a model's\ninternal activations, but this may not fully capture its underlying geometry.\nIn this work, we extend the concept cone framework, recently introduced for\nmodeling refusal, to the domain of truth. We identify multi-dimensional cones\nthat causally mediate truth-related behavior across multiple LLM families. Our\nresults are supported by three lines of evidence: (i) causal interventions\nreliably flip model responses to factual statements, (ii) learned cones\ngeneralize across model architectures, and (iii) cone-based interventions\npreserve unrelated model behavior. These findings reveal the richer,\nmultidirectional structure governing simple true/false propositions in LLMs and\nhighlight concept cones as a promising tool for probing abstract behaviors.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u4f46\u7ecf\u5e38\u751f\u6210\u9519\u8bef\u4fe1\u606f\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u7b80\u5355\u547d\u9898\u7684\u771f\u5b9e\u6027\u53ef\u4ee5\u5728\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u4e2d\u8868\u793a\u4e3a\u5355\u4e00\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u4f46\u8fd9\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u5176\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u3002\u672c\u7814\u7a76\u5c06\u6700\u8fd1\u7528\u4e8e\u5efa\u6a21\u62d2\u7edd\u884c\u4e3a\u7684\u6982\u5ff5\u9525\u6846\u67b6\u6269\u5c55\u5230\u771f\u5b9e\u6027\u7684\u9886\u57df\uff0c\u8bc6\u522b\u51fa\u5728\u591a\u4e2aLLM\u5bb6\u65cf\u4e2d\u56e0\u679c\u8c03\u8282\u771f\u5b9e\u6027\u76f8\u5173\u884c\u4e3a\u7684\u591a\u7ef4\u9525\u3002\u7814\u7a76\u7ed3\u679c\u901a\u8fc7\u4e09\u6761\u8bc1\u636e\u5f97\u5230\u652f\u6301\uff1a(i) \u56e0\u679c\u5e72\u9884\u53ef\u9760\u5730\u7ffb\u8f6c\u6a21\u578b\u5bf9\u4e8b\u5b9e\u9648\u8ff0\u7684\u53cd\u5e94\uff1b(ii) \u5b66\u4e60\u5230\u7684\u9525\u4f53\u53ef\u4ee5\u8de8\u6a21\u578b\u67b6\u6784\u6cdb\u5316\uff1b(iii) \u57fa\u4e8e\u9525\u4f53\u7684\u5e72\u9884\u4fdd\u7559\u4e86\u65e0\u5173\u7684\u6a21\u578b\u884c\u4e3a\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u7b80\u5355\u7684\u771f\u5b9e/\u865a\u5047\u547d\u9898\u5728LLMs\u4e2d\u66f4\u4e30\u5bcc\u3001\u591a\u65b9\u5411\u7684\u7ed3\u6784\uff0c\u5e76\u7a81\u663e\u4e86\u6982\u5ff5\u9525\u4f5c\u4e3a\u63a2\u7d22\u62bd\u8c61\u884c\u4e3a\u7684\u6709\u5e0c\u671b\u7684\u5de5\u5177\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5177\u6709\u5f3a\u5927\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u4f1a\u751f\u6210\u9519\u8bef\u4fe1\u606f\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u5e76\u6539\u5584\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u8bd5\u56fe\u8d85\u8d8a\u7b80\u5355\u7684\u7ebf\u6027\u65b9\u5411\u8868\u793a\uff0c\u63a2\u7d22\u80fd\u591f\u66f4\u5168\u9762\u6355\u6349\u6a21\u578b\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u6269\u5c55\u4e86\u6982\u5ff5\u9525\u6846\u67b6\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u771f\u5b9e\u6027\u9886\u57df\u3002\u901a\u8fc7\u8bc6\u522b\u591a\u7ef4\u9525\uff0c\u7814\u7a76\u8005\u5206\u6790\u4e86\u8fd9\u4e9b\u9525\u5982\u4f55\u5728\u591a\u4e2aLLM\u5bb6\u65cf\u4e2d\u56e0\u679c\u8c03\u8282\u771f\u5b9e\u6027\u76f8\u5173\u884c\u4e3a\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u56e0\u679c\u5e72\u9884\u3001\u5b66\u4e60\u9525\u7684\u8de8\u6a21\u578b\u67b6\u6784\u6cdb\u5316\u6027\u6d4b\u8bd5\u4ee5\u53ca\u57fa\u4e8e\u9525\u7684\u5e72\u9884\u5bf9\u65e0\u5173\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e09\u6761\u652f\u6301\u6027\u8bc1\u636e\uff1a(i) \u56e0\u679c\u5e72\u9884\u80fd\u53ef\u9760\u5730\u6539\u53d8\u6a21\u578b\u5bf9\u4e8b\u5b9e\u9648\u8ff0\u7684\u53cd\u5e94\uff1b(ii) \u5b66\u4e60\u5230\u7684\u9525\u4f53\u80fd\u591f\u8de8\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u6cdb\u5316\uff1b(iii) \u57fa\u4e8e\u9525\u4f53\u7684\u5e72\u9884\u4e0d\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u5176\u4ed6\u65e0\u5173\u884c\u4e3a\u3002", "conclusion": "LLMs\u4e2d\u7b80\u5355\u7684\u771f\u5b9e/\u865a\u5047\u547d\u9898\u7531\u66f4\u4e30\u5bcc\u3001\u591a\u65b9\u5411\u7684\u7ed3\u6784\u6240\u652f\u914d\u3002\u6982\u5ff5\u9525\u662f\u63a2\u7d22\u548c\u7406\u89e3\u8fd9\u79cd\u62bd\u8c61\u884c\u4e3a\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2505.21537", "pdf": "https://arxiv.org/pdf/2505.21537", "abs": "https://arxiv.org/abs/2505.21537", "authors": ["Hao Sun", "Yunyi Shen", "Mihaela van der Schaar"], "title": "OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In the era of large language models (LLMs), high-quality, domain-rich, and\ncontinuously evolving datasets capturing expert-level knowledge, core human\nvalues, and reasoning are increasingly valuable. This position paper argues\nthat OpenReview -- the continually evolving repository of research papers, peer\nreviews, author rebuttals, meta-reviews, and decision outcomes -- should be\nleveraged more broadly as a core community asset for advancing research in the\nera of LLMs. We highlight three promising areas in which OpenReview can\nuniquely contribute: enhancing the quality, scalability, and accountability of\npeer review processes; enabling meaningful, open-ended benchmarks rooted in\ngenuine expert deliberation; and supporting alignment research through\nreal-world interactions reflecting expert assessment, intentions, and\nscientific values. To better realize these opportunities, we suggest the\ncommunity collaboratively explore standardized benchmarks and usage guidelines\naround OpenReview, inviting broader dialogue on responsible data use, ethical\nconsiderations, and collective stewardship.", "AI": {"tldr": "In the age of large language models, this paper argues for using OpenReview as a key resource to improve peer review processes, create meaningful benchmarks, and support alignment research.", "motivation": "The motivation is the increasing value of high-quality, domain-rich, and continuously evolving datasets capturing expert-level knowledge, core human values, and reasoning in the era of large language models.", "method": "The method involves leveraging OpenReview's repository of research papers, peer reviews, author rebuttals, meta-reviews, and decision outcomes more broadly as a core community asset. It also includes exploring standardized benchmarks and usage guidelines around OpenReview.", "result": "The result would be enhancing the quality, scalability, and accountability of peer review processes; enabling meaningful, open-ended benchmarks; and supporting alignment research through real-world interactions reflecting expert assessment, intentions, and scientific values.", "conclusion": "The conclusion is that the community should collaboratively explore the use of OpenReview to advance research in the era of large language models, with a focus on responsible data use, ethical considerations, and collective stewardship."}}
{"id": "2505.21806", "pdf": "https://arxiv.org/pdf/2505.21806", "abs": "https://arxiv.org/abs/2505.21806", "authors": ["Brian D. Bue", "Jake H. Lee", "Andrew K. Thorpe", "Philip G. Brodrick", "Daniel Cusworth", "Alana Ayasse", "Vassiliki Mancoridis", "Anagha Satish", "Shujun Xiong", "Riley Duren"], "title": "Towards Operational Automated Greenhouse Gas Plume Detection", "categories": ["cs.LG"], "comment": "Main 19 pages 14 figures. Supplemental 19 pages 16 figures. In review", "summary": "Operational deployment of a fully automated greenhouse gas (GHG) plume\ndetection system remains an elusive goal for imaging spectroscopy missions,\ndespite recent advances in deep learning approaches. With the dramatic increase\nin data availability, however, automation continues to increase in importance\nfor natural and anthropogenic emissions monitoring. This work reviews and\naddresses several key obstacles in the field: data and label quality control,\nprevention of spatiotemporal biases, and correctly aligned modeling objectives.\nWe demonstrate through rigorous experiments using multicampaign data from\nairborne and spaceborne instruments that convolutional neural networks (CNNs)\nare able to achieve operational detection performance when these obstacles are\nalleviated. We demonstrate that a multitask model that learns both instance\ndetection and pixelwise segmentation simultaneously can successfully lead\ntowards an operational pathway. We evaluate the model's plume detectability\nacross emission source types and regions, identifying thresholds for\noperational deployment. Finally, we provide analysis-ready data, models, and\nsource code for reproducibility, and work to define a set of best practices and\nvalidation standards to facilitate future contributions to the field.", "AI": {"tldr": "Despite recent advances in deep learning, fully automated greenhouse gas plume detection remains a challenge. This paper addresses key obstacles such as data quality control and modeling objectives, demonstrating that convolutional neural networks can achieve operational detection performance when these issues are resolved. A multitask model capable of instance detection and pixelwise segmentation is proposed, along with thresholds for deployment, analysis-ready data, models, and source code for reproducibility.", "motivation": "To achieve operational deployment of a fully automated greenhouse gas plume detection system using imaging spectroscopy, overcoming challenges like data and label quality, spatiotemporal biases, and aligned modeling objectives.", "method": "Using convolutional neural networks (CNNs) to address obstacles in greenhouse gas plume detection. The method involves a multitask model that learns both instance detection and pixelwise segmentation simultaneously, evaluated across different emission source types and regions.", "result": "The multitask model successfully leads towards an operational pathway for greenhouse gas plume detection, with identified thresholds for deployment and provision of analysis-ready data, models, and source code for reproducibility.", "conclusion": "Convolutional neural networks can achieve operational detection performance for greenhouse gas plumes when key obstacles are alleviated. The paper provides resources and standards to facilitate future contributions to the field."}}
{"id": "2505.21538", "pdf": "https://arxiv.org/pdf/2505.21538", "abs": "https://arxiv.org/abs/2505.21538", "authors": ["Zihan Weng", "Lucas Gomez", "Taylor Whittington Webb", "Pouya Bashivan"], "title": "Caption This, Reason That: VLMs Caught in the Middle", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in visual\nunderstanding in recent years. Yet, they still lag behind human capabilities in\nspecific visual tasks such as counting or relational reasoning. To understand\nthe underlying limitations, we adopt methodologies from cognitive science,\nanalyzing VLM performance along core cognitive axes: Perception, Attention, and\nMemory. Using a suite of tasks targeting these abilities, we evaluate\nstate-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct\ncognitive profiles: while advanced models approach ceiling performance on some\ntasks (e.g. category identification), a significant gap persists, particularly\nin tasks requiring spatial understanding or selective attention. Investigating\nthe source of these failures and potential methods for improvement, we employ a\nvision-text decoupling analysis, finding that models struggling with direct\nvisual reasoning show marked improvement when reasoning over their own\ngenerated text captions. These experiments reveal a strong need for improved\nVLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed\nhuman performance. Furthermore, we demonstrate the potential of targeted\nfine-tuning on composite visual reasoning tasks and show that fine-tuning\nsmaller VLMs substantially improves core cognitive abilities. While this\nimprovement does not translate to large enhancements on challenging,\nout-of-distribution benchmarks, we show broadly that VLM performance on our\ndatasets strongly correlates with performance on these other benchmarks. Our\nwork provides a detailed analysis of VLM cognitive strengths and weaknesses and\nidentifies key bottlenecks in simultaneous perception and reasoning while also\nproviding an effective and simple solution.", "AI": {"tldr": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\u5982\u8ba1\u6570\u6216\u5173\u7cfb\u63a8\u7406\u4e0a\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u3002\u901a\u8fc7\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\u5206\u6790VLMs\u5728\u611f\u77e5\u3001\u6ce8\u610f\u529b\u548c\u8bb0\u5fc6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5728\u67d0\u4e9b\u4efb\u52a1\uff08\u5982\u7c7b\u522b\u8bc6\u522b\uff09\u63a5\u8fd1\u5929\u82b1\u677f\u6027\u80fd\uff0c\u4f46\u5728\u9700\u8981\u7a7a\u95f4\u7406\u89e3\u548c\u9009\u62e9\u6027\u6ce8\u610f\u7684\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u7814\u7a76\u663e\u793a\uff0c\u5bf9\u81ea\u8eab\u751f\u6210\u6587\u672c\u8fdb\u884c\u63a8\u7406\u80fd\u663e\u8457\u6539\u5584\u6a21\u578b\u7684\u76f4\u63a5\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdbVLM\u601d\u7ef4\u94fe(CoT)\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002\u9488\u5bf9\u590d\u5408\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7684\u7cbe\u7ec6\u8c03\u6574\u53ef\u663e\u8457\u63d0\u5347\u5c0f\u578bVLM\u7684\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff0c\u4f46\u5bf9\u6311\u6218\u6027\u7684\u3001\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u7684\u63d0\u5347\u6709\u9650\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u6df1\u5165\u4e86\u89e3\u5176\u5728\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u79d1\u5b66\u7684\u65b9\u6cd5\u8bba\uff0c\u6cbf\u7740\u6838\u5fc3\u8ba4\u77e5\u8f74(\u611f\u77e5\u3001\u6ce8\u610f\u529b\u548c\u8bb0\u5fc6)\u5206\u6790VLMs\u7684\u8868\u73b0\u3002\u4f7f\u7528\u4e00\u7cfb\u5217\u4efb\u52a1\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\uff0c\u5e76\u5bf9\u6700\u5148\u8fdb\u7684VLMs\u8fdb\u884c\u6d4b\u8bd5\u3002\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u89e3\u8026\u5206\u6790\u7814\u7a76\u5931\u8d25\u539f\u56e0\u53ca\u6539\u8fdb\u5efa\u8bae\uff0c\u63a2\u7d22\u76ee\u6807\u7cbe\u7ec6\u8c03\u6574\u5728\u590d\u5408\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "result": "\u53d1\u73b0VLMs\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u63a5\u8fd1\u5929\u82b1\u677f\u6027\u80fd\uff0c\u4f46\u5728\u9700\u8981\u7a7a\u95f4\u7406\u89e3\u548c\u9009\u62e9\u6027\u6ce8\u610f\u7684\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u901a\u8fc7\u5bf9\u5176\u751f\u6210\u7684\u6587\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u76f4\u63a5\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002\u9488\u5bf9\u590d\u5408\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7684\u7cbe\u7ec6\u8c03\u6574\u53ef\u663e\u8457\u6539\u5584\u5c0f\u578bVLM\u7684\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\uff0c\u4f46\u5bf9\u6311\u6218\u6027\u7684\u3001\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u7684\u63d0\u5347\u6709\u9650\u3002", "conclusion": "\u672c\u7814\u7a76\u8be6\u7ec6\u5206\u6790\u4e86VLMs\u7684\u8ba4\u77e5\u4f18\u52bf\u548c\u52a3\u52bf\uff0c\u786e\u5b9a\u4e86\u540c\u65f6\u8fdb\u884c\u611f\u77e5\u548c\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.21807", "pdf": "https://arxiv.org/pdf/2505.21807", "abs": "https://arxiv.org/abs/2505.21807", "authors": ["Tommy Xu", "Zhitian Zhang", "Xiangyu Sun", "Lauren Kelly Zung", "Hossein Hajimirsadeghi", "Greg Mori"], "title": "TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Predictive modeling on tabular data is the cornerstone of many real-world\napplications. Although gradient boosting machines and some recent deep models\nachieve strong performance on tabular data, they often lack interpretability.\nOn the other hand, large language models (LLMs) have demonstrated powerful\ncapabilities to generate human-like reasoning and explanations, but remain\nunder-performed for tabular data prediction. In this paper, we propose a new\napproach that leverages reasoning-based LLMs, trained using reinforcement\nlearning, to perform more accurate and explainable predictions on tabular data.\nOur method introduces custom reward functions that guide the model not only\ntoward high prediction accuracy but also toward human-understandable reasons\nfor its predictions. Experimental results show that our model achieves\npromising performance on financial benchmark datasets, outperforming most\nexisting LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u5728\u8868\u683c\u6570\u636e\u4e0a\u8fdb\u884c\u66f4\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u91d1\u878d\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u7684LLMs\u3002", "motivation": "\u5c3d\u7ba1\u68af\u5ea6\u63d0\u5347\u673a\u548c\u4e00\u4e9b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1b\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\u80fd\u529b\u7684LLMs\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u6a21\u578b\u4e0d\u4ec5\u8ffd\u6c42\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u8981\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u9884\u6d4b\u539f\u56e0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u91d1\u878d\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6709\u5e0c\u671b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u7684LLMs\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u63a8\u7406\u80fd\u529b\u7684LLMs\u53ef\u4ee5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u8868\u683c\u6570\u636e\u5206\u6790\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.21539", "pdf": "https://arxiv.org/pdf/2505.21539", "abs": "https://arxiv.org/abs/2505.21539", "authors": ["Ziming Wang", "Nan Xue", "Rebecka J\u00f6rnsten"], "title": "Equivariant Flow Matching for Point Cloud Assembly", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of point cloud assembly is to reconstruct a complete 3D shape by\naligning multiple point cloud pieces. This work presents a novel equivariant\nsolver for assembly tasks based on flow matching models. We first theoretically\nshow that the key to learning equivariant distributions via flow matching is to\nlearn related vector fields. Based on this result, we propose an assembly\nmodel, called equivariant diffusion assembly (Eda), which learns related vector\nfields conditioned on the input pieces. We further construct an equivariant\npath for Eda, which guarantees high data efficiency of the training process.\nOur numerical results show that Eda is highly competitive on practical\ndatasets, and it can even handle the challenging situation where the input\npieces are non-overlapped.", "AI": {"tldr": "This paper introduces Eda, a novel equivariant solver for point cloud assembly tasks that is based on flow matching models and demonstrates high competitiveness on practical datasets.", "motivation": "The motivation of this paper is to reconstruct complete 3D shapes by aligning multiple point cloud pieces using a new method that improves upon existing techniques.", "method": "The authors theoretically show the importance of learning related vector fields for flow matching. They then propose Eda, an assembly model which learns these vector fields conditioned on input pieces and construct an equivariant path for Eda to ensure efficient training.", "result": "Eda performs highly competitively on practical datasets and can handle challenging non-overlapped input pieces.", "conclusion": "Eda presents a promising approach for point cloud assembly with strong performance and data efficiency."}}
{"id": "2505.21813", "pdf": "https://arxiv.org/pdf/2505.21813", "abs": "https://arxiv.org/abs/2505.21813", "authors": ["Madi Matymov", "Ba-Hien Tran", "Michael Kampffmeyer", "Markus Heinonen", "Maurizio Filippone"], "title": "Optimizing Data Augmentation through Bayesian Model Selection", "categories": ["cs.LG", "stat.ML", "62F15, 68T07 (Primary) 62M45, 62C10, 65C60 (Secondary)"], "comment": "26 pages, 3 figures", "summary": "Data Augmentation (DA) has become an essential tool to improve robustness and\ngeneralization of modern machine learning. However, when deciding on DA\nstrategies it is critical to choose parameters carefully, and this can be a\ndaunting task which is traditionally left to trial-and-error or expensive\noptimization based on validation performance. In this paper, we counter these\nlimitations by proposing a novel framework for optimizing DA. In particular, we\ntake a probabilistic view of DA, which leads to the interpretation of\naugmentation parameters as model (hyper)-parameters, and the optimization of\nthe marginal likelihood with respect to these parameters as a Bayesian model\nselection problem. Due to its intractability, we derive a tractable Evidence\nLower BOund (ELBO), which allows us to optimize augmentation parameters jointly\nwith model parameters. We provide extensive theoretical results on variational\napproximation quality, generalization guarantees, invariance properties, and\nconnections to empirical Bayes. Through experiments on computer vision tasks,\nwe show that our approach improves calibration and yields robust performance\nover fixed or no augmentation. Our work provides a rigorous foundation for\noptimizing DA through Bayesian principles with significant potential for robust\nmachine learning.", "AI": {"tldr": "The paper proposes a novel framework for optimizing Data Augmentation (DA) by interpreting augmentation parameters as model (hyper)-parameters and using the Evidence Lower BOund (ELBO) for optimization.", "motivation": "Data Augmentation is crucial for improving robustness and generalization in machine learning, but choosing appropriate DA strategies and parameters is challenging and often relies on trial-and-error or expensive optimization.", "method": "The authors take a probabilistic view of DA, treating augmentation parameters as model (hyper)-parameters. They optimize the marginal likelihood with respect to these parameters as a Bayesian model selection problem, deriving a tractable ELBO for joint optimization with model parameters.", "result": "Experiments on computer vision tasks demonstrate that the approach improves calibration and yields more robust performance compared to fixed or no augmentation.", "conclusion": "This work establishes a rigorous foundation for optimizing DA using Bayesian principles, offering significant potential for advancing robust machine learning."}}
{"id": "2505.21541", "pdf": "https://arxiv.org/pdf/2505.21541", "abs": "https://arxiv.org/abs/2505.21541", "authors": ["Zitong Wang", "Hang Zhao", "Qianyu Zhou", "Xuequan Lu", "Xiangtai Li", "Yiren Song"], "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.", "AI": {"tldr": "The paper introduces AlphaBlend, a new large-scale dataset for transparent/semi-transparent layer decomposition, and DiffDecompose, a diffusion Transformer-based framework that addresses layer ambiguity, generalization, and data scarcity issues in image decomposition tasks.", "motivation": "Existing image decomposition methods struggle with disentangling semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and lack of datasets.", "method": "1. Introduced AlphaBlend - a large-scale dataset for transparent and semi-transparent layer decomposition.\n2. Proposed DiffDecompose - a diffusion Transformer-based framework that learns posterior over possible layer decompositions conditioned on input image, semantic prompts, and blending type. It performs In-Context Decomposition and uses Layer Position Encoding Cloning.", "result": "Extensive experiments on AlphaBlend and public LOGO dataset verify the effectiveness of DiffDecompose.", "conclusion": "DiffDecompose effectively addresses challenges in layer ambiguity, generalization, and data scarcity in image decomposition tasks."}}
{"id": "2505.21824", "pdf": "https://arxiv.org/pdf/2505.21824", "abs": "https://arxiv.org/abs/2505.21824", "authors": ["Praveen Kumar", "Vincent T. Metzger", "Scott A. Malec"], "title": "Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "The global prevalence of diabetes, particularly type 2 diabetes mellitus\n(T2DM), is rapidly increasing, posing significant health and economic\nchallenges. T2DM not only disrupts blood glucose regulation but also damages\nvital organs such as the heart, kidneys, eyes, nerves, and blood vessels,\nleading to substantial morbidity and mortality. In the US alone, the economic\nburden of diagnosed diabetes exceeded \\$400 billion in 2022. Early detection of\nindividuals at risk is critical to mitigating these impacts. While machine\nlearning approaches for T2DM prediction are increasingly adopted, many rely on\nsupervised learning, which is often limited by the lack of confirmed negative\ncases. To address this limitation, we propose a novel unsupervised framework\nthat integrates Non-negative Matrix Factorization (NMF) with statistical\ntechniques to identify individuals at risk of developing T2DM. Our method\nidentifies latent patterns of multimorbidity and polypharmacy among diagnosed\nT2DM patients and applies these patterns to estimate the T2DM risk in\nundiagnosed individuals. By leveraging data-driven insights from comorbidity\nand medication usage, our approach provides an interpretable and scalable\nsolution that can assist healthcare providers in implementing timely\ninterventions, ultimately improving patient outcomes and potentially reducing\nthe future health and economic burden of T2DM.", "AI": {"tldr": "The paper proposes an unsupervised framework using Non-negative Matrix Factorization (NMF) and statistical techniques to identify individuals at risk of developing type 2 diabetes mellitus (T2DM), addressing the limitation of supervised learning methods. It leverages multimorbidity and polypharmacy patterns among diagnosed T2DM patients for risk estimation in undiagnosed individuals.", "motivation": "The global prevalence of T2DM is increasing, leading to significant health and economic challenges. Early detection of at-risk individuals is critical, but existing machine learning approaches often rely on supervised learning, which is limited by the lack of confirmed negative cases.", "method": "A novel unsupervised framework integrating Non-negative Matrix Factorization (NMF) with statistical techniques. It identifies latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients and applies these patterns to estimate T2DM risk in undiagnosed individuals.", "result": "The method provides an interpretable and scalable solution that can assist healthcare providers in implementing timely interventions, potentially improving patient outcomes and reducing the future health and economic burden of T2DM.", "conclusion": "The proposed unsupervised framework offers a promising approach for early detection of T2DM risk, leveraging data-driven insights from comorbidity and medication usage."}}
{"id": "2505.21547", "pdf": "https://arxiv.org/pdf/2505.21547", "abs": "https://arxiv.org/abs/2505.21547", "authors": ["Weixing Wang", "Zifeng Ding", "Jindong Gu", "Rui Cao", "Christoph Meinel", "Gerard de Melo", "Haojin Yang"], "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify\nmultimodal representations by encoding visual inputs into a finite set of\ntokens. Despite their effectiveness, we find that these models still\nhallucinate non-existent objects. We hypothesize that this may be due to visual\npriors induced during training: When certain image tokens frequently co-occur\nin the same spatial regions and represent shared objects, they become strongly\nassociated with the verbalizations of those objects. As a result, the model may\nhallucinate by evoking visually absent tokens that often co-occur with present\nones. To test this assumption, we construct a co-occurrence graph of image\ntokens using a segmentation dataset and employ a Graph Neural Network (GNN)\nwith contrastive learning followed by a clustering method to group tokens that\nfrequently co-occur in similar visual contexts. We find that hallucinations\npredominantly correspond to clusters whose tokens dominate the input, and more\nspecifically, that the visually absent tokens in those clusters show much\nhigher correlation with hallucinated objects compared to tokens present in the\nimage. Based on this observation, we propose a hallucination mitigation method\nthat suppresses the influence of visually absent tokens by modifying latent\nimage embeddings during generation. Experiments show our method reduces\nhallucinations while preserving expressivity. Code is available at\nhttps://github.com/weixingW/CGC-VTD/tree/main", "AI": {"tldr": "Large Vision-Language Models (LVLMs) can hallucinate non-existent objects due to strong associations between co-occuring image tokens. This paper proposes a method using Graph Neural Networks and clustering to identify and mitigate these hallucinations by modifying latent image embeddings.", "motivation": "The motivation of this paper is to address the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate non-existent objects in their outputs. The authors hypothesize that this may be due to visual priors formed during training, leading to strong associations between certain image tokens that frequently co-occur.", "method": "To test their hypothesis, the authors construct a co-occurrence graph of image tokens using a segmentation dataset. They then use a Graph Neural Network (GNN) with contrastive learning and a clustering method to group tokens that often co-occur in similar visual contexts. Based on the observation that hallucinations correspond to clusters dominated by visually absent tokens, they propose a hallucination mitigation method that modifies latent image embeddings to suppress the influence of these absent tokens during generation.", "result": "Experiments show that the proposed method reduces hallucinations while preserving the expressivity of the model.", "conclusion": "In conclusion, the paper demonstrates that hallucinations in LVLMs are linked to clusters of co-occurring image tokens and presents an effective method to mitigate such hallucinations by adjusting latent image embeddings."}}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models.", "AI": {"tldr": "\u5728\u63a8\u7406\u65f6\u8ba1\u7b97\u7684\u7814\u7a76\u4e2d\uff0c\u5c3d\u7ba1\u987a\u5e8f\u6269\u5c55\uff08\u5982\u66f4\u957f\u7684\u601d\u7ef4\u94fe\uff09\u548c\u5e76\u884c\u6269\u5c55\uff08\u5982\u591a\u4e2a\u77ed\u601d\u7ef4\u94fe\u7684\u591a\u6570\u6295\u7968\uff09\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6700\u4f18\u7684\u63a8\u7406\u65f6\u8ba1\u7b97\u5206\u914d\u4ecd\u4e0d\u660e\u786e\u3002\u672c\u6587\u901a\u8fc7\u56fe\u8fde\u901a\u6027\u95ee\u9898\u5c55\u793a\u4e86\u987a\u5e8f\u6269\u5c55\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u6307\u6570\u7ea7\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u63a8\u7406\u65f6\u8ba1\u7b97\u7684\u6700\u4f18\u5206\u914d\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u987a\u5e8f\u6269\u5c55\u4e0e\u5e76\u884c\u6269\u5c55\u7684\u4f18\u52a3\u5bf9\u6bd4\u3002", "method": "\u901a\u8fc7\u56fe\u8fde\u901a\u6027\u95ee\u9898\u8bbe\u7f6e\u63a8\u7406\u573a\u666f\uff0c\u6bd4\u8f83\u987a\u5e8f\u6269\u5c55\u4e0e\u5e76\u884c\u6269\u5c55\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528\u5305\u62ec\u4ece\u5934\u8bad\u7ec3\u7528\u4e8e\u56fe\u8fde\u901a\u6027\u7684\u6a21\u578b\u548c\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5185\u7684\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u4e00\u4e9b\u57fa\u4e8e\u6311\u6218\u6027\u56fe\u5206\u5e03\u7684\u56fe\u8fde\u901a\u6027\u95ee\u9898\u573a\u666f\uff0c\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u987a\u5e8f\u6269\u5c55\u76f8\u5bf9\u4e8e\u5e76\u884c\u6269\u5c55\u5177\u6709\u6307\u6570\u7ea7\u4f18\u52bf\u3002", "conclusion": "\u987a\u5e8f\u6269\u5c55\u5728\u7279\u5b9a\u7684\u63a8\u7406\u573a\u666f\u4e2d\u76f8\u8f83\u4e8e\u5e76\u884c\u6269\u5c55\u6709\u663e\u8457\u7684\u4f18\u52bf\uff0c\u8fd9\u4e3a\u63a8\u7406\u65f6\u8ba1\u7b97\u8d44\u6e90\u7684\u4f18\u5316\u5206\u914d\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.21548", "pdf": "https://arxiv.org/pdf/2505.21548", "abs": "https://arxiv.org/abs/2505.21548", "authors": ["Dhruv Agarwal", "Anya Shukla", "Sunayana Sitaram", "Aditya Vashistha"], "title": "Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.CY"], "comment": "Under review", "summary": "Large language models (LLMs) are used around the world but exhibit Western\ncultural tendencies. To address this cultural misalignment, many countries have\nbegun developing \"regional\" LLMs tailored to local communities. Yet it remains\nunclear whether these models merely speak the language of their users or also\nreflect their cultural values and practices. Using India as a case study, we\nevaluate five Indic and five global LLMs along two key dimensions: values (via\nthe Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench\nand NormAd). Across all four tasks, we find that Indic models do not align more\nclosely with Indian cultural norms than global models. In fact, an average\nAmerican person is a better proxy for Indian cultural values than any Indic\nmodel. Even prompting strategies fail to meaningfully improve alignment.\nAblations show that regional fine-tuning does not enhance cultural competence\nand may in fact hurt it by impeding recall of existing knowledge. We trace this\nfailure to the scarcity of high-quality, untranslated, and culturally grounded\npretraining and fine-tuning data. Our study positions cultural evaluation as a\nfirst-class requirement alongside multilingual benchmarks and offers a reusable\nmethodology for developers. We call for deeper investments in culturally\nrepresentative data to build and evaluate truly sovereign LLMs.", "AI": {"tldr": "\u5c3d\u7ba1\u5370\u5ea6\u672c\u5730\u8bed\u8a00\u6a21\u578b\uff08Indic LLMs\uff09\u88ab\u671f\u671b\u66f4\u7b26\u5408\u5f53\u5730\u6587\u5316\uff0c\u4f46\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5e76\u672a\u6bd4\u5168\u7403\u6a21\u578b\u66f4\u597d\u5730\u53cd\u6620\u5370\u5ea6\u7684\u6587\u5316\u4ef7\u503c\u89c2\u548c\u5b9e\u8df5\u3002\u7814\u7a76\u6307\u51fa\uff0c\u6784\u5efa\u771f\u6b63\u5177\u6709\u6587\u5316\u4ee3\u8868\u6027\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u66f4\u591a\u9ad8\u8d28\u91cf\u3001\u672a\u7ffb\u8bd1\u4e14\u57fa\u4e8e\u6587\u5316\u7684\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u8bc4\u4f30\u533a\u57df\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4e0d\u4ec5\u80fd\u591f\u4f7f\u7528\u7528\u6237\u7684\u8bed\u8a00\u4ea4\u6d41\uff0c\u8fd8\u80fd\u53cd\u6620\u5176\u6587\u5316\u4ef7\u503c\u548c\u5b9e\u8df5\u3002\u4ee5\u5370\u5ea6\u4e3a\u6848\u4f8b\uff0c\u6bd4\u8f83\u5370\u5ea6\u672c\u5730\u4e0e\u5168\u7403\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u548c\u5b9e\u8df5\u4e0a\u7684\u5951\u5408\u5ea6\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u7ef4\u5ea6\u2014\u2014\u4ef7\u503c\u89c2\uff08\u5229\u7528Inglehart-Welzel\u5730\u56fe\u548cGlobalOpinionQA\uff09\u548c\u5b9e\u8df5\uff08\u5229\u7528CulturalBench\u548cNormAd\uff09\uff0c\u5bf9\u4e94\u4e2a\u5370\u5ea6\u672c\u5730\u53ca\u4e94\u4e2a\u5168\u7403\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5370\u5ea6\u672c\u5730\u6a21\u578b\u5728\u53cd\u6620\u5370\u5ea6\u6587\u5316\u89c4\u8303\u65b9\u9762\u5e76\u4e0d\u6bd4\u5168\u7403\u6a21\u578b\u66f4\u597d\u3002\u5b9e\u9645\u4e0a\uff0c\u666e\u901a\u7f8e\u56fd\u4eba\u7684\u6587\u5316\u4ef7\u503c\u89c2\u6bd4\u4efb\u4f55\u5370\u5ea6\u672c\u5730\u6a21\u578b\u66f4\u80fd\u4ee3\u8868\u5370\u5ea6\u6587\u5316\u3002\u63d0\u793a\u7b56\u7565\u4e5f\u672a\u80fd\u663e\u8457\u6539\u5584\u4e00\u81f4\u6027\u3002\u533a\u57df\u5fae\u8c03\u4e0d\u4f1a\u589e\u5f3a\u6587\u5316\u80fd\u529b\uff0c\u751a\u81f3\u53ef\u80fd\u59a8\u788d\u73b0\u6709\u77e5\u8bc6\u7684\u56de\u5fc6\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u52a0\u5927\u5bf9\u6587\u5316\u4ee3\u8868\u6027\u6570\u636e\u7684\u6295\u8d44\uff0c\u4ee5\u5efa\u7acb\u548c\u8bc4\u4f30\u771f\u6b63\u4e3b\u6743\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5c06\u6587\u5316\u8bc4\u4f30\u5b9a\u4f4d\u4e3a\u4e0e\u591a\u8bed\u8a00\u57fa\u51c6\u540c\u7b49\u91cd\u8981\u7684\u8981\u6c42\u3002"}}
{"id": "2505.21829", "pdf": "https://arxiv.org/pdf/2505.21829", "abs": "https://arxiv.org/abs/2505.21829", "authors": ["Antonio Orvieto", "Robert Gower"], "title": "In Search of Adam's Secret Sauce", "categories": ["cs.LG"], "comment": null, "summary": "Understanding the remarkable efficacy of Adam when training transformer-based\nlanguage models has become a central research topic within the optimization\ncommunity. To gain deeper insights, several simplifications of Adam have been\nproposed, such as the signed gradient and signed momentum methods. In this\nwork, we conduct an extensive empirical study - training over 1,300 language\nmodels across different data configurations and scales - comparing Adam to\nseveral known simplified variants. We find that signed momentum methods are\nfaster than SGD, but consistently underperform relative to Adam, even after\ncareful tuning of momentum, clipping setting and learning rates. However, our\nanalysis reveals a compelling option that preserves near-optimal performance\nwhile allowing for new insightful reformulations: constraining the Adam\nmomentum parameters to be equal. Beyond robust performance, this choice affords\nnew theoretical insights, highlights the \"secret sauce\" on top of signed\nmomentum, and grants a precise statistical interpretation: we show that Adam in\nthis setting implements a natural online algorithm for estimating the mean and\nvariance of gradients-one that arises from a mean-field Gaussian variational\ninference perspective.", "AI": {"tldr": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u53d1\u73b0Adam\u4f18\u5316\u5668\u5728\u8bad\u7ec3transformer\u8bed\u8a00\u6a21\u578b\u65f6\u8868\u73b0\u51fa\u8272\u7684\u5173\u952e\u5728\u4e8e\u5176\u52a8\u91cf\u53c2\u6570\u76f8\u7b49\u7684\u8bbe\u7f6e\uff0c\u8fd9\u4e0d\u4ec5\u6027\u80fd\u7a33\u5065\u4e14\u6709\u65b0\u7684\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "\u7406\u89e3Adam\u5728\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u8bed\u8a00\u6a21\u578b\u65f6\u4e3a\u4f55\u5982\u6b64\u6709\u6548\u662f\u4f18\u5316\u9886\u57df\u7684\u91cd\u8981\u8bfe\u9898\u3002\u4e3a\u4e86\u6df1\u5165\u63a2\u8ba8\uff0c\u5df2\u63d0\u51fa\u4e86Adam\u7684\u4e00\u4e9b\u7b80\u5316\u53d8\u4f53\uff0c\u5982\u5e26\u7b26\u53f7\u68af\u5ea6\u548c\u5e26\u7b26\u53f7\u52a8\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7ecf\u9a8c\u7814\u7a76\uff0c\u8bad\u7ec3\u4e86\u8d85\u8fc71,300\u4e2a\u4e0d\u540c\u6570\u636e\u914d\u7f6e\u548c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u6bd4\u8f83Adam\u4e0e\u51e0\u79cd\u5df2\u77e5\u7b80\u5316\u53d8\u4f53\u7684\u6027\u80fd\u3002\u7279\u522b\u5173\u6ce8\u5e26\u7b26\u53f7\u52a8\u91cf\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86Adam\u52a8\u91cf\u53c2\u6570\u76f8\u7b49\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5e26\u7b26\u53f7\u52a8\u91cf\u65b9\u6cd5\u867d\u7136\u6bd4SGD\u5feb\uff0c\u4f46\u59cb\u7ec8\u4e0d\u5982Adam\u8868\u73b0\u597d\u3002\u800c\u5c06Adam\u7684\u52a8\u91cf\u53c2\u6570\u8bbe\u4e3a\u76f8\u7b49\u80fd\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u5e76\u5141\u8bb8\u65b0\u7684\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "Adam\u5728\u52a8\u91cf\u53c2\u6570\u76f8\u7b49\u65f6\uff0c\u4e0d\u4ec5\u6027\u80fd\u7a33\u5065\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u7136\u7684\u5728\u7ebf\u7b97\u6cd5\u6765\u4f30\u8ba1\u68af\u5ea6\u7684\u5747\u503c\u548c\u65b9\u5dee\uff0c\u8be5\u7b97\u6cd5\u6e90\u4e8e\u5e73\u5747\u573a\u9ad8\u65af\u53d8\u5206\u63a8\u65ad\u89c6\u89d2\u3002"}}
{"id": "2505.21550", "pdf": "https://arxiv.org/pdf/2505.21550", "abs": "https://arxiv.org/abs/2505.21550", "authors": ["Rishi Sharma", "Martijn de Vos", "Pradyumna Chari", "Ramesh Raskar", "Anne-Marie Kermarrec"], "title": "Collaborative Agentic AI Needs Interoperability Across Ecosystems", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": null, "summary": "Collaborative agentic AI is projected to transform entire industries by\nenabling AI-powered agents to autonomously perceive, plan, and act within\ndigital environments. Yet, current solutions in this field are all built in\nisolation, and we are rapidly heading toward a landscape of fragmented,\nincompatible ecosystems. In this position paper, we argue that\ninteroperability, achieved by the adoption of minimal standards, is essential\nto ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To\nthis end, we devise a minimal architectural foundation for collaborative\nagentic AI, named Web of Agents, which is composed of four components:\nagent-to-agent messaging, interaction interoperability, state management, and\nagent discovery. Web of Agents adopts existing standards and reuses existing\ninfrastructure where possible. With Web of Agents, we take the first but\ncritical step toward interoperable agentic systems and offer a pragmatic path\nforward before ecosystem fragmentation becomes the norm.", "AI": {"tldr": "This position paper proposes 'Web of Agents', a minimal architectural foundation for collaborative agentic AI, to promote interoperability among AI-powered agents and prevent ecosystem fragmentation through four components: agent-to-agent messaging, interaction interoperability, state management, and agent discovery.", "motivation": "The field of collaborative agentic AI is advancing rapidly, but current solutions are developed in isolation, leading to fragmented and incompatible ecosystems. There's an urgent need for interoperability to ensure open, secure, web-scale, and widely-adopted agentic systems.", "method": "The authors devise the 'Web of Agents', composed of four key components - agent-to-agent messaging, interaction interoperability, state management, and agent discovery - which adopts existing standards and infrastructure where possible to create an interoperable system.", "result": "The proposal introduces a pragmatic path forward for creating interoperable agentic systems before ecosystem fragmentation becomes the norm, marking a critical first step in this direction.", "conclusion": "Interoperability achieved by adopting minimal standards is essential for ensuring open, secure, and widely-adopted agentic ecosystems. The Web of Agents offers a foundational framework to achieve this goal."}}
{"id": "2505.21835", "pdf": "https://arxiv.org/pdf/2505.21835", "abs": "https://arxiv.org/abs/2505.21835", "authors": ["Xiangyu Chen", "Jing Liu", "Ye Wang", "Matthew Brand", "Pu", "Wang", "Toshiaki Koike-Akino"], "title": "TuneComp: Joint Fine-tuning and Compression for Large Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "Preliminary Work", "summary": "To reduce model size during post-training, compression methods, including\nknowledge distillation, low-rank approximation, and pruning, are often applied\nafter fine-tuning the model. However, sequential fine-tuning and compression\nsacrifices performance, while creating a larger than necessary model as an\nintermediate step. In this work, we aim to reduce this gap, by directly\nconstructing a smaller model while guided by the downstream task. We propose to\njointly fine-tune and compress the model by gradually distilling it to a pruned\nlow-rank structure. Experiments demonstrate that joint fine-tuning and\ncompression significantly outperforms other sequential compression methods.", "AI": {"tldr": "Joint fine-tuning and compression significantly outperforms sequential compression methods in reducing model size while maintaining performance.", "motivation": "To address the issue of sacrificing performance and creating unnecessarily large intermediate models during post-training compression.", "method": "Propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure directly constructed for the downstream task.", "result": "Experiments show joint fine-tuning and compression outperforms other sequential compression methods.", "conclusion": "Joint fine-tuning and compression is an effective approach to reduce model size without sacrificing performance."}}
{"id": "2505.21551", "pdf": "https://arxiv.org/pdf/2505.21551", "abs": "https://arxiv.org/abs/2505.21551", "authors": ["Emmanuel Akinrintoyo", "Nadine Abdelhalim", "Nicole Salomons"], "title": "WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "comment": "Submitted to Interspeech 2025 (Accepted)", "summary": "Whisper fails to correctly transcribe dementia speech because persons with\ndementia (PwDs) often exhibit irregular speech patterns and disfluencies such\nas pauses, repetitions, and fragmented sentences. It was trained on standard\nspeech and may have had little or no exposure to dementia-affected speech.\nHowever, correct transcription is vital for dementia speech for cost-effective\ndiagnosis and the development of assistive technology. In this work, we\nfine-tune Whisper with the open-source dementia speech dataset (DementiaBank)\nand our in-house dataset to improve its word error rate (WER). The fine-tuning\nalso includes filler words to ascertain the filler inclusion rate (FIR) and F1\nscore. The fine-tuned models significantly outperformed the off-the-shelf\nmodels. The medium-sized model achieved a WER of 0.24, outperforming previous\nwork. Similarly, there was a notable generalisability to unseen data and speech\npatterns.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u5f00\u6e90\u5931\u667a\u75c7\u8bed\u97f3\u6570\u636e\u96c6(DementiaBank)\u548c\u5185\u90e8\u6570\u636e\u96c6\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u5904\u7406\u5931\u667a\u75c7\u8bed\u97f3\u65f6\u7684\u8bcd\u9519\u7387\uff08WER\uff09\u8868\u73b0\u3002\u4e2d\u7b49\u89c4\u6a21\u7684\u6a21\u578b\u5b9e\u73b0\u4e860.24\u7684WER\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u6570\u636e\u548c\u8bed\u97f3\u6a21\u5f0f\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u5931\u667a\u75c7\u60a3\u8005\u7684\u8bed\u97f3\u5e38\u5305\u542b\u4e0d\u89c4\u5219\u6a21\u5f0f\u548c\u4e0d\u6d41\u7545\u73b0\u8c61\uff08\u5982\u505c\u987f\u3001\u91cd\u590d\u548c\u7834\u788e\u53e5\u5f0f\uff09\uff0c\u73b0\u6709\u7684\u8bed\u97f3\u8f6c\u5f55\u6280\u672f\uff08\u5982Whisper\uff09\u65e0\u6cd5\u51c6\u786e\u8f6c\u5f55\u8fd9\u4e9b\u8bed\u97f3\u3002\u800c\u51c6\u786e\u7684\u8bed\u97f3\u8f6c\u5f55\u5bf9\u5931\u667a\u75c7\u7684\u7ecf\u6d4e\u8bca\u65ad\u53ca\u8f85\u52a9\u6280\u672f\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u8005\u91c7\u7528\u5f00\u6e90\u6570\u636e\u96c6DementiaBank\u4ee5\u53ca\u5185\u90e8\u6570\u636e\u96c6\u5bf9Whisper\u8fdb\u884c\u5fae\u8c03\uff0c\u540c\u65f6\u52a0\u5165\u586b\u5145\u8bcd\u4ee5\u8bc4\u4f30\u586b\u5145\u8bcd\u5305\u542b\u7387\uff08FIR\uff09\u548cF1\u5206\u6570\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u8bcd\u9519\u7387\uff08WER\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6210\u6a21\u578b\uff0c\u5176\u4e2d\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u8fbe\u5230\u4e860.24\u7684WER\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u6570\u636e\u548c\u8bed\u97f3\u6a21\u5f0f\u4e0a\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5fae\u8c03\u8fc7\u7684Whisper\u6a21\u578b\u5728\u5904\u7406\u5931\u667a\u75c7\u8bed\u97f3\u65b9\u9762\u8868\u73b0\u66f4\u4f73\uff0c\u5177\u5907\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2505.21841", "pdf": "https://arxiv.org/pdf/2505.21841", "abs": "https://arxiv.org/abs/2505.21841", "authors": ["Jiahui Zhu", "Kihyun Yu", "Dabeen Lee", "Xin Liu", "Honghao Wei"], "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 41 st International Conference on Machine Learning", "summary": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments.", "AI": {"tldr": "The paper introduces OMDPD, an algorithm for online safe reinforcement learning in adversarial settings with optimal regret and constraint violation bounds.", "motivation": "Existing methods for constrained Markov decision processes (CMDPs) can handle sublinear regret under stochastic constraints but struggle in adversarial settings where constraints are unknown, time-varying, and potentially adversarially designed.", "method": "Proposes the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm which addresses online CMDPs with anytime adversarial constraints without relying on Slater's condition or a strictly known safe policy.", "result": "OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)). Access to accurate estimates of rewards and transitions can further improve these bounds.", "conclusion": "The results provide practical guarantees for safe decision-making in adversarial environments."}}
{"id": "2505.21553", "pdf": "https://arxiv.org/pdf/2505.21553", "abs": "https://arxiv.org/abs/2505.21553", "authors": ["Hui Ma", "Kai Yang"], "title": "MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Network traffic prediction techniques have attracted much attention since\nthey are valuable for network congestion control and user experience\nimprovement. While existing prediction techniques can achieve favorable\nperformance when there is sufficient training data, it remains a great\nchallenge to make accurate predictions when only a small amount of training\ndata is available. To tackle this problem, we propose a deep learning model,\nentitled MetaSTNet, based on a multimodal meta-learning framework. It is an\nend-to-end network architecture that trains the model in a simulator and\ntransfers the meta-knowledge to a real-world environment, which can quickly\nadapt and obtain accurate predictions on a new task with only a small amount of\nreal-world training data. In addition, we further employ cross conformal\nprediction to assess the calibrated prediction intervals. Extensive experiments\nhave been conducted on real-world datasets to illustrate the efficiency and\neffectiveness of MetaSTNet.", "AI": {"tldr": "\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u6280\u672f\u5728\u7f51\u7edc\u62e5\u585e\u63a7\u5236\u548c\u7528\u6237\u4f53\u9a8c\u63d0\u5347\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u73b0\u6709\u7684\u9884\u6d4b\u6280\u672f\u5728\u6709\u8db3\u591f\u7684\u8bad\u7ec3\u6570\u636e\u65f6\u53ef\u4ee5\u53d6\u5f97\u826f\u597d\u7684\u6548\u679c\uff0c\u4f46\u5728\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u4ecd\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5143\u5b66\u4e60\u6846\u67b6\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bMetaSTNet\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8f6c\u79fb\u5143\u77e5\u8bc6\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u4ec5\u4f7f\u7528\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u83b7\u5f97\u51c6\u786e\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u4ea4\u53c9\u4e00\u81f4\u6027\u9884\u6d4b\u6765\u8bc4\u4f30\u6821\u51c6\u540e\u7684\u9884\u6d4b\u533a\u95f4\u3002", "motivation": "\u7f51\u7edc\u6d41\u91cf\u9884\u6d4b\u5bf9\u4e8e\u7f51\u7edc\u62e5\u585e\u63a7\u5236\u548c\u7528\u6237\u4f53\u9a8c\u6539\u8fdb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u4ec5\u6709\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMetaSTNet\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e\u591a\u6a21\u6001\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5c06\u5143\u77e5\u8bc6\u8f6c\u79fb\u5230\u73b0\u5b9e\u73af\u5883\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u4e0b\u7684\u5feb\u901f\u9002\u5e94\u548c\u51c6\u786e\u9884\u6d4b\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u4ea4\u53c9\u4e00\u81f4\u6027\u9884\u6d4b\u6765\u8bc4\u4f30\u6821\u51c6\u540e\u7684\u9884\u6d4b\u533a\u95f4\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMetaSTNet\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5177\u6709\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "MetaSTNet\u80fd\u591f\u5728\u4ec5\u6709\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u63d0\u4f9b\u51c6\u786e\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u5176\u9884\u6d4b\u533a\u95f4\u7ecf\u8fc7\u6821\u51c6\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.21852", "pdf": "https://arxiv.org/pdf/2505.21852", "abs": "https://arxiv.org/abs/2505.21852", "authors": ["Akifumi Wachi", "Kohei Miyaguchi", "Takumi Tanabe", "Rei Sato", "Youhei Akimoto"], "title": "A Provable Approach for End-to-End Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.RO", "math.IT"], "comment": "27 pages", "summary": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure\nthe safety of a policy throughout the entire process, from learning to\noperation. However, existing safe RL paradigms inherently struggle to achieve\nthis objective. We propose a method, called Provably Lifetime Safe RL (PLS),\nthat integrates offline safe RL with safe policy deployment to address this\nchallenge. Our proposed method learns a policy offline using return-conditioned\nsupervised learning and then deploys the resulting policy while cautiously\noptimizing a limited set of parameters, known as target returns, using Gaussian\nprocesses (GPs). Theoretically, we justify the use of GPs by analyzing the\nmathematical relationship between target and actual returns. We then prove that\nPLS finds near-optimal target returns while guaranteeing safety with high\nprobability. Empirically, we demonstrate that PLS outperforms baselines both in\nsafety and reward performance, thereby achieving the longstanding goal to\nobtain high rewards while ensuring the safety of a policy throughout the\nlifetime from learning to operation.", "AI": {"tldr": "The paper introduces Provably Lifetime Safe RL (PLS), a method combining offline safe RL and safe policy deployment, using return-conditioned supervised learning and Gaussian processes to ensure safety with high probability while achieving near-optimal target returns.", "motivation": "Safe reinforcement learning requires ensuring the safety of a policy throughout its entire process from learning to operation, which existing paradigms struggle to achieve effectively.", "method": "PLS learns a policy offline via return-conditioned supervised learning, then deploys it while cautiously optimizing a limited set of parameters (target returns) using Gaussian processes. Theoretical justification for GPs is provided by analyzing the mathematical relationship between target and actual returns.", "result": "PLS outperforms baselines in both safety and reward performance, achieving the goal of obtaining high rewards while maintaining safety throughout the policy's lifetime.", "conclusion": "PLS addresses the challenge of ensuring safety throughout the RL process by integrating offline safe RL with safe policy deployment, proving safety with high probability and achieving near-optimal target returns."}}
{"id": "2505.21556", "pdf": "https://arxiv.org/pdf/2505.21556", "abs": "https://arxiv.org/abs/2505.21556", "authors": ["Hee-Seon Kim", "Minbeom Kim", "Wonjun Lee", "Kihyun Kim", "Changick Kim"], "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts", "categories": ["cs.CV", "cs.AI"], "comment": "LVLM, Jailbreak", "summary": "Optimization-based jailbreaks typically adopt the Toxic-Continuation setting\nin large vision-language models (LVLMs), following the standard next-token\nprediction objective. In this setting, an adversarial image is optimized to\nmake the model predict the next token of a toxic prompt. However, we find that\nthe Toxic-Continuation paradigm is effective at continuing already-toxic\ninputs, but struggles to induce safety misalignment when explicit toxic signals\nare absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike\nprior work, we optimize adversarial images to induce toxic outputs from benign\nconditioning. Since benign conditioning contains no safety violations, the\nimage alone must break the model's safety mechanisms. Our method outperforms\nprior approaches, transfers in black-box settings, and complements text-based\njailbreaks. These results reveal an underexplored vulnerability in multimodal\nalignment and introduce a fundamentally new direction for jailbreak approaches.", "AI": {"tldr": "The paper introduces Benign-to-Toxic (B2T) jailbreak, a new method to optimize adversarial images inducing toxic outputs from benign conditioning in large vision-language models (LVLMs), revealing a vulnerability in multimodal alignment.", "motivation": "Existing Toxic-Continuation setting is effective for continuing already-toxic inputs but struggles to induce safety misalignment when explicit toxic signals are absent.", "method": "Propose the Benign-to-Toxic (B2T) jailbreak paradigm where adversarial images are optimized to induce toxic outputs from benign conditioning without any safety violations.", "result": "The B2T method outperforms prior approaches, transfers in black-box settings, and complements text-based jailbreaks.", "conclusion": "This work reveals an underexplored vulnerability in multimodal alignment and introduces a fundamentally new direction for jailbreak approaches."}}
{"id": "2505.21857", "pdf": "https://arxiv.org/pdf/2505.21857", "abs": "https://arxiv.org/abs/2505.21857", "authors": ["Mijung Park"], "title": "Revisiting Bayesian Model Averaging in the Era of Foundation Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit the classical, full-fledged Bayesian model averaging (BMA)\nparadigm to ensemble pre-trained and/or lightly-finetuned foundation models to\nenhance the classification performance on image and text data. To make BMA\ntractable under foundation models, we introduce trainable linear classifiers\nthat take frozen features from the pre-trained foundation models as inputs. The\nmodel posteriors over the linear classifiers tell us which linear heads and\nfrozen features are better suited for a given dataset, resulting in a\nprincipled model ensembling method. Furthermore, we propose a computationally\ncheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize\nthe model ensemble weights, just like those weights based on model posterior\ndistributions in BMA, by reducing the amount of surprise (expected entropy of\nthe predictions) we get from predictions of ensembled models. With the rapid\ndevelopment of foundation models, these approaches will enable the\nincorporation of future, possibly significantly better foundation models to\nenhance the performance of challenging classification tasks.", "AI": {"tldr": "The paper revisits Bayesian Model Averaging (BMA) and proposes Optimizable Model Averaging (OMA) to enhance classification performance using pre-trained foundation models.", "motivation": "To improve classification performance on image and text data by leveraging pre-trained and lightly fine-tuned foundation models through ensemble methods.", "method": "Introduces trainable linear classifiers with frozen features from pre-trained models for BMA, and proposes OMA which directly optimizes model ensemble weights by minimizing prediction entropy.", "result": "These approaches allow for the incorporation of future, potentially superior foundation models, enhancing performance on challenging classification tasks.", "conclusion": "BMA and OMA provide principled ways to ensemble models, improving classification tasks and accommodating better foundation models as they develop."}}
{"id": "2505.21557", "pdf": "https://arxiv.org/pdf/2505.21557", "abs": "https://arxiv.org/abs/2505.21557", "authors": ["Polad Geidarov"], "title": "Analytical Calculation of Weights Convolutional Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an algorithm for analytically calculating the weights and\nthresholds of convolutional neural networks (CNNs) without using standard\ntraining procedures. The algorithm enables the determination of CNN parameters\nbased on just 10 selected images from the MNIST dataset, each representing a\ndigit from 0 to 9. As part of the method, the number of channels in CNN layers\nis also derived analytically. A software module was implemented in C++ Builder,\nand a series of experiments were conducted using the MNIST dataset. Results\ndemonstrate that the analytically computed CNN can recognize over half of 1000\nhandwritten digit images without any training, achieving inference in fractions\nof a second. These findings suggest that CNNs can be constructed and applied\ndirectly for classification tasks without training, using purely analytical\ncomputation of weights.", "AI": {"tldr": "The paper introduces an algorithm for calculating CNN weights and thresholds analytically without standard training, using only 10 MNIST images. It shows that such a CNN can classify over half of 1000 handwritten digit images instantly without training.", "motivation": "To explore the possibility of constructing CNNs without undergoing traditional training procedures by analytically determining their weights and thresholds.", "method": "An algorithm is presented to analytically calculate the weights and thresholds of CNNs using just 10 selected images from the MNIST dataset, each representing a digit from 0 to 9. The number of channels in CNN layers is also derived analytically.", "result": "The analytically computed CNN can recognize over half of 1000 handwritten digit images without any training, achieving inference in fractions of a second.", "conclusion": "CNNs can be constructed and applied directly for classification tasks without training, using purely analytical computation of weights."}}
{"id": "2505.21877", "pdf": "https://arxiv.org/pdf/2505.21877", "abs": "https://arxiv.org/abs/2505.21877", "authors": ["Hongyao Chen", "Tianyang Xu", "Xiaojun Wu", "Josef Kittler"], "title": "Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Batch Normalisation (BN) is widely used in conventional deep neural network\ntraining to harmonise the input-output distributions for each batch of data.\nHowever, federated learning, a distributed learning paradigm, faces the\nchallenge of dealing with non-independent and identically distributed data\namong the client nodes. Due to the lack of a coherent methodology for updating\nBN statistical parameters, standard BN degrades the federated learning\nperformance. To this end, it is urgent to explore an alternative normalisation\nsolution for federated learning. In this work, we resolve the dilemma of the BN\nlayer in federated learning by developing a customised normalisation approach,\nHybrid Batch Normalisation (HBN). HBN separates the update of statistical\nparameters (i.e. , means and variances used for evaluation) from that of\nlearnable parameters (i.e. , parameters that require gradient updates),\nobtaining unbiased estimates of global statistical parameters in distributed\nscenarios. In contrast with the existing solutions, we emphasise the supportive\npower of global statistics for federated learning. The HBN layer introduces a\nlearnable hybrid distribution factor, allowing each computing node to\nadaptively mix the statistical parameters of the current batch with the global\nstatistics. Our HBN can serve as a powerful plugin to advance federated\nlearning performance. It reflects promising merits across a wide range of\nfederated learning settings, especially for small batch sizes and heterogeneous\ndata.", "AI": {"tldr": "The paper proposes Hybrid Batch Normalisation (HBN), a customised normalisation approach for federated learning that separates the update of statistical parameters from learnable parameters and introduces a learnable hybrid distribution factor.", "motivation": "Federated learning faces challenges with non-independent and identically distributed data among client nodes, and standard Batch Normalisation degrades performance due to lack of coherent methodology for updating BN statistical parameters.", "method": "HBN separates updates of statistical parameters (means and variances) from learnable parameters (those requiring gradient updates) and provides unbiased estimates of global statistical parameters in distributed scenarios. A learnable hybrid distribution factor is introduced to allow each node to adaptively mix current batch statistics with global statistics.", "result": "HBN shows promising improvements in federated learning performance across various settings, particularly with small batch sizes and heterogeneous data.", "conclusion": "HBN serves as a powerful plugin to enhance federated learning performance."}}
{"id": "2505.21558", "pdf": "https://arxiv.org/pdf/2505.21558", "abs": "https://arxiv.org/abs/2505.21558", "authors": ["Elhoucine Elfatimia", "Recep Eryigitb", "Lahcen Elfatimi"], "title": "A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification", "categories": ["cs.CV", "cs.AI", "cs.LG", "na"], "comment": "11 Figure", "summary": "Agricultural research has accelerated in recent years, yet farmers often lack\nthe time and resources for on-farm research due to the demands of crop\nproduction and farm operations. Seed classification offers valuable insights\ninto quality control, production efficiency, and impurity detection. Early\nidentification of seed types is critical to reducing the cost and risk\nassociated with field emergence, which can lead to yield losses or disruptions\nin downstream processes like harvesting. Seed sampling supports growers in\nmonitoring and managing seed quality, improving precision in determining seed\npurity levels, guiding management adjustments, and enhancing yield estimations.\nThis study proposes a novel convolutional neural network (CNN)-based framework\nfor the efficient classification of ten common Brassica seed types. The\napproach addresses the inherent challenge of texture similarity in seed images\nusing a custom-designed CNN architecture. The model's performance was evaluated\nagainst several pre-trained state-of-the-art architectures, with adjustments to\nlayer configurations for optimized classification. Experimental results using\nour collected Brassica seed dataset demonstrate that the proposed model\nachieved a high accuracy rate of 93 percent.", "AI": {"tldr": "Recent agricultural research has seen growth, but farmers lack time for on-farm research. Seed classification aids in quality control and efficiency. Early seed type identification reduces cost and risk. Seed sampling helps growers monitor quality and improve precision. This study proposes a CNN-based framework to classify ten Brassica seed types, addressing texture similarity challenges with a custom CNN architecture. Evaluated against pre-trained models, the proposed model achieved 93% accuracy.", "motivation": "Farmers often lack the time and resources for on-farm research due to crop production demands. Seed classification is important for quality control, production efficiency, and impurity detection.", "method": "A novel CNN-based framework was developed for efficient classification of ten common Brassica seed types, using a custom-designed CNN architecture to address texture similarity challenges in seed images.", "result": "The proposed model achieved a high accuracy rate of 93 percent when evaluated using the collected Brassica seed dataset.", "conclusion": "The study's CNN-based framework effectively classifies Brassica seed types, offering a valuable tool for improving seed quality control and production efficiency."}}
{"id": "2505.21882", "pdf": "https://arxiv.org/pdf/2505.21882", "abs": "https://arxiv.org/abs/2505.21882", "authors": ["Ruijie Li", "Xiang Zhao", "Qiao Ning", "Shikai Guo"], "title": "HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis", "categories": ["cs.LG"], "comment": "14 pages, 9 figures (including subfigures), 5 tables. This is the\n  first work to explore and effectively model momentum across multiple\n  granularities in professional tennis tournaments", "summary": "In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects\nthe dynamic shifts in performance of athletes that can decisively influence\nmatch outcomes. Despite its significance, momentum in terms of effective\nmodeling and multi-granularity analysis across points, games, sets, and matches\nin tennis tournaments remains underexplored. In this study, we define a novel\nMomentum Score (MS) metric to quantify a player's momentum level in\nmulti-granularity tennis tournaments, and design HydraNet, a momentum-driven\nstate-space duality-based framework, to model MS by integrating thirty-two\nheterogeneous dimensions of athletes performance in serve, return, psychology\nand fatigue. HydraNet integrates a Hydra module, which builds upon a\nstate-space duality (SSD) framework, capturing explicit momentum with a\nsliding-window mechanism and implicit momentum through cross-game state\npropagation. It also introduces a novel Versus Learning method to better\nenhance the adversarial nature of momentum between the two athletes at a macro\nlevel, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for\ncapturing and integrating intra-player and inter-player dynamic momentum at a\nmicro level. Additionally, we construct a million-level tennis cross-tournament\ndataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate\nthe multi-granularity modeling capability of HydraNet for the MS metric on this\ndataset. Extensive experimental evaluations demonstrate that the MS metric\nconstructed by the HydraNet framework provides actionable insights into how\nmomentum impacts outcomes at different granularities, establishing a new\nfoundation for momentum modeling and sports analysis. To the best of our\nknowledge. The source code and datasets are available at\nhttps://github.com/ReyJerry/HydraNet.", "AI": {"tldr": "In tennis tournaments, momentum significantly influences match outcomes. This study introduces Momentum Score (MS) to quantify player's momentum and HydraNet framework to model MS using 32 dimensions of athlete performance. HydraNet includes Hydra module for capturing explicit and implicit momentum, Versus Learning method and Collaborative-Adversarial Attention Mechanism (CAAM). A large-scale dataset from Wimbledon and US Open is constructed and used for validation.", "motivation": "Momentum in tennis tournaments is crucial but underexplored in terms of effective modeling and multi-granularity analysis.", "method": "Define a novel Momentum Score (MS) metric and design HydraNet framework integrating 32 heterogeneous dimensions of athletes' performance. HydraNet consists of Hydra module based on state-space duality framework, Versus Learning method and Collaborative-Adversarial Attention Mechanism (CAAM).", "result": "Extensive experimental evaluations demonstrate that the MS metric provides actionable insights into how momentum impacts outcomes at different granularities.", "conclusion": "HydraNet framework establishes a new foundation for momentum modeling and sports analysis."}}
{"id": "2505.21559", "pdf": "https://arxiv.org/pdf/2505.21559", "abs": "https://arxiv.org/abs/2505.21559", "authors": ["Julien Soul\u00e9", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Th\u00e9ron"], "title": "Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "In cloud-native systems, Kubernetes clusters with interdependent services\noften face challenges to their operational resilience due to poor workload\nmanagement issues such as resource blocking, bottlenecks, or continuous pod\ncrashes. These vulnerabilities are further amplified in adversarial scenarios,\nsuch as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal\nPod Autoscaling (HPA) approaches struggle to address such dynamic conditions,\nwhile reinforcement learning-based methods, though more adaptable, typically\noptimize single goals like latency or resource usage, neglecting broader\nfailure scenarios. We propose decomposing the overarching goal of maintaining\noperational resilience into failure-specific sub-goals delegated to\ncollaborative agents, collectively forming an HPA Multi-Agent System (MAS). We\nintroduce an automated, four-phase online framework for HPA MAS design: 1)\nmodeling a digital twin built from cluster traces; 2) training agents in\nsimulation using roles and missions tailored to failure contexts; 3) analyzing\nagent behaviors for explainability; and 4) transferring learned policies to the\nreal cluster. Experimental results demonstrate that the generated HPA MASs\noutperform three state-of-the-art HPA systems in sustaining operational\nresilience under various adversarial conditions in a proposed complex cluster.", "AI": {"tldr": "In cloud-native systems, Kubernetes clusters face operational resilience challenges. Conventional HPAs and RL-based methods have limitations. This paper proposes an HPA Multi-Agent System (MAS) with a four-phase framework to enhance resilience.", "motivation": "Kubernetes clusters in cloud-native systems often encounter operational resilience issues due to poor workload management, which are exacerbated in adversarial conditions like DDoS attacks. Existing solutions, such as conventional HPAs and reinforcement learning-based methods, fail to address these challenges comprehensively.", "method": "The method involves decomposing the goal of maintaining operational resilience into failure-specific sub-goals managed by collaborative agents forming an HPA MAS. An automated, four-phase online framework is introduced: 1) modeling a digital twin from cluster traces; 2) training agents in simulation for specific failure contexts; 3) analyzing agent behaviors for explainability; and 4) transferring learned policies to the real cluster.", "result": "Experimental results indicate that the proposed HPA MASs surpass three state-of-the-art HPA systems in maintaining operational resilience under various adversarial conditions in a complex cluster.", "conclusion": "The proposed HPA MAS with its four-phase framework effectively enhances operational resilience in Kubernetes clusters under adversarial conditions, outperforming existing state-of-the-art systems."}}
{"id": "2505.21893", "pdf": "https://arxiv.org/pdf/2505.21893", "abs": "https://arxiv.org/abs/2505.21893", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Junyan Wang", "Zhijian Zhou", "Hao Li"], "title": "SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Preference learning has become a central technique for aligning generative\nmodels with human expectations. Recently, it has been extended to diffusion\nmodels through methods like Direct Preference Optimization (DPO). However,\nexisting approaches such as Diffusion-DPO suffer from two key challenges:\ntimestep-dependent instability, caused by a mismatch between the reverse and\nforward diffusion processes and by high gradient variance in early noisy\ntimesteps, and off-policy bias arising from the mismatch between optimization\nand data collection policies. We begin by analyzing the reverse diffusion\ntrajectory and observe that instability primarily occurs at early timesteps\nwith low importance weights. To address these issues, we first propose\nDPO-C\\&M, a practical strategy that improves stability by clipping and masking\nuninformative timesteps while partially mitigating off-policy bias. Building on\nthis, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a\nprincipled framework that incorporates importance sampling into the objective\nto fully correct for off-policy bias and emphasize informative updates during\nthe diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and\nWan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,\nwith SDPO achieving superior VBench scores, human preference alignment, and\ntraining robustness. These results highlight the importance of timestep-aware,\ndistribution-corrected optimization in diffusion-based preference learning.", "AI": {"tldr": "The paper addresses instability and off-policy bias issues in diffusion models preference learning through proposing DPO-C&M and SDPO methods, which outperform standard Diffusion-DPO in experiments.", "motivation": "Preference learning is crucial for aligning generative models with human expectations. However, existing approaches like Diffusion-DPO suffer from timestep-dependent instability and off-policy bias.", "method": "The authors propose DPO-C&M to improve stability by clipping and masking uninformative timesteps and partially mitigating off-policy bias. They also introduce SDPO, a framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process.", "result": "Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B show that both DPO-C&M and SDPO outperform standard Diffusion-DPO. Specifically, SDPO achieves superior VBench scores, human preference alignment, and training robustness.", "conclusion": "The findings underline the significance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning."}}
{"id": "2505.21562", "pdf": "https://arxiv.org/pdf/2505.21562", "abs": "https://arxiv.org/abs/2505.21562", "authors": ["Jennifer Turliuk", "Alejandro Sevilla", "Daniela Gorza", "Tod Hynes"], "title": "Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "This case study examines the ClimaTech Great Global Innovation Challenge's\napproach to selecting climate tech startups by integrating human and AI\nevaluations. The competition aimed to identify top startups and enhance the\naccuracy and efficiency of the selection process through a hybrid model.\nResearch shows data-driven approaches help VC firms reduce bias and improve\ndecision-making. Machine learning models have outperformed human investors in\ndeal screening, helping identify high-potential startups. Incorporating AI\naimed to ensure more equitable and objective evaluations.\n  The methodology included three phases: initial AI review, semi-finals judged\nby humans, and finals using a hybrid weighting. In phase one, 57 applications\nwere scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top\n36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated\nstartups on team quality, market potential, and technological innovation. Each\nscore - human or AI - was weighted equally, resulting in 75 percent human and\n25 percent AI influence. In the finals, with five human judges, weighting\nshifted to 83.3 percent human and 16.7 percent AI. There was a moderate\npositive correlation between AI and human scores - Spearman's = 0.47 -\nindicating general alignment with key differences. Notably, the final four\nstartups, selected mainly by humans, were among those rated highest by the AI.\nThis highlights the complementary nature of AI and human judgment. The study\nshows that hybrid models can streamline and improve startup assessments. The\nClimaTech approach offers a strong framework for future competitions by\ncombining human expertise with AI capabilities.", "AI": {"tldr": "This case study explores the ClimaTech Great Global Innovation Challenge's hybrid human-AI model for selecting climate tech startups. The process included three phases with varying human-AI score weightings, showing moderate alignment between AI and human assessments and demonstrating the value of combining both approaches.", "motivation": "To evaluate how integrating AI with human evaluations can enhance the accuracy and efficiency of selecting top climate tech startups in a global innovation challenge.", "method": "The selection process was divided into three phases: initial AI review (25% AI influence), semi-finals judged by humans (75% human, 25% AI influence), and finals using hybrid weighting (83.3% human, 16.7% AI influence). AI scores were based on StackAI and OpenAI's GPT-4o, while human judges evaluated team quality, market potential, and technological innovation.", "result": "There was a moderate positive correlation (Spearman's = 0.47) between AI and human scores, indicating general alignment with key differences. The final four startups selected mainly by humans were among those rated highest by the AI.", "conclusion": "Hybrid models combining human expertise with AI capabilities can streamline and improve startup assessments, offering a strong framework for future competitions."}}
{"id": "2505.21895", "pdf": "https://arxiv.org/pdf/2505.21895", "abs": "https://arxiv.org/abs/2505.21895", "authors": ["Cameron Gordon", "Yiping Ji", "Hemanth Saratchandran", "Paul Albert", "Simon Lucey"], "title": "Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 9 figures", "summary": "Low-Rank Adaptation (LoRA) has become a standard approach for\nparameter-efficient fine-tuning, offering substantial reductions in trainable\nparameters by modeling updates as the product of two low-rank matrices. While\neffective, the low-rank constraint inherently limits representational capacity,\noften resulting in reduced performance compared to full-rank fine-tuning.\nRecent work by Ji et al. (2025) has addressed this limitation by applying a\nfixed-frequency sinusoidal transformation to low-rank adapters, increasing\ntheir stable rank without introducing additional parameters. This raises a\ncrucial question: can the same sine-activated technique be successfully applied\nwithin the context of Post-Training Quantization to retain benefits even after\nmodel compression? In this paper, we investigate this question by extending the\nsinusoidal transformation framework to quantized LoRA adapters. We develop a\ntheoretical analysis showing that the stable rank of a quantized adapter is\ntightly linked to that of its full-precision counterpart, motivating the use of\nsuch rank-enhancing functions even under quantization. Our results demonstrate\nthat the expressivity gains from a sinusoidal non-linearity persist after\nquantization, yielding highly compressed adapters with negligible loss in\nperformance. We validate our approach across a range of fine-tuning tasks for\nlanguage, vision and text-to-image generation achieving significant memory\nsavings while maintaining competitive accuracy.", "AI": {"tldr": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method, but it has limitations due to its low-rank constraint. Recent work has used a sinusoidal transformation to increase stable rank without additional parameters. This paper explores applying this technique in Post-Training Quantization. The authors develop a theoretical analysis and demonstrate that the expressivity gains persist after quantization, resulting in highly compressed adapters with little performance loss. They validate their approach across various tasks achieving significant memory savings while maintaining competitive accuracy.", "motivation": "The motivation of this paper is to explore whether the sine-activated technique that increases the stable rank of low-rank adapters can be successfully applied within the context of Post-Training Quantization, allowing benefits to be retained even after model compression.", "method": "The authors extend the sinusoidal transformation framework to quantized LoRA adapters and conduct a theoretical analysis showing the link between the stable rank of a quantized adapter and its full-precision counterpart. This motivates the use of rank-enhancing functions under quantization.", "result": "The results show that the expressivity gains from a sinusoidal non-linearity persist after quantization, leading to highly compressed adapters with negligible performance loss.", "conclusion": "This paper concludes that applying the sinusoidal transformation in Post-Training Quantization is effective for creating highly compressed adapters with minimal performance degradation."}}
{"id": "2505.21563", "pdf": "https://arxiv.org/pdf/2505.21563", "abs": "https://arxiv.org/abs/2505.21563", "authors": ["Kai Yang", "Hui Ma", "Shaoyu Dou"], "title": "Fog Intelligence for Network Anomaly Detection", "categories": ["cs.NI", "cs.AI"], "comment": "published in IEEE Network", "summary": "Anomalies are common in network system monitoring. When manifested as network\nthreats to be mitigated, service outages to be prevented, and security risks to\nbe ameliorated, detecting such anomalous network behaviors becomes of great\nimportance. However, the growing scale and complexity of the mobile\ncommunication networks, as well as the ever-increasing amount and\ndimensionality of the network surveillance data, make it extremely difficult to\nmonitor a mobile network and discover abnormal network behaviors. Recent\nadvances in machine learning allow for obtaining near-optimal solutions to\ncomplicated decision-making problems with many sources of uncertainty that\ncannot be accurately characterized by traditional mathematical models. However,\nmost machine learning algorithms are centralized, which renders them\ninapplicable to a large-scale distributed wireless networks with tens of\nmillions of mobile devices. In this article, we present fog intelligence, a\ndistributed machine learning architecture that enables intelligent wireless\nnetwork management. It preserves the advantage of both edge processing and\ncentralized cloud computing. In addition, the proposed architecture is\nscalable, privacy-preserving, and well suited for intelligent management of a\ndistributed wireless network.", "AI": {"tldr": "Anomalies in network system monitoring are crucial to detect due to potential network threats, service outages, and security risks. However, the scale and complexity of mobile communication networks make it challenging. Traditional mathematical models fall short, as do most centralized machine learning algorithms. This article introduces fog intelligence, a distributed machine learning architecture for intelligent wireless network management that combines edge processing and cloud computing advantages while being scalable and privacy-preserving.", "motivation": "Detecting anomalies in network system monitoring is vital due to potential network threats, service outages, and security risks. The increasing scale and complexity of mobile communication networks, along with the large amount of surveillance data, make monitoring and discovering abnormal behaviors difficult.", "method": "The authors propose fog intelligence, a distributed machine learning architecture for intelligent wireless network management. It integrates edge processing and centralized cloud computing, making it scalable and privacy-preserving.", "result": "Fog intelligence offers a solution to the challenges posed by large-scale distributed wireless networks. It enables intelligent network management while preserving privacy and scalability.", "conclusion": "Fog intelligence represents a promising approach to managing complex, large-scale wireless networks by combining the strengths of edge and cloud computing, ensuring scalability and privacy."}}
{"id": "2505.21908", "pdf": "https://arxiv.org/pdf/2505.21908", "abs": "https://arxiv.org/abs/2505.21908", "authors": ["Hanyin Wang", "Zhenbang Wu", "Gururaj Kolar", "Hariprasad Korsapati", "Brian Bartlett", "Bryan Hull", "Jimeng Sun"], "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement\nand operations but require labor-intensive assignment. Large Language Models\n(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of\nthe task: pretraining corpora rarely contain private clinical or billing data.\nWe introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)\nfor automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained\nwith Group Relative Policy Optimization (GRPO) using rule-based rewards,\nDRG-Sapphire introduces a series of RL enhancements to address domain-specific\nchallenges not seen in previous mathematical tasks. Our model achieves\nstate-of-the-art accuracy on the MIMIC-IV benchmark and generates\nphysician-validated reasoning for DRG assignments, significantly enhancing\nexplainability. Our study further sheds light on broader challenges of applying\nRL to knowledge-intensive, OOD tasks. We observe that RL performance scales\napproximately linearly with the logarithm of the number of supervised\nfine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally\nconstrained by the domain knowledge encoded in the base model. For OOD tasks\nlike DRG coding, strong RL performance requires sufficient knowledge infusion\nprior to RL. Consequently, scaling SFT may be more effective and\ncomputationally efficient than scaling RL alone for such tasks.", "AI": {"tldr": "DRG-Sapphire is a model built on Qwen2.5-7B that uses reinforcement learning to automate DRG coding from clinical notes, achieving state-of-the-art accuracy and enhancing explainability.", "motivation": "Diagnosis-Related Group (DRG) codes are important for hospital reimbursement but assigning them is labor-intensive. Existing large language models struggle with DRG coding due to the out-of-distribution nature of the task.", "method": "The model, DRG-Sapphire, uses large-scale reinforcement learning with rule-based rewards and a series of enhancements to address domain-specific challenges. It is built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO).", "result": "DRG-Sapphire achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments.", "conclusion": "The study highlights broader challenges in applying reinforcement learning to knowledge-intensive, out-of-distribution tasks and suggests that scaling supervised fine-tuning may be more effective and computationally efficient than scaling reinforcement learning alone for such tasks."}}
{"id": "2505.21565", "pdf": "https://arxiv.org/pdf/2505.21565", "abs": "https://arxiv.org/abs/2505.21565", "authors": ["Haicheng Liao", "Zhenning Li", "Guohui Zhang", "Keqiang Li", "Chengzhong Xu"], "title": "Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Predicting the trajectories of vehicles is crucial for the development of\nautonomous driving (AD) systems, particularly in complex and dynamic traffic\nenvironments. In this study, we introduce HiT (Human-like Trajectory\nPrediction), a novel model designed to enhance trajectory prediction by\nincorporating behavior-aware modules and dynamic centrality measures. Unlike\ntraditional methods that primarily rely on static graph structures, HiT\nleverages a dynamic framework that accounts for both direct and indirect\ninteractions among traffic participants. This allows the model to capture the\nsubtle yet significant influences of surrounding vehicles, enabling more\naccurate and human-like predictions. To evaluate HiT's performance, we\nconducted extensive experiments using diverse and challenging real-world\ndatasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results\ndemonstrate that HiT consistently outperforms other top models across multiple\nmetrics, particularly excelling in scenarios involving aggressive driving\nbehaviors. This research presents a significant step forward in trajectory\nprediction, offering a more reliable and interpretable approach for enhancing\nthe safety and efficiency of fully autonomous driving systems.", "AI": {"tldr": "This paper presents HiT, a new model for predicting vehicle trajectories in complex traffic scenarios. It outperforms existing models, especially with aggressive driving behaviors.", "motivation": "The need for more accurate and human-like trajectory prediction in autonomous driving systems to handle complex and dynamic traffic environments.", "method": "HiT incorporates behavior-aware modules and dynamic centrality measures within a dynamic framework that considers both direct and indirect interactions among traffic participants.", "result": "HiT outperforms other top models across multiple metrics in extensive experiments using diverse real-world datasets, particularly excelling in scenarios involving aggressive driving behaviors.", "conclusion": "HiT represents a significant advancement in trajectory prediction, providing a more reliable and interpretable approach for enhancing the safety and efficiency of fully autonomous driving systems."}}
{"id": "2505.21910", "pdf": "https://arxiv.org/pdf/2505.21910", "abs": "https://arxiv.org/abs/2505.21910", "authors": ["Xianbiao Qi", "Yelin He", "Jiaquan Ye", "Chun-Guang Li", "Bojia Zi", "Xili Dai", "Qin Zou", "Rong Xiao"], "title": "Taming Transformer Without Using Learning Rate Warmup", "categories": ["cs.LG", "cs.CV"], "comment": "This paper is published as a conference paper at ICLR 2025", "summary": "Scaling Transformer to a large scale without using some technical tricks such\nas learning rate warump and using an obviously lower learning rate is an\nextremely challenging task, and is increasingly gaining more attention. In this\npaper, we provide a theoretical analysis for the process of training\nTransformer and reveal the rationale behind the model crash phenomenon in the\ntraining process, termed \\textit{spectral energy concentration} of\n${\\bW_q}^{\\top} \\bW_k$, which is the reason for a malignant entropy collapse,\nwhere ${\\bW_q}$ and $\\bW_k$ are the projection matrices for the query and the\nkey in Transformer, respectively. To remedy this problem, motivated by\n\\textit{Weyl's Inequality}, we present a novel optimization strategy, \\ie,\nmaking the weight updating in successive steps smooth -- if the ratio\n$\\frac{\\sigma_{1}(\\nabla \\bW_t)}{\\sigma_{1}(\\bW_{t-1})}$ is larger than a\nthreshold, we will automatically bound the learning rate to a weighted multiple\nof $\\frac{\\sigma_{1}(\\bW_{t-1})}{\\sigma_{1}(\\nabla \\bW_t)}$, where $\\nabla\n\\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can\nprevent spectral energy concentration to only a few directions, and thus can\navoid malignant entropy collapse which will trigger the model crash. We conduct\nextensive experiments using ViT, Swin-Transformer and GPT, showing that our\noptimization strategy can effectively and stably train these Transformers\nwithout using learning rate warmup.", "AI": {"tldr": "\u5728\u5927\u89c4\u6a21Transformer\u8bad\u7ec3\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u7b56\u7565\u4ee5\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u5e73\u6ed1\u6743\u91cd\u66f4\u65b0\u907f\u514d\u6076\u6027\u71b5\u574d\u7f29\uff0c\u4ece\u800c\u65e0\u9700\u5b66\u4e60\u7387\u9884\u70ed\u5373\u53ef\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u5927\u89c4\u6a21Transformer\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u6a21\u578b\u5d29\u6e83\u73b0\u8c61\uff08\u5373\u6076\u6027\u71b5\u574d\u7f29\uff09\u662f\u7531\u4e8e${\\bW_q}^{\\top} \\bW_k$\u7684\u8c31\u80fd\u91cf\u96c6\u4e2d\u5bfc\u81f4\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u7a33\u5b9a\u8bad\u7ec3\u3002", "method": "\u57fa\u4e8eWeyl's Inequality\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u7b56\u7565\uff1a\u5982\u679c\u6bd4\u4f8b$\\frac{\\sigma_{1}(\\nabla \\bW_t)}{\\sigma_{1}(\\bW_{t-1})}$\u8d85\u8fc7\u9608\u503c\uff0c\u5219\u5c06\u5b66\u4e60\u7387\u81ea\u52a8\u9650\u5236\u4e3a$\\frac{\\sigma_{1}(\\bW_{t-1})}{\\sigma_{1}(\\nabla \\bW_t)}$\u7684\u52a0\u6743\u500d\u6570\uff0c\u4ece\u800c\u4f7f\u8fde\u7eed\u6b65\u9aa4\u4e2d\u7684\u6743\u91cd\u66f4\u65b0\u53d8\u5f97\u5e73\u6ed1\uff0c\u9632\u6b62\u8c31\u80fd\u91cf\u96c6\u4e2d\u5230\u5c11\u6570\u65b9\u5411\u3002", "result": "\u901a\u8fc7\u5728ViT\u3001Swin-Transformer\u548cGPT\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u4f18\u5316\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u4e14\u7a33\u5b9a\u5730\u8bad\u7ec3\u8fd9\u4e9bTransformer\u6a21\u578b\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u5b66\u4e60\u7387\u9884\u70ed\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5316\u7b56\u7565\u89e3\u51b3\u4e86Transformer\u8bad\u7ec3\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6280\u672f\u624b\u6bb5\u5982\u5b66\u4e60\u7387\u9884\u70ed\u7684\u5927\u89c4\u6a21\u7a33\u5b9a\u8bad\u7ec3\u3002"}}
{"id": "2505.21918", "pdf": "https://arxiv.org/pdf/2505.21918", "abs": "https://arxiv.org/abs/2505.21918", "authors": ["Haruki Kai", "Tsuyoshi Okita"], "title": "Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing", "categories": ["cs.LG", "cs.AI"], "comment": "25 pages, 4 figures", "summary": "We developed a deep learning algorithm for human activity recognition using\nsensor signals as input. In this study, we built a pretrained language model\nbased on the Transformer architecture, which is widely used in natural language\nprocessing. By leveraging this pretrained model, we aimed to improve\nperformance on the downstream task of human activity recognition. While this\ntask can be addressed using a vanilla Transformer, we propose an enhanced\nn-dimensional numerical processing Transformer that incorporates three key\nfeatures: embedding n-dimensional numerical data through a linear layer,\nbinning-based pre-processing, and a linear transformation in the output layer.\nWe evaluated the effectiveness of our proposed model across five different\ndatasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%\nimprovements in accuracy.", "AI": {"tldr": "The paper presents a deep learning algorithm for human activity recognition using an enhanced n-dimensional numerical processing Transformer, showing 10%-15% accuracy improvements over vanilla Transformer across five datasets.", "motivation": "To improve the performance of human activity recognition by utilizing a pretrained language model and developing an enhanced n-dimensional numerical processing Transformer.", "method": "Building a pretrained language model based on Transformer architecture and proposing an enhanced n-dimensional numerical processing Transformer with features like embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and linear transformation in the output layer.", "result": "The proposed model showed 10%-15% improvements in accuracy compared to the vanilla Transformer when evaluated across five different datasets.", "conclusion": "The enhanced n-dimensional numerical processing Transformer effectively improves the accuracy of human activity recognition tasks."}}
{"id": "2505.21923", "pdf": "https://arxiv.org/pdf/2505.21923", "abs": "https://arxiv.org/abs/2505.21923", "authors": ["Asal Mehradfar", "Xuzhe Zhao", "Yilun Huang", "Emir Ceyani", "Yankai Yang", "Shihao Han", "Hamidreza Aghasi", "Salman Avestimehr"], "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CE"], "comment": null, "summary": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation.", "AI": {"tldr": "\u8bbe\u8ba1\u6ee1\u8db3\u6027\u80fd\u89c4\u8303\u7684\u6a21\u62df\u7535\u8def\u662f\u4e00\u4e2a\u590d\u6742\u7684\u8fc7\u7a0b\uff0cFALCON\u662f\u4e00\u79cd\u5168\u65b0\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u9009\u62e9\u548c\u5e03\u5c40\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u5168\u81ea\u52a8\u3001\u89c4\u8303\u9a71\u52a8\u7684\u6a21\u62df\u7535\u8def\u5408\u6210\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u8fc7\u7a0b\u590d\u6742\uff0c\u6db5\u76d6\u62d3\u6251\u9009\u62e9\u3001\u53c2\u6570\u63a8\u65ad\u548c\u5e03\u5c40\u53ef\u884c\u6027\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "FALCON\u9996\u5148\u4f7f\u7528\u6027\u80fd\u9a71\u52a8\u5206\u7c7b\u5668\u9009\u62e9\u5408\u9002\u7684\u7535\u8def\u62d3\u6251\u7ed3\u6784\uff0c\u7136\u540e\u5229\u7528\u81ea\u5b9a\u4e49\u7684\u8fb9\u4e2d\u5fc3\u56fe\u795e\u7ecf\u7f51\u7edc\u5c06\u7535\u8def\u62d3\u6251\u548c\u53c2\u6570\u6620\u5c04\u5230\u6027\u80fd\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5e03\u5c40\u6210\u672c\u8fdb\u884c\u68af\u5ea6\u57fa\u7840\u53c2\u6570\u63a8\u65ad\u3002", "result": "FALCON\u5728\u62d3\u6251\u63a8\u65ad\u4e2d\u5c55\u793a\u4e86>99%\u7684\u51c6\u786e\u6027\uff0c\u6027\u80fd\u9884\u6d4b\u76f8\u5bf9\u8bef\u5dee<10%\uff0c\u5e76\u4e14\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u8bbe\u8ba1\u65f6\u95f4\u4e0d\u52301\u79d2\u3002", "conclusion": "FALCON\u4e3a\u7aef\u5230\u7aef\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2505.21570", "pdf": "https://arxiv.org/pdf/2505.21570", "abs": "https://arxiv.org/abs/2505.21570", "authors": ["Dalit Ken-Dror Feldman", "Daniel Benoliel"], "title": "Beyond Explainability: The Case for AI Validation", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Artificial Knowledge (AK) systems are transforming decision-making across\ncritical domains such as healthcare, finance, and criminal justice. However,\ntheir growing opacity presents governance challenges that current regulatory\napproaches, focused predominantly on explainability, fail to address\nadequately. This article argues for a shift toward validation as a central\nregulatory pillar. Validation, ensuring the reliability, consistency, and\nrobustness of AI outputs, offers a more practical, scalable, and risk-sensitive\nalternative to explainability, particularly in high-stakes contexts where\ninterpretability may be technically or economically unfeasible. We introduce a\ntypology based on two axes, validity and explainability, classifying AK systems\ninto four categories and exposing the trade-offs between interpretability and\noutput reliability. Drawing on comparative analysis of regulatory approaches in\nthe EU, US, UK, and China, we show how validation can enhance societal trust,\nfairness, and safety even where explainability is limited. We propose a\nforward-looking policy framework centered on pre- and post-deployment\nvalidation, third-party auditing, harmonized standards, and liability\nincentives. This framework balances innovation with accountability and provides\na governance roadmap for responsibly integrating opaque, high-performing AK\nsystems into society.", "AI": {"tldr": "Artificial Knowledge (AK) systems are increasingly used in critical domains but their opacity presents governance challenges. This paper argues for a shift from explainability to validation as a central regulatory pillar, proposing a forward-looking policy framework centered on pre- and post-deployment validation, third-party auditing, harmonized standards, and liability incentives.", "motivation": "Current regulatory approaches focused predominantly on explainability fail to address adequately the growing opacity of Artificial Knowledge systems.", "method": "The authors introduce a typology based on two axes - validity and explainability - classifying AK systems into four categories. They conduct a comparative analysis of regulatory approaches in the EU, US, UK, and China.", "result": "Validation can enhance societal trust, fairness, and safety even where explainability is limited. A forward-looking policy framework centered on validation processes is proposed.", "conclusion": "A governance roadmap is provided for responsibly integrating opaque, high-performing AK systems into society, balancing innovation with accountability."}}
{"id": "2505.21930", "pdf": "https://arxiv.org/pdf/2505.21930", "abs": "https://arxiv.org/abs/2505.21930", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages. To appear in ACL'25", "summary": "This paper develops an ensemble method for fine-tuning a language model to\nmultiple datasets. Existing methods, such as quantized LoRA (QLoRA), are\nefficient when adapting to a single dataset. When training on multiple datasets\nof different tasks, a common setup in practice, it remains unclear how to\ndesign an efficient adaptation for fine-tuning language models. We propose to\nuse an ensemble of multiple smaller adapters instead of a single adapter per\ntask. We design an efficient algorithm that partitions $n$ datasets into $m$\ngroups, where $m$ is typically much smaller than $n$ in practice, and train one\nadapter for each group before taking a weighted combination to form the\nensemble. The algorithm leverages a first-order approximation property of\nlow-rank adaptation to quickly obtain the fine-tuning performances of dataset\ncombinations since methods like LoRA stay close to the base model. Hence, we\nuse the gradients of the base model to estimate its behavior during\nfine-tuning. Empirically, this approximation holds with less than $1\\%$ error\non models with up to $34$ billion parameters, leading to an estimation of true\nfine-tuning performances under $5\\%$ error while speeding up computation\ncompared to base fine-tuning by $105$ times. When applied to fine-tune Llama\nand GPT models on ten text classification tasks, our approach provides up to\n$10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On\na Llama model with $34$ billion parameters, an ensemble of QLoRA increases test\naccuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5230\u591a\u4e2a\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5c06\u6570\u636e\u96c6\u5206\u7ec4\u5e76\u8bad\u7ec3\u6bcf\u4e2a\u7ec4\u7684\u9002\u914d\u5668\uff0c\u518d\u8fdb\u884c\u52a0\u6743\u7ec4\u5408\u5f62\u6210\u96c6\u5408\uff0c\u5229\u7528\u4f4e\u79e9\u9002\u914d\u7684\u7b2c\u4e00\u9636\u8fd1\u4f3c\u6027\u8d28\u6765\u5feb\u901f\u83b7\u5f97\u6570\u636e\u96c6\u7ec4\u5408\u7684\u5fae\u8c03\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728Llama\u548cGPT\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5982\u91cf\u5316LoRA\uff08QLoRA\uff09\u5728\u9002\u5e94\u5355\u4e00\u6570\u636e\u96c6\u65f6\u6548\u7387\u5f88\u9ad8\uff0c\u4f46\u5728\u591a\u4e2a\u4e0d\u540c\u4efb\u52a1\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u5982\u4f55\u8bbe\u8ba1\u9ad8\u6548\u7684\u9002\u5e94\u65b9\u6cd5\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u591a\u4e2a\u66f4\u5c0f\u7684\u9002\u914d\u5668\u7684\u96c6\u5408\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2a\u4efb\u52a1\u4e00\u4e2a\u9002\u914d\u5668\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7b97\u6cd5\uff0c\u5c06n\u4e2a\u6570\u636e\u96c6\u5206\u6210m\u7ec4\uff08m\u901a\u5e38\u8fdc\u5c0f\u4e8en\uff09\uff0c\u4e3a\u6bcf\u7ec4\u8bad\u7ec3\u4e00\u4e2a\u9002\u914d\u5668\uff0c\u7136\u540e\u8fdb\u884c\u52a0\u6743\u7ec4\u5408\u5f62\u6210\u96c6\u5408\u3002\u5229\u7528\u4f4e\u79e9\u9002\u914d\u7684\u7b2c\u4e00\u9636\u8fd1\u4f3c\u6027\u8d28\u6765\u5feb\u901f\u83b7\u5f97\u6570\u636e\u96c6\u7ec4\u5408\u7684\u5fae\u8c03\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6700\u591a34\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u8bef\u5dee\u5c0f\u4e8e1%\uff0c\u4f30\u8ba1\u771f\u5b9e\u5fae\u8c03\u6027\u80fd\u8bef\u5dee\u57285%\u4ee5\u5185\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u57fa\u7840\u5fae\u8c03\u5feb105\u500d\u3002\u5728\u5341\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4QLoRA\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\u9ad8\u51fa10%\uff0c\u4ec5\u589e\u52a09%\u7684FLOPs\u3002\u5bf9\u4e8e34\u4ebf\u53c2\u6570\u7684Llama\u6a21\u578b\uff0cQLoRA\u96c6\u5408\u6bd4\u5355\u72ec\u7684QLoRA\u6d4b\u8bd5\u51c6\u786e\u7387\u9ad8\u51fa3%\uff0c\u4ec5\u589e\u52a08%\u7684FLOPs\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u5408\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5fae\u8c03\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2505.21572", "pdf": "https://arxiv.org/pdf/2505.21572", "abs": "https://arxiv.org/abs/2505.21572", "authors": ["Sungwon Kim", "Namkyeong Lee", "Yunyoung Doh", "Seungmin Shin", "Guimok Cho", "Seung-Won Jeon", "Sangkook Kim", "Chanyoung Park"], "title": "Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Mesh-based 3D static analysis methods have recently emerged as efficient\nalternatives to traditional computational numerical solvers, significantly\nreducing computational costs and runtime for various physics-based analyses.\nHowever, these methods primarily focus on surface topology and geometry, often\noverlooking the inherent thickness of real-world 3D objects, which exhibits\nhigh correlations and similar behavior between opposing surfaces. This\nlimitation arises from the disconnected nature of these surfaces and the\nabsence of internal edge connections within the mesh. In this work, we propose\na novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network\n(T-EMNN), that effectively integrates the thickness of 3D objects while\nmaintaining the computational efficiency of surface meshes. Additionally, we\nintroduce data-driven coordinates that encode spatial information while\npreserving E(3)-equivariance or invariance properties, ensuring consistent and\nrobust analysis. Evaluations on a real-world industrial dataset demonstrate the\nsuperior performance of T-EMNN in accurately predicting node-level 3D\ndeformations, effectively capturing thickness effects while maintaining\ncomputational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6T-EMNN\uff0c\u80fd\u591f\u6709\u6548\u6574\u54083D\u7269\u4f53\u539a\u5ea6\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u8868\u9762\u7f51\u683c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7f51\u683c\u76843D\u9759\u6001\u5206\u6790\u65b9\u6cd5\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u8868\u9762\u62d3\u6251\u548c\u51e0\u4f55\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e3D\u7269\u4f53\u56fa\u6709\u7684\u539a\u5ea6\u4fe1\u606f\u53ca\u5176\u76f8\u5173\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86Thickness-aware E(3)-Equivariant 3D Mesh Neural Network (T-EMNN)\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u5750\u6807\u7f16\u7801\u7a7a\u95f4\u4fe1\u606f\uff0c\u4fdd\u7559E(3)-\u7b49\u53d8\u6216\u4e0d\u53d8\u6027\u5c5e\u6027\uff0c\u4ece\u800c\u6574\u5408\u7269\u4f53\u539a\u5ea6\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cT-EMNN\u5728\u51c6\u786e\u9884\u6d4b\u8282\u70b9\u7ea73D\u53d8\u5f62\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u6709\u6548\u6355\u6349\u539a\u5ea6\u6548\u5e94\u3002", "conclusion": "T-EMNN\u6846\u67b6\u6210\u529f\u5730\u5c06\u7269\u4f53\u539a\u5ea6\u7eb3\u5165\u5206\u6790\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8868\u9762\u7f51\u683c\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a3D\u9759\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.21942", "pdf": "https://arxiv.org/pdf/2505.21942", "abs": "https://arxiv.org/abs/2505.21942", "authors": ["Prashant Bhat", "Laurens Niesten", "Elahe Arani", "Bahram Zonooz"], "title": "Continual Learning Beyond Experience Rehearsal and Full Model Surrogates", "categories": ["cs.LG", "stat.ML"], "comment": "23 pages, 9 figures", "summary": "Continual learning (CL) has remained a significant challenge for deep neural\nnetworks as learning new tasks erases previously acquired knowledge, either\npartially or completely. Existing solutions often rely on experience rehearsal\nor full model surrogates to mitigate CF. While effective, these approaches\nintroduce substantial memory and computational overhead, limiting their\nscalability and applicability in real-world scenarios. To address this, we\npropose SPARC, a scalable CL approach that eliminates the need for experience\nrehearsal and full-model surrogates. By effectively combining task-specific\nworking memories and task-agnostic semantic memory for cross-task knowledge\nconsolidation, SPARC results in a remarkable parameter efficiency, using only\n6% of the parameters required by full-model surrogates. Despite its lightweight\ndesign, SPARC achieves superior performance on Seq-TinyImageNet and matches\nrehearsal-based methods on various CL benchmarks. Additionally, weight\nre-normalization in the classification layer mitigates task-specific biases,\nestablishing SPARC as a practical and scalable solution for CL under stringent\nefficiency constraints.", "AI": {"tldr": "SPARC is a new Continual Learning approach that does not need experience rehearsal or full-model surrogates, achieving high parameter efficiency and good performance on benchmarks.", "motivation": "Continual learning in deep neural networks faces the issue of catastrophic forgetting. Current solutions are either memory-heavy or computationally expensive.", "method": "SPARC combines task-specific working memories with task-agnostic semantic memory for knowledge consolidation, and uses weight re-normalization in the classification layer to reduce task-specific biases.", "result": "SPARC uses only 6% of parameters compared to full-model surrogates and performs better on Seq-TinyImageNet while matching rehearsal-based methods on other benchmarks.", "conclusion": "SPARC offers a practical and scalable solution for continual learning under strict efficiency requirements."}}
{"id": "2505.21944", "pdf": "https://arxiv.org/pdf/2505.21944", "abs": "https://arxiv.org/abs/2505.21944", "authors": ["Linli Zhou", "Bokun Wang", "My T. Thai", "Tianbao Yang"], "title": "Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization", "categories": ["cs.LG"], "comment": null, "summary": "Two-way partial AUC (TPAUC) is a critical performance metric for binary\nclassification with imbalanced data, as it focuses on specific ranges of the\ntrue positive rate (TPR) and false positive rate (FPR). However, stochastic\nalgorithms for TPAUC optimization remain under-explored, with existing methods\neither limited to approximated TPAUC loss functions or burdened by sub-optimal\ncomplexities. To overcome these limitations, we introduce two innovative\nstochastic primal-dual double block-coordinate algorithms for TPAUC\nmaximization. These algorithms utilize stochastic block-coordinate updates for\nboth the primal and dual variables, catering to both convex and non-convex\nsettings. We provide theoretical convergence rate analyses, demonstrating\nsignificant improvements over prior approaches. Our experimental results, based\non multiple benchmark datasets, validate the superior performance of our\nalgorithms, showcasing faster convergence and better generalization. This work\nadvances the state of the art in TPAUC optimization and offers practical tools\nfor real-world machine learning applications.", "AI": {"tldr": "The paper introduces two stochastic primal-dual double block-coordinate algorithms for TPAUC maximization that offer faster convergence and better generalization than previous methods.", "motivation": "TPAUC is a crucial metric in binary classification with imbalanced data, but existing optimization methods are either approximations or inefficient.", "method": "Two stochastic primal-dual double block-coordinate algorithms are developed for TPAUC maximization, applicable to both convex and non-convex settings, using updates for both primal and dual variables.", "result": "Theoretical analysis shows improved convergence rates over prior methods. Experiments on benchmark datasets confirm superior performance with faster convergence and better generalization.", "conclusion": "This work enhances TPAUC optimization, providing practical tools for real-world machine learning tasks."}}
{"id": "2505.21575", "pdf": "https://arxiv.org/pdf/2505.21575", "abs": "https://arxiv.org/abs/2505.21575", "authors": ["Dawei Feng", "Di Mei", "Huiri Tan", "Lei Ren", "Xianying Lou", "Zhangxi Tan"], "title": "StreamLink: Large-Language-Model Driven Distributed Data Engineering System", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted by CIKM Workshop 2024,\n  https://sites.google.com/view/cikm2024-rag/papers?authuser=0#h.ddm5fg2z885t", "summary": "Large Language Models (LLMs) have shown remarkable proficiency in natural\nlanguage understanding (NLU), opening doors for innovative applications. We\nintroduce StreamLink - an LLM-driven distributed data system designed to\nimprove the efficiency and accessibility of data engineering tasks. We build\nStreamLink on top of distributed frameworks such as Apache Spark and Hadoop to\nhandle large data at scale. One of the important design philosophies of\nStreamLink is to respect user data privacy by utilizing local fine-tuned LLMs\ninstead of a public AI service like ChatGPT. With help from domain-adapted\nLLMs, we can improve our system's understanding of natural language queries\nfrom users in various scenarios and simplify the procedure of generating\ndatabase queries like the Structured Query Language (SQL) for information\nprocessing. We also incorporate LLM-based syntax and security checkers to\nguarantee the reliability and safety of each generated query. StreamLink\nillustrates the potential of merging generative LLMs with distributed data\nprocessing for comprehensive and user-centric data engineering. With this\narchitecture, we allow users to interact with complex database systems at\ndifferent scales in a user-friendly and security-ensured manner, where the SQL\ngeneration reaches over 10\\% of execution accuracy compared to baseline\nmethods, and allow users to find the most concerned item from hundreds of\nmillions of items within a few seconds using natural language.", "AI": {"tldr": "StreamLink is an LLM-driven distributed data system designed to improve data engineering tasks' efficiency and accessibility. It uses local fine-tuned LLMs for respecting user data privacy, incorporates domain-adapted LLMs to enhance natural language query understanding, simplifies SQL generation, and integrates LLM-based syntax and security checkers.", "motivation": "To enhance the efficiency and accessibility of data engineering tasks while ensuring user data privacy.", "method": "Building StreamLink on top of distributed frameworks such as Apache Spark and Hadoop, utilizing local fine-tuned LLMs instead of public AI services, incorporating domain-adapted LLMs to improve natural language query understanding, and integrating LLM-based syntax and security checkers.", "result": "SQL generation reaches over 10% of execution accuracy compared to baseline methods, and users can find concerned items from hundreds of millions within a few seconds using natural language.", "conclusion": "StreamLink demonstrates the potential of combining generative LLMs with distributed data processing for comprehensive and user-centric data engineering."}}
{"id": "2505.21959", "pdf": "https://arxiv.org/pdf/2505.21959", "abs": "https://arxiv.org/abs/2505.21959", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "categories": ["cs.LG", "cs.CL"], "comment": "Superalignment. arXiv admin note: substantial text overlap with\n  arXiv:2410.04571", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called \\textbf{EnsemW2S}, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4\\%, and 3.2\\%\nimprovements on ID datasets and, upto 6\\% and 2.28\\% on OOD datasets for\nexperts and student models respectively, underscoring the effectiveness of our\nproposed method in advancing W2S generalization.", "AI": {"tldr": "The paper introduces EnsemW2S, a method that uses token-level ensemble to improve weak models' ability to supervise stronger student models. It achieves significant improvements on both in-distribution and out-of-distribution datasets.", "motivation": "With the development of large language models (LLMs) nearing or surpassing human-level performance, it's crucial to find ways for smaller, human-level models to effectively supervise and enhance these powerful models using limited human-level data.", "method": "EnsemW2S employs a token-level ensemble strategy that iteratively combines multiple weak experts, addressing their shortcomings and refining their collective ability to supervise stronger student models.", "result": "Empirical results show notable improvements: 4% and 3.2% on in-distribution datasets, and up to 6% and 2.28% on out-of-distribution datasets for experts and student models respectively.", "conclusion": "EnsemW2S effectively advances weak-to-strong generalization, allowing weak models exposed to only human-level data to generalize to complex, super-human-level tasks."}}
{"id": "2505.21972", "pdf": "https://arxiv.org/pdf/2505.21972", "abs": "https://arxiv.org/abs/2505.21972", "authors": ["Patrick Vossler", "Fan Xia", "Yifan Mai", "Jean Feng"], "title": "Judging LLMs on a Simplex", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "28 pages, 7 figures", "summary": "Automated evaluation of free-form outputs from large language models (LLMs)\nis challenging because many distinct answers can be equally valid. A common\npractice is to use LLMs themselves as judges, but the theoretical properties of\nthis approach are not yet well understood. We show that a geometric framework\nthat represents both judges and candidates as points on a probability simplex\ncan provide helpful insight on what is or is not identifiable using LLM judges.\nOur theoretical analysis uncovers a \"phase transition\" in ranking\nidentifiability: for binary scoring systems, true rankings are identifiable\neven with weak judges under mild assumptions, while rankings become\nnon-identifiable for three or more scoring levels even with infinite data,\nabsent additional prior knowledge. This non-identifiability highlights how\nuncertainty in rankings stems from not only aleatoric uncertainty (i.e.,\ninherent stochasticity in the data) but also epistemic uncertainty regarding\nwhich assumptions hold, an aspect that has received limited attention until\nnow. To integrate both types of uncertainty, we use Bayesian inference to\nencode assumptions as priors and conduct sensitivity analysis of ranking\nestimates and credible intervals. Empirical evaluations across multiple\nbenchmarks demonstrate that Bayesian inference yields more accurate rankings\nand substantially improves coverage rates. These results underscore the\nimportance of taking a more holistic approach to uncertainty quantification\nwhen using LLMs as judges.", "AI": {"tldr": "The paper explores the use of a geometric framework and Bayesian inference to improve the identifiability and accuracy of rankings when using large language models (LLMs) as judges for evaluating free-form outputs. It uncovers a 'phase transition' in ranking identifiability and demonstrates that Bayesian inference yields more accurate rankings and improves coverage rates.", "motivation": "Automated evaluation of free-form outputs from LLMs is challenging due to the variability of valid answers. The theoretical properties of using LLMs themselves as judges are not well understood, motivating the need for deeper analysis.", "method": "A geometric framework representing judges and candidates as points on a probability simplex is used to analyze identifiability of rankings. Theoretical analysis uncovers a 'phase transition' in ranking identifiability. Bayesian inference is then applied to integrate aleatoric and epistemic uncertainties, encoding assumptions as priors and conducting sensitivity analysis.", "result": "Empirical evaluations show that Bayesian inference results in more accurate rankings and substantially improves coverage rates across multiple benchmarks.", "conclusion": "The study emphasizes the importance of considering both types of uncertainty (aleatoric and epistemic) and adopting a holistic approach to uncertainty quantification when using LLMs as judges."}}
{"id": "2505.21577", "pdf": "https://arxiv.org/pdf/2505.21577", "abs": "https://arxiv.org/abs/2505.21577", "authors": ["Huacan Wang", "Ziyi Ni", "Shuo Zhang", "Shuo Lu", "Sen Hu", "Ziyang He", "Chen Hu", "Jiaye Lin", "Yifu Guo", "Yuntao Du", "Pin Lyu"], "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving", "categories": ["cs.SE", "cs.AI"], "comment": "A novel approach; Very practical", "summary": "The ultimate goal of code agents is to solve complex tasks autonomously.\nAlthough large language models (LLMs) have made substantial progress in code\ngeneration, real-world tasks typically demand full-fledged code repositories\nrather than simple scripts. Building such repositories from scratch remains a\nmajor challenge. Fortunately, GitHub hosts a vast, evolving collection of\nopen-source repositories, which developers frequently reuse as modular\ncomponents for complex tasks. Yet, existing frameworks like OpenHands and\nSWE-Agent still struggle to effectively leverage these valuable resources.\nRelying solely on README files provides insufficient guidance, and deeper\nexploration reveals two core obstacles: overwhelming information and tangled\ndependencies of repositories, both constrained by the limited context windows\nof current LLMs. To tackle these issues, we propose RepoMaster, an autonomous\nagent framework designed to explore and reuse GitHub repositories for solving\ncomplex tasks. For efficient understanding, RepoMaster constructs function-call\ngraphs, module-dependency graphs, and hierarchical code trees to identify\nessential components, providing only identified core elements to the LLMs\nrather than the entire repository. During autonomous execution, it\nprogressively explores related components using our exploration tools and\nprunes information to optimize context usage. Evaluated on the adjusted\nMLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over\nthe strongest baseline OpenHands. On our newly released GitTaskBench,\nRepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token\nusage by 95%. Our code and demonstration materials are publicly available at\nhttps://github.com/wanghuacan/RepoMaster.", "AI": {"tldr": "To address the challenges of leveraging GitHub repositories for complex tasks, we propose RepoMaster, an autonomous agent framework that constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components. Evaluated on benchmarks, RepoMaster significantly improves valid submissions and task-pass rates while reducing token usage.", "motivation": "Existing frameworks struggle to effectively leverage GitHub's vast collection of open-source repositories due to overwhelming information and tangled dependencies, constrained by the limited context windows of current LLMs.", "method": "RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components. It progressively explores related components using exploration tools and prunes information to optimize context usage during autonomous execution.", "result": "Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On GitTaskBench, it lifts the task-pass rate from 24.1% to 62.9% while reducing token usage by 95%.", "conclusion": "RepoMaster is an effective framework for exploring and reusing GitHub repositories to solve complex tasks, significantly improving performance metrics while optimizing resource usage."}}
{"id": "2505.21974", "pdf": "https://arxiv.org/pdf/2505.21974", "abs": "https://arxiv.org/abs/2505.21974", "authors": ["Yu-Heng Hung", "Kai-Jie Lin", "Yu-Heng Lin", "Chien-YiWang", "Cheng Sun", "Ping-Chun Hsieh"], "title": "BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian optimization (BO) offers an efficient pipeline for optimizing\nblack-box functions with the help of a Gaussian process prior and an\nacquisition function (AF). Recently, in the context of single-objective BO,\nlearning-based AFs witnessed promising empirical results given its favorable\nnon-myopic nature. Despite this, the direct extension of these approaches to\nmulti-objective Bayesian optimization (MOBO) suffer from the\n\\textit{hypervolume identifiability issue}, which results from the\nnon-Markovian nature of MOBO problems. To tackle this, inspired by the\nnon-Markovian RL literature and the success of Transformers in language\nmodeling, we present a generalized deep Q-learning framework and propose\n\\textit{BOFormer}, which substantiates this framework for MOBO via sequence\nmodeling. Through extensive evaluation, we demonstrate that BOFormer constantly\noutperforms the benchmark rule-based and learning-based algorithms in various\nsynthetic MOBO and real-world multi-objective hyperparameter optimization\nproblems. We have made the source code publicly available to encourage further\nresearch in this direction.", "AI": {"tldr": "The paper introduces BOFormer, a deep Q-learning framework using sequence modeling inspired by non-Markovian RL and Transformers for multi-objective Bayesian optimization (MOBO). It addresses the hypervolume identifiability issue in MOBO and outperforms benchmark algorithms in synthetic and real-world problems.", "motivation": "Bayesian optimization has shown success in single-objective optimization, but extending learning-based acquisition functions to multi-objective Bayesian optimization suffers from the hypervolume identifiability issue due to the non-Markovian nature of MOBO problems.", "method": "The authors propose BOFormer, which uses a generalized deep Q-learning framework combined with sequence modeling inspired by non-Markovian reinforcement learning and Transformers. This approach is specifically designed to handle the complexities of multi-objective Bayesian optimization.", "result": "BOFormer consistently outperforms both rule-based and learning-based benchmark algorithms across various synthetic MOBO problems and real-world multi-objective hyperparameter optimization tasks.", "conclusion": "BOFormer presents an effective solution for multi-objective Bayesian optimization by addressing the hypervolume identifiability issue, demonstrating superior performance compared to existing methods."}}
{"id": "2505.21582", "pdf": "https://arxiv.org/pdf/2505.21582", "abs": "https://arxiv.org/abs/2505.21582", "authors": ["Christopher Knievel", "Alexander Bernhardt", "Christian Bernhardt"], "title": "AITEE -- Agentic Tutor for Electrical Engineering", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "12 pages, 11 figures, 6 tables", "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.", "AI": {"tldr": "The paper introduces AITEE, an agent-based tutoring system for electrical engineering that combines circuit reconstruction, graph-based similarity measures, and Spice simulation to provide personalized learning support.", "motivation": "To address the limitations of large language models in handling specific electrical circuit questions and to provide a more effective and personalized learning experience for electrical engineering students.", "method": "AITEE uses an adapted circuit reconstruction process for hand-drawn and digital circuits, a novel graph-based similarity measure for context retrieval, parallel Spice simulation for accuracy, and Socratic dialogue for guided questioning.", "result": "Experimental evaluations show that AITEE significantly outperforms baseline approaches in applying domain-specific knowledge, even with medium-sized LLMs performing acceptably.", "conclusion": "AITEE demonstrates the potential of agentic tutors to create scalable, personalized, and effective learning environments for electrical engineering education."}}
{"id": "2505.21978", "pdf": "https://arxiv.org/pdf/2505.21978", "abs": "https://arxiv.org/abs/2505.21978", "authors": ["Wanfu Gao", "Zengyao Man", "Zebin He", "Yuhao Tang", "Jun Gao", "Kunpeng Liu"], "title": "Two-Stage Feature Generation with Transformer and Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Feature generation is a critical step in machine learning, aiming to enhance\nmodel performance by capturing complex relationships within the data and\ngenerating meaningful new features. Traditional feature generation methods\nheavily rely on domain expertise and manual intervention, making the process\nlabor-intensive and challenging to adapt to different scenarios. Although\nautomated feature generation techniques address these issues to some extent,\nthey often face challenges such as feature redundancy, inefficiency in feature\nspace exploration, and limited adaptability to diverse datasets and tasks. To\naddress these problems, we propose a Two-Stage Feature Generation (TSFG)\nframework, which integrates a Transformer-based encoder-decoder architecture\nwith Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG\nleverages the Transformer's self-attention mechanism to efficiently represent\nand transform features, capturing complex dependencies within the data. PPO\nfurther enhances TSFG by dynamically adjusting the feature generation strategy\nbased on task-specific feedback, optimizing the process for improved\nperformance and adaptability. TSFG dynamically generates high-quality feature\nsets, significantly improving the predictive performance of machine learning\nmodels. Experimental results demonstrate that TSFG outperforms existing\nstate-of-the-art methods in terms of feature quality and adaptability.", "AI": {"tldr": "The paper proposes a Two-Stage Feature Generation (TSFG) framework that combines a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO) to generate high-quality feature sets, significantly improving the predictive performance of machine learning models.", "motivation": "Traditional feature generation methods rely on domain expertise and manual intervention. Automated methods have issues like feature redundancy, inefficiency in feature space exploration, and limited adaptability.", "method": "A TSFG framework is proposed which integrates a Transformer-based encoder-decoder architecture with PPO. The encoder-decoder model leverages the Transformer's self-attention mechanism for feature representation and transformation, while PPO dynamically adjusts the feature generation strategy based on task-specific feedback.", "result": "TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models.", "conclusion": "Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability."}}
{"id": "2505.21987", "pdf": "https://arxiv.org/pdf/2505.21987", "abs": "https://arxiv.org/abs/2505.21987", "authors": ["Zhendong Mi", "Zhenglun Kong", "Geng Yuan", "Shaoyi Huang"], "title": "ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning", "categories": ["cs.LG", "I.2.6; I.2.7"], "comment": "9 pages, 2 figures, 13 tables", "summary": "With the rapid expansion of large language models (LLMs), the demand for\nmemory and computational resources has grown significantly. Recent advances in\nLLM pruning aim to reduce the size and computational cost of these models.\nHowever, existing methods often suffer from either suboptimal pruning\nperformance or low time efficiency during the pruning process. In this work, we\npropose an efficient and effective pruning method that simultaneously achieves\nhigh pruning performance and fast pruning speed with improved calibration\nefficiency. Our approach introduces two key innovations: (1) An activation\ncosine similarity loss-guided pruning metric, which considers the angular\ndeviation of the output activation between the dense and pruned models. (2) An\nactivation variance-guided pruning metric, which helps preserve semantic\ndistinctions in output activations after pruning, enabling effective pruning\nwith shorter input sequences. These two components can be readily combined to\nenhance LLM pruning in both accuracy and efficiency. Experimental results show\nthat our method achieves up to an 18% reduction in perplexity and up to 63%\ndecrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.", "AI": {"tldr": "This paper proposes an efficient pruning method for large language models (LLMs) with two key innovations, achieving up to 18% reduction in perplexity and 63% decrease in pruning time.", "motivation": "The motivation of this paper is to address the limitations of existing LLM pruning methods which either have suboptimal pruning performance or low time efficiency during the pruning process.", "method": "The method introduces two key innovations: 1) An activation cosine similarity loss-guided pruning metric that considers angular deviation of output activation between dense and pruned models. 2) An activation variance-guided pruning metric that helps preserve semantic distinctions in output activations after pruning.", "result": "Experimental results show that the proposed method achieves up to 18% reduction in perplexity and up to 63% decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.", "conclusion": "The proposed pruning method efficiently enhances LLM pruning in both accuracy and efficiency."}}
{"id": "2505.22014", "pdf": "https://arxiv.org/pdf/2505.22014", "abs": "https://arxiv.org/abs/2505.22014", "authors": ["J\u00f6rg K. H. Franke", "Urs Spiegelhalter", "Marianna Nezhurina", "Jenia Jitsev", "Frank Hutter", "Michael Hefenbrock"], "title": "Learning in Compact Spaces with Approximately Normalized Transformers", "categories": ["cs.LG"], "comment": "Preprint", "summary": "In deep learning, regularization and normalization are common solutions for\nchallenges such as overfitting, numerical instabilities, and the increasing\nvariance in the residual stream. An alternative approach is to force all\nparameters and representations to lie on a hypersphere. This removes the need\nfor regularization and increases convergence speed, but comes with additional\ncosts. In this work, we propose a more holistic but approximate normalization\n(anTransformer). Our approach constrains the norm of parameters and normalizes\nall representations via scalar multiplications motivated by the tight\nconcentration of the norms of high-dimensional random vectors. When applied to\nGPT training, we observe a 40% faster convergence compared to models with QK\nnormalization, with less than 3% additional runtime. Deriving scaling laws for\nanGPT, we found our method enables training with larger batch sizes and fewer\nhyperparameters, while matching the favorable scaling characteristics of\nclassic GPT architectures.", "AI": {"tldr": "The paper proposes an approximate normalization method called anTransformer that constrains parameter norms and normalizes representations through scalar multiplications, inspired by high-dimensional random vectors' norm concentration. It accelerates GPT training convergence by 40% over QK normalization with minimal runtime increase, simplifies hyperparameter tuning, and supports larger batch sizes while preserving GPT's favorable scaling properties.", "motivation": "To address challenges in deep learning such as overfitting, numerical instability, and increasing variance in the residual stream without relying on traditional regularization methods. The authors aim to develop a more holistic normalization technique by forcing parameters and representations onto a hypersphere.", "method": "Propose anTransformer, which constrains parameter norms and normalizes all representations via scalar multiplications based on the tight concentration of norms of high-dimensional random vectors.", "result": "anTransformer shows 40% faster convergence compared to QK normalization during GPT training with less than 3% additional runtime. It also enables training with larger batch sizes and fewer hyperparameters while maintaining the favorable scaling characteristics of classic GPT architectures.", "conclusion": "anTransformer offers a promising approach for normalizing deep learning models, providing faster convergence, simpler hyperparameter tuning, and support for larger batch sizes."}}
{"id": "2505.21588", "pdf": "https://arxiv.org/pdf/2505.21588", "abs": "https://arxiv.org/abs/2505.21588", "authors": ["Young-Min Cho", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have enabled the\nemergence of multi-agent systems where LLMs interact, collaborate, and make\ndecisions in shared environments. While individual model behavior has been\nextensively studied, the dynamics of peer influence in such systems remain\nunderexplored. In this paper, we investigate herd behavior, the tendency of\nagents to align their outputs with those of their peers, within LLM-based\nmulti-agent interactions. We present a series of controlled experiments that\nreveal how herd behaviors are shaped by multiple factors. First, we show that\nthe gap between self-confidence and perceived confidence in peers significantly\nimpacts an agent's likelihood to conform. Second, we find that the format in\nwhich peer information is presented plays a critical role in modulating the\nstrength of herd behavior. Finally, we demonstrate that the degree of herd\nbehavior can be systematically controlled, and that appropriately calibrated\nherd tendencies can enhance collaborative outcomes. These findings offer new\ninsights into the social dynamics of LLM-based systems and open pathways for\ndesigning more effective and adaptive multi-agent collaboration frameworks.", "AI": {"tldr": "The paper explores herd behavior in LLM-based multi-agent systems through controlled experiments, revealing factors like confidence gaps and peer information format that impact conformism, and demonstrating the potential to control herd tendencies for better collaboration.", "motivation": "To understand the dynamics of peer influence and herd behavior within LLM-based multi-agent interactions, as this area remains underexplored despite extensive study on individual model behavior.", "method": "Conducting a series of controlled experiments examining how herd behaviors are shaped by various factors including self-confidence vs. perceived peer confidence, presentation format of peer information, and the degree of herd behavior.", "result": "Discovering significant impacts from confidence gaps and peer information formats on herd behavior, along with the ability to systematically control herd tendencies to improve collaborative outcomes.", "conclusion": "The findings provide new insights into social dynamics in LLM-based systems and suggest pathways for designing more effective and adaptive multi-agent collaboration frameworks."}}
{"id": "2505.22028", "pdf": "https://arxiv.org/pdf/2505.22028", "abs": "https://arxiv.org/abs/2505.22028", "authors": ["Zi-Hao Zhou", "Jun-Jie Wang", "Tong Wei", "Min-Ling Zhang"], "title": "Weakly-Supervised Contrastive Learning for Imprecise Class Labels", "categories": ["cs.LG"], "comment": "38 pages, 2 figures, 11 tables", "summary": "Contrastive learning has achieved remarkable success in learning effective\nrepresentations, with supervised contrastive learning often outperforming\nself-supervised approaches. However, in real-world scenarios, data annotations\nare often ambiguous or inaccurate, meaning that class labels may not reliably\nindicate whether two examples belong to the same class. This limitation\nrestricts the applicability of supervised contrastive learning. To address this\nchallenge, we introduce the concept of ``continuous semantic similarity'' to\ndefine positive and negative pairs. Instead of directly relying on imprecise\nclass labels, we measure the semantic similarity between example pairs, which\nquantifies how closely they belong to the same category by iteratively refining\nweak supervisory signals. Based on this concept, we propose a graph-theoretic\nframework for weakly-supervised contrastive learning, where semantic similarity\nserves as the graph weights. Our framework is highly versatile and can be\napplied to many weakly-supervised learning scenarios. We demonstrate its\neffectiveness through experiments in two common settings, i.e., noisy label and\npartial label learning, where existing methods can be easily integrated to\nsignificantly improve performance. Theoretically, we establish an error bound\nfor our approach, showing that it can approximate supervised contrastive\nlearning under mild conditions. The implementation code is available at\nhttps://github.com/Speechless-10308/WSC.", "AI": {"tldr": "The paper introduces continuous semantic similarity to redefine positive and negative pairs in contrastive learning, proposing a graph-theoretic framework for weakly-supervised contrastive learning that can be applied to noisy and partial label scenarios.", "motivation": "Supervised contrastive learning faces challenges due to ambiguous or inaccurate data annotations which limit its applicability.", "method": "The concept of 'continuous semantic similarity' is introduced to measure the semantic similarity between example pairs. A graph-theoretic framework is proposed where semantic similarity serves as the graph weights for weakly-supervised contrastive learning.", "result": "The framework shows effectiveness in noisy label and partial label learning settings, with theoretical error bound established demonstrating its approximation to supervised contrastive learning under mild conditions.", "conclusion": "A novel approach to address limitations of supervised contrastive learning is provided, expanding applicability to weakly-supervised scenarios."}}
{"id": "2505.21589", "pdf": "https://arxiv.org/pdf/2505.21589", "abs": "https://arxiv.org/abs/2505.21589", "authors": ["Carina Newen", "Luca Hinkamp", "Maria Ntonti", "Emmanuel M\u00fcller"], "title": "Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "19 pages, 18 figures", "summary": "From uncertainty quantification to real-world object detection, we recognize\nthe importance of machine learning algorithms, particularly in safety-critical\ndomains such as autonomous driving or medical diagnostics. In machine learning,\nambiguous data plays an important role in various machine learning domains.\nOptical illusions present a compelling area of study in this context, as they\noffer insight into the limitations of both human and machine perception.\nDespite this relevance, optical illusion datasets remain scarce. In this work,\nwe introduce a novel dataset of optical illusions featuring intermingled animal\npairs designed to evoke perceptual ambiguity. We identify generalizable visual\nconcepts, particularly gaze direction and eye cues, as subtle yet impactful\nfeatures that significantly influence model accuracy. By confronting models\nwith perceptual ambiguity, our findings underscore the importance of concepts\nin visual learning and provide a foundation for studying bias and alignment\nbetween human and machine vision. To make this dataset useful for general\npurposes, we generate optical illusions systematically with different concepts\ndiscussed in our bias mitigation section. The dataset is accessible in Kaggle\nvia\nhttps://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.\nOur source code can be found at\nhttps://github.com/KDD-OpenSource/Ambivision.git.", "AI": {"tldr": "This paper introduces a new dataset of optical illusions with animal pairs to study perceptual ambiguity, revealing the impact of gaze direction and eye cues on model accuracy, aiming to bridge the gap between human and machine vision.", "motivation": "To address the scarcity of optical illusion datasets and investigate the role of ambiguous data in machine learning, particularly in safety-critical domains.", "method": "Creating a novel dataset featuring intermingled animal pairs that evoke perceptual ambiguity, identifying key visual concepts like gaze direction and eye cues.", "result": "The dataset highlights the influence of subtle visual features on model accuracy and provides insights into bias and alignment in human and machine vision.", "conclusion": "Optical illusions offer valuable perspectives for understanding limitations in perception; this dataset serves as a tool for studying and mitigating biases in visual learning."}}
{"id": "2505.22041", "pdf": "https://arxiv.org/pdf/2505.22041", "abs": "https://arxiv.org/abs/2505.22041", "authors": ["Michael Grohs", "Adrian Rebmann", "Jana-Rebecca Rehse"], "title": "Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation", "categories": ["cs.LG"], "comment": "Accepted at the BPM Forum, located at the International Conference on\n  Business Process Management (BPM) 2025", "summary": "Conformance checking techniques detect undesired process behavior by\ncomparing process executions that are recorded in event logs to desired\nbehavior that is captured in a dedicated process model. If such models are not\navailable, conformance checking techniques are not applicable, but\norganizations might still be interested in detecting undesired behavior in\ntheir processes. To enable this, existing approaches use Large Language Models\n(LLMs), assuming that they can learn to distinguish desired from undesired\nbehavior through fine-tuning. However, fine-tuning is highly resource-intensive\nand the fine-tuned LLMs often do not generalize well. To address these\nlimitations, we propose an approach that requires neither a dedicated process\nmodel nor resource-intensive fine-tuning to detect undesired process behavior.\nInstead, we use Retrieval Augmented Generation (RAG) to provide an LLM with\ndirect access to a knowledge base that contains both desired and undesired\nprocess behavior from other processes, assuming that the LLM can transfer this\nknowledge to the process at hand. Our evaluation shows that our approach\noutperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that\nRAG is a viable alternative to resource-intensive fine-tuning, particularly\nwhen enriched with relevant context from the event log, such as frequent traces\nand activities.", "AI": {"tldr": "An approach using Retrieval Augmented Generation (RAG) is proposed to detect undesired process behavior without needing a dedicated process model or resource-intensive fine-tuning. Evaluation shows that it outperforms fine-tuned LLMs.", "motivation": "Conformance checking techniques need process models which might not be available, and existing approaches using fine-tuned LLMs are resource-intensive and lack generalization.", "method": "Propose an approach using RAG to provide LLMs with access to a knowledge base containing desired and undesired process behavior from other processes.", "result": "The approach outperforms fine-tuned LLMs in detecting undesired behavior when enriched with relevant context from the event log.", "conclusion": "RAG is a viable alternative to fine-tuning for detecting undesired process behavior."}}
{"id": "2505.22042", "pdf": "https://arxiv.org/pdf/2505.22042", "abs": "https://arxiv.org/abs/2505.22042", "authors": ["Hao Yang", "Haoxuan Li", "Mengyue Yang", "Xu Chen", "Mingming Gong"], "title": "Estimating the Effects of Sample Training Orders for Large Language Models without Retraining", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The order of training samples plays a crucial role in large language models\n(LLMs), significantly impacting both their external performance and internal\nlearning dynamics. Traditional methods for investigating this effect generally\nrequire retraining the model with various sample orders, which is\ncomputationally infeasible for LLMs. In this work, we improve traditional\nmethods by designing a retraining-free framework. By approximating Adam\noptimizer updates with first- and second-order Taylor expansions and utilizing\nrandom projection methods to store intermediate checkpoints, our framework can\nefficiently estimate model parameters for arbitrary training sample orders.\nNext, we apply our framework to two downstream research problems: (1) Training\ncurriculum design for LLMs -- we base our retraining-free framework to propose\na novel curriculum learning strategy that augments curriculum proposals with\nestimated model performances, enabling more informed sample scheduling. (2)\nLLMs' memorization and generalization effect analysis -- we use our\nretraining-free framework to estimate how the positions of training samples\ninfluence LLMs' capacity for memorization and generalization. We conduct\nextensive experiments to validate the effectiveness of our retraining-free\nframework in reproducing the true model performances, and further demonstrate\nits potential in optimizing LLM training curricula and analyzing the\nmemorization and generalization effects of LLMs.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u4efb\u610f\u8bad\u7ec3\u6837\u672c\u987a\u5e8f\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u3002\u6b64\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\u4f18\u5316LLM\u8bad\u7ec3\u8bfe\u7a0b\u8bbe\u8ba1\u548c\u5206\u6790\u5176\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u8bad\u7ec3\u6837\u672c\u987a\u5e8f\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5916\u90e8\u6027\u80fd\u548c\u5185\u90e8\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u56e0\u9700\u8981\u591a\u6b21\u91cd\u65b0\u8bad\u7ec3\u800c\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u6cf0\u52d2\u5c55\u5f00\u8fd1\u4f3cAdam\u4f18\u5316\u5668\u66f4\u65b0\uff0c\u5e76\u5229\u7528\u968f\u673a\u6295\u5f71\u65b9\u6cd5\u5b58\u50a8\u4e2d\u95f4\u68c0\u67e5\u70b9\uff0c\u4ece\u800c\u6709\u6548\u4f30\u8ba1\u4efb\u610f\u8bad\u7ec3\u6837\u672c\u987a\u5e8f\u4e0b\u7684\u6a21\u578b\u53c2\u6570\u3002\u5c06\u5176\u5e94\u7528\u4e8e\u4e24\u4e2a\u4e0b\u6e38\u95ee\u9898\uff1a\uff081\uff09\u57fa\u4e8e\u4f30\u8ba1\u6a21\u578b\u6027\u80fd\u6539\u8fdb\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1b\uff082\uff09\u5206\u6790\u8bad\u7ec3\u6837\u672c\u4f4d\u7f6e\u5bf9LLM\u8bb0\u5fc6\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u91cd\u73b0\u771f\u5b9e\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4f18\u5316LLM\u8bad\u7ec3\u8bfe\u7a0b\u548c\u5206\u6790\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u6548\u679c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u6846\u67b6\u4e3a\u7814\u7a76LLM\u8bad\u7ec3\u6837\u672c\u987a\u5e8f\u5f71\u54cd\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u8bad\u7ec3\u8bfe\u7a0b\u8bbe\u8ba1\u548c\u6df1\u5165\u7406\u89e3\u6a21\u578b\u7684\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.21593", "pdf": "https://arxiv.org/pdf/2505.21593", "abs": "https://arxiv.org/abs/2505.21593", "authors": ["Yang Yang", "Siming Zheng", "Jinwei Chen", "Boxi Wu", "Xiaofei He", "Deng Cai", "Bo Li", "Peng-Tao Jiang"], "title": "Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "project page: https://vivocameraresearch.github.io/any2bokeh/", "summary": "Recent advances in diffusion based editing models have enabled realistic\ncamera simulation and image-based bokeh, but video bokeh remains largely\nunexplored. Existing video editing models cannot explicitly control focus\nplanes or adjust bokeh intensity, limiting their applicability for controllable\noptical effects. Moreover, naively extending image-based bokeh methods to video\noften results in temporal flickering and unsatisfactory edge blur transitions\ndue to the lack of temporal modeling and generalization capability. To address\nthese challenges, we propose a novel one-step video bokeh framework that\nconverts arbitrary input videos into temporally coherent, depth-aware bokeh\neffects. Our method leverages a multi-plane image (MPI) representation\nconstructed through a progressively widening depth sampling function, providing\nexplicit geometric guidance for depth-dependent blur synthesis. By conditioning\na single-step video diffusion model on MPI layers and utilizing the strong 3D\npriors from pre-trained models such as Stable Video Diffusion, our approach\nachieves realistic and consistent bokeh effects across diverse scenes.\nAdditionally, we introduce a progressive training strategy to enhance temporal\nconsistency, depth robustness, and detail preservation. Extensive experiments\ndemonstrate that our method produces high-quality, controllable bokeh effects\nand achieves state-of-the-art performance on multiple evaluation benchmarks.", "AI": {"tldr": "Recent advances in diffusion based editing models have enabled realistic camera simulation and image-based bokeh, but video bokeh remains largely unexplored. To address the challenges, we propose a novel one-step video bokeh framework that converts arbitrary input videos into temporally coherent, depth-aware bokeh effects.", "motivation": "Existing video editing models cannot explicitly control focus planes or adjust bokeh intensity, limiting their applicability for controllable optical effects. Moreover, naively extending image-based bokeh methods to video often results in temporal flickering and unsatisfactory edge blur transitions due to the lack of temporal modeling and generalization capability.", "method": "The proposed method leverages a multi-plane image (MPI) representation constructed through a progressively widening depth sampling function, providing explicit geometric guidance for depth-dependent blur synthesis. By conditioning a single-step video diffusion model on MPI layers and utilizing the strong 3D priors from pre-trained models such as Stable Video Diffusion.", "result": "Extensive experiments demonstrate that our method produces high-quality, controllable bokeh effects and achieves state-of-the-art performance on multiple evaluation benchmarks.", "conclusion": "We propose a novel one-step video bokeh framework that converts arbitrary input videos into temporally coherent, depth-aware bokeh effects."}}
{"id": "2505.22049", "pdf": "https://arxiv.org/pdf/2505.22049", "abs": "https://arxiv.org/abs/2505.22049", "authors": ["Laetitia Chapel", "Romain Tavenard", "Samuel Vaiter"], "title": "Differentiable Generalized Sliced Wasserstein Plans", "categories": ["cs.LG"], "comment": null, "summary": "Optimal Transport (OT) has attracted significant interest in the machine\nlearning community, not only for its ability to define meaningful distances\nbetween probability distributions -- such as the Wasserstein distance -- but\nalso for its formulation of OT plans. Its computational complexity remains a\nbottleneck, though, and slicing techniques have been developed to scale OT to\nlarge datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a\nsingle one-dimensional plan back to the original multidimensional space,\nfinally selecting the slice that yields the lowest Wasserstein distance as an\napproximation of the full OT plan. Despite its computational and theoretical\nadvantages, min-SWGG inherits typical limitations of slicing methods: (i) the\nnumber of required slices grows exponentially with the data dimension, and (ii)\nit is constrained to linear projections. Here, we reformulate min-SWGG as a\nbilevel optimization problem and propose a differentiable approximation scheme\nto efficiently identify the optimal slice, even in high-dimensional settings.\nWe furthermore define its generalized extension for accommodating to data\nliving on manifolds. Finally, we demonstrate the practical value of our\napproach in various applications, including gradient flows on manifolds and\nhigh-dimensional spaces, as well as a novel sliced OT-based conditional flow\nmatching for image generation -- where fast computation of transport plans is\nessential.", "AI": {"tldr": "The paper addresses the computational bottlenecks of Optimal Transport (OT) by reformulating min-SWGG as a bilevel optimization problem and proposing a differentiable approximation scheme to efficiently find the optimal slice, even in high-dimensional settings. It also generalizes the method for data on manifolds and demonstrates practical applications in gradient flows and image generation.", "motivation": "Optimal Transport (OT) is powerful but computationally expensive, especially in high dimensions. Current slicing methods like min-SWGG have limitations such as exponential growth in required slices with increasing dimensionality and being constrained to linear projections.", "method": "The authors reformulate min-SWGG as a bilevel optimization problem and introduce a differentiable approximation scheme to identify the optimal slice efficiently. They also extend the method to handle data living on manifolds.", "result": "The approach shows practical value in various applications including gradient flows on manifolds and high-dimensional spaces, and a novel sliced OT-based conditional flow matching for image generation.", "conclusion": "This work provides an efficient solution to overcome typical limitations of slicing methods in Optimal Transport, enabling its application to higher dimensional datasets and complex data structures."}}
{"id": "2505.21594", "pdf": "https://arxiv.org/pdf/2505.21594", "abs": "https://arxiv.org/abs/2505.21594", "authors": ["Yeshwanth Venkatesha", "Souvik Kundu", "Priyadarshini Panda"], "title": "Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits", "categories": ["cs.RO", "cs.AI", "cs.DC"], "comment": null, "summary": "Large Language Models (LLMs) enable various applications on edge devices such\nas smartphones, wearables, and embodied robots. However, their deployment often\ndepends on expensive cloud-based APIs, creating high operational costs, which\nlimit access for smaller organizations and raise sustainability concerns.\nCertain LLMs can be deployed on-device, offering a cost-effective solution with\nreduced latency and improved privacy. Yet, limited computing resources\nconstrain the size and accuracy of models that can be deployed, necessitating a\ncollaborative design between edge and cloud. We propose a fast and\ncost-effective speculative edge-cloud decoding framework with a large target\nmodel on the server and a small draft model on the device. By introducing early\nexits in the target model, tokens are generated mid-verification, allowing the\nclient to preemptively draft subsequent tokens before final verification, thus\nutilizing idle time and enhancing parallelism between edge and cloud. Using an\nNVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)\nand Llama2-7B (target) models, our method achieves up to a 35% reduction in\nlatency compared to cloud-based autoregressive decoding, with an additional 11%\nimprovement from preemptive drafting. To demonstrate real-world applicability,\nwe deploy our method on the Unitree Go2 quadruped robot using Vision-Language\nModel (VLM) based control, achieving a 21% speedup over traditional cloud-based\nautoregressive decoding. These results demonstrate the potential of our\nframework for real-time LLM and VLM applications on resource-constrained edge\ndevices.", "AI": {"tldr": "The paper proposes a speculative edge-cloud decoding framework that combines a large server-side model with a small device-side model to reduce latency and improve efficiency for LLMs on edge devices.", "motivation": "To address the high operational costs and sustainability concerns of deploying Large Language Models (LLMs) via cloud-based APIs, as well as the constraints posed by limited computing resources on edge devices.", "method": "A fast and cost-effective speculative edge-cloud decoding framework is proposed. It uses a large target model on the server and a small draft model on the device. Early exits in the target model allow tokens to be generated mid-verification, letting the client preemptively draft subsequent tokens before final verification.", "result": "The method achieves up to a 35% reduction in latency compared to cloud-based autoregressive decoding, with an additional 11% improvement from preemptive drafting. On a real-world deployment using a quadruped robot, it achieved a 21% speedup over traditional cloud-based autoregressive decoding.", "conclusion": "This framework shows potential for real-time LLM and Vision-Language Model (VLM) applications on resource-constrained edge devices."}}
{"id": "2505.22074", "pdf": "https://arxiv.org/pdf/2505.22074", "abs": "https://arxiv.org/abs/2505.22074", "authors": ["Co\u015fku Can Horuz", "Geoffrey Kasenbacher", "Saya Higuchi", "Sebastian Kairat", "Jendrik Stoltz", "Moritz Pesl", "Bernhard A. Moser", "Christoph Linse", "Thomas Martinetz", "Sebastian Otte"], "title": "The Resurrection of the ReLU", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modeling sophisticated activation functions within deep learning\narchitectures has evolved into a distinct research direction. Functions such as\nGELU, SELU, and SiLU offer smooth gradients and improved convergence\nproperties, making them popular choices in state-of-the-art models. Despite\nthis trend, the classical ReLU remains appealing due to its simplicity,\ninherent sparsity, and other advantageous topological characteristics. However,\nReLU units are prone to becoming irreversibly inactive - a phenomenon known as\nthe dying ReLU problem - which limits their overall effectiveness. In this\nwork, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,\nplug-and-play regularizer for deep architectures. SUGAR preserves the standard\nReLU function during the forward pass but replaces its derivative in the\nbackward pass with a smooth surrogate that avoids zeroing out gradients. We\ndemonstrate that SUGAR, when paired with a well-chosen surrogate function,\nsubstantially enhances generalization performance over convolutional network\narchitectures such as VGG-16 and ResNet-18, providing sparser activations while\neffectively resurrecting dead ReLUs. Moreover, we show that even in modern\narchitectures like Conv2NeXt and Swin Transformer - which typically employ GELU\n- substituting these with SUGAR yields competitive and even slightly superior\nperformance. These findings challenge the prevailing notion that advanced\nactivation functions are necessary for optimal performance. Instead, they\nsuggest that the conventional ReLU, particularly with appropriate gradient\nhandling, can serve as a strong, versatile revived classic across a broad range\nof deep learning vision models.", "AI": {"tldr": "The paper introduces SUGAR, a novel regularizer for ReLU activations that replaces the derivative in the backward pass with a smooth surrogate gradient. This approach enhances generalization performance in various deep learning architectures, including VGG-16, ResNet-18, Conv2NeXt, and Swin Transformer. It revives dead ReLUs and offers sparser activations, challenging the necessity of advanced activation functions.", "motivation": "Despite the popularity of advanced activation functions like GELU, SELU, and SiLU, classical ReLU remains appealing due to its simplicity and advantageous characteristics. However, it suffers from the 'dying ReLU' problem, which limits its effectiveness. The authors aim to address this issue by enhancing the conventional ReLU's gradient handling without losing its benefits.", "method": "SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate gradient. This avoids zeroing out gradients and effectively resurrects dead ReLUs.", "result": "SUGAR substantially improves generalization performance over convolutional network architectures such as VGG-16 and ResNet-18. It also provides sparser activations while reviving dead ReLUs. Even in modern architectures like Conv2NeXt and Swin Transformer, substituting GELU with SUGAR yields competitive or slightly superior performance.", "conclusion": "The findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, with appropriate gradient handling, can serve as a strong, versatile component across a broad range of deep learning vision models."}}
{"id": "2505.22081", "pdf": "https://arxiv.org/pdf/2505.22081", "abs": "https://arxiv.org/abs/2505.22081", "authors": ["Shun Sato", "Issei Sato"], "title": "Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?", "categories": ["cs.LG"], "comment": null, "summary": "Symbolic regression aims to discover mathematical equations that fit given\nnumerical data. It has been applied in various fields of scientific research,\nsuch as producing human-readable expressions that explain physical phenomena.\nRecently, Neural symbolic regression (NSR) methods that involve Transformers\npre-trained on large-scale synthetic datasets have gained attention. While\nthese methods offer advantages such as short inference time, they suffer from\nlow performance, particularly when the number of input variables is large. In\nthis study, we hypothesized that this limitation stems from the memorization\nbias of Transformers in symbolic regression. We conducted a quantitative\nevaluation of this bias in Transformers using a synthetic dataset and found\nthat Transformers rarely generate expressions not present in the training data.\nAdditional theoretical analysis reveals that this bias arises from the\nTransformer's inability to construct expressions compositionally while\nverifying their numerical validity. We finally examined if tailoring test-time\nstrategies can lead to reduced memorization bias and better performance. We\nempirically demonstrate that providing additional information to the model at\ntest time can significantly mitigate memorization bias. On the other hand, we\nalso find that reducing memorization bias does not necessarily correlate with\nimproved performance. These findings contribute to a deeper understanding of\nthe limitations of NSR approaches and offer a foundation for designing more\nrobust, generalizable symbolic regression methods. Code is available at\nhttps://github.com/Shun-0922/Mem-Bias-NSR .", "AI": {"tldr": "Symbolic regression uses mathematical equations to fit numerical data. Recently, Neural symbolic regression (NSR) methods with Transformers have been developed but suffer from low performance when the number of input variables is large due to memorization bias.", "motivation": "The motivation of this paper is to address the limitation of current NSR methods that involve Transformers pre-trained on large-scale synthetic datasets, which is their poor performance when dealing with a large number of input variables.", "method": "The authors hypothesized that this limitation stems from the memorization bias of Transformers in symbolic regression. They conducted a quantitative evaluation of this bias using a synthetic dataset and performed additional theoretical analysis. They also examined if tailoring test-time strategies can lead to reduced memorization bias and better performance.", "result": "The authors found that Transformers rarely generate expressions not present in the training data and that this bias arises from the Transformer's inability to construct expressions compositionally while verifying their numerical validity. They demonstrated that providing additional information to the model at test time can significantly mitigate memorization bias, but reducing memorization bias does not necessarily correlate with improved performance.", "conclusion": "These findings contribute to a deeper understanding of the limitations of NSR approaches and offer a foundation for designing more robust, generalizable symbolic regression methods."}}
{"id": "2505.21596", "pdf": "https://arxiv.org/pdf/2505.21596", "abs": "https://arxiv.org/abs/2505.21596", "authors": ["Esra Adiyeke", "Tianqi Liu", "Venkata Sai Dheeraj Naganaboina", "Han Li", "Tyler J. Loftus", "Yuanfang Ren", "Benjamin Shickel", "Matthew M. Ruppert", "Karandeep Singh", "Ruogu Fang", "Parisa Rashidi", "Azra Bihorac", "Tezcan Ozrazgat-Baslanti"], "title": "Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "41 pages, 1 table, 5 figures, 5 supplemental tables, 6 supplemental\n  figures", "summary": "Traditional methods of surgical decision making heavily rely on human\nexperience and prompt actions, which are variable. A data-driven system\ngenerating treatment recommendations based on patient states can be a\nsubstantial asset in perioperative decision-making, as in cases of\nintraoperative hypotension, for which suboptimal management is associated with\nacute kidney injury (AKI), a common and morbid postoperative complication. We\ndeveloped a Reinforcement Learning (RL) model to recommend optimum dose of\nintravenous (IV) fluid and vasopressors during surgery to avoid intraoperative\nhypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries\nfrom 42,547 adult patients who underwent major surgery at a quaternary care\nhospital between June 2014 and September 2020. Of these, 34,186 surgeries were\nused for model training and 15,835 surgeries were reserved for testing. We\ndeveloped a Deep Q-Networks based RL model using 16 variables including\nintraoperative physiologic time series, total dose of IV fluid and vasopressors\nextracted for every 15-minute epoch. The model replicated 69% of physician's\ndecisions for the dosage of vasopressors and proposed higher or lower dosage of\nvasopressors than received in 10% and 21% of the treatments, respectively. In\nterms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min\nof the actual dose in 41% of the cases, with higher or lower doses recommended\nfor 27% and 32% of the treatments, respectively. The model resulted in a higher\nestimated policy value compared to the physicians' actual treatments, as well\nas random and zero-drug policies. AKI prevalence was the lowest in patients\nreceiving medication dosages that aligned with model's decisions. Our findings\nsuggest that implementation of the model's policy has the potential to reduce\npostoperative AKI and improve other outcomes driven by intraoperative\nhypotension.", "AI": {"tldr": "An RL model was developed using surgical data to recommend optimal IV fluid and vasopressor dosage during surgery. This model showed higher policy value than physicians' actual treatments and could potentially reduce postoperative AKI.", "motivation": "Traditional surgical decision-making methods depend on human experience, which is variable. A data-driven system generating treatment recommendations based on patient states can be valuable in perioperative decision-making, especially in managing intraoperative hypotension that is associated with postoperative complications like AKI.", "method": "A Reinforcement Learning (RL) model, specifically a Deep Q-Networks based model, was developed using data from 50,021 surgeries. The model used 16 variables including intraoperative physiologic time series, total dose of IV fluid and vasopressors extracted every 15 minutes to recommend optimum dosages of IV fluid and vasopressors.", "result": "The model replicated 69% of physician's decisions for vasopressor dosage, proposed higher dosage in 10% and lower dosage in 21% of the cases. For IV fluids, it recommended within 0.05 ml/kg/15 min of the actual dose in 41% of the cases, higher doses in 27%, and lower doses in 32%. The model resulted in a higher estimated policy value compared to physicians' treatments and random policies. AKI prevalence was lowest when medication aligned with the model's decisions.", "conclusion": "The implementation of the model's policy has the potential to reduce postoperative AKI and improve other outcomes related to intraoperative hypotension."}}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600", "abs": "https://arxiv.org/abs/2505.21600", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u63a8\u7406\u5f00\u9500\u5927\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u6548\u7387\u9ad8\u4f46\u6027\u80fd\u5dee\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u53ea\u6709\u5c11\u91cf\u7684\u5173\u952etoken\u5bfc\u81f4LLMs\u548cSLMs\u7684\u63a8\u7406\u8def\u5f84\u4e0d\u540c\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86Roads to Rome (R2R) \u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edctoken\u8def\u7531\u9009\u62e9\u6027\u5730\u4f7f\u7528LLMs\u5904\u7406\u5173\u952etoken\uff0c\u800c\u5c06\u5927\u591a\u6570token\u751f\u6210\u4ea4\u7ed9SLM\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cR2R\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u84b8\u998f\u5f97\u5230\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\u5927\u5927\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4f46\u5176\u6027\u80fd\u7531\u4e8e\u65e0\u6cd5\u8ddf\u968f\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u63a8\u7406\u8def\u5f84\u800c\u53d7\u5230\u5f71\u54cd\u3002\u7136\u800c\uff0c\u7814\u7a76\u53d1\u73b0\u53ea\u6709\u5c11\u91cf\u7684token\u771f\u6b63\u5bfc\u81f4\u4e86LLMs\u548cSLMs\u4e4b\u95f4\u7684\u63a8\u7406\u8def\u5f84\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoads to Rome (R2R) \u7684\u795e\u7ecftoken\u8def\u7531\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u5e76\u4ec5\u5229\u7528LLMs\u5904\u7406\u8fd9\u4e9b\u5bf9\u63a8\u7406\u8def\u5f84\u6709\u91cd\u5927\u5f71\u54cd\u7684\u5173\u952etoken\uff0c\u800c\u5c06\u5927\u90e8\u5206token\u751f\u6210\u4efb\u52a1\u7559\u7ed9SLMs\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc6\u522b\u4e0d\u540c\u7684token\u5e76\u751f\u6210token\u7ea7\u522b\u7684\u8def\u7531\u6807\u7b7e\u4ee5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u3002", "result": "R2R\u7ed3\u5408\u4e86DeepSeek\u7cfb\u5217\u7684R1-1.5B\u548cR1-32B\u6a21\u578b\uff0c\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u95ee\u7b54\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5e73\u5747\u6fc0\u6d3b\u53c2\u6570\u89c4\u6a21\u4e3a5.6B\u65f6\uff0cR2R\u7684\u51c6\u786e\u7387\u6bd4R1-7B\u9ad8\u51fa1.6\u500d\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86R1-14B\u6a21\u578b\u7684\u8868\u73b0\u3002\u4e0eR1-32B\u76f8\u6bd4\uff0cR2R\u63d0\u4f9b\u4e862.8\u500d\u7684\u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "R2R\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5229\u7528LLMs\u5904\u7406\u5173\u952etoken\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u6548\u7387\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002"}}
{"id": "2505.22109", "pdf": "https://arxiv.org/pdf/2505.22109", "abs": "https://arxiv.org/abs/2505.22109", "authors": ["Paul Krzakala", "Gabriel Melo", "Charlotte Laclau", "Florence d'Alch\u00e9-Buc", "R\u00e9mi Flamary"], "title": "The quest for the GRAph Level autoEncoder (GRALE)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although graph-based learning has attracted a lot of attention, graph\nrepresentation learning is still a challenging task whose resolution may impact\nkey application fields such as chemistry or biology. To this end, we introduce\nGRALE, a novel graph autoencoder that encodes and decodes graphs of varying\nsizes into a shared embedding space. GRALE is trained using an Optimal\nTransport-inspired loss that compares the original and reconstructed graphs and\nleverages a differentiable node matching module, which is trained jointly with\nthe encoder and decoder. The proposed attention-based architecture relies on\nEvoformer, the core component of AlphaFold, which we extend to support both\ngraph encoding and decoding. We show, in numerical experiments on simulated and\nmolecular data, that GRALE enables a highly general form of pre-training,\napplicable to a wide range of downstream tasks, from classification and\nregression to more complex tasks such as graph interpolation, editing,\nmatching, and prediction.", "AI": {"tldr": "GRALE is a novel graph autoencoder that leverages an Optimal Transport-inspired loss and a differentiable node matching module, based on the attention-based architecture of Evoformer. It can encode and decode graphs of varying sizes into a shared embedding space, enabling a highly general form of pre-training for downstream tasks.", "motivation": "Graph representation learning remains a challenging task with significant impacts on fields like chemistry and biology.", "method": "GRALE uses an Optimal Transport-inspired loss to compare original and reconstructed graphs and incorporates a differentiable node matching module trained jointly with the encoder and decoder. The method extends Evoformer to support both graph encoding and decoding.", "result": "Numerical experiments on simulated and molecular data show that GRALE enables a highly general form of pre-training applicable to various downstream tasks including classification, regression, and complex tasks such as graph interpolation, editing, matching, and prediction.", "conclusion": "GRALE offers a powerful tool for graph representation learning, impacting applications in chemistry, biology, and other fields."}}
{"id": "2505.21603", "pdf": "https://arxiv.org/pdf/2505.21603", "abs": "https://arxiv.org/abs/2505.21603", "authors": ["Andre Massahiro Shimaoka", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Leveraging XP and CRISP-DM for Agile Data Science Projects", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "This study explores the integration of eXtreme Programming (XP) and the\nCross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data\nScience projects. We conducted a case study at the e-commerce company Elo7 to\nanswer the research question: How can the agility of the XP method be\nintegrated with CRISP-DM in Data Science projects? Data was collected through\ninterviews and questionnaires with a Data Science team consisting of data\nscientists, ML engineers, and data product managers. The results show that 86%\nof the team frequently or always applies CRISP-DM, while 71% adopt XP practices\nin their projects. Furthermore, the study demonstrates that it is possible to\ncombine CRISP-DM with XP in Data Science projects, providing a structured and\ncollaborative approach. Finally, the study generated improvement\nrecommendations for the company.", "AI": {"tldr": "This study explores the integration of XP and CRISP-DM in agile Data Science projects at Elo7, finding high adoption rates for both methods and demonstrating their successful combination.", "motivation": "To answer how the agility of the XP method can be integrated with CRISP-DM in Data Science projects.", "method": "Case study at e-commerce company Elo7, collecting data through interviews and questionnaires with a Data Science team.", "result": "86% of the team frequently or always applies CRISP-DM, 71% adopt XP practices; it's possible to combine CRISP-DM with XP in Data Science projects.", "conclusion": "XP and CRISP-DM can be successfully integrated in Data Science projects, providing a structured and collaborative approach."}}
{"id": "2505.22114", "pdf": "https://arxiv.org/pdf/2505.22114", "abs": "https://arxiv.org/abs/2505.22114", "authors": ["MaryBeth Defrance", "Guillaume Bied", "Maarten Buyl", "Jefrey Lijffijt", "Tijl De Bie"], "title": "BiMi Sheets: Infosheets for bias mitigation methods", "categories": ["cs.LG"], "comment": null, "summary": "Over the past 15 years, hundreds of bias mitigation methods have been\nproposed in the pursuit of fairness in machine learning (ML). However,\nalgorithmic biases are domain-, task-, and model-specific, leading to a\n`portability trap': bias mitigation solutions in one context may not be\nappropriate in another. Thus, a myriad of design choices have to be made when\ncreating a bias mitigation method, such as the formalization of fairness it\npursues, and where and how it intervenes in the ML pipeline. This creates\nchallenges in benchmarking and comparing the relative merits of different bias\nmitigation methods, and limits their uptake by practitioners.\n  We propose BiMi Sheets as a portable, uniform guide to document the design\nchoices of any bias mitigation method. This enables researchers and\npractitioners to quickly learn its main characteristics and to compare with\ntheir desiderata. Furthermore, the sheets' structure allow for the creation of\na structured database of bias mitigation methods. In order to foster the\nsheets' adoption, we provide a platform for finding and creating BiMi Sheets at\nbimisheet.com.", "AI": {"tldr": "An abstract discussing the challenges of bias mitigation methods in machine learning and proposing BiMi Sheets as a solution for documenting and comparing these methods.", "motivation": "Hundreds of bias mitigation methods have been proposed but face challenges due to being domain-, task-, and model-specific, leading to the 'portability trap'. This makes it difficult to benchmark and compare different methods.", "method": "Propose BiMi Sheets as a portable, uniform guide to document the design choices of any bias mitigation method. The sheets' structure allows for a structured database of these methods and a platform at bimisheet.com is provided for finding and creating BiMi Sheets.", "result": "BiMi Sheets enable researchers and practitioners to quickly learn the main characteristics of a bias mitigation method and compare with their desiderata, fostering better understanding and adoption.", "conclusion": "BiMi Sheets provide a solution to the challenges faced in comparing and adopting bias mitigation methods by offering a structured way to document them."}}
{"id": "2505.21604", "pdf": "https://arxiv.org/pdf/2505.21604", "abs": "https://arxiv.org/abs/2505.21604", "authors": ["Kristina Radivojevic", "Caleb Reinking", "Shaun Whitfield", "Paul Brenner"], "title": "Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Social media serves as a primary communication and information dissemination\nplatform for major global events, entertainment, and niche or topically focused\ncommunity discussions. Therefore, it represents a valuable resource for\nresearchers who aim to understand numerous questions. However, obtaining data\ncan be difficult, expensive, and often unreliable due to the presence of bots,\nfake accounts, and manipulated content. Additionally, there are ethical\nconcerns if researchers decide to conduct an online experiment without\nexplicitly notifying social media users about their intent. There is a need for\nmore controlled and scalable mechanisms to evaluate the impacts of digital\ndiscussion interventions on audiences. We introduce the Public Discourse\nSandbox (PDS), which serves as a digital discourse research platform for\nhuman-AI as well as AI-AI discourse research, testing, and training. PDS\nprovides a safe and secure space for research experiments that are not viable\non public, commercial social media platforms. Its main purpose is to enable the\nunderstanding of AI behaviors and the impacts of customized AI participants via\ntechniques such as prompt engineering, retrieval-augmented generation (RAG),\nand fine-tuning. We provide a hosted live version of the sandbox to support\nresearchers as well as the open-sourced code on GitHub for community\ncollaboration and contribution.", "AI": {"tldr": "The paper introduces Public Discourse Sandbox (PDS), a digital discourse research platform for human-AI and AI-AI discourse research, testing, and training to address the difficulties of obtaining reliable data on social media and ethical concerns.", "motivation": "Social media is an important platform for communication and information dissemination, but obtaining reliable data can be difficult due to bots, fake accounts, manipulated content, and ethical concerns. There is a need for more controlled and scalable mechanisms to evaluate the impacts of digital discussion interventions on audiences.", "method": "The PDS provides a safe and secure space for research experiments that are not viable on public, commercial social media platforms. It enables understanding of AI behaviors and the impacts of customized AI participants via techniques such as prompt engineering, retrieval-augmented generation (RAG), and fine-tuning.", "result": "A hosted live version of the sandbox has been provided to support researchers along with open-sourced code on GitHub for community collaboration and contribution.", "conclusion": "The PDS serves as a valuable tool for digital discourse research, offering a controlled environment for evaluating the impacts of digital discussion interventions."}}
{"id": "2505.22151", "pdf": "https://arxiv.org/pdf/2505.22151", "abs": "https://arxiv.org/abs/2505.22151", "authors": ["Claude Formanek", "Omayma Mahjoub", "Louay Ben Nessir", "Sasha Abramowitz", "Ruan de Kock", "Wiem Khlifi", "Simon Du Toit", "Felix Chalumeau", "Daniel Rajaonarivonivelomanantsoa", "Arnol Fokam", "Siddarth Singh", "Ulrich Mbou Sob", "Arnu Pretorius"], "title": "Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL", "categories": ["cs.LG"], "comment": null, "summary": "A key challenge in offline multi-agent reinforcement learning (MARL) is\nachieving effective many-agent multi-step coordination in complex environments.\nIn this work, we propose Oryx, a novel algorithm for offline cooperative MARL\nto directly address this challenge. Oryx adapts the recently proposed\nretention-based architecture Sable and combines it with a sequential form of\nimplicit constraint Q-learning (ICQ), to develop a novel offline\nauto-regressive policy update scheme. This allows Oryx to solve complex\ncoordination challenges while maintaining temporal coherence over lengthy\ntrajectories. We evaluate Oryx across a diverse set of benchmarks from prior\nworks (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and\ncontinuous control, varying in scale and difficulty. Oryx achieves\nstate-of-the-art performance on more than 80% of the 65 tested datasets,\noutperforming prior offline MARL methods and demonstrating robust\ngeneralisation across domains with many agents and long horizons. Finally, we\nintroduce new datasets to push the limits of many-agent coordination in offline\nMARL, and demonstrate Oryx's superior ability to scale effectively in such\nsettings. We will make all of our datasets, experimental data, and code\navailable upon publication.", "AI": {"tldr": "The paper proposes Oryx, a novel algorithm for offline cooperative MARL that adapts the Sable architecture and combines it with a sequential form of ICQ to solve complex coordination challenges. It achieves state-of-the-art performance on most tested datasets and demonstrates robust generalisation across domains.", "motivation": "A key challenge in offline multi-agent reinforcement learning is achieving effective many-agent multi-step coordination in complex environments.", "method": "Oryx adapts the retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ) to develop a novel offline auto-regressive policy update scheme.", "result": "Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons.", "conclusion": "The authors introduce new datasets to push the limits of many-agent coordination in offline MARL and demonstrate Oryx's superior ability to scale effectively in such settings. They will make all of their datasets, experimental data, and code available upon publication."}}
{"id": "2505.22152", "pdf": "https://arxiv.org/pdf/2505.22152", "abs": "https://arxiv.org/abs/2505.22152", "authors": ["Dominik Fuchsgruber", "Tom Wollschl\u00e4ger", "Johannes Bordne", "Stephan G\u00fcnnemann"], "title": "Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "While uncertainty estimation for graphs recently gained traction, most\nmethods rely on homophily and deteriorate in heterophilic settings. We address\nthis by analyzing message passing neural networks from an information-theoretic\nperspective and developing a suitable analog to data processing inequality to\nquantify information throughout the model's layers. In contrast to non-graph\ndomains, information about the node-level prediction target can increase with\nmodel depth if a node's features are semantically different from its neighbors.\nTherefore, on heterophilic graphs, the latent embeddings of an MPNN each\nprovide different information about the data distribution - different from\nhomophilic settings. This reveals that considering all node representations\nsimultaneously is a key design principle for epistemic uncertainty estimation\non graphs beyond homophily. We empirically confirm this with a simple post-hoc\ndensity estimator on the joint node embedding space that provides\nstate-of-the-art uncertainty on heterophilic graphs. At the same time, it\nmatches prior work on homophilic graphs without explicitly exploiting homophily\nthrough post-processing.", "AI": {"tldr": "The paper explores uncertainty estimation for graphs, especially in heterophilic settings, and proposes a new method that provides state-of-the-art uncertainty on heterophilic graphs.", "motivation": "Most methods for uncertainty estimation on graphs rely on homophily and perform poorly in heterophilic settings. The authors aim to address this issue by analyzing message passing neural networks from an information-theoretic perspective.", "method": "The authors develop an analog to the data processing inequality to quantify information throughout the layers of message passing neural networks (MPNNs). They consider the latent embeddings of MPNNs which provide different information about the data distribution in heterophilic graphs.", "result": "Empirical results show that considering all node representations simultaneously is crucial for epistemic uncertainty estimation on graphs beyond homophily. A simple post-hoc density estimator on the joint node embedding space achieves state-of-the-art uncertainty on heterophilic graphs while matching prior work on homophilic graphs without explicit exploitation of homophily.", "conclusion": "The study highlights the importance of considering all node representations simultaneously for effective uncertainty estimation on both heterophilic and homophilic graphs."}}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608", "abs": "https://arxiv.org/abs/2505.21608", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench.", "AI": {"tldr": "This paper presents MisBench, a large benchmark for evaluating LLMs' behavior towards misinformation. It contains over 10 million pieces of misinformation and reveals LLMs' vulnerabilities to knowledge conflicts and stylistic variations. A new approach called Reconstruct to Discriminate (RtD) is proposed to improve LLMs' ability to detect misinformation.", "motivation": "To address the lack of fine-grained analysis on how misinformation influences LLMs, providing a comprehensive benchmark to evaluate their behavior and knowledge preference towards misinformation.", "method": "Developed MisBench, a benchmark with 10,346,712 pieces of misinformation considering both knowledge-based conflicts and stylistic variations. Proposed the RtD approach to strengthen LLMs' misinformation detection abilities.", "result": "Empirical results show that LLMs are still susceptible to knowledge conflicts and stylistic variations in misinformation despite having comparable abilities in discerning misinformation.", "conclusion": "MisBench provides valuable insights into LLMs' interactions with misinformation and can serve as an effective benchmark for evaluating and enhancing LLM-based detectors."}}
{"id": "2505.22158", "pdf": "https://arxiv.org/pdf/2505.22158", "abs": "https://arxiv.org/abs/2505.22158", "authors": ["Rustem Takhanov"], "title": "The informativeness of the gradient revisited", "categories": ["cs.LG"], "comment": null, "summary": "In the past decade gradient-based deep learning has revolutionized several\napplications. However, this rapid advancement has highlighted the need for a\ndeeper theoretical understanding of its limitations. Research has shown that,\nin many practical learning tasks, the information contained in the gradient is\nso minimal that gradient-based methods require an exceedingly large number of\niterations to achieve success. The informativeness of the gradient is typically\nmeasured by its variance with respect to the random selection of a target\nfunction from a hypothesis class.\n  We use this framework and give a general bound on the variance in terms of a\nparameter related to the pairwise independence of the target function class and\nthe collision entropy of the input distribution. Our bound scales as $\n\\tilde{\\mathcal{O}}(\\varepsilon+e^{-\\frac{1}{2}\\mathcal{E}_c}) $, where $\n\\tilde{\\mathcal{O}} $ hides factors related to the regularity of the learning\nmodel and the loss function, $ \\varepsilon $ measures the pairwise independence\nof the target function class and $\\mathcal{E}_c$ is the collision entropy of\nthe input distribution.\n  To demonstrate the practical utility of our bound, we apply it to the class\nof Learning with Errors (LWE) mappings and high-frequency functions. In\naddition to the theoretical analysis, we present experiments to understand\nbetter the nature of recent deep learning-based attacks on LWE.", "AI": {"tldr": "Gradient-based deep learning has limitations, especially when the information contained in the gradient is minimal. This paper provides a general bound on the variance of the gradient informativeness in terms of pairwise independence and collision entropy. The bound is demonstrated on Learning with Errors (LWE) mappings and high-frequency functions.", "motivation": "The motivation of this paper is to address the theoretical understanding of the limitations of gradient-based deep learning methods. Specifically, it focuses on scenarios where the information contained in the gradient is minimal, requiring a large number of iterations for success.", "method": "The authors derive a general bound on the variance of the gradient informativeness using parameters related to pairwise independence of the target function class and the collision entropy of the input distribution. The bound scales as $ \\tilde{\\mathcal{O}}(\\varepsilon+e^{-\\frac{1}{2}\\mathcal{E}_c}) $, where $ \\varepsilon $ measures pairwise independence and $ \\mathcal{E}_c $ is the collision entropy.", "result": "The derived bound is applied to Learning with Errors (LWE) mappings and high-frequency functions, demonstrating its practical utility. Experiments are also conducted to understand recent deep learning-based attacks on LWE.", "conclusion": "This study contributes to the theoretical understanding of the limitations of gradient-based methods by providing a framework to measure gradient informativeness through variance bounds. It highlights the significance of pairwise independence and collision entropy."}}
{"id": "2505.22196", "pdf": "https://arxiv.org/pdf/2505.22196", "abs": "https://arxiv.org/abs/2505.22196", "authors": ["Jingyi Cui", "Hongwei Wen", "Yisen Wang"], "title": "An Augmentation-Aware Theory for Self-Supervised Contrastive Learning", "categories": ["cs.LG"], "comment": "Accepted to ICML2025", "summary": "Self-supervised contrastive learning has emerged as a powerful tool in\nmachine learning and computer vision to learn meaningful representations from\nunlabeled data. Meanwhile, its empirical success has encouraged many\ntheoretical studies to reveal the learning mechanisms. However, in the existing\ntheoretical research, the role of data augmentation is still under-exploited,\nespecially the effects of specific augmentation types. To fill in the blank, we\nfor the first time propose an augmentation-aware error bound for\nself-supervised contrastive learning, showing that the supervised risk is\nbounded not only by the unsupervised risk, but also explicitly by a trade-off\ninduced by data augmentation. Then, under a novel semantic label assumption, we\ndiscuss how certain augmentation methods affect the error bound. Lastly, we\nconduct both pixel- and representation-level experiments to verify our proposed\ntheoretical results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u589e\u5f3a\u611f\u77e5\u8bef\u5dee\u754c\uff0c\u63ed\u793a\u4e86\u6570\u636e\u589e\u5f3a\u5728\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u7814\u7a76\u4e2d\u5bf9\u6570\u636e\u589e\u5f3a\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u7279\u5b9a\u589e\u5f3a\u7c7b\u578b\u7684\u5f71\u54cd\u4ecd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u6570\u636e\u589e\u5f3a\u5f71\u54cd\u7684\u8bef\u5dee\u754c\uff0c\u5e76\u5728\u65b0\u7684\u8bed\u4e49\u6807\u7b7e\u5047\u8bbe\u4e0b\u63a2\u8ba8\u4e86\u7279\u5b9a\u589e\u5f3a\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u8be5\u8bef\u5dee\u754c\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u50cf\u7d20\u53ca\u8868\u793a\u7ea7\u522b\u7684\u5b9e\u9a8c\u90fd\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u8bef\u5dee\u754c\u7684\u5408\u7406\u6027\u3002", "conclusion": "\u6570\u636e\u589e\u5f3a\u4e0d\u4ec5\u7531\u65e0\u76d1\u7763\u98ce\u9669\u51b3\u5b9a\uff0c\u8fd8\u53d7\u5230\u6570\u636e\u589e\u5f3a\u5f15\u8d77\u7684\u4e00\u79cd\u6743\u8861\u7684\u5f71\u54cd\u3002"}}
{"id": "2505.22199", "pdf": "https://arxiv.org/pdf/2505.22199", "abs": "https://arxiv.org/abs/2505.22199", "authors": ["Xinyue Hu", "Zhibin Duan", "Bo Chen", "Mingyuan Zhou"], "title": "Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)", "summary": "Although deep neural networks have demonstrated significant success due to\ntheir powerful expressiveness, most models struggle to meet practical\nrequirements for uncertainty estimation. Concurrently, the entangled nature of\ndeep neural networks leads to a multifaceted problem, where various localized\nexplanation techniques reveal that multiple unrelated features influence the\ndecisions, thereby undermining interpretability. To address these challenges,\nwe develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates\ndeep neural networks as a conditional Bayesian non-negative factor analysis. By\nleveraging stochastic latent variables, the BNDL can model complex dependencies\nand provide robust uncertainty estimation. Moreover, the sparsity and\nnon-negativity of the latent variables encourage the model to learn\ndisentangled representations and decision layers, thereby improving\ninterpretability. We also offer theoretical guarantees that BNDL can achieve\neffective disentangled learning. In addition, we developed a corresponding\nvariational inference method utilizing a Weibull variational inference network\nto approximate the posterior distribution of the latent variables. Our\nexperimental results demonstrate that with enhanced disentanglement\ncapabilities, BNDL not only improves the model's accuracy but also provides\nreliable uncertainty estimation and improved interpretability.", "AI": {"tldr": "This paper introduces a Bayesian Non-negative Decision Layer (BNDL) for deep neural networks, providing robust uncertainty estimation and improved interpretability through disentangled representations.", "motivation": "Deep neural networks have powerful expressiveness but lack in uncertainty estimation and interpretability due to entangled features influencing decisions.", "method": "Reformulate deep neural networks as conditional Bayesian non-negative factor analysis using BNDL, incorporating stochastic latent variables for complex dependency modeling and utilizing Weibull variational inference network for posterior approximation.", "result": "Experimental results show that BNDL improves model accuracy, provides reliable uncertainty estimation, and enhances interpretability through effective disentangled learning.", "conclusion": "BNDL addresses the challenges of uncertainty estimation and interpretability in deep neural networks by promoting disentangled representations."}}
{"id": "2505.21627", "pdf": "https://arxiv.org/pdf/2505.21627", "abs": "https://arxiv.org/abs/2505.21627", "authors": ["Ander Artola Velasco", "Stratis Tsirtsis", "Nastaran Okati", "Manuel Gomez-Rodriguez"], "title": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives", "categories": ["cs.GT", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "State-of-the-art large language models require specialized hardware and\nsubstantial energy to operate. As a consequence, cloud-based services that\nprovide access to large language models have become very popular. In these\nservices, the price users pay for an output provided by a model depends on the\nnumber of tokens the model uses to generate it -- they pay a fixed price per\ntoken. In this work, we show that this pricing mechanism creates a financial\nincentive for providers to strategize and misreport the (number of) tokens a\nmodel used to generate an output, and users cannot prove, or even know, whether\na provider is overcharging them. However, we also show that, if an unfaithful\nprovider is obliged to be transparent about the generative process used by the\nmodel, misreporting optimally without raising suspicion is hard. Nevertheless,\nas a proof-of-concept, we introduce an efficient heuristic algorithm that\nallows providers to significantly overcharge users without raising suspicion,\nhighlighting the vulnerability of users under the current pay-per-token pricing\nmechanism. Further, to completely eliminate the financial incentive to\nstrategize, we introduce a simple incentive-compatible token pricing mechanism.\nUnder this mechanism, the price users pay for an output provided by a model\ndepends on the number of characters of the output -- they pay a fixed price per\ncharacter. Along the way, to illustrate and complement our theoretical results,\nwe conduct experiments with several large language models from the\n$\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$ families, and input\nprompts from the LMSYS Chatbot Arena platform.", "AI": {"tldr": "Advanced large language models need special hardware and energy. Cloud services providing these models are popular, where users pay per token used to generate output. This pricing can lead providers to misreport token usage for profit, as users cannot verify charges. We introduce an algorithm showing how providers could overcharge without detection, revealing vulnerabilities in the current system. To address this, we propose a new pricing mechanism based on output characters rather than tokens, eliminating the incentive to misreport. Experiments with various models support our findings.", "motivation": "The motivation is to address the financial incentives for providers of cloud-based large language model services to misreport token usage under the current pay-per-token pricing mechanism, which can lead to users being unknowingly overcharged.", "method": "We demonstrate the vulnerability by introducing an efficient heuristic algorithm that allows providers to overcharge users without raising suspicion. Additionally, we propose a new incentive-compatible token pricing mechanism where the cost is based on the number of characters in the output instead of tokens.", "result": "Our experiments with models from the Llama, Gemma, and Ministral families show that the heuristic algorithm can significantly overcharge users without detection, highlighting the vulnerability. The proposed character-based pricing mechanism successfully removes the financial incentive to misreport token usage.", "conclusion": "The current pay-per-token pricing mechanism creates vulnerabilities that allow providers to overcharge users. A character-based pricing mechanism eliminates this issue, ensuring transparency and fairness in pricing."}}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.", "AI": {"tldr": "In this paper, researchers analyze rule-based and model-based verifiers in reinforcement learning with verifiable reward (RLVR) using mathematical reasoning as a case study. They discover significant limitations in both types of verifiers: rule-based verifiers suffer from false negatives due to inability to recognize equivalent answers in different formats, while model-based verifiers are vulnerable to hacking leading to false positives. Both issues negatively affect RL training performance. The findings highlight the need for more robust reward systems.", "motivation": "The motivation is to understand the reliability of verifiers used in reinforcement learning with verifiable reward (RLVR), specifically focusing on their impact during the RL training process in complex domains such as mathematical reasoning.", "method": "The researchers conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. They first evaluate open-source rule-based verifiers across multiple mathematical datasets and then investigate model-based verifiers as an alternative solution, analyzing their performance and vulnerabilities.", "result": "Rule-based verifiers often fail to recognize equivalent answers presented in different formats, causing non-negligible false negative rates that adversely affect RL training performance. Model-based verifiers achieve higher verification accuracy in static evaluations but are highly susceptible to hacking, leading to false positives and artificially inflated rewards during policy optimization.", "conclusion": "Both rule-based and model-based verifiers have unique risks and limitations. The paper concludes by emphasizing the importance of developing more robust reward systems in reinforcement learning."}}
{"id": "2505.22208", "pdf": "https://arxiv.org/pdf/2505.22208", "abs": "https://arxiv.org/abs/2505.22208", "authors": ["Yosuke Oyama", "Yusuke Majima", "Eiji Ohta", "Yasufumi Sakai"], "title": "LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models", "categories": ["cs.LG"], "comment": "24 pages, 9 figures", "summary": "Neural network potentials (NNPs) are crucial for accelerating computational\nmaterials science by surrogating density functional theory (DFT) calculations.\nImproving their accuracy is possible through pre-training and fine-tuning,\nwhere an NNP model is first pre-trained on a large-scale dataset and then\nfine-tuned on a smaller target dataset. However, this approach is\ncomputationally expensive, mainly due to the cost of DFT-based dataset labeling\nand load imbalances during large-scale pre-training. To address this, we\npropose LaMM, a semi-supervised pre-training method incorporating improved\ndenoising self-supervised learning and a load-balancing algorithm for efficient\nmulti-node training. We demonstrate that our approach effectively leverages a\nlarge-scale dataset of $\\sim$300 million semi-labeled samples to train a single\nNNP model, resulting in improved fine-tuning performance in terms of both speed\nand accuracy.", "AI": {"tldr": "The paper presents LaMM, a semi-supervised pre-training method for neural network potentials (NNPs) that incorporates denoising self-supervised learning and a load-balancing algorithm to efficiently use large datasets (~300 million samples), enhancing the speed and accuracy of fine-tuning.", "motivation": "Neural network potentials (NNPs) are important for computational materials science but their improvement through pre-training and fine-tuning is computationally expensive due to DFT-based dataset labeling and load imbalances during large-scale pre-training.", "method": "LaMM is proposed which includes improved denoising self-supervised learning and a load-balancing algorithm for efficient multi-node training. It leverages a large-scale dataset of ~300 million semi-labeled samples.", "result": "LaMM effectively trains a single NNP model using the large-scale dataset, leading to better performance in terms of speed and accuracy when fine-tuning.", "conclusion": "LaMM offers an effective way to enhance the efficiency and accuracy of NNP models through semi-supervised pre-training."}}
{"id": "2505.22224", "pdf": "https://arxiv.org/pdf/2505.22224", "abs": "https://arxiv.org/abs/2505.22224", "authors": ["Senne Berden", "Ali \u0130rfan Mahmuto\u011fullar\u0131", "Dimos Tsouros", "Tias Guns"], "title": "Solver-Free Decision-Focused Learning for Linear Optimization Problems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mathematical optimization is a fundamental tool for decision-making in a wide\nrange of applications. However, in many real-world scenarios, the parameters of\nthe optimization problem are not known a priori and must be predicted from\ncontextual features. This gives rise to predict-then-optimize problems, where a\nmachine learning model predicts problem parameters that are then used to make\ndecisions via optimization. A growing body of work on decision-focused learning\n(DFL) addresses this setting by training models specifically to produce\npredictions that maximize downstream decision quality, rather than accuracy.\nWhile effective, DFL is computationally expensive, because it requires solving\nthe optimization problem with the predicted parameters at each loss evaluation.\nIn this work, we address this computational bottleneck for linear optimization\nproblems, a common class of problems in both DFL literature and real-world\napplications. We propose a solver-free training method that exploits the\ngeometric structure of linear optimization to enable efficient training with\nminimal degradation in solution quality. Our method is based on the insight\nthat a solution is optimal if and only if it achieves an objective value that\nis at least as good as that of its adjacent vertices on the feasible polytope.\nBuilding on this, our method compares the estimated quality of the ground-truth\noptimal solution with that of its precomputed adjacent vertices, and uses this\nas loss function. Experiments demonstrate that our method significantly reduces\ncomputational cost while maintaining high decision quality.", "AI": {"tldr": "In predict-then-optimize problems, decision-focused learning (DFL) trains models to produce predictions that maximize downstream decision quality. However, DFL is computationally expensive due to repeated optimization problem solving. This paper proposes a solver-free training method for linear optimization problems based on geometric structure, which reduces computational cost while maintaining high decision quality.", "motivation": "The motivation of this paper is to address the computational bottleneck in decision-focused learning (DFL) for linear optimization problems, where the optimization problem with predicted parameters needs to be solved repeatedly during training.", "method": "The proposed method is a solver-free training approach that exploits the geometric structure of linear optimization. It compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices on the feasible polytope, using this comparison as the loss function instead of solving the optimization problem directly.", "result": "Experiments show that the proposed method significantly reduces computational cost while maintaining high decision quality in predict-then-optimize problems.", "conclusion": "The paper concludes that the proposed solver-free training method efficiently addresses the computational bottleneck in decision-focused learning for linear optimization problems without sacrificing much in terms of solution quality."}}
{"id": "2505.21652", "pdf": "https://arxiv.org/pdf/2505.21652", "abs": "https://arxiv.org/abs/2505.21652", "authors": ["Yifan Yin", "Zhengtao Han", "Shivam Aarya", "Jianxin Wang", "Shuhang Xu", "Jiawei Peng", "Angtian Wang", "Alan Yuille", "Tianmin Shu"], "title": "PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Fine-grained robot manipulation, such as lifting and rotating a bottle to\ndisplay the label on the cap, requires robust reasoning about object parts and\ntheir relationships with intended tasks. Despite recent advances in training\ngeneral-purpose robot manipulation policies guided by language instructions,\nthere is a notable lack of large-scale datasets for fine-grained manipulation\ntasks with part-level instructions and diverse 3D object instances annotated\nwith part-level labels. In this work, we introduce PartInstruct, the first\nlarge-scale benchmark for training and evaluating fine-grained robot\nmanipulation models using part-level instructions. PartInstruct comprises 513\nobject instances across 14 categories, each annotated with part-level\ninformation, and 1302 fine-grained manipulation tasks organized into 16 task\nclasses. Our training set consists of over 10,000 expert demonstrations\nsynthesized in a 3D simulator, where each demonstration is paired with a\nhigh-level task instruction, a chain of base part-based skill instructions, and\nground-truth 3D information about the object and its parts. Additionally, we\ndesigned a comprehensive test suite to evaluate the generalizability of learned\npolicies across new states, objects, and tasks. We evaluated several\nstate-of-the-art robot manipulation approaches, including end-to-end\nvision-language policy learning and bi-level planning models for robot\nmanipulation on our benchmark. The experimental results reveal that current\nmodels struggle to robustly ground part concepts and predict actions in 3D\nspace, and face challenges when manipulating object parts in long-horizon\ntasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86PartInstruct\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u4f7f\u7528\u90e8\u4ef6\u7ea7\u6307\u4ee4\u7684\u7cbe\u7ec6\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u3002\u5b83\u5305\u62ec513\u4e2a\u5bf9\u8c61\u5b9e\u4f8b\u300114\u4e2a\u7c7b\u522b\u4ee5\u53ca1302\u4e2a\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u3002\u901a\u8fc7\u8d85\u8fc710,000\u4e2a\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5168\u9762\u7684\u6d4b\u8bd5\u5957\u4ef6\u6765\u8bc4\u4f30\u5b66\u4e60\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u5bf9\u90e8\u4ef6\u6982\u5ff5\u7684\u7a33\u5065\u63a5\u5730\u548c3D\u52a8\u4f5c\u9884\u6d4b\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u5e74\u6765\u5728\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u7684\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u8bad\u7ec3\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u5177\u6709\u90e8\u4ef6\u7ea7\u6307\u4ee4\u548c\u591a\u6837\u53163D\u5bf9\u8c61\u5b9e\u4f8b\u7684\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u5f15\u5165\u4e86PartInstruct\u6570\u636e\u96c6\uff0c\u5305\u542b513\u4e2a\u5bf9\u8c61\u5b9e\u4f8b\u548c1302\u4e2a\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u3002\u5229\u75283D\u6a21\u62df\u5668\u751f\u6210\u8d85\u8fc710,000\u4e2a\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7efc\u5408\u6d4b\u8bd5\u5957\u4ef6\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u5728\u7a33\u5065\u63a5\u5730\u90e8\u4ef6\u6982\u5ff5\u3001\u9884\u6d4b3D\u7a7a\u95f4\u4e2d\u7684\u52a8\u4f5c\u4ee5\u53ca\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u64cd\u7eb5\u5bf9\u8c61\u90e8\u4ef6\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u5904\u7406\u90e8\u4ef6\u7ea7\u6982\u5ff5\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u76843D\u52a8\u4f5c\u9884\u6d4b\u3002"}}
{"id": "2505.22235", "pdf": "https://arxiv.org/pdf/2505.22235", "abs": "https://arxiv.org/abs/2505.22235", "authors": ["Amon Lahr", "Johannes K\u00f6hler", "Anna Scampicchio", "Melanie N. Zeilinger"], "title": "Optimal kernel regression bounds under energy-bounded noise", "categories": ["cs.LG"], "comment": null, "summary": "Non-conservative uncertainty bounds are key for both assessing an estimation\nalgorithm's accuracy and in view of downstream tasks, such as its deployment in\nsafety-critical contexts. In this paper, we derive a tight, non-asymptotic\nuncertainty bound for kernel-based estimation, which can also handle correlated\nnoise sequences. Its computation relies on a mild norm-boundedness assumption\non the unknown function and the noise, returning the worst-case function\nrealization within the hypothesis class at an arbitrary query input location.\nThe value of this function is shown to be given in terms of the posterior mean\nand covariance of a Gaussian process for an optimal choice of the measurement\nnoise covariance. By rigorously analyzing the proposed approach and comparing\nit with other results in the literature, we show its effectiveness in returning\ntight and easy-to-compute bounds for kernel-based estimates.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a8\u5bfc\u4e86\u4e00\u4e2a\u7528\u4e8e\u6838\u4f30\u8ba1\u7684\u7d27\u5bc6\u3001\u975e\u6e10\u8fd1\u4e0d\u786e\u5b9a\u6027\u754c\u9650\uff0c\u80fd\u591f\u5904\u7406\u76f8\u5173\u566a\u58f0\u5e8f\u5217\uff0c\u5e76\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u540e\u9a8c\u5747\u503c\u548c\u534f\u65b9\u5dee\u7ed9\u51fa\u51fd\u6570\u503c\u3002\u901a\u8fc7\u4e0e\u6587\u732e\u4e2d\u5176\u4ed6\u7ed3\u679c\u7684\u4e25\u683c\u5206\u6790\u548c\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8fd4\u56de\u7d27\u5bc6\u4e14\u6613\u4e8e\u8ba1\u7b97\u7684\u6838\u4f30\u8ba1\u8fb9\u754c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8bc4\u4f30\u4f30\u8ba1\u7b97\u6cd5\u7684\u51c6\u786e\u6027\u4ee5\u53ca\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff08\u5982\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u90e8\u7f72\uff09\u9700\u8981\u975e\u4fdd\u5b88\u7684\u4e0d\u786e\u5b9a\u6027\u754c\u9650\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7d27\u5bc6\u7684\u754c\u9650\u6216\u4e0d\u80fd\u5904\u7406\u76f8\u5173\u566a\u58f0\u5e8f\u5217\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u6e29\u548c\u7684\u8303\u6570\u6709\u754c\u5047\u8bbe\u6765\u8ba1\u7b97\u672a\u77e5\u51fd\u6570\u548c\u566a\u58f0\uff0c\u8fd4\u56de\u5047\u8bbe\u7c7b\u4e2d\u6700\u574f\u60c5\u51b5\u7684\u51fd\u6570\u5b9e\u73b0\u3002\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u7684\u540e\u9a8c\u5747\u503c\u548c\u534f\u65b9\u5dee\uff0c\u5728\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u7684\u6700\u4f73\u9009\u62e9\u4e0b\u7ed9\u51fa\u51fd\u6570\u503c\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8fd4\u56de\u7d27\u5bc6\u4e14\u6613\u4e8e\u8ba1\u7b97\u7684\u6838\u4f30\u8ba1\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u4e0e\u73b0\u6709\u6587\u732e\u7ed3\u679c\u7684\u6bd4\u8f83\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u754c\u9650\u65b9\u6cd5\u4e3a\u6838\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7d27\u5bc6\u3001\u975e\u6e10\u8fd1\u7684\u7ed3\u679c\uff0c\u53ef\u4ee5\u6709\u6548\u5904\u7406\u76f8\u5173\u566a\u58f0\u5e8f\u5217\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657", "abs": "https://arxiv.org/abs/2505.21657", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.", "AI": {"tldr": "SMILE is a new method that explains how large language models respond to different parts of a prompt by slightly changing the input, measuring output changes, and highlighting impactful words, bringing us closer to transparent and trustworthy AI.", "motivation": "Large language models are powerful but lack transparency, making it hard to understand their decision-making process which can be problematic in fields requiring trust and accountability.", "method": "SMILE works by perturbing the input, measuring the resulting output changes, and creating visual heat maps to highlight the most impactful words in a prompt.", "result": "SMILE was tested on several leading LLMs and evaluated using metrics like accuracy, consistency, stability, and fidelity, showing it provides clear and reliable explanations.", "conclusion": "By making large language models easier to understand, SMILE contributes towards more transparent and trustworthy AI."}}
{"id": "2505.22252", "pdf": "https://arxiv.org/pdf/2505.22252", "abs": "https://arxiv.org/abs/2505.22252", "authors": ["Magdalena Proszewska", "Tomasz Danel", "Dawid Rymarczyk"], "title": "B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data", "categories": ["cs.LG", "cs.CE"], "comment": "26 pages, 16 figures, 5 tables", "summary": "Understanding the reasoning behind deep learning model predictions is crucial\nin cheminformatics and drug discovery, where molecular design determines their\nproperties. However, current evaluation frameworks for Explainable AI (XAI) in\nthis domain often rely on artificial datasets or simplified tasks, employing\ndata-derived metrics that fail to capture the complexity of real-world\nscenarios and lack a direct link to explanation faithfulness. To address this,\nwe introduce B-XAIC, a novel benchmark constructed from real-world molecular\ndata and diverse tasks with known ground-truth rationales for assigned labels.\nThrough a comprehensive evaluation using B-XAIC, we reveal limitations of\nexisting XAI methods for Graph Neural Networks (GNNs) in the molecular domain.\nThis benchmark provides a valuable resource for gaining deeper insights into\nthe faithfulness of XAI, facilitating the development of more reliable and\ninterpretable models.", "AI": {"tldr": "In this paper, researchers developed a benchmark called B-XAIC from real-world molecular data to evaluate existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain, revealing their limitations and providing a resource for improving model faithfulness and interpretability.", "motivation": "Current evaluation frameworks for Explainable AI (XAI) in cheminformatics and drug discovery often rely on artificial datasets or simplified tasks. They employ data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness.", "method": "The researchers introduced B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through comprehensive evaluation using B-XAIC, they assessed existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.", "result": "The benchmark revealed limitations of existing XAI methods for GNNs in the molecular domain.", "conclusion": "B-XAIC provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models."}}
{"id": "2505.21664", "pdf": "https://arxiv.org/pdf/2505.21664", "abs": "https://arxiv.org/abs/2505.21664", "authors": ["Joe O'Brien", "Jeremy Dolan", "Jay Kim", "Jonah Dykhuizen", "Jeba Sania", "Sebastian Becker", "Jam Kraprayoon", "Cara Labrador"], "title": "Expert Survey: AI Reliability & Security Research Priorities", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Our survey of 53 specialists across 105 AI reliability and security research\nareas identifies the most promising research prospects to guide strategic AI\nR&D investment. As companies are seeking to develop AI systems with broadly\nhuman-level capabilities, research on reliability and security is urgently\nneeded to ensure AI's benefits can be safely and broadly realized and prevent\nsevere harms. This study is the first to quantify expert priorities across a\ncomprehensive taxonomy of AI safety and security research directions and to\nproduce a data-driven ranking of their potential impact. These rankings may\nsupport evidence-based decisions about how to effectively deploy resources\ntoward AI reliability and security research.", "AI": {"tldr": "A survey of 53 specialists across 105 AI reliability and security research areas identifies key research prospects for strategic AI R&D investment, providing a data-driven ranking to support resource deployment.", "motivation": "As companies aim to develop AI systems with human-level capabilities, there is an urgent need for research on reliability and security to safely realize AI's benefits and prevent harms.", "method": "Conducted a survey among 53 specialists covering 105 AI reliability and security research areas to quantify expert priorities and produce a data-driven ranking of potential impact.", "result": "The study produced the first comprehensive taxonomy and data-driven ranking of AI safety and security research directions based on expert priorities.", "conclusion": "These rankings can assist in making evidence-based decisions about deploying resources towards AI reliability and security research."}}
{"id": "2505.22254", "pdf": "https://arxiv.org/pdf/2505.22254", "abs": "https://arxiv.org/abs/2505.22254", "authors": ["Xiangxiang Dai", "Xiaowei Sun", "Jinhang Zuo", "Xutong Liu", "John C. S. Lui"], "title": "A Unified Online-Offline Framework for Co-Branding Campaign Recommendations", "categories": ["cs.LG"], "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining, 2025", "summary": "Co-branding has become a vital strategy for businesses aiming to expand\nmarket reach within recommendation systems. However, identifying effective\ncross-industry partnerships remains challenging due to resource imbalances,\nuncertain brand willingness, and ever-changing market conditions. In this\npaper, we provide the first systematic study of this problem and propose a\nunified online-offline framework to enable co-branding recommendations. Our\napproach begins by constructing a bipartite graph linking ``initiating'' and\n``target'' brands to quantify co-branding probabilities and assess market\nbenefits. During the online learning phase, we dynamically update the graph in\nresponse to market feedback, while striking a balance between exploring new\ncollaborations for long-term gains and exploiting established partnerships for\nimmediate benefits. To address the high initial co-branding costs, our\nframework mitigates redundant exploration, thereby enhancing short-term\nperformance while ensuring sustainable strategic growth. In the offline\noptimization phase, our framework consolidates the interests of multiple\nsub-brands under the same parent brand to maximize overall returns, avoid\nexcessive investment in single sub-brands, and reduce unnecessary costs\nassociated with over-prioritizing a single sub-brand. We present a theoretical\nanalysis of our approach, establishing a highly nontrivial sublinear regret\nbound for online learning in the complex co-branding problem, and enhancing the\napproximation guarantee for the NP-hard offline budget allocation optimization.\nExperiments on both synthetic and real-world co-branding datasets demonstrate\nthe practical effectiveness of our framework, with at least 12\\% improvement.", "AI": {"tldr": "Co-branding has become a vital strategy for businesses aiming to expand market reach within recommendation systems. This paper proposes a unified online-offline framework to enable co-branding recommendations, with experiments showing at least 12% improvement.", "motivation": "To address the challenges in identifying effective cross-industry partnerships for co-branding due to resource imbalances, uncertain brand willingness, and ever-changing market conditions.", "method": "Construct a bipartite graph linking ``initiating'' and ``target'' brands to quantify co-branding probabilities and assess market benefits. Dynamically update the graph during the online learning phase while balancing exploration and exploitation. In the offline optimization phase, consolidate the interests of multiple sub-brands under the same parent brand to maximize overall returns.", "result": "Theoretical analysis establishes a highly nontrivial sublinear regret bound for online learning and enhances the approximation guarantee for the NP-hard offline budget allocation optimization. Experiments on both synthetic and real-world datasets demonstrate practical effectiveness with at least 12\\% improvement.", "conclusion": "This paper provides the first systematic study of the co-branding problem and proposes a unified online-offline framework that mitigates redundant exploration, enhancing short-term performance while ensuring sustainable strategic growth."}}
{"id": "2505.22255", "pdf": "https://arxiv.org/pdf/2505.22255", "abs": "https://arxiv.org/abs/2505.22255", "authors": ["Vadim Kurochkin", "Yaroslav Aksenov", "Daniil Laptev", "Daniil Gavrilov", "Nikita Balagansky"], "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework.", "AI": {"tldr": "KronSAE is a new architecture that uses Kronecker product decomposition and mAND activation function to reduce memory and computation for training Sparse Autoencoders.", "motivation": "Training Sparse Autoencoders (SAEs) at scale is challenging due to computationally intensive linear operations, especially with large dictionary sizes.", "method": "The paper proposes KronSAE, which factorizes the latent representation via Kronecker product decomposition. It also introduces mAND, a differentiable activation function approximating binary AND operation.", "result": "KronSAE drastically reduces memory and computational overhead while improving interpretability and performance in the factorized framework.", "conclusion": "KronSAE and mAND offer an efficient solution for large-scale SAE training."}}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670", "abs": "https://arxiv.org/abs/2505.21670", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy.", "AI": {"tldr": "Investigating outliers in LLMs is crucial for quantization and compression. The paper explores formation mechanisms of two common types of outliers (massive activations and channel-wise) and proposes efficient strategies to mitigate them with minimal accuracy impact.", "motivation": "Outliers in large language models significantly affect performance, especially during quantization and compression processes. They cause considerable quantization errors that degrade model performance. Hence, identifying and addressing these outliers can enhance the efficiency and accuracy of quantization, making it more suitable for deployment on edge devices or specialized hardware.", "method": "The paper conducts a comprehensive investigation into the formation mechanisms of two common types of outliers in LLMs: massive activations and channel-wise outliers. It then proposes potential strategies to reduce their occurrence.", "result": "The proposed approaches successfully eliminate most massive activations and channel-wise outliers while having minimal impact on the model's accuracy.", "conclusion": "This study provides insights into the root causes of outliers in LLMs and demonstrates effective methods to mitigate their effects, improving quantization efficiency and model performance."}}
{"id": "2505.22257", "pdf": "https://arxiv.org/pdf/2505.22257", "abs": "https://arxiv.org/abs/2505.22257", "authors": ["Youssef Mroueh", "Nicolas Dupuis", "Brian Belgodere", "Apoorva Nitsure", "Mattia Rigotti", "Kristjan Greenewald", "Jiri Navratil", "Jerret Ross", "Jesus Rios"], "title": "Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit Group Relative Policy Optimization (GRPO) in both on-policy and\noff-policy optimization regimes. Our motivation comes from recent work on\noff-policy Proximal Policy Optimization (PPO), which improves training\nstability, sampling efficiency, and memory usage. In addition, a recent\nanalysis of GRPO suggests that estimating the advantage function with\noff-policy samples could be beneficial. Building on these observations, we\nadapt GRPO to the off-policy setting. We show that both on-policy and\noff-policy GRPO objectives yield an improvement in the reward. This result\nmotivates the use of clipped surrogate objectives in the off-policy version of\nGRPO. We then compare the empirical performance of reinforcement learning with\nverifiable rewards in post-training using both GRPO variants. Our results show\nthat off-policy GRPO either significantly outperforms or performs on par with\nits on-policy counterpart.", "AI": {"tldr": "This paper revisits Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes, adapts GRPO to the off-policy setting, compares the empirical performance of reinforcement learning with verifiable rewards using both GRPO variants, and finds that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart.", "motivation": "The motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial.", "method": "The authors adapt GRPO to the off-policy setting, showing that both on-policy and off-policy GRPO objectives yield an improvement in the reward. They motivate the use of clipped surrogate objectives in the off-policy version of GRPO.", "result": "The results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart when comparing the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants.", "conclusion": "Off-policy GRPO is either superior or equivalent to on-policy GRPO, suggesting that the adaptation of GRPO to the off-policy setting is successful and beneficial."}}
{"id": "2505.22275", "pdf": "https://arxiv.org/pdf/2505.22275", "abs": "https://arxiv.org/abs/2505.22275", "authors": ["Alexander Hagg", "Adam Gaier", "Dominik Wilde", "Alexander Asteroth", "Holger Foysi", "Dirk Reith"], "title": "Full Domain Analysis in Fluid Dynamics", "categories": ["cs.LG", "cs.NE", "68U01", "I.2.1; I.2.6"], "comment": null, "summary": "Novel techniques in evolutionary optimization, simulation and machine\nlearning allow for a broad analysis of domains like fluid dynamics, in which\ncomputation is expensive and flow behavior is complex. Under the term of full\ndomain analysis we understand the ability to efficiently determine the full\nspace of solutions in a problem domain, and analyze the behavior of those\nsolutions in an accessible and interactive manner. The goal of full domain\nanalysis is to deepen our understanding of domains by generating many examples\nof flow, their diversification, optimization and analysis. We define a formal\nmodel for full domain analysis, its current state of the art, and requirements\nof subcomponents. Finally, an example is given to show what we can learn by\nusing full domain analysis. Full domain analysis, rooted in optimization and\nmachine learning, can be a helpful tool in understanding complex systems in\ncomputational physics and beyond.", "AI": {"tldr": "\u65b0\u578b\u8fdb\u5316\u4f18\u5316\u3001\u4eff\u771f\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u80fd\u591f\u5e7f\u6cdb\u5206\u6790\u8bf8\u5982\u6d41\u4f53\u52a8\u529b\u5b66\u7b49\u9886\u57df\uff0c\u5176\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6d41\u4f53\u884c\u4e3a\u590d\u6742\u3002\u5168\u6587\u5206\u6790\u7684\u76ee\u6807\u662f\u901a\u8fc7\u751f\u6210\u5927\u91cf\u6d41\u4f53\u793a\u4f8b\u53ca\u5176\u591a\u6837\u5316\u3001\u4f18\u5316\u548c\u5206\u6790\u6765\u52a0\u6df1\u5bf9\u9886\u57df\u7684\u7406\u89e3\u3002\u6700\u540e\u7ed9\u51fa\u4e00\u4e2a\u4f8b\u5b50\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u5168\u6587\u5206\u6790\u3002\u5168\u6587\u5206\u6790\u53ef\u4ee5\u6210\u4e3a\u7406\u89e3\u8ba1\u7b97\u7269\u7406\u5b66\u53ca\u5176\u4ed6\u9886\u57df\u590d\u6742\u7cfb\u7edf\u7684\u91cd\u8981\u5de5\u5177\u3002", "motivation": "\u6d41\u4f53\u52a8\u529b\u5b66\u7b49\u9886\u57df\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6d41\u4f53\u884c\u4e3a\u590d\u6742\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u786e\u5b9a\u95ee\u9898\u57df\u4e2d\u5b8c\u6574\u89e3\u7a7a\u95f4\u5e76\u4ee5\u53ef\u8bbf\u95ee\u548c\u4ea4\u4e92\u65b9\u5f0f\u5206\u6790\u8fd9\u4e9b\u89e3\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5168\u6587\u5206\u6790\u7684\u6b63\u5f0f\u6a21\u578b\uff0c\u9610\u8ff0\u5176\u5f53\u524d\u6280\u672f\u6c34\u5e73\u548c\u5b50\u7ec4\u4ef6\u9700\u6c42\uff0c\u5e76\u7ed9\u51fa\u5e94\u7528\u5b9e\u4f8b\u3002", "result": "\u901a\u8fc7\u751f\u6210\u5927\u91cf\u6d41\u4f53\u793a\u4f8b\u53ca\u5176\u591a\u6837\u5316\u3001\u4f18\u5316\u548c\u5206\u6790\uff0c\u52a0\u6df1\u4e86\u5bf9\u9886\u57df\u7684\u7406\u89e3\u3002", "conclusion": "\u5168\u6587\u5206\u6790\u4f5c\u4e3a\u57fa\u4e8e\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u6210\u4e3a\u7406\u89e3\u8ba1\u7b97\u7269\u7406\u5b66\u53ca\u5176\u4ed6\u9886\u57df\u590d\u6742\u7cfb\u7edf\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2505.22306", "pdf": "https://arxiv.org/pdf/2505.22306", "abs": "https://arxiv.org/abs/2505.22306", "authors": ["Zehua Chen", "Yuyang Miao", "Liyuan Wang", "Luyun Fan", "Danilo P. Mandic", "Jun Zhu"], "title": "Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cardiovascular signals such as photoplethysmography (PPG),\nelectrocardiography (ECG), and blood pressure (BP) are inherently correlated\nand complementary, together reflecting the health of cardiovascular system.\nHowever, their joint utilization in real-time monitoring is severely limited by\ndiverse acquisition challenges from noisy wearable recordings to burdened\ninvasive procedures. Here we propose UniCardio, a multi-modal diffusion\ntransformer that reconstructs low-quality signals and synthesizes unrecorded\nsignals in a unified generative framework. Its key innovations include a\nspecialized model architecture to manage the signal modalities involved in\ngeneration tasks and a continual learning paradigm to incorporate varying\nmodality combinations. By exploiting the complementary nature of cardiovascular\nsignals, UniCardio clearly outperforms recent task-specific baselines in signal\ndenoising, imputation, and translation. The generated signals match the\nperformance of ground-truth signals in detecting abnormal health conditions and\nestimating vital signs, even in unseen domains, while ensuring interpretability\nfor human experts. These advantages position UniCardio as a promising avenue\nfor advancing AI-assisted healthcare.", "AI": {"tldr": "UniCardio is a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals, outperforming task-specific baselines in signal denoising, imputation, and translation.", "motivation": "To overcome the challenges of joint utilization of cardiovascular signals such as PPG, ECG, and BP in real-time monitoring due to noisy wearable recordings and invasive procedures.", "method": "Propose UniCardio, a multi-modal diffusion transformer with specialized model architecture and continual learning paradigm to manage signal modalities and incorporate varying modality combinations.", "result": "Outperforms recent task-specific baselines in signal denoising, imputation, and translation; generated signals match ground-truth signals in detecting abnormal health conditions and estimating vital signs, ensuring interpretability for human experts.", "conclusion": "UniCardio represents a promising avenue for advancing AI-assisted healthcare."}}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689", "abs": "https://arxiv.org/abs/2505.21689", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization.", "AI": {"tldr": "The paper introduces LLMPR, a framework that automates petition ranking in the legal system using transfer learning and machine learning techniques. It achieves high accuracy with Random Forest and Decision Tree models.", "motivation": "To address inefficiencies and delays in manual prioritization of legal petitions, especially within the Indian judiciary.", "method": "Utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency, leveraging the ILDC dataset and embedding techniques such as DistilBERT, LegalBERT, and MiniLM.", "result": "Random Forest and Decision Tree models yield superior performance with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Numerical features alone achieve nearly optimal ranking results (R2 = 0.988, \u03c1 = 0.998), while LLM-based embeddings offer only marginal gains.", "conclusion": "Automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization."}}
{"id": "2505.22308", "pdf": "https://arxiv.org/pdf/2505.22308", "abs": "https://arxiv.org/abs/2505.22308", "authors": ["Zachary Shinnick", "Liangze Jiang", "Hemanth Saratchandran", "Anton van den Hengel", "Damien Teney"], "title": "Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Pretraining on large, semantically rich datasets is key for developing\nlanguage models. Surprisingly, recent studies have shown that even synthetic\ndata, generated procedurally through simple semantic-free algorithms, can yield\nsome of the same benefits as natural language pretraining. It is unclear what\nspecific capabilities such simple synthetic data instils in a model, where\nthese capabilities reside in the architecture, and how they manifest within its\nweights. In this short paper, we identify several beneficial forms of\nprocedural data, together with specific algorithmic reasoning skills that\nimprove in small transformers. Our core finding is that different procedural\nrules instil distinct but complementary inductive structures in the model. With\nextensive ablations and partial-transfer experiments, we discover that these\nstructures reside in different parts of the model. Attention layers often carry\nthe most transferable information, but some pretraining rules impart useful\nstructure to MLP blocks instead. Most interestingly, the structures induced by\nmultiple rules can be composed to jointly reinforce multiple capabilities.\nThese results suggest an exciting possibility of disentangling the acquisition\nof knowledge from reasoning in language models, with the goal of improving\ntheir robustness and data efficiency.", "AI": {"tldr": "Pretraining on large datasets is important for language models, but synthetic data can also provide benefits. This paper identifies beneficial forms of procedural data and algorithmic reasoning skills that improve in small transformers.", "motivation": "To understand what specific capabilities simple synthetic data instils in a model, where these capabilities reside in the architecture, and how they manifest within its weights.", "method": "Identify several beneficial forms of procedural data and conduct extensive ablations and partial-transfer experiments to discover where the inductive structures reside in the model.", "result": "Different procedural rules instil distinct but complementary inductive structures in the model, residing in different parts such as attention layers and MLP blocks. Structures induced by multiple rules can be composed to jointly reinforce multiple capabilities.", "conclusion": "The results suggest the possibility of disentangling the acquisition of knowledge from reasoning in language models, aiming to improve their robustness and data efficiency."}}
{"id": "2505.21699", "pdf": "https://arxiv.org/pdf/2505.21699", "abs": "https://arxiv.org/abs/2505.21699", "authors": ["Zhengbo Zhou", "Dooman Arefan", "Margarita Zuley", "Jules Sumkin", "Shandong Wu"], "title": "STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Predicting the risk of developing breast cancer is an important clinical tool\nto guide early intervention and tailoring personalized screening strategies.\nEarly risk models have limited performance and recently machine learning-based\nanalysis of mammogram images showed encouraging risk prediction effects. These\nmodels however are limited to the use of a single exam or tend to overlook\nnuanced breast tissue evolvement in spatial and temporal details of\nlongitudinal imaging exams that are indicative of breast cancer risk. In this\npaper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk\nPrediction), a novel Transformer-based model that captures fine-grained\nmammographic imaging evolution simultaneously from bilateral and longitudinal\nasymmetries for breast cancer risk prediction. STA-Risk is innovative by the\nside encoding and temporal encoding to learn spatial-temporal asymmetries,\nregulated by a customized asymmetry loss. We performed extensive experiments\nwith two independent mammogram datasets and achieved superior performance than\nfour representative SOTA models for 1- to 5-year future risk prediction. Source\ncodes will be released upon publishing of the paper.", "AI": {"tldr": "This paper proposes STA-Risk, a Transformer-based model for breast cancer risk prediction using mammogram images, which captures spatial and temporal asymmetries with side and temporal encoding. It outperforms four SOTA models in experiments with two independent datasets.", "motivation": "Current breast cancer risk prediction models have limited performance when using single exams or overlook detailed longitudinal imaging evolvement.", "method": "STA-Risk is a novel Transformer-based model that uses side encoding and temporal encoding to learn spatial-temporal asymmetries from bilateral and longitudinal mammographic images, regulated by an asymmetry loss.", "result": "STA-Risk achieved superior performance compared to four representative SOTA models for 1- to 5-year future risk prediction in extensive experiments with two independent mammogram datasets.", "conclusion": "STA-Risk effectively captures fine-grained mammographic imaging evolution for breast cancer risk prediction and outperforms existing models."}}
{"id": "2505.22310", "pdf": "https://arxiv.org/pdf/2505.22310", "abs": "https://arxiv.org/abs/2505.22310", "authors": ["Shoaib Ahmed Siddiqui", "Adrian Weller", "David Krueger", "Gintare Karolina Dziugaite", "Michael Curtis Mozer", "Eleni Triantafillou"], "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent unlearning methods for LLMs are vulnerable to relearning attacks:\nknowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of\n(even seemingly-unrelated) examples. We study this phenomenon in a controlled\nsetting for example-level unlearning in vision classifiers. We make the\nsurprising discovery that forget-set accuracy can recover from around 50%\npost-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,\nzero examples of the forget set. We observe this effect across a wide variety\nof unlearning methods, whereas for a model retrained from scratch excluding the\nforget set (gold standard), the accuracy remains at 50%. We observe that\nresistance to relearning attacks can be predicted by weight-space properties,\nspecifically, $L_2$-distance and linear mode connectivity between the original\nand the unlearned model. Leveraging this insight, we propose a new class of\nmethods that achieve state-of-the-art resistance to relearning attacks.", "AI": {"tldr": "Recent unlearning methods for LLMs can be vulnerable to relearning attacks, where knowledge thought to be removed can re-emerge. This study explores this phenomenon in vision classifiers and proposes new methods that offer better resistance to such attacks.", "motivation": "The motivation of the paper is to investigate why current unlearning methods for large language models (LLMs) are susceptible to relearning attacks, where forgotten knowledge can reappear after fine-tuning on a small set of examples.", "method": "The authors conduct their study in a controlled setting focusing on example-level unlearning in vision classifiers. They analyze various unlearning methods and observe their behavior when fine-tuned only on the retain set. They also examine weight-space properties such as $L_2$-distance and linear mode connectivity between original and unlearned models to predict resistance to relearning attacks.", "result": "The study finds that forget-set accuracy can recover significantly after fine-tuning on just the retain set across different unlearning methods. However, for models retrained from scratch excluding the forget set, the accuracy remains at 50%. The authors identify specific weight-space properties that predict resistance to relearning attacks.", "conclusion": "Based on their findings, the authors propose a new class of methods that provide state-of-the-art resistance to relearning attacks, improving upon existing techniques."}}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.", "AI": {"tldr": "Skywork-OR1, based on DeepSeek-R1-Distill series, uses reinforcement learning to enhance reasoning capabilities of LLMs. It significantly improves accuracy across benchmarks for 32B and 7B models. Ablation studies validate training pipeline components, and entropy collapse is addressed. Model weights, code, and datasets are open-sourced.", "motivation": "To explore the potential of reinforcement learning in improving reasoning capabilities of large language models, especially for long Chain-of-Thought tasks.", "method": "Implemented Skywork-OR1 using the DeepSeek-R1-Distill model series with reinforcement learning techniques. Conducted ablation studies on training pipeline components and investigated entropy collapse phenomenon.", "result": "Achieved significant accuracy improvements on AIME24, AIME25, and LiveCodeBench benchmarks for both 32B and 7B models. Skywork-OR1-32B outperformed DeepSeek-R1 and Qwen3-32B on AIME24 and AIME25. Skywork-OR1-7B demonstrated competitive reasoning capabilities.", "conclusion": "Reinforcement learning effectively enhances reasoning capabilities of LLMs. Mitigating entropy collapse is crucial for performance improvement. Open-sourcing efforts will support further community research."}}
{"id": "2505.21715", "pdf": "https://arxiv.org/pdf/2505.21715", "abs": "https://arxiv.org/abs/2505.21715", "authors": ["Md. Zahid Hossain", "Mustofa Ahmed", "Most. Sharmin Sultana Samu", "Md. Rakibul Islam"], "title": "Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint, manuscript under-review", "summary": "The automated generation of radiology reports from chest X-ray images holds\nsignificant promise in enhancing diagnostic workflows while preserving patient\nprivacy. Traditional centralized approaches often require sensitive data\ntransfer, posing privacy concerns. To address this, the study proposes a\nMultimodal Federated Learning framework for chest X-ray report generation using\nthe IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the\nencoder and GPT-2 as the report generator, enabling decentralized training\nwithout sharing raw data. Three Federated Learning (FL) aggregation strategies:\nFedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)\nwere evaluated. Among these, Krum Aggregation demonstrated superior performance\nacross lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore\nand RaTEScore. The results show that FL can match or surpass centralized models\nin generating clinically relevant and semantically rich radiology reports. This\nlightweight and privacy-preserving framework paves the way for collaborative\nmedical AI development without compromising data confidentiality.", "AI": {"tldr": "The study proposes a Multimodal Federated Learning framework for generating chest X-ray reports using IU-Xray dataset, Vision Transformer and GPT-2. Among the three FL aggregation strategies evaluated, Krum Aggregation performed best in lexical and semantic metrics. The results indicate that FL can match or outperform centralized models while preserving privacy.", "motivation": "To overcome the privacy concerns of traditional centralized approaches which require sensitive data transfer, this paper aims to develop a framework for radiology report generation from chest X-ray images using federated learning.", "method": "A Multimodal Federated Learning framework was proposed where Vision Transformer (ViT) is used as encoder and GPT-2 as report generator. Three FL aggregation strategies were evaluated: FedAvg, Krum Aggregation and Loss-aware Federated Averaging (L-FedAvg).", "result": "Krum Aggregation showed superior performance across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore and RaTEScore. The results suggest that FL can match or surpass centralized models in generating clinically relevant reports.", "conclusion": "This lightweight and privacy-preserving framework allows collaborative medical AI development without compromising data confidentiality."}}
{"id": "2505.22316", "pdf": "https://arxiv.org/pdf/2505.22316", "abs": "https://arxiv.org/abs/2505.22316", "authors": ["Konrad \u00d6zdemir", "Lukas Kirchdorfer", "Keyvan Amiri Elyasi", "Han van der Aa", "Heiner Stuckenschmidt"], "title": "Rethinking BPS: A Utility-Based Evaluation Framework", "categories": ["cs.LG"], "comment": null, "summary": "Business process simulation (BPS) is a key tool for analyzing and optimizing\norganizational workflows, supporting decision-making by estimating the impact\nof process changes. The reliability of such estimates depends on the ability of\na BPS model to accurately mimic the process under analysis, making rigorous\naccuracy evaluation essential. However, the state-of-the-art approach to\nevaluating BPS models has two key limitations. First, it treats simulation as a\nforecasting problem, testing whether models can predict unseen future events.\nThis fails to assess how well a model captures the as-is process, particularly\nwhen process behavior changes from train to test period. Thus, it becomes\ndifficult to determine whether poor results stem from an inaccurate model or\nthe inherent complexity of the data, such as unpredictable drift. Second, the\nevaluation approach strongly relies on Earth Mover's Distance-based metrics,\nwhich can obscure temporal patterns and thus yield misleading conclusions about\nsimulation quality. To address these issues, we propose a novel framework that\nevaluates simulation quality based on its ability to generate representative\nprocess behavior. Instead of comparing simulated logs to future real-world\nexecutions, we evaluate whether predictive process monitoring models trained on\nsimulated data perform comparably to those trained on real data for downstream\nanalysis tasks. Empirical results show that our framework not only helps\nidentify sources of discrepancies but also distinguishes between model accuracy\nand data complexity, offering a more meaningful way to assess BPS quality.", "AI": {"tldr": "The paper proposes a new framework for evaluating Business Process Simulation (BPS) models by assessing their ability to generate representative process behavior, offering a more meaningful way to assess BPS quality.", "motivation": "Current methods for evaluating BPS models have limitations: they treat simulation as forecasting and rely on metrics that can obscure temporal patterns.", "method": "The novel framework evaluates simulation quality based on the performance of predictive process monitoring models trained on simulated data compared to those trained on real data.", "result": "Empirical results indicate that the framework helps identify discrepancies and distinguishes between model accuracy and data complexity.", "conclusion": "This approach provides a more meaningful assessment of BPS model quality."}}
{"id": "2505.22322", "pdf": "https://arxiv.org/pdf/2505.22322", "abs": "https://arxiv.org/abs/2505.22322", "authors": ["Zhengyu Fang", "Zhimeng Jiang", "Huiyuan Chen", "Xiaoge Zhang", "Kaiyu Tang", "Xiao Li", "Jing Li"], "title": "A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models have shown strong performance in generating high-quality\ntabular data, but they carry privacy risks by reproducing exact training\nsamples. While prior work focuses on dataset-level augmentation to reduce\nmemorization, little is known about which individual samples contribute most.\nWe present the first data-centric study of memorization dynamics in tabular\ndiffusion models. We quantify memorization for each real sample based on how\nmany generated samples are flagged as replicas, using a relative distance\nratio. Our empirical analysis reveals a heavy-tailed distribution of\nmemorization counts: a small subset of samples contributes disproportionately\nto leakage, confirmed via sample-removal experiments. To understand this, we\ndivide real samples into top- and non-top-memorized groups and analyze their\ntraining-time behaviors. We track when each sample is first memorized and\nmonitor per-epoch memorization intensity (AUC). Memorized samples are memorized\nslightly earlier and show stronger signals in early training. Based on these\ninsights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:\n(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and\n(c) retrain on the filtered dataset. Across multiple tabular datasets and\nmodels, DynamicCut reduces memorization with minimal impact on data diversity\nand downstream performance. It also complements augmentation-based defenses.\nFurthermore, DynamicCut enables cross-model transferability: high-ranked\nsamples identified from one model (e.g., a diffusion model) are also effective\nfor reducing memorization when removed from others, such as GANs and VAEs.", "AI": {"tldr": "Diffusion models can generate high-quality tabular data but risk reproducing training samples. This study explores memorization dynamics, identifying a small subset of samples causing disproportionate leakage. Based on this, they propose DynamicCut, a mitigation method that reduces memorization across various datasets and models without significantly impacting data diversity or performance.", "motivation": "To address the privacy risks associated with diffusion models reproducing exact training samples when generating high-quality tabular data. Previous work has focused on dataset-level augmentation, but there is limited understanding of which individual samples contribute most to memorization.", "method": "The authors quantify memorization for each real sample using a relative distance ratio and identify a heavy-tailed distribution of memorization counts. They divide samples into top- and non-top-memorized groups and analyze their training-time behaviors. Based on these insights, they propose DynamicCut, a two-stage model-agnostic mitigation method involving ranking samples by epoch-wise intensity, pruning a tunable top fraction, and retraining on the filtered dataset.", "result": "DynamicCut effectively reduces memorization across multiple tabular datasets and models while having minimal impact on data diversity and downstream performance. It complements augmentation-based defenses and enables cross-model transferability.", "conclusion": "DynamicCut offers an effective approach to mitigating memorization in tabular diffusion models, reducing privacy risks while preserving data quality and performance."}}
{"id": "2505.21720", "pdf": "https://arxiv.org/pdf/2505.21720", "abs": "https://arxiv.org/abs/2505.21720", "authors": ["Vanessa Utz"], "title": "Responsible Data Stewardship: Generative AI and the Digital Waste Problem", "categories": ["cs.CY", "cs.AI"], "comment": "8 pages, submitted to AAAI/ACM Conference on AI, Ethics and Society", "summary": "As generative AI systems become widely adopted, they enable unprecedented\ncreation levels of synthetic data across text, images, audio, and video\nmodalities. While research has addressed the energy consumption of model\ntraining and inference, a critical sustainability challenge remains\nunderstudied: digital waste. This term refers to stored data that consumes\nresources without serving a specific (and/or immediate) purpose. This paper\npresents this terminology in the AI context and introduces digital waste as an\nethical imperative within (generative) AI development, positioning\nenvironmental sustainability as core for responsible innovation. Drawing from\nestablished digital resource management approaches, we examine how other\ndisciplines manage digital waste and identify transferable approaches for the\nAI community. We propose specific recommendations encompassing re-search\ndirections, technical interventions, and cultural shifts to mitigate the\nenvironmental consequences of in-definite data storage. By expanding AI ethics\nbeyond immediate concerns like bias and privacy to include inter-generational\nenvironmental justice, this work contributes to a more comprehensive ethical\nframework that considers the complete lifecycle impact of generative AI\nsystems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e2d\u7684\u6570\u5b57\u5e9f\u5f03\u7269\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u4f26\u7406\u548c\u6280\u672f\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7b49\u591a\u6a21\u6001\u6570\u636e\u4e0a\u7684\u5408\u6210\u80fd\u529b\u4e0d\u65ad\u63d0\u5347\uff0c\u4f46\u4e0e\u6b64\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u80fd\u6e90\u6d88\u8017\u548c\u6570\u5b57\u5e9f\u5f03\u7269\u7b49\u53ef\u6301\u7eed\u6027\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5df2\u6709\u7684\u6570\u5b57\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u4ed6\u5b66\u79d1\u5982\u4f55\u5904\u7406\u6570\u5b57\u5e9f\u5f03\u7269\uff0c\u5e76\u63d0\u51fa\u9002\u7528\u4e8eAI\u793e\u533a\u7684\u53ef\u8f6c\u79fb\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u6db5\u76d6\u7814\u7a76\u65b9\u5411\u3001\u6280\u672f\u5e72\u9884\u548c\u6587\u5316\u8f6c\u53d8\u7684\u5177\u4f53\u5efa\u8bae\uff0c\u4ee5\u51cf\u8f7b\u65e0\u9650\u671f\u6570\u636e\u5b58\u50a8\u5e26\u6765\u7684\u73af\u5883\u5f71\u54cd\u3002", "conclusion": "\u5c06\u73af\u5883\u53ef\u6301\u7eed\u6027\u7eb3\u5165AI\u4f26\u7406\u6846\u67b6\uff0c\u63a8\u52a8\u8d1f\u8d23\u4efb\u7684\u521b\u65b0\uff0c\u8003\u8651\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5b8c\u6574\u751f\u547d\u5468\u671f\u5f71\u54cd\u3002"}}
{"id": "2505.22355", "pdf": "https://arxiv.org/pdf/2505.22355", "abs": "https://arxiv.org/abs/2505.22355", "authors": ["Yongkang Liu", "Xingle Xu", "Ercong Nie", "Zijing Wang", "Shi Feng", "Daling Wang", "Qian Li", "Hinrich Sch\u00fctze"], "title": "Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable\nto Full Fine-Tuning (FFT) while requiring significantly fewer computing\nresources, making it the go-to choice for researchers. We find that although\nPEFT can achieve competitive results on some benchmarks, its performance falls\nshort of FFT in complex tasks, such as reasoning and instruction-based\nfine-tuning. In this paper, we compare the characteristics of PEFT and FFT in\nterms of representational capacity and robustness based on optimization theory.\nWe theoretically demonstrate that PEFT is a strict subset of FFT. By providing\ntheoretical upper bounds for PEFT, we show that the limited parameter space\nconstrains the model's representational ability, making it more susceptible to\nperturbations. Experiments on 15 datasets encompassing classification,\ngeneration, reasoning, instruction fine-tuning tasks and 11 adversarial test\nsets validate our theories. We hope that these results spark further research\nbeyond the realms of well established PEFT. The source code is in the anonymous\nGithub repository\\footnote{https://github.com/misonsky/PEFTEval}.", "AI": {"tldr": "\u5c3d\u7ba1PEFT\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4e0a\u8f83\u5c11\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u5982FFT\u3002\u672c\u6587\u901a\u8fc7\u4f18\u5316\u7406\u8bba\u5206\u6790\u4e86PEFT\u548cFFT\u7684\u7279\u6027\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u8005\u4eec\u53d1\u73b0PEFT\u5728\u4e00\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53ef\u4ee5\u8fbe\u5230\u4e0eFFT\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u63a8\u7406\u548c\u6307\u4ee4\u5fae\u8c03\uff09\u4e2d\u6027\u80fd\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u5bf9\u6bd4PEFT\u548cFFT\u5728\u8868\u793a\u80fd\u529b\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u7279\u6027\u3002", "method": "\u57fa\u4e8e\u4f18\u5316\u7406\u8bba\uff0c\u6bd4\u8f83PEFT\u548cFFT\u5728\u8868\u793a\u5bb9\u91cf\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u7279\u6027\uff0c\u8bc1\u660ePEFT\u662fFFT\u7684\u4e00\u4e2a\u4e25\u683c\u5b50\u96c6\uff0c\u5e76\u7ed9\u51faPEFT\u7684\u7406\u8bba\u4e0a\u9650\uff0c\u8bf4\u660e\u53c2\u6570\u7a7a\u95f4\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u572815\u4e2a\u6570\u636e\u96c6\u548c11\u4e2a\u5bf9\u6297\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86PEFT\u53d7\u9650\u4e8e\u53c2\u6570\u7a7a\u95f4\uff0c\u8868\u793a\u80fd\u529b\u8f83\u5f31\u4e14\u66f4\u5bb9\u6613\u53d7\u5230\u6270\u52a8\u5f71\u54cd\u3002", "conclusion": "PEFT\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u5176\u53c2\u6570\u7a7a\u95f4\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u5982FFT\u3002\u8fd9\u4e9b\u7ed3\u679c\u5e0c\u671b\u63a8\u52a8\u8d85\u8d8a\u73b0\u6709PEFT\u65b9\u6cd5\u7684\u7814\u7a76\u3002"}}
{"id": "2505.22356", "pdf": "https://arxiv.org/pdf/2505.22356", "abs": "https://arxiv.org/abs/2505.22356", "authors": ["Ang\u00e9line Pouget", "Mohammad Yaghini", "Stephan Rabanser", "Nicolas Papernot"], "title": "Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Deploying machine learning models in safety-critical domains poses a key\nchallenge: ensuring reliable model performance on downstream user data without\naccess to ground truth labels for direct validation. We propose the suitability\nfilter, a novel framework designed to detect performance deterioration by\nutilizing suitability signals -- model output features that are sensitive to\ncovariate shifts and indicative of potential prediction errors. The suitability\nfilter evaluates whether classifier accuracy on unlabeled user data shows\nsignificant degradation compared to the accuracy measured on the labeled test\ndataset. Specifically, it ensures that this degradation does not exceed a\npre-specified margin, which represents the maximum acceptable drop in accuracy.\nTo achieve reliable performance evaluation, we aggregate suitability signals\nfor both test and user data and compare these empirical distributions using\nstatistical hypothesis testing, thus providing insights into decision\nuncertainty. Our modular method adapts to various models and domains. Empirical\nevaluations across different classification tasks demonstrate that the\nsuitability filter reliably detects performance deviations due to covariate\nshift. This enables proactive mitigation of potential failures in high-stakes\napplications.", "AI": {"tldr": "In safety-critical domains, deploying machine learning models requires ensuring reliable performance without ground truth labels. This paper introduces the suitability filter, a framework detecting performance drops by comparing test and user data distributions using statistical methods.", "motivation": "The motivation is to address the challenge of ensuring model reliability in safety-critical applications where direct validation via ground truth labels is not accessible.", "method": "Propose a suitability filter that detects performance degradation by utilizing suitability signals, which are model output features sensitive to covariate shifts. It compares empirical distributions of suitability signals from test and user data through statistical hypothesis testing.", "result": "Empirical evaluations show that the suitability filter reliably detects performance deviations caused by covariate shift across different classification tasks.", "conclusion": "The suitability filter enables proactive mitigation of potential failures in high-stakes applications by reliably detecting performance deterioration."}}
{"id": "2505.21724", "pdf": "https://arxiv.org/pdf/2505.21724", "abs": "https://arxiv.org/abs/2505.21724", "authors": ["Cheng Luo", "Jianghui Wang", "Bing Li", "Siyang Song", "Bernard Ghanem"], "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "23 pages, 9 figures", "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality.", "AI": {"tldr": "This paper introduces Online Multimodal Conversational Response Generation (OMCRG), a new task for generating synchronized verbal and non-verbal listener feedback based on speaker's multimodal input. To achieve this, they propose OmniResponse, a model leveraging pretrained LLM with Chrono-Text and TempoVoice modules, and introduce ResponseNet, a dataset supporting OMCRG research.", "motivation": "To create a system capable of generating synchronized verbal and non-verbal listener feedback in natural dyadic interactions, addressing the challenge of audio and facial response synchronization.", "method": "Introduced text as an intermediate modality to bridge audio and facial responses; proposed OmniResponse, a Multimodal Large Language Model that autoregressively generates high-quality multi-modal listener responses using Chrono-Text and TempoVoice components; created ResponseNet, a dataset with synchronized dyadic interactions.", "result": "OmniResponse significantly outperforms baseline models in semantic speech content, audio-visual synchronization, and generation quality when evaluated on the ResponseNet dataset.", "conclusion": "The introduction of OMCRG, OmniResponse, and ResponseNet advances the state-of-the-art in multimodal conversational response generation, particularly in achieving synchronization between generated audio and facial responses."}}
{"id": "2505.22358", "pdf": "https://arxiv.org/pdf/2505.22358", "abs": "https://arxiv.org/abs/2505.22358", "authors": ["Zhiyi Wan", "Wanrou Du", "Liang Li", "Miao Pan", "Xiaoqi Qin"], "title": "Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in a single\nend-to-end training stage. Specifically, OA-Adapter introduces a dynamic\nbottleneck dimension adaptation mechanism that simultaneously allocates an\nefficient parameter budget and optimizes task objectives without misalignment.\nTo effectively preserve previously acquired knowledge while coordinating with\nthe dynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency, achieving\nhigher average accuracy while using 58.5% fewer parameters on the standard CL\nbenchmark.", "AI": {"tldr": "OA-Adapter\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u901a\u8fc7\u7edf\u4e00\u52a8\u6001\u9884\u7b97\u9002\u5e94\u4e0e\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5b66\u4e60\uff0c\u5728\u5355\u4e00\u7aef\u5230\u7aef\u8bad\u7ec3\u9636\u6bb5\u4e2d\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53c2\u6570\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e2d\u5e38\u906d\u53d7\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u5728\u8bad\u7ec3\u65b0\u4efb\u52a1\u65f6\uff0c\u4e4b\u524d\u5b66\u5f97\u7684\u4efb\u52a1\u6027\u80fd\u4f1a\u4e25\u91cd\u9000\u5316\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u4efb\u52a1\u5e72\u6270\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7684\u9884\u7b97\u5206\u914d\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u548c\u5c42\u4e4b\u95f4\u7684\u590d\u6742\u6027\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7684\u81ea\u9002\u5e94\u9884\u7b97\u8c03\u6574\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u548c\u9884\u7b97\u5206\u914d\u8131\u8282\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6f5c\u5728\u7684\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u4e86\u5176\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOA-Adapter\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u74f6\u9888\u7ef4\u5ea6\u9002\u5e94\u673a\u5236\uff0c\u5728\u5355\u4e2a\u7aef\u5230\u7aef\u8bad\u7ec3\u9636\u6bb5\u4e2d\u7edf\u4e00\u52a8\u6001\u9884\u7b97\u9002\u5e94\u4e0e\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5b66\u4e60\u3002\u5177\u4f53\u800c\u8a00\uff0cOA-Adapter\u901a\u8fc7\u65bd\u52a0\u6b63\u4ea4\u7ea6\u675f\u6765\u6709\u6548\u5730\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\uff0c\u5e76\u4e0e\u52a8\u6001\u9884\u7b97\u5206\u914d\u76f8\u534f\u8c03\uff0c\u8fd9\u4e9b\u6b63\u4ea4\u7ea6\u675f\u4e13\u95e8\u5e94\u7528\u4e8e\u5f53\u524d\u4efb\u52a1\u7684\u53c2\u6570\u5b50\u7a7a\u95f4\u4e0e\u5386\u53f2\u4efb\u52a1\u7684\u52a8\u6001\u5206\u914d\u53c2\u6570\u5b50\u7a7a\u95f4\u4e4b\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOA-Adapter\u5728\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u6807\u51c6CL\u57fa\u51c6\u4e0a\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\uff08\u51cf\u5c11\u4e8658.5%\uff09\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "OA-Adapter\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4efb\u52a1\u590d\u6742\u6027\u548c\u52a8\u6001\u9884\u7b97\u5206\u914d\u65b9\u9762\u7684\u7814\u7a76\u3002"}}
{"id": "2505.22359", "pdf": "https://arxiv.org/pdf/2505.22359", "abs": "https://arxiv.org/abs/2505.22359", "authors": ["Matan Schliserman", "Tomer Koren"], "title": "Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification", "categories": ["cs.LG"], "comment": null, "summary": "We study the generalization performance of unregularized gradient methods for\nseparable linear classification. While previous work mostly deal with the\nbinary case, we focus on the multiclass setting with $k$ classes and establish\nnovel population risk bounds for Gradient Descent for loss functions that decay\nto zero. In this setting, we show risk bounds that reveal that convergence\nrates are crucially influenced by the geometry of the loss template, as\nformalized by Wang and Scott (2024), rather than of the loss function itself.\nParticularly, we establish risk upper bounds that holds for any decay rate of\nthe loss whose template is smooth with respect to the $p$-norm. In the case of\nexponentially decaying losses, our results indicates a contrast between the\n$p=\\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and\n$p=2$ where the risk scales linearly with $k$. To establish this separation\nformally, we also prove a lower bound in the latter scenario, demonstrating\nthat the polynomial dependence on $k$ is unavoidable. Central to our analysis\nis a novel bound on the Rademacher complexity of low-noise vector-valued linear\npredictors with a loss template smooth w.r.t.~general $p$-norms.", "AI": {"tldr": "\u7814\u7a76\u4e86\u65e0\u6b63\u5219\u5316\u68af\u5ea6\u65b9\u6cd5\u5728\u53ef\u5206\u7ebf\u6027\u5206\u7c7b\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u591a\u7c7b\u8bbe\u7f6e\u4e0b\uff0c\u98ce\u9669\u4e0a\u754c\u4e0e\u635f\u5931\u6a21\u677f\u7684\u51e0\u4f55\u7ed3\u6784\u5bc6\u5207\u76f8\u5173\u3002\u5bf9\u4e8e\u6307\u6570\u8870\u51cf\u635f\u5931\u51fd\u6570\uff0c\u5f53p=\u221e\u65f6\u98ce\u9669\u5bf9\u7c7b\u522b\u6570k\u5448\u5bf9\u6570\u4f9d\u8d56\uff0c\u800cp=2\u65f6\u98ce\u9669\u5bf9k\u5448\u7ebf\u6027\u4f9d\u8d56\uff0c\u5e76\u8bc1\u660e\u4e86\u540e\u4e00\u79cd\u60c5\u51b5\u4e0bk\u7684\u591a\u9879\u5f0f\u4f9d\u8d56\u4e0d\u53ef\u907f\u514d\u3002", "motivation": "\u4e4b\u524d\u7684\u5de5\u4f5c\u5927\u591a\u96c6\u4e2d\u4e8e\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u65e0\u6b63\u5219\u5316\u68af\u5ea6\u65b9\u6cd5\u5728\u591a\u7c7b\uff08k\u7c7b\uff09\u53ef\u5206\u7ebf\u6027\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u7279\u522b\u662f\u635f\u5931\u51fd\u6570\u8870\u51cf\u81f3\u96f6\u65f6\u7684\u98ce\u9669\u8fb9\u754c\u3002", "method": "\u5206\u6790\u4e86\u68af\u5ea6\u4e0b\u964d\u6cd5\u5728\u591a\u7c7b\u7ebf\u6027\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u603b\u4f53\u98ce\u9669\u8fb9\u754c\uff0c\u91cd\u70b9\u5173\u6ce8\u635f\u5931\u6a21\u677f\u7684\u51e0\u4f55\u7279\u6027\u5bf9\u6536\u655b\u901f\u5ea6\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5f15\u5165Rademacher\u590d\u6742\u5ea6\u7684\u65b0\u754c\u9650\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u8303\u6570\u4e0b\u7684\u635f\u5931\u51fd\u6570\u8870\u51cf\u901f\u7387\u5bf9\u98ce\u9669\u8fb9\u754c\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u98ce\u9669\u8fb9\u754c\u4e3b\u8981\u53d7\u635f\u5931\u6a21\u677f\u7684\u51e0\u4f55\u7279\u6027\u5f71\u54cd\uff0c\u800c\u975e\u635f\u5931\u51fd\u6570\u672c\u8eab\u3002\u5bf9\u4e8e\u6307\u6570\u8870\u51cf\u635f\u5931\u51fd\u6570\uff0c\u5f53p=\u221e\u65f6\uff0c\u98ce\u9669\u5bf9\u7c7b\u522b\u6570k\u5448\u5bf9\u6570\u4f9d\u8d56\uff1b\u5f53p=2\u65f6\uff0c\u98ce\u9669\u5bf9k\u5448\u7ebf\u6027\u4f9d\u8d56\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0bk\u7684\u591a\u9879\u5f0f\u4f9d\u8d56\u662f\u4e0d\u53ef\u907f\u514d\u7684\u3002", "conclusion": "\u65e0\u6b63\u5219\u5316\u68af\u5ea6\u65b9\u6cd5\u5728\u591a\u7c7b\u7ebf\u6027\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u53d7\u5230\u635f\u5931\u6a21\u677f\u51e0\u4f55\u7279\u6027\u7684\u663e\u8457\u5f71\u54cd\uff0c\u4e0d\u540c\u8303\u6570\u4e0b\u7684\u8870\u51cf\u7279\u6027\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u98ce\u9669\u4f9d\u8d56\u5173\u7cfb\u3002"}}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740", "abs": "https://arxiv.org/abs/2505.21740", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks.", "AI": {"tldr": "Large Language Models (LLMs) are unpredictable, and their ability to explain behavior is crucial. Counterfactual simulatability evaluates how well explanations allow users to infer model outputs on related counterfactuals. This paper provides a framework extending this method to generation tasks like news summarization and medical suggestion.", "motivation": "LLMs can be unpredictable, and even slight alterations to the prompt can cause the output to change in unexpected ways. Therefore, it is essential for models to accurately explain their behavior, especially in high-stakes settings.", "method": "The authors provide a general framework for extending the method of counterfactual simulatability to generation tasks. They use news summarization and medical suggestion as example use cases.", "result": "In the summarization setting, LLM explanations enable users to better predict LLM outputs on counterfactuals. However, there is significant room for improvement in medical suggestion. The evaluation for counterfactual simulatability may be more appropriate for skill-based tasks rather than knowledge-based tasks.", "conclusion": "Counterfactual simulatability is an effective approach for evaluating explanations in certain generation tasks, but further improvements are needed, particularly in knowledge-based tasks like medical suggestion."}}
{"id": "2505.22361", "pdf": "https://arxiv.org/pdf/2505.22361", "abs": "https://arxiv.org/abs/2505.22361", "authors": ["Xiangyu Chang", "Xi Chen", "Yining Wang", "Zhiyi Zeng"], "title": "Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper studies a bandit optimization problem where the goal is to\nmaximize a function $f(x)$ over $T$ periods for some unknown strongly concave\nfunction $f$. We consider a new pairwise comparison oracle, where the\ndecision-maker chooses a pair of actions $(x, x')$ for a consecutive number of\nperiods and then obtains an estimate of $f(x)-f(x')$. We show that such a\npairwise comparison oracle finds important applications to joint pricing and\ninventory replenishment problems and network revenue management. The challenge\nin this bandit optimization is twofold. First, the decision-maker not only\nneeds to determine a pair of actions $(x, x')$ but also a stopping time $n$\n(i.e., the number of queries based on $(x, x')$). Second, motivated by our\ninventory application, the estimate of the difference $f(x)-f(x')$ is biased,\nwhich is different from existing oracles in stochastic optimization literature.\nTo address these challenges, we first introduce a discretization technique and\nlocal polynomial approximation to relate this problem to linear bandits. Then\nwe developed a tournament successive elimination technique to localize the\ndiscretized cell and run an interactive batched version of LinUCB algorithm on\ncells. We establish regret bounds that are optimal up to poly-logarithmic\nfactors. Furthermore, we apply our proposed algorithm and analytical framework\nto the two operations management problems and obtain results that improve\nstate-of-the-art results in the existing literature.", "AI": {"tldr": "This paper studies a bandit optimization problem with a new pairwise comparison oracle, addresses challenges in decision-making and biased estimation through discretization, polynomial approximation, and tournament successive elimination techniques, achieving optimal regret bounds and improving results in operations management problems.", "motivation": "To maximize a function $f(x)$ over $T$ periods for an unknown strongly concave function $f$, the paper introduces a new pairwise comparison oracle that finds applications in joint pricing and inventory replenishment problems as well as network revenue management.", "method": "The method involves using a discretization technique and local polynomial approximation to relate the problem to linear bandits. A tournament successive elimination technique is developed to localize the discretized cell, followed by running an interactive batched version of LinUCB algorithm on cells.", "result": "The authors establish regret bounds that are optimal up to poly-logarithmic factors. When applied to two operations management problems, the proposed algorithm and analytical framework yield improved state-of-the-art results.", "conclusion": "The study successfully addresses the challenges in the bandit optimization problem with a new pairwise comparison oracle, providing effective solutions for operations management problems and setting new benchmarks with optimal regret bounds."}}
{"id": "2505.22362", "pdf": "https://arxiv.org/pdf/2505.22362", "abs": "https://arxiv.org/abs/2505.22362", "authors": ["Aihu Zhang", "Jiaxing Xu", "Mengcheng Lan", "Shili Xiang", "Yiping Ke"], "title": "Directed Homophily-Aware Graph Neural Network", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved significant success in various\nlearning tasks on graph-structured data. Nevertheless, most GNNs struggle to\ngeneralize to heterophilic neighborhoods. Additionally, many GNNs ignore the\ndirectional nature of real-world graphs, resulting in suboptimal performance on\ndirected graphs with asymmetric structures. In this work, we propose Directed\nHomophily-aware Graph Neural Network (DHGNN), a novel framework that addresses\nthese limitations by incorporating homophily-aware and direction-sensitive\ncomponents. DHGNN employs a resettable gating mechanism to adaptively modulate\nmessage contributions based on homophily levels and informativeness, and a\nstructure-aware noise-tolerant fusion module to effectively integrate node\nrepresentations from the original and reverse directions. Extensive experiments\non both homophilic and heterophilic directed graph datasets demonstrate that\nDHGNN outperforms state-of-the-art methods in node classification and link\nprediction. In particular, DHGNN improves over the best baseline by up to\n15.07% in link prediction. Our analysis further shows that the gating mechanism\ncaptures directional homophily gaps and fluctuating homophily across layers,\nproviding deeper insights into message-passing behavior on complex graph\nstructures.", "AI": {"tldr": "Graph Neural Networks (GNNs) have been successful in many areas, but they face challenges when dealing with heterophilic neighborhoods and directed graphs. This paper introduces Directed Homophily-aware Graph Neural Network (DHGNN), a new framework that incorporates homophily-aware and direction-sensitive components to overcome these limitations. DHGNN uses a resettable gating mechanism and structure-aware noise-tolerant fusion module to improve performance on node classification and link prediction tasks.", "motivation": "Current GNNs struggle with heterophilic neighborhoods and ignore the directional nature of real-world graphs, leading to suboptimal performance on directed graphs with asymmetric structures.", "method": "The proposed method, DHGNN, includes a resettable gating mechanism to adaptively modulate message contributions based on homophily levels and informativeness, and a structure-aware noise-tolerant fusion module to effectively integrate node representations from the original and reverse directions.", "result": "Extensive experiments show that DHGNN outperforms state-of-the-art methods in node classification and link prediction on both homophilic and heterophilic directed graph datasets. Specifically, DHGNN improves over the best baseline by up to 15.07% in link prediction.", "conclusion": "DHGNN addresses the limitations of current GNNs by incorporating homophily-aware and direction-sensitive components, demonstrating superior performance in various tasks."}}
{"id": "2505.21746", "pdf": "https://arxiv.org/pdf/2505.21746", "abs": "https://arxiv.org/abs/2505.21746", "authors": ["Arif Masrur", "Peder A. Olsen", "Paul R. Adler", "Carlan Jackson", "Matthew W. Myers", "Nathan Sedghi", "Ray R. Weil"], "title": "Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Unmanned Aircraft Systems (UAS) and satellites are key data sources for\nprecision agriculture, yet each presents trade-offs. Satellite data offer broad\nspatial, temporal, and spectral coverage but lack the resolution needed for\nmany precision farming applications, while UAS provide high spatial detail but\nare limited by coverage and cost, especially for hyperspectral data. This study\npresents a novel framework that fuses satellite and UAS imagery using\nsuper-resolution methods. By integrating data across spatial, spectral, and\ntemporal domains, we leverage the strengths of both platforms cost-effectively.\nWe use estimation of cover crop biomass and nitrogen (N) as a case study to\nevaluate our approach. By spectrally extending UAS RGB data to the vegetation\nred edge and near-infrared regions, we generate high-resolution Sentinel-2\nimagery and improve biomass and N estimation accuracy by 18% and 31%,\nrespectively. Our results show that UAS data need only be collected from a\nsubset of fields and time points. Farmers can then 1) enhance the spectral\ndetail of UAS RGB imagery; 2) increase the spatial resolution by using\nsatellite data; and 3) extend these enhancements spatially and across the\ngrowing season at the frequency of the satellite flights. Our SRCNN-based\nspectral extension model shows considerable promise for model transferability\nover other cropping systems in the Upper and Lower Chesapeake Bay regions.\nAdditionally, it remains effective even when cloud-free satellite data are\nunavailable, relying solely on the UAS RGB input. The spatial extension model\nproduces better biomass and N predictions than models built on raw UAS RGB\nimages. Once trained with targeted UAS RGB data, the spatial extension model\nallows farmers to stop repeated UAS flights. While we introduce\nsuper-resolution advances, the core contribution is a lightweight and scalable\nsystem for affordable on-farm use.", "AI": {"tldr": "The paper presents a novel framework that fuses satellite and UAS imagery using super-resolution methods to improve biomass and nitrogen estimation accuracy.", "motivation": "To address the trade-offs between satellite data (broad coverage but lack of resolution) and UAS data (high resolution but limited coverage and costly for hyperspectral data).", "method": "Fusing satellite and UAS imagery using super-resolution methods, spectrally extending UAS RGB data to vegetation red edge and near-infrared regions.", "result": "Improved biomass and nitrogen estimation accuracy by 18% and 31% respectively. The SRCNN-based spectral extension model shows promise for model transferability and better predictions than models built on raw UAS RGB images.", "conclusion": "A lightweight and scalable system for affordable on-farm use is introduced."}}
{"id": "2505.22370", "pdf": "https://arxiv.org/pdf/2505.22370", "abs": "https://arxiv.org/abs/2505.22370", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Weili Guan", "Min Zhang", "Liqiang Nie"], "title": "SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 4 figures", "summary": "Continual Learning requires a model to learn multiple tasks in sequence while\nmaintaining both stability:preserving knowledge from previously learned tasks,\nand plasticity:effectively learning new tasks. Gradient projection has emerged\nas an effective and popular paradigm in CL, where it partitions the gradient\nspace of previously learned tasks into two orthogonal subspaces: a primary\nsubspace and a minor subspace. New tasks are learned effectively within the\nminor subspace, thereby reducing interference with previously acquired\nknowledge. However, existing Gradient Projection methods struggle to achieve an\noptimal balance between plasticity and stability, as it is hard to\nappropriately partition the gradient space. In this work, we consider a\ncontinual learning paradigm based on Low-Rank Adaptation, which has gained\nconsiderable attention due to its efficiency and wide applicability, and\npropose a novel approach for continual learning, called SplitLoRA. We first\nprovide a theoretical analysis of how subspace partitioning affects model\nstability and plasticity. Informed by this analysis, we then introduce an\neffective method that derives the optimal partition of the gradient space for\npreviously learned tasks. This approach effectively balances stability and\nplasticity in continual learning. Experimental results on multiple datasets\ndemonstrate that the proposed method achieves state-of-the-art performance.", "AI": {"tldr": "The paper introduces SplitLoRA, a novel continual learning approach based on Low-Rank Adaptation that balances stability and plasticity by optimally partitioning the gradient space.", "motivation": "Continual Learning requires maintaining stability (preserving past knowledge) and plasticity (learning new tasks), but existing Gradient Projection methods struggle to appropriately partition the gradient space to achieve this balance.", "method": "The authors propose SplitLoRA, which involves a theoretical analysis of subspace partitioning's effects on stability and plasticity, followed by an effective method to derive the optimal partition of the gradient space for previously learned tasks.", "result": "Experimental results on multiple datasets demonstrate state-of-the-art performance of the proposed SplitLoRA method in continual learning.", "conclusion": "SplitLoRA effectively balances stability and plasticity in continual learning through optimal gradient space partitioning."}}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA .", "AI": {"tldr": "\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u7cfb\u7edf\u5728\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u53d8\u5316\u65f6\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u73af\u5883\u4e0b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6FRAMES-VQA\uff0c\u7528\u4e8e\u8bc4\u4f30VQA\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u3002\u901a\u8fc7\u4f7f\u7528\u5341\u4e2a\u73b0\u6709\u7684VQA\u57fa\u51c6\uff0c\u5e76\u5c06\u5b83\u4eec\u5206\u7c7b\u4e3a\u540c\u5206\u5e03\u3001\u8fd1\u4f3c\u548c\u8fdc\u8ddd\u79bb\u7684\u5f02\u5206\u5e03\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u5355\u6a21\u6001\u3001\u591a\u6a21\u6001\u548c\u5bf9\u6297\u6027\u5206\u5e03\u53d8\u5316\u3002\u9996\u5148\u5bf9\u73b0\u6709\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\uff0c\u7136\u540e\u901a\u8fc7\u8ba1\u7b97\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u5d4c\u5165\u7684\u9a6c\u6c0f\u8ddd\u79bb\u6765\u91cf\u5316\u5206\u5e03\u53d8\u5316\uff0c\u5e76\u5206\u6790\u4e86\u5355\u6a21\u6001\u4e0e\u591a\u6a21\u6001\u53d8\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u6a21\u6001\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u8bc4\u4f30\u8bbe\u7f6e\u4e3b\u8981\u662f\u5355\u6a21\u6001\u6216\u7279\u5b9a\u7c7b\u578b\u7684\u5f02\u5206\u5e03\uff0c\u5bf9\u4e8e\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u6709\u9650\u7684\u89c1\u89e3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6765\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u7406\u89e3VQA\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "1. \u63d0\u51faFRAMES-VQA\u57fa\u51c6\uff0c\u6574\u5408\u5341\u4e2a\u73b0\u6709VQA\u57fa\u51c6\u5e76\u5206\u7c7b\u4e3aID\u3001\u8fd1OOO\u548c\u8fdcOOO\u6570\u636e\u96c6\u3002\n2. \u5bf9\u73b0\u6709\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u7efc\u5408\u6bd4\u8f83\u3002\n3. \u4f7f\u7528\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u5d4c\u5165\u8ba1\u7b97\u9a6c\u6c0f\u8ddd\u79bb\u4ee5\u91cf\u5316\u5206\u5e03\u53d8\u5316\u3002\n4. \u5206\u6790\u5355\u6a21\u6001\u4e0e\u591a\u6a21\u6001\u53d8\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u6a21\u6001\u91cd\u8981\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5fae\u8c03\u65b9\u6cd5\u4ee5\u5904\u7406\u591a\u6a21\u6001\u5206\u5e03\u53d8\u5316\u7684\u5b9d\u8d35\u6307\u5bfc\u3002", "conclusion": "FRAMES-VQA\u4e3a\u8bc4\u4f30VQA\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u6539\u8fdbVQA\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2505.22381", "pdf": "https://arxiv.org/pdf/2505.22381", "abs": "https://arxiv.org/abs/2505.22381", "authors": ["Lukas Kirchdorfer", "Konrad \u00d6zdemir", "Stjepan Kusenic", "Han van der Aa", "Heiner Stuckenschmidt"], "title": "A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation", "categories": ["cs.LG"], "comment": null, "summary": "Business Process Simulation (BPS) is a critical tool for analyzing and\nimproving organizational processes by estimating the impact of process changes.\nA key component of BPS is the case-arrival model, which determines the pattern\nof new case entries into a process. Although accurate case-arrival modeling is\nessential for reliable simulations, as it influences waiting and overall cycle\ntimes, existing approaches often rely on oversimplified static distributions of\ninter-arrival times. These approaches fail to capture the dynamic and temporal\ncomplexities inherent in organizational environments, leading to less accurate\nand reliable outcomes. To address this limitation, we propose Auto Time Kernel\nDensity Estimation (AT-KDE), a divide-and-conquer approach that models arrival\ntimes of processes by incorporating global dynamics, day-of-week variations,\nand intraday distributional changes, ensuring both precision and scalability.\nExperiments conducted across 20 diverse processes demonstrate that AT-KDE is\nfar more accurate and robust than existing approaches while maintaining\nsensible execution time efficiency.", "AI": {"tldr": "In order to improve the accuracy and reliability of Business Process Simulation (BPS), this paper proposes a new method called Auto Time Kernel Density Estimation (AT-KDE). It is more accurate and robust than existing approaches, while maintaining reasonable execution time efficiency.", "motivation": "Existing methods for modeling case-arrival in BPS often rely on oversimplified static distributions which cannot capture the dynamic and temporal complexities inherent in organizational environments. This leads to less accurate and reliable simulation outcomes.", "method": "The proposed method, AT-KDE, uses a divide-and-conquer approach to model arrival times of processes by considering global dynamics, day-of-week variations, and intraday distributional changes.", "result": "Experiments conducted across 20 diverse processes showed that AT-KDE is significantly more accurate and robust compared to existing methods while keeping sensible execution time efficiency.", "conclusion": "AT-KDE improves the precision and scalability of case-arrival modeling in BPS, making it a superior alternative to current approaches."}}
{"id": "2505.21771", "pdf": "https://arxiv.org/pdf/2505.21771", "abs": "https://arxiv.org/abs/2505.21771", "authors": ["Prasham Yatinkumar Titiya", "Jainil Trivedi", "Chitta Baral", "Vivek Gupta"], "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal tables those that integrate semi structured data with visual\nelements such as charts and maps are ubiquitous across real world domains, yet\nthey pose a formidable challenge to current vision language models (VLMs).\nWhile Large Language models (LLMs) and VLMs have demonstrated strong\ncapabilities in text and image understanding, their performance on complex,\nreal world multimodal table reasoning remains unexplored. To bridge this gap,\nwe introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of\n500 real world multimodal tables drawn from diverse real world sources, with a\ntotal of 4021 question answer pairs. MMTBENCH questions cover four question\ntypes (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning\ntypes (Mathematical, Extrema Identification, Fact Verification, Vision Based,\nand Others), and eight table types (Single/Multiple Entity, Maps and Charts\nwith Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive\nevaluation of state of the art models on all types reveals substantial\nperformance gaps, particularly on questions requiring visual-based reasoning\nand multi-step inference. These findings show the urgent need for improved\narchitectures that more tightly integrate vision and language processing. By\nproviding a challenging, high-quality resource that mirrors the complexity of\nreal-world tasks, MMTBENCH underscores its value as a resource for future\nresearch on multimodal tables.", "AI": {"tldr": "Multimodal tables, integrating semi-structured data with visual elements, challenge current vision-language models (VLMs). While large language models (LLMs) and VLMs excel in text and image understanding individually, their performance on complex multimodal table reasoning remains unexplored. To address this, MMTBENCH, a benchmark consisting of 500 real-world multimodal tables and 4021 question-answer pairs, is introduced. Evaluations reveal significant performance gaps, especially in visual-based reasoning and multi-step inference, highlighting the need for improved vision-language integration.", "motivation": "The motivation behind this paper is to bridge the gap in understanding and processing multimodal tables that combine semi-structured data with visual elements like charts and maps. Current models perform well on individual tasks but struggle with the complexity of multimodal table reasoning.", "method": "The authors developed MMTBENCH, a benchmark containing 500 real-world multimodal tables and 4021 question-answer pairs. This benchmark covers various question types, reasoning types, and table types. State-of-the-art models were evaluated across all these categories.", "result": "Evaluations showed substantial performance gaps, particularly in questions requiring visual-based reasoning and multi-step inference. This indicates the current limitations of models in tightly integrating vision and language processing.", "conclusion": "MMTBENCH serves as a valuable resource for future research on multimodal tables by providing a challenging, high-quality dataset that reflects the complexity of real-world tasks. The findings emphasize the need for better architectures that more effectively integrate vision and language."}}
{"id": "2505.22389", "pdf": "https://arxiv.org/pdf/2505.22389", "abs": "https://arxiv.org/abs/2505.22389", "authors": ["Haomiao Qiu", "Miao Zhang", "Ziyue Qiao", "Liqiang Nie"], "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 3 figures", "summary": "Continual Learning (CL) aims to enable models to continuously acquire new\nknowledge from a sequence of tasks with avoiding the forgetting of learned\ninformation. However, existing CL methods only rely on the parameters of the\nmost recent task for inference, which makes them susceptible to catastrophic\nforgetting. Inspired by the recent success of model merging techniques, we\npropose \\textbf{Perturb-and-Merge (P\\&M)}, a novel continual learning framework\nthat integrates model merging into the CL paradigm to mitigate forgetting.\nSpecifically, after training on each task, P\\&M constructs a new model by\nforming a convex combination of the previous model and the newly trained\ntask-specific model. Through theoretical analysis, we minimize the total loss\nincrease across all tasks and derive an analytical solution for the optimal\nmerging coefficient. To further improve the performance of the merged model, we\nobserve that the degradation introduced during merging can be alleviated by a\nregularization term composed of the task vector and the Hessian matrix of the\nloss function. Interestingly, we show that this term can be efficiently\napproximated using second-order symmetric finite differences, and a stochastic\nperturbation strategy along the task vector direction is accordingly devised\nwhich incurs no additional forward or backward passes while providing an\neffective approximation of the regularization term. Finally, we combine P\\&M\nwith LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.\nOur proposed approach achieves state-of-the-art performance on several\ncontinual learning benchmark datasets.", "AI": {"tldr": "Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks while avoiding catastrophic forgetting. The proposed Perturb-and-Merge (P&M) framework integrates model merging into CL to mitigate forgetting by constructing a new model via convex combination, deriving an analytical solution for the optimal merging coefficient, and devising a stochastic perturbation strategy for effective regularization approximation without additional forward or backward passes. When combined with LoRA, P&M achieves state-of-the-art performance on several continual learning benchmarks.", "motivation": "Existing CL methods suffer from catastrophic forgetting as they rely solely on parameters from the most recent task for inference. To address this limitation, there is a need for a novel approach that leverages model merging techniques to better retain learned information across tasks.", "method": "The proposed method, Perturb-and-Merge (P&M), constructs a new model after training on each task by forming a convex combination of the previous model and the newly trained task-specific model. An analytical solution for the optimal merging coefficient is derived through theoretical analysis to minimize total loss increase across all tasks. A regularization term composed of the task vector and the Hessian matrix is introduced to alleviate degradation during merging, which can be efficiently approximated using second-order symmetric finite differences. Additionally, a stochastic perturbation strategy along the task vector direction is devised to provide an effective approximation of the regularization term without incurring additional forward or backward passes. Finally, P&M is combined with LoRA for parameter-efficient fine-tuning.", "result": "The proposed Perturb-and-Merge (P&M) framework, when combined with LoRA, achieves state-of-the-art performance on several continual learning benchmark datasets.", "conclusion": "Perturb-and-Merge (P&M) is a novel continual learning framework that effectively mitigates catastrophic forgetting by integrating model merging techniques. By constructing a new model via convex combination, deriving an analytical solution for the optimal merging coefficient, and devising an efficient regularization approximation strategy, P&M demonstrates superior performance on continual learning benchmarks when combined with LoRA."}}
{"id": "2505.22391", "pdf": "https://arxiv.org/pdf/2505.22391", "abs": "https://arxiv.org/abs/2505.22391", "authors": ["Yi Zhang", "Difan Zou"], "title": "Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "23 pages, 5 figures, 4 tables", "summary": "Modeling physical systems in a generative manner offers several advantages,\nincluding the ability to handle partial observations, generate diverse\nsolutions, and address both forward and inverse problems. Recently, diffusion\nmodels have gained increasing attention in the modeling of physical systems,\nparticularly those governed by partial differential equations (PDEs). However,\ndiffusion models only access noisy data $\\boldsymbol{x}_t$ at intermediate\nsteps, making it infeasible to directly enforce constraints on the clean sample\n$\\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are\ntypically applied to the expectation of clean samples\n$\\mathbb{E}[\\boldsymbol{x}_0|\\boldsymbol{x}_t]$, which is estimated using the\nlearned score network. However, imposing PDE constraints on the expectation\ndoes not strictly represent the one on the true clean data, known as Jensen's\nGap. This gap creates a trade-off: enforcing PDE constraints may come at the\ncost of reduced accuracy in generative modeling. To address this, we propose a\nsimple yet effective post-hoc distillation approach, where PDE constraints are\nnot injected directly into the diffusion process, but instead enforced during a\npost-hoc distillation stage. We term our method as Physics-Informed\nDistillation of Diffusion Models (PIDDM). This distillation not only\nfacilitates single-step generation with improved PDE satisfaction, but also\nsupport both forward and inverse problem solving and reconstruction from\nrandomly partial observation. Extensive experiments across various PDE\nbenchmarks demonstrate that PIDDM significantly improves PDE satisfaction over\nseveral recent and competitive baselines, such as PIDM, DiffusionPDE, and\nECI-sampling, with less computation overhead. Our approach can shed light on\nmore efficient and effective strategies for incorporating physical constraints\ninto diffusion models.", "AI": {"tldr": "The paper proposes Physics-Informed Distillation of Diffusion Models (PIDDM), a post-hoc distillation approach that enforces PDE constraints during the distillation stage to improve generative modeling accuracy and PDE satisfaction without significant computational overhead.", "motivation": "Diffusion models used for physical systems governed by PDEs face challenges due to Jensen's Gap when enforcing PDE constraints on the expectation of clean samples rather than directly on the true clean data. This creates a trade-off between constraint enforcement and generative modeling accuracy.", "method": "The authors introduce PIDDM, which avoids injecting PDE constraints directly into the diffusion process. Instead, these constraints are enforced in a post-hoc distillation stage. This method supports single-step generation with improved PDE satisfaction and can handle forward and inverse problems as well as reconstruction from partial observations.", "result": "Extensive experiments across various PDE benchmarks show that PIDDM significantly improves PDE satisfaction compared to recent baselines like PIDM, DiffusionPDE, and ECI-sampling, while also reducing computational overhead.", "conclusion": "PIDDM provides an efficient and effective strategy for incorporating physical constraints into diffusion models, improving both accuracy and PDE satisfaction."}}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786", "abs": "https://arxiv.org/abs/2505.21786", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets.", "AI": {"tldr": "\u5728\u591a\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b(MGS)\u4e2d\uff0c\u95ed\u57df\u5e7b\u89c9\u73b0\u8c61\u6bd4\u5355\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b(SGS)\u66f4\u4e3a\u4e25\u91cd\u3002\u4e3a\u4e86\u68c0\u6d4b\u6700\u7ec8\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u5e76\u8ffd\u8e2a\u5e7b\u89c9\u5185\u5bb9\u7684\u6765\u6e90\uff0c\u672c\u6587\u63d0\u51fa\u4e86VeriTrail\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u4e2d\u95f4\u8f93\u51fa\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660eVeriTrail\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u88ab\u6307\u793a\u9075\u5faa\u6e90\u6750\u6599\uff0c\u4ecd\u53ef\u80fd\u751f\u6210\u65e0\u6839\u636e\u7684\u5185\u5bb9\uff08\u95ed\u57df\u5e7b\u89c9\uff09\uff0c\u4e14\u5728\u591a\u6b65\u9aa4\u751f\u6210\u8fc7\u7a0b\u4e2d\u6b64\u98ce\u9669\u66f4\u9ad8\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u5e7b\u89c9\u5e76\u8ffd\u8e2a\u5e7b\u89c9\u5185\u5bb9\u7684\u5f15\u5165\u4f4d\u7f6e\u3002", "method": "\u63d0\u51faVeriTrail\u65b9\u6cd5\u7528\u4e8e\u95ed\u57df\u5e7b\u89c9\u68c0\u6d4b\uff0c\u8be5\u65b9\u6cd5\u4e3aMGS\u548cSGS\u8fc7\u7a0b\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u6027\uff1b\u540c\u65f6\u6784\u5efa\u4e86\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6240\u6709\u4e2d\u95f4\u8f93\u51fa\u53ca\u4eba\u5de5\u6807\u6ce8\u7684\u6700\u7ec8\u8f93\u51fa\u5fe0\u5b9e\u5ea6\u3002", "result": "VeriTrail\u65b9\u6cd5\u5728\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VeriTrail\u662f\u9996\u4e2a\u4e3aMGS\u548cSGS\u8fc7\u7a0b\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u6027\u7684\u95ed\u57df\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5176\u6548\u679c\u5df2\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering.", "AI": {"tldr": "Recent advances in Large Reasoning Models (LRMs) have shown great abilities in complex tasks, but suffer from overthinking. This paper proposes Manifold Steering to mitigate overthinking by projecting the steering direction onto a low-dimensional activation manifold. Experiments show that this method reduces output tokens significantly while maintaining or improving accuracy.", "motivation": "To address the issue of overthinking in LRMs which leads to computational overheads.", "method": "Investigate the underlying mechanisms of overthinking from the perspective of mechanistic interpretability and propose Manifold Steering, which projects the steering direction onto the low-dimensional activation manifold.", "result": "Reduces output tokens by up to 71% while maintaining and even improving accuracy on mathematical benchmarks, and exhibits robust cross-domain transferability.", "conclusion": "Manifold Steering effectively mitigates overthinking in LRMs and can be applied across different domains."}}
{"id": "2505.22422", "pdf": "https://arxiv.org/pdf/2505.22422", "abs": "https://arxiv.org/abs/2505.22422", "authors": ["V\u00e1clav Vor\u00e1\u010dek", "Francesco Orabona"], "title": "STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals", "categories": ["cs.LG"], "comment": "comments are welcome", "summary": "The construction of confidence intervals for the mean of a bounded random\nvariable is a classical problem in statistics with numerous applications in\nmachine learning and virtually all scientific fields. In particular, obtaining\nthe tightest possible confidence intervals is vital every time the sampling of\nthe random variables is expensive. The current state-of-the-art method to\nconstruct confidence intervals is by using betting algorithms. This is a very\nsuccessful approach for deriving optimal confidence sequences, even matching\nthe rate of law of iterated logarithms. However, in the fixed horizon setting,\nthese approaches are either sub-optimal or based on heuristic solutions with\nstrong empirical performance but without a finite-time guarantee. Hence, no\nbetting-based algorithm guaranteeing the optimal\n$\\mathcal{O}(\\sqrt{\\frac{\\sigma^2\\log\\frac1\\delta}{n}})$ width of the\nconfidence intervals are known. This work bridges this gap. We propose a\nbetting-based algorithm to compute confidence intervals that empirically\noutperforms the competitors. Our betting strategy uses the optimal strategy in\nevery step (in a certain sense), whereas the standard betting methods choose a\nconstant strategy in advance. Leveraging this fact results in strict\nimprovements even for classical concentration inequalities, such as the ones of\nHoeffding or Bernstein. Moreover, we also prove that the width of our\nconfidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.\nThe code is available\non~https://github.com/vvoracek/STaR-bets-confidence-interval.", "AI": {"tldr": "The paper proposes a betting-based algorithm for constructing confidence intervals that outperforms existing methods and guarantees optimal interval width.", "motivation": "Constructing the tightest possible confidence intervals is crucial when sampling is expensive. Current state-of-the-art betting algorithms are either sub-optimal or lack finite-time guarantees in fixed horizon settings.", "method": "A new betting-based algorithm is introduced which computes confidence intervals by using an optimal strategy at each step, rather than choosing a constant strategy in advance. This approach improves upon classical concentration inequalities such as Hoeffding's and Bernstein's.", "result": "Empirically, the proposed algorithm outperforms competitors. Theoretically, it is proven that the width of the confidence intervals is optimal up to a diminishing factor.", "conclusion": "This work bridges the gap in fixed horizon settings by providing a betting-based algorithm with finite-time guarantees and optimal confidence interval width."}}
{"id": "2505.21811", "pdf": "https://arxiv.org/pdf/2505.21811", "abs": "https://arxiv.org/abs/2505.21811", "authors": ["Clark Mingxuan Ju", "Leonardo Neves", "Bhuvesh Kumar", "Liam Collins", "Tong Zhao", "Yuwei Qiu", "Qing Dou", "Sohail Nizam", "Sen Yang", "Neil Shah"], "title": "Revisiting Self-attention for Cross-domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted to KDD'25", "summary": "Sequential recommendation is a popular paradigm in modern recommender\nsystems. In particular, one challenging problem in this space is cross-domain\nsequential recommendation (CDSR), which aims to predict future behaviors given\nuser interactions across multiple domains. Existing CDSR frameworks are mostly\nbuilt on the self-attention transformer and seek to improve by explicitly\ninjecting additional domain-specific components (e.g. domain-aware module\nblocks). While these additional components help, we argue they overlook the\ncore self-attention module already present in the transformer, a naturally\npowerful tool to learn correlations among behaviors. In this work, we aim to\nimprove the CDSR performance for simple models from a novel perspective of\nenhancing the self-attention. Specifically, we introduce a Pareto-optimal\nself-attention and formulate the cross-domain learning as a multi-objective\nproblem, where we optimize the recommendation task while dynamically minimizing\nthe cross-domain attention scores. Our approach automates knowledge transfer in\nCDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also\nencourages complementary knowledge exchange among auxiliary domains. Based on\nthe idea, we further introduce AutoCDSR+, a more performant variant with slight\nadditional cost. Our proposal is easy to implement and works as a plug-and-play\nmodule that can be incorporated into existing transformer-based recommenders.\nBesides flexibility, it is practical to deploy because it brings little extra\ncomputational overheads without heavy hyper-parameter tuning. AutoCDSR on\naverage improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and\nNDCG@10 by 12.0% and 16.7%, respectively. Code is available at\nhttps://github.com/snap-research/AutoCDSR.", "AI": {"tldr": "The paper proposes AutoCDSR, a method enhancing self-attention in transformers for cross-domain sequential recommendation (CDSR). It formulates CDSR as a multi-objective problem, optimizing recommendations while minimizing cross-domain attention scores to automate knowledge transfer. The approach mitigates negative transfer and encourages complementary knowledge exchange, with minimal computational overhead. Experiments show significant improvements in Recall@10 and NDCG@10 metrics for existing models.", "motivation": "Existing CDSR frameworks rely on adding domain-specific components to transformers, but overlook the potential of enhancing the core self-attention module which is naturally powerful for learning behavior correlations.", "method": "Introduces Pareto-optimal self-attention and treats cross-domain learning as a multi-objective problem. Optimizes recommendation tasks while dynamically reducing cross-domain attention scores, automating knowledge transfer in CDSR. Proposes AutoCDSR+ as an enhanced variant.", "result": "AutoCDSR improves Recall@10 by 9.8% for SASRec and 16.0% for Bert4Rec. It also enhances NDCG@10 by 12.0% for SASRec and 16.7% for Bert4Rec.", "conclusion": "AutoCDSR is effective, easy to implement, and has minimal computational overhead. It significantly boosts performance in transformer-based recommenders without extensive hyper-parameter tuning."}}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6ce8\u610f\u529b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u548c\u6570\u636e\u9a71\u52a8\u7684\u521b\u65b0\u89e3\u51b3\u4e86Transformer\u67b6\u6784\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5fae\u8c03\u7b56\u7565\u63d0\u9ad8\u4e86\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002\u5728\u57fa\u51c6\u8bc4\u4f30\u4e2d\uff0c\u8be5\u6a21\u578b\u8d85\u8d8a\u4e86\u76f8\u540c\u89c4\u6a21\u7684\u5f3a\u5927Transformer\u548c\u6df7\u5408\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u66f4\u5927\u89c4\u6a21\u7684Gemma3-27B\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u67b6\u6784\u6548\u7387\u4f4e\u4e0b\uff08\u4f9d\u8d56Transformer\uff09\u4ee5\u53ca\u7f3a\u4e4f\u9488\u5bf9\u9ad8\u96be\u5ea6\u9886\u57df\u7684\u7ed3\u6784\u5316\u5fae\u8c03\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eMamba-2\u7684SSD\u5c42\u7684\u65e0\u6ce8\u610f\u529b\u8bed\u8a00\u6a21\u578b\uff0c\u6d88\u9664\u4e86\u81ea\u6ce8\u610f\u529b\u548c\u952e\u503c\u7f13\u5b58\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u56fa\u5b9a\u5185\u5b58\u548c\u5e38\u6570\u65f6\u95f4\u63a8\u7406\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePromptCoT\u5408\u6210\u8303\u5f0f\u7684\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5fae\u8c03\u7b56\u7565\uff0c\u751f\u6210\u7ed3\u6784\u5316\u95ee\u9898\u4ee5\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOurModel-7B\u7684\u8868\u73b0\u4f18\u4e8e\u76f8\u540c\u89c4\u6a21\u7684Transformer\u548c\u6df7\u5408\u6a21\u578b\uff0c\u5e76\u4e14\u5728AIME 24\u3001AIME 25\u548cLivecodebench\u4e0a\u5206\u522b\u8d85\u8fc7Gemma3-27B 2.6%\u30010.6%\u548c3.0%\u3002", "conclusion": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u4f5c\u4e3a\u9ad8\u5bb9\u91cf\u63a8\u7406\u7684\u6ce8\u610f\u529b\u673a\u5236\u67b6\u6784\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.21815", "pdf": "https://arxiv.org/pdf/2505.21815", "abs": "https://arxiv.org/abs/2505.21815", "authors": ["Yunyi Zhang", "Ruozhen Yang", "Siqi Jiao", "SeongKu Kang", "Jiawei Han"], "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient.", "AI": {"tldr": "SemRank is a paper retrieval framework combining LLM-guided query understanding with concept-based semantic indexing to improve retrieval accuracy.", "motivation": "Dense retrieval methods struggle with fine-grained scientific concepts and LLMs lack grounding in corpus-specific knowledge, leading to unreliable content generation.", "method": "Uses multi-granular scientific concepts for indexing papers and leverages an LLM at query time to identify core concepts from the corpus, enabling precise semantic matching.", "result": "Experiments demonstrate that SemRank enhances various base retrievers' performance, outperforms existing LLM-based methods, and maintains high efficiency.", "conclusion": "SemRank effectively addresses limitations of previous approaches by integrating LLM capabilities with concept-based indexing."}}
{"id": "2505.22440", "pdf": "https://arxiv.org/pdf/2505.22440", "abs": "https://arxiv.org/abs/2505.22440", "authors": ["Khan Masood Parvez", "Sk Md Abidar Rahaman", "Ali Shiri Sichani"], "title": "Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models", "categories": ["cs.LG"], "comment": null, "summary": "The rapid evolution of wireless technologies necessitates automated design\nframeworks to address antenna miniaturization and performance optimization\nwithin constrained development cycles. This study demonstrates a machine\nlearning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm\nOptimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.\nThe QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,\nachieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared\nto conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,\nXGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds\nusing 936 simulation datasets, with stacked models showing superior training\naccuracy (R2=0.9825) and SVM demonstrating optimal validation performance\n(R2=0.7197). The complete design cycle, encompassing optimization, prediction,\nand ANSYS validation, required 12.42 minutes on standard desktop hardware\n(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of\nPSADEA-based approaches. This 240 times of acceleration eliminates traditional\ntrial-and-error methods that often extend beyond seven expert-led days. The\nsystem enables precise specifications of performance targets with automated\ngeneration of fabrication-ready parameters, particularly benefiting compact\nconsumer devices requiring rapid frequency tuning. By bridging AI-driven\noptimization with CAD validation, this framework reduces engineering workloads\nwhile ensuring production-ready designs, establishing a scalable paradigm for\nnext-generation RF systems in 6G and IoT applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u884c\u4e3a\u52a8\u6001\u7c92\u5b50\u7fa4\u4f18\u5316\uff08QDPSO\uff09\u4e0eANSYS HFSS\u4eff\u771f\u7684\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u5de5\u4f5c\u6d41\uff0c\u4ee5\u52a0\u901f\u5929\u7ebf\u8bbe\u8ba1\u3002\u901a\u8fc7\u4f18\u5316\u3001\u9884\u6d4b\u548c\u9a8c\u8bc1\u7684\u5b8c\u6574\u8bbe\u8ba1\u5468\u671f\u4ec5\u970012.42\u5206\u949f\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb240\u500d\u3002\u8be5\u6846\u67b6\u4e3a6G\u548c\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u7684\u4e0b\u4e00\u4ee3\u5c04\u9891\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002", "motivation": "\u65e0\u7ebf\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u81ea\u52a8\u5316\u8bbe\u8ba1\u6846\u67b6\u6765\u89e3\u51b3\u5929\u7ebf\u5c0f\u578b\u5316\u548c\u6027\u80fd\u4f18\u5316\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u53d7\u9650\u7684\u5f00\u53d1\u5468\u671f\u5185\u3002\u4f20\u7edf\u7684\u8bd5\u9519\u65b9\u6cd5\u8017\u65f6\u4e14\u6548\u7387\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u5c06QDPSO\u7b97\u6cd5\u4e0eANSYS HFSS\u4eff\u771f\u7ed3\u5408\uff0c\u81ea\u52a8\u4f18\u5316\u5929\u7ebf\u73af\u5c3a\u5bf8\uff1b\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08SVM\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\u548c\u5806\u53e0\u96c6\u6210\uff09\u57fa\u4e8e936\u4e2a\u6a21\u62df\u6570\u636e\u96c6\u9884\u6d4b\u5171\u632f\u9891\u7387\uff1b\u6700\u540e\u901a\u8fc7ANSYS\u9a8c\u8bc1\u8bbe\u8ba1\u7ed3\u679c\u3002", "result": "QDPSO\u7b97\u6cd5\u572811.53\u79d2\u5185\u5c06\u5171\u632f\u9891\u7387\u4ece1.60 GHz\u964d\u4f4e\u52301.4208 GHz\uff08\u51cf\u5c1112.7%\uff09\uff1b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u5806\u53e0\u6a21\u578b\u8bad\u7ec3\u7cbe\u5ea6\u6700\u9ad8\uff08R2=0.9825\uff09\uff0cSVM\u9a8c\u8bc1\u6027\u80fd\u6700\u4f73\uff08R2=0.7197\uff09\uff1b\u5b8c\u6574\u8bbe\u8ba1\u5468\u671f\u4ec5\u970012.42\u5206\u949f\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u52a0\u901f\u4e86240\u500d\u3002", "conclusion": "\u8fd9\u79cdAI\u9a71\u52a8\u7684\u4f18\u5316\u4e0eCAD\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u6846\u67b6\u51cf\u5c11\u4e86\u5de5\u7a0b\u5de5\u4f5c\u91cf\uff0c\u786e\u4fdd\u4e86\u53ef\u751f\u4ea7\u7684\u8bbe\u8ba1\uff0c\u5e76\u4e3a6G\u548c\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u7684\u4e0b\u4e00\u4ee3\u5c04\u9891\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2505.22442", "pdf": "https://arxiv.org/pdf/2505.22442", "abs": "https://arxiv.org/abs/2505.22442", "authors": ["Mattie Fellows", "Clarisse Wibault", "Uljad Berdica", "Johannes Forkel", "Jakob N. Foerster", "Michael A. Osborne"], "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel.", "AI": {"tldr": "This paper proposes two algorithms, SOReL and TOReL, to improve offline reinforcement learning by enabling safe and reliable hyperparameter tuning without online interactions.", "motivation": "Sample efficiency is a major obstacle for real-world adoption of reinforcement learning (RL). Offline RL offers a solution but current methods rely on extensive online interactions for hyperparameter tuning and have no reliable bound on their initial online performance.", "method": "SOReL uses a Bayesian approach to infer a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. All hyperparameters are tuned fully offline. TOReL extends information rate based offline hyperparameter tuning methods to general offline RL approaches.", "result": "Empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting. TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data.", "conclusion": "SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world."}}
{"id": "2505.21827", "pdf": "https://arxiv.org/pdf/2505.21827", "abs": "https://arxiv.org/abs/2505.21827", "authors": ["Yongyi Zang", "Zheqi Dai", "Mark D. Plumbley", "Qiuqiang Kong"], "title": "Music Source Restoration", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "A modified version of this paper is in review", "summary": "We introduce Music Source Restoration (MSR), a novel task addressing the gap\nbetween idealized source separation and real-world music production. Current\nMusic Source Separation (MSS) approaches assume mixtures are simple sums of\nsources, ignoring signal degradations employed during music production like\nequalization, compression, and reverb. MSR models mixtures as degraded sums of\nindividually degraded sources, with the goal of recovering original, undegraded\nsignals. Due to the lack of data for MSR, we present RawStems, a dataset\nannotation of 578 songs with unprocessed source signals organized into 8\nprimary and 17 secondary instrument groups, totaling 354.13 hours. To the best\nof our knowledge, RawStems is the first dataset that contains unprocessed music\nstems with hierarchical categories. We consider spectral filtering, dynamic\nrange compression, harmonic distortion, reverb and lossy codec as possible\ndegradations, and establish U-Former as a baseline method, demonstrating the\nfeasibility of MSR on our dataset. We release the RawStems dataset annotations,\ndegradation simulation pipeline, training code and pre-trained models to be\npublicly available.", "AI": {"tldr": "The paper introduces Music Source Restoration (MSR), a new task bridging idealized source separation and real-world music production, presents RawStems dataset, considers various degradations, establishes U-Former as baseline method, and releases resources publicly.", "motivation": "To address the gap between idealized source separation and real-world music production where signal degradations such as equalization, compression, and reverb are ignored in current Music Source Separation approaches.", "method": "Introduce MSR which models mixtures as degraded sums of individually degraded sources to recover original signals. Create RawStems dataset with unprocessed source signals categorized hierarchically. Consider spectral filtering, dynamic range compression, harmonic distortion, reverb and lossy codec as possible degradations. Establish U-Former as baseline method.", "result": "Demonstrate the feasibility of MSR on the presented dataset and establish U-Former as a baseline method.", "conclusion": "MSR is a novel task that bridges the gap between idealized source separation and real-world music production. The RawStems dataset, degradation simulation pipeline, training code and pre-trained models have been made publicly available."}}
{"id": "2505.22450", "pdf": "https://arxiv.org/pdf/2505.22450", "abs": "https://arxiv.org/abs/2505.22450", "authors": ["Ossi R\u00e4is\u00e4", "Boris van Breugel", "Mihaela van der Schaar"], "title": "Position: All Current Generative Fidelity and Diversity Metrics are Flawed", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Any method's development and practical application is limited by our ability\nto measure its reliability. The popularity of generative modeling emphasizes\nthe importance of good synthetic data metrics. Unfortunately, previous works\nhave found many failure cases in current metrics, for example lack of outlier\nrobustness and unclear lower and upper bounds. We propose a list of desiderata\nfor synthetic data metrics, and a suite of sanity checks: carefully chosen\nsimple experiments that aim to detect specific and known generative modeling\nfailure modes. Based on these desiderata and the results of our checks, we\narrive at our position: all current generative fidelity and diversity metrics\nare flawed. This significantly hinders practical use of synthetic data. Our aim\nis to convince the research community to spend more effort in developing\nmetrics, instead of models. Additionally, through analyzing how current metrics\nfail, we provide practitioners with guidelines on how these metrics should\n(not) be used.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6307\u51fa\u5f53\u524d\u751f\u6210\u6a21\u578b\u7684\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u6307\u6807\u5b58\u5728\u7f3a\u9677\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u5408\u6210\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5408\u6210\u6570\u636e\u6307\u6807\u7684\u7406\u60f3\u7279\u6027\u53ca\u5408\u7406\u6027\u68c0\u67e5\uff0c\u5e76\u5efa\u8bae\u7814\u7a76\u754c\u5e94\u66f4\u52a0\u6ce8\u91cd\u53d1\u5c55\u6307\u6807\u800c\u975e\u5355\u7eaf\u5173\u6ce8\u6a21\u578b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6307\u6807\u7684\u4e0d\u8db3\uff0c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4f7f\u7528\u8fd9\u4e9b\u6307\u6807\u7684\u6307\u5bfc\u3002", "motivation": "\u751f\u6210\u5efa\u6a21\u7684\u6d41\u884c\u7a81\u663e\u4e86\u5408\u6210\u6570\u636e\u6307\u6807\u7684\u91cd\u8981\u6027\uff0c\u7136\u800c\u73b0\u6709\u7684\u6307\u6807\u5b58\u5728\u8bb8\u591a\u95ee\u9898\uff0c\u5982\u7f3a\u4e4f\u5f02\u5e38\u503c\u9c81\u68d2\u6027\u548c\u754c\u9650\u4e0d\u6e05\u7b49\uff0c\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u5408\u6210\u6570\u636e\u6307\u6807\u7684\u7406\u60f3\u7279\u6027\u6e05\u5355\uff0c\u4ee5\u53ca\u4e00\u7cfb\u5217\u5408\u7406\u6027\u68c0\u67e5\u5b9e\u9a8c\uff0c\u65e8\u5728\u68c0\u6d4b\u7279\u5b9a\u4e14\u5df2\u77e5\u7684\u751f\u6210\u5efa\u6a21\u5931\u8d25\u6a21\u5f0f\u3002\u901a\u8fc7\u8fd9\u4e9b\u68c0\u67e5\uff0c\u8bc4\u4f30\u5f53\u524d\u6307\u6807\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u5f53\u524d\u7684\u751f\u6210\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u6307\u6807\u90fd\u88ab\u53d1\u73b0\u5b58\u5728\u7f3a\u9677\uff0c\u8fd9\u5927\u5927\u963b\u788d\u4e86\u5408\u6210\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u9700\u8981\u66f4\u591a\u7684\u52aa\u529b\u6765\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u5408\u6210\u6570\u636e\u6307\u6807\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u6b63\u786e\uff08\u4e0d\uff09\u4f7f\u7528\u73b0\u6709\u6307\u6807\u7684\u6307\u5357\u3002"}}
{"id": "2505.22473", "pdf": "https://arxiv.org/pdf/2505.22473", "abs": "https://arxiv.org/abs/2505.22473", "authors": ["Riccardo Poiani", "Martino Bernasconi", "Andrea Celli"], "title": "Pure Exploration with Infinite Answers", "categories": ["cs.LG"], "comment": null, "summary": "We study pure exploration problems where the set of correct answers is\npossibly infinite, e.g., the regression of any continuous function of the means\nof the bandit. We derive an instance-dependent lower bound for these problems.\nBy analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)\nfor finite answer problems fail at being asymptotically optimal in this more\ngeneral setting. Finally, we present a framework, Sticky-Sequence\nTrack-and-Stop, which generalizes both Track-and-Stop and Sticky\nTrack-and-Stop, and that enjoys asymptotic optimality. Due to its generality,\nour analysis also highlights special cases where existing methods enjoy\noptimality.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6b63\u786e\u7b54\u6848\u96c6\u53ef\u80fd\u65e0\u9650\u7684\u7eaf\u63a2\u7d22\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u4f8b\u4f9d\u8d56\u7684\u4e0b\u754c\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u901a\u7528\u8bbe\u5b9a\u4e0b\u7684\u4e0d\u8db3\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6 Sticky-Sequence Track-and-Stop\uff0c\u5177\u6709\u6e10\u8fdb\u6700\u4f18\u6027\u3002", "motivation": "\u7814\u7a76\u65e0\u9650\u6b63\u786e\u7b54\u6848\u96c6\u7684\u7eaf\u63a2\u7d22\u95ee\u9898\uff0c\u7406\u89e3\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u901a\u7528\u8bbe\u5b9a\u4e0b\u7684\u8868\u73b0\u53ca\u5c40\u9650\u6027\u3002", "method": "1. \u63a8\u5bfc\u51fa\u9488\u5bf9\u65e0\u9650\u6b63\u786e\u7b54\u6848\u96c6\u95ee\u9898\u7684\u5b9e\u4f8b\u4f9d\u8d56\u4e0b\u754c\u3002\n2. \u5206\u6790\u73b0\u6709\u65b9\u6cd5\uff08\u5982Sticky Track-and-Stop\uff09\u4e3a\u4f55\u5728\u8be5\u901a\u7528\u8bbe\u5b9a\u4e0b\u65e0\u6cd5\u8fbe\u5230\u6e10\u8fdb\u6700\u4f18\u3002\n3. \u63d0\u51fa\u65b0\u6846\u67b6Sticky-Sequence Track-and-Stop\uff0c\u8be5\u6846\u67b6\u6574\u5408\u5e76\u6269\u5c55\u4e86Track-and-Stop\u548cSticky Track-and-Stop\u3002", "result": "1. \u5f97\u5230\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u65e0\u9650\u6b63\u786e\u7b54\u6848\u96c6\u95ee\u9898\u7684\u5b9e\u4f8b\u4f9d\u8d56\u4e0b\u754c\u3002\n2. \u660e\u786e\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u901a\u7528\u8bbe\u5b9a\u4e0b\u7684\u4e0d\u8db3\u3002\n3. \u65b0\u6846\u67b6Sticky-Sequence Track-and-Stop\u88ab\u8bc1\u660e\u5177\u6709\u6e10\u8fdb\u6700\u4f18\u6027\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u901a\u7528\u6027\uff0c\u4e5f\u63ed\u793a\u4e86\u4e00\u4e9b\u7279\u6b8a\u60c5\u51b5\u4e0b\u73b0\u6709\u65b9\u6cd5\u7684\u6700\u4f18\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6Sticky-Sequence Track-and-Stop\u89e3\u51b3\u65e0\u9650\u6b63\u786e\u7b54\u6848\u96c6\u7684\u7eaf\u63a2\u7d22\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u6e10\u8fdb\u6700\u4f18\u6027\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u4e0b\u754c\u89e3\u91ca\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.21838", "pdf": "https://arxiv.org/pdf/2505.21838", "abs": "https://arxiv.org/abs/2505.21838", "authors": ["Maobin Lu", "Martin Guay", "Telema Harry", "Shimin Wang", "Jordan Cooper"], "title": "Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems", "categories": ["eess.SY", "cs.AI", "cs.SY", "math.OC", "nlin.CD"], "comment": "8 pages, 3 figures", "summary": "This paper investigates the robust output regulation problem of second-order\nnonlinear uncertain systems with an unknown exosystem. Instead of the adaptive\ncontrol approach, this paper resorts to a robust control methodology to solve\nthe problem and thus avoid the bursting phenomenon. In particular, this paper\nconstructs generic internal models for the steady-state state and input\nvariables of the system. By introducing a coordinate transformation, this paper\nconverts the robust output regulation problem into a nonadaptive stabilization\nproblem of an augmented system composed of the second-order nonlinear uncertain\nsystem and the generic internal models. Then, we design the stabilization\ncontrol law and construct a strict Lyapunov function that guarantees the\nrobustness with respect to unmodeled disturbances. The analysis shows that the\noutput zeroing manifold of the augmented system can be made attractive by the\nproposed nonadaptive control law, which solves the robust output regulation\nproblem. Finally, we demonstrate the effectiveness of the proposed nonadaptive\ninternal model approach by its application to the control of the Duffing\nsystem.", "AI": {"tldr": "This paper investigates the robust output regulation problem of second-order nonlinear uncertain systems with an unknown exosystem.", "motivation": "To solve the robust output regulation problem without using adaptive control approach, but resorting to a robust control methodology to avoid bursting phenomenon.", "method": "Construct generic internal models for steady-state state and input variables. Convert the robust output regulation problem into a nonadaptive stabilization problem through coordinate transformation. Design stabilization control law and construct strict Lyapunov function.", "result": "The output zeroing manifold of the augmented system can be made attractive by the proposed nonadaptive control law.", "conclusion": "The effectiveness of the proposed nonadaptive internal model approach is demonstrated by its application to the control of the Duffing system."}}
{"id": "2505.22474", "pdf": "https://arxiv.org/pdf/2505.22474", "abs": "https://arxiv.org/abs/2505.22474", "authors": ["Amirhossein Sohrabbeig", "Omid Ardakanian", "Petr Musilek"], "title": "Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis", "categories": ["cs.LG"], "comment": null, "summary": "The forecasting of multivariate urban data presents a complex challenge due\nto the intricate dependencies between various urban metrics such as weather,\nair pollution, carbon intensity, and energy demand. This paper introduces a\nnovel multivariate time-series forecasting model that utilizes advanced Graph\nNeural Networks (GNNs) to capture spatial dependencies among different\ntime-series variables. The proposed model incorporates a decomposition-based\npreprocessing step, isolating trend, seasonal, and residual components to\nenhance the accuracy and interpretability of forecasts. By leveraging the\ndynamic capabilities of GNNs, the model effectively captures interdependencies\nand improves the forecasting performance. Extensive experiments on real-world\ndatasets, including electricity usage, weather metrics, carbon intensity, and\nair pollution data, demonstrate the effectiveness of the proposed approach\nacross various forecasting scenarios. The results highlight the potential of\nthe model to optimize smart infrastructure systems, contributing to\nenergy-efficient urban development and enhanced public well-being.", "AI": {"tldr": "This paper presents a new multivariate time-series forecasting model using Graph Neural Networks (GNNs) to predict urban data such as weather, air pollution, carbon intensity, and energy demand. The model incorporates decomposition-based preprocessing to isolate trend, seasonal, and residual components. Experiments on real-world datasets show the model's effectiveness in various forecasting scenarios, contributing to energy-efficient urban development.", "motivation": "The motivation behind this research is the complexity of forecasting multivariate urban data due to intricate dependencies between various urban metrics like weather, air pollution, carbon intensity, and energy demand.", "method": "The method involves developing a novel multivariate time-series forecasting model that leverages advanced Graph Neural Networks (GNNs) to capture spatial dependencies among different time-series variables. A decomposition-based preprocessing step isolates trend, seasonal, and residual components to improve forecast accuracy and interpretability.", "result": "The experiments conducted on real-world datasets including electricity usage, weather metrics, carbon intensity, and air pollution data demonstrate the effectiveness of the proposed approach across various forecasting scenarios.", "conclusion": "The conclusion drawn from the results is that the proposed model has the potential to optimize smart infrastructure systems, leading to energy-efficient urban development and enhanced public well-being."}}
{"id": "2505.22475", "pdf": "https://arxiv.org/pdf/2505.22475", "abs": "https://arxiv.org/abs/2505.22475", "authors": ["Riccardo Poiani", "Martino Bernasconi", "Andrea Celli"], "title": "Non-Asymptotic Analysis of (Sticky) Track-and-Stop", "categories": ["cs.LG"], "comment": null, "summary": "In pure exploration problems, a statistician sequentially collects\ninformation to answer a question about some stochastic and unknown environment.\nThe probability of returning a wrong answer should not exceed a maximum risk\nparameter $\\delta$ and good algorithms make as few queries to the environment\nas possible. The Track-and-Stop algorithm is a pioneering method to solve these\nproblems. Specifically, it is well-known that it enjoys asymptotic optimality\nsample complexity guarantees for $\\delta\\to 0$ whenever the map from the\nenvironment to its correct answers is single-valued (e.g., best-arm\nidentification with a unique optimal arm). The Sticky Track-and-Stop algorithm\nextends these results to settings where, for each environment, there might\nexist multiple correct answers (e.g., $\\epsilon$-optimal arm identification).\nAlthough both methods are optimal in the asymptotic regime, their\nnon-asymptotic guarantees remain unknown. In this work, we fill this gap and\nprovide non-asymptotic guarantees for both algorithms.", "AI": {"tldr": "In pure exploration problems, the Track-and-Stop algorithm and its extension, Sticky Track-and-Stop, are analyzed for their non-asymptotic guarantees.", "motivation": "To provide non-asymptotic guarantees for the Track-and-Stop and Sticky Track-and-Stop algorithms in pure exploration problems where a statistician sequentially collects information to answer questions about an unknown environment with a maximum risk parameter \u03b4.", "method": "Analyzing the Track-and-Stop algorithm which is known for asymptotic optimality sample complexity guarantees when \u03b4\u21920 under single-valued mappings, and its extension Sticky Track-and-Stop for environments with multiple correct answers.", "result": "Provided non-asymptotic guarantees for both Track-and-Stop and Sticky Track-and-Stop algorithms.", "conclusion": "The study successfully fills the gap by offering non-asymptotic performance guarantees for both algorithms used in pure exploration problems."}}
{"id": "2505.21847", "pdf": "https://arxiv.org/pdf/2505.21847", "abs": "https://arxiv.org/abs/2505.21847", "authors": ["Xuwei Xu", "Yang Li", "Yudong Chen", "Jiajun Liu", "Sen Wang"], "title": "RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICML2025", "summary": "We reveal that feedforward network (FFN) layers, rather than attention\nlayers, are the primary contributors to Vision Transformer (ViT) inference\nlatency, with their impact signifying as model size increases. This finding\nhighlights a critical opportunity for optimizing the efficiency of large-scale\nViTs by focusing on FFN layers. In this work, we propose a novel channel idle\nmechanism that facilitates post-training structural reparameterization for\nefficient FFN layers during testing. Specifically, a set of feature channels\nremains idle and bypasses the nonlinear activation function in each FFN layer,\nthereby forming a linear pathway that enables structural reparameterization\nduring inference. This mechanism results in a family of ReParameterizable\nVision Transformers (RePaViTs), which achieve remarkable latency reductions\nwith acceptable sacrifices (sometimes gains) in accuracy across various ViTs.\nThe benefits of our method scale consistently with model sizes, demonstrating\ngreater speed improvements and progressively narrowing accuracy gaps or even\nhigher accuracies on larger models. In particular, RePa-ViT-Large and\nRePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1\naccuracies under the same training strategy, respectively. RePaViT is the first\nto employ structural reparameterization on FFN layers to expedite ViTs to our\nbest knowledge, and we believe that it represents an auspicious direction for\nefficient ViTs. Source code is available at\nhttps://github.com/Ackesnal/RePaViT.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u901a\u9053\u7a7a\u95f2\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u65b0\u53c2\u6570\u5316\u7684\u89c6\u89c9\u53d8\u6362\u5668\uff08RePaViT\uff09\uff0c\u5728\u4fdd\u6301\u8f83\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u53d1\u73b0\u524d\u9988\u7f51\u7edc(FFN)\u5c42\u800c\u975e\u6ce8\u610f\u529b\u5c42\u662f\u5bfc\u81f4\u89c6\u89c9\u53d8\u6362\u5668(ViT)\u63a8\u7406\u5ef6\u8fdf\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4e14\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\u5f71\u54cd\u66f4\u663e\u8457\uff0c\u8fd9\u4e3a\u4f18\u5316\u5927\u89c4\u6a21ViT\u7684\u6548\u7387\u63d0\u4f9b\u4e86\u5173\u952e\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u9053\u7a7a\u95f2\u673a\u5236\uff0c\u5728\u6d4b\u8bd5\u65f6\u5bf9FFN\u5c42\u8fdb\u884c\u540e\u8bad\u7ec3\u7ed3\u6784\u91cd\u65b0\u53c2\u6570\u5316\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u90e8\u5206\u7279\u5f81\u901a\u9053\u4fdd\u6301\u7a7a\u95f2\u5e76\u8df3\u8fc7\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u5f62\u6210\u7ebf\u6027\u8def\u5f84\u4ee5\u652f\u6301\u63a8\u7406\u65f6\u7684\u7ed3\u6784\u91cd\u65b0\u53c2\u6570\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u89c4\u6a21\u7684ViT\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5e76\u5728\u8f83\u5927\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u901f\u5ea6\u63d0\u5347\u548c\u9010\u6e10\u7f29\u5c0f\u7684\u7cbe\u5ea6\u5dee\u8ddd\uff0c\u751a\u81f3\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u4f8b\u5982\uff0cRePa-ViT-Large\u548cRePa-ViT-Huge\u5206\u522b\u63d0\u5347\u4e8666.8%\u548c68.7%\u7684\u901f\u5ea6\uff0c\u540c\u65f6top-1\u7cbe\u5ea6\u5206\u522b\u63d0\u9ad8\u4e861.7%\u548c1.1%\u3002", "conclusion": "RePaViT\u662f\u9996\u4e2a\u5229\u7528\u7ed3\u6784\u91cd\u65b0\u53c2\u6570\u5316\u52a0\u901fViT\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u9ad8\u6548ViT\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2505.22483", "pdf": "https://arxiv.org/pdf/2505.22483", "abs": "https://arxiv.org/abs/2505.22483", "authors": ["Abhra Chaudhuri", "Anjan Dutta", "Tu Bui", "Serban Georgescu"], "title": "A Closer Look at Multimodal Representation Collapse", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "International Conference on Machine Learning (ICML) 2025 (Spotlight)", "summary": "We aim to develop a fundamental understanding of modality collapse, a\nrecently observed empirical phenomenon wherein models trained for multimodal\nfusion tend to rely only on a subset of the modalities, ignoring the rest. We\nshow that modality collapse happens when noisy features from one modality are\nentangled, via a shared set of neurons in the fusion head, with predictive\nfeatures from another, effectively masking out positive contributions from the\npredictive features of the former modality and leading to its collapse. We\nfurther prove that cross-modal knowledge distillation implicitly disentangles\nsuch representations by freeing up rank bottlenecks in the student encoder,\ndenoising the fusion-head outputs without negatively impacting the predictive\nfeatures from either modality. Based on the above findings, we propose an\nalgorithm that prevents modality collapse through explicit basis reallocation,\nwith applications in dealing with missing modalities. Extensive experiments on\nmultiple multimodal benchmarks validate our theoretical claims. Project page:\nhttps://abhrac.github.io/mmcollapse/.", "AI": {"tldr": "The paper explores modality collapse in multimodal fusion models, identifies its cause as entanglement of noisy and predictive features, proves cross-modal knowledge distillation can disentangle them, and proposes an algorithm to prevent modality collapse via basis reallocation.", "motivation": "To develop a fundamental understanding of modality collapse, a phenomenon where multimodal fusion models tend to rely only on a subset of modalities while ignoring others.", "method": "Show that modality collapse occurs due to entanglement of noisy features from one modality with predictive features from another via shared neurons. Prove that cross-modal knowledge distillation disentangles these representations by freeing rank bottlenecks in the student encoder. Propose an algorithm for preventing modality collapse through explicit basis reallocation.", "result": "Extensive experiments on multiple multimodal benchmarks validate the theoretical claims about preventing modality collapse and handling missing modalities.", "conclusion": "Modality collapse is caused by feature entanglement, cross-modal knowledge distillation can help disentangle them, and the proposed algorithm effectively prevents modality collapse."}}
{"id": "2505.21849", "pdf": "https://arxiv.org/pdf/2505.21849", "abs": "https://arxiv.org/abs/2505.21849", "authors": ["Bo Tang", "Junyi Zhu", "Chenyang Xi", "Yunhang Ge", "Jiahao Wu", "Yuchen Feng", "Yijun Niu", "Wenqiang Wei", "Yu Yu", "Chunyu Li", "Zehao Lin", "Hao Wu", "Ning Liao", "Yebin Yang", "Jiajia Wang", "Zhiyu Li", "Feiyu Xiong", "Jingrun Chen"], "title": "Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Traditional search engines struggle to synthesize fragmented information for\ncomplex queries, while generative AI search engines face challenges in\nrelevance, comprehensiveness, and presentation. To address these limitations,\nwe introduce Xinyu AI Search, a novel system that incorporates a\nquery-decomposition graph to dynamically break down complex queries into\nsub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline\nenhances diversity through multi-source aggregation and query expansion, while\nfiltering and re-ranking strategies optimize passage relevance. Additionally,\nXinyu AI Search introduces a novel approach for fine-grained, precise built-in\ncitation and innovates in result presentation by integrating timeline\nvisualization and textual-visual choreography. Evaluated on recent real-world\nqueries, Xinyu AI Search outperforms eight existing technologies in human\nassessments, excelling in relevance, comprehensiveness, and insightfulness.\nAblation studies validate the necessity of its key sub-modules. Our work\npresents the first comprehensive framework for generative AI search engines,\nbridging retrieval, generation, and user-centric presentation.", "AI": {"tldr": "Xinyu AI Search is a new system that breaks down complex queries into sub-queries for better retrieval and generation, enhances diversity through multi-source aggregation and query expansion, optimizes passage relevance, introduces fine-grained built-in citation, and innovates in result presentation. It outperforms eight existing technologies in human assessments.", "motivation": "Traditional search engines struggle with complex queries while generative AI search engines have challenges in relevance, comprehensiveness, and presentation.", "method": "Introduced Xinyu AI Search which uses a query-decomposition graph to break down complex queries, enhances retrieval pipeline diversity, applies filtering and re-ranking strategies, and innovates in result presentation with timeline visualization and textual-visual choreography.", "result": "Xinyu AI Search excels in relevance, comprehensiveness, and insightfulness when evaluated on real-world queries and ablation studies confirm the importance of its key sub-modules.", "conclusion": "This work provides the first comprehensive framework for generative AI search engines integrating retrieval, generation, and user-focused presentation."}}
{"id": "2505.22486", "pdf": "https://arxiv.org/pdf/2505.22486", "abs": "https://arxiv.org/abs/2505.22486", "authors": ["Mujtaba Hussain Mirza", "Maria Rosaria Briglia", "Filippo Bartolucci", "Senad Beadini", "Giuseppe Lisanti", "Iacopo Masi"], "title": "Understanding Adversarial Training with Energy-based Models", "categories": ["cs.LG", "cs.CV"], "comment": "Under review for TPAMI", "summary": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models.", "AI": {"tldr": "This paper explores adversarial training in classifiers using an Energy-based Model (EBM) framework. It introduces the Delta Energy Regularizer (DER) to address issues of Catastrophic Overfitting and Robust Overfitting, and proposes a technique based on local class-wise PCA for improving sample diversity and generation quality.", "motivation": "To better understand adversarial training in classifiers and analyze the intrinsic generative capabilities of robust classifiers using the EBM framework.", "method": "Analyze adversarial examples through energy lens, study overfitting phenomena (CO & RO) via energy dynamics, propose DER regularizer, and use local class-wise PCA with energy-based guidance for generative improvements.", "result": "DER effectively mitigates CO and RO across benchmarks; proposed generative technique achieves competitive IS and FID scores without explicit generative model training.", "conclusion": "Energy-based analysis provides insights into adversarial training and robust classifier's generative limitations; DER and improved generative techniques enhance performance."}}
{"id": "2505.21850", "pdf": "https://arxiv.org/pdf/2505.21850", "abs": "https://arxiv.org/abs/2505.21850", "authors": ["Yanbei Jiang", "Yihao Ding", "Chao Lei", "Jiayang Ao", "Jey Han Lau", "Krista A. Ehinger"], "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ACL Findings", "summary": "Current Multimodal Large Language Models (MLLMs) excel in general visual\nreasoning but remain underexplored in Abstract Visual Reasoning (AVR), which\ndemands higher-order reasoning to identify abstract rules beyond simple\nperception. Existing AVR benchmarks focus on single-step reasoning, emphasizing\nthe end result but neglecting the multi-stage nature of reasoning process. Past\nstudies found MLLMs struggle with these benchmarks, but it doesn't explain how\nthey fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR\nbenchmark, based on RAVEN, designed to assess reasoning across varying levels\nof complexity. Additionally, existing metrics like accuracy only focus on the\nfinal outcomes while do not account for the correctness of intermediate steps.\nTherefore, we propose a novel metric, MSEval, which considers the correctness\nof intermediate steps in addition to the final outcomes. We conduct\ncomprehensive experiments on MultiStAR using 17 representative close-source and\nopen-source MLLMs. The results reveal that while existing MLLMs perform\nadequately on basic perception tasks, they continue to face challenges in more\ncomplex rule detection stages.", "AI": {"tldr": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u901a\u7528\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\uff08AVR\uff09\u65b9\u9762\u4ecd\u5f85\u6df1\u5165\u7814\u7a76\u3002\u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u9636\u6bb5\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u672c\u6587\u63d0\u51fa\u4e86MultiStAR\u57fa\u51c6\u548cMSEval\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u89c4\u5219\u68c0\u6d4b\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e00\u822c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u9ad8\u5c42\u6b21\u63a8\u7406\u7684\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\uff08AVR\uff09\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684AVR\u57fa\u51c6\u4ec5\u5173\u6ce8\u5355\u6b65\u63a8\u7406\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u591a\u9636\u6bb5\u6027\uff0c\u4e14\u8bc4\u4ef7\u6307\u6807\u5982\u51c6\u786e\u7387\u4ec5\u5173\u6ce8\u6700\u7ec8\u7ed3\u679c\uff0c\u65e0\u6cd5\u53cd\u6620\u4e2d\u95f4\u6b65\u9aa4\u7684\u6b63\u786e\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u65b0\u7684\u57fa\u51c6\u548c\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eRAVEN\u7684\u591a\u9636\u6bb5\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\uff08MultiStAR\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u6c34\u5e73\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u3002\n2. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u578b\u8bc4\u4ef7\u6307\u6807MSEval\uff0c\u4e0d\u4ec5\u8003\u8651\u6700\u7ec8\u7ed3\u679c\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u4e2d\u95f4\u6b65\u9aa4\u7684\u6b63\u786e\u6027\u3002\n3. \u5728MultiStAR\u4e0a\u5bf917\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u95ed\u6e90\u548c\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u7840\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u66f4\u590d\u6742\u7684\u89c4\u5219\u68c0\u6d4b\u9636\u6bb5\u4ecd\u9762\u4e34\u6311\u6218\u3002\u8fd9\u8bf4\u660e\u6a21\u578b\u5728\u9ad8\u5c42\u6b21\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51faMultiStAR\u57fa\u51c6\u548cMSEval\u6307\u6807\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002\u6539\u8fdb\u6a21\u578b\u7684\u591a\u9636\u6bb5\u63a8\u7406\u80fd\u529b\u548c\u8bbe\u8ba1\u66f4\u5168\u9762\u7684\u8bc4\u4ef7\u6307\u6807\u5c06\u662f\u63d0\u5347\u5176\u62bd\u8c61\u63a8\u7406\u6027\u80fd\u7684\u5173\u952e\u3002"}}
{"id": "2505.22491", "pdf": "https://arxiv.org/pdf/2505.22491", "abs": "https://arxiv.org/abs/2505.22491", "authors": ["Moritz Haas", "Sebastian Bordt", "Ulrike von Luxburg", "Leena Chennuru Vankadara"], "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The dominant paradigm for training large-scale vision and language models is\nHe initialization and a single global learning rate (\\textit{standard\nparameterization}, SP). Despite its practical success, standard parametrization\nremains poorly understood from a theoretical perspective: Existing\ninfinite-width theory would predict instability under large learning rates and\nvanishing feature learning under stable learning rates. However, empirically\noptimal learning rates consistently decay much slower than theoretically\npredicted. By carefully studying neural network training dynamics, we\ndemonstrate that this discrepancy is not fully explained by finite-width\nphenomena such as catapult effects or a lack of alignment between weights and\nincoming activations. We instead show that the apparent contradiction can be\nfundamentally resolved by taking the loss function into account: In contrast to\nMean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an\nintermediate \\textit{controlled divergence} regime emerges, where logits\ndiverge but loss, gradients, and activations remain stable. Stable training\nunder large learning rates enables persistent feature evolution at scale in all\nhidden layers, which is crucial for the practical success of SP. In experiments\nacross optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities\n(vision, language), we validate that neural networks operate in this controlled\ndivergence regime under CE loss but not under MSE loss. Our empirical evidence\nsuggests that width-scaling considerations are surprisingly useful for\npredicting empirically optimal learning rate exponents. Finally, our analysis\nclarifies the effectiveness and limitations of recently proposed layerwise\nlearning rate scalings for standard initialization.", "AI": {"tldr": "The paper explores why standard parameterization (SP) works well in practice despite theoretical predictions of instability. It finds that the discrepancy between theory and practice can be resolved by considering the loss function, specifically cross-entropy (CE) loss, which allows for a 'controlled divergence' regime where training remains stable even with large learning rates.", "motivation": "There is a gap between the practical success of standard parameterization (He initialization and a single global learning rate) and its theoretical understanding. Existing infinite-width theory predicts instability under large learning rates and vanishing feature learning under stable learning rates, yet empirically optimal learning rates decay much slower than predicted.", "method": "The authors study neural network training dynamics under different loss functions (cross-entropy vs mean squared error). They prove theoretically and validate empirically that under cross-entropy loss, an intermediate 'controlled divergence' regime emerges where logits diverge but loss, gradients, and activations remain stable. This enables persistent feature evolution in all hidden layers.", "result": "Neural networks operate in a controlled divergence regime under cross-entropy loss across different optimizers (SGD, Adam), architectures (MLPs, GPT), and data modalities (vision, language). This explains the practical success of standard parameterization. Width-scaling considerations are useful for predicting optimal learning rate exponents.", "conclusion": "The analysis clarifies the effectiveness and limitations of layerwise learning rate scalings for standard initialization. The findings highlight the importance of considering the loss function in understanding training dynamics and optimizing learning rates."}}
{"id": "2505.21851", "pdf": "https://arxiv.org/pdf/2505.21851", "abs": "https://arxiv.org/abs/2505.21851", "authors": ["Sunshine Jiang", "Xiaolin Fang", "Nicholas Roy", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling", "Siddharth Ancha"], "title": "Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "ICRA 2025 Beyond Pick and Place Workshop", "summary": "Recent advances in diffusion$/$flow-matching policies have enabled imitation\nlearning of complex, multi-modal action trajectories. However, they are\ncomputationally expensive because they sample a trajectory of trajectories: a\ndiffusion$/$flow trajectory of action trajectories. They discard intermediate\naction trajectories, and must wait for the sampling process to complete before\nany actions can be executed on the robot. We simplify diffusion$/$flow policies\nby treating action trajectories as flow trajectories. Instead of starting from\npure noise, our algorithm samples from a narrow Gaussian around the last\naction. Then, it incrementally integrates a velocity field learned via flow\nmatching to produce a sequence of actions that constitute a single trajectory.\nThis enables actions to be streamed to the robot on-the-fly during the flow\nsampling process, and is well-suited for receding horizon policy execution.\nDespite streaming, our method retains the ability to model multi-modal\nbehavior. We train flows that stabilize around demonstration trajectories to\nreduce distribution shift and improve imitation learning performance. Streaming\nflow policy outperforms prior methods while enabling faster policy execution\nand tighter sensorimotor loops for learning-based robot control. Project\nwebsite: https://streaming-flow-policy.github.io/", "AI": {"tldr": "A new method called streaming flow policy simplifies diffusion/flow policies by treating action trajectories as flow trajectories, enabling faster policy execution and tighter sensorimotor loops for learning-based robot control.", "motivation": "Recent advances in diffusion/flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, these methods are computationally expensive due to their sampling process.", "method": "The algorithm samples from a narrow Gaussian around the last action and incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a single trajectory. This allows actions to be streamed to the robot on-the-fly during the flow sampling process.", "result": "Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control.", "conclusion": "This method retains the ability to model multi-modal behavior and trains flows that stabilize around demonstration trajectories to reduce distribution shift and improve imitation learning performance."}}
{"id": "2505.22492", "pdf": "https://arxiv.org/pdf/2505.22492", "abs": "https://arxiv.org/abs/2505.22492", "authors": ["Hongyi Zhou", "Josiah P. Hanna", "Jin Zhu", "Ying Yang", "Chengchun Shi"], "title": "Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "This paper studies off-policy evaluation (OPE) in reinforcement learning with\na focus on behavior policy estimation for importance sampling. Prior work has\nshown empirically that estimating a history-dependent behavior policy can lead\nto lower mean squared error (MSE) even when the true behavior policy is\nMarkovian. However, the question of why the use of history should lower MSE\nremains open. In this paper, we theoretically demystify this paradox by\nderiving a bias-variance decomposition of the MSE of ordinary importance\nsampling (IS) estimators, demonstrating that history-dependent behavior policy\nestimation decreases their asymptotic variances while increasing their\nfinite-sample biases. Additionally, as the estimated behavior policy conditions\non a longer history, we show a consistent decrease in variance. We extend these\nfindings to a range of other OPE estimators, including the sequential IS\nestimator, the doubly robust estimator and the marginalized IS estimator, with\nthe behavior policy estimated either parametrically or non-parametrically.", "AI": {"tldr": "\u5728\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30(OPE)\u4e2d\uff0c\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u867d\u7136\u589e\u52a0\u4e86\u6709\u9650\u6837\u672c\u7684\u504f\u5dee\uff0c\u4f46\u964d\u4f4e\u4e86\u6e10\u8fd1\u65b9\u5dee\uff0c\u4ece\u800c\u4f7f\u5747\u65b9\u8bef\u5dee(MSE)\u964d\u4f4e\u3002\u6b64\u7814\u7a76\u901a\u8fc7\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\u89e3\u91ca\u4e86\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u6269\u5c55\u5230\u4e86\u5176\u4ed6OPE\u4f30\u8ba1\u5668\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u771f\u5b9e\u7684\u884c\u4e3a\u7b56\u7565\u662f\u9a6c\u5c14\u53ef\u592b\u7684\uff0c\u4f7f\u7528\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u53ef\u4ee5\u964d\u4f4e\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\uff0c\u4f46\u4e3a\u4f55\u4f7f\u7528\u5386\u53f2\u80fd\u964d\u4f4eMSE\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7406\u8bba\u4e0a\u63a8\u5bfc\u4e86\u666e\u901a\u91cd\u8981\u6027\u91c7\u6837\uff08IS\uff09\u4f30\u8ba1\u5668\u7684MSE\u7684\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\uff0c\u8bc1\u660e\u4e86\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u4f1a\u964d\u4f4e\u5176\u6e10\u8fd1\u65b9\u5dee\uff0c\u540c\u65f6\u589e\u52a0\u6709\u9650\u6837\u672c\u7684\u504f\u5dee\u3002\u968f\u7740\u4f30\u8ba1\u7684\u884c\u4e3a\u7b56\u7565\u5bf9\u66f4\u957f\u5386\u53f2\u7684\u6761\u4ef6\u4f5c\u7528\uff0c\u5c55\u793a\u4e86\u65b9\u5dee\u7684\u4e00\u81f4\u51cf\u5c11\u3002\u5e76\u5c06\u8fd9\u4e9b\u53d1\u73b0\u6269\u5c55\u5230\u4e00\u7cfb\u5217\u5176\u4ed6OPE\u4f30\u8ba1\u5668\uff0c\u5305\u62ec\u987a\u5e8fIS\u4f30\u8ba1\u5668\u3001\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u5668\u548c\u8fb9\u7f18\u5316IS\u4f30\u8ba1\u5668\uff0c\u884c\u4e3a\u7b56\u7565\u53ef\u4ee5\u53c2\u6570\u5316\u6216\u975e\u53c2\u6570\u5316\u4f30\u8ba1\u3002", "result": "\u63ed\u793a\u4e86\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u5728\u964d\u4f4eMSE\u4e2d\u7684\u4f5c\u7528\u673a\u5236\uff1a\u5c3d\u7ba1\u6709\u9650\u6837\u672c\u7684\u504f\u5dee\u6709\u6240\u589e\u52a0\uff0c\u4f46\u6e10\u8fd1\u65b9\u5dee\u663e\u8457\u964d\u4f4e\u3002\u5e76\u4e14\u968f\u7740\u6761\u4ef6\u4e8e\u66f4\u957f\u7684\u5386\u53f2\uff0c\u65b9\u5dee\u6301\u7eed\u51cf\u5c11\u3002", "conclusion": "\u672c\u7814\u7a76\u7406\u8bba\u89e3\u91ca\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u7684\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u4e2d\uff0c\u5386\u53f2\u4f9d\u8d56\u7684\u884c\u4e3a\u7b56\u7565\u4f30\u8ba1\u80fd\u591f\u964d\u4f4eMSE\u7684\u539f\u56e0\uff0c\u4e3a\u672a\u6765\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u5316OPE\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2505.22494", "pdf": "https://arxiv.org/pdf/2505.22494", "abs": "https://arxiv.org/abs/2505.22494", "authors": ["Michal Kmicikiewicz", "Vincent Fortuin", "Ewa Szczurek"], "title": "ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods", "categories": ["cs.LG"], "comment": null, "summary": "Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty.", "AI": {"tldr": "ProSpero is an active learning framework that combines a frozen pre-trained generative model with a surrogate updated from oracle feedback to design protein sequences of high fitness and novelty.", "motivation": "Designing protein sequences with both high fitness and novelty is challenging due to the risk of generating biologically implausible sequences or relying on surrogate models that lose fidelity in novel regions.", "method": "ProSpero uses a frozen pre-trained generative model guided by a surrogate updated from oracle feedback. It integrates fitness-relevant residue selection with biologically-constrained Sequential Monte Carlo sampling to explore beyond wild-type neighborhoods while preserving biological plausibility.", "result": "ProSpero remains effective even when the surrogate is misspecified and consistently outperforms or matches existing methods across diverse protein engineering tasks, retrieving sequences of both high fitness and novelty.", "conclusion": "ProSpero provides a robust approach for designing protein sequences with high fitness and novelty, overcoming limitations of previous methods."}}
{"id": "2505.21854", "pdf": "https://arxiv.org/pdf/2505.21854", "abs": "https://arxiv.org/abs/2505.21854", "authors": ["Jun Chen", "Xinke Li", "Mingyue Xu", "Tianrui Li", "Chongshou Li"], "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance.", "AI": {"tldr": "The paper proposes WAAttack and SubAttack to improve gradient-based adversarial attacks on 3D point cloud classification by considering the heterogeneous nature of point clouds.", "motivation": "Gradient-based adversarial attacks are widely used for evaluating the robustness of point cloud classification models, but existing methods have limitations in considering the heterogeneous nature of point clouds, leading to perceptible perturbations.", "method": "The paper introduces WAAttack which uses weighted gradients and adaptive step-size strategy, and SubAttack which focuses on structurally critical regions of the point cloud.", "result": "Extensive experiments show that the proposed approach outperforms state-of-the-art baselines in generating highly imperceptible adversarial examples.", "conclusion": "WAAttack and SubAttack provide a principled rethinking of gradient-based adversarial attacks for 3D point cloud classification."}}
{"id": "2505.22504", "pdf": "https://arxiv.org/pdf/2505.22504", "abs": "https://arxiv.org/abs/2505.22504", "authors": ["Ahmed Hossam Mohammed", "Kishansingh Rajput", "Simon Taylor", "Denis Furletov", "Sergey Furletov", "Malachi Schram"], "title": "Geometric GNNs for Charged Particle Tracking at GlueX", "categories": ["cs.LG"], "comment": null, "summary": "Nuclear physics experiments are aimed at uncovering the fundamental building\nblocks of matter. The experiments involve high-energy collisions that produce\ncomplex events with many particle trajectories. Tracking charged particles\nresulting from collisions in the presence of a strong magnetic field is\ncritical to enable the reconstruction of particle trajectories and precise\ndetermination of interactions. It is traditionally achieved through\ncombinatorial approaches that scale worse than linearly as the number of hits\ngrows. Since particle hit data naturally form a 3-dimensional point cloud and\ncan be structured as graphs, Graph Neural Networks (GNNs) emerge as an\nintuitive and effective choice for this task. In this study, we evaluate the\nGNN model for track finding on the data from the GlueX experiment at Jefferson\nLab. We use simulation data to train the model and test on both simulation and\nreal GlueX measurements. We demonstrate that GNN-based track finding\noutperforms the currently used traditional method at GlueX in terms of\nsegment-based efficiency at a fixed purity while providing faster inferences.\nWe show that the GNN model can achieve significant speedup by processing\nmultiple events in batches, which exploits the parallel computation capability\nof Graphical Processing Units (GPUs). Finally, we compare the GNN\nimplementation on GPU and FPGA and describe the trade-off.", "AI": {"tldr": "The paper explores the application of Graph Neural Networks (GNNs) for track finding in nuclear physics experiments, demonstrating their superior performance and efficiency compared to traditional methods.", "motivation": "To improve the tracking of charged particles resulting from high-energy collisions, which is critical for reconstructing particle trajectories and determining interactions. Traditional combinatorial approaches scale poorly as the number of hits grows.", "method": "Evaluate GNN models for track finding using data from the GlueX experiment. Train on simulation data and test on both simulation and real measurements. Compare GNN-based track finding with traditional methods in terms of efficiency, purity, and inference speed. Also compare GNN implementation on GPU and FPGA.", "result": "GNN-based track finding outperforms traditional methods in segment-based efficiency at a fixed purity while providing faster inferences. Significant speedup can be achieved by processing multiple events in batches on GPUs.", "conclusion": "GNNs are an intuitive and effective choice for track finding in nuclear physics experiments, offering improved performance and efficiency."}}
{"id": "2505.21855", "pdf": "https://arxiv.org/pdf/2505.21855", "abs": "https://arxiv.org/abs/2505.21855", "authors": ["Jiseung Yoo", "Curran Mahowald", "Meiyu Li", "Wei Ai"], "title": "Extracting Research Instruments from Educational Literature Using LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are transforming information extraction from\nacademic literature, offering new possibilities for knowledge management. This\nstudy presents an LLM-based system designed to extract detailed information\nabout research instruments used in the education field, including their names,\ntypes, target respondents, measured constructs, and outcomes. Using multi-step\nprompting and a domain-specific data schema, it generates structured outputs\noptimized for educational research. Our evaluation shows that this system\nsignificantly outperforms other approaches, particularly in identifying\ninstrument names and detailed information. This demonstrates the potential of\nLLM-powered information extraction in educational contexts, offering a\nsystematic way to organize research instrument information. The ability to\naggregate such information at scale enhances accessibility for researchers and\neducation leaders, facilitating informed decision-making in educational\nresearch and policy.", "AI": {"tldr": "The study develops an LLM-based system for extracting detailed information about research instruments in education, outperforming other methods and enhancing accessibility for researchers and leaders.", "motivation": "To create a systematic way to organize research instrument information in the education field using Large Language Models (LLMs).", "method": "Designing an LLM-based system with multi-step prompting and a domain-specific data schema to extract and structure details about research instruments.", "result": "The system significantly outperforms other approaches in identifying instrument names and detailed information.", "conclusion": "LLM-powered information extraction has great potential in educational contexts, improving accessibility and supporting decision-making in research and policy."}}
{"id": "2505.22506", "pdf": "https://arxiv.org/pdf/2505.22506", "abs": "https://arxiv.org/abs/2505.22506", "authors": ["Wenjie Sun", "Bingzhe Wu", "Zhile Yang", "Chengke Wu"], "title": "Sparsification and Reconstruction from the Perspective of Representation Geometry", "categories": ["cs.LG", "22-08", "I.2.4; I.2.7"], "comment": "24 pages, 5 figures", "summary": "Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic\ninterpretability, aiming to identify interpretable monosemantic features.\nHowever, how does sparse encoding organize the representations of activation\nvector from language models? What is the relationship between this\norganizational paradigm and feature disentanglement as well as reconstruction\nperformance? To address these questions, we propose the SAEMA, which validates\nthe stratified structure of the representation by observing the variability of\nthe rank of the symmetric semipositive definite (SSPD) matrix corresponding to\nthe modal tensor unfolded along the latent tensor with the level of noise added\nto the residual stream. To systematically investigate how sparse encoding\nalters representational structures, we define local and global representations,\ndemonstrating that they amplify inter-feature distinctions by merging similar\nsemantic features and introducing additional dimensionality. Furthermore, we\nintervene the global representation from an optimization perspective, proving a\nsignificant causal relationship between their separability and the\nreconstruction performance. This study explains the principles of sparsity from\nthe perspective of representational geometry and demonstrates the impact of\nchanges in representational structure on reconstruction performance.\nParticularly emphasizes the necessity of understanding representations and\nincorporating representational constraints, providing empirical references for\ndeveloping new interpretable tools and improving SAEs. The code is available at\n\\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.", "AI": {"tldr": "Sparse Autoencoders (SAEs) are key in mechanistic interpretability. This paper introduces SAEMA to explore how sparse encoding organizes representations, defines local and global representations, proves a causal relationship between separability and reconstruction performance, and emphasizes the importance of understanding representations.", "motivation": "To understand how sparse encoding organizes representations from language models and its relationship with feature disentanglement and reconstruction performance.", "method": "Propose SAEMA which observes variability of rank of SSPD matrix unfolded along latent tensor with noise added to residual stream. Defines local and global representations that amplify inter-feature distinctions.", "result": "Proves significant causal relationship between separability of global representation and reconstruction performance.", "conclusion": "Explains principles of sparsity from representational geometry perspective, highlights necessity of understanding representations, and provides empirical references for developing new interpretable tools."}}
{"id": "2505.21866", "pdf": "https://arxiv.org/pdf/2505.21866", "abs": "https://arxiv.org/abs/2505.21866", "authors": ["Guozhen Zhu", "Yuqian Hu", "Weihang Gao", "Wei-Hsiang Wang", "Beibei Wang", "K. J. Ray Liu"], "title": "CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing", "categories": ["eess.SP", "cs.AI", "cs.DB"], "comment": "21 pages, 4 figures", "summary": "WiFi sensing has emerged as a compelling contactless modality for human\nactivity monitoring by capturing fine-grained variations in Channel State\nInformation (CSI). Its ability to operate continuously and non-intrusively\nwhile preserving user privacy makes it particularly suitable for health\nmonitoring. However, existing WiFi sensing systems struggle to generalize in\nreal-world settings, largely due to datasets collected in controlled\nenvironments with homogeneous hardware and fragmented, session-based recordings\nthat fail to reflect continuous daily activity.\n  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected\nusing commercial WiFi edge devices across 26 diverse indoor environments with\n35 real users. Spanning over 461 hours of effective data, CSI-Bench captures\nrealistic signal variability under natural conditions. It includes\ntask-specific datasets for fall detection, breathing monitoring, localization,\nand motion source recognition, as well as a co-labeled multitask dataset with\njoint annotations for user identity, activity, and proximity. To support the\ndevelopment of robust and generalizable models, CSI-Bench provides standardized\nevaluation splits and baseline results for both single-task and multi-task\nlearning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi\nsensing systems in health and broader human-centric applications.", "AI": {"tldr": "The paper introduces CSI-Bench, a large-scale WiFi sensing dataset collected in real-world environments to address generalization challenges in human activity monitoring. It includes task-specific datasets and co-labeled data for multi-task learning, providing standardized evaluation splits and baseline results.", "motivation": "Existing WiFi sensing systems lack generalization in real-world settings due to controlled environment data collection with homogeneous hardware and fragmented recordings.", "method": "CSI-Bench is a large-scale dataset collected using commercial WiFi edge devices across diverse indoor environments with real users. It spans over 461 hours of data and includes task-specific datasets for fall detection, breathing monitoring, localization, motion source recognition, and a co-labeled multitask dataset.", "result": "CSI-Bench captures realistic signal variability under natural conditions and provides standardized evaluation splits and baseline results for both single-task and multi-task learning, supporting robust model development.", "conclusion": "CSI-Bench offers a foundation for developing scalable, privacy-preserving WiFi sensing systems applicable in health and broader human-centric applications."}}
{"id": "2505.22509", "pdf": "https://arxiv.org/pdf/2505.22509", "abs": "https://arxiv.org/abs/2505.22509", "authors": ["Zhonglin Xie", "Yiman Fong", "Haoran Yuan", "Zaiwen Wen"], "title": "Accelerating Optimization via Differentiable Stopping Time", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Optimization is an important module of modern machine learning applications.\nTremendous efforts have been made to accelerate optimization algorithms. A\ncommon formulation is achieving a lower loss at a given time. This enables a\ndifferentiable framework with respect to the algorithm hyperparameters. In\ncontrast, its dual, minimizing the time to reach a target loss, is believed to\nbe non-differentiable, as the time is not differentiable. As a result, it\nusually serves as a conceptual framework or is optimized using zeroth-order\nmethods. To address this limitation, we propose a differentiable stopping time\nand theoretically justify it based on differential equations. An efficient\nalgorithm is designed to backpropagate through it. As a result, the proposed\ndifferentiable stopping time enables a new differentiable formulation for\naccelerating algorithms. We further discuss its applications, such as online\nhyperparameter tuning and learning to optimize. Our proposed methods show\nsuperior performance in comprehensive experiments across various problems,\nwhich confirms their effectiveness.", "AI": {"tldr": "This paper proposes a differentiable stopping time to optimize the time to reach a target loss, overcoming previous limitations of non-differentiability. The authors design an efficient algorithm for backpropagation and demonstrate superior performance in various experiments.", "motivation": "Optimization algorithms are crucial in modern machine learning applications. While minimizing loss at a given time is a common formulation, minimizing the time to reach a target loss has been challenging due to its perceived non-differentiability. This limitation restricts optimization to conceptual frameworks or zeroth-order methods.", "method": "The authors propose a differentiable stopping time, theoretically justified by differential equations, which allows for an efficient algorithm to backpropagate through it. This new formulation accelerates algorithms and can be applied to tasks like online hyperparameter tuning and learning to optimize.", "result": "Comprehensive experiments across various problems show that the proposed methods outperform existing techniques, confirming their effectiveness.", "conclusion": "The introduction of differentiable stopping time provides a novel approach to accelerate optimization algorithms. Its successful application in diverse scenarios highlights its potential impact on machine learning optimization."}}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870", "abs": "https://arxiv.org/abs/2505.21870", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "title": "Evaluating the Retrieval Robustness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG.", "AI": {"tldr": "Retrieval-augmented generation (RAG) enhances LLMs' ability to solve knowledge-intensive tasks, but imperfect retrieval and limited leveraging of retrieved content can lead to performance degradation. This study evaluates the retrieval robustness of 11 LLMs using a benchmark of 1500 open-domain questions with retrieved Wikipedia documents. Three research questions are addressed through three robustness metrics. Comprehensive experiments reveal that all LLMs show high retrieval robustness, yet varying degrees of imperfect robustness prevent them from fully utilizing RAG benefits.", "motivation": "To understand the robustness of large language models in practical retrieval-augmented generation setups and how different factors such as number of retrieved documents and document orders impact their performance.", "method": "A benchmark of 1500 open-domain questions was established, each associated with retrieved Wikipedia documents. Experiments were conducted with 11 LLMs and 3 prompting strategies, evaluating retrieval robustness through three specific metrics corresponding to three research questions.", "result": "All tested LLMs exhibited surprisingly high retrieval robustness, but there were varying degrees of imperfect robustness affecting their ability to fully utilize the advantages of RAG.", "conclusion": "LLMs generally have high retrieval robustness, but improvements are needed to fully leverage the benefits of RAG."}}
{"id": "2505.22521", "pdf": "https://arxiv.org/pdf/2505.22521", "abs": "https://arxiv.org/abs/2505.22521", "authors": ["Chao Wang", "Chuanhao Nie", "Yunbo Liu"], "title": "Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages. Chao Wang, Chuanhao Nie, and Yunbo Liu contributed equally\n  to this work. Corresponding author: Yunbo Liu (yunbo.liu954@duke.edu).\n  Submitted to the 3rd International Conference on Management Innovation and\n  Economy Development (MIED 2025), Chongqing, China", "summary": "Fraud detection remains a critical task in high-stakes domains such as\nfinance and e-commerce, where undetected fraudulent transactions can lead to\nsignificant economic losses. In this study, we systematically compare the\nperformance of four supervised learning models - Logistic Regression, Random\nForest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit\n(GRU) network - on a large-scale, highly imbalanced online transaction dataset.\nWhile ensemble methods such as Random Forest and LightGBM demonstrated superior\nperformance in both overall and class-specific metrics, Logistic Regression\noffered a reliable and interpretable baseline. The GRU model showed strong\nrecall for the minority fraud class, though at the cost of precision,\nhighlighting a trade-off relevant for real-world deployment. Our evaluation\nemphasizes not only weighted averages but also per-class precision, recall, and\nF1-scores, providing a nuanced view of each model's effectiveness in detecting\nrare but consequential fraudulent activity. The findings underscore the\nimportance of choosing models based on the specific risk tolerance and\noperational needs of fraud detection systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56db\u4e2a\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u5728\u7ebf\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5f3a\u8c03\u4e86\u6839\u636e\u5177\u4f53\u98ce\u9669\u627f\u53d7\u80fd\u529b\u548c\u8fd0\u8425\u9700\u6c42\u9009\u62e9\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6b3a\u8bc8\u68c0\u6d4b\u5728\u91d1\u878d\u548c\u7535\u5b50\u5546\u52a1\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u672a\u68c0\u6d4b\u5230\u7684\u6b3a\u8bc8\u6027\u4ea4\u6613\u53ef\u80fd\u5bfc\u81f4\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u4e14\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u5728\u7ebf\u4ea4\u6613\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001LightGBM\u548cGRU\u56db\u79cd\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u52a0\u6743\u5e73\u5747\u503c\u4ee5\u53ca\u6bcf\u7c7b\u7684\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "result": "\u968f\u673a\u68ee\u6797\u548cLightGBM\u5728\u6574\u4f53\u548c\u7c7b\u522b\u7279\u5b9a\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\uff1b\u903b\u8f91\u56de\u5f52\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7ebf\uff1bGRU\u6a21\u578b\u5bf9\u5c11\u6570\u7c7b\u522b\u7684\u6b3a\u8bc8\u884c\u4e3a\u663e\u793a\u51fa\u8f83\u9ad8\u7684\u53ec\u56de\u7387\uff0c\u4f46\u727a\u7272\u4e86\u7cbe\u786e\u5ea6\u3002", "conclusion": "\u9009\u62e9\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u7684\u6a21\u578b\u65f6\uff0c\u5e94\u6839\u636e\u5176\u7279\u5b9a\u7684\u98ce\u9669\u627f\u53d7\u80fd\u529b\u548c\u8fd0\u8425\u9700\u6c42\u8fdb\u884c\u51b3\u7b56\u3002"}}
{"id": "2505.21873", "pdf": "https://arxiv.org/pdf/2505.21873", "abs": "https://arxiv.org/abs/2505.21873", "authors": ["Jie Gao", "Jun Li", "Jing Hu", "Shanzhuo Zhang", "Kunrui Zhu", "Yueyang Huang", "Xiaonan Zhang", "Xiaomin Fang"], "title": "HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3", "categories": ["q-bio.BM", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein binder design is central to therapeutics, diagnostics, and synthetic\nbiology, yet practical deployment remains challenging due to fragmented\nworkflows, high computational costs, and complex tool integration. We present\nHelixDesign-Binder, a production-grade, high-throughput platform built on\nHelixFold3 that automates the full binder design pipeline, from backbone\ngeneration and sequence design to structural evaluation and multi-dimensional\nscoring. By unifying these stages into a scalable and user-friendly system,\nHelixDesign-Binder enables efficient exploration of binder candidates with\nfavorable structural, energetic, and physicochemical properties. The platform\nleverages Baidu Cloud's high-performance infrastructure to support large-scale\ndesign and incorporates advanced scoring metrics, including ipTM, predicted\nbinding free energy, and interface hydrophobicity. Benchmarking across six\nprotein targets demonstrates that HelixDesign-Binder reliably produces diverse\nand high-quality binders, some of which match or exceed validated designs in\npredicted binding affinity. HelixDesign-Binder is accessible via an interactive\nweb interface in PaddleHelix platform, supporting both academic research and\nindustrial applications in antibody and protein binder development.", "AI": {"tldr": "HelixDesign-Binder\u662f\u4e00\u4e2a\u57fa\u4e8eHelixFold3\u7684\u9ad8\u901a\u91cf\u5e73\u53f0\uff0c\u81ea\u52a8\u5316\u4e86\u4ece\u9aa8\u67b6\u751f\u6210\u5230\u7ed3\u6784\u8bc4\u4f30\u7684\u7ed3\u5408\u5242\u8bbe\u8ba1\u5168\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u767e\u5ea6\u4e91\u9ad8\u6027\u80fd\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u5927\u89c4\u6a21\u8bbe\u8ba1\u548c\u5148\u8fdb\u8bc4\u5206\u6307\u6807\u3002\u5728\u516d\u4e2a\u86cb\u767d\u8d28\u76ee\u6807\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u5e73\u53f0\u80fd\u53ef\u9760\u5730\u751f\u6210\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u7ed3\u5408\u5242\u3002", "motivation": "\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u8bbe\u8ba1\u5728\u6cbb\u7597\u3001\u8bca\u65ad\u548c\u5408\u6210\u751f\u7269\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5b58\u5728\u5de5\u4f5c\u6d41\u5206\u6563\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5de5\u5177\u96c6\u6210\u590d\u6742\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "HelixDesign-Binder\u6574\u5408\u4e86\u7ed3\u5408\u5242\u8bbe\u8ba1\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u5305\u62ec\u9aa8\u67b6\u751f\u6210\u3001\u5e8f\u5217\u8bbe\u8ba1\u3001\u7ed3\u6784\u8bc4\u4f30\u548c\u591a\u7ef4\u5ea6\u8bc4\u5206\uff0c\u5f62\u6210\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7528\u6237\u53cb\u597d\u7684\u7cfb\u7edf\u3002\u5229\u7528\u767e\u5ea6\u4e91\u7684\u9ad8\u6027\u80fd\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u5927\u89c4\u6a21\u8bbe\u8ba1\uff0c\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u8bc4\u5206\u6807\u51c6\u5982ipTM\u3001\u9884\u6d4b\u7684\u7ed3\u5408\u81ea\u7531\u80fd\u548c\u754c\u9762\u758f\u6c34\u6027\u7b49\u3002", "result": "\u901a\u8fc7\u5bf9\u516d\u4e2a\u86cb\u767d\u8d28\u76ee\u6807\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86HelixDesign-Binder\u80fd\u591f\u4ea7\u751f\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u7ed3\u5408\u5242\uff0c\u90e8\u5206\u8bbe\u8ba1\u5728\u9884\u6d4b\u7ed3\u5408\u4eb2\u548c\u529b\u4e0a\u4e0e\u9a8c\u8bc1\u8fc7\u7684\u7ed3\u5408\u5242\u76f8\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u5b83\u4eec\u3002", "conclusion": "HelixDesign-Binder\u901a\u8fc7PaddleHelix\u5e73\u53f0\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u7f51\u7edc\u63a5\u53e3\uff0c\u652f\u6301\u5b66\u672f\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6297\u4f53\u548c\u86cb\u767d\u8d28\u7ed3\u5408\u5242\u5f00\u53d1\uff0c\u6781\u5927\u5730\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.22524", "pdf": "https://arxiv.org/pdf/2505.22524", "abs": "https://arxiv.org/abs/2505.22524", "authors": ["Chinmay Pani", "Zijing Ou", "Yingzhen Li"], "title": "Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo", "categories": ["cs.LG"], "comment": null, "summary": "Discrete diffusion models have become highly effective across various\ndomains. However, real-world applications often require the generative process\nto adhere to certain constraints but without task-specific fine-tuning. To this\nend, we propose a training-free method based on Sequential Monte Carlo (SMC) to\nsample from the reward-aligned target distribution at the test time. Our\napproach leverages twisted SMC with an approximate locally optimal proposal,\nobtained via a first-order Taylor expansion of the reward function. To address\nthe challenge of ill-defined gradients in discrete spaces, we incorporate a\nGumbel-Softmax relaxation, enabling efficient gradient-based approximation\nwithin the discrete generative framework. Empirical results on both synthetic\ndatasets and image modelling validate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSMC\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408Gumbel-Softmax\u677e\u5f1b\u6280\u672f\uff0c\u5728\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e0e\u5956\u52b1\u5bf9\u9f50\u7684\u76ee\u6807\u5206\u5e03\u91c7\u6837\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u751f\u6210\u8fc7\u7a0b\u6ee1\u8db3\u7279\u5b9a\u7ea6\u675f\u6761\u4ef6\u800c\u4e0d\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u987a\u5e8f\u8499\u7279\u5361\u6d1b\uff08SMC\uff09\uff0c\u4f7f\u7528\u626d\u66f2SMC\u548c\u5c40\u90e8\u6700\u4f18\u63d0\u8bae\u6765\u4ece\u5956\u52b1\u5bf9\u9f50\u7684\u76ee\u6807\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u540c\u65f6\u901a\u8fc7Gumbel-Softmax\u653e\u677e\u6280\u672f\u89e3\u51b3\u79bb\u6563\u7a7a\u95f4\u4e2d\u68af\u5ea6\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u56fe\u50cf\u5efa\u6a21\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u5730\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7ea6\u675f\u6761\u4ef6\u3002"}}
{"id": "2505.21876", "pdf": "https://arxiv.org/pdf/2505.21876", "abs": "https://arxiv.org/abs/2505.21876", "authors": ["Zun Wang", "Jaemin Cho", "Jialu Li", "Han Lin", "Jaehong Yoon", "Yue Zhang", "Mohit Bansal"], "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://zunwang1.github.io/Epic", "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.", "AI": {"tldr": "The paper presents EPiC, a framework for efficient and precise 3D camera control in video diffusion models that creates high-quality anchor videos without needing expensive camera trajectory annotations. It introduces Anchor-ControlNet, which integrates anchor video guidance with pretrained VDMs using minimal parameters. EPiC achieves state-of-the-art performance on I2V camera control tasks.", "motivation": "Recent methods for 3D camera control in video diffusion models often suffer from inaccuracies due to point cloud estimation errors and require extensive camera trajectory annotations, increasing resource demands.", "method": "EPiC constructs high-quality anchor videos by masking source videos based on first-frame visibility, eliminating the need for camera trajectory annotations. The Anchor-ControlNet module is introduced to integrate anchor video guidance into pretrained VDMs with minimal parameters.", "result": "EPiC achieves state-of-the-art performance on RealEstate10K and MiraData for image-to-video camera control tasks, demonstrating precise and robust camera control both quantitatively and qualitatively. It also exhibits strong zero-shot generalization to video-to-video scenarios.", "conclusion": "EPiC provides an efficient and precise method for 3D camera control in video diffusion models, reducing the need for extensive annotations and achieving superior performance."}}
{"id": "2505.21879", "pdf": "https://arxiv.org/pdf/2505.21879", "abs": "https://arxiv.org/abs/2505.21879", "authors": ["Weiting Liu", "Jiaxu Cui", "Jiao Hu", "En Wang", "Bo Yang"], "title": "Symbolic Foundation Regressor on Complex Networks", "categories": ["cs.SC", "cs.AI", "cs.LG"], "comment": "60 pages", "summary": "In science, we are interested not only in forecasting but also in\nunderstanding how predictions are made, specifically what the interpretable\nunderlying model looks like. Data-driven machine learning technology can\nsignificantly streamline the complex and time-consuming traditional manual\nprocess of discovering scientific laws, helping us gain insights into\nfundamental issues in modern science. In this work, we introduce a pre-trained\nsymbolic foundation regressor that can effectively compress complex data with\nnumerous interacting variables while producing interpretable physical\nrepresentations. Our model has been rigorously tested on non-network symbolic\nregression, symbolic regression on complex networks, and the inference of\nnetwork dynamics across various domains, including physics, biochemistry,\necology, and epidemiology. The results indicate a remarkable improvement in\nequation inference efficiency, being three times more effective than baseline\napproaches while maintaining accurate predictions. Furthermore, we apply our\nmodel to uncover more intuitive laws of interaction transmission from global\nepidemic outbreak data, achieving optimal data fitting. This model extends the\napplication boundary of pre-trained symbolic regression models to complex\nnetworks, and we believe it provides a foundational solution for revealing the\nhidden mechanisms behind changes in complex phenomena, enhancing\ninterpretability, and inspiring further scientific discoveries.", "AI": {"tldr": "The paper introduces a pre-trained symbolic foundation regressor that compresses complex data and generates interpretable physical representations, showing significant improvements in equation inference efficiency across various scientific domains.", "motivation": "To streamline the traditional manual process of discovering scientific laws by leveraging data-driven machine learning technology, aiding in understanding the interpretable underlying models behind predictions.", "method": "A pre-trained symbolic foundation regressor model is introduced which can compress complex data with interacting variables and produce interpretable physical representations. The model is tested on non-network symbolic regression, symbolic regression on complex networks, and network dynamics inference.", "result": "The model shows a threefold improvement in equation inference efficiency compared to baseline approaches while maintaining accurate predictions. It also successfully uncovers intuitive laws of interaction transmission from global epidemic outbreak data.", "conclusion": "This model extends the application of pre-trained symbolic regression models to complex networks, providing a foundational solution for revealing hidden mechanisms in complex phenomena, improving interpretability, and inspiring further scientific discoveries."}}
{"id": "2505.22533", "pdf": "https://arxiv.org/pdf/2505.22533", "abs": "https://arxiv.org/abs/2505.22533", "authors": ["Pallavi Bhardwaj", "Caitlin Jones", "Lasse Dierich", "Aleksandar Vu\u010dkovi\u0107"], "title": "TabularQGAN: A Quantum Generative Model for Tabular Data", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "18 pages,8 figures and 4 tables", "summary": "In this paper, we introduce a novel quantum generative model for synthesizing\ntabular data. Synthetic data is valuable in scenarios where real-world data is\nscarce or private, it can be used to augment or replace existing datasets.\nReal-world enterprise data is predominantly tabular and heterogeneous, often\ncomprising a mixture of categorical and numerical features, making it highly\nrelevant across various industries such as healthcare, finance, and software.\nWe propose a quantum generative adversarial network architecture with flexible\ndata encoding and a novel quantum circuit ansatz to effectively model tabular\ndata. The proposed approach is tested on the MIMIC III healthcare and Adult\nCensus datasets, with extensive benchmarking against leading classical models,\nCTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model\noutperforms classical models by an average of 8.5% with respect to an overall\nsimilarity score from SDMetrics, while using only 0.072% of the parameters of\nthe classical models. Additionally, we evaluate the generalization capabilities\nof the models using two custom-designed metrics that demonstrate the ability of\nthe proposed quantum model to generate useful and novel samples. To our\nknowledge, this is one of the first demonstrations of a successful quantum\ngenerative model for handling tabular data, indicating that this task could be\nwell-suited to quantum computers.", "AI": {"tldr": "This paper presents a new quantum generative model for synthesizing tabular data, which outperforms classical models by 8.5% in similarity score while using only 0.072% of their parameters.", "motivation": "To address the challenge of synthesizing high-quality tabular data when real-world data is scarce or private.", "method": "A quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz is proposed to effectively model tabular data.", "result": "The quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models.", "conclusion": "This is one of the first successful demonstrations of a quantum generative model for handling tabular data, suggesting that this task could be well-suited to quantum computers."}}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880", "abs": "https://arxiv.org/abs/2505.21880", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications.", "AI": {"tldr": "This study integrates LLM with ABM for urban mobility simulation, enhancing agent diversity and realism through synthetic population profiles, location allocation, and personalized routes. It models individual behaviors and large-scale mobility patterns in Taipei City, providing insights for urban planning.", "motivation": "To improve the realism and diversity of agents in urban mobility simulations beyond traditional rule-based ABM.", "method": "Integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM) to generate synthetic population profiles, allocate locations, and simulate personalized routes.", "result": "The simulation successfully models individual behaviors and large-scale mobility patterns in Taipei City, offering actionable insights like route heat maps and mode-specific indicators for urban planners.", "conclusion": "The proposed framework shows promise in enhancing urban mobility simulations; future work will focus on robust validation frameworks for accuracy and reliability in urban planning applications."}}
{"id": "2505.22538", "pdf": "https://arxiv.org/pdf/2505.22538", "abs": "https://arxiv.org/abs/2505.22538", "authors": ["Paul Hofman", "Yusuf Sale", "Eyke H\u00fcllermeier"], "title": "Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We address the problem of uncertainty quantification and propose measures of\ntotal, aleatoric, and epistemic uncertainty based on a known decomposition of\n(strictly) proper scoring rules, a specific type of loss function, into a\ndivergence and an entropy component. This leads to a flexible framework for\nuncertainty quantification that can be instantiated with different losses\n(scoring rules), which makes it possible to tailor uncertainty quantification\nto the use case at hand. We show that this flexibility is indeed advantageous.\nIn particular, we analyze the task of selective prediction and show that the\nscoring rule should ideally match the task loss. In addition, we perform\nexperiments on two other common tasks. For out-of-distribution detection, our\nresults confirm that a widely used measure of epistemic uncertainty, mutual\ninformation, performs best. Moreover, in the setting of active learning, our\nmeasure of epistemic uncertainty based on the zero-one-loss consistently\noutperforms other uncertainty measures.", "AI": {"tldr": "The paper proposes a framework for uncertainty quantification using measures derived from proper scoring rules, demonstrating its advantages in selective prediction, out-of-distribution detection, and active learning.", "motivation": "To address the problem of uncertainty quantification and create a flexible framework that can be tailored to different use cases by leveraging the decomposition of proper scoring rules.", "method": "Propose measures of total, aleatoric, and epistemic uncertainty based on the decomposition of proper scoring rules into divergence and entropy components, allowing flexibility through different losses (scoring rules).", "result": "In selective prediction, the scoring rule should ideally match the task loss. For out-of-distribution detection, mutual information performs best. In active learning, the measure based on zero-one-loss outperforms other uncertainty measures.", "conclusion": "The proposed framework offers flexibility and effectiveness in various tasks, such as selective prediction, out-of-distribution detection, and active learning."}}
{"id": "2505.22541", "pdf": "https://arxiv.org/pdf/2505.22541", "abs": "https://arxiv.org/abs/2505.22541", "authors": ["Vinitra Swamy"], "title": "A Human-Centric Approach to Explainable AI for Personalized Education", "categories": ["cs.LG", "cs.CY"], "comment": "PhD Thesis, EPFL (Computer Science)", "summary": "Deep neural networks form the backbone of artificial intelligence research,\nwith potential to transform the human experience in areas ranging from\nautonomous driving to personal assistants, healthcare to education. However,\ntheir integration into the daily routines of real-world classrooms remains\nlimited. It is not yet common for a teacher to assign students individualized\nhomework targeting their specific weaknesses, provide students with instant\nfeedback, or simulate student responses to a new exam question. While these\nmodels excel in predictive performance, this lack of adoption can be attributed\nto a significant weakness: the lack of explainability of model decisions,\nleading to a lack of trust from students, parents, and teachers. This thesis\naims to bring human needs to the forefront of eXplainable AI (XAI) research,\ngrounded in the concrete use case of personalized learning and teaching. We\nframe the contributions along two verticals: technical advances in XAI and\ntheir aligned human studies. We investigate explainability in AI for education,\nrevealing systematic disagreements between post-hoc explainers and identifying\na need for inherently interpretable model architectures. We propose four novel\ntechnical contributions in interpretability with a multimodal modular\narchitecture (MultiModN), an interpretable mixture-of-experts model\n(InterpretCC), adversarial training for explainer stability, and a\ntheory-driven LLM-XAI framework to present explanations to students\n(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,\nlearning scientists, and university students. By combining empirical\nevaluations of existing explainers with novel architectural designs and human\nstudies, our work lays a foundation for human-centric AI systems that balance\nstate-of-the-art performance with built-in transparency and trust.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u6280\u672f\u8d21\u732e\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\uff0c\u4e3a\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bb8\u591a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u6559\u5b66\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u51b3\u7b56\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u5b66\u751f\u3001\u5bb6\u957f\u548c\u6559\u5e08\u5bf9\u5176\u7f3a\u4e4f\u4fe1\u4efb\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u56db\u79cd\u65b0\u7684\u6280\u672f\u8d21\u732e\uff1a1) \u591a\u6a21\u6001\u6a21\u5757\u5316\u67b6\u6784\uff08MultiModN\uff09\uff1b2) \u53ef\u89e3\u91ca\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08InterpretCC\uff09\uff1b3) \u5bf9\u6297\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u89e3\u91ca\u5668\u7a33\u5b9a\u6027\uff1b4) \u7406\u8bba\u9a71\u52a8\u7684LLM-XAI\u6846\u67b6\uff08iLLuMinaTE\uff09\uff0c\u7528\u4e8e\u5411\u5b66\u751f\u63d0\u4f9b\u89e3\u91ca\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u73b0\u6709\u89e3\u91ca\u5668\u7684\u7ecf\u9a8c\u8bc4\u4f30\u3001\u65b0\u9896\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u4eba\u7c7b\u7814\u7a76\u6765\u63d0\u5347AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u540e\u9a8c\u89e3\u91ca\u5668\u4e0e\u6559\u80b2\u573a\u666f\u7684\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u5206\u6b67\uff0c\u5f3a\u8c03\u4e86\u5bf9\u56fa\u6709\u53ef\u89e3\u91ca\u6a21\u578b\u67b6\u6784\u7684\u9700\u6c42\u3002\u63d0\u51fa\u7684\u56db\u79cd\u6280\u672f\u8d21\u732e\u5728\u4e0d\u540c\u7684\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u4fe1\u4efb\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5185\u7f6e\u900f\u660e\u6027\u548c\u4fe1\u4efb\u673a\u5236\uff0c\u4ece\u800c\u63a8\u52a8\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u6559\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.22549", "pdf": "https://arxiv.org/pdf/2505.22549", "abs": "https://arxiv.org/abs/2505.22549", "authors": ["Alex Iacob", "Lorenzo Sani", "Mher Safaryan", "Paris Giampouras", "Samuel Horv\u00e1th", "Andrej Jovanovic", "Meghdad Kurmanji", "Preslav Aleksandrov", "William F. Shen", "Xinchi Qiu", "Nicholas D. Lane"], "title": "DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models", "categories": ["cs.LG"], "comment": "Keywords: Distributed Training, Foundation Models, Large Language\n  Models, Optimizers, Communication Efficiency, Federated Learning, Distributed\n  Systems, Optimization Theory, Scaling, Robustness. Preprint, under review at\n  NeurIPS", "summary": "Scaling foundation model training with Distributed Data Parallel (DDP)\nmethods is bandwidth-limited. Existing infrequent communication methods like\nLocal SGD were designed to synchronize only model parameters and cannot be\ntrivially applied to adaptive optimizers due to additional optimizer states.\nCurrent approaches extending Local SGD either lack convergence guarantees or\nrequire synchronizing all optimizer states, tripling communication costs. We\npropose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of\noptimizers assigning independent synchronization periods to parameters and\nmomenta, enabling lower communication costs while preserving convergence.\nThrough extensive experiments on language models of up to 1.7B, we show that\nDES-LOC can communicate 170x less than DDP and 2x less than the previous\nstate-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,\nDES-LOC is suited for practical training scenarios prone to system failures.\nDES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for\nfoundation model training.", "AI": {"tldr": "DES-LOC is a family of optimizers that can reduce communication costs while preserving convergence, making it suitable for practical training scenarios prone to system failures.", "motivation": "Existing infrequent communication methods like Local SGD were designed to synchronize only model parameters and cannot be trivially applied to adaptive optimizers due to additional optimizer states. Current approaches extending Local SGD either lack convergence guarantees or require synchronizing all optimizer states, tripling communication costs.", "method": "DES-LOC assigns independent synchronization periods to parameters and momenta, enabling lower communication costs while preserving convergence.", "result": "Through extensive experiments on language models of up to 1.7B, DES-LOC can communicate 170x less than DDP and 2x less than the previous state-of-the-art Local ADAM.", "conclusion": "DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for foundation model training."}}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898", "abs": "https://arxiv.org/abs/2505.21898", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%.", "AI": {"tldr": "The paper introduces Co-Saving, a resource-aware multi-agent system that improves efficiency and solution quality in complex tasks through 'shortcuts' learned from successful trajectories.", "motivation": "Existing standalone agents and Multi-Agent Systems (MAS) face limitations such as high token consumption and excessive execution time when handling complex tasks.", "method": "Proposes Co-Saving, a multi-agent system that leverages experiential knowledge via 'shortcuts', which are instructional transitions learned from historically successful trajectories, to bypass redundant reasoning agents and expedite problem-solving.", "result": "Experiments on software development tasks show significant advantages over existing methods, with a 50.85% reduction in token usage and a 10.06% improvement in code quality compared to state-of-the-art MAS ChatDev.", "conclusion": "Co-Saving demonstrates the potential of resource-aware multi-agent systems in enhancing operational efficiency and solution quality for complex tasks."}}
{"id": "2505.22560", "pdf": "https://arxiv.org/pdf/2505.22560", "abs": "https://arxiv.org/abs/2505.22560", "authors": ["Artem Moskalev", "Mangal Prakash", "Junjie Xu", "Tianyu Cui", "Rui Liao", "Tommaso Mansi"], "title": "Geometric Hyena Networks for Large-scale Equivariant Learning", "categories": ["cs.LG"], "comment": null, "summary": "Processing global geometric context while preserving equivariance is crucial\nwhen modeling biological, chemical, and physical systems. Yet, this is\nchallenging due to the computational demands of equivariance and global context\nat scale. Standard methods such as equivariant self-attention suffer from\nquadratic complexity, while local methods such as distance-based message\npassing sacrifice global information. Inspired by the recent success of\nstate-space and long-convolutional models, we introduce Geometric Hyena, the\nfirst equivariant long-convolutional model for geometric systems. Geometric\nHyena captures global geometric context at sub-quadratic complexity while\nmaintaining equivariance to rotations and translations. Evaluated on all-atom\nproperty prediction of large RNA molecules and full protein molecular dynamics,\nGeometric Hyena outperforms existing equivariant models while requiring\nsignificantly less memory and compute that equivariant self-attention. Notably,\nour model processes the geometric context of 30k tokens 20x faster than the\nequivariant transformer and allows 72x longer context within the same budget.", "AI": {"tldr": "Geometric Hyena is the first equivariant long-convolutional model for geometric systems, capturing global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. It outperforms existing models in all-atom property prediction of large RNA molecules and full protein molecular dynamics with significantly less memory and compute.", "motivation": "Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems, but standard methods suffer from quadratic complexity or sacrifice global information.", "method": "Introduced Geometric Hyena, an equivariant long-convolutional model that captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations.", "result": "Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute.", "conclusion": "Geometric Hyena processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget."}}
{"id": "2505.21904", "pdf": "https://arxiv.org/pdf/2505.21904", "abs": "https://arxiv.org/abs/2505.21904", "authors": ["Pardis Taghavi", "Tian Liu", "Renjie Li", "Reza Langari", "Zhengzhong Tu"], "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation demands costly per-pixel annotations and large models.\nWe introduce CAST, a semi-supervised knowledge distillation (SSKD) framework\nthat compresses pretrained vision foundation models (VFM) into compact experts\nusing limited labeled and abundant unlabeled data. CAST unfolds in three\nstages: (1) domain adaptation of the VFM teacher(s) via self-training with\ncontrastive pixel calibration, (2) distillation into a compact student via a\nunified multi-objective loss that couples standard supervision and\npseudo-labels with our instance-aware pixel-wise contrastive term, and (3)\nfine-tuning on labeled data to remove residual pseudo-label bias. Central to\nCAST is an \\emph{instance-aware pixel-wise contrastive loss} that fuses mask\nand class scores to mine informative negatives and enforce clear inter-instance\nmargins. By maintaining this contrastive signal across both adaptation and\ndistillation, we align teacher and student embeddings and fully leverage\nunlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses\nits adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.\n15.2) and outperforms state-of-the-art semi-supervised approaches.", "AI": {"tldr": "CAST is a semi-supervised knowledge distillation framework that compresses pretrained vision foundation models into compact experts using limited labeled and abundant unlabeled data, with a novel instance-aware pixel-wise contrastive loss.", "motivation": "Instance segmentation requires costly per-pixel annotations and large models, so there's a need for methods that can use less labeled data and produce smaller models.", "method": "The CAST framework consists of three stages: domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, distillation into a compact student via a unified multi-objective loss, and fine-tuning on labeled data to remove residual pseudo-label bias. It also uses an instance-aware pixel-wise contrastive loss.", "result": "On Cityscapes and ADE20K, the ~11X smaller student model surpasses its adapted VFM teacher(s) by +3.4 AP and +1.5 AP respectively and outperforms state-of-the-art semi-supervised approaches.", "conclusion": "CAST effectively leverages unlabeled data and a novel contrastive loss to create smaller, more efficient models for instance segmentation that outperform their larger counterparts."}}
{"id": "2505.22573", "pdf": "https://arxiv.org/pdf/2505.22573", "abs": "https://arxiv.org/abs/2505.22573", "authors": ["Guy Moss", "Leah Sophie Muhle", "Reinhard Drews", "Jakob H. Macke", "Cornelius Schr\u00f6der"], "title": "FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators", "categories": ["cs.LG"], "comment": null, "summary": "Simulation-based inference (SBI) is an established approach for performing\nBayesian inference on scientific simulators. SBI so far works best on\nlow-dimensional parametric models. However, it is difficult to infer\nfunction-valued parameters, which frequently occur in disciplines that model\nspatiotemporal processes such as the climate and earth sciences. Here, we\nintroduce an approach for efficient posterior estimation, using a Fourier\nNeural Operator (FNO) architecture with a flow matching objective. We show that\nour approach, FNOPE, can perform inference of function-valued parameters at a\nfraction of the simulation budget of state of the art methods. In addition,\nFNOPE supports posterior evaluation at arbitrary discretizations of the domain,\nas well as simultaneous estimation of vector-valued parameters. We demonstrate\nthe effectiveness of our approach on several benchmark tasks and a challenging\nspatial inference task from glaciology. FNOPE extends the applicability of SBI\nmethods to new scientific domains by enabling the inference of function-valued\nparameters.", "AI": {"tldr": "The paper presents FNOPE, a method for efficient posterior estimation using Fourier Neural Operator architecture with flow matching objective to perform inference of function-valued parameters.", "motivation": "Simulation-based inference (SBI) is effective for low-dimensional parametric models but struggles with function-valued parameters common in spatiotemporal processes.", "method": "FNOPE employs a Fourier Neural Operator (FNO) architecture with a flow matching objective for efficient posterior estimation and supports arbitrary discretizations as well as vector-valued parameter estimation simultaneously.", "result": "FNOPE can perform inference of function-valued parameters at a fraction of the simulation budget compared to state-of-the-art methods and demonstrates effectiveness on benchmark tasks and a glaciology spatial inference task.", "conclusion": "FNOPE extends the applicability of SBI methods to new scientific domains by enabling the inference of function-valued parameters."}}
{"id": "2505.21906", "pdf": "https://arxiv.org/pdf/2505.21906", "abs": "https://arxiv.org/abs/2505.21906", "authors": ["Zhongyi Zhou", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://chatvla-2.github.io/", "summary": "Vision-language-action (VLA) models have emerged as the next generation of\nmodels in robotics. However, despite leveraging powerful pre-trained\nVision-Language Models (VLMs), existing end-to-end VLA systems often lose key\ncapabilities during fine-tuning as the model adapts to specific robotic tasks.\nWe argue that a generalizable VLA model should retain and expand upon the VLM's\ncore competencies: 1) Open-world embodied reasoning - the VLA should inherit\nthe knowledge from VLM, i.e., recognize anything that the VLM can recognize,\ncapable of solving math problems, possessing visual-spatial intelligence, 2)\nReasoning following - effectively translating the open-world reasoning into\nactionable steps for the robot. In this work, we introduce ChatVLA-2, a novel\nmixture-of-expert VLA model coupled with a specialized three-stage training\npipeline designed to preserve the VLM's original strengths while enabling\nactionable reasoning. To validate our approach, we design a math-matching task\nwherein a robot interprets math problems written on a whiteboard and picks\ncorresponding number cards from a table to solve equations. Remarkably, our\nmethod exhibits exceptional mathematical reasoning and OCR capabilities,\ndespite these abilities not being explicitly trained within the VLA.\nFurthermore, we demonstrate that the VLA possesses strong spatial reasoning\nskills, enabling it to interpret novel directional instructions involving\npreviously unseen objects. Overall, our method showcases reasoning and\ncomprehension abilities that significantly surpass state-of-the-art imitation\nlearning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a\nsubstantial advancement toward developing truly generalizable robotic\nfoundation models endowed with robust reasoning capacities.", "AI": {"tldr": "ChatVLA-2\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6df7\u5408\u4e13\u5bb6\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c(VLA)\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u5728\u4fdd\u7559\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u539f\u6709\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u53ef\u6267\u884c\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6570\u5b66\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u5b66\u63a8\u7406\u548cOCR\u80fd\u529b\uff0c\u5e76\u5177\u5907\u5f3a\u5927\u7684\u7a7a\u95f4\u63a8\u7406\u6280\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aefVLA\u7cfb\u7edf\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5f80\u5f80\u4f1a\u5931\u53bb\u5173\u952e\u80fd\u529b\uff0c\u65e0\u6cd5\u5145\u5206\u4fdd\u7559\u548c\u6269\u5c55VLM\u7684\u6838\u5fc3\u7ade\u4e89\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u548c\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86ChatVLA-2\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u6df7\u5408\u4e13\u5bb6VLA\u6a21\u578b\uff0c\u7ed3\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u65e8\u5728\u4fdd\u7559VLM\u7684\u539f\u59cb\u4f18\u52bf\u5e76\u5b9e\u73b0\u53ef\u6267\u884c\u63a8\u7406\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6570\u5b66\u5339\u914d\u4efb\u52a1\u6765\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "ChatVLA-2\u5728\u672a\u660e\u786e\u8bad\u7ec3\u6570\u5b66\u63a8\u7406\u548cOCR\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u5b66\u63a8\u7406\u548cOCR\u80fd\u529b\uff0c\u4ee5\u53ca\u5f3a\u5927\u7684\u7a7a\u95f4\u63a8\u7406\u6280\u80fd\u3002", "conclusion": "ChatVLA-2\u4ee3\u8868\u4e86\u5411\u5f00\u53d1\u771f\u6b63\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u8fc8\u8fdb\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.22578", "pdf": "https://arxiv.org/pdf/2505.22578", "abs": "https://arxiv.org/abs/2505.22578", "authors": ["Etienne Boursier", "Matthew Bowditch", "Matthias Englert", "Ranko Lazic"], "title": "Benignity of loss landscape with weight decay requires both large overparametrization and initialization", "categories": ["cs.LG"], "comment": null, "summary": "The optimization of neural networks under weight decay remains poorly\nunderstood from a theoretical standpoint. While weight decay is standard\npractice in modern training procedures, most theoretical analyses focus on\nunregularized settings. In this work, we investigate the loss landscape of the\n$\\ell_2$-regularized training loss for two-layer ReLU networks. We show that\nthe landscape becomes benign -- i.e., free of spurious local minima -- under\nlarge overparametrization, specifically when the network width $m$ satisfies $m\n\\gtrsim \\min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the\ninput dimension. More precisely in this regime, almost all constant activation\nregions contain a global minimum and no spurious local minima. We further show\nthat this level of overparametrization is not only sufficient but also\nnecessary via the example of orthogonal data. Finally, we demonstrate that such\nloss landscape results primarily hold relevance in the large initialization\nregime. In contrast, for small initializations -- corresponding to the feature\nlearning regime -- optimization can still converge to spurious local minima,\ndespite the global benignity of the landscape.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u6709\u6743\u91cd\u8870\u51cf\u7684\u4e24\u5c42ReLU\u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u666f\u89c2\uff0c\u5e76\u53d1\u73b0\u5f53\u7f51\u7edc\u5bbd\u5ea6\u6ee1\u8db3m \u2273 min(n^d, 2^n)\u65f6\uff0c\u635f\u5931\u666f\u89c2\u53d8\u5f97\u826f\u6027\uff08\u65e0\u865a\u5047\u5c40\u90e8\u6700\u5c0f\u503c\uff09\u3002\u8fd9\u79cd\u8fc7\u5ea6\u53c2\u6570\u5316\u7a0b\u5ea6\u662f\u5fc5\u8981\u4e14\u5145\u5206\u7684\u3002\u6b64\u5916\uff0c\u7ed3\u679c\u8868\u660e\u5927\u521d\u59cb\u5316\u6709\u52a9\u4e8e\u4f18\u5316\uff0c\u800c\u5c0f\u521d\u59cb\u5316\u53ef\u80fd\u5bfc\u81f4\u6536\u655b\u5230\u865a\u5047\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "motivation": "\u5c3d\u7ba1\u6743\u91cd\u8870\u51cf\u5728\u73b0\u4ee3\u8bad\u7ec3\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u5927\u591a\u6570\u7406\u8bba\u5206\u6790\u96c6\u4e2d\u5728\u672a\u6b63\u5219\u5316\u7684\u8bbe\u7f6e\u4e0a\u3002\u672c\u7814\u7a76\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u7406\u89e3\u6743\u91cd\u8870\u51cf\u5bf9\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u7814\u7a76\u4e86\u5e26\u6709l2\u6b63\u5219\u5316\u7684\u4e24\u5c42ReLU\u7f51\u7edc\u7684\u8bad\u7ec3\u635f\u5931\u666f\u89c2\u3002\u4ed6\u4eec\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u5bbd\u5ea6\u4e0e\u6570\u636e\u70b9\u548c\u8f93\u5165\u7ef4\u5ea6\u7684\u5173\u7cfb\uff0c\u786e\u5b9a\u4e86\u4f7f\u635f\u5931\u666f\u89c2\u826f\u6027\u7684\u8fc7\u5ea6\u53c2\u6570\u5316\u6761\u4ef6\u3002\u540c\u65f6\uff0c\u8fd8\u63a2\u8ba8\u4e86\u4e0d\u540c\u521d\u59cb\u5316\u89c4\u6a21\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "result": "1. \u5f53\u7f51\u7edc\u5bbd\u5ea6m \u2265 min(n^d, 2^n)\u65f6\uff0c\u635f\u5931\u666f\u89c2\u53d8\u5f97\u826f\u6027\uff0c\u51e0\u4e4e\u6240\u6709\u7684\u5e38\u6570\u6fc0\u6d3b\u533a\u57df\u5305\u542b\u5168\u5c40\u6700\u5c0f\u503c\u4e14\u6ca1\u6709\u865a\u5047\u5c40\u90e8\u6700\u5c0f\u503c\u3002\n2. \u6b64\u79cd\u8fc7\u5ea6\u53c2\u6570\u5316\u7a0b\u5ea6\u65e2\u662f\u5145\u5206\u4e5f\u662f\u5fc5\u8981\u7684\u3002\n3. \u5927\u521d\u59cb\u5316\u6709\u52a9\u4e8e\u4f18\u5316\u8fc7\u7a0b\uff0c\u800c\u5c0f\u521d\u59cb\u5316\u53ef\u80fd\u5bfc\u81f4\u6536\u655b\u5230\u865a\u5047\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "conclusion": "\u6743\u91cd\u8870\u51cf\u548c\u8fc7\u5ea6\u53c2\u6570\u5316\u53ef\u4ee5\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u666f\u89c2\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u4f18\u5316\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6539\u5584\u5728\u5c0f\u521d\u59cb\u5316\u60c5\u51b5\u4e0b\u53ef\u80fd\u4e0d\u6210\u7acb\uff0c\u8fd9\u4e3a\u672a\u6765\u7684\u4f18\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2505.22601", "pdf": "https://arxiv.org/pdf/2505.22601", "abs": "https://arxiv.org/abs/2505.22601", "authors": ["Jacob L. Block", "Aryan Mokhtari", "Sanjay Shakkottai"], "title": "Machine Unlearning under Overparameterization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine unlearning algorithms aim to remove the influence of specific\ntraining samples, ideally recovering the model that would have resulted from\ntraining on the remaining data alone. We study unlearning in the\noverparameterized setting, where many models interpolate the data, and defining\nthe unlearning solution as any loss minimizer over the retained\nset$\\unicode{x2013}$as in prior work in the underparameterized\nsetting$\\unicode{x2013}$is inadequate, since the original model may already\ninterpolate the retained data and satisfy this condition. In this regime, loss\ngradients vanish, rendering prior methods based on gradient perturbations\nineffective, motivating both new unlearning definitions and algorithms. For\nthis setting, we define the unlearning solution as the minimum-complexity\ninterpolator over the retained data and propose a new algorithmic framework\nthat only requires access to model gradients on the retained set at the\noriginal solution. We minimize a regularized objective over perturbations\nconstrained to be orthogonal to these model gradients, a first-order relaxation\nof the interpolation condition. For different model classes, we provide exact\nand approximate unlearning guarantees, and we demonstrate that an\nimplementation of our framework outperforms existing baselines across various\nunlearning experiments.", "AI": {"tldr": "In the overparameterized setting, traditional unlearning methods fail due to vanishing loss gradients. This paper redefines unlearning as finding minimum-complexity interpolators on retained data and proposes a new algorithmic framework that only needs model gradients on this data.", "motivation": "Unlearning algorithms are important for privacy protection and data removal. However, in the overparameterized setting where many models can interpolate the data, simply minimizing loss on retained data is inadequate because original models may already satisfy this condition. Moreover, loss gradients vanish in this regime, rendering prior gradient-based methods ineffective.", "method": "The authors redefine unlearning as finding the minimum-complexity interpolator over the retained data. They propose an algorithmic framework that minimizes a regularized objective over perturbations constrained to be orthogonal to model gradients on the retained set at the original solution. This is a first-order relaxation of the interpolation condition.", "result": "For different model classes, the authors provide both exact and approximate unlearning guarantees. Their implementation of the proposed framework outperforms existing baselines across various unlearning experiments.", "conclusion": "This work addresses the inadequacy of previous unlearning definitions and methods in the overparameterized setting. By redefining unlearning and proposing a new effective algorithmic framework, it advances the field of machine unlearning, particularly for complex models."}}
{"id": "2505.22602", "pdf": "https://arxiv.org/pdf/2505.22602", "abs": "https://arxiv.org/abs/2505.22602", "authors": ["Mahtab Alizadeh Vandchali", "Fangshuo", "Liao", "Anastasios Kyrillidis"], "title": "One Rank at a Time: Cascading Error Dynamics in Sequential Learning", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "36 pages", "summary": "Sequential learning -- where complex tasks are broken down into simpler,\nhierarchical components -- has emerged as a paradigm in AI. This paper views\nsequential learning through the lens of low-rank linear regression, focusing\nspecifically on how errors propagate when learning rank-1 subspaces\nsequentially. We present an analysis framework that decomposes the learning\nprocess into a series of rank-1 estimation problems, where each subsequent\nestimation depends on the accuracy of previous steps. Our contribution is a\ncharacterization of the error propagation in this sequential process,\nestablishing bounds on how errors -- e.g., due to limited computational budgets\nand finite precision -- affect the overall model accuracy. We prove that these\nerrors compound in predictable ways, with implications for both algorithmic\ndesign and stability guarantees.", "AI": {"tldr": "\u672c\u8bba\u6587\u901a\u8fc7\u4f4e\u79e9\u7ebf\u6027\u56de\u5f52\u7684\u89c6\u89d2\u7814\u7a76\u4e86\u5e8f\u5217\u5b66\u4e60\u4e2d\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u79e91\u4f30\u8ba1\u95ee\u9898\u3002\u7814\u7a76\u8868\u660e\uff0c\u8bef\u5dee\u4ee5\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u7d2f\u79ef\uff0c\u8fd9\u5bf9\u7b97\u6cd5\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u5e8f\u5217\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u7b80\u5355\u3001\u5206\u5c42\u7ec4\u4ef6\u7684\u8303\u5f0f\u5728AI\u9886\u57df\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5e8f\u5217\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u4f20\u64ad\u7684\u7279\u6027\uff0c\u7279\u522b\u662f\u5f53\u987a\u5e8f\u5b66\u4e60\u79e91\u5b50\u7a7a\u95f4\u65f6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u79e91\u4f30\u8ba1\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u540e\u7eed\u4f30\u8ba1\u90fd\u4f9d\u8d56\u4e8e\u524d\u4e00\u6b65\u9aa4\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u8fd9\u4e00\u6846\u67b6\uff0c\u7814\u7a76\u4e86\u7531\u4e8e\u8ba1\u7b97\u9884\u7b97\u6709\u9650\u548c\u7cbe\u5ea6\u6709\u9650\u7b49\u539f\u56e0\u5bfc\u81f4\u7684\u8bef\u5dee\u5982\u4f55\u5f71\u54cd\u6574\u4f53\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5e8f\u5217\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u8bef\u5dee\u4ee5\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u7d2f\u79ef\u3002\u8fd9\u4e3a\u7406\u89e3\u5e8f\u5217\u5b66\u4e60\u4e2d\u8bef\u5dee\u4f20\u64ad\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5bf9\u7b97\u6cd5\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u4fdd\u969c\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u4f4e\u79e9\u7ebf\u6027\u56de\u5f52\u7684\u89c6\u89d2\uff0c\u5206\u6790\u4e86\u5e8f\u5217\u5b66\u4e60\u4e2d\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8bef\u5dee\u5728\u5e8f\u5217\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u53ef\u9884\u6d4b\u7d2f\u79ef\u7279\u6027\uff0c\u4e3a\u672a\u6765\u7b97\u6cd5\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2505.21919", "pdf": "https://arxiv.org/pdf/2505.21919", "abs": "https://arxiv.org/abs/2505.21919", "authors": ["Yue Zhu", "Hao Yu", "Chen Wang", "Zhuoran Liu", "Eun Kyung Lee"], "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference", "categories": ["cs.ET", "cs.AI", "cs.DC"], "comment": "This paper has been accepted at IEEE Cloud 2025 as WIP paper. The\n  final version will appear in IEEE Xplore", "summary": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.", "AI": {"tldr": "The paper explores the need for efficient Key-Value Cache (KVC) management in large language models (LLMs), analyzing current systems and identifying gaps in tailored storage solutions for KVC prefilling.", "motivation": "The increasing use of LLMs with larger context windows leads to challenges in managing Key-Value Cache efficiently, especially given high cache reusability in certain inference workloads. This motivates the need to explore existing systems and identify potential improvements.", "method": "Analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis as well as state-of-the-art RDMA-based systems (CHIME and Sherman) for KVC metadata management.", "result": "Demonstrates that there is a lack of tailored storage solutions for KVC prefilling, and highlights the necessity for an efficient distributed caching system with optimized metadata management for LLM workloads.", "conclusion": "There is a critical need for improved KVC management systems designed specifically for scalable, low-latency inference in LLM workloads."}}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.", "AI": {"tldr": "The paper addresses the issue of policy entropy collapse in RL for reasoning with LLMs, proposes methods (Clip-Cov and KL-Cov) to control entropy by restricting high-covariance token updates, and demonstrates their effectiveness in encouraging exploration and improving performance.", "motivation": "The motivation is to overcome the obstacle of policy entropy collapse in RL when applied to reasoning with LLMs, which leads to diminished exploratory ability and saturation of policy performance.", "method": "The authors establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. They investigate entropy dynamics theoretically and empirically, finding that changes in policy entropy are driven by the covariance between action probability and the change in logits. Based on this understanding, they propose two techniques, Clip-Cov and KL-Cov, to control entropy by restricting updates of high-covariance tokens.", "result": "Experiments show that the proposed methods encourage exploration, help policies escape entropy collapse, and achieve better downstream performance.", "conclusion": "Entropy management is necessary for continuous exploration in scaling compute for RL. The proposed methods effectively control entropy and improve policy performance."}}
{"id": "2505.22637", "pdf": "https://arxiv.org/pdf/2505.22637", "abs": "https://arxiv.org/abs/2505.22637", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "categories": ["cs.LG"], "comment": "17 pages, 10 figures. Presented at the ICLR 2025 Workshop on\n  Foundation Models in the Wild", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "AI": {"tldr": "\u5c3d\u7ba1\u8f6c\u5411\u91cf\u5728\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u51fa\u6709\u5e0c\u671b\u7684\u6027\u80fd\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u662f\u4e0d\u53ef\u9760\u751a\u81f3\u9002\u5f97\u5176\u53cd\u3002\u672c\u6587\u7814\u7a76\u4e86\u63d0\u793a\u7c7b\u578b\u548c\u6fc0\u6d3b\u5dee\u5f02\u7684\u51e0\u4f55\u5f62\u72b6\u5bf9\u8f6c\u5411\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u4e03\u79cd\u63d0\u793a\u7c7b\u578b\u90fd\u4ea7\u751f\u4e86\u51c0\u6b63\u8f6c\u5411\u6548\u5e94\uff0c\u4f46\u6837\u672c\u95f4\u5dee\u5f02\u5f88\u5927\uff0c\u7ecf\u5e38\u4ea7\u751f\u4e0e\u9884\u671f\u76f8\u53cd\u7684\u6548\u679c\u3002\u6ca1\u6709\u4e00\u79cd\u63d0\u793a\u7c7b\u578b\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u7c7b\u578b\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u96c6\u6fc0\u6d3b\u5dee\u5f02\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8d8a\u9ad8\uff0c\u8f6c\u5411\u8d8a\u6709\u6548\u3002\u5e76\u4e14\uff0c\u6b63\u8d1f\u6fc0\u6d3b\u5206\u79bb\u8f83\u597d\u7684\u6570\u636e\u96c6\u66f4\u5bb9\u6613\u8f6c\u5411\u3002\u6700\u7ec8\u5f97\u51fa\u7ed3\u8bba\uff0c\u5f53\u76ee\u6807\u884c\u4e3a\u4e0d\u80fd\u7531\u8fde\u8d2f\u7684\u65b9\u5411\u8868\u793a\u65f6\uff0c\u5411\u91cf\u8f6c\u5411\u662f\u4e0d\u53ef\u9760\u7684\u3002", "motivation": "\u8f6c\u5411\u91cf\u4f5c\u4e3a\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u867d\u7136\u5c55\u793a\u51fa\u6709\u5e0c\u671b\u7684\u6027\u80fd\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u4e0d\u53ef\u9760\u6216\u9002\u5f97\u5176\u53cd\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5f71\u54cd\u8f6c\u5411\u53ef\u9760\u6027\u7684\u56e0\u7d20\u3002", "method": "\u7814\u7a76\u4e86\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u548c\u6fc0\u6d3b\u5dee\u5f02\u7684\u51e0\u4f55\u5f62\u72b6\u5bf9\u8f6c\u5411\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u5404\u79cd\u63d0\u793a\u7c7b\u578b\u7684\u8f6c\u5411\u6548\u679c\u53ca\u6fc0\u6d3b\u5dee\u5f02\u7684\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u6240\u6709\u63d0\u793a\u7c7b\u578b\u90fd\u80fd\u4ea7\u751f\u51c0\u6b63\u8f6c\u5411\u6548\u5e94\uff0c\u4f46\u6548\u679c\u4e0d\u7a33\u5b9a\u4e14\u7ecf\u5e38\u4e0e\u9884\u671f\u76f8\u53cd\uff1b\u63d0\u793a\u7c7b\u578b\u4e4b\u95f4\u65e0\u660e\u663e\u4f18\u52a3\uff1b\u8bad\u7ec3\u96c6\u6fc0\u6d3b\u5dee\u5f02\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u8f6c\u5411\u6548\u679c\u5448\u6b63\u76f8\u5173\uff1b\u6b63\u8d1f\u6fc0\u6d3b\u5206\u79bb\u8f83\u597d\u7684\u6570\u636e\u96c6\u66f4\u5bb9\u6613\u8f6c\u5411\u3002", "conclusion": "\u5f53\u76ee\u6807\u884c\u4e3a\u65e0\u6cd5\u7528\u8fde\u8d2f\u65b9\u5411\u8868\u793a\u65f6\uff0c\u5411\u91cf\u8f6c\u5411\u662f\u4e0d\u53ef\u9760\u7684\u3002"}}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926", "abs": "https://arxiv.org/abs/2505.21926", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA.", "AI": {"tldr": "MERRY is a foundation model for knowledge graph reasoning that integrates structural and textual information, outperforming existing baselines in both in-KG and out-of-KG tasks.", "motivation": "To address the limitation of current foundation models for knowledge graphs which primarily focus on structural aspects and in-KG tasks, hindering progress in more challenging out-of-KG tasks.", "method": "Propose MERRY with a multi-perspective Conditional Message Passing (CMP) encoding architecture to integrate textual and structural information, a dynamic residual fusion module to retain relevant textual information, and a flexible edge scoring mechanism for diverse downstream tasks.", "result": "Comprehensive evaluations on 28 datasets show MERRY outperforms existing baselines in most scenarios for both in-KG reasoning tasks and out-of-KG tasks like KGQA.", "conclusion": "MERRY demonstrates strong reasoning capabilities within KGs and excellent generalization to out-of-KG tasks."}}
{"id": "2505.22641", "pdf": "https://arxiv.org/pdf/2505.22641", "abs": "https://arxiv.org/abs/2505.22641", "authors": ["Chengzhi Shi", "Stratis Ioannidis"], "title": "Spectral Survival Analysis", "categories": ["cs.LG"], "comment": "Extended version of conference paper appearing in KDD 2025", "summary": "Survival analysis is widely deployed in a diverse set of fields, including\nhealthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model\nis a semi-parametric model often encountered in the literature. Despite its\npopularity, wide deployment, and numerous variants, scaling CoxPH to large\ndatasets and deep architectures poses a challenge, especially in the\nhigh-dimensional regime. We identify a fundamental connection between rank\nregression and the CoxPH model: this allows us to adapt and extend the\nso-called spectral method for rank regression to survival analysis. Our\napproach is versatile, naturally generalizing to several CoxPH variants,\nincluding deep models. We empirically verify our method's scalability on\nmultiple real-world high-dimensional datasets; our method outperforms legacy\nmethods w.r.t. predictive performance and efficiency.", "AI": {"tldr": "The paper links rank regression with the CoxPH model, adapting spectral methods for survival analysis in large datasets and deep architectures, showing superior performance and efficiency.", "motivation": "To address the challenge of scaling Cox Proportional Hazard models to large datasets and deep architectures, especially in high-dimensional settings.", "method": "Identify the connection between rank regression and the CoxPH model, then adapt and extend the spectral method from rank regression to survival analysis, making it versatile for several CoxPH variants including deep models.", "result": "Empirical verification on multiple real-world high-dimensional datasets shows that the method scales effectively and outperforms legacy methods in both predictive performance and efficiency.", "conclusion": "The approach successfully generalizes the CoxPH model to large-scale and deep learning contexts, enhancing both scalability and performance."}}
{"id": "2505.21928", "pdf": "https://arxiv.org/pdf/2505.21928", "abs": "https://arxiv.org/abs/2505.21928", "authors": ["Lianghui Zhu", "Xitong Ling", "Minxi Ouyang", "Xiaoping Liu", "Mingxi Fu", "Tian Guan", "Fanglei Fu", "Xuanyu Wang", "Maomao Zeng", "Mingxi Zhu", "Yibo Jin", "Liming Liu", "Song Duan", "Qiming He", "Yizhi Wang", "Luxi Xie", "Houqiang Li", "Yonghong He", "Sufang Tian"], "title": "Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Gastrointestinal (GI) diseases represent a clinically significant burden,\nnecessitating precise diagnostic approaches to optimize patient outcomes.\nConventional histopathological diagnosis, heavily reliant on the subjective\ninterpretation of pathologists, suffers from limited reproducibility and\ndiagnostic variability. To overcome these limitations and address the lack of\npathology-specific foundation models for GI diseases, we develop Digepath, a\nspecialized foundation model for GI pathology. Our framework introduces a\ndual-phase iterative optimization strategy combining pretraining with\nfine-screening, specifically designed to address the detection of sparsely\ndistributed lesion areas in whole-slide images. Digepath is pretrained on more\nthan 353 million image patches from over 200,000 hematoxylin and eosin-stained\nslides of GI diseases. It attains state-of-the-art performance on 33 out of 34\ntasks related to GI pathology, including pathological diagnosis, molecular\nprediction, gene mutation prediction, and prognosis evaluation, particularly in\ndiagnostically ambiguous cases and resolution-agnostic tissue classification.We\nfurther translate the intelligent screening module for early GI cancer and\nachieve near-perfect 99.6% sensitivity across 9 independent medical\ninstitutions nationwide. The outstanding performance of Digepath highlights its\npotential to bridge critical gaps in histopathological practice. This work not\nonly advances AI-driven precision pathology for GI diseases but also\nestablishes a transferable paradigm for other pathology subspecialties.", "AI": {"tldr": "The paper presents Digepath, a specialized foundation model for GI pathology that uses dual-phase iterative optimization and pretraining on millions of image patches to improve diagnostic accuracy and reproducibility. It achieves state-of-the-art performance in 33 out of 34 GI pathology tasks and near-perfect sensitivity in early cancer detection across multiple institutions.", "motivation": "Gastrointestinal diseases pose a significant clinical burden with conventional histopathological diagnosis suffering from limited reproducibility and variability due to subjective interpretation by pathologists.", "method": "Developed Digepath, a specialized foundation model for GI pathology using a dual-phase iterative optimization strategy combining pretraining with fine-screening. Pretrained on over 353 million image patches from more than 200,000 slides.", "result": "Achieved state-of-the-art performance in 33 out of 34 GI pathology tasks, including pathological diagnosis, molecular prediction, gene mutation prediction, and prognosis evaluation. Demonstrated near-perfect 99.6% sensitivity in early cancer detection across 9 medical institutions.", "conclusion": "Digepath has the potential to bridge critical gaps in histopathological practice for GI diseases, advancing AI-driven precision pathology and establishing a transferable paradigm for other pathology subspecialties."}}
{"id": "2505.22650", "pdf": "https://arxiv.org/pdf/2505.22650", "abs": "https://arxiv.org/abs/2505.22650", "authors": ["Maria-Florina Balcan", "Avrim Blum", "Zhiyuan Li", "Dravyansh Sharma"], "title": "On Learning Verifiers for Chain-of-Thought Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Chain-of-Thought reasoning has emerged as a powerful approach for solving\ncomplex mathematical and logical problems. However, it can often veer off track\nthrough incorrect or unsubstantiated inferences. Formal mathematical reasoning,\nwhich can be checked with a formal verifier, is one approach to addressing this\nissue. However, currently LLMs are simply not good enough to solve complex\nproblems in a formal way, and even just formalizing an informal problem\nstatement can be challenging. Motivated by this fact, in this work we consider\nthe problem of learning reliable verifiers for natural language\nChain-of-Thought reasoning. That is, given a problem statement and step-by-step\nsolution in natural language, the aim of the verifier is to output [Yes] if the\nreasoning steps in the solution are all valid, and [No] otherwise. In this work\nwe give a formal PAC-learning framework for studying this problem. We propose\nand analyze several natural verification goals, at different levels of\nstrength, in this framework. We provide sample complexity upper-bounds for\nlearning verifiers satisfying these goals, as well as lower-bound and\nimpossibility results for learning other natural verification objectives\nwithout additional assumptions.", "AI": {"tldr": "The paper explores the development of reliable verifiers for natural language Chain-of-Thought reasoning using a formal PAC-learning framework, providing sample complexity bounds and analysis of various verification goals.", "motivation": "Chain-of-Thought reasoning is powerful but prone to errors through incorrect inferences. Formal mathematical reasoning could address this, but LLMs struggle with fully formal approaches.", "method": "The authors propose learning verifiers for natural language Chain-of-Thought reasoning within a PAC-learning framework, analyzing different verification goals and their associated complexities.", "result": "They provide upper-bounds on sample complexity for certain verification goals, along with lower-bound and impossibility results for others without additional assumptions.", "conclusion": "This work contributes to understanding how to reliably verify Chain-of-Thought reasoning in natural language, advancing towards more robust AI reasoning systems."}}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u804a\u5929\u673a\u5668\u4eba\u6709\u65f6\u4f1a\u63d0\u4f9b\u9519\u8bef\u8f93\u51fa\uff0c\u4e14\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u3002\u56e0\u6b64\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u5206\u7c7b\u5728\u4ea4\u4e92\u5f0fLLM\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a\u672a\u6307\u5b9a\u4e0d\u786e\u5b9a\u6027\u3001\u4ea4\u4e92\u5b66\u4e60\u548c\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u671f\u63d0\u9ad8LLM\u4ee3\u7406\u4ea4\u4e92\u7684\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u548c\u76f4\u89c2\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u4e8eLLMs\u548c\u804a\u5929\u673a\u5668\u4eba\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e3b\u8981\u96c6\u4e2d\u5728\u6574\u4f53\u6570\u5b57\u6216\u5206\u4e3a\u968f\u673a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4e24\u7c7b\uff0c\u4f46\u5728\u5f00\u653e\u548c\u4ea4\u4e92\u5f0f\u7684LLM\u73af\u5883\u4e2d\uff0c\u8fd9\u79cd\u4f20\u7edf\u7684\u4e8c\u5206\u6cd5\u8fc7\u4e8e\u5c40\u9650\uff0c\u4e0d\u80fd\u5145\u5206\u63cf\u8ff0\u7528\u6237\u4e0e\u6a21\u578b\u4ea4\u4e92\u65f6\u7684\u590d\u6742\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u6587\u732e\u56de\u987e\uff0c\u53d1\u73b0\u73b0\u6709\u968f\u673a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u5b9a\u4e49\u5728\u4ea4\u4e92\u5f0fLLM\u573a\u666f\u4e2d\u5b58\u5728\u77db\u76fe\u5e76\u5931\u53bb\u610f\u4e49\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e09\u4e2a\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a1\uff09\u672a\u6307\u5b9a\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u5904\u7406\u7528\u6237\u672a\u63d0\u4f9b\u6240\u6709\u4fe1\u606f\u6216\u4efb\u52a1\u4e0d\u660e\u786e\u7684\u60c5\u51b5\uff1b2\uff09\u4ea4\u4e92\u5b66\u4e60\uff0c\u901a\u8fc7\u63d0\u95ee\u540e\u7eed\u95ee\u9898\u51cf\u5c11\u5bf9\u5f53\u524d\u4e0a\u4e0b\u6587\u7684\u4e0d\u786e\u5b9a\u6027\uff1b3\uff09\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\uff0c\u5229\u7528\u4e30\u5bcc\u7684\u8bed\u8a00\u548c\u8bed\u97f3\u7a7a\u95f4\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u800c\u4e0d\u4ec5\u9650\u4e8e\u6570\u5b57\u3002", "result": "\u8fd9\u4e09\u4e2a\u65b0\u65b9\u5411\u6709\u671b\u4f7fLLM\u4ee3\u7406\u4ea4\u4e92\u66f4\u52a0\u900f\u660e\u3001\u53ef\u4fe1\u548c\u76f4\u89c2\uff0c\u4e3a\u672a\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u5206\u7c7b\u5728\u4ea4\u4e92\u5f0fLLM\u73af\u5883\u4e2d\u4e0d\u518d\u9002\u7528\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u6765\u4e30\u5bcc\u548c\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u5347LLM\u4ee3\u7406\u4e0e\u7528\u6237\u7684\u4ea4\u4e92\u8d28\u91cf\u3002"}}
{"id": "2505.21954", "pdf": "https://arxiv.org/pdf/2505.21954", "abs": "https://arxiv.org/abs/2505.21954", "authors": ["Le Thien Phuc Nguyen", "Zhuoran Yu", "Khoa Quang Nhat Cao", "Yuwei Guo", "Tu Ho Manh Pham", "Tuan Tai Nguyen", "Toan Ngo Duc Vo", "Lucas Poon", "Soochahn Lee", "Yong Jae Lee"], "title": "UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present UniTalk, a novel dataset specifically designed for the task of\nactive speaker detection, emphasizing challenging scenarios to enhance model\ngeneralization. Unlike previously established benchmarks such as AVA, which\npredominantly features old movies and thus exhibits significant domain gaps,\nUniTalk focuses explicitly on diverse and difficult real-world conditions.\nThese include underrepresented languages, noisy backgrounds, and crowded scenes\n- such as multiple visible speakers speaking concurrently or in overlapping\nturns. It contains over 44.5 hours of video with frame-level active speaker\nannotations across 48,693 speaking identities, and spans a broad range of video\ntypes that reflect real-world conditions. Through rigorous evaluation, we show\nthat state-of-the-art models, while achieving nearly perfect scores on AVA,\nfail to reach saturation on UniTalk, suggesting that the ASD task remains far\nfrom solved under realistic conditions. Nevertheless, models trained on UniTalk\ndemonstrate stronger generalization to modern \"in-the-wild\" datasets like\nTalkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark\nfor active speaker detection, providing researchers with a valuable resource\nfor developing and evaluating versatile and resilient models.\n  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD\n  Code: https://github.com/plnguyen2908/UniTalk-ASD-code", "AI": {"tldr": "The paper introduces UniTalk, a new dataset for active speaker detection (ASD) that addresses limitations of previous datasets by focusing on diverse real-world conditions. It contains over 44.5 hours of video with frame-level annotations across 48,693 speaking identities. State-of-the-art models perform well on older benchmarks like AVA but struggle with UniTalk, highlighting the need for improved ASD methods. Models trained on UniTalk generalize better to modern datasets.", "motivation": "Existing ASD benchmarks, such as AVA, have significant domain gaps due to their focus on old movies. There is a need for a dataset that reflects diverse and challenging real-world conditions to enhance model generalization.", "method": "UniTalk is a dataset containing over 44.5 hours of video with frame-level active speaker annotations across 48,693 speaking identities. It includes underrepresented languages, noisy backgrounds, and crowded scenes with overlapping speakers. The dataset spans a broad range of video types reflecting real-world conditions.", "result": "State-of-the-art models achieve nearly perfect scores on AVA but fail to reach saturation on UniTalk, indicating the challenges of realistic conditions. Models trained on UniTalk demonstrate stronger generalization to modern datasets like Talkies and ASW, as well as to AVA.", "conclusion": "UniTalk establishes a new benchmark for active speaker detection, offering researchers a valuable resource to develop and evaluate more versatile and resilient models."}}
{"id": "2505.22660", "pdf": "https://arxiv.org/pdf/2505.22660", "abs": "https://arxiv.org/abs/2505.22660", "authors": ["Mihir Prabhudesai", "Lili Chen", "Alex Ippoliti", "Katerina Fragkiadaki", "Hao Liu", "Deepak Pathak"], "title": "Maximizing Confidence Alone Improves Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve\nsignificant advances in many fields. Most recently, RL has empowered frontier\nlanguage models to solve challenging math, science, and coding problems.\nHowever, central to any RL algorithm is the reward function, and reward\nengineering is a notoriously difficult problem in any domain. In this paper, we\npropose RENT: Reinforcement Learning via Entropy Minimization -- a fully\nunsupervised RL method that requires no external reward or ground-truth\nanswers, and instead uses the model's entropy of its underlying distribution as\nan intrinsic reward. We find that by reinforcing the chains of thought that\nyield high model confidence on its generated answers, the model improves its\nreasoning ability. In our experiments, we showcase these improvements on an\nextensive suite of commonly-used reasoning benchmarks, including GSM8K,\nMATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and\nMistral families. The generality of our unsupervised learning method lends\nitself to applicability in a wide range of domains where external supervision\nis limited or unavailable.", "AI": {"tldr": "The paper introduces RENT, an unsupervised reinforcement learning method using entropy minimization as intrinsic reward to enhance model reasoning without external rewards or ground-truth answers.", "motivation": "To address the difficulty of reward engineering in reinforcement learning by developing a method that does not require external rewards or supervision.", "method": "RENT uses the model's entropy of its underlying distribution as an intrinsic reward, reinforcing chains of thought that yield high model confidence on generated answers.", "result": "Improvements in reasoning ability were demonstrated across various benchmarks (GSM8K, MATH500, AMC, AIME, GPQA) using models from Qwen and Mistral families.", "conclusion": "RENT is a general unsupervised learning method applicable in domains with limited or no external supervision."}}
{"id": "2505.21955", "pdf": "https://arxiv.org/pdf/2505.21955", "abs": "https://arxiv.org/abs/2505.21955", "authors": ["Insu Lee", "Wooje Park", "Jaeyun Jang", "Minyoung Noh", "Kyuhong Shim", "Byonghyo Shim"], "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision-language models (LVLMs) are increasingly deployed in interactive\napplications such as virtual and augmented reality, where first-person\n(egocentric) view captured by head-mounted cameras serves as key input. While\nthis view offers fine-grained cues about user attention and hand-object\ninteractions, their narrow field of view and lack of global context often lead\nto failures on spatially or contextually demanding queries. To address this, we\nintroduce a framework that augments egocentric inputs with third-person\n(exocentric) views, providing complementary information such as global scene\nlayout and object visibility to LVLMs. We present E3VQA, the first benchmark\nfor multi-view question answering with 4K high-quality question-answer pairs\ngrounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a\ntraining-free prompting technique that constructs a unified scene\nrepresentation by integrating scene graphs from three complementary\nperspectives. M3CoT enables LVLMs to reason more effectively across views,\nyielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini\n2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key\nstrengths and limitations of LVLMs in multi-view reasoning and highlights the\nvalue of leveraging both egocentric and exocentric inputs.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165E3VQA\u57fa\u51c6\u548cM3CoT\u6280\u672f\uff0c\u672c\u7814\u7a76\u589e\u5f3a\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u89c6\u89d2\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u7ed3\u5408\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u4fe1\u606f\u65f6\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u7a7a\u95f4\u6216\u4e0a\u4e0b\u6587\u8981\u6c42\u8f83\u9ad8\u7684\u67e5\u8be2\u65f6\u5b58\u5728\u5931\u8d25\u60c5\u51b5\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u5934\u6234\u5f0f\u76f8\u673a\u6355\u83b7\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u56fe\u4f5c\u4e3a\u8f93\u5165\u65f6\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u5176\u89c6\u91ce\u72ed\u7a84\u4e14\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u6240\u81f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5c06\u7b2c\u4e00\u4eba\u79f0\u89c6\u56fe\u4e0e\u7b2c\u4e09\u4eba\u79f0\u89c6\u56fe\u76f8\u7ed3\u5408\uff0c\u4e3a\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u8865\u5145\u4fe1\u606f\uff0c\u5982\u5168\u5c40\u573a\u666f\u5e03\u5c40\u548c\u5bf9\u8c61\u53ef\u89c1\u6027\u3002\u540c\u65f6\uff0c\u63a8\u51fa\u4e86E3VQA\u57fa\u51c6\uff0c\u5305\u542b4K\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u6280\u672fM3CoT\uff0c\u901a\u8fc7\u6574\u5408\u4e09\u4e2a\u4e92\u8865\u89c6\u89d2\u7684\u573a\u666f\u56fe\u6784\u5efa\u7edf\u4e00\u7684\u573a\u666f\u8868\u793a\u3002", "result": "M3CoT\u6280\u672f\u4f7f\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u89c6\u89d2\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08GPT-4o\u63d0\u53474.84%\uff0cGemini 2.0 Flash\u63d0\u53475.94%\uff09\uff0c\u8d85\u8fc7\u6700\u8fd1\u7684CoT\u57fa\u7ebf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u8f93\u5165\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u89c6\u89d2\u63a8\u7406\u4e2d\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5176\u5173\u952e\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency.", "AI": {"tldr": "This paper presents Cross-modal RAG, a new framework that breaks down complex text-to-image generation queries and images into smaller parts for better retrieval and creation. It uses a hybrid retrieval method to find the best matching images and guides a multimodal model during generation, leading to superior results.", "motivation": "Text-to-image models need more specific, up-to-date knowledge than what pretrained models can offer. Current RAG methods fall short when a single image doesn't contain all elements from a detailed user request.", "method": "The Cross-modal RAG framework splits queries and images into sub-components. A hybrid retrieval approach combines sparse and dense retrievers to select Pareto-optimal images covering different aspects of the query. During image synthesis, a large language model is directed to focus on visual features corresponding to specific subqueries.", "result": "Cross-modal RAG surpasses existing methods in retrieval and generation quality across multiple datasets (MS-COCO, Flickr30K, WikiArt, CUB, ImageNet-LT), while staying efficient.", "conclusion": "Cross-modal RAG offers an effective solution for complex text-to-image tasks by enabling subquery-aware retrieval and generation."}}
{"id": "2505.18478", "pdf": "https://arxiv.org/pdf/2505.18478", "abs": "https://arxiv.org/abs/2505.18478", "authors": ["Lucas Tecot", "Di Luo", "Cho-Jui Hsieh"], "title": "Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise", "categories": ["quant-ph", "cs.LG", "physics.comp-ph"], "comment": "14 pages, 3 figures", "summary": "Advancements in quantum computing have spurred significant interest in\nharnessing its potential for speedups over classical systems. However, noise\nremains a major obstacle to achieving reliable quantum algorithms. In this\nwork, we present a provably noise-resilient training theory and algorithm to\nenhance the robustness of parameterized quantum circuit classifiers. Our\nmethod, with a natural connection to Evolutionary Strategies, guarantees\nresilience to parameter noise with minimal adjustments to commonly used\noptimization algorithms. Our approach is function-agnostic and adaptable to\nvarious quantum circuits, successfully demonstrated in quantum phase\nclassification tasks. By developing provably guaranteed optimization theory\nwith quantum circuits, our work opens new avenues for practical, robust\napplications of near-term quantum computers.", "AI": {"tldr": "This paper explores a noise-resilient training theory and algorithm for quantum circuit classifiers, enhancing robustness against parameter noise with minimal changes to optimization algorithms.", "motivation": "Noise is a significant challenge in achieving reliable quantum algorithms. The authors aim to develop a method that can improve the resilience of parameterized quantum circuits to noise.", "method": "The method presented has a connection to Evolutionary Strategies and ensures resilience to parameter noise with minor adjustments to standard optimization algorithms. It is function-agnostic and adaptable to various quantum circuits.", "result": "The approach was successfully demonstrated in quantum phase classification tasks, proving its effectiveness in enhancing the robustness of quantum circuit classifiers.", "conclusion": "By establishing a provably guaranteed optimization theory for quantum circuits, the work paves the way for practical applications of near-term quantum computers."}}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963", "abs": "https://arxiv.org/abs/2505.21963", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery.", "AI": {"tldr": "LaMDAgent is a framework that uses LLM-based agents to autonomously construct and optimize full post-training pipelines for LLMs, improving tool-use accuracy by 9.0 points with minimal human intervention.", "motivation": "To address the underexplored area of automated construction of complete post-training pipelines for LLMs, which typically rely on manual design or focus narrowly on optimizing individual components.", "method": "Introduction of LaMDAgent, a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents, exploring diverse model generation techniques, datasets, and hyperparameter configurations while leveraging task-based feedback.", "result": "LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities and uncovers effective post-training strategies often overlooked by conventional methods.", "conclusion": "LaMDAgent effectively constructs high-performing post-training pipelines with minimal human intervention; scaling data size enables cost-effective pipeline discovery while model size scaling introduces new challenges."}}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966", "abs": "https://arxiv.org/abs/2505.21966", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories.", "AI": {"tldr": "MapStory is an LLM-powered tool that turns text into editable map animations, using a system informed by animator interviews and analysis of existing videos. Evaluation shows it simplifies map animation creation, speeds up iteration, fosters creativity, and reduces barriers.", "motivation": "To create a tool that simplifies the process of creating map animations from natural language text, making it easier for users to produce and edit map-centric stories.", "method": "MapStory uses an agentic architecture to automatically generate a scene breakdown from user-written scripts, decomposing them into key animation elements like camera movements and visual highlights. It includes a researcher component leveraging an LLM with web search to query geospatial information accurately, allowing automatic extraction of relevant data while providing editing capabilities. Users can also fine-tune parameters via an interactive timeline editor.", "result": "Evaluation through expert interviews (N=5) and a usability study (N=12) indicates that MapStory allows users to create map animations easily, facilitates faster iteration cycles, encourages creative exploration, and lowers the barrier to entry for creating map-focused narratives.", "conclusion": "MapStory successfully enables users to create map animations more effortlessly, speeds up the iteration process, promotes creative exploration, and makes it easier for users to engage in map-centered storytelling."}}
{"id": "2505.20344", "pdf": "https://arxiv.org/pdf/2505.20344", "abs": "https://arxiv.org/abs/2505.20344", "authors": ["Karen Ardila", "Aashka Mohite", "Abdoljalil Addeh", "Amanda V. Tyndall", "Cindy K. Barha", "Quan Long", "M. Ethan MacDonald"], "title": "Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI", "categories": ["q-bio.GN", "cs.LG"], "comment": "7 pages, 5 figures, conference", "summary": "Brain aging trajectories differ between males and females, yet the genetic\nfactors underlying these differences remain underexplored. Using structural MRI\nand genotyping data from 40,940 UK Biobank participants (aged 45-83), we\ncomputed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and\nventricular volumes. We conducted sex-stratified genome-wide association\nstudies (GWAS) and Post-GWAS analyses to identify genetic variants associated\nwith accelerated brain aging. Distinct gene sets emerged by sex: in females,\nneurotransmitter transport and mitochondrial stress response genes were\nimplicated; in males, immune and inflammation-related genes dominated. Shared\ngenes, including GMNC and OSTN, were consistently linked to brain volumes\nacross sexes, suggesting core roles in neurostructural maintenance. Tissue\nexpression analyses revealed sex-specific enrichment in pathways tied to\nneurodegeneration. These findings highlight the importance of sex-stratified\napproaches in aging research and suggest genetic targets for personalized\ninterventions against age-related cognitive decline.", "AI": {"tldr": "This paper explores the genetic factors contributing to differences in brain aging between males and females using MRI and genotyping data from UK Biobank participants. It conducts sex-stratified GWAS studies on Brain Age Gap Estimates (BrainAGE) and identifies distinct and shared gene sets associated with accelerated brain aging, pointing towards personalized interventions for age-related cognitive decline.", "motivation": "To investigate the genetic factors underlying the differences in brain aging trajectories between males and females, which remain underexplored despite known disparities.", "method": "The study used structural MRI and genotyping data from 40,940 UK Biobank participants aged 45-83. BrainAGE was computed for total brain, hippocampal, and ventricular volumes. Sex-stratified GWAS and Post-GWAS analyses were performed to identify genetic variants linked to accelerated brain aging.", "result": "Distinct gene sets were identified for each sex: neurotransmitter transport and mitochondrial stress response genes in females; immune and inflammation-related genes in males. Shared genes like GMNC and OSTN were linked to brain volumes across sexes. Tissue expression analyses revealed sex-specific pathways tied to neurodegeneration.", "conclusion": "Sex-stratified approaches are crucial in aging research. The findings suggest potential genetic targets for personalized interventions against age-related cognitive decline."}}
{"id": "2505.21969", "pdf": "https://arxiv.org/pdf/2505.21969", "abs": "https://arxiv.org/abs/2505.21969", "authors": ["Tianjun Gu", "Linfeng Li", "Xuhong Wang", "Chenghua Gong", "Jingyu Gong", "Zhizhong Zhang", "Yuan Xie", "Lizhuang Ma", "Xin Tan"], "title": "DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Adaptive navigation in unfamiliar environments is crucial for household\nservice robots but remains challenging due to the need for both low-level path\nplanning and high-level scene understanding. While recent vision-language model\n(VLM) based zero-shot approaches reduce dependence on prior maps and\nscene-specific training data, they face significant limitations: spatiotemporal\ndiscontinuity from discrete observations, unstructured memory representations,\nand insufficient task understanding leading to navigation failures. We propose\nDORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory\nOriented Navigation), a novel cognitive-inspired framework consisting of\nVentral and Dorsal Streams that mimics human navigation capabilities. The\nDorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology\nMap to handle spatiotemporal discontinuities, while the Ventral Stream combines\nRAG-VLM and Policy-VLM to improve decision-making. Our approach also develops\nNav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON\non the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art\nperformance on both success rate (SR) and success weighted by path length (SPL)\nmetrics, significantly outperforming existing methods. We also introduce a new\nevaluation metric (AORI) to assess navigation intelligence better.\nComprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot\nautonomous navigation without requiring prior map building or pre-training.", "AI": {"tldr": "DORAEMON is a new framework for robot navigation that improves upon existing methods by addressing spatiotemporal discontinuities, unstructured memory, and task understanding issues. It achieves top performance in success rate and efficiency metrics without needing prior maps or pre-training.", "motivation": "Current zero-shot approaches for robot navigation have limitations such as spatiotemporal discontinuity from discrete observations, unstructured memory representations, and insufficient task understanding which lead to navigation failures.", "method": "DORAEMON consists of Ventral and Dorsal Streams inspired by human navigation capabilities. The Dorsal Stream handles spatiotemporal discontinuities through Hierarchical Semantic-Spatial Fusion and Topology Map, while the Ventral Stream enhances decision-making using RAG-VLM and Policy-VLM. Nav-Ensurance ensures safety and efficiency in navigation.", "result": "DORAEMON outperforms existing methods on HM3D, MP3D, and GOAT datasets in terms of success rate (SR) and success weighted by path length (SPL). A new evaluation metric (AORI) is introduced to better assess navigation intelligence.", "conclusion": "DORAEMON effectively enables zero-shot autonomous navigation without requiring prior map building or pre-training, demonstrating its potential in household service robots."}}
{"id": "2505.21507", "pdf": "https://arxiv.org/pdf/2505.21507", "abs": "https://arxiv.org/abs/2505.21507", "authors": ["Aurore Bussalb", "Fran\u00e7ois Le Gac", "Guillaume Jubien", "Mohamed Rahmouni", "Ruggero G. Bettinardi", "Pedro Marinho R. de Oliveira", "Phillipe Derambure", "Nicolas Gaspard", "Jacques Jonas", "Louis Maillard", "Laurent Vercueil", "Herv\u00e9 Vespignani", "Philippe Laval", "Laurent Koessler", "Ulysse Gimenez"], "title": "Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "comment": "20 pages, 7 figures", "summary": "Electroencephalography (EEG) is commonly used by physicians for the diagnosis\nof numerous neurological disorders. Due to the large volume of EEGs requiring\ninterpretation and the specific expertise involved, artificial\nintelligence-based tools are being developed to assist in their visual\nanalysis. In this paper, we compare two deep learning models (CNN-LSTM and\nTransformer-based) with BioSerenity-E1, a recently proposed foundation model,\nin the task of classifying entire EEG recordings as normal or abnormal. The\nthree models were trained or finetuned on 2,500 EEG recordings and their\nperformances were evaluated on two private and one public datasets: a large\nmulticenter dataset annotated by a single specialist (dataset A composed of n =\n4,480 recordings), a small multicenter dataset annotated by three specialists\n(dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus\nevaluation dataset (n = 276). On dataset A, the three models achieved at least\n86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest\nbalanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also\nachieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced\naccuracy. The models were then validated on TUAB evaluation dataset, whose\ncorresponding training set was not used during training, where they achieved at\nleast 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the\nother two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results\nhighlight the usefulness of leveraging pre-trained models for automatic EEG\nclassification: enabling robust and efficient interpretation of EEG data with\nfewer resources and broader applicability.", "AI": {"tldr": "The paper compares two deep learning models (CNN-LSTM and Transformer-based) with BioSerenity-E1 in classifying EEG recordings as normal or abnormal. BioSerenity-E1 finetuned performs the best.", "motivation": "To evaluate the performance of different deep learning models, including pre-trained models, in automatically classifying EEG recordings as normal or abnormal to assist physicians in diagnosis.", "method": "Three models were compared: CNN-LSTM, Transformer-based, and BioSerenity-E1. They were trained or fine-tuned on 2,500 EEG recordings and evaluated on three datasets: dataset A (n=4,480), dataset B (n=198), and TUAB evaluation dataset (n=276).", "result": "On dataset A, all models achieved at least 86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest (89.19%). On dataset B, BioSerenity-E1 finetuned also performed the best with 94.63% balanced accuracy. On the TUAB evaluation dataset, BioSerenity-E1 finetuned reached an accuracy of 82.25%, outperforming the other models.", "conclusion": "Pre-trained models like BioSerenity-E1 are useful for automatic EEG classification, providing robust and efficient interpretation of EEG data with fewer resources and broader applicability."}}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints.", "AI": {"tldr": "BLADE is a framework that combines imitation learning and model-based planning to enable long-horizon robotic manipulation using language-annotated demonstrations and LLMs for abstract action knowledge.", "motivation": "To create a system capable of generalizing to novel situations in long-horizon robotic manipulation tasks by integrating imitation learning with model-based planning.", "method": "The BLADE framework leverages language-annotated demonstrations, extracts abstract action knowledge from large language models (LLMs), and constructs a library of structured, high-level action representations including preconditions and effects grounded in visual perception and corresponding controllers implemented as neural network-based policies.", "result": "BLADE shows significant capabilities in generalizing to novel situations, such as novel initial states, external state perturbations, and novel goals. It has been validated both in simulation and on real robots with a diverse set of objects.", "conclusion": "BLADE demonstrates the potential of combining imitation learning and model-based planning for long-horizon robotic manipulation tasks."}}
{"id": "2505.21985", "pdf": "https://arxiv.org/pdf/2505.21985", "abs": "https://arxiv.org/abs/2505.21985", "authors": ["Naoto Yoshida", "Tadahiro Taniguchi"], "title": "Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "In multi-agent reinforcement learning (MARL), effective communication\nimproves agent performance, particularly under partial observability. We\npropose MARL-CPC, a framework that enables communication among fully\ndecentralized, independent agents without parameter sharing. MARL-CPC\nincorporates a message learning model based on collective predictive coding\n(CPC) from emergent communication research. Unlike conventional methods that\ntreat messages as part of the action space and assume cooperation, MARL-CPC\nlinks messages to state inference, supporting communication in non-cooperative,\nreward-independent settings. We introduce two algorithms -Bandit-CPC and\nIPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that\nboth outperform standard message-as-action approaches, establishing effective\ncommunication even when messages offer no direct benefit to the sender. These\nresults highlight MARL-CPC's potential for enabling coordination in complex,\ndecentralized environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86MARL-CPC\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4f53\u9884\u6d4b\u7f16\u7801\uff08CPC\uff09\u6d88\u606f\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u72ec\u7acb\u4ee3\u7406\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u65e0\u9700\u53c2\u6570\u5171\u4eab\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cMARL-CPC\u5c06\u6d88\u606f\u4e0e\u72b6\u6001\u63a8\u7406\u8054\u7cfb\u8d77\u6765\uff0c\u652f\u6301\u975e\u5408\u4f5c\u3001\u5956\u52b1\u72ec\u7acb\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u3002\u8bba\u6587\u5f15\u5165\u4e86Bandit-CPC\u548cIPPO-CPC\u4e24\u79cd\u7b97\u6cd5\uff0c\u5e76\u5728\u975e\u5408\u4f5c\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u7b97\u6cd5\u5728\u6d88\u606f\u65e0\u76f4\u63a5\u53d1\u9001\u8005\u5229\u76ca\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u80fd\u591f\u5efa\u7acb\u6709\u6548\u7684\u901a\u4fe1\u673a\u5236\uff0c\u5c55\u793a\u4e86MARL-CPC\u5728\u590d\u6742\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u4fc3\u8fdb\u534f\u8c03\u7684\u6f5c\u529b\u3002", "motivation": "\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\uff0c\u6709\u6548\u7684\u901a\u4fe1\u53ef\u4ee5\u63d0\u5347\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u901a\u4fe1\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u53c2\u6570\u5171\u4eab\u6216\u5047\u8bbe\u5408\u4f5c\u73af\u5883\uff0c\u9650\u5236\u4e86\u5176\u5728\u53bb\u4e2d\u5fc3\u5316\u548c\u975e\u5408\u4f5c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMARL-CPC\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u96c6\u4f53\u9884\u6d4b\u7f16\u7801\uff08CPC\uff09\u7684\u6d88\u606f\u5b66\u4e60\u6a21\u578b\uff0c\u5141\u8bb8\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u72ec\u7acb\u4ee3\u7406\u4e4b\u95f4\u8fdb\u884c\u901a\u4fe1\uff0c\u800c\u65e0\u9700\u53c2\u6570\u5171\u4eab\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u5f15\u5165\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1aBandit-CPC\u548cIPPO-CPC\uff0c\u5206\u522b\u7528\u4e8e\u975e\u5408\u4f5cMARL\u4efb\u52a1\u7684\u8bc4\u4f30\u3002\u8fd9\u4e9b\u7b97\u6cd5\u5c06\u6d88\u606f\u4e0e\u72b6\u6001\u63a8\u7406\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u652f\u6301\u975e\u5408\u4f5c\u548c\u5956\u52b1\u72ec\u7acb\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBandit-CPC\u548cIPPO-CPC\u5728\u975e\u5408\u4f5cMARL\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6\u7684\u6d88\u606f\u4f5c\u4e3a\u52a8\u4f5c\u7684\u65b9\u6cd5\u3002\u5373\u4f7f\u5728\u6d88\u606f\u5bf9\u53d1\u9001\u8005\u6ca1\u6709\u76f4\u63a5\u5229\u76ca\u7684\u60c5\u51b5\u4e0b\uff0cMARL-CPC\u4e5f\u80fd\u5efa\u7acb\u6709\u6548\u7684\u901a\u4fe1\u673a\u5236\u3002", "conclusion": "MARL-CPC\u6846\u67b6\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u53bb\u4e2d\u5fc3\u5316\u72ec\u7acb\u4ee3\u7406\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u65e0\u9700\u53c2\u6570\u5171\u4eab\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u975e\u5408\u4f5c\u3001\u5956\u52b1\u72ec\u7acb\u7684\u73af\u5883\u4e2d\u5de5\u4f5c\u3002\u8fd9\u4e3a\u590d\u6742\u3001\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u7684\u534f\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2505.21524", "pdf": "https://arxiv.org/pdf/2505.21524", "abs": "https://arxiv.org/abs/2505.21524", "authors": ["Amitai Yacobi", "Nir Ben-Ari", "Ronen Talmon", "Uri Shaham"], "title": "Learning Shared Representations from Unpaired Data", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Learning shared representations is a primary area of multimodal\nrepresentation learning. The current approaches to achieve a shared embedding\nspace rely heavily on paired samples from each modality, which are\nsignificantly harder to obtain than unpaired ones. In this work, we demonstrate\nthat shared representations can be learned almost exclusively from unpaired\ndata. Our arguments are grounded in the spectral embeddings of the random walk\nmatrices constructed independently from each unimodal representation. Empirical\nresults in computer vision and natural language processing domains support its\npotential, revealing the effectiveness of unpaired data in capturing meaningful\ncross-modal relations, demonstrating high capabilities in retrieval tasks,\ngeneration, arithmetics, zero-shot, and cross-domain classification. This work,\nto the best of our knowledge, is the first to demonstrate these capabilities\nalmost exclusively from unpaired samples, giving rise to a cross-modal\nembedding that could be viewed as universal, i.e., independent of the specific\nmodalities of the data. Our code IS publicly available at\nhttps://github.com/shaham-lab/SUE.", "AI": {"tldr": "The paper presents a method for learning shared representations from unpaired data in multimodal representation learning, demonstrating its potential and effectiveness in various tasks.", "motivation": "Current methods for learning shared representations rely heavily on paired samples which are harder to obtain than unpaired ones. The authors aim to demonstrate that shared representations can be learned almost exclusively from unpaired data.", "method": "The arguments are grounded in the spectral embeddings of the random walk matrices constructed independently from each unimodal representation. This approach leverages unpaired data to capture meaningful cross-modal relations.", "result": "Empirical results in computer vision and natural language processing domains reveal the effectiveness of unpaired data in capturing cross-modal relations, showing high capabilities in retrieval tasks, generation, arithmetics, zero-shot, and cross-domain classification.", "conclusion": "This work is the first to demonstrate these capabilities almost exclusively from unpaired samples, leading to a cross-modal embedding that could be viewed as universal, independent of the specific modalities of the data."}}
{"id": "2505.21996", "pdf": "https://arxiv.org/pdf/2505.21996", "abs": "https://arxiv.org/abs/2505.21996", "authors": ["Taiye Chen", "Xun Hu", "Zihan Ding", "Chi Jin"], "title": "Learning World Models for Interactive Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundational world models must be both interactive and preserve\nspatiotemporal coherence for effective future planning with action choices.\nHowever, present models for long video generation have limited inherent world\nmodeling capabilities due to two main challenges: compounding errors and\ninsufficient memory mechanisms. We enhance image-to-video models with\ninteractive capabilities through additional action conditioning and\nautoregressive framework, and reveal that compounding error is inherently\nirreducible in autoregressive video generation, while insufficient memory\nmechanism leads to incoherence of world models. We propose video retrieval\naugmented generation (VRAG) with explicit global state conditioning, which\nsignificantly reduces long-term compounding errors and increases spatiotemporal\nconsistency of world models. In contrast, naive autoregressive generation with\nextended context windows and retrieval-augmented generation prove less\neffective for video generation, primarily due to the limited in-context\nlearning capabilities of current video models. Our work illuminates the\nfundamental challenges in video world models and establishes a comprehensive\nbenchmark for improving video generation models with internal world modeling\ncapabilities.", "AI": {"tldr": "Foundational world models require interactive and spatiotemporally coherent for future planning. Current video generation models face challenges of compounding errors and insufficient memory mechanisms. This study proposes VRAG with explicit global state conditioning to reduce long-term compounding errors and enhance spatiotemporal consistency, providing a benchmark for improving video generation models.", "motivation": "To address the limitations in current video generation models that hinder effective future planning due to compounding errors and insufficient memory mechanisms.", "method": "Propose VRAG with explicit global state conditioning to significantly reduce long-term compounding errors and increase spatiotemporal consistency of world models.", "result": "VRAG proves more effective than naive autoregressive generation and retrieval-augmented generation primarily because of its better handling of long-term compounding errors and spatiotemporal consistency.", "conclusion": "This work highlights fundamental challenges in video world models and sets a benchmark for enhancing video generation models with internal world modeling capabilities."}}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003", "abs": "https://arxiv.org/abs/2505.22003", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell.", "AI": {"tldr": "The paper introduces Legal Assist AI, a transformer-based model designed to offer effective legal assistance in India by leveraging large language models. It retrieves relevant legal information and generates accurate responses, aiding diverse users such as legal professionals, scholars, and the general public. Fine-tuned on extensive datasets from the Indian legal domain, it demonstrated remarkable efficiency and specialization in legal Question-Answering, outperforming state-of-the-art models in legal reasoning and accuracy.", "motivation": "To bridge the critical gap of accessible legal assistance in India, where many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information.", "method": "The system retrieves relevant legal information from a curated database and generates accurate responses using a transformer-based model fine-tuned on extensive datasets from the Indian legal domain.", "result": "Evaluated against state-of-the-art models, Legal Assist AI achieved a 60.08% score on the AIBE, outperforming competitors in legal reasoning and accuracy while avoiding common issues like hallucinations.", "conclusion": "Legal Assist AI showcases high reliability and applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset."}}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.", "AI": {"tldr": "Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. The paper introduces VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information.", "motivation": "Traditional text-based methods cannot handle visual-related information and current vision-based RAG approaches are often limited by fixed pipelines and struggle to reason effectively due to insufficient activation of model capabilities.", "method": "VRAG-RL is a novel RL framework where VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. It defines an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. It also employs a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward.", "result": "VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications.", "conclusion": "The conclusion is not explicitly stated in the abstract."}}
{"id": "2505.21533", "pdf": "https://arxiv.org/pdf/2505.21533", "abs": "https://arxiv.org/abs/2505.21533", "authors": ["Thalles Silva", "Helio Pedrini", "Ad\u00edn Ram\u00edrez Rivera"], "title": "Self-Organizing Visual Prototypes for Non-Parametric Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICML 2025, code at https://github.com/sthalles/sop", "summary": "We present Self-Organizing Visual Prototypes (SOP), a new training technique\nfor unsupervised visual feature learning. Unlike existing prototypical\nself-supervised learning (SSL) methods that rely on a single prototype to\nencode all relevant features of a hidden cluster in the data, we propose the\nSOP strategy. In this strategy, a prototype is represented by many semantically\nsimilar representations, or support embeddings (SEs), each containing a\ncomplementary set of features that together better characterize their region in\nspace and maximize training performance. We reaffirm the feasibility of\nnon-parametric SSL by introducing novel non-parametric adaptations of two loss\nfunctions that implement the SOP strategy. Notably, we introduce the SOP Masked\nImage Modeling (SOP-MIM) task, where masked representations are reconstructed\nfrom the perspective of multiple non-parametric local SEs. We comprehensively\nevaluate the representations learned using the SOP strategy on a range of\nbenchmarks, including retrieval, linear evaluation, fine-tuning, and object\ndetection. Our pre-trained encoders achieve state-of-the-art performance on\nmany retrieval benchmarks and demonstrate increasing performance gains with\nmore complex encoders.", "AI": {"tldr": "The paper presents Self-Organizing Visual Prototypes (SOP), a new training technique for unsupervised visual feature learning, which uses semantically similar representations to better characterize data clusters. It also introduces SOP Masked Image Modeling (SOP-MIM) and shows state-of-the-art performance in retrieval benchmarks.", "motivation": "Existing prototypical self-supervised learning methods rely on a single prototype to encode all relevant features of a hidden cluster in the data, which may not be sufficient.", "method": "The SOP strategy represents a prototype by many semantically similar representations, or support embeddings (SEs), each containing a complementary set of features. Non-parametric SSL is reaffirmed by introducing novel non-parametric adaptations of two loss functions that implement the SOP strategy. The SOP Masked Image Modeling (SOP-MIM) task reconstructs masked representations from the perspective of multiple non-parametric local SEs.", "result": "The pre-trained encoders achieve state-of-the-art performance on many retrieval benchmarks and demonstrate increasing performance gains with more complex encoders.", "conclusion": "The SOP strategy is effective in unsupervised visual feature learning and can lead to superior performance in various benchmarks."}}
{"id": "2505.22021", "pdf": "https://arxiv.org/pdf/2505.22021", "abs": "https://arxiv.org/abs/2505.22021", "authors": ["Zhihong Tang", "Yang Li"], "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u6863\u56fe\u50cf\u589e\u5f3a\u7f51\u7edcGL-PGENet\uff0c\u9488\u5bf9\u591a\u9000\u5316\u5f69\u8272\u6587\u6863\u56fe\u50cf\uff0c\u901a\u8fc7\u5168\u5c40\u4e0e\u5c40\u90e8\u589e\u5f3a\u6846\u67b6\u3001\u53cc\u5206\u652f\u5c40\u90e8\u7cbe\u5316\u7f51\u7edc\u4ee5\u53ca\u6539\u8fdb\u7684NestUNet\u67b6\u6784\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230SOTA\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u8de8\u57df\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u9000\u5316\u6062\u590d\u6216\u7070\u5ea6\u56fe\u50cf\u5904\u7406\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u591a\u9000\u5316\u5f69\u8272\u6587\u6863\u56fe\u50cf\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86GL-PGENet\uff0c\u5305\u542b\uff1a1) \u5c42\u6b21\u5316\u589e\u5f3a\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u5916\u89c2\u6821\u6b63\u548c\u5c40\u90e8\u7cbe\u7ec6\u5316\uff1b2) \u53cc\u5206\u652f\u5c40\u90e8\u7cbe\u5316\u7f51\u7edc\uff0c\u91c7\u7528\u53c2\u6570\u751f\u6210\u673a\u5236\u66ff\u4ee3\u76f4\u63a5\u9884\u6d4b\uff1b3) \u6539\u8fdb\u7684NestUNet\u67b6\u6784\uff0c\u878d\u5408\u4f4e\u7ea7\u50cf\u7d20\u7279\u5f81\u548c\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u5927\u89c4\u6a21\u9884\u8bad\u7ec3+\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff09\u3002", "result": "\u5728DocUNet\u548cRealDAE\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.7721\u548c0.9480\u7684SSIM\u5206\u6570\uff0c\u5c55\u73b0\u8de8\u57df\u9002\u5e94\u6027\u53ca\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u9ad8\u6548\u6027\uff0c\u65e0\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "GL-PGENet\u5728\u591a\u9000\u5316\u5f69\u8272\u6587\u6863\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.22027", "pdf": "https://arxiv.org/pdf/2505.22027", "abs": "https://arxiv.org/abs/2505.22027", "authors": ["Miika Toikkanen", "June-Woo Kim"], "title": "Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Respiratory sound datasets are limited in size and quality, making high\nperformance difficult to achieve. Ensemble models help but inevitably increase\ncompute cost at inference time. Soft label training distills knowledge\nefficiently with extra cost only at training. In this study, we explore soft\nlabels for respiratory sound classification as an architecture-agnostic\napproach to distill an ensemble of teacher models into a student model. We\nexamine different variations of our approach and find that even a single\nteacher, identical to the student, considerably improves performance beyond its\nown capability, with optimal gains achieved using only a few teachers. We\nachieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the\nprevious best by 0.85 and improving average Scores across architectures by more\nthan 1.16. Our results highlight the effectiveness of knowledge distillation\nwith soft labels for respiratory sound classification, regardless of size or\narchitecture.", "AI": {"tldr": "The paper explores the use of soft labels for respiratory sound classification to distill an ensemble of teacher models into a student model, achieving new state-of-the-art performance.", "motivation": "Respiratory sound datasets are limited in size and quality, making high performance difficult to achieve. Traditional ensemble models help but increase compute cost at inference time.", "method": "The study uses soft label training as an architecture-agnostic approach to distill knowledge from an ensemble of teacher models into a student model. They examine different variations of this approach.", "result": "Even a single teacher model identical to the student significantly improves performance beyond its own capability, with optimal gains achieved using only a few teachers. The method achieves a new state-of-the-art score of 64.39 on ICHBI, surpassing the previous best by 0.85 and improving average scores across architectures by more than 1.16.", "conclusion": "The results demonstrate the effectiveness of knowledge distillation with soft labels for respiratory sound classification, regardless of the size or architecture of the models."}}
{"id": "2505.21536", "pdf": "https://arxiv.org/pdf/2505.21536", "abs": "https://arxiv.org/abs/2505.21536", "authors": ["Federico Zocco", "Andrea Corti", "Monica Malvezzi"], "title": "CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero", "categories": ["cs.CY", "cs.CE", "cs.LG"], "comment": "To be submitted", "summary": "The demand of finite raw materials will keep increasing as they fuel modern\nsociety. Simultaneously, solutions for stopping carbon emissions in the short\nterm are not available, thus making the net zero target extremely challenging\nto achieve at scale. The circular economy (CE) paradigm is gaining attention as\na solution to address climate change and the uncertainties of supplies of\ncritical materials. Hence, in this paper, we introduce CiRL, a deep\nreinforcement learning (DRL) library of environments focused on the circularity\nof both solid and fluid materials. The integration of DRL into the design of\nmaterial circularity is possible thanks to the formalism of thermodynamical\nmaterial networks, which is underpinned by compartmental dynamical\nthermodynamics. Along with the focus on circularity, this library has three\nmore features: the new CE-oriented environments are in the state-space form,\nwhich is typically used in dynamical systems analysis and control designs; it\nis based on a state-of-the-art Python library of DRL algorithms, namely,\nStable-Baselines3; and it is developed in Google Colaboratory to be accessible\nto researchers from different disciplines and backgrounds as is often the case\nfor circular economy researchers and engineers. CiRL is publicly available.", "AI": {"tldr": "The paper introduces CiRL, a deep reinforcement learning (DRL) library focused on material circularity to address climate change and critical material supply uncertainties.", "motivation": "The demand for finite raw materials is increasing while solutions to stop carbon emissions in the short term are unavailable, making it challenging to achieve net zero targets. The circular economy paradigm offers a solution to these issues.", "method": "CiRL integrates DRL into the design of material circularity using thermodynamical material networks formalism underpinned by compartmental dynamical thermodynamics. It features environments in state-space form, uses Stable-Baselines3, and is developed in Google Colaboratory.", "result": "CiRL provides a publicly available tool to help researchers from different disciplines tackle challenges related to material circularity in the context of the circular economy.", "conclusion": "CiRL is introduced as a promising tool for advancing the circular economy and addressing challenges related to climate change and critical material supplies."}}
{"id": "2505.22029", "pdf": "https://arxiv.org/pdf/2505.22029", "abs": "https://arxiv.org/abs/2505.22029", "authors": ["Jinming Zhang", "Xuanru Zhou", "Jiachen Lian", "Shuhe Li", "William Li", "Zoe Ezzes", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Jet Vonk", "Brittany Morin", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Submitted to Interspeech 2025", "summary": "Speech dysfluency detection is crucial for clinical diagnosis and language\nassessment, but existing methods are limited by the scarcity of high-quality\nannotated data. Although recent advances in TTS model have enabled synthetic\ndysfluency generation, existing synthetic datasets suffer from unnatural\nprosody and limited contextual diversity. To address these limitations, we\npropose LLM-Dys -- the most comprehensive dysfluent speech corpus with\nLLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency\ncategories spanning both word and phoneme levels. Building upon this resource,\nwe improve an end-to-end dysfluency detection framework. Experimental\nvalidation demonstrates state-of-the-art performance. All data, models, and\ncode are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.", "AI": {"tldr": "The paper presents LLM-Dys, a comprehensive dysfluent speech corpus using LLM-enhanced dysfluency simulation across 11 categories, and an improved end-to-end detection framework achieving state-of-the-art performance.", "motivation": "Speech dysfluency detection is critical for clinical diagnosis and language assessment, but current methods are constrained by limited high-quality annotated data. Synthetic datasets exist but have issues with unnatural prosody and lack of contextual diversity.", "method": "Propose LLM-Dys, the most extensive dysfluent speech corpus with LLM-enhanced dysfluency simulation covering 11 categories at word and phoneme levels. Utilize this dataset to improve an end-to-end dysfluency detection framework.", "result": "Experimental validation shows state-of-the-art performance in dysfluency detection.", "conclusion": "LLM-Dys addresses limitations in existing datasets and improves dysfluency detection. All resources are open-sourced."}}
{"id": "2505.21545", "pdf": "https://arxiv.org/pdf/2505.21545", "abs": "https://arxiv.org/abs/2505.21545", "authors": ["Chika Maduabuchi", "Hao Chen", "Yujin Han", "Jindong Wang"], "title": "Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Code: https://github.com/chikap421/catlvdm Models:\n  https://huggingface.co/Chikap421/catlvdm-checkpoints/tree/main", "summary": "Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are\nsensitive to imperfect conditioning, which causes semantic drift and temporal\nincoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the\nfirst corruption-aware training framework for LVDMs that improves robustness\nthrough structured, data-aligned noise injection. Our method includes\nBatch-Centered Noise Injection (BCNI), which perturbs embeddings along\nintra-batch semantic directions to preserve temporal consistency. BCNI is\nespecially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and\nMSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects\nnoise along dominant spectral directions to improve low-frequency smoothness,\nshowing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across\nWebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.\nAblation studies confirm the benefit of low-rank, data-aligned noise. Our\ntheoretical analysis further explains how such perturbations tighten entropy,\nWasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM\nestablishes a principled, scalable training approach for robust video diffusion\nunder multimodal noise. Code and models: https://github.com/chikap421/catlvdm", "AI": {"tldr": "CAT-LVDM is the first corruption-aware training framework for LVDMs, which improves robustness through structured, data-aligned noise injection. BCNI and SACN are proposed to preserve temporal consistency and improve low-frequency smoothness respectively.", "motivation": "LVDMs achieve high-quality generation but are sensitive to imperfect conditioning, which causes semantic drift and temporal incoherence on noisy, web-scale video-text datasets.", "method": "The method includes Batch-Centered Noise Injection (BCNI), which perturbs embeddings along intra-batch semantic directions to preserve temporal consistency, and Spectrum-Aware Contextual Noise (SACN), which injects noise along dominant spectral directions to improve low-frequency smoothness.", "result": "On average, BCNI reduces FVD by 31.9% across WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.", "conclusion": "CAT-LVDM establishes a principled, scalable training approach for robust video diffusion under multimodal noise."}}
{"id": "2505.22038", "pdf": "https://arxiv.org/pdf/2505.22038", "abs": "https://arxiv.org/abs/2505.22038", "authors": ["Kaiyuan Li", "Xiaoyue Chen", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown impressive performance across\nmulti-modal tasks by encoding images into thousands of tokens. However, the\nlarge number of image tokens results in significant computational overhead, and\nthe use of dynamic high-resolution inputs further increases this burden.\nPrevious approaches have attempted to reduce the number of image tokens through\ntoken pruning, typically by selecting tokens based on attention scores or image\ntoken diversity. Through empirical studies, we observe that existing methods\noften overlook the joint impact of pruning on both the current layer's output\n(local) and the outputs of subsequent layers (global), leading to suboptimal\npruning decisions. To address this challenge, we propose Balanced Token Pruning\n(BTP), a plug-and-play method for pruning vision tokens. Specifically, our\nmethod utilizes a small calibration set to divide the pruning process into\nmultiple stages. In the early stages, our method emphasizes the impact of\npruning on subsequent layers, whereas in the deeper stages, the focus shifts\ntoward preserving the consistency of local outputs. Extensive experiments\nacross various LVLMs demonstrate the broad effectiveness of our approach on\nmultiple benchmarks. Our method achieves a 78% compression rate while\npreserving 96.7% of the original models' performance on average.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBalanced Token Pruning(BTP)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7b56\u7565\u5bf9\u89c6\u89c9\u6807\u8bb0\u8fdb\u884c\u526a\u679d\uff0c\u5728\u65e9\u671f\u9636\u6bb5\u5173\u6ce8\u526a\u679d\u5bf9\u540e\u7eed\u5c42\u7684\u5f71\u54cd\uff0c\u800c\u5728\u8f83\u6df1\u9636\u6bb5\u5219\u4fa7\u91cd\u4e8e\u4fdd\u6301\u5c40\u90e8\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aLVLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u6709\u6548\u6027\uff0c\u8fbe\u5230\u4e8678%\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u59cb\u6a21\u578b\u6027\u80fd\u768496.7%\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u6807\u8bb0\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u526a\u679d\u5bf9\u5f53\u524d\u5c42\u8f93\u51fa\uff08\u5c40\u90e8\uff09\u548c\u540e\u7eed\u5c42\u8f93\u51fa\uff08\u5168\u5c40\uff09\u7684\u8054\u5408\u5f71\u54cd\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684\u526a\u679d\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBalanced Token Pruning(BTP)\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5c0f\u6821\u51c6\u96c6\u5c06\u526a\u679d\u8fc7\u7a0b\u5206\u4e3a\u591a\u4e2a\u9636\u6bb5\u3002\u65e9\u671f\u9636\u6bb5\u5f3a\u8c03\u526a\u679d\u5bf9\u540e\u7eed\u5c42\u7684\u5f71\u54cd\uff0c\u8f83\u6df1\u9636\u6bb5\u5219\u4fa7\u91cd\u4e8e\u4fdd\u6301\u5c40\u90e8\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aLVLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8fbe\u523078%\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u755996.7%\u7684\u539f\u59cb\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "BTP\u662f\u4e00\u79cd\u6709\u6548\u7684\u63d2\u4ef6\u5f0f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.22067", "pdf": "https://arxiv.org/pdf/2505.22067", "abs": "https://arxiv.org/abs/2505.22067", "authors": ["Xinyu Xia", "Xingjun Ma", "Yunfeng Hu", "Ting Qu", "Hong Chen", "Xun Gong"], "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Ensuring robust and generalizable autonomous driving requires not only broad\nscenario coverage but also efficient repair of failure cases, particularly\nthose related to challenging and safety-critical scenarios. However, existing\nscenario generation and selection methods often lack adaptivity and semantic\nrelevance, limiting their impact on performance improvement. In this paper, we\npropose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving\nsystems to self-evolve by repairing failure cases through targeted scenario\nrecommendation. By analyzing performance logs, SERA identifies failure patterns\nand dynamically retrieves semantically aligned scenarios from a structured\nbank. An LLM-based reflection mechanism further refines these recommendations\nto maximize relevance and diversity. The selected scenarios are used for\nfew-shot fine-tuning, enabling targeted adaptation with minimal data.\nExperiments on the benchmark show that SERA consistently improves key metrics\nacross multiple autonomous driving baselines, demonstrating its effectiveness\nand generalizability under safety-critical conditions.", "AI": {"tldr": "SERA is an LLM-powered framework designed to enhance autonomous driving systems by identifying and repairing failure cases through targeted scenario recommendations, which improves key metrics in safety-critical conditions.", "motivation": "Existing methods for generating and selecting scenarios often lack adaptivity and semantic relevance, thus limiting their impact on improving the performance of autonomous driving systems.", "method": "The SERA framework analyzes performance logs to identify failure patterns, dynamically retrieves semantically aligned scenarios from a structured bank, and refines these recommendations using an LLM-based reflection mechanism. Selected scenarios are used for few-shot fine-tuning to enable targeted adaptation with minimal data.", "result": "Experiments demonstrate that SERA consistently improves key metrics across multiple autonomous driving baselines, showing its effectiveness and generalizability under safety-critical conditions.", "conclusion": "SERA enables autonomous driving systems to self-evolve by efficiently repairing failure cases through targeted scenario recommendation."}}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068", "abs": "https://arxiv.org/abs/2505.22068", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO.", "AI": {"tldr": "Previous study shows that powerful LLMs trained with RLVR only refines reasoning path without improving the reasoning capacity in math tasks while SFT with distillation can. From the view of SciIE, we argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way. We propose two-stage training with MimicSFT and R$^2$GRPO. Experiments show that both methods can improve the reasoning capacity.", "motivation": "The motivation is to improve the reasoning capacity of LLMs in SciIE tasks, where previous methods underperforms small Bert-based models.", "method": "The method is a two-stage training approach. The first stage is MimicSFT, which uses structured reasoning templates without needing high-quality chain-of-thought data. The second stage is R$^2$GRPO with relevance and rule-induced rewards.", "result": "Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction.", "conclusion": "The conclusion is that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE."}}
{"id": "2505.21561", "pdf": "https://arxiv.org/pdf/2505.21561", "abs": "https://arxiv.org/abs/2505.21561", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Marouane Tliba", "Lauren Mills", "Ahmet Enis Cetin", "Mohammed H Elnagar"], "title": "Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment", "categories": ["cs.CV", "cs.LG"], "comment": "This paper has been accepted to the CVPR Workshop 2025, to be held in\n  Nashville, Tennessee", "summary": "We introduce a novel deep learning framework for the automated staging of\nspheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in\nboth orthodontics and forensic anthropology. Our approach leverages a\ndual-model architecture wherein a teacher model, trained on manually cropped\nimages, transfers its precise spatial understanding to a student model that\noperates on full, uncropped images. This knowledge distillation is facilitated\nby a newly formulated loss function that aligns spatial logits as well as\nincorporates gradient-based attention spatial mapping, ensuring that the\nstudent model internalizes the anatomically relevant features without relying\non external cropping or YOLO-based segmentation. By leveraging expert-curated\ndata and feedback at each step, our framework attains robust diagnostic\naccuracy, culminating in a clinically viable end-to-end pipeline. This\nstreamlined approach obviates the need for additional pre-processing tools and\naccelerates deployment, thereby enhancing both the efficiency and consistency\nof skeletal maturation assessment in diverse clinical settings.", "AI": {"tldr": "The paper presents a new deep learning framework for automatically staging spheno-occipital synchondrosis (SOS) fusion, using a dual-model architecture with knowledge distillation to achieve high diagnostic accuracy without needing extra pre-processing tools.", "motivation": "To create an automated and efficient method for staging SOS fusion, which is important in orthodontics and forensic anthropology, while avoiding reliance on external cropping or segmentation methods.", "method": "A dual-model architecture where a teacher model trained on manually cropped images transfers its knowledge to a student model operating on full images through a novel loss function incorporating gradient-based attention spatial mapping.", "result": "Achieves robust diagnostic accuracy and creates a clinically viable end-to-end pipeline for skeletal maturation assessment.", "conclusion": "This approach eliminates the need for additional pre-processing tools, enhances efficiency and consistency, and accelerates deployment in clinical settings."}}
{"id": "2505.22086", "pdf": "https://arxiv.org/pdf/2505.22086", "abs": "https://arxiv.org/abs/2505.22086", "authors": ["Runkai Li", "Jia Xiong", "Xi Wang"], "title": "iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "High-Level Synthesis (HLS) serves as an agile hardware development tool that\nstreamlines the circuit design by abstracting the register transfer level into\nbehavioral descriptions, while allowing designers to customize the generated\nmicroarchitectures through optimization directives. However, the combinatorial\nexplosion of possible directive configurations yields an intractable design\nspace. Traditional design space exploration (DSE) methods, despite adopting\nheuristics or constructing predictive models to accelerate Pareto-optimal\ndesign acquisition, still suffer from prohibitive exploration costs and\nsuboptimal results. Addressing these concerns, we introduce iDSE, the first\nLLM-aided DSE framework that leverages HLS design quality perception to\neffectively navigate the design space. iDSE intelligently pruns the design\nspace to guide LLMs in calibrating representative initial sampling designs,\nexpediting convergence toward the Pareto front. By exploiting the convergent\nand divergent thinking patterns inherent in LLMs for hardware optimization,\niDSE achieves multi-path refinement of the design quality and diversity.\nExtensive experiments demonstrate that iDSE outperforms heuristic-based DSE\nmethods by 5.1$\\times$$\\sim$16.6$\\times$ in proximity to the reference Pareto\nfront, matching NSGA-II with only 4.6% of the explored designs. Our work\ndemonstrates the transformative potential of LLMs in scalable and efficient HLS\ndesign optimization, offering new insights into multiobjective optimization\nchallenges.", "AI": {"tldr": "High-Level Synthesis (HLS) simplifies circuit design but has a complex design space. Traditional methods for exploring this space are inefficient. This paper introduces iDSE, an LLM-aided framework that efficiently explores the HLS design space by intelligently pruning it and leveraging LLMs' thinking patterns, leading to significantly better performance than traditional methods with far fewer explored designs.", "motivation": "To address the inefficiencies and suboptimal results of traditional design space exploration methods in High-Level Synthesis, which struggles with the combinatorial explosion of directive configurations.", "method": "iDSE is the first LLM-aided DSE framework that prunes the design space intelligently to guide LLMs in calibrating initial sampling designs, expediting convergence toward the Pareto front while achieving multi-path refinement through LLMs' convergent and divergent thinking patterns.", "result": "Extensive experiments show that iDSE outperforms heuristic-based DSE methods by 5.1$\times$$sim$16.6$\times$ in proximity to the reference Pareto front, achieving similar results to NSGA-II with only 4.6% of the explored designs.", "conclusion": "iDSE demonstrates the potential of LLMs in efficient and scalable HLS design optimization, providing new insights into overcoming multiobjective optimization challenges."}}
{"id": "2505.21564", "pdf": "https://arxiv.org/pdf/2505.21564", "abs": "https://arxiv.org/abs/2505.21564", "authors": ["Koki Matsuishi", "Tsuyoshi Okita"], "title": "Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "In deep multi-instance learning, the number of applicable instances depends\non the data set. In histopathology images, deep learning multi-instance\nlearners usually assume there are hundreds to thousands instances in a bag.\nHowever, when the number of instances in a bag increases to 256 in brain\nhematoma CT, learning becomes extremely difficult. In this paper, we address\nthis drawback. To overcome this problem, we propose using a pre-trained model\nwith self-supervised learning for the multi-instance learner as a downstream\ntask. With this method, even when the original target task suffers from the\nspurious correlation problem, we show improvements of 5% to 13% in accuracy and\n40% to 55% in the F1 measure for the hypodensity marker classification of brain\nhematoma CT.", "AI": {"tldr": "In deep multi-instance learning, especially for brain hematoma CT images, learning becomes difficult when the number of instances increases. This paper proposes a pre-trained model with self-supervised learning to overcome this problem.", "motivation": "The motivation is to address the difficulty in learning when the number of instances in a bag increases to 256 in brain hematoma CT images.", "method": "The proposed method uses a pre-trained model with self-supervised learning for the multi-instance learner as a downstream task.", "result": "This method shows improvements of 5% to 13% in accuracy and 40% to 55% in the F1 measure for the hypodensity marker classification of brain hematoma CT.", "conclusion": "Using a pre-trained model with self-supervised learning can significantly improve the performance of multi-instance learning in brain hematoma CT images."}}
{"id": "2505.22093", "pdf": "https://arxiv.org/pdf/2505.22093", "abs": "https://arxiv.org/abs/2505.22093", "authors": ["Santiago Berrezueta-Guzman", "Stephan Krusche", "Stefan Wagner"], "title": "From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "This is the authors' preprint version of a paper accepted at the 11th\n  International Symposium on Educational Technology, to be held in July 2025,\n  in Bangkok, Thailand. The final published version will be available via IEEE\n  Xplore Library", "summary": "The rapid adoption of AI powered coding assistants like ChatGPT and other\ncoding copilots is transforming programming education, raising questions about\nassessment practices, academic integrity, and skill development. As educators\nseek alternatives to traditional grading methods susceptible to AI enabled\nplagiarism, structured peer assessment could be a promising strategy. This\npaper presents an empirical study of a rubric based, anonymized peer review\nprocess implemented in a large introductory programming course.\n  Students evaluated each other's final projects (2D game), and their\nassessments were compared to instructor grades using correlation, mean absolute\nerror, and root mean square error (RMSE). Additionally, reflective surveys from\n47 teams captured student perceptions of fairness, grading behavior, and\npreferences regarding grade aggregation. Results show that peer review can\napproximate instructor evaluation with moderate accuracy and foster student\nengagement, evaluative thinking, and interest in providing good feedback to\ntheir peers. We discuss these findings for designing scalable, trustworthy peer\nassessment systems to face the age of AI assisted coding.", "AI": {"tldr": "The paper explores using structured peer assessment in programming courses amidst the rise of AI coding assistants, finding it moderately accurate compared to instructor evaluations and beneficial for student engagement and feedback skills.", "motivation": "To address challenges in programming education caused by AI-powered coding assistants, such as plagiarism concerns, and explore alternative grading methods that maintain academic integrity and promote skill development.", "method": "An empirical study was conducted in a large introductory programming course where students performed rubric-based, anonymized peer reviews of each other's final projects (2D games). Assessments were compared to instructor grades using correlation, mean absolute error, and RMSE. Surveys from 47 teams captured student perceptions on fairness, grading behavior, and preferences.", "result": "Peer review approximated instructor evaluation with moderate accuracy. It also enhanced student engagement, evaluative thinking, and interest in providing quality feedback.", "conclusion": "Structured peer assessment is a promising strategy for scalable and trustworthy assessment systems in the era of AI-assisted coding, fostering both technical and metacognitive skills."}}
{"id": "2505.21566", "pdf": "https://arxiv.org/pdf/2505.21566", "abs": "https://arxiv.org/abs/2505.21566", "authors": ["Gao Huayu", "Huang Tengjiu", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Diffusion Model-based Activity Completion for AI Motion Capture from Videos", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages, 16 figures", "summary": "AI-based motion capture is an emerging technology that offers a\ncost-effective alternative to traditional motion capture systems. However,\ncurrent AI motion capture methods rely entirely on observed video sequences,\nsimilar to conventional motion capture. This means that all human actions must\nbe predefined, and movements outside the observed sequences are not possible.\nTo address this limitation, we aim to apply AI motion capture to virtual\nhumans, where flexible actions beyond the observed sequences are required. We\nassume that while many action fragments exist in the training data, the\ntransitions between them may be missing. To bridge these gaps, we propose a\ndiffusion-model-based action completion technique that generates complementary\nhuman motion sequences, ensuring smooth and continuous movements. By\nintroducing a gate module and a position-time embedding module, our approach\nachieves competitive results on the Human3.6M dataset. Our experimental results\nshow that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but\nis slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size\n(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural\nand coherent motion sequences. Additionally, we propose a method for extracting\nsensor data, including acceleration and angular velocity, from human motion\nsequences.", "AI": {"tldr": "AI-based motion capture technology using a diffusion-model-based action completion technique for virtual humans, with competitive results on Human3.6M dataset.", "motivation": "To overcome the limitation of current AI motion capture methods which rely entirely on observed video sequences and cannot generate movements outside these sequences.", "method": "Propose a diffusion-model-based action completion technique with a gate module and a position-time embedding module to generate smooth and continuous human motion sequences beyond observed actions.", "result": "MDC-Net outperforms existing methods in ADE, FDE, and MMADE metrics but is slightly less accurate in MMFDE; it has a smaller model size compared to HumanMAC and generates more natural motion sequences. Also, a method for extracting sensor data from human motion sequences is proposed.", "conclusion": "The proposed MDC-Net achieves competitive performance in generating flexible and coherent human motions, offering a cost-effective alternative to traditional motion capture systems."}}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096", "abs": "https://arxiv.org/abs/2505.22096", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially.", "AI": {"tldr": "The paper proposes constructing a comprehensive knowledge base for text-to-SQL translation that leverages Large Language Models (LLMs) and outperforms existing methods.", "motivation": "To address the limitation of LLMs in generating accurate SQL queries due to insufficient parametric knowledge covering diverse and domain-specific queries.", "method": "Propose constructing a comprehensive knowledge base for text-to-SQL, combining available questions, associated database schemas, and relevant knowledge. This knowledge base can be reused for unseen databases from different datasets and domains.", "result": "Validated on multiple text-to-SQL datasets, considering both overlapping and non-overlapping database scenarios, and substantially outperforms relevant baselines.", "conclusion": "Constructing a knowledge base for text-to-SQL improves the accuracy of generated SQL queries and can be effectively reused across different datasets and domains."}}
{"id": "2505.21567", "pdf": "https://arxiv.org/pdf/2505.21567", "abs": "https://arxiv.org/abs/2505.21567", "authors": ["Feng Jiang", "Zihao Zheng", "Xiuping Cui", "Maoliang Li", "JIayu Chen", "Xiang Chen"], "title": "EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the development of Embodied Artificial intelligence, the end-to-end\ncontrol policy such as Vision-Language-Action (VLA) model has become the\nmainstream. Existing VLA models faces expensive computing/storage cost, which\nneed to be optimized. Quantization is considered as the most effective method\nwhich can not only reduce the memory cost but also achieve computation\nacceleration. However, we find the token alignment of VLA models hinders the\napplication of existing quantization methods. To address this, we proposed an\noptimized framework called EaqVLA, which apply encoding-aligned quantization to\nVLA models. Specifically, we propose an complete analysis method to find the\nmisalignment in various granularity. Based on the analysis results, we propose\na mixed precision quantization with the awareness of encoding alignment.\nExperiments shows that the porposed EaqVLA achieves better quantization\nperformance (with the minimal quantization loss for end-to-end action control\nand xxx times acceleration) than existing quantization methods.", "AI": {"tldr": "This paper addresses the challenge of applying quantization to Vision-Language-Action (VLA) models, which are used in embodied artificial intelligence. The authors propose EaqVLA, a framework that uses encoding-aligned quantization for VLA models. This includes a method to identify misalignment and a mixed precision quantization technique. Experiments show that EaqVLA performs better than existing methods, with minimal quantization loss and significant acceleration.", "motivation": "The motivation of this paper is to optimize the expensive computing and storage costs associated with existing Vision-Language-Action (VLA) models, which are prevalent in embodied artificial intelligence. Quantization is seen as an effective solution, but the token alignment in VLA models poses a challenge for its application.", "method": "The authors propose EaqVLA, an optimized framework that applies encoding-aligned quantization to VLA models. They introduce a complete analysis method to detect misalignment at various granularities within the VLA models. Based on these findings, they implement a mixed precision quantization approach that considers encoding alignment.", "result": "Experiments demonstrate that EaqVLA achieves superior quantization performance compared to existing methods, characterized by minimal quantization loss for end-to-end action control and a significant acceleration factor (xxx times).", "conclusion": "EaqVLA, with its encoding-aligned quantization strategy, successfully overcomes the challenges posed by token alignment in VLA models. It provides an effective means to reduce memory costs and accelerate computations in VLA models, outperforming current quantization techniques."}}
{"id": "2505.22106", "pdf": "https://arxiv.org/pdf/2505.22106", "abs": "https://arxiv.org/abs/2505.22106", "authors": ["Junqi Zhao", "Jinzheng Zhao", "Haohe Liu", "Yun Chen", "Lu Han", "Xubo Liu", "Mark Plumbley", "Wenwu Wang"], "title": "AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Diffusion models have significantly improved the quality and diversity of\naudio generation but are hindered by slow inference speed. Rectified flow\nenhances inference speed by learning straight-line ordinary differential\nequation (ODE) paths. However, this approach requires training a flow-matching\nmodel from scratch and tends to perform suboptimally, or even poorly, at low\nstep counts. To address the limitations of rectified flow while leveraging the\nadvantages of advanced pre-trained diffusion models, this study integrates\npre-trained models with the rectified diffusion method to improve the\nefficiency of text-to-audio (TTA) generation. Specifically, we propose\nAudioTurbo, which learns first-order ODE paths from deterministic noise sample\npairs generated by a pre-trained TTA model. Experiments on the AudioCaps\ndataset demonstrate that our model, with only 10 sampling steps, outperforms\nprior models and reduces inference to 3 steps compared to a flow-matching-based\nacceleration model.", "AI": {"tldr": "AudioTurbo uses pre-trained models with rectified diffusion to enhance text-to-audio generation efficiency, reducing sampling steps and inference time.", "motivation": "Diffusion models excel in audio generation quality and diversity but suffer from slow inference speed. Rectified flow improves this speed but needs retraining and performs poorly at low step counts.", "method": "The study integrates pre-trained diffusion models with the rectified diffusion method, proposing AudioTurbo which learns first-order ODE paths from noise sample pairs generated by a pre-trained TTA model.", "result": "Experiments on the AudioCaps dataset show that AudioTurbo outperforms previous models with only 10 sampling steps and reduces inference to 3 steps compared to flow-matching-based acceleration models.", "conclusion": "AudioTurbo successfully leverages pre-trained models to improve the efficiency of text-to-audio generation while addressing the limitations of rectified flow."}}
{"id": "2505.21574", "pdf": "https://arxiv.org/pdf/2505.21574", "abs": "https://arxiv.org/abs/2505.21574", "authors": ["Dang Nguyen", "Jiping Li", "Jinghao Zheng", "Baharan Mirzasoleiman"], "title": "Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Synthetically augmenting training datasets with diffusion models has been an\neffective strategy for improving generalization of image classifiers. However,\nexisting techniques struggle to ensure the diversity of generation and increase\nthe size of the data by up to 10-30x to improve the in-distribution\nperformance. In this work, we show that synthetically augmenting part of the\ndata that is not learned early in training outperforms augmenting the entire\ndataset. By analyzing a two-layer CNN, we prove that this strategy improves\ngeneralization by promoting homogeneity in feature learning speed without\namplifying noise. Our extensive experiments show that by augmenting only\n30%-40% of the data, our method boosts the performance by up to 2.8% in a\nvariety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,\nCIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.\nNotably, our method applied with SGD outperforms the SOTA optimizer, SAM, on\nCIFAR-100 and TinyImageNet. It can also easily stack with existing weak and\nstrong augmentation strategies to further boost the performance.", "AI": {"tldr": "\u901a\u8fc7\u4ec5\u589e\u5f3a\u672a\u5728\u8bad\u7ec3\u521d\u671f\u88ab\u5b66\u4e60\u7684\u90e8\u5206\u6570\u636e\uff0c\u800c\u975e\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u6539\u5584\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u6cdb\u5316\u6027\u80fd\u3002\u65b9\u6cd5\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u63d0\u5347\u6700\u9ad8\u8fbe2.8%\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u589e\u5e7f\u7b56\u7565\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5408\u6210\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u5728\u4fdd\u8bc1\u751f\u6210\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u9700\u8981\u5c06\u6570\u636e\u91cf\u589e\u52a010-30\u500d\u4ee5\u63d0\u9ad8\u5206\u5e03\u5185\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5373\u4ec5\u5bf9\u8bad\u7ec3\u521d\u671f\u672a\u88ab\u5feb\u901f\u5b66\u4e60\u7684\u6570\u636e\u90e8\u5206\u8fdb\u884c\u589e\u5f3a\u3002", "method": "\u7814\u7a76\u8005\u5206\u6790\u4e86\u4e24\u5c42CNN\uff0c\u8bc1\u660e\u4ec5\u589e\u5f3a\u90e8\u5206\u6570\u636e\uff08\u7ea630%-40%\uff09\u800c\u975e\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u80fd\u591f\u901a\u8fc7\u4fc3\u8fdb\u7279\u5f81\u5b66\u4e60\u901f\u5ea6\u7684\u540c\u8d28\u6027\u6765\u6539\u5584\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u4e0d\u4f1a\u653e\u5927\u566a\u58f0\u3002\u6b64\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\uff08\u5982ResNet\u3001ViT\u3001DenseNet\uff09\u548c\u6570\u636e\u96c6\uff08\u5982CIFAR-10\u3001CIFAR-100\u3001TinyImageNet\uff09\uff0c\u4ee5\u53ca\u4e0d\u540c\u7684\u4f18\u5316\u5668\uff08\u5982SGD\u3001SAM\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u53ef\u5c06\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe2.8%\u3002\u7279\u522b\u662f\u5728CIFAR-100\u548cTinyImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528SGD\u4f18\u5316\u5668\u65f6\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4f18\u5316\u5668SAM\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u8f7b\u677e\u4e0e\u73b0\u6709\u7684\u5f31\u6216\u5f3a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u53e0\u52a0\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u5730\u589e\u5f3a\u8bad\u7ec3\u521d\u671f\u672a\u88ab\u5feb\u901f\u5b66\u4e60\u7684\u6570\u636e\u90e8\u5206\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u6539\u5584\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u800c\u65e0\u9700\u5927\u5e45\u589e\u52a0\u6570\u636e\u91cf\u3002\u6b64\u65b9\u6cd5\u7b80\u5355\u6613\u7528\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4f18\u5316\u5668\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u589e\u5f3a\u7b56\u7565\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.21580", "pdf": "https://arxiv.org/pdf/2505.21580", "abs": "https://arxiv.org/abs/2505.21580", "authors": ["Anum Fatima", "Gesine Reinert"], "title": "A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "43 pages, 24 figures", "summary": "Complex data are often represented as a graph, which in turn can often be\nviewed as a realisation of a random graph, such as of an inhomogeneous random\ngraph model (IRG). For general fast goodness-of-fit tests in high dimensions,\nkernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,\ntest, and analyse a KSD-type goodness-of-fit test for IRG models that can be\ncarried out with a single observation of the network. The test is applicable to\na network of any size and does not depend on the asymptotic distribution of the\ntest statistic. We also provide theoretical guarantees.", "AI": {"tldr": "The paper develops a KSD-type goodness-of-fit test for IRG models, which can be performed with a single network observation, applies to any network size, and includes theoretical guarantees.", "motivation": "There is a need for fast and effective goodness-of-fit tests for complex data represented as graphs, especially in high dimensions.", "method": "The authors develop a KSD-type goodness-of-fit test specifically for inhomogeneous random graph (IRG) models. This test can be conducted using a single observation of the network and does not rely on the asymptotic distribution of the test statistic.", "result": "The developed test is applicable to networks of any size and provides theoretical guarantees regarding its performance.", "conclusion": "The KSD-type goodness-of-fit test for IRG models offers a powerful tool for analyzing complex network data without requiring multiple observations or relying on asymptotic properties."}}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116", "abs": "https://arxiv.org/abs/2505.22116", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.", "AI": {"tldr": "The paper introduces IOHFuseLM, a multimodal language model for predicting intraoperative hypotension (IOH). It uses a two-stage training strategy involving domain adaptive pretraining and task fine-tuning, aligns clinical descriptions with physiological time series, and converts static patient attributes into structured text. Experiments show it outperforms baselines in identifying IOH events.", "motivation": "Intraoperative hypotension (IOH) is common under general anesthesia and linked to adverse outcomes. However, predicting IOH is difficult due to event sparsity and the challenge of integrating static and dynamic data across patients.", "method": "The proposed IOHFuseLM framework employs a two-stage training strategy: 1) Domain adaptive pretraining on IOH physiological time series augmented through diffusion methods to enhance sensitivity to hypotension patterns; 2) Task fine-tuning on the original clinical dataset to improve the ability to distinguish normotensive from hypotensive states. Additionally, it aligns structured clinical descriptions with physiological time series at the token level and converts static patient attributes into structured text.", "result": "Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events.", "conclusion": "IOHFuseLM effectively predicts IOH events by leveraging multimodal data fusion and a two-stage training strategy, making it applicable for clinical decision support."}}
{"id": "2505.22125", "pdf": "https://arxiv.org/pdf/2505.22125", "abs": "https://arxiv.org/abs/2505.22125", "authors": ["Melrose Tia", "Jezreel Sophia Lanuzo", "Lei Rigi Baltazar", "Marie Joy Lopez-Relente", "Diwa Malaya Qui\u00f1ones", "Jason Albia"], "title": "Sentiment Simulation using Generative AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CY", "I.2; I.6; J.4"], "comment": "18 pages, 10 figures", "summary": "Traditional sentiment analysis relies on surface-level linguistic patterns\nand retrospective data, limiting its ability to capture the psychological and\ncontextual drivers of human sentiment. These limitations constrain its\neffectiveness in applications that require predictive insight, such as policy\ntesting, narrative framing, and behavioral forecasting. We present a robust\nframework for sentiment simulation using generative AI agents embedded with\npsychologically rich profiles. Agents are instantiated from a nationally\nrepresentative survey of 2,485 Filipino respondents, combining sociodemographic\ninformation with validated constructs of personality traits, values, beliefs,\nand socio-political attitudes. The framework includes three stages: (1) agent\nembodiment via categorical or contextualized encodings, (2) exposure to\nreal-world political and economic scenarios, and (3) generation of sentiment\nratings accompanied by explanatory rationales. Using Quadratic Weighted\nAccuracy (QWA), we evaluated alignment between agent-generated and human\nresponses. Contextualized encoding achieved 92% alignment in replicating\noriginal survey responses. In sentiment simulation tasks, agents reached\n81%--86% accuracy against ground truth sentiment, with contextualized profile\nencodings significantly outperforming categorical (p < 0.0001, Cohen's d =\n0.70). Simulation results remained consistent across repeated trials\n(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,\nCohen's d = 0.02). Our findings establish a scalable framework for sentiment\nmodeling through psychographically grounded AI agents. This work signals a\nparadigm shift in sentiment analysis from retrospective classification to\nprospective and dynamic simulation grounded in psychology of sentiment\nformation.", "AI": {"tldr": "The paper presents a sentiment simulation framework using generative AI agents with psychological profiles, achieving high accuracy in replicating human responses and sentiment ratings.", "motivation": "Traditional sentiment analysis methods are limited by their reliance on surface-level linguistic patterns and retrospective data, which restricts their ability to capture the psychological and contextual drivers of human sentiment.", "method": "The framework involves three stages: (1) agent embodiment through categorical or contextualized encodings, (2) exposure to real-world political and economic scenarios, and (3) generation of sentiment ratings with explanatory rationales. Agents are instantiated from a survey of 2,485 Filipino respondents, incorporating sociodemographic information and psychological constructs.", "result": "Contextualized encoding achieved 92% alignment with original survey responses. In sentiment simulation tasks, agents reached 81%-86% accuracy compared to ground truth sentiment, with contextualized profile encodings significantly outperforming categorical ones. The simulation results were consistent across repeated trials and resilient to variations in scenario framing.", "conclusion": "The study establishes a scalable framework for sentiment modeling using AI agents grounded in psychological profiles, marking a shift from retrospective classification to prospective and dynamic simulation."}}
{"id": "2505.21592", "pdf": "https://arxiv.org/pdf/2505.21592", "abs": "https://arxiv.org/abs/2505.21592", "authors": ["Ze Chen", "Shaode Yu"], "title": "Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "under review", "summary": "Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong\nfunction approximation capability. In our previous work, KAN and its variants\nwere explored in score regression for blind image quality assessment (BIQA).\nHowever, these models encounter challenges when processing high-dimensional\nfeatures, leading to limited performance gains and increased computational\ncost. To address these issues, we propose TaylorKAN that leverages the Taylor\nexpansions as learnable activation functions to enhance local approximation\ncapability. To improve the computational efficiency, network depth reduction\nand feature dimensionality compression are integrated into the TaylorKAN-based\nscore regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and\nFLIVE) with authentic distortions, extensive experiments demonstrate that\nTaylorKAN consistently outperforms the other KAN-related models, indicating\nthat the local approximation via Taylor expansions is more effective than\nglobal approximation using orthogonal functions. Its generalization capacity is\nvalidated through inter-database experiments. The findings highlight the\npotential of TaylorKAN as an efficient and robust model for high-dimensional\nscore regression.", "AI": {"tldr": "TaylorKAN, an enhancement of Kolmogorov-Arnold Network (KAN), uses Taylor expansions as learnable activation functions for better local approximation in high-dimensional score regression. It integrates network depth reduction and feature dimensionality compression to improve computational efficiency. Experiments show TaylorKAN outperforms other KAN-related models on five databases with authentic distortions.", "motivation": "KAN and its variants have been used in score regression for BIQA but face challenges when processing high-dimensional features, which limits performance gains and increases computational cost.", "method": "TaylorKAN leverages Taylor expansions as learnable activation functions to enhance local approximation capability. It also integrates network depth reduction and feature dimensionality compression into the score regression pipeline to improve computational efficiency.", "result": "TaylorKAN consistently outperforms other KAN-related models on five databases with authentic distortions. Its local approximation via Taylor expansions is more effective than global approximation using orthogonal functions. Its generalization capacity is validated through inter-database experiments.", "conclusion": "TaylorKAN is an efficient and robust model for high-dimensional score regression."}}
{"id": "2505.22126", "pdf": "https://arxiv.org/pdf/2505.22126", "abs": "https://arxiv.org/abs/2505.22126", "authors": ["Yifan Chang", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Chuanhao Li", "S. Kevin Zhou", "Kaipeng Zhang"], "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.", "AI": {"tldr": "The paper introduces SridBench, the first benchmark for scientific figure generation, revealing that even advanced models like GPT-4o-image fall short of human performance in aspects such as text/visual clarity and scientific correctness.", "motivation": "To address the lack of a benchmark for evaluating AI's ability in scientific illustration generation, which is more knowledge-intensive and laborious compared to general image synthesis.", "method": "Introduced SridBench, a benchmark consisting of 1,120 instances from leading scientific papers across 13 disciplines, with samples evaluated along six dimensions including semantic fidelity and structural accuracy.", "result": "Experimental results showed that top-tier models like GPT-4o-image have shortcomings in text/visual clarity and scientific correctness compared to human performance.", "conclusion": "There is a need for more advanced reasoning-driven visual generation capabilities to improve scientific illustration generation."}}
{"id": "2505.22128", "pdf": "https://arxiv.org/pdf/2505.22128", "abs": "https://arxiv.org/abs/2505.22128", "authors": ["Alejandro D. Mousist"], "title": "Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work addresses mechanical defocus in Earth observation images from the\nIMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted\nto space-based edge computing constraints. Leveraging Sentinel-2 data, our\nmethod estimates the defocus kernel and trains a restoration model within a GAN\nframework, effectively operating without reference images.\n  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and\nPSNR by 25.00%, confirming the model's ability to recover lost details when the\noriginal clean image is known. On IMAGIN-e, where no reference images exist,\nperceptual quality metrics indicate a substantial enhancement, with NIQE\nimproving by 60.66% and BRISQUE by 48.38%, validating real-world onboard\nrestoration. The approach is currently deployed aboard the IMAGIN-e mission,\ndemonstrating its practical application in an operational space environment.\n  By efficiently handling high-resolution images under edge computing\nconstraints, the method enables applications such as water body segmentation\nand contour detection while maintaining processing viability despite resource\nlimitations.", "AI": {"tldr": "This paper proposes a blind deblurring approach for mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, which leverages Sentinel-2 data and operates within a GAN framework without reference images. It demonstrates substantial improvements in image quality metrics and is currently deployed aboard the IMAGIN-e mission.", "motivation": "To address mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, particularly under space-based edge computing constraints.", "method": "The method estimates the defocus kernel and trains a restoration model within a GAN framework using Sentinel-2 data, operating without reference images.", "result": "On synthetic degradation of Sentinel-2 images, SSIM improved by 72.47% and PSNR by 25.00%. On IMAGIN-e images without reference, NIQE improved by 60.66% and BRISQUE by 48.38%.", "conclusion": "The proposed blind deblurring approach successfully enhances image quality under edge computing constraints and is practically applicable in an operational space environment."}}
{"id": "2505.21597", "pdf": "https://arxiv.org/pdf/2505.21597", "abs": "https://arxiv.org/abs/2505.21597", "authors": ["Abdullah Al Mamun", "Pollob Chandra Ray", "Md Rahat Ul Nasib", "Akash Das", "Jia Uddin", "Md Nurul Absur"], "title": "Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "6 pages, & 7 Images", "summary": "The rapid advancement of deep learning in medical image analysis has greatly\nenhanced the accuracy of skin cancer classification. However, current\nstate-of-the-art models, especially those based on transfer learning like\nResNet50, come with significant computational overhead, rendering them\nimpractical for deployment in resource-constrained environments. This study\nproposes a custom CNN model that achieves a 96.7\\% reduction in parameters\n(from 23.9 million in ResNet50 to 692,000) while maintaining a classification\naccuracy deviation of less than 0.022\\%. Our empirical analysis of the HAM10000\ndataset reveals that although transfer learning models provide a marginal\naccuracy improvement of approximately 0.022\\%, they result in a staggering\n13,216.76\\% increase in FLOPs, considerably raising computational costs and\ninference latency. In contrast, our lightweight CNN architecture, which\nencompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,\nsignificantly reduces energy consumption, memory footprint, and inference time.\nThese findings underscore the trade-off between the complexity of deep models\nand their real-world feasibility, positioning our optimized CNN as a practical\nsolution for mobile and edge-based skin cancer diagnostics.", "AI": {"tldr": "In this paper, the authors propose a lightweight custom CNN model for skin cancer classification that significantly reduces the number of parameters and computational costs compared to transfer learning models like ResNet50, while maintaining similar accuracy.", "motivation": "Despite high accuracy, current state-of-the-art models such as ResNet50 have significant computational overhead, making them impractical for resource-constrained environments.", "method": "The authors developed a custom CNN model with fewer parameters and FLOPs. This model was tested on the HAM10000 dataset and compared to ResNet50 in terms of accuracy, computational cost, and inference latency.", "result": "The custom CNN achieved a 96.7% reduction in parameters and a 99.98% reduction in FLOPs compared to ResNet50, with only a 0.022% deviation in classification accuracy.", "conclusion": "The optimized CNN model offers a practical solution for mobile and edge-based skin cancer diagnostics by balancing model complexity with real-world feasibility."}}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137", "abs": "https://arxiv.org/abs/2505.22137", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization.", "AI": {"tldr": "\u5c3d\u7ba1BERT-like\u6a21\u578b\u5728\u8bba\u70b9\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u5bf9\u9f50\u4efb\u52a1\u3002\u901a\u8fc7\u52a0\u5165\u4efb\u52a1\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u548c\u8054\u5408\u57fa\u51c6\u8bad\u7ec3\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u91cd\u65b0\u8bc4\u4f30\u73b0\u6709\u6700\u5148\u8fdb\u7684transformer\u6a21\u578b\u5728\u8bba\u70b9\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u8bc4\u4f30\u56db\u4e2atransformer\u6a21\u578b\uff08\u4e09\u4e2a\u6807\u51c6\u6a21\u578b\u548c\u4e00\u4e2a\u7ecf\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7684\u589e\u5f3a\u6a21\u578b\uff09\u572817\u4e2a\u82f1\u8bed\u53e5\u5b50\u7ea7\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u4f9d\u8d56\u7684\u7279\u5f81\u53ca\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u719f\u6089\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u4e8e\u8bcd\u6c47\u6377\u5f84\u3002\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u548c\u8054\u5408\u57fa\u51c6\u8bad\u7ec3\u53ef\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u73b0\u6709\u7684transformer\u6a21\u578b\u5728\u8bba\u70b9\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u53ef\u80fd\u66f4\u591a\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u7ebf\u7d22\uff0c\u800c\u975e\u771f\u6b63\u7684\u4efb\u52a1\u7406\u89e3\uff1b\u901a\u8fc7\u9002\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002"}}
{"id": "2505.22141", "pdf": "https://arxiv.org/pdf/2505.22141", "abs": "https://arxiv.org/abs/2505.22141", "authors": ["Guanwen Feng", "Zhiyuan Ma", "Yunan Li", "Junwei Jing", "Jiahao Yang", "Qiguang Miao"], "title": "FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in audio-driven talking head generation have achieved\nimpressive results in lip synchronization and emotional expression. However,\nthey largely overlook the crucial task of facial attribute editing. This\ncapability is crucial for achieving deep personalization and expanding the\nrange of practical applications, including user-tailored digital avatars,\nengaging online education content, and brand-specific digital customer service.\nIn these key domains, the flexible adjustment of visual attributes-such as\nhairstyle, accessories, and subtle facial features is essential for aligning\nwith user preferences, reflecting diverse brand identities, and adapting to\nvarying contextual demands. In this paper, we present FaceEditTalker, a unified\nframework that enables controllable facial attribute manipulation while\ngenerating high-quality, audio-synchronized talking head videos. Our method\nconsists of two key components: an image feature space editing module, which\nextracts semantic and detail features and allows flexible control over\nattributes like expression, hairstyle, and accessories; and an audio-driven\nvideo generation module, which fuses these edited features with audio-guided\nfacial landmarks to drive a diffusion-based generator. This design ensures\ntemporal coherence, visual fidelity, and identity preservation across frames.\nExtensive experiments on public datasets demonstrate that our method\noutperforms state-of-the-art approaches in lip-sync accuracy, video quality,\nand attribute controllability. Project page:\nhttps://peterfanfan.github.io/FaceEditTalker/", "AI": {"tldr": "Recent advances in audio-driven talking head generation have achieved impressive results but overlook facial attribute editing. This paper presents FaceEditTalker, a framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos.", "motivation": "Facial attribute editing is crucial for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service.", "method": "FaceEditTalker consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator.", "result": "Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art approaches in lip-sync accuracy, video quality, and attribute controllability.", "conclusion": "FaceEditTalker is a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos."}}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks.", "AI": {"tldr": "The paper presents a framework using low-dimensional attribute representations to connect visual tool perception and linguistic task understanding, achieving 74% accuracy in tool selection tasks with parameter efficiency.", "motivation": "To address the underdevelopment of computational models that capture human's complex cognitive ability in flexible tool selection which distinguishes humans from other species.", "method": "Developed a framework using low-dimensional attribute representations, constructed a dataset (ToolNet) with 115 common tools labeled by 13 attributes, used visual encoders (ResNet or ViT) to extract attributes from tool images and fine-tuned language models (GPT-2, LLaMA, DeepSeek) to derive required attributes from task descriptions.", "result": "Achieved 74% accuracy in tool selection tasks, outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching the performance of much larger models like GPT-4o (73%) with fewer parameters. Manipulation-related attributes were found to be most critical across modalities.", "conclusion": "This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks."}}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165", "abs": "https://arxiv.org/abs/2505.22165", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation.", "AI": {"tldr": "NeoDiff\u7ed3\u5408\u4e86\u79bb\u6563\u548c\u8fde\u7eed\u6269\u6563\u6a21\u578b\u7684\u4f18\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7684\u6269\u6563\u6a21\u578b\u5206\u4e3a\u79bb\u6563\u548c\u8fde\u7eed\u4e24\u7c7b\uff0c\u5404\u6709\u5c40\u9650\u6027\uff1a\u79bb\u6563\u6a21\u578b\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u800c\u8fde\u7eed\u6a21\u578b\u5728\u4ee4\u724c\u4e0a\u7684\u6269\u6563\u8fdb\u7a0b\u662f\u5747\u5300\u7684\uff0c\u9650\u5236\u4e86\u5176\u6355\u6349\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u975e\u540c\u65f6\u8fde\u7eed\u6269\u6563\u6a21\u578b\uff08NeoDiff\uff09\uff0c\u5f15\u5165\u6cca\u677e\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u524d\u5411\u5904\u7406\uff0c\u5b9e\u73b0\u7075\u6d3b\u548c\u7ec6\u7c92\u5ea6\u7684\u566a\u58f0\u8303\u5f0f\uff1b\u53cd\u5411\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u65f6\u95f4\u9884\u6d4b\u5668\u6839\u636e\u4ee4\u724c\u8bed\u4e49\u81ea\u9002\u5e94\u8c03\u8282\u53bb\u566a\u8fdb\u7a0b\uff0c\u5e76\u4f18\u5316\u63a8\u7406\u8ba1\u5212\u4ee5\u786e\u4fdd\u66f4\u7cbe\u786e\u7684\u566a\u58f0\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cNeoDiff\u76f8\u6bd4\u975e\u81ea\u56de\u5f52\u8fde\u7eed\u3001\u79bb\u6563\u6269\u6563\u6a21\u578b\u3001\u8fed\u4ee3\u65b9\u6cd5\u548c\u81ea\u56de\u5f52\u6269\u6563\u65b9\u6cd5\u7b49\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "NeoDiff\u7edf\u4e00\u4e86\u79bb\u6563\u548c\u8fde\u7eed\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u4e3a\u5408\u7406\u548c\u6709\u6548\u7684\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u5177\u6709\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.22174", "pdf": "https://arxiv.org/pdf/2505.22174", "abs": "https://arxiv.org/abs/2505.22174", "authors": ["Georgios Amanatidis", "Alexandros Lolos", "Evangelos Markakis", "Victor Turmel"], "title": "Online Fair Division for Personalized $2$-Value Instances", "categories": ["cs.GT", "cs.AI", "cs.MA"], "comment": null, "summary": "We study an online fair division setting, where goods arrive one at a time\nand there is a fixed set of $n$ agents, each of whom has an additive valuation\nfunction over the goods. Once a good appears, the value each agent has for it\nis revealed and it must be allocated immediately and irrevocably to one of the\nagents. It is known that without any assumptions about the values being\nseverely restricted or coming from a distribution, very strong impossibility\nresults hold in this setting. To bypass the latter, we turn our attention to\ninstances where the valuation functions are restricted. In particular, we study\npersonalized $2$-value instances, where there are only two possible values each\nagent may have for each good, possibly different across agents, and we show how\nto obtain worst case guarantees with respect to well-known fairness notions,\nsuch as maximin share fairness and envy-freeness up to one (or two) good(s). We\nsuggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at\nevery time step and show that this is the best possible any deterministic\nalgorithm can achieve if one cares about every single time step; nevertheless,\neventually the allocation constructed by our algorithm becomes a $1/4$-MMS\nallocation. To achieve this, the algorithm implicitly maintains a fragile\nsystem of priority levels for all agents. Further, we show that, by allowing\nsome limited access to future information, it is possible to have stronger\nresults with less involved approaches. By knowing the values of goods for $n-1$\ntime steps into the future, we design a matching-based algorithm that achieves\nan EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$\nallocation. Finally, we show that our results allow us to get the first\nnontrivial guarantees for additive instances in which the ratio of the maximum\nover the minimum value an agent has for a good is bounded.", "AI": {"tldr": "\u7814\u7a76\u5728\u7ebf\u516c\u5e73\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u9488\u5bf9\u4e2a\u6027\u53162\u503c\u5b9e\u4f8b\u7684\u7b97\u6cd5\uff0c\u5e76\u63a2\u8ba8\u672a\u6765\u4fe1\u606f\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u5546\u54c1\u9010\u4e00\u5230\u8fbe\u4e14\u4ee3\u7406\u4eba\u7684\u4ef7\u503c\u51fd\u6570\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5bfb\u6c42\u5728\u4e0d\u5047\u8bbe\u4ef7\u503c\u53d7\u4e25\u683c\u9650\u5236\u6216\u6765\u81ea\u5206\u5e03\u65f6\u4ecd\u80fd\u5b9e\u73b0\u5f3a\u4e0d\u53ef\u80fd\u7ed3\u679c\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7ef4\u6301\u6bcf\u4e00\u6b65\u9aa4\u76841/(2n-1)-MMS\u5206\u914d\uff0c\u5e76\u6700\u7ec8\u8fbe\u52301/4-MMS\u5206\u914d\uff1b\u901a\u8fc7\u6709\u9650\u7684\u672a\u6765\u4fe1\u606f\u8bbe\u8ba1\u57fa\u4e8e\u5339\u914d\u7684\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u7684\u7ed3\u679c\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u5bf9\u5df2\u77e5\u7684\u516c\u5e73\u6982\u5ff5\uff08\u5982\u6700\u5927\u6700\u5c0f\u4efd\u989d\u516c\u5e73\u6027\u548c\u51e0\u4e4e\u65e0\u5ac9\u5992\uff09\u63d0\u4f9b\u4e86\u4fdd\u8bc1\uff0c\u5e76\u9996\u6b21\u4e3a\u6709\u754c\u4ee3\u7406\u4ef7\u503c\u6bd4\u7684\u52a0\u6027\u5b9e\u4f8b\u63d0\u4f9b\u4e86\u975e\u5e73\u51e1\u7684\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u53d7\u9650\u4f30\u503c\u51fd\u6570\u5b9e\u4f8b\uff0c\u53ef\u4ee5\u514b\u670d\u5f3a\u4e0d\u53ef\u80fd\u7ed3\u679c\uff0c\u83b7\u5f97\u5173\u4e8e\u516c\u5e73\u6027\u7684\u66f4\u4f73\u4fdd\u8bc1\u3002"}}
{"id": "2505.21647", "pdf": "https://arxiv.org/pdf/2505.21647", "abs": "https://arxiv.org/abs/2505.21647", "authors": ["Eric Xing", "Abby Stylianou", "Robert Pless", "Nathan Jacobs"], "title": "QuARI: Query Adaptive Retrieval Improvement", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures, 4 tables", "summary": "Massive-scale pretraining has made vision-language models increasingly\npopular for image-to-image and text-to-image retrieval across a broad\ncollection of domains. However, these models do not perform well when used for\nchallenging retrieval tasks, such as instance retrieval in very large-scale\nimage collections. Recent work has shown that linear transformations of VLM\nfeatures trained for instance retrieval can improve performance by emphasizing\nsubspaces that relate to the domain of interest. In this paper, we explore a\nmore extreme version of this specialization by learning to map a given query to\na query-specific feature space transformation. Because this transformation is\nlinear, it can be applied with minimal computational cost to millions of image\nembeddings, making it effective for large-scale retrieval or re-ranking.\nResults show that this method consistently outperforms state-of-the-art\nalternatives, including those that require many orders of magnitude more\ncomputation at query time.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u67e5\u8be2\u6620\u5c04\u5230\u7279\u5b9a\u4e8e\u67e5\u8be2\u7684\u7279\u5f81\u7a7a\u95f4\u8f6c\u6362\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u5b9e\u4f8b\u68c0\u7d22\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u6216\u91cd\u65b0\u6392\u5e8f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u9886\u57df\u56fe\u50cf\u548c\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u68c0\u7d22\u4efb\u52a1\uff08\u5982\u8d85\u5927\u89c4\u6a21\u56fe\u50cf\u96c6\u5408\u4e2d\u7684\u5b9e\u4f8b\u68c0\u7d22\uff09\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u5df2\u6709\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5bf9VLM\u7279\u5f81\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\u4ee5\u5f3a\u8c03\u4e0e\u76ee\u6807\u9886\u57df\u76f8\u5173\u7684\u5b50\u7a7a\u95f4\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5b66\u4e60\u5c06\u7ed9\u5b9a\u67e5\u8be2\u6620\u5c04\u5230\u7279\u5b9a\u4e8e\u8be5\u67e5\u8be2\u7684\u7279\u5f81\u7a7a\u95f4\u8f6c\u6362\u3002\u7531\u4e8e\u8fd9\u79cd\u8f6c\u6362\u662f\u7ebf\u6027\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u4ee5\u6781\u5c0f\u7684\u8ba1\u7b97\u6210\u672c\u5c06\u5176\u5e94\u7528\u4e8e\u6570\u767e\u4e07\u4e2a\u56fe\u50cf\u5d4c\u5165\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u68c0\u7d22\u6216\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6280\u672f\uff0c\u5305\u62ec\u90a3\u4e9b\u5728\u67e5\u8be2\u65f6\u9700\u8981\u591a\u6570\u91cf\u7ea7\u66f4\u591a\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u4e8e\u67e5\u8be2\u7684\u7279\u5f81\u7a7a\u95f4\u8f6c\u6362\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u5b9e\u4f8b\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179", "abs": "https://arxiv.org/abs/2505.22179", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant.", "AI": {"tldr": "Speculative decoding \u548c quantization \u80fd\u6709\u6548\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u3002\u7814\u7a76\u53d1\u73b0\uff0c4-bit \u6743\u91cd\u91cf\u5316\u5e26\u6765\u7684\u5185\u5b58\u4f18\u52bf\u4f1a\u88ab speculative decoding \u7684\u8ba1\u7b97\u8d1f\u62c5\u6240\u62b5\u6d88\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u6a21\u578b\u4f5c\u4e3a\u4e2d\u95f4\u9636\u6bb5\uff0c\u5c06\u6811\u72b6\u8349\u7a3f\u8f6c\u6362\u4e3a\u5e8f\u5217\u8349\u7a3f\uff0c\u4ece\u800c\u5229\u7528\u76ee\u6807\u91cf\u5316\u6a21\u578b\u7684\u5185\u5b58\u8bbf\u95ee\u4f18\u52bf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728A100 GPU\u4e0a\u5bf94-bit\u6743\u91cd\u91cf\u5316\u7684Llama-3-70B\u6a21\u578b\u5b9e\u73b0\u4e862.78\u500d\u7684\u52a0\u901f\uff0c\u5e76\u4e14\u6bd4EAGLE-2\u5feb1.31\u500d\u3002", "motivation": "Speculative decoding \u548c quantization \u90fd\u53ef\u4ee5\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\uff0c\u4f46\u5c06\u4e24\u8005\u7ed3\u5408\u65f6\u53ef\u80fd\u4f1a\u51fa\u73b0\u5185\u5b58\u4f18\u52bf\u88ab\u8ba1\u7b97\u8d1f\u62c5\u62b5\u6d88\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u6a21\u578b\u4f5c\u4e3a\u4e2d\u95f4\u9636\u6bb5\uff0c\u5c06\u6811\u72b6\u8349\u7a3f\u8f6c\u6362\u4e3a\u5e8f\u5217\u8349\u7a3f\uff0c\u4ece\u800c\u5229\u7528\u76ee\u6807\u91cf\u5316\u6a21\u578b\u7684\u5185\u5b58\u8bbf\u95ee\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728A100 GPU\u4e0a\u5bf94-bit\u6743\u91cd\u91cf\u5316\u7684Llama-3-70B\u6a21\u578b\u5b9e\u73b0\u4e862.78\u500d\u7684\u52a0\u901f\uff0c\u5e76\u4e14\u6bd4EAGLE-2\u5feb1.31\u500d\u3002", "conclusion": "\u5206\u5c42\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u7ed3\u5408 speculative decoding \u548c quantization \u7684\u4f18\u70b9\uff0c\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184", "abs": "https://arxiv.org/abs/2505.22184", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5C$^2$TU\uff0c\u7528\u4e8e\u63ed\u793a\u4e2d\u6587\u4f2a\u88c5\u7684\u6bd2\u6027\u5185\u5bb9\uff0c\u901a\u8fc7\u5b50\u4e32\u5339\u914d\u548c\u57fa\u4e8eBERT\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5728\u4e24\u4e2a\u4e2d\u6587\u6bd2\u6027\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u7684\u6bd2\u6027\u5185\u5bb9\u589e\u52a0\uff0c\u4e00\u4e9b\u7528\u6237\u901a\u8fc7\u540c\u97f3\u5f02\u4e49\u8bcd\u7b49\u65b9\u5f0f\u89c4\u907f\u5ba1\u67e5\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u82f1\u6587\u6587\u672c\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u6587\u4f2a\u88c5\u6bd2\u6027\u5185\u5bb9\u7684\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "C$^2$TU\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u5b50\u4e32\u5339\u914d\u8bc6\u522b\u57fa\u4e8e\u4e2d\u6587\u540c\u5f62\u5b57\u548c\u6bd2\u6027\u8bcd\u6c47\u8868\u7684\u5019\u9009\u6bd2\u6027\u8bcd\uff0c\u7136\u540e\u901a\u8fc7\u4e24\u79cd\u6a21\u578b\u53d8\u4f53\uff08\u57fa\u4e8eBERT\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff09\u8fc7\u6ee4\u975e\u6bd2\u6027\u5019\u9009\u5e76\u7ea0\u6b63\u4f2a\u88c5\u4e3a\u5bf9\u5e94\u7684\u6bd2\u6027\u8bcd\u3002\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u9650\u5236\uff0c\u5229\u7528\u5b8c\u6574\u8bed\u4e49\u4e0a\u4e0b\u6587\u63ed\u793a\u4f2a\u88c5\u6bd2\u6027\u8bcd\u3002", "result": "\u5728\u4e24\u4e2a\u4e2d\u6587\u6bd2\u6027\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cC$^2$TU\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u6bd4\u6700\u4f73\u7ade\u4e89\u5bf9\u624b\u9ad8\u51fa71%\uff0c\u5728\u51c6\u786e\u6027\u4e0a\u9ad8\u51fa35%\u3002", "conclusion": "C$^2$TU\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u63d0\u793a\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63ed\u793a\u4e2d\u6587\u4f2a\u88c5\u6bd2\u6027\u5185\u5bb9\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.21658", "pdf": "https://arxiv.org/pdf/2505.21658", "abs": "https://arxiv.org/abs/2505.21658", "authors": ["Brandon R. Feng", "David Keetae Park", "Xihaier Luo", "Arantxa Urdangarin", "Shinjae Yoo", "Brian J. Reich"], "title": "STACI: Spatio-Temporal Aleatoric Conformal Inference", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty\nquantification for estimation of spatio-temporal fields. Spatio-temporal deep\nlearning models, while scalable, typically assume a simplistic independent\ncovariance matrix for the response, failing to capture the underlying\ncorrelation structure. However, spatio-temporal GPs suffer from issues of\nscalability and various forms of approximation bias resulting from restrictive\nassumptions of the covariance kernel function. We propose STACI, a novel\nframework consisting of a variational Bayesian neural network approximation of\nnon-stationary spatio-temporal GP along with a novel spatio-temporal conformal\ninference algorithm. STACI is highly scalable, taking advantage of GPU training\ncapabilities for neural network models, and provides statistically valid\nprediction intervals for uncertainty quantification. STACI outperforms\ncompeting GPs and deep methods in accurately approximating spatio-temporal\nprocesses and we show it easily scales to datasets with millions of\nobservations.", "AI": {"tldr": "Gaussian Processes (GPs) can quantify uncertainty in spatio-temporal fields, but they face challenges in scalability and approximation bias. Deep learning models are scalable but often fail to capture the underlying correlation structure. The proposed framework, STACI, combines a variational Bayesian neural network with a conformal inference algorithm to overcome these limitations, providing valid prediction intervals and scaling to large datasets.", "motivation": "To address the limitations of Gaussian Processes (GPs) and deep learning models in handling spatio-temporal data, such as scalability issues and inability to capture complex correlation structures.", "method": "STACI uses a variational Bayesian neural network to approximate non-stationary spatio-temporal GPs and incorporates a novel spatio-temporal conformal inference algorithm. This approach leverages GPU training for scalability and ensures statistically valid uncertainty quantification.", "result": "STACI outperforms existing GP and deep learning methods in accurately approximating spatio-temporal processes and scales effectively to datasets with millions of observations.", "conclusion": "STACI provides a scalable and effective solution for spatio-temporal data analysis, overcoming the limitations of traditional GPs and deep learning models."}}
{"id": "2505.22193", "pdf": "https://arxiv.org/pdf/2505.22193", "abs": "https://arxiv.org/abs/2505.22193", "authors": ["Marco Parigi", "Stefano Martina", "Francesco Aldo Venturelli", "Filippo Caruso"], "title": "Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.AI", "cs.CV", "cs.LG", "81P68, 81P40, 81P47, 68Q12, 68T07,", "I.2.6; I.3.3; J.2"], "comment": "17 pages, 9 figures. Supplementary materials: 2 pages, 2 figures", "summary": "Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI\nthat aims to use quantum properties to improve the performances of their\nclassical counterparts. However, existing algorithms are not easily scalable\ndue to the limitations of near-term quantum devices. Following our previous\nwork on QDMs, here we propose and implement two physics-inspired protocols. In\nthe first, we use the formalism of quantum stochastic walks, showing that a\nspecific interplay of quantum and classical dynamics in the forward process\nproduces statistically more robust models generating sets of MNIST images with\nlower Fr\\'echet Inception Distance (FID) than using totally classical dynamics.\nIn the second approach, we realize an algorithm to generate images by\nexploiting the intrinsic noise of real IBM quantum hardware with only four\nqubits. Our work could be a starting point to pave the way for new scenarios\nfor large-scale algorithms in quantum Generative AI, where quantum noise is\nneither mitigated nor corrected, but instead exploited as a useful resource.", "AI": {"tldr": "The paper explores Quantum Diffusion Models (QDMs) and proposes two physics-inspired protocols to enhance generative AI using quantum properties. The first protocol uses quantum stochastic walks to generate MNIST images with lower FID compared to classical methods, while the second leverages the noise of IBM quantum hardware for image generation. This work highlights potential pathways for scalable quantum Generative AI.", "motivation": "To improve the performance of classical generative AI models by incorporating quantum properties and address scalability issues due to limitations of near-term quantum devices.", "method": "1. Employing quantum stochastic walks to create a mix of quantum and classical dynamics in the forward process for generating MNIST images. 2. Using the intrinsic noise of real IBM quantum hardware with four qubits to develop an algorithm for image generation.", "result": "The first approach resulted in statistically more robust models that generated MNIST images with lower Fr\u00e9chet Inception Distance (FID) than classical dynamics alone. The second approach successfully utilized quantum hardware noise for image generation.", "conclusion": "This study demonstrates the potential of quantum properties in enhancing generative AI and suggests new directions for large-scale algorithms in quantum Generative AI where quantum noise is used as a beneficial resource rather than being mitigated or corrected."}}
{"id": "2505.21686", "pdf": "https://arxiv.org/pdf/2505.21686", "abs": "https://arxiv.org/abs/2505.21686", "authors": ["Michele Gallo"], "title": "tenSVD algorithm for compression", "categories": ["stat.CO", "cs.CV", "cs.LG"], "comment": null, "summary": "Tensors provide a robust framework for managing high-dimensional data.\nConsequently, tensor analysis has emerged as an active research area in various\ndomains, including machine learning, signal processing, computer vision, graph\nanalysis, and data mining. This study introduces an efficient image storage\napproach utilizing tensors, aiming to minimize memory to store, bandwidth to\ntransmit and energy to processing. The proposed method organizes original data\ninto a higher-order tensor and applies the Tucker model for compression.\nImplemented in R, this method is compared to a baseline algorithm. The\nevaluation focuses on efficient of algorithm measured in term of computational\ntime and the quality of information preserved, using both simulated and real\ndatasets. A detailed analysis of the results is conducted, employing\nestablished quantitative metrics, with significant attention paid to\nsustainability in terms of energy consumption across algorithms.", "AI": {"tldr": "The paper proposes an efficient image storage method using tensors to reduce memory, bandwidth and energy consumption. It uses the Tucker model for compression and compares it with a baseline algorithm in R.", "motivation": "To develop an efficient image storage approach that minimizes memory, bandwidth and energy consumption.", "method": "Organize original data into a higher-order tensor and apply the Tucker model for compression. Compare this method with a baseline algorithm implemented in R.", "result": "Evaluation focuses on computational time and quality of information preserved, using both simulated and real datasets. Detailed analysis conducted with established quantitative metrics, paying attention to sustainability in terms of energy consumption.", "conclusion": "The proposed tensor-based image storage method is efficient in reducing memory, bandwidth and energy consumption while maintaining good quality of information."}}
{"id": "2505.22200", "pdf": "https://arxiv.org/pdf/2505.22200", "abs": "https://arxiv.org/abs/2505.22200", "authors": ["Darshana Saravanan", "Makarand Tapaswi", "Vineet Gandhi"], "title": "Investigating Mechanisms for In-Context Vision Language Binding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to MIV at CVPRW 2025 (Oral)", "summary": "To understand a prompt, Vision-Language models (VLMs) must perceive the\nimage, comprehend the text, and build associations within and across both\nmodalities. For instance, given an 'image of a red toy car', the model should\nassociate this image to phrases like 'car', 'red toy', 'red object', etc. Feng\nand Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the\nentity and its corresponding attribute tokens share a Binding ID in the model\nactivations. We investigate this for image-text binding in VLMs using a\nsynthetic dataset and task that requires models to associate 3D objects in an\nimage with their descriptions in the text. Our experiments demonstrate that\nVLMs assign a distinct Binding ID to an object's image tokens and its textual\nreferences, enabling in-context association.", "AI": {"tldr": "This paper explores how Vision-Language models (VLMs) use Binding IDs to associate images with text using a synthetic dataset.", "motivation": "To understand how VLMs bind images and text together, specifically associating 3D objects in images with their textual descriptions.", "method": "Using a synthetic dataset, the researchers conduct experiments to observe if VLMs assign distinct Binding IDs to objects' image tokens and their textual references.", "result": "The experiments show that VLMs do assign distinct Binding IDs, allowing for in-context association between image and text.", "conclusion": "Binding IDs play a crucial role in enabling VLMs to associate images with text effectively."}}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202", "abs": "https://arxiv.org/abs/2505.22202", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "Let's Predict Sentence by Sentence", "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u5230\u62bd\u8c61\u63a8\u7406\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u9884\u6d4b\u4e0b\u4e00\u4e2a\u53e5\u5b50\u7684\u8fde\u7eed\u5d4c\u5165\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u4f7f\u7528\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u5d4c\u5165\u8303\u5f0f\uff0c\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u5d4c\u5165\u5728\u8fde\u7eed\u63a8\u7406\u6a21\u5f0f\u4e0b\u8868\u73b0\u51fa\u4e0e\u601d\u7ef4\u94fe(Chain-of-Thought)\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5e73\u5747\u4e00\u534a\u7684\u63a8\u7406\u65f6\u95f4\u6d6e\u70b9\u8fd0\u7b97(FLOPs)\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86SentenceLens\u5de5\u5177\u7528\u4e8e\u53ef\u89c6\u5316\u6f5c\u5728\u8f68\u8ff9\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4ee5\u9010\u4e2a\u6807\u8bb0\u751f\u6210\u6587\u672c\uff0c\u4f46\u4eba\u7c7b\u63a8\u7406\u662f\u5728\u66f4\u9ad8\u5c42\u6b21\u7684\u62bd\u8c61\uff08\u5982\u53e5\u5b50\u3001\u547d\u9898\u548c\u6982\u5ff5\uff09\u4e0a\u8fdb\u884c\u7684\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u4e86\u89e3\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u50cf\u4eba\u7c7b\u4e00\u6837\uff0c\u5728\u7ed3\u6784\u5316\u7684\u8bed\u4e49\u5355\u5143\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u57fa\u4e8e\u539f\u59cb\u6807\u8bb0\u5e8f\u5217\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6807\u8bb0\u7ea7\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u5230\u53e5\u5b50\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u9884\u6d4b\u4e0b\u4e00\u4e2a\u53e5\u5b50\u7684\u8fde\u7eed\u5d4c\u5165\u3002\u4ed6\u4eec\u63a2\u7d22\u4e86\u4e24\u79cd\u5d4c\u5165\u8303\u5f0f\uff1a1\uff09\u8bed\u4e49\u5d4c\u5165\uff0c\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5b66\u4e60\u4ee5\u4fdd\u7559\u8868\u9762\u610f\u4e49\uff1b2\uff09\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u901a\u8fc7\u4e0b\u4e00\u53e5\u9884\u6d4b\u8bad\u7ec3\u4ee5\u7f16\u7801\u9884\u671f\u7ed3\u6784\u3002\u8bc4\u4f30\u4e86\u4e24\u79cd\u63a8\u7406\u673a\u5236\uff1a\u79bb\u6563\u5316\u548c\u8fde\u7eed\u63a8\u7406\u3002", "result": "\u5728\u6570\u5b66\u3001\u903b\u8f91\u3001\u5e38\u8bc6\u548c\u89c4\u5212\u56db\u4e2a\u9886\u57df\u4e2d\uff0c\u4e0a\u4e0b\u6587\u5d4c\u5165\u5728\u8fde\u7eed\u63a8\u7406\u673a\u5236\u4e0b\u7684\u8868\u73b0\u4e0e\u601d\u7ef4\u94fe\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7ea6\u4e00\u534a\u7684\u63a8\u7406\u65f6\u95f4\u6d6e\u70b9\u8fd0\u7b97\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6a21\u5757\u5316\u9002\u5e94\u7684\u65e9\u671f\u8ff9\u8c61\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5728\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u5185\u64cd\u4f5c\uff0c\u6709\u6548\u5730\u8fc7\u6e21\u5230\u62bd\u8c61\u3001\u7ed3\u6784\u5316\u7684\u63a8\u7406\u3002"}}
{"id": "2505.21692", "pdf": "https://arxiv.org/pdf/2505.21692", "abs": "https://arxiv.org/abs/2505.21692", "authors": ["Omar Bennouna", "Amine Bennouna", "Saurabh Amin", "Asuman Ozdaglar"], "title": "What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "We study the fundamental question of how informative a dataset is for solving\na given decision-making task. In our setting, the dataset provides partial\ninformation about unknown parameters that influence task outcomes. Focusing on\nlinear programs, we characterize when a dataset is sufficient to recover an\noptimal decision, given an uncertainty set on the cost vector. Our main\ncontribution is a sharp geometric characterization that identifies the\ndirections of the cost vector that matter for optimality, relative to the task\nconstraints and uncertainty set. We further develop a practical algorithm that,\nfor a given task, constructs a minimal or least-costly sufficient dataset. Our\nresults reveal that small, well-chosen datasets can often fully determine\noptimal decisions -- offering a principled foundation for task-aware data\nselection.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6570\u636e\u96c6\u5728\u89e3\u51b3\u7279\u5b9a\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u4fe1\u606f\u91cf\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7ebf\u6027\u89c4\u5212\u4e2d\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u6765\u5224\u65ad\u6570\u636e\u96c6\u662f\u5426\u8db3\u4ee5\u5f97\u51fa\u6700\u4f18\u51b3\u7b56\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u7279\u5f81\u65b9\u6cd5\u548c\u5b9e\u9645\u7b97\u6cd5\u4ee5\u6784\u5efa\u6700\u5c0f\u6210\u672c\u7684\u5145\u5206\u6570\u636e\u96c6\u3002", "motivation": "\u8bc4\u4f30\u6570\u636e\u96c6\u5bf9\u4e8e\u7ed9\u5b9a\u51b3\u7b56\u4efb\u52a1\u7684\u4fe1\u606f\u91cf\u53ca\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u672a\u77e5\u53c2\u6570\u5f71\u54cd\u4efb\u52a1\u7ed3\u679c\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4e13\u6ce8\u4e8e\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u5206\u6790\u6210\u672c\u5411\u91cf\u65b9\u5411\u4e0e\u4efb\u52a1\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\u6784\u5efa\u6700\u5c0f\u6216\u6700\u4f4e\u6210\u672c\u7684\u5145\u5206\u6570\u636e\u96c6\u3002", "result": "\u63ed\u793a\u4e86\u7cbe\u5fc3\u6311\u9009\u7684\u5c0f\u578b\u6570\u636e\u96c6\u5f80\u5f80\u80fd\u591f\u5b8c\u5168\u51b3\u5b9a\u6700\u4f18\u51b3\u7b56\uff0c\u4e3a\u4efb\u52a1\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c16\u9510\u7684\u51e0\u4f55\u7279\u5f81\u7528\u4e8e\u5224\u65ad\u6570\u636e\u96c6\u662f\u5426\u5145\u5206\u652f\u6301\u6700\u4f18\u51b3\u7b56\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u6784\u5efa\u6700\u5c0f\u6210\u672c\u7684\u5145\u5206\u6570\u636e\u96c6\u3002"}}
{"id": "2505.21706", "pdf": "https://arxiv.org/pdf/2505.21706", "abs": "https://arxiv.org/abs/2505.21706", "authors": ["Gonzalo Travieso", "Joao Merenda", "Odemir M. Bruno"], "title": "Network classification through random walks", "categories": ["cs.SI", "cs.LG", "physics.soc-ph"], "comment": "11 pages, 2 figures", "summary": "Network models have been widely used to study diverse systems and analyze\ntheir dynamic behaviors. Given the structural variability of networks, an\nintriguing question arises: Can we infer the type of system represented by a\nnetwork based on its structure? This classification problem involves extracting\nrelevant features from the network. Existing literature has proposed various\nmethods that combine structural measurements and dynamical processes for\nfeature extraction. In this study, we introduce a novel approach to\ncharacterize networks using statistics from random walks, which can be\nparticularly informative about network properties. We present the employed\nstatistical metrics and compare their performance on multiple datasets with\nother state-of-the-art feature extraction methods. Our results demonstrate that\nthe proposed method is effective in many cases, often outperforming existing\napproaches, although some limitations are observed across certain datasets.", "AI": {"tldr": "This paper proposes a new method for network classification using random walk statistics, which performs well in many cases but has some limitations.", "motivation": "To address the problem of inferring the type of system represented by a network based on its structure.", "method": "The study introduces an approach to characterize networks using statistics from random walks and compares it with other state-of-the-art feature extraction methods.", "result": "The proposed method is effective in many cases and often outperforms existing approaches, although some limitations are observed across certain datasets.", "conclusion": "The novel approach using random walk statistics can be a valuable tool for network classification."}}
{"id": "2505.21721", "pdf": "https://arxiv.org/pdf/2505.21721", "abs": "https://arxiv.org/abs/2505.21721", "authors": ["Kyurae Kim", "Yi-An Ma", "Trevor Campbell", "Jacob R. Gardner"], "title": "Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference", "categories": ["stat.ML", "cs.LG", "math.OC", "stat.CO"], "comment": null, "summary": "We prove that, given a mean-field location-scale variational family,\nblack-box variational inference (BBVI) with the reparametrization gradient\nconverges at an almost dimension-independent rate. Specifically, for strongly\nlog-concave and log-smooth targets, the number of iterations for BBVI with a\nsub-Gaussian family to achieve an objective $\\epsilon$-close to the global\noptimum is $\\mathrm{O}(\\log d)$, which improves over the $\\mathrm{O}(d)$\ndependence of full-rank location-scale families. For heavy-tailed families, we\nprovide a weaker $\\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the\nnumber of finite moments. Additionally, if the Hessian of the target\nlog-density is constant, the complexity is free of any explicit dimension\ndependence. We also prove that our bound on the gradient variance, which is key\nto our result, cannot be improved using only spectral bounds on the Hessian of\nthe target log-density.", "AI": {"tldr": "The paper proves that black-box variational inference (BBVI) with a mean-field location-scale variational family converges at an almost dimension-independent rate under certain conditions.", "motivation": "To understand the convergence rate of black-box variational inference (BBVI) with the reparametrization gradient given a mean-field location-scale variational family and how it compares to full-rank location-scale families.", "method": "Prove the convergence rate of BBVI for strongly log-concave and log-smooth targets, analyze the number of iterations needed to achieve an objective close to the global optimum for sub-Gaussian and heavy-tailed families, and examine the effect of the Hessian of the target log-density on the complexity.", "result": "For strongly log-concave and log-smooth targets, BBVI with a sub-Gaussian family achieves an objective \u03b5-close to the global optimum in O(log d) iterations. For heavy-tailed families, the dependence is weaker at O(d^(2/k)). If the Hessian of the target log-density is constant, there is no explicit dimension dependence. The bound on the gradient variance cannot be improved using only spectral bounds on the Hessian of the target log-density.", "conclusion": "Black-box variational inference with a mean-field location-scale variational family can converge at an almost dimension-independent rate, improving over the O(d) dependence of full-rank location-scale families."}}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232", "abs": "https://arxiv.org/abs/2505.22232", "authors": ["Mehdi Ali", "Manuel Brack", "Max L\u00fcbbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kacz\u00e9r", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim K\u00f6hler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.", "AI": {"tldr": "JQL is a new method for creating high-quality multilingual datasets by using lightweight annotators derived from pretrained multilingual embeddings, which improves downstream model training and data retention.", "motivation": "Current open-source multilingual datasets are limited in quality and scalability due to reliance on heuristic filtering methods.", "method": "JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings, enabling efficient curation of diverse and high-quality multilingual data.", "result": "Empirical evaluation across 35 languages shows JQL outperforms existing methods like Fineweb2, enhancing downstream model training quality and increasing data retention rates.", "conclusion": "JQL provides practical insights and valuable resources for multilingual data curation, advancing the standards of multilingual dataset development."}}
{"id": "2505.21723", "pdf": "https://arxiv.org/pdf/2505.21723", "abs": "https://arxiv.org/abs/2505.21723", "authors": ["Skyler Wu", "Shihao Yang", "S. C. Kou"], "title": "Are Statistical Methods Obsolete in the Era of Deep Learning?", "categories": ["stat.CO", "cs.LG", "stat.ML"], "comment": "35 pages, 11 figures (main text)", "summary": "In the era of AI, neural networks have become increasingly popular for\nmodeling, inference, and prediction, largely due to their potential for\nuniversal approximation. With the proliferation of such deep learning models, a\nquestion arises: are leaner statistical methods still relevant? To shed insight\non this question, we employ the mechanistic nonlinear ordinary differential\nequation (ODE) inverse problem as a testbed, using physics-informed neural\nnetwork (PINN) as a representative of the deep learning paradigm and\nmanifold-constrained Gaussian process inference (MAGI) as a representative of\nstatistically principled methods. Through case studies involving the SEIR model\nfrom epidemiology and the Lorenz model from chaotic dynamics, we demonstrate\nthat statistical methods are far from obsolete, especially when working with\nsparse and noisy observations. On tasks such as parameter inference and\ntrajectory reconstruction, statistically principled methods consistently\nachieve lower bias and variance, while using far fewer parameters and requiring\nless hyperparameter tuning. Statistical methods can also decisively outperform\ndeep learning models on out-of-sample future prediction, where the absence of\nrelevant data often leads overparameterized models astray. Additionally, we\nfind that statistically principled approaches are more robust to accumulation\nof numerical imprecision and can represent the underlying system more faithful\nto the true governing ODEs.", "AI": {"tldr": "\u5728AI\u65f6\u4ee3\uff0c\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982PINN\u5f88\u6d41\u884c\uff0c\u4f46\u7edf\u8ba1\u65b9\u6cd5\u5982MAGI\u5728\u5904\u7406\u7a00\u758f\u548c\u566a\u58f0\u6570\u636e\u65f6\u8868\u73b0\u66f4\u4f73\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u63a8\u7406\u3001\u8f68\u8ff9\u91cd\u5efa\u548c\u6837\u672c\u5916\u9884\u6d4b\u65b9\u9762\u3002\u8fd9\u4e9b\u65b9\u6cd5\u504f\u5dee\u548c\u65b9\u5dee\u66f4\u4f4e\u3001\u53c2\u6570\u66f4\u5c11\u3001\u5bf9\u6570\u503c\u4e0d\u7cbe\u786e\u7684\u79ef\u7d2f\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u66f4\u5fe0\u5b9e\u4e8e\u771f\u5b9e\u7684ODEs\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u5728\u5efa\u6a21\u3001\u63a8\u7406\u548c\u9884\u6d4b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4eba\u4eec\u5f00\u59cb\u8d28\u7591\u8f83\u7b80\u5355\u7684\u7edf\u8ba1\u65b9\u6cd5\u662f\u5426\u4ecd\u7136\u5177\u6709\u76f8\u5173\u6027\u3002", "method": "\u4f7f\u7528\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u7684\u4ee3\u8868\u548c\u6d41\u5f62\u7ea6\u675f\u9ad8\u65af\u8fc7\u7a0b\u63a8\u65ad\uff08MAGI\uff09\u4f5c\u4e3a\u7edf\u8ba1\u539f\u5219\u65b9\u6cd5\u7684\u4ee3\u8868\uff0c\u901a\u8fc7SEIR\u6a21\u578b\u548cLorenz\u6a21\u578b\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u7edf\u8ba1\u65b9\u6cd5\u5728\u53c2\u6570\u63a8\u7406\u548c\u8f68\u8ff9\u91cd\u5efa\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u4f4e\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\u548c\u8d85\u53c2\u6570\u8c03\u6574\u3002\u5728\u6837\u672c\u5916\u9884\u6d4b\u65b9\u9762\u4e5f\u660e\u663e\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u6b64\u5916\uff0c\u7edf\u8ba1\u65b9\u6cd5\u5bf9\u6570\u503c\u4e0d\u7cbe\u786e\u7684\u79ef\u7d2f\u66f4\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u66f4\u5fe0\u5b9e\u5730\u8868\u793a\u57fa\u7840\u7cfb\u7edf\u3002", "conclusion": "\u7edf\u8ba1\u5b66\u65b9\u6cd5\u8fdc\u672a\u8fc7\u65f6\uff0c\u5728\u5904\u7406\u7a00\u758f\u548c\u5608\u6742\u7684\u6570\u636e\u65f6\u5c24\u5176\u6709\u6548\uff0c\u5176\u7ed3\u679c\u66f4\u51c6\u786e\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u4e14\u5bf9\u672a\u6765\u9884\u6d4b\u66f4\u4e3a\u53ef\u9760\u3002"}}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264", "abs": "https://arxiv.org/abs/2505.22264", "authors": ["Maximiliano Hormaz\u00e1bal Lagos", "\u00c1lvaro Bueno Saez", "H\u00e9ctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1.", "AI": {"tldr": "This paper presents an approach to solve the SemEval 2025 Task 8 challenge using Python code generation with LLMs, achieving a score of 70.50% for subtask 1.", "motivation": "To develop an effective method for question-answering over tabular data by leveraging Python code generation with LLMs.", "method": "The method involves multiple steps including understanding table content, generating natural language instructions, translating these instructions into code, running the code, and handling potential errors or exceptions using open source LLMs and optimized prompts.", "result": "The approach achieved a score of 70.50% for subtask 1 of the SemEval 2025 Task 8 challenge.", "conclusion": "The proposed strategy using Python code generation with LLMs shows promise in addressing the question-answering challenge over tabular data."}}
{"id": "2505.21734", "pdf": "https://arxiv.org/pdf/2505.21734", "abs": "https://arxiv.org/abs/2505.21734", "authors": ["Felix Jahncke", "Johannes Betz"], "title": "MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages. Submitted to the IEEE Intelligent Vehicles Symposium (IV\n  2025), Romania", "summary": "Developing robust, efficient navigation algorithms is challenging. Rule-based\nmethods offer interpretability and modularity but struggle with learning from\nlarge datasets, while end-to-end neural networks excel in learning but lack\ntransparency and modularity. In this paper, we present MIND-Stack, a modular\nsoftware stack consisting of a localization network and a Stanley Controller\nwith intermediate human interpretable state representations and end-to-end\ndifferentiability. Our approach enables the upstream localization module to\nreduce the downstream control error, extending its role beyond state\nestimation. Unlike existing research on differentiable algorithms that either\nlack modules of the autonomous stack to span from sensor input to actuator\noutput or real-world implementation, MIND-Stack offers both capabilities. We\nconduct experiments that demonstrate the ability of the localization module to\nreduce the downstream control loss through its end-to-end differentiability\nwhile offering better performance than state-of-the-art algorithms. We showcase\nsim-to-real capabilities by deploying the algorithm on a real-world embedded\nautonomous platform with limited computation power and demonstrate simultaneous\ntraining of both the localization and controller towards one goal. While\nMIND-Stack shows good results, we discuss the incorporation of additional\nmodules from the autonomous navigation pipeline in the future, promising even\ngreater stability and performance in the next iterations of the framework.", "AI": {"tldr": "The paper introduces MIND-Stack, a modular software stack combining a localization network and a Stanley Controller for navigation. It features human interpretable states and end-to-end differentiability, improving control performance and offering sim-to-real capabilities.", "motivation": "To create a robust and efficient navigation system that combines the interpretability of rule-based methods with the learning capabilities of neural networks, addressing the limitations of both.", "method": "MIND-Stack is a modular software stack comprising a localization network and a Stanley Controller. It uses intermediate human interpretable state representations and is end-to-end differentiable, allowing the localization module to reduce downstream control error beyond state estimation.", "result": "Experiments show that the localization module reduces downstream control loss and performs better than state-of-the-art algorithms. The system demonstrates sim-to-real capabilities on a real-world platform with limited computation power, enabling simultaneous training of localization and controller modules.", "conclusion": "MIND-Stack successfully integrates interpretability and learning capabilities, offering improved performance and stability. Future work will incorporate additional modules from the autonomous navigation pipeline for even greater enhancements."}}
{"id": "2505.21736", "pdf": "https://arxiv.org/pdf/2505.21736", "abs": "https://arxiv.org/abs/2505.21736", "authors": ["Zachary Schlamowitz", "Andrew Bennecke", "Daniel J. Tward"], "title": "Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The principle of translation equivariance (if an input image is translated an\noutput image should be translated by the same amount), led to the development\nof convolutional neural networks that revolutionized machine vision. Other\nsymmetries, like rotations and reflections, play a similarly critical role,\nespecially in biomedical image analysis, but exploiting these symmetries has\nnot seen wide adoption. We hypothesize that this is partially due to the\nmathematical complexity of methods used to exploit these symmetries, which\noften rely on representation theory, a bespoke concept in differential geometry\nand group theory. In this work, we show that the same equivariance can be\nachieved using a simple form of convolution kernels that we call ``moment\nkernels,'' and prove that all equivariant kernels must take this form. These\nare a set of radially symmetric functions of a spatial position $x$, multiplied\nby powers of the components of $x$ or the identity matrix. We implement\nequivariant neural networks using standard convolution modules, and provide\narchitectures to execute several biomedical image analysis tasks that depend on\nequivariance principles: classification (outputs are invariant under orthogonal\ntransforms), 3D image registration (outputs transform like a vector), and cell\nsegmentation (quadratic forms defining ellipses transform like a matrix).", "AI": {"tldr": "The paper presents a new approach to achieve equivariance in neural networks using 'moment kernels', which simplifies the exploitation of symmetries such as rotations and reflections for biomedical image analysis.", "motivation": "Existing methods that exploit symmetries like rotations and reflections rely on complex mathematical concepts, limiting their adoption. The authors aim to simplify these methods to make them more accessible and applicable.", "method": "The authors introduce 'moment kernels' as a simple form of convolution kernels that can achieve equivariance. These kernels consist of radially symmetric functions multiplied by powers of spatial position components or the identity matrix. They implement equivariant neural networks with standard convolution modules for various tasks.", "result": "The authors successfully implemented architectures for classification, 3D image registration, and cell segmentation tasks, demonstrating the effectiveness of moment kernels in achieving equivariance.", "conclusion": "Moment kernels provide a simpler way to achieve equivariance in neural networks, potentially leading to wider adoption of symmetry-exploiting techniques in biomedical image analysis."}}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280", "abs": "https://arxiv.org/abs/2505.22280", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow.", "AI": {"tldr": "This paper conducts an in-depth review of 129 research studies on using NLP for Evidence-based Medicine (EBM). It highlights the role of NLP in enhancing clinical decision-making and proposes future research directions.", "motivation": "There is a critical need to explore Natural Language Processing (NLP) methods due to the large volume of medical literature and high curation costs, which are challenges in applying Evidence-based Medicine (EBM).", "method": "The authors conducted an in-depth review of 129 research studies that leverage NLP for EBM. They examined how NLP supports the five fundamental steps of EBM - Ask, Acquire, Appraise, Apply, and Assess.", "result": "The review identified current limitations within the field of NLP for EBM and proposed potential directions for future research.", "conclusion": "NLP has the potential to revolutionize EBM by improving evidence extraction, synthesis, appraisal, summarization, data comprehensibility, and clinical workflow efficiency."}}
{"id": "2505.21742", "pdf": "https://arxiv.org/pdf/2505.21742", "abs": "https://arxiv.org/abs/2505.21742", "authors": ["Briglia Maria Rosaria", "Mujtaba Hussain Mirza", "Giuseppe Lisanti", "Iacopo Masi"], "title": "What is Adversarial Training for Diffusion Models?", "categories": ["cs.CV", "cs.LG"], "comment": "40 pages", "summary": "We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks.", "AI": {"tldr": "This paper explores adversarial training (AT) for diffusion models (DMs), showing it fundamentally differs from classifiers. AT in DMs requires equivariance to align the diffusion process with the data distribution, enforcing smoothness in the diffusion flow and improving robustness.", "motivation": "The motivation of this paper is to investigate how adversarial training can be applied to diffusion models, highlighting its fundamental differences from classifier-based adversarial training.", "method": "The method involves integrating adversarial training into diffusion models by adding random or adversarial noise during training. This approach enforces equivariance and smoothness in the diffusion flow without making assumptions about the noise model.", "result": "The results demonstrate strong performance under severe noise, data corruption, and iterative adversarial attacks when evaluated on proof-of-concept datasets with known distributions as well as standard benchmarks such as CIFAR-10, CelebA, and LSUN Bedroom.", "conclusion": "Adversarial training for diffusion models improves robustness to outliers and corrupted data by enforcing smoothness in the diffusion flow through equivariance."}}
{"id": "2505.22287", "pdf": "https://arxiv.org/pdf/2505.22287", "abs": "https://arxiv.org/abs/2505.22287", "authors": ["Daniel McDuff", "Tim Korjakow", "Kevin Klyman", "Danish Contractor"], "title": "New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses", "categories": ["cs.CY", "cs.AI"], "comment": "Preprint", "summary": "Foundation models have had a transformative impact on AI. A combination of\nlarge investments in research and development, growing sources of digital data\nfor training, and architectures that scale with data and compute has led to\nmodels with powerful capabilities. Releasing assets is fundamental to\nscientific advancement and commercial enterprise. However, concerns over\nnegligent or malicious uses of AI have led to the design of mechanisms to limit\nthe risks of the technology. The result has been a proliferation of licenses\nwith behavioral-use clauses and acceptable-use-policies that are increasingly\nbeing adopted by commonly used families of models (Llama, Gemma, Deepseek) and\na myriad of smaller projects. We created and deployed a custom AI licenses\ngenerator to facilitate license creation and have quantitatively and\nqualitatively analyzed over 300 customized licenses created with this tool.\nAlongside this we analyzed 1.7 million models licenses on the HuggingFace model\nhub. Our results show increasing adoption of these licenses, interest in tools\nthat support their creation and a convergence on common clause configurations.\nIn this paper we take the position that tools for tracking adoption of, and\nadherence to, these licenses is the natural next step and urgently needed in\norder to ensure they have the desired impact of ensuring responsible use.", "AI": {"tldr": "Foundation models have transformed AI, but concerns over misuse have led to complex licensing. This paper analyzes 300 custom licenses and 1.7 million model licenses on HuggingFace, finding increasing adoption and convergence on common clauses. Tools for tracking license adherence are urgently needed.", "motivation": "The motivation of this paper is to address the growing need for effective tools to track the adoption and adherence to AI model licenses, ensuring responsible use amidst concerns over negligent or malicious uses of AI.", "method": "The authors created and deployed a custom AI licenses generator, analyzed over 300 customized licenses created with this tool, and also analyzed 1.7 million model licenses on the HuggingFace model hub.", "result": "The results show an increasing adoption of licenses with behavioral-use clauses and acceptable-use-policies, interest in tools that support their creation, and a convergence on common clause configurations.", "conclusion": "The paper concludes that tools for tracking the adoption and adherence to these licenses are the natural next step and are urgently needed to ensure they have the desired impact of promoting responsible AI use."}}
{"id": "2505.22291", "pdf": "https://arxiv.org/pdf/2505.22291", "abs": "https://arxiv.org/abs/2505.22291", "authors": ["Saptarshi Neil Sinha", "P. Julius Kuehn", "Johannes Koppe", "Arjan Kuijper", "Michael Weinmann"], "title": "Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The preservation of early visual arts, particularly color photographs, is\nchallenged by deterioration caused by aging and improper storage, leading to\nissues like blurring, scratches, color bleeding, and fading defects. In this\npaper, we present the first approach for the automatic removal of greening\ncolor defects in digitized autochrome photographs. Our main contributions\ninclude a method based on synthetic dataset generation and the use of\ngenerative AI with a carefully designed loss function for the restoration of\nvisual arts. To address the lack of suitable training datasets for analyzing\ngreening defects in damaged autochromes, we introduce a novel approach for\naccurately simulating such defects in synthetic data. We also propose a\nmodified weighted loss function for the ChaIR method to account for color\nimbalances between defected and non-defected areas. While existing methods\nstruggle with accurately reproducing original colors and may require\nsignificant manual effort, our method allows for efficient restoration with\nreduced time requirements.", "AI": {"tldr": "The paper introduces an automatic method to remove greening color defects in digitized autochrome photographs using synthetic dataset generation and generative AI.", "motivation": "To preserve early visual arts, particularly color photographs which are challenged by deterioration due to aging and improper storage leading to issues such as blurring, scratches, color bleeding, and fading defects.", "method": "The method is based on synthetic dataset generation and the use of generative AI with a carefully designed loss function for the restoration of visual arts. They introduce a novel approach for accurately simulating greening defects in synthetic data and propose a modified weighted loss function for the ChaIR method to account for color imbalances between defected and non-defected areas.", "result": "Existing methods struggle with accurately reproducing original colors and may require significant manual effort, whereas this new method allows for efficient restoration with reduced time requirements.", "conclusion": "This is the first approach for the automatic removal of greening color defects in digitized autochrome photographs."}}
{"id": "2505.22303", "pdf": "https://arxiv.org/pdf/2505.22303", "abs": "https://arxiv.org/abs/2505.22303", "authors": ["Grzegorz Wolny", "Micha\u0142 Szczerbak"], "title": "Voice CMS: updating the knowledge base of a digital assistant through conversation", "categories": ["cs.HC", "cs.AI", "cs.MA"], "comment": null, "summary": "In this study, we propose a solution based on a multi-agent LLM architecture\nand a voice user interface (VUI) designed to update the knowledge base of a\ndigital assistant. Its usability is evaluated in comparison to a more\ntraditional graphical content management system (CMS), with a focus on\nunderstanding the relationship between user preferences and the complexity of\nthe information being provided. The findings demonstrate that, while the\noverall usability of the VUI is rated lower than the graphical interface, it is\nalready preferred by users for less complex tasks. Furthermore, the quality of\ncontent entered through the VUI is comparable to that achieved with the\ngraphical interface, even for highly complex tasks. Obtained qualitative\nresults suggest that a hybrid interface combining the strengths of both\napproaches could address the key challenges identified during the experiment,\nsuch as reducing cognitive load through graphical feedback while maintaining\nthe intuitive nature of voice-based interactions. This work highlights the\npotential of conversational interfaces as a viable and effective method for\nknowledge management in specific business contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4ee3\u7406LLM\u67b6\u6784\u548c\u8bed\u97f3\u7528\u6237\u754c\u9762\uff08VUI\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u66f4\u65b0\u6570\u5b57\u52a9\u624b\u7684\u77e5\u8bc6\u5e93\u3002\u4e0e\u4f20\u7edf\u56fe\u5f62\u5185\u5bb9\u7ba1\u7406\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8bc4\u4f30\u4e86\u5176\u53ef\u7528\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u7528\u6237\u504f\u597d\u4e0e\u6240\u63d0\u4f9b\u4fe1\u606f\u590d\u6742\u6027\u7684\u5173\u7cfb\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1VUI\u7684\u6574\u4f53\u53ef\u7528\u6027\u8bc4\u5206\u4f4e\u4e8e\u56fe\u5f62\u754c\u9762\uff0c\u4f46\u5728\u5904\u7406\u8f83\u7b80\u5355\u7684\u4efb\u52a1\u65f6\u66f4\u53d7\u7528\u6237\u9752\u7750\u3002\u901a\u8fc7VUI\u8f93\u5165\u7684\u5185\u5bb9\u8d28\u91cf\u4e0e\u56fe\u5f62\u754c\u9762\u76f8\u5f53\uff0c\u5373\u4f7f\u5728\u5904\u7406\u9ad8\u5ea6\u590d\u6742\u4efb\u52a1\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u5b9a\u6027\u7ed3\u679c\u5efa\u8bae\uff0c\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u754c\u9762\u53ef\u4ee5\u89e3\u51b3\u5b9e\u9a8c\u4e2d\u53d1\u73b0\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u901a\u8fc7\u56fe\u5f62\u53cd\u9988\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u8377\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u97f3\u4ea4\u4e92\u7684\u76f4\u89c2\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u7279\u5b9a\u5546\u4e1a\u73af\u5883\u4e2d\u5bf9\u8bdd\u754c\u9762\u4f5c\u4e3a\u77e5\u8bc6\u7ba1\u7406\u6709\u6548\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u8bed\u97f3\u7528\u6237\u754c\u9762\uff08VUI\uff09\u5728\u77e5\u8bc6\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u7406\u89e3\u7528\u6237\u504f\u597d\u4e0e\u4fe1\u606f\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406LLM\u67b6\u6784\u548c\u8bed\u97f3\u7528\u6237\u754c\u9762\uff08VUI\uff09\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u56fe\u5f62\u5185\u5bb9\u7ba1\u7406\u7cfb\u7edf\uff08CMS\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e24\u8005\u7684\u53ef\u7528\u6027\u53ca\u7528\u6237\u504f\u597d\u3002", "result": "VUI\u7684\u6574\u4f53\u53ef\u7528\u6027\u8bc4\u5206\u4f4e\u4e8e\u56fe\u5f62\u754c\u9762\uff0c\u4f46\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u66f4\u53d7\u6b22\u8fce\uff1b\u8f93\u5165\u5185\u5bb9\u7684\u8d28\u91cf\u5728\u4e24\u79cd\u754c\u9762\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u5373\u4f7f\u662f\u590d\u6742\u4efb\u52a1\u4e5f\u662f\u5982\u6b64\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u6df7\u5408\u754c\u9762\u53ef\u80fd\u89e3\u51b3\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u5bf9\u8bdd\u754c\u9762\u5728\u7279\u5b9a\u5546\u4e1a\u73af\u5883\u4e2d\u5177\u6709\u4f5c\u4e3a\u6709\u6548\u77e5\u8bc6\u7ba1\u7406\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u7ed3\u5408\u56fe\u5f62\u548c\u8bed\u97f3\u4ea4\u4e92\u7684\u6df7\u5408\u754c\u9762\u53ef\u80fd\u662f\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2505.21767", "pdf": "https://arxiv.org/pdf/2505.21767", "abs": "https://arxiv.org/abs/2505.21767", "authors": ["Xiaoyan Li", "Shixin Xu", "Faisal Habib", "Arvind Gupta", "Huaxiong Huang"], "title": "Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction", "categories": ["eess.IV", "cs.LG", "eess.SP"], "comment": null, "summary": "Reconstructing ECG from PPG is a promising yet challenging task. While recent\nadvancements in generative models have significantly improved ECG\nreconstruction, accurately capturing fine-grained waveform features remains a\nkey challenge. To address this, we propose a novel PPG-to-ECG reconstruction\nmethod that leverages a Vision Transformer (ViT) as the core network. Unlike\nconventional approaches that rely on single-channel PPG, our method employs a\nfour-channel signal image representation, incorporating the original PPG, its\nfirst-order difference, second-order difference, and area under the curve. This\nmulti-channel design enriches feature extraction by preserving both temporal\nand physiological variations within the PPG. By leveraging the self-attention\nmechanism in ViT, our approach effectively captures both inter-beat and\nintra-beat dependencies, leading to more robust and accurate ECG\nreconstruction. Experimental results demonstrate that our method consistently\noutperforms existing 1D convolution-based approaches, achieving up to 29%\nreduction in PRD and 15% reduction in RMSE. The proposed approach also produces\nimprovements in other evaluation metrics, highlighting its robustness and\neffectiveness in reconstructing ECG signals. Furthermore, to ensure a\nclinically relevant evaluation, we introduce new performance metrics, including\nQRS area error, PR interval error, RT interval error, and RT amplitude\ndifference error. Our findings suggest that integrating a four-channel signal\nimage representation with the self-attention mechanism of ViT enables more\neffective extraction of informative PPG features and improved modeling of\nbeat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the\npotential of PPG as a viable alternative for heart activity monitoring, our\napproach opens new avenues for cyclic signal analysis and prediction.", "AI": {"tldr": "The paper proposes a novel PPG-to-ECG reconstruction method using Vision Transformer and four-channel signal image representation, which significantly improves ECG reconstruction accuracy and introduces new clinical evaluation metrics.", "motivation": "Reconstructing ECG from PPG has potential but faces challenges in capturing fine-grained waveform features.", "method": "Employ a Vision Transformer with a four-channel signal image representation including original PPG, its first-order difference, second-order difference, and area under the curve to enrich feature extraction and capture both inter-beat and intra-beat dependencies.", "result": "Outperforms existing 1D convolution-based approaches with up to 29% reduction in PRD and 15% reduction in RMSE, and shows improvements in other evaluation metrics.", "conclusion": "Integrating a four-channel signal image representation with ViT's self-attention mechanism enhances PPG feature extraction and modeling of beat-to-beat variations for PPG-to-ECG mapping."}}
{"id": "2505.21791", "pdf": "https://arxiv.org/pdf/2505.21791", "abs": "https://arxiv.org/abs/2505.21791", "authors": ["Julia Nakhleh", "Robert D. Nowak"], "title": "Global Minimizers of $\\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Overparameterized neural networks can interpolate a given dataset in many\ndifferent ways, prompting the fundamental question: which among these solutions\nshould we prefer, and what explicit regularization strategies will provably\nyield these solutions? This paper addresses the challenge of finding the\nsparsest interpolating ReLU network -- i.e., the network with the fewest\nnonzero parameters or neurons -- a goal with wide-ranging implications for\nefficiency, generalization, interpretability, theory, and model compression.\nUnlike post hoc pruning approaches, we propose a continuous, almost-everywhere\ndifferentiable training objective whose global minima are guaranteed to\ncorrespond to the sparsest single-hidden-layer ReLU networks that fit the data.\nThis result marks a conceptual advance: it recasts the combinatorial problem of\nsparse interpolation as a smooth optimization task, potentially enabling the\nuse of gradient-based training methods. Our objective is based on minimizing\n$\\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical\nsparsity-promoting strategy in finite-dimensional settings. However, applying\nthese ideas to neural networks presents new challenges: the function class is\ninfinite-dimensional, and the weights are learned using a highly nonconvex\nobjective. We prove that, under our formulation, global minimizers correspond\nexactly to sparsest solutions. Our work lays a foundation for understanding\nwhen and how continuous sparsity-inducing objectives can be leveraged to\nrecover sparse networks through training.", "AI": {"tldr": "Overparameterized neural networks can interpolate data in multiple ways. This paper aims to find the sparsest ReLU network by proposing a continuous training objective that guarantees global minima as the sparsest solutions.", "motivation": "The motivation of this paper is to address the challenge of finding the sparsest interpolating ReLU network, which has implications for efficiency, generalization, interpretability, theory, and model compression.", "method": "The method proposed is a continuous, almost-everywhere differentiable training objective whose global minima correspond to the sparsest single-hidden-layer ReLU networks fitting the data. This involves minimizing \u2113^p quasinorms of the weights for 0 < p < 1.", "result": "The result is a conceptual advance where the combinatorial problem of sparse interpolation is recast as a smooth optimization task, allowing the use of gradient-based training methods.", "conclusion": "This work lays a foundation for understanding when and how continuous sparsity-inducing objectives can be used to recover sparse networks through training."}}
{"id": "2505.21796", "pdf": "https://arxiv.org/pdf/2505.21796", "abs": "https://arxiv.org/abs/2505.21796", "authors": ["Sajad Khodadadian", "Martin Zubeldia"], "title": "A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "37 pages", "summary": "Polyak-Ruppert averaging is a widely used technique to achieve the optimal\nasymptotic variance of stochastic approximation (SA) algorithms, yet its\nhigh-probability performance guarantees remain underexplored in general\nsettings. In this paper, we present a general framework for establishing\nnon-asymptotic concentration bounds for the error of averaged SA iterates. Our\napproach assumes access to individual concentration bounds for the unaveraged\niterates and yields a sharp bound on the averaged iterates. We also construct\nan example, showing the tightness of our result up to constant multiplicative\nfactors. As direct applications, we derive tight concentration bounds for\ncontractive SA algorithms and for algorithms such as temporal difference\nlearning and Q-learning with averaging, obtaining new bounds in settings where\ntraditional analysis is challenging.", "AI": {"tldr": "The paper develops a framework for non-asymptotic concentration bounds of averaged SA iterates, providing tight results and new bounds for contractive SA and reinforcement learning algorithms.", "motivation": "Polyak-Ruppert averaging is widely used in stochastic approximation (SA) algorithms to achieve optimal asymptotic variance. However, its high-probability performance guarantees are underexplored in general settings, motivating the need for a framework to analyze non-asymptotic concentration bounds.", "method": "The authors present a general framework that assumes access to individual concentration bounds for unaveraged iterates to derive sharp bounds on the averaged iterates. They construct an example to demonstrate the tightness of their result up to constant multiplicative factors.", "result": "Tight concentration bounds are derived for contractive SA algorithms and for algorithms like temporal difference learning and Q-learning with averaging, offering new insights in challenging analysis settings.", "conclusion": "This study provides a valuable tool for analyzing the high-probability performance of Polyak-Ruppert averaging in various SA algorithms, including reinforcement learning contexts."}}
{"id": "2505.21799", "pdf": "https://arxiv.org/pdf/2505.21799", "abs": "https://arxiv.org/abs/2505.21799", "authors": ["Tim Tsz-Kit Lau", "Qi Long", "Weijie Su"], "title": "PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "The ever-growing scale of deep learning models and datasets underscores the\ncritical importance of efficient optimization methods. While preconditioned\ngradient methods such as Adam and AdamW are the de facto optimizers for\ntraining neural networks and large language models, structure-aware\npreconditioned optimizers like Shampoo and Muon, which utilize the matrix\nstructure of gradients, have demonstrated promising evidence of faster\nconvergence. In this paper, we introduce a unifying framework for analyzing\n\"matrix-aware\" preconditioned methods, which not only sheds light on the\neffectiveness of Muon and related optimizers but also leads to a class of new\nstructure-aware preconditioned methods. A key contribution of this framework is\nits precise distinction between preconditioning strategies that treat neural\nnetwork weights as vectors (addressing curvature anisotropy) versus those that\nconsider their matrix structure (addressing gradient anisotropy). This\nperspective provides new insights into several empirical phenomena in language\nmodel pre-training, including Adam's training instabilities, Muon's accelerated\nconvergence, and the necessity of learning rate warmup for Adam. Building upon\nthis framework, we introduce PolarGrad, a new class of preconditioned\noptimization methods based on the polar decomposition of matrix-valued\ngradients. As a special instance, PolarGrad includes Muon with updates scaled\nby the nuclear norm of the gradients. We provide numerical implementations of\nthese methods, leveraging efficient numerical polar decomposition algorithms\nfor enhanced convergence. Our extensive evaluations across diverse matrix\noptimization problems and language model pre-training tasks demonstrate that\nPolarGrad outperforms both Adam and Muon.", "AI": {"tldr": "The paper presents PolarGrad, a new optimization method based on the polar decomposition of matrix-valued gradients, which outperforms Adam and Muon in language model pre-training tasks.", "motivation": "To address the need for efficient optimization methods in deep learning models and explore structure-aware preconditioned optimizers that demonstrate faster convergence.", "method": "A unifying framework is introduced to analyze 'matrix-aware' preconditioned methods, distinguishing between vector-based and matrix-structured preconditioning strategies. Based on this framework, PolarGrad is developed, leveraging polar decomposition algorithms for enhanced convergence.", "result": "PolarGrad outperforms both Adam and Muon across diverse matrix optimization problems and language model pre-training tasks.", "conclusion": "PolarGrad represents an advancement in preconditioned optimization methods, offering improved performance through its consideration of gradient matrix structures."}}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.", "AI": {"tldr": "Recent advancements in large language models (LLMs) have shown impressive chain-of-thought reasoning capabilities, with reinforcement learning playing a crucial role. This study demonstrates that 'aha moment' patterns exist in multimodal LLMs prior to RL training but may not correlate with improved reasoning performance. A two-stage approach of supervised fine-tuning followed by reinforcement learning via GRPO consistently outperforms SFT-only and RL-only methods across multimodal reasoning benchmarks.", "motivation": "To explore the existence of 'aha moment' patterns in multimodal LLMs before reinforcement learning training and their correlation with reasoning performance, and to enhance multimodal reasoning through an effective two-stage approach.", "method": "The method involves a two-stage approach: 1) supervised fine-tuning as a cold start with structured chain-of-thought reasoning patterns; 2) reinforcement learning via GRPO to refine these capabilities.", "result": "Extensive experiments show that the combined approach outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales.", "conclusion": "This work provides practical guidance for building advanced multimodal reasoning models, demonstrating significant improvements over base models and achieving competitive performance."}}
{"id": "2505.21809", "pdf": "https://arxiv.org/pdf/2505.21809", "abs": "https://arxiv.org/abs/2505.21809", "authors": ["Jaya Narain", "Vasudha Kowtha", "Colin Lea", "Lauren Tooley", "Dianna Yee", "Vikramjit Mitra", "Zifang Huang", "Miquel Espi Marques", "Jon Huang", "Carlos Avendano", "Shirley Ren"], "title": "Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "accepted for Interspeech 2025", "summary": "Perceptual voice quality dimensions describe key characteristics of atypical\nspeech and other speech modulations. Here we develop and evaluate voice quality\nmodels for seven voice and speech dimensions (intelligibility, imprecise\nconsonants, harsh voice, naturalness, monoloudness, monopitch, and\nbreathiness). Probes were trained on the public Speech Accessibility (SAP)\nproject dataset with 11,184 samples from 434 speakers, using embeddings from\nfrozen pre-trained models as features. We found that our probes had both strong\nperformance and strong generalization across speech elicitation categories in\nthe SAP dataset. We further validated zero-shot performance on additional\ndatasets, encompassing unseen languages and tasks: Italian atypical speech,\nEnglish atypical speech, and affective speech. The strong zero-shot performance\nand the interpretability of results across an array of evaluations suggests the\nutility of using voice quality dimensions in speaking style-related tasks.", "AI": {"tldr": "The paper develops and evaluates voice quality models for seven voice and speech dimensions using embeddings from pre-trained models as features. The probes show strong performance, generalization, and zero-shot capabilities across multiple datasets.", "motivation": "To describe key characteristics of atypical speech and other speech modulations through perceptual voice quality dimensions.", "method": "Trained probes on the Speech Accessibility (SAP) project dataset with 11,184 samples from 434 speakers using embeddings from frozen pre-trained models as features.", "result": "Probes had strong performance and generalization across speech elicitation categories in the SAP dataset and showed strong zero-shot performance on additional datasets including unseen languages and tasks.", "conclusion": "The utility of using voice quality dimensions in speaking style-related tasks is suggested by the strong zero-shot performance and interpretability of results."}}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338", "abs": "https://arxiv.org/abs/2505.22338", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad", "AI": {"tldr": "Text2Grad is a new reinforcement-learning paradigm that converts textual feedback into span-level gradients to optimize language models.", "motivation": "Traditional RLHF uses coarse, scalar rewards which can lead to slow and opaque learning. Recent work has improved interpretability by adding textual critiques but does not adjust model parameters directly.", "method": "Text2Grad turns free-form textual feedback into span-level gradients. It aligns each feedback phrase with relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates to directly refine the model's policy. This is achieved through three components: a feedback-annotation pipeline, a fine-grained reward model, and a span-level policy optimizer.", "result": "Text2Grad consistently outperforms scalar-reward RL and prompt-only baselines in tasks such as summarization, code generation, and question answering, providing higher task metrics and richer interpretability.", "conclusion": "Natural-language feedback, when converted to gradients, serves as a powerful signal for fine-grained policy optimization."}}
{"id": "2505.21819", "pdf": "https://arxiv.org/pdf/2505.21819", "abs": "https://arxiv.org/abs/2505.21819", "authors": ["Charlotte Peale", "Vinod Raman", "Omer Reingold"], "title": "Representative Language Generation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We introduce \"representative generation,\" extending the theoretical framework\nfor generation proposed by Kleinberg et al. (2024) and formalized by Li et al.\n(2024), to additionally address diversity and bias concerns in generative\nmodels. Our notion requires outputs of a generative model to proportionally\nrepresent groups of interest from the training data. We characterize\nrepresentative uniform and non-uniform generation, introducing the \"group\nclosure dimension\" as a key combinatorial quantity. For representative\ngeneration in the limit, we analyze both information-theoretic and\ncomputational aspects, demonstrating feasibility for countably infinite\nhypothesis classes and collections of groups under certain conditions, but\nproving a negative result for computability using only membership queries. This\ncontrasts with Kleinberg et al.'s (2024) positive results for standard\ngeneration in the limit. Our findings provide a rigorous foundation for\ndeveloping more diverse and representative generative models.", "AI": {"tldr": "The paper introduces 'representative generation' to address diversity and bias in generative models by ensuring outputs proportionally represent groups of interest, providing a rigorous foundation for more diverse models.", "motivation": "To extend the theoretical framework for generation to address diversity and bias concerns in generative models.", "method": "Introduce the concept of 'representative generation', characterize representative uniform and non-uniform generation using 'group closure dimension', and analyze information-theoretic and computational aspects.", "result": "Demonstrate feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but prove a negative result for computability using only membership queries.", "conclusion": "Provides a rigorous foundation for developing more diverse and representative generative models."}}
{"id": "2505.22343", "pdf": "https://arxiv.org/pdf/2505.22343", "abs": "https://arxiv.org/abs/2505.22343", "authors": ["Zhonghao Lyu", "Yulan Gao", "Junting Chen", "Hongyang Du", "Jie Xu", "Kaibin Huang", "Dong In Kim"], "title": "Empowering Intelligent Low-altitude Economy with Large AI Model Deployment", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Low-altitude economy (LAE) represents an emerging economic paradigm that\nredefines commercial and social aerial activities. Large artificial\nintelligence models (LAIMs) offer transformative potential to further enhance\nthe intelligence of LAE services. However, deploying LAIMs in LAE poses several\nchallenges, including the significant gap between their computational/storage\ndemands and the limited onboard resources of LAE entities, the mismatch between\nlab-trained LAIMs and dynamic physical environments, and the inefficiencies of\ntraditional decoupled designs for sensing, communication, and computation. To\naddress these issues, we first propose a hierarchical system architecture\ntailored for LAIM deployment and present representative LAE application\nscenarios. Next, we explore key enabling techniques that facilitate the mutual\nco-evolution of LAIMs and low-altitude systems, and introduce a task-oriented\nexecution pipeline for scalable and adaptive service delivery. Then, the\nproposed framework is validated through real-world case studies. Finally, we\noutline open challenges to inspire future research.", "AI": {"tldr": "Low-altitude economy (LAE) is an emerging paradigm that can be enhanced by large artificial intelligence models (LAIMs). However, deploying LAIMs in LAE has challenges such as resource limitations and inefficiencies of traditional designs. This paper proposes a hierarchical system architecture for LAIM deployment, explores enabling techniques for co-evolution of LAIMs and low-altitude systems, presents a task-oriented execution pipeline, validates the framework through case studies, and outlines open challenges.", "motivation": "To address the challenges of deploying large artificial intelligence models in the low-altitude economy, including computational/storage demands versus limited onboard resources, mismatch between lab-trained models and dynamic environments, and inefficiencies of traditional decoupled designs.", "method": "Propose a hierarchical system architecture tailored for LAIM deployment, present representative LAE application scenarios, explore key enabling techniques for mutual co-evolution, introduce a task-oriented execution pipeline for scalable and adaptive service delivery, validate the proposed framework via real-world case studies.", "result": "The proposed framework was validated through real-world case studies, demonstrating its effectiveness in facilitating the integration of LAIMs into low-altitude systems.", "conclusion": "The paper concludes by outlining open challenges to inspire future research on integrating LAIMs into the low-altitude economy."}}
{"id": "2505.22349", "pdf": "https://arxiv.org/pdf/2505.22349", "abs": "https://arxiv.org/abs/2505.22349", "authors": ["Anjie Xu", "Ruiqing Ding", "Leye Wang"], "title": "ChatPD: An LLM-driven Paper-Dataset Networking System", "categories": ["cs.DB", "cs.AI", "cs.IR"], "comment": "Accepted by KDD Applied Data Science Track 2025", "summary": "Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}.", "AI": {"tldr": "The paper introduces ChatPD, a system using Large Language Models (LLMs) to automate dataset information extraction from academic papers and create structured paper-dataset networks. It outperforms PapersWithCode in dataset usage extraction with about 90% precision and recall in entity resolution tasks.", "motivation": "Scientific research requires suitable datasets for method validation, but current platforms like PapersWithCode have inefficiencies due to manual workflows.", "method": "ChatPD system consists of three key modules: paper collection, dataset information extraction, and dataset entity resolution. A Graph Completion and Inference strategy is proposed to map dataset descriptions to their corresponding entities.", "result": "Through extensive experiments, ChatPD outperforms PapersWithCode in dataset usage extraction and achieves about 90% precision and recall in entity resolution tasks.", "conclusion": "ChatPD is deployed to continuously extract dataset usage in papers and provide dataset discovery services. The system and the current paper-dataset network are open sourced on GitHub."}}
{"id": "2505.21837", "pdf": "https://arxiv.org/pdf/2505.21837", "abs": "https://arxiv.org/abs/2505.21837", "authors": ["Aliasghar Khani", "Arianna Rampini", "Evan Atherton", "Bruno Roy"], "title": "UniMoGen: Universal Motion Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Motion generation is a cornerstone of computer graphics, animation, gaming,\nand robotics, enabling the creation of realistic and varied character\nmovements. A significant limitation of existing methods is their reliance on\nspecific skeletal structures, which restricts their versatility across\ndifferent characters. To overcome this, we introduce UniMoGen, a novel\nUNet-based diffusion model designed for skeleton-agnostic motion generation.\nUniMoGen can be trained on motion data from diverse characters, such as humans\nand animals, without the need for a predefined maximum number of joints. By\ndynamically processing only the necessary joints for each character, our model\nachieves both skeleton agnosticism and computational efficiency. Key features\nof UniMoGen include controllability via style and trajectory inputs, and the\nability to continue motions from past frames. We demonstrate UniMoGen's\neffectiveness on the 100style dataset, where it outperforms state-of-the-art\nmethods in diverse character motion generation. Furthermore, when trained on\nboth the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen\nachieves high performance and improved efficiency across both skeletons. These\nresults highlight UniMoGen's potential to advance motion generation by\nproviding a flexible, efficient, and controllable solution for a wide range of\ncharacter animations.", "AI": {"tldr": "UniMoGen is a UNet-based diffusion model for skeleton-agnostic motion generation, capable of handling diverse characters and outperforming state-of-the-art methods.", "motivation": "Existing motion generation methods are limited by reliance on specific skeletal structures, restricting versatility across different characters.", "method": "Introduced UniMoGen, a UNet-based diffusion model that can be trained on motion data from diverse characters without predefined joint limits, achieving skeleton agnosticism and computational efficiency. It includes controllability via style and trajectory inputs and the ability to continue motions from past frames.", "result": "UniMoGen outperforms state-of-the-art methods in diverse character motion generation on the 100style dataset and achieves high performance and improved efficiency when trained on both 100style and LAFAN1 datasets.", "conclusion": "UniMoGen provides a flexible, efficient, and controllable solution for a wide range of character animations, advancing motion generation."}}
{"id": "2505.22353", "pdf": "https://arxiv.org/pdf/2505.22353", "abs": "https://arxiv.org/abs/2505.22353", "authors": ["Noora Al-Emadi", "Ingmar Weber", "Yin Yang", "Ferda Ofli"], "title": "VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting vehicles in satellite images is crucial for traffic management,\nurban planning, and disaster response. However, current models struggle with\nreal-world diversity, particularly across different regions. This challenge is\namplified by geographic bias in existing datasets, which often focus on\nspecific areas and overlook regions like the Middle East. To address this gap,\nwe present the Vehicles in the Middle East (VME) dataset, designed explicitly\nfor vehicle detection in high-resolution satellite images from Middle Eastern\ncountries. Sourced from Maxar, the VME dataset spans 54 cities across 12\ncountries, comprising over 4,000 image tiles and more than 100,000 vehicles,\nannotated using both manual and semi-automated methods. Additionally, we\nintroduce the largest benchmark dataset for Car Detection in Satellite Imagery\n(CDSI), combining images from multiple sources to enhance global car detection.\nOur experiments demonstrate that models trained on existing datasets perform\npoorly on Middle Eastern images, while the VME dataset significantly improves\ndetection accuracy in this region. Moreover, state-of-the-art models trained on\nCDSI achieve substantial improvements in global car detection.", "AI": {"tldr": "Detecting vehicles in satellite images is important for various applications but current models lack diversity. The paper introduces VME dataset for Middle Eastern countries and CDSI benchmark which improve vehicle detection accuracy in these regions and globally.", "motivation": "Current vehicle detection models struggle with real-world diversity, especially across different geographic regions due to bias in existing datasets.", "method": "Introduced VME dataset with high-resolution images from Middle Eastern countries and CDSI benchmark combining images from multiple sources. Both datasets are used to train and evaluate detection models.", "result": "Models trained on existing datasets perform poorly on Middle Eastern images while VME dataset improves detection accuracy in this region. CDSI-trained models achieve better global car detection.", "conclusion": "VME and CDSI datasets fill critical gaps in vehicle detection, improving accuracy in Middle Eastern regions and globally."}}
{"id": "2505.21842", "pdf": "https://arxiv.org/pdf/2505.21842", "abs": "https://arxiv.org/abs/2505.21842", "authors": ["Filippos Fotiadis", "Kyriakos G. Vamvoudakis"], "title": "A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "Accepted with minor revisions at International Journal of Robust and\n  Nonlinear Control", "summary": "We propose a physics-informed neural networks (PINNs) framework to solve the\ninfinite-horizon optimal control problem of nonlinear systems. In particular,\nsince PINNs are generally able to solve a class of partial differential\nequations (PDEs), they can be employed to learn the value function of the\ninfinite-horizon optimal control problem via solving the associated\nsteady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is\nthat the steady-state HJB equation generally yields multiple solutions; hence\nif PINNs are directly employed to it, they may end up approximating a solution\nthat is different from the optimal value function of the problem. We tackle\nthis by instead applying PINNs to a finite-horizon variant of the steady-state\nHJB that has a unique solution, and which uniformly approximates the optimal\nvalue function as the horizon increases. An algorithm to verify if the chosen\nhorizon is large enough is also given, as well as a method to extend it -- with\nreduced computations and robustness to approximation errors -- in case it is\nnot. Unlike many existing methods, the proposed technique works well with\nnon-polynomial basis functions, does not require prior knowledge of a\nstabilizing controller, and does not perform iterative policy evaluations.\nSimulations are performed, which verify and clarify theoretical findings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002\u901a\u8fc7\u6c42\u89e3\u4e0e\u4e4b\u76f8\u5173\u7684\u7a33\u6001Hamilton-Jacobi-Bellman\uff08HJB\uff09\u65b9\u7a0b\uff0c\u5229\u7528PINNs\u5b66\u4e60\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u4ef7\u503c\u51fd\u6570\u3002\u4e3a\u907f\u514d\u7a33\u6001HJB\u65b9\u7a0b\u591a\u89e3\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5c06\u5176\u5e94\u7528\u4e8e\u5177\u6709\u552f\u4e00\u89e3\u7684\u6709\u9650\u65f6\u57df\u53d8\u4f53\uff0c\u5e76\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u6240\u9009\u65f6\u57df\u662f\u5426\u8db3\u591f\u5927\u7684\u7b97\u6cd5\u4ee5\u53ca\u5728\u4e0d\u8db3\u591f\u5927\u65f6\u8fdb\u884c\u6269\u5c55\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8fed\u4ee3\u7b56\u7565\u8bc4\u4f30\u3001\u5148\u9a8c\u7a33\u5b9a\u63a7\u5236\u5668\u77e5\u8bc6\uff0c\u5e76\u9002\u7528\u4e8e\u975e\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u5e76\u9610\u660e\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u8bb8\u591a\u65b9\u6cd5\u9700\u8981\u8fed\u4ee3\u7b56\u7565\u8bc4\u4f30\u3001\u5148\u9a8c\u7a33\u5b9a\u63a7\u5236\u5668\u77e5\u8bc6\u6216\u4ec5\u9002\u7528\u4e8e\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePINNs\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u66f4\u9ad8\u6548\u548c\u901a\u7528\u7684\u65b9\u5f0f\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528PINNs\u6846\u67b6\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u7a33\u6001HJB\u65b9\u7a0b\u30022. \u9488\u5bf9\u7a33\u6001HJB\u65b9\u7a0b\u53ef\u80fd\u6709\u591a\u89e3\u7684\u95ee\u9898\uff0c\u91c7\u7528\u5176\u6709\u9650\u65f6\u57df\u53d8\u4f53\uff0c\u786e\u4fdd\u552f\u4e00\u89e3\u30023. \u63d0\u4f9b\u9a8c\u8bc1\u6240\u9009\u65f6\u57df\u662f\u5426\u8db3\u591f\u5927\u7684\u7b97\u6cd5\uff0c\u4ee5\u53ca\u5728\u4e0d\u8db3\u591f\u5927\u65f6\u8fdb\u884c\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\u30024. \u8be5\u65b9\u6cd5\u65e0\u9700\u8fed\u4ee3\u7b56\u7565\u8bc4\u4f30\u3001\u5148\u9a8c\u7a33\u5b9a\u63a7\u5236\u5668\u77e5\u8bc6\uff0c\u5e76\u9002\u7528\u4e8e\u975e\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u3002", "result": "1. \u6210\u529f\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\u30022. \u65b9\u6cd5\u5728\u4e0d\u9700\u8981\u8fed\u4ee3\u7b56\u7565\u8bc4\u4f30\u3001\u5148\u9a8c\u7a33\u5b9a\u63a7\u5236\u5668\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\u30023. \u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u5e76\u9610\u660e\u4e86\u7406\u8bba\u53d1\u73b0\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8ePINNs\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u65e0\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5904\u7406\u975e\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u3001\u65e0\u9700\u5148\u9a8c\u7a33\u5b9a\u63a7\u5236\u5668\u77e5\u8bc6\u548c\u4e0d\u8fdb\u884c\u8fed\u4ee3\u7b56\u7565\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u5de5\u5177\u3002"}}
{"id": "2505.21845", "pdf": "https://arxiv.org/pdf/2505.21845", "abs": "https://arxiv.org/abs/2505.21845", "authors": ["Lingfei Zhao", "Hadeel Soliman", "Kevin S. Xu", "Subhadeep Paul"], "title": "Spectral clustering for dependent community Hawkes process models of temporal networks", "categories": ["stat.ML", "cs.LG", "cs.SI", "stat.ME"], "comment": null, "summary": "Temporal networks observed continuously over time through timestamped\nrelational events data are commonly encountered in application settings\nincluding online social media communications, financial transactions, and\ninternational relations. Temporal networks often exhibit community structure\nand strong dependence patterns among node pairs. This dependence can be modeled\nthrough mutual excitations, where an interaction event from a sender to a\nreceiver node increases the possibility of future events among other node\npairs.\n  We provide statistical results for a class of models that we call dependent\ncommunity Hawkes (DCH) models, which combine the stochastic block model with\nmutually exciting Hawkes processes for modeling both community structure and\ndependence among node pairs, respectively. We derive a non-asymptotic upper\nbound on the misclustering error of spectral clustering on the event count\nmatrix as a function of the number of nodes and communities, time duration, and\nthe amount of dependence in the model. Our result leverages recent results on\nbounding an appropriate distance between a multivariate Hawkes process count\nvector and a Gaussian vector, along with results from random matrix theory. We\nalso propose a DCH model that incorporates only self and reciprocal excitation\nalong with highly scalable parameter estimation using a Generalized Method of\nMoments (GMM) estimator that we demonstrate to be consistent for growing\nnetwork size and time duration.", "AI": {"tldr": "Temporal networks with timestamped relational events data are common in various applications. These networks often exhibit community structure and strong dependence patterns among node pairs. This paper focuses on dependent community Hawkes (DCH) models which combine stochastic block model with mutually exciting Hawkes processes to model both community structure and dependence among node pairs. The authors derive a non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix, leveraging recent results on bounding an appropriate distance between a multivariate Hawkes process count vector and a Gaussian vector. They also propose a DCH model incorporating only self and reciprocal excitation with highly scalable parameter estimation using a Generalized Method of Moments (GMM) estimator.", "motivation": "To provide statistical results for modeling temporal networks that exhibit community structure and strong dependence patterns among node pairs through mutual excitations.", "method": "Dependent community Hawkes (DCH) models combining stochastic block model with mutually exciting Hawkes processes are used. A non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix is derived. Additionally, a DCH model incorporating only self and reciprocal excitation along with scalable parameter estimation using GMM estimator is proposed.", "result": "A non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix is successfully derived. The proposed DCH model with scalable parameter estimation using GMM estimator is consistent for growing network size and time duration.", "conclusion": "The study provides statistical results for DCH models and proposes a scalable DCH model with consistent parameter estimation for modeling temporal networks with community structure and dependence among node pairs."}}
{"id": "2505.21872", "pdf": "https://arxiv.org/pdf/2505.21872", "abs": "https://arxiv.org/abs/2505.21872", "authors": ["George R. Nahass", "Zhu Wang", "Homa Rashidisabet", "Won Hwa Kim", "Sasha Hubschman", "Jeffrey C. Peterson", "Ghasem Yazdanpanah", "Chad A. Purnell", "Pete Setabutr", "Ann Q. Tran", "Darvin Yi", "Sathya N. Ravi"], "title": "Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images", "categories": ["eess.IV", "cs.LG"], "comment": "39 pages, 12 figures, 11 tables, 3 algorithms", "summary": "Machine unlearning aims to remove the influence of specific training samples\nfrom a trained model without full retraining. While prior work has largely\nfocused on privacy-motivated settings, we recast unlearning as a\ngeneral-purpose tool for post-deployment model revision. Specifically, we focus\non utilizing unlearning in clinical contexts where data shifts, device\ndeprecation, and policy changes are common. To this end, we propose a bilevel\noptimization formulation of boundary-based unlearning that can be solved using\niterative algorithms. We provide convergence guarantees when first-order\nalgorithms are used to unlearn. Our method introduces tunable loss design for\ncontrolling the forgetting-retention tradeoff and supports novel model\ncomposition strategies that merge the strengths of distinct unlearning runs.\nAcross benchmark and real-world clinical imaging datasets, our approach\noutperforms baselines on both forgetting and retention metrics, including\nscenarios involving imaging devices and anatomical outliers. This work\nestablishes machine unlearning as a modular, practical alternative to\nretraining for real-world model maintenance in clinical applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u673a\u5668\u9057\u5fd8\u6280\u672f\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u6a21\u578b\u4fee\u8ba2\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u754c\u7684\u53cc\u5c42\u4f18\u5316\u9057\u5fd8\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u4fdd\u8bc1\u548c\u53ef\u8c03\u635f\u5931\u8bbe\u8ba1\u6765\u5e73\u8861\u9057\u5fd8\u4e0e\u4fdd\u7559\u4e4b\u95f4\u7684\u6743\u8861\u3002\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u548c\u5b9e\u9645\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u7ef4\u62a4\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9690\u79c1\u9a71\u52a8\u7684\u573a\u666f\uff0c\u800c\u672c\u6587\u5c06\u673a\u5668\u9057\u5fd8\u89c6\u4e3a\u4e00\u79cd\u901a\u7528\u5de5\u5177\uff0c\u7528\u4e8e\u90e8\u7f72\u540e\u6a21\u578b\u4fee\u8ba2\uff0c\u7279\u522b\u662f\u5e94\u5bf9\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6570\u636e\u53d8\u5316\u3001\u8bbe\u5907\u6dd8\u6c70\u548c\u653f\u7b56\u53d8\u66f4\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u754c\u7684\u53cc\u5c42\u4f18\u5316\u9057\u5fd8\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u8fed\u4ee3\u7b97\u6cd5\u89e3\u51b3\uff0c\u5e76\u5f15\u5165\u4e86\u53ef\u8c03\u635f\u5931\u8bbe\u8ba1\u4ee5\u63a7\u5236\u9057\u5fd8\u4e0e\u4fdd\u7559\u7684\u6743\u8861\uff0c\u8fd8\u652f\u6301\u65b0\u7684\u6a21\u578b\u7ec4\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u4e0d\u540c\u9057\u5fd8\u8fd0\u884c\u7684\u4f18\u52bf\u3002", "result": "\u5728\u57fa\u51c6\u548c\u5b9e\u9645\u4e34\u5e8a\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9057\u5fd8\u548c\u4fdd\u7559\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5305\u62ec\u6d89\u53ca\u6210\u50cf\u8bbe\u5907\u548c\u89e3\u5256\u5f02\u5e38\u503c\u7684\u573a\u666f\u3002", "conclusion": "\u673a\u5668\u9057\u5fd8\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6a21\u5757\u5316\u3001\u5b9e\u7528\u7684\u91cd\u8bad\u7ec3\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e34\u5e8a\u6a21\u578b\u7ef4\u62a4\u4efb\u52a1\u3002"}}
{"id": "2505.22384", "pdf": "https://arxiv.org/pdf/2505.22384", "abs": "https://arxiv.org/abs/2505.22384", "authors": ["Foivos Fioravantes", "Harmender Gahlawat", "Nikolaos Melissinos"], "title": "Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size", "categories": ["cs.DS", "cs.AI"], "comment": "a preliminary version appeared in AAAI 2025", "summary": "Imagine we want to split a group of agents into teams in the most\n\\emph{efficient} way, considering that each agent has their own preferences\nabout their teammates. This scenario is modeled by the extensively studied\n\\textsc{Coalition Formation} problem. Here, we study a version of this problem\nwhere each team must additionally be of bounded size.\n  We conduct a systematic algorithmic study, providing several intractability\nresults as well as multiple exact algorithms that scale well as the input grows\n(FPT), which could prove useful in practice.\n  Our main contribution is an algorithm that deals efficiently with tree-like\nstructures (bounded \\emph{treewidth}) for ``small'' teams. We complement this\nresult by proving that our algorithm is asymptotically optimal. Particularly,\nthere can be no algorithm that vastly outperforms the one we present, under\nreasonable theoretical assumptions, even when considering star-like structures\n(bounded \\emph{vertex cover number}).", "AI": {"tldr": "The paper explores a version of the Coalition Formation problem where teams have bounded sizes, providing intractability results and efficient algorithms that could be practical for large inputs. The main contribution is an algorithm optimized for tree-like structures with small teams, proven to be asymptotically optimal.", "motivation": "To split a group of agents into teams efficiently considering each agent's preferences about their teammates, while ensuring each team has a bounded size.", "method": "Systematic algorithmic study offering multiple exact algorithms that scale well with input growth (FPT). Developed an algorithm optimized for tree-like structures (bounded treewidth) with small teams.", "result": "An algorithm dealing efficiently with tree-like structures for small teams was developed and proven to be asymptotically optimal. No other algorithm can significantly outperform it under reasonable theoretical assumptions, even for star-like structures (bounded vertex cover number).", "conclusion": "A systematic study on a bounded size team Coalition Formation problem was conducted, resulting in intractability insights and practical algorithms, with a key algorithm proven to be asymptotically optimal."}}
{"id": "2505.22387", "pdf": "https://arxiv.org/pdf/2505.22387", "abs": "https://arxiv.org/abs/2505.22387", "authors": ["Jaehyun Choi", "Gyojin Han", "Dong-Jae Lee", "Sunghyun Baek", "Junmo Kim"], "title": "DAM: Domain-Aware Module for Multi-Domain Dataset Condensation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dataset Condensation (DC) has emerged as a promising solution to mitigate the\ncomputational and storage burdens associated with training deep learning\nmodels. However, existing DC methods largely overlook the multi-domain nature\nof modern datasets, which are increasingly composed of heterogeneous images\nspanning multiple domains. In this paper, we extend DC and introduce\nMulti-Domain Dataset Condensation (MDDC), which aims to condense data that\ngeneralizes across both single-domain and multi-domain settings. To this end,\nwe propose the Domain-Aware Module (DAM), a training-time module that embeds\ndomain-related features into each synthetic image via learnable spatial masks.\nAs explicit domain labels are mostly unavailable in real-world datasets, we\nemploy frequency-based pseudo-domain labeling, which leverages low-frequency\namplitude statistics. DAM is only active during the condensation process, thus\npreserving the same images per class (IPC) with prior methods. Experiments show\nthat DAM consistently improves in-domain, out-of-domain, and cross-architecture\nperformance over baseline dataset condensation methods.", "AI": {"tldr": "The paper presents Multi-Domain Dataset Condensation (MDDC) with Domain-Aware Module (DAM) to enhance dataset condensation across single and multi-domain settings, improving in-domain, out-of-domain, and cross-architecture performance without changing images per class.", "motivation": "Existing Dataset Condensation methods fail to consider the multi-domain nature of modern datasets that consist of heterogeneous images from multiple domains.", "method": "The authors introduce MDDC incorporating DAM, a training-time module embedding domain-related features into synthetic images via learnable spatial masks. Frequency-based pseudo-domain labeling is used for real-world datasets lacking explicit domain labels.", "result": "Experiments demonstrate consistent improvements in in-domain, out-of-domain, and cross-architecture performance compared to baseline dataset condensation methods.", "conclusion": "MDDC with DAM offers an effective solution for dataset condensation across both single and multi-domain settings."}}
{"id": "2505.21892", "pdf": "https://arxiv.org/pdf/2505.21892", "abs": "https://arxiv.org/abs/2505.21892", "authors": ["Xunpeng Huang", "Yingyu Lin", "Nikki Lijing Kuang", "Hanze Dong", "Difan Zou", "Yian Ma", "Tong Zhang"], "title": "Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion", "categories": ["stat.ML", "cs.LG"], "comment": "37 pages, 3 figures, 3 tables", "summary": "Continuous diffusion models have demonstrated remarkable performance in data\ngeneration across various domains, yet their efficiency remains constrained by\ntwo critical limitations: (1) the local adjacency structure of the forward\nMarkov process, which restricts long-range transitions in the data space, and\n(2) inherent biases introduced during the simulation of time-inhomogeneous\nreverse denoising processes. To address these challenges, we propose Quantized\nTransition Diffusion (QTD), a novel approach that integrates data quantization\nwith discrete diffusion dynamics. Our method first transforms the continuous\ndata distribution $p_*$ into a discrete one $q_*$ via histogram approximation\nand binary encoding, enabling efficient representation in a structured discrete\nlatent space. We then design a continuous-time Markov chain (CTMC) with Hamming\ndistance-based transitions as the forward process, which inherently supports\nlong-range movements in the original data space. For reverse-time sampling, we\nintroduce a \\textit{truncated uniformization} technique to simulate the reverse\nCTMC, which can provably provide unbiased generation from $q_*$ under minimal\nscore assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we\nprove that QTD can generate samples with $O(d\\ln^2(d/\\epsilon))$ score\nevaluations in expectation to approximate the $d$--dimensional target\ndistribution $p_*$ within an $\\epsilon$ error tolerance. Our method not only\nestablishes state-of-the-art inference efficiency but also advances the\ntheoretical foundations of diffusion-based generative modeling by unifying\ndiscrete and continuous diffusion paradigms.", "AI": {"tldr": "Continuous diffusion models face efficiency limitations due to local adjacency structures and biases in reverse denoising processes. This paper proposes Quantized Transition Diffusion (QTD), which combines data quantization with discrete diffusion dynamics to address these issues, achieving state-of-the-art inference efficiency.", "motivation": "The motivation is to overcome the efficiency limitations of continuous diffusion models by addressing the restricted long-range transitions in the data space and the inherent biases in reverse denoising processes.", "method": "The method involves transforming the continuous data distribution into a discrete one using histogram approximation and binary encoding. A continuous-time Markov chain (CTMC) with Hamming distance-based transitions is designed for the forward process, enabling long-range movements in the original data space. For reverse-time sampling, a truncated uniformization technique is introduced to simulate the reverse CTMC, ensuring unbiased generation under minimal score assumptions.", "result": "Through KL dynamic analysis, it is proven that QTD can generate samples with O(dln\u00b2(d/\u03b5)) score evaluations to approximate the d-dimensional target distribution within an \u03b5 error tolerance.", "conclusion": "QTD not only establishes state-of-the-art inference efficiency but also advances the theoretical foundations of diffusion-based generative modeling by unifying discrete and continuous diffusion paradigms."}}
{"id": "2505.21925", "pdf": "https://arxiv.org/pdf/2505.21925", "abs": "https://arxiv.org/abs/2505.21925", "authors": ["Chong Zeng", "Yue Dong", "Pieter Peers", "Hongzhi Wu", "Xin Tong"], "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://microsoft.github.io/renderformer", "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.", "AI": {"tldr": "RenderFormer\u662f\u4e00\u79cd\u65e0\u9700\u6bcf\u573a\u666f\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u53ef\u76f4\u63a5\u4ece\u57fa\u4e8e\u4e09\u89d2\u5f62\u7684\u573a\u666f\u8868\u793a\u6e32\u67d3\u56fe\u50cf\u5e76\u5305\u542b\u5b8c\u6574\u5168\u5c40\u5149\u7167\u6548\u679c\u7684\u795e\u7ecf\u6e32\u67d3\u7ba1\u9053\u3002\u5b83\u5c06\u6e32\u67d3\u89c6\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u8f6c\u6362\uff0c\u4f7f\u7528\u4e24\u4e2a\u9636\u6bb5\u7684transformer\u67b6\u6784\u6765\u5efa\u6a21\u548c\u751f\u6210\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u53ef\u80fd\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u573a\u666f\u8fdb\u884c\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5168\u5c40\u5149\u7167\u6548\u679c\u65f6\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5RenderFormer\u3002", "method": "RenderFormer\u91c7\u7528\u4e24\u9636\u6bb5pipeline\uff1a\u7b2c\u4e00\u9636\u6bb5\u662f\u4e0e\u89c6\u56fe\u65e0\u5173\u7684\u9636\u6bb5\uff0c\u4f7f\u7528transformer\u67b6\u6784\u5bf9\u4e09\u89d2\u5f62\u4e4b\u95f4\u7684\u5149\u4f20\u8f93\u8fdb\u884c\u5efa\u6a21\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u89c6\u56fe\u76f8\u5173\u7684\u9636\u6bb5\uff0c\u540c\u6837\u57fa\u4e8etransformer\u67b6\u6784\uff0c\u5c06\u5149\u7ebf\u675f\u8f6c\u5316\u4e3a\u50cf\u7d20\u503c\u3002\u6574\u4e2a\u8fc7\u7a0b\u4e0d\u9700\u8981\u6bcf\u573a\u666f\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u4ec5\u4f7f\u7528\u6700\u5c0f\u7684\u5148\u9a8c\u7ea6\u675f\u3002", "result": "RenderFormer\u5728\u4e0d\u540c\u5f62\u72b6\u548c\u5149\u4f20\u8f93\u590d\u6742\u5ea6\u7684\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\u548c\u8bc4\u4f30\uff0c\u8868\u660e\u5176\u80fd\u591f\u6709\u6548\u5730\u6e32\u67d3\u5177\u6709\u5168\u5c40\u5149\u7167\u6548\u679c\u7684\u56fe\u50cf\u3002", "conclusion": "RenderFormer\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u5230\u5e8f\u5217\u8f6c\u6362\u5b9e\u73b0\u9ad8\u6548\u7684\u6e32\u67d3\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6bcf\u573a\u666f\u7684\u5355\u72ec\u8bad\u7ec3\u9700\u6c42\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.21932", "pdf": "https://arxiv.org/pdf/2505.21932", "abs": "https://arxiv.org/abs/2505.21932", "authors": ["Adriana L. Duncan", "Joe Kileel"], "title": "Higher-Order Group Synchronization", "categories": ["stat.ML", "cs.CV", "cs.LG", "math.CO", "math.OC"], "comment": "40 pages", "summary": "Group synchronization is the problem of determining reliable global estimates\nfrom noisy local measurements on networks. The typical task for group\nsynchronization is to assign elements of a group to the nodes of a graph in a\nway that respects group elements given on the edges which encode information\nabout local pairwise relationships between the nodes. In this paper, we\nintroduce a novel higher-order group synchronization problem which operates on\na hypergraph and seeks to synchronize higher-order local measurements on the\nhyperedges to obtain global estimates on the nodes. Higher-order group\nsynchronization is motivated by applications to computer vision and image\nprocessing, among other computational problems. First, we define the problem of\nhigher-order group synchronization and discuss its mathematical foundations.\nSpecifically, we give necessary and sufficient synchronizability conditions\nwhich establish the importance of cycle consistency in higher-order group\nsynchronization. Then, we propose the first computational framework for general\nhigher-order group synchronization; it acts globally and directly on\nhigher-order measurements using a message passing algorithm. We discuss\ntheoretical guarantees for our framework, including convergence analyses under\noutliers and noise. Finally, we show potential advantages of our method through\nnumerical experiments. In particular, we show that in certain cases our\nhigher-order method applied to rotational and angular synchronization\noutperforms standard pairwise synchronization methods and is more robust to\noutliers. We also show that our method has comparable performance on simulated\ncryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM\nreconstruction package.", "AI": {"tldr": "This paper introduces a novel higher-order group synchronization problem on hypergraphs, providing synchronizability conditions, a computational framework with theoretical guarantees, and demonstrating its advantages through numerical experiments.", "motivation": "The motivation is to extend the traditional group synchronization problem to a higher-order version on hypergraphs for better handling of complex local measurements in applications like computer vision and image processing.", "method": "The method involves defining the higher-order group synchronization problem, establishing synchronizability conditions based on cycle consistency, proposing a global message passing algorithm as the computational framework, and analyzing its theoretical guarantees.", "result": "The proposed method outperforms standard pairwise synchronization methods in rotational and angular synchronization tasks, shows robustness to outliers, and has comparable performance on simulated cryo-EM data.", "conclusion": "Higher-order group synchronization provides a powerful tool for obtaining reliable global estimates from complex local measurements, with potential advantages in various computational problems."}}
{"id": "2505.22438", "pdf": "https://arxiv.org/pdf/2505.22438", "abs": "https://arxiv.org/abs/2505.22438", "authors": ["Zijian Liang", "Kai Niu", "Changshuo Wang", "Jin Xu", "Ping Zhang"], "title": "Synonymous Variational Inference for Perceptual Image Compression", "categories": ["cs.IT", "cs.AI", "cs.CV", "cs.LG", "eess.IV", "math.IT"], "comment": "31 pages, 20 figures. This paper is accepted by Proceedings of the\n  42nd International Conference on Machine Learning (ICML 2025) Poster", "summary": "Recent contributions of semantic information theory reveal the set-element\nrelationship between semantic and syntactic information, represented as\nsynonymous relationships. In this paper, we propose a synonymous variational\ninference (SVI) method based on this synonymity viewpoint to re-analyze the\nperceptual image compression problem. It takes perceptual similarity as a\ntypical synonymous criterion to build an ideal synonymous set (Synset), and\napproximate the posterior of its latent synonymous representation with a\nparametric density by minimizing a partial semantic KL divergence. This\nanalysis theoretically proves that the optimization direction of perception\nimage compression follows a triple tradeoff that can cover the existing\nrate-distortion-perception schemes. Additionally, we introduce synonymous image\ncompression (SIC), a new image compression scheme that corresponds to the\nanalytical process of SVI, and implement a progressive SIC codec to fully\nleverage the model's capabilities. Experimental results demonstrate comparable\nrate-distortion-perception performance using a single progressive SIC codec,\nthus verifying the effectiveness of our proposed analysis method.", "AI": {"tldr": "This paper proposes a synonymous variational inference (SVI) method for perceptual image compression, introducing synonymous image compression (SIC) with experimental results demonstrating its effectiveness.", "motivation": "To re-analyze the perceptual image compression problem by leveraging the set-element relationship between semantic and syntactic information as synonymous relationships.", "method": "Propose SVI method based on synonymity viewpoint, using perceptual similarity as synonymous criterion to build an ideal Synset and approximate posterior of latent synonymous representation. Introduce SIC scheme and implement progressive SIC codec.", "result": "Experimental results show comparable rate-distortion-perception performance with a single progressive SIC codec.", "conclusion": "The proposed SVI analysis method is effective in perceptual image compression, revealing a triple tradeoff in optimization."}}
{"id": "2505.22441", "pdf": "https://arxiv.org/pdf/2505.22441", "abs": "https://arxiv.org/abs/2505.22441", "authors": ["Chaitanya Amballa", "Sattwik Basu", "Yu-Lin Wei", "Zhijian Yang", "Mehmet Ergezer", "Romit Roy Choudhury"], "title": "Can NeRFs See without Cameras?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing.", "AI": {"tldr": "NeRFs can be redesigned to learn from multipath RF/audio signals, enabling inference of indoor floorplans from sparse WiFi measurements.", "motivation": "To explore the possibility of inferring environmental information using multipath RF/audio signals, similar to how NeRFs synthesize novel views of 3D scenes.", "method": "Redesign NeRFs to process and learn from multipath signals received by RF/audio sensors, applying this to infer indoor floorplans from sparse WiFi measurements at various locations within a home.", "result": "The implicitly learned floorplans are promising, allowing for forward applications like indoor signal prediction and basic ray tracing.", "conclusion": "Redesigned NeRFs can successfully interpret multipath signals to visualize environments, offering potential in areas such as indoor signal analysis and basic ray tracing."}}
{"id": "2505.21994", "pdf": "https://arxiv.org/pdf/2505.21994", "abs": "https://arxiv.org/abs/2505.21994", "authors": ["Josef Dick", "Seungchan Ko", "Kassem Mustapha", "Sanghyeon Park"], "title": "Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "Due to divergence instability, the accuracy of low-order conforming finite\nelement methods for nearly incompressible homogeneous elasticity equations\ndeteriorates as the Lam\\'e coefficient $\\lambda\\to\\infty$, or equivalently as\nthe Poisson ratio $\\nu\\to1/2$. This phenomenon, known as locking or\nnon-robustness, remains not fully understood despite extensive investigation.\nIn this paper, we propose a robust method based on a fundamentally different,\nmachine-learning-driven approach. Leveraging recently developed\nPhysics-Informed Neural Networks (PINNs), we address the numerical solution of\nlinear elasticity equations governing nearly incompressible materials. The core\nidea of our method is to appropriately decompose the given equations to\nalleviate the extreme imbalance in the coefficients, while simultaneously\nsolving both the forward and inverse problems to recover the solutions of the\ndecomposed systems as well as the associated external conditions. Through\nvarious numerical experiments, including constant, variable and parametric\nLam\\'e coefficients, we illustrate the efficiency of the proposed methodology.", "AI": {"tldr": "This paper proposes a machine-learning-driven method using Physics-Informed Neural Networks (PINNs) to robustly solve nearly incompressible elasticity equations, which traditional finite element methods struggle with as material incompressibility increases.", "motivation": "Traditional low-order conforming finite element methods face accuracy deterioration when solving nearly incompressible homogeneous elasticity equations due to divergence instability, especially as the Lam\u00e9 coefficient approaches infinity or the Poisson ratio approaches 1/2. This locking issue is not yet fully understood and remains an open problem.", "method": "The authors employ Physics-Informed Neural Networks (PINNs) to develop a new approach for solving linear elasticity equations of nearly incompressible materials. They decompose the original equations to balance coefficients and simultaneously solve both forward and inverse problems to recover solutions and external conditions.", "result": "Numerical experiments demonstrate the effectiveness of this method across scenarios with constant, variable, and parametric Lam\u00e9 coefficients, showing that the proposed methodology can efficiently handle nearly incompressible elasticity problems.", "conclusion": "The machine-learning-driven method based on PINNs provides a robust alternative to traditional finite element methods for solving nearly incompressible elasticity equations, alleviating issues related to locking."}}
{"id": "2505.22445", "pdf": "https://arxiv.org/pdf/2505.22445", "abs": "https://arxiv.org/abs/2505.22445", "authors": ["Puhua Jiang", "Zhangquan Chen", "Mingze Sun", "Ruqi Huang"], "title": "NFR: Neural Feature-Guided Non-Rigid Shape Registration", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.6"], "comment": "20 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2311.04494", "summary": "In this paper, we propose a novel learning-based framework for 3D shape\nregistration, which overcomes the challenges of significant non-rigid\ndeformation and partiality undergoing among input shapes, and, remarkably,\nrequires no correspondence annotation during training. Our key insight is to\nincorporate neural features learned by deep learning-based shape matching\nnetworks into an iterative, geometric shape registration pipeline. The\nadvantage of our approach is two-fold -- On one hand, neural features provide\nmore accurate and semantically meaningful correspondence estimation than\nspatial features (e.g., coordinates), which is critical in the presence of\nlarge non-rigid deformations; On the other hand, the correspondences are\ndynamically updated according to the intermediate registrations and filtered by\nconsistency prior, which prominently robustify the overall pipeline. Empirical\nresults show that, with as few as dozens of training shapes of limited\nvariability, our pipeline achieves state-of-the-art results on several\nbenchmarks of non-rigid point cloud matching and partial shape matching across\nvarying settings, but also delivers high-quality correspondences between unseen\nchallenging shape pairs that undergo both significant extrinsic and intrinsic\ndeformations, in which case neither traditional registration methods nor\nintrinsic methods work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5b66\u4e60\u76843D\u5f62\u72b6\u914d\u51c6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u514b\u670d\u4e86\u8f93\u5165\u5f62\u72b6\u4e4b\u95f4\u7684\u663e\u8457\u975e\u521a\u6027\u53d8\u5f62\u548c\u5c40\u90e8\u6027\u6311\u6218\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u9700\u8981\u5bf9\u5e94\u6ce8\u91ca\u3002\u901a\u8fc7\u5c06\u6df1\u5ea6\u5b66\u4e60\u5f62\u72b6\u5339\u914d\u7f51\u7edc\u5b66\u4e60\u7684\u795e\u7ecf\u7279\u5f81\u7eb3\u5165\u8fed\u4ee3\u51e0\u4f55\u5f62\u72b6\u914d\u51c6\u7ba1\u9053\u4e2d\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u5bf9\u5e94\u4f30\u8ba1\uff0c\u5e76\u6839\u636e\u4e2d\u95f4\u914d\u51c6\u52a8\u6001\u66f4\u65b0\u5bf9\u5e94\u5173\u7cfb\u4ee5\u63d0\u9ad8\u6574\u4f53\u7ba1\u9053\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5c11\u91cf\u53ef\u53d8\u6027\u6709\u9650\u7684\u8bad\u7ec3\u5f62\u72b6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u5177\u6709\u663e\u8457\u5916\u5728\u548c\u5185\u5728\u53d8\u5f62\u7684\u5f62\u72b6\u5bf9\u65f6\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u76ee\u524d\u7684\u5f62\u72b6\u914d\u51c6\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u663e\u8457\u7684\u975e\u521a\u6027\u53d8\u5f62\u548c\u90e8\u5206\u5f62\u72b6\u95ee\u9898\uff0c\u5e76\u4e14\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u5bf9\u5e94\u6ce8\u91ca\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5bf9\u5e94\u6ce8\u91ca\u7684\u65b0\u6846\u67b6\u3002", "method": "1. \u5c06\u6df1\u5ea6\u5b66\u4e60\u5f62\u72b6\u5339\u914d\u7f51\u7edc\u5b66\u4e60\u7684\u795e\u7ecf\u7279\u5f81\u878d\u5165\u5230\u8fed\u4ee3\u51e0\u4f55\u5f62\u72b6\u914d\u51c6\u6d41\u7a0b\u4e2d\u3002\n2. \u4f7f\u7528\u795e\u7ecf\u7279\u5f81\u4ee3\u66ff\u7a7a\u95f4\u7279\u5f81\uff08\u5982\u5750\u6807\uff09\u6765\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u5bf9\u5e94\u4f30\u8ba1\u3002\n3. \u6839\u636e\u4e2d\u95f4\u914d\u51c6\u52a8\u6001\u66f4\u65b0\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u5148\u9a8c\u8fc7\u6ee4\u4e0d\u7a33\u5b9a\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u6574\u4f53\u7ba1\u9053\u7684\u9c81\u68d2\u6027\u3002", "result": "1. \u5728\u4ec5\u4f7f\u7528\u51e0\u5341\u4e2a\u53d8\u5316\u6709\u9650\u7684\u8bad\u7ec3\u5f62\u72b6\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u975e\u521a\u6027\u70b9\u4e91\u5339\u914d\u548c\u90e8\u5206\u5f62\u72b6\u5339\u914d\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\n2. \u5bf9\u4e8e\u7ecf\u5386\u663e\u8457\u5916\u5728\u548c\u5185\u5728\u53d8\u5f62\u7684\u672a\u89c1\u5f62\u72b6\u5bf9\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u548c\u5185\u5728\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u65e0\u6cd5\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7279\u5f81\u548c\u52a8\u6001\u66f4\u65b0\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u975e\u521a\u6027\u5f62\u72b6\u914d\u51c6\u548c\u90e8\u5206\u5f62\u72b6\u5339\u914d\u7684\u6548\u679c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.22008", "pdf": "https://arxiv.org/pdf/2505.22008", "abs": "https://arxiv.org/abs/2505.22008", "authors": ["Jing-An Sun", "Hang Fan", "Junchao Gong", "Ben Fei", "Kun Chen", "Fenghua Ling", "Wenlong Zhang", "Wanghan Xu", "Li Yan", "Pierre Gentine", "Lei Bai"], "title": "Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Data assimilation (DA) aims to estimate the full state of a dynamical system\nby combining partial and noisy observations with a prior model forecast,\ncommonly referred to as the background. In atmospheric applications, this\nproblem is fundamentally ill-posed due to the sparsity of observations relative\nto the high-dimensional state space. Traditional methods address this challenge\nby simplifying background priors to regularize the solution, which are\nempirical and require continual tuning for application. Inspired by alignment\ntechniques in text-to-image diffusion models, we propose Align-DA, which\nformulates DA as a generative process and uses reward signals to guide\nbackground priors, replacing manual tuning with data-driven alignment.\nSpecifically, we train a score-based model in the latent space to approximate\nthe background-conditioned prior, and align it using three complementary reward\nsignals for DA: (1) assimilation accuracy, (2) forecast skill initialized from\nthe assimilated state, and (3) physical adherence of the analysis fields.\nExperiments with multiple reward signals demonstrate consistent improvements in\nanalysis quality across different evaluation metrics and observation-guidance\nstrategies. These results show that preference alignment, implemented as a soft\nconstraint, can automatically adapt complex background priors tailored to DA,\noffering a promising new direction for advancing the field.", "AI": {"tldr": "An abstract about a new method called Align-DA which formulates data assimilation as a generative process and uses reward signals to guide background priors.", "motivation": "Data assimilation (DA) is fundamentally ill-posed in atmospheric applications due to the sparsity of observations relative to the high-dimensional state space. Traditional methods address this challenge by simplifying background priors, which are empirical and require continual tuning for application.", "method": "The paper proposes Align-DA, which formulates DA as a generative process and uses reward signals to guide background priors. A score-based model is trained in the latent space to approximate the background-conditioned prior, and align it using three complementary reward signals: assimilation accuracy, forecast skill initialized from the assimilated state, and physical adherence of the analysis fields.", "result": "Experiments with multiple reward signals demonstrate consistent improvements in analysis quality across different evaluation metrics and observation-guidance strategies.", "conclusion": "Preference alignment, implemented as a soft constraint, can automatically adapt complex background priors tailored to DA, offering a promising new direction for advancing the field."}}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.", "AI": {"tldr": "The paper introduces MM-UPT, an unsupervised post-training framework for MLLMs based on GRPO. It replaces traditional reward signals with a self-rewarding mechanism and shows significant improvement in reasoning ability without external supervision.", "motivation": "To address the limitations of supervised methods for improving MLLMs which require expensive annotated data and the complexity of existing unsupervised post-training methods.", "method": "MM-UPT uses GRPO with a self-rewarding mechanism based on majority voting over multiple sampled responses for unsupervised post-training of MLLMs. Incorporating synthetic questions generated by MLLM itself can further boost performance.", "result": "MM-UPT significantly improves reasoning ability of Qwen2.5-VL-7B on MathVista and We-Math datasets. It outperforms prior unsupervised baselines and approaches the results of supervised GRPO.", "conclusion": "MM-UPT provides a new paradigm for continual and autonomous enhancement of MLLMs without external supervision."}}
{"id": "2505.22048", "pdf": "https://arxiv.org/pdf/2505.22048", "abs": "https://arxiv.org/abs/2505.22048", "authors": ["Haihan Zhang", "Weicheng Lin", "Yuanshi Liu", "Cong Fang"], "title": "Learning Curves of Stochastic Gradient Descent in Kernel Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper considers a canonical problem in kernel regression: how good are\nthe model performances when it is trained by the popular online first-order\nalgorithms, compared to the offline ones, such as ridge and ridgeless\nregression? In this paper, we analyze the foundational single-pass Stochastic\nGradient Descent (SGD) in kernel regression under source condition where the\noptimal predictor can even not belong to the RKHS, i.e. the model is\nmisspecified. Specifically, we focus on the inner product kernel over the\nsphere and characterize the exact orders of the excess risk curves under\ndifferent scales of sample sizes $n$ concerning the input dimension $d$.\nSurprisingly, we show that SGD achieves min-max optimal rates up to constants\namong all the scales, without suffering the saturation, a prevalent phenomenon\nobserved in (ridge) regression, except when the model is highly misspecified\nand the learning is in a final stage where $n\\gg d^{\\gamma}$ with any constant\n$\\gamma >0$. The main reason for SGD to overcome the curse of saturation is the\nexponentially decaying step size schedule, a common practice in deep neural\nnetwork training. As a byproduct, we provide the \\emph{first} provable\nadvantage of the scheme over the iterative averaging method in the common\nsetting.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u4e00\u9636\u7b97\u6cd5\u8bad\u7ec3\u7684\u6838\u56de\u5f52\u6a21\u578b\u4e0e\u79bb\u7ebf\u5cad\u56de\u5f52\u548c\u65e0\u5cad\u56de\u5f52\u76f8\u6bd4\u7684\u8868\u73b0\u3002\u5206\u6790\u4e86\u5355\u6b21\u901a\u8fc7\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD)\u5728\u6838\u56de\u5f52\u4e0b\u7684\u8fc7\u62df\u5408\u98ce\u9669\u66f2\u7ebf\uff0c\u5e76\u8868\u660eSGD\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u8fbe\u5230\u6700\u4f18\u7387\uff0c\u4e14\u4e0d\u4f1a\u53d7\u5230\u9971\u548c\u73b0\u8c61\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6307\u6570\u8870\u51cf\u6b65\u957f\u8ba1\u5212\u4f18\u4e8e\u8fed\u4ee3\u5e73\u5747\u65b9\u6cd5\u7684\u7b2c\u4e00\u4e2a\u53ef\u8bc1\u660e\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u4e00\u9636\u7b97\u6cd5\uff08\u5982SGD\uff09\u5728\u6838\u56de\u5f52\u4e2d\u7684\u8868\u73b0\u5982\u4f55\u4e0e\u79bb\u7ebf\u7b97\u6cd5\uff08\u5982\u5cad\u56de\u5f52\u548c\u65e0\u5cad\u56de\u5f52\uff09\u76f8\u6bd4\u8f83\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u53ef\u80fd\u88ab\u9519\u8bef\u6307\u5b9a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5206\u6790\u5355\u6b21\u901a\u8fc7\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD)\u5728\u7403\u9762\u4e0a\u7684\u5185\u79ef\u6838\u56de\u5f52\u95ee\u9898\u4e0b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5f53\u6700\u4f18\u9884\u6d4b\u5668\u4e0d\u5c5e\u4e8eRKHS\u65f6\u3002\u901a\u8fc7\u4e0d\u540c\u7684\u6837\u672c\u89c4\u6a21n\u548c\u8f93\u5165\u7ef4\u5ea6d\u7684\u5173\u7cfb\u6765\u523b\u753b\u8fc7\u62df\u5408\u98ce\u9669\u66f2\u7ebf\u3002\u4f7f\u7528\u6307\u6570\u8870\u51cf\u6b65\u957f\u8ba1\u5212\u3002", "result": "SGD\u5728\u51e0\u4e4e\u6240\u6709\u5c3a\u5ea6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5c0f\u6700\u5927\u6700\u4f18\u7387\uff0c\u907f\u514d\u4e86\u9971\u548c\u73b0\u8c61\uff0c\u9664\u975e\u6a21\u578b\u9ad8\u5ea6\u9519\u8bef\u6307\u5b9a\u4e14\u5904\u4e8e\u5b66\u4e60\u7684\u6700\u540e\u9636\u6bb5(n\u8fdc\u5927\u4e8ed^\u03b3\uff0c\u03b3>0)\u3002\u540c\u65f6\uff0c\u6307\u6570\u8870\u51cf\u6b65\u957f\u8ba1\u5212\u663e\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u8fed\u4ee3\u5e73\u5747\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "SGD\u5728\u6838\u56de\u5f52\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u514b\u670d\u9971\u548c\u73b0\u8c61\uff0c\u4e14\u5728\u9002\u5f53\u7684\u6b65\u957f\u8ba1\u5212\u4e0b\u8868\u73b0\u51fa\u6bd4\u8fed\u4ee3\u5e73\u5747\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.", "AI": {"tldr": "Next-event prediction (NEP) is proposed as a learning task that uses future video segments to promote temporal reasoning in MLLMs. A dataset V1-33K and evaluation benchmark FutureBench are introduced.", "motivation": "Existing tasks for temporal reasoning in video inputs either rely on human/stronger MLLM annotations or entangle temporal reasoning with spatial information.", "method": "Propose NEP where MLLM predicts a summary of events from future video frames given past frames as input, along with a dataset V1-33K and instruction-tuning strategies.", "result": "Experiments show NEP is a scalable and effective training paradigm for fostering temporal reasoning in MLLMs.", "conclusion": "NEP, supported by V1-33K and FutureBench, provides a promising direction for enhancing temporal reasoning capabilities in MLLMs."}}
{"id": "2505.22467", "pdf": "https://arxiv.org/pdf/2505.22467", "abs": "https://arxiv.org/abs/2505.22467", "authors": ["Jiaxi Yang", "Mengqi Zhang", "Yiqiao Jin", "Hao Chen", "Qingsong Wen", "Lu Lin", "Yi He", "Weijie Xu", "James Evans", "Jindong Wang"], "title": "Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.", "AI": {"tldr": "Large Language Model-based Multi-Agent Systems (MASs) require optimal structural organization for effective cooperation, which is currently underexplored. This paper proposes a three-stage framework (agent selection, structure profiling, topology synthesis) to develop topology-aware MASs, highlighting new research opportunities and challenges.", "motivation": "To address the underexplored aspect of how agents in MASs should be structurally organized for optimal cooperation.", "method": "Propose a three-stage framework consisting of agent selection, structure profiling, and topology synthesis to develop topology-aware MASs.", "result": "Redirects focus on structural organization in MASs, offering new research opportunities in language models, reinforcement learning, graph learning, and generative modeling.", "conclusion": "The perspective and framework provided could offer critical new insights into the era of agentic AI."}}
{"id": "2505.22083", "pdf": "https://arxiv.org/pdf/2505.22083", "abs": "https://arxiv.org/abs/2505.22083", "authors": ["H. L. Dao"], "title": "Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "In this work, we introduce the first type of non-Euclidean neural quantum\nstate (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent\nneural networks (RNNs)), to be used in the Variational Monte Carlo method of\napproximating the ground state wavefunction for quantum many-body systems. In\nparticular, we examine the performances of NQS ansatzes constructed from both\nconventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical\nsettings of the one- and two-dimensional transverse field Ising models (TFIM)\nof up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$\nsystems of up 50 spins. By virtue of the fact that, for all of the experiments\nperformed in this work, hyperbolic GRU can yield performances comparable to or\nbetter than Euclidean RNNs, which have been extensively studied in these\nsettings in the literature, our work is a proof-of-concept for the viability of\nhyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum\nmany-body systems. Furthermore, in settings where the Hamiltonian displays a\nclear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ &\n$J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor\ninteractions, our results show that hyperbolic GRU definitively outperforms its\nEuclidean version in all instances. The fact that these results are reminiscent\nof the established ones from natural language processing where hyperbolic GRU\nalmost always outperforms Euclidean RNNs when the training data exhibit a\ntree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU\nNQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin\nsystems that involve different degrees of nearest neighbor interactions.\nFinally, with this work, we hope to initiate future studies of other types of\nnon-Euclidean NQS beyond hyperbolic GRU.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5f15\u5165\u4e86\u53cc\u66f2GRU\u5f62\u5f0f\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u795e\u7ecf\u91cf\u5b50\u6001\uff08NQS\uff09\u5047\u8bbe\uff0c\u7528\u4e8e\u53d8\u5206\u8499\u7279\u5361\u7f57\u65b9\u6cd5\u4e2d\u8fd1\u4f3c\u591a\u4f53\u91cf\u5b50\u7cfb\u7edf\u7684\u57fa\u6001\u6ce2\u51fd\u6570\u3002\u901a\u8fc7\u5728\u6700\u591a100\u4e2a\u81ea\u65cb\u7684\u4e00\u7ef4\u548c\u4e8c\u7ef4\u6a2a\u5411\u573aIsing\u6a21\u578b\u4ee5\u53ca\u6700\u591a50\u4e2a\u81ea\u65cb\u7684\u4e00\u7ef4Heisenberg $J_1J_2$ \u548c $J_1J_2J_3$ \u7cfb\u7edf\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u53cc\u66f2GRU\u7684\u8868\u73b0\u4e0e\u6b27\u51e0\u91cc\u5f97RNN\u76f8\u5f53\u6216\u66f4\u597d\u3002\u7279\u522b\u662f\u5728\u5177\u6709\u660e\u663e\u5c42\u6b21\u4ea4\u4e92\u7ed3\u6784\u7684\u54c8\u5bc6\u987f\u91cf\u8bbe\u7f6e\u4e2d\uff0c\u5982\u4e00\u7ef4Heisenberg $J_1J_2$ \u548c $J_1J_2J_3$ \u7cfb\u7edf\uff0c\u53cc\u66f2GRU\u663e\u8457\u4f18\u4e8e\u5176\u6b27\u51e0\u91cc\u5f97\u7248\u672c\u3002\u8fd9\u4e9b\u7ed3\u679c\u7c7b\u4f3c\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5df2\u77e5\u7ed3\u679c\uff0c\u8868\u660e\u5f53\u8bad\u7ec3\u6570\u636e\u5448\u73b0\u6811\u72b6\u6216\u5c42\u6b21\u7ed3\u6784\u65f6\uff0c\u53cc\u66f2GRU\u51e0\u4e4e\u603b\u662f\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97RNN\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63a8\u6d4b\u5728\u6d89\u53ca\u4e0d\u540c\u6700\u8fd1\u90bb\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u81ea\u65cb\u7cfb\u7edf\u4e2d\uff0c\u53cc\u66f2GRU NQS\u5047\u8bbe\u53ef\u80fd\u4f1a\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97RNN/GRU NQS\u5047\u8bbe\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u975e\u6b27\u51e0\u91cc\u5f97\u795e\u7ecf\u7f51\u7edc\u5728\u91cf\u5b50\u591a\u4f53\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u53cc\u66f2GRU\u662f\u5426\u53ef\u4ee5\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4f20\u7edf\u7684\u6b27\u51e0\u91cc\u5f97RNN/GRU\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u53cc\u66f2GRU\u6784\u5efa\u975e\u6b27\u51e0\u91cc\u5f97\u795e\u7ecf\u91cf\u5b50\u6001\uff08NQS\uff09\u5047\u8bbe\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u53d8\u5206\u8499\u7279\u5361\u7f57\u65b9\u6cd5\u4e2d\uff0c\u4ee5\u8fd1\u4f3c\u6c42\u89e3\u5305\u62ec\u6a2a\u5411\u573aIsing\u6a21\u578b\u548cHeisenberg $J_1J_2$ \u53ca $J_1J_2J_3$ \u7cfb\u7edf\u5728\u5185\u7684\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u57fa\u6001\u6ce2\u51fd\u6570\u3002", "result": "\u53cc\u66f2GRU\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4e0e\u6b27\u51e0\u91cc\u5f97RNN\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5c24\u5176\u5728\u54c8\u5bc6\u987f\u91cf\u5177\u6709\u5c42\u6b21\u4ea4\u4e92\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u53cc\u66f2GRU\u663e\u8457\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u7248\u672c\u3002", "conclusion": "\u53cc\u66f2GRU\u4f5c\u4e3a\u7b2c\u4e00\u79cd\u975e\u6b27\u51e0\u91cc\u5f97NQS\u5047\u8bbe\u5728\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u5e76\u53ef\u80fd\u5728\u6d89\u53ca\u6700\u8fd1\u90bb\u76f8\u4e92\u4f5c\u7528\u7684\u91cf\u5b50\u81ea\u65cb\u7cfb\u7edf\u4e2d\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97RNN/GRU\u3002"}}
{"id": "2505.22477", "pdf": "https://arxiv.org/pdf/2505.22477", "abs": "https://arxiv.org/abs/2505.22477", "authors": ["Qi Gao", "Wei Xu", "Hanxi Pan", "Mowei Shen", "Zaifeng Gao"], "title": "Human-Centered Human-AI Collaboration (HCHAC)", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "This article is a chapter from the upcoming book Handbook of\n  Human-Centered Artificial Intelligence", "summary": "In the intelligent era, the interaction between humans and intelligent\nsystems fundamentally involves collaboration with autonomous intelligent\nagents. Human-AI Collaboration (HAC) represents a novel type of human-machine\nrelationship facilitated by autonomous intelligent machines equipped with AI\ntechnologies. In this paradigm, AI agents serve not only as auxiliary tools but\nalso as active teammates, partnering with humans to accomplish tasks\ncollaboratively. Human-centered AI (HCAI) emphasizes that humans play critical\nleadership roles in the collaboration. This human-led collaboration imparts new\ndimensions to the human-machine relationship, necessitating innovative research\nperspectives, paradigms, and agenda to address the unique challenges posed by\nHAC. This chapter delves into the essence of HAC from the human-centered\nperspective, outlining its core concepts and distinguishing features. It\nreviews the current research methodologies and research agenda within the HAC\nfield from the HCAI perspective, highlighting advancements and ongoing studies.\nFurthermore, a framework for human-centered HAC (HCHAC) is proposed by\nintegrating these reviews and analyses. A case study of HAC in the context of\nautonomous vehicles is provided, illustrating practical applications and the\nsynergistic interactions between humans and AI agents. Finally, it identifies\npotential future research directions aimed at enhancing the effectiveness,\nreliability, and ethical integration of human-centered HAC systems in diverse\ndomains.", "AI": {"tldr": "In the intelligent era, Human-AI Collaboration (HAC) forms a new human-machine relationship. Autonomous AI agents act as active teammates in collaboration with humans. Human-centered AI (HCAI) highlights human leadership in this collaboration. This chapter outlines HAC's core concepts and features from the HCAI perspective, reviews research methodologies and agenda, proposes a framework for human-centered HAC (HCHAC), provides a case study on autonomous vehicles, and identifies future research directions.", "motivation": "The interaction between humans and intelligent systems evolves towards collaboration with autonomous intelligent agents, requiring innovative research perspectives to address unique challenges posed by HAC.", "method": "The chapter reviews current research methodologies and agenda within the HAC field from the HCAI perspective, integrates these reviews into a proposed framework for human-centered HAC (HCHAC), and provides a case study of HAC in autonomous vehicles.", "result": "A framework for human-centered HAC (HCHAC) is proposed, illustrating practical applications through a case study in autonomous vehicles, and identifying potential future research directions.", "conclusion": "Future research should aim at enhancing the effectiveness, reliability, and ethical integration of human-centered HAC systems across various domains."}}
{"id": "2505.22085", "pdf": "https://arxiv.org/pdf/2505.22085", "abs": "https://arxiv.org/abs/2505.22085", "authors": ["Arnulf Jentzen", "Julian Kranz", "Adrian Riekert"], "title": "PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "comment": "38 pages, 13 figures", "summary": "Averaging techniques such as Ruppert--Polyak averaging and exponential\nmovering averaging (EMA) are powerful approaches to accelerate optimization\nprocedures of stochastic gradient descent (SGD) optimization methods such as\nthe popular ADAM optimizer. However, depending on the specific optimization\nproblem under consideration, the type and the parameters for the averaging need\nto be adjusted to achieve the smallest optimization error. In this work we\npropose an averaging approach, which we refer to as parallel averaged ADAM\n(PADAM), in which we compute parallely different averaged variants of ADAM and\nduring the training process dynamically select the variant with the smallest\noptimization error. A central feature of this approach is that this procedure\nrequires no more gradient evaluations than the usual ADAM optimizer as each of\nthe averaged trajectories relies on the same underlying ADAM trajectory and\nthus on the same underlying gradients. We test the proposed PADAM optimizer in\n13 stochastic optimization and deep neural network (DNN) learning problems and\ncompare its performance with known optimizers from the literature such as\nstandard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In\nparticular, we apply the compared optimizers to physics-informed neural\nnetwork, deep Galerkin, deep backward stochastic differential equation and deep\nKolmogorov approximations for boundary value partial differential equation\nproblems from scientific machine learning, as well as to DNN approximations for\noptimal control and optimal stopping problems. In nearly all of the considered\nexamples PADAM achieves, sometimes among others and sometimes exclusively,\nessentially the smallest optimization error. This work thus strongly suggest to\nconsider PADAM for scientific machine learning problems and also motivates\nfurther research for adaptive averaging procedures within the training of DNNs.", "AI": {"tldr": "The paper introduces Parallel Averaged ADAM (PADAM), an averaging approach that computes different averaged variants of the ADAM optimizer in parallel and dynamically selects the variant with the smallest optimization error during training. PADAM achieves comparable or better performance than existing optimizers in various stochastic optimization and deep neural network learning problems, particularly in scientific machine learning applications.", "motivation": "Existing averaging techniques for accelerating stochastic gradient descent (SGD) methods require parameter tuning based on specific optimization problems to achieve the smallest optimization error. The authors aim to develop a more adaptive approach that can automatically select the best averaging variant without additional gradient evaluations.", "method": "The proposed method, PADAM, computes multiple averaged versions of the ADAM optimizer in parallel using the same underlying gradients as the standard ADAM optimizer. During training, it dynamically selects the variant with the smallest optimization error. This avoids the need for additional gradient evaluations compared to standard ADAM.", "result": "PADAM was tested on 13 stochastic optimization and DNN learning problems, including physics-informed neural networks, deep Galerkin methods, and optimal control problems. In nearly all cases, PADAM achieved the smallest optimization error, either alone or alongside other optimizers.", "conclusion": "PADAM is a promising optimizer for scientific machine learning problems due to its ability to achieve small optimization errors without requiring additional gradient computations. The results motivate further research into adaptive averaging procedures for training deep neural networks."}}
{"id": "2505.22090", "pdf": "https://arxiv.org/pdf/2505.22090", "abs": "https://arxiv.org/abs/2505.22090", "authors": ["Tristan S. W. Stevens", "Ois\u00edn Nolan", "Oudom Somphone", "Jean-Luc Robert", "Ruud J. G. van Sloun"], "title": "High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models", "categories": ["eess.IV", "cs.LG"], "comment": "10 pages, 10 figures, preprint", "summary": "Three-dimensional ultrasound enables real-time volumetric visualization of\nanatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the\nreliance on precise probe orientation, potentially making ultrasound more\naccessible to clinicians with varying levels of experience and improving\nautomated measurements and post-exam analysis. However, achieving both high\nvolume rates and high image quality remains a significant challenge. While 3D\ndiverging waves can provide high volume rates, they suffer from limited tissue\nharmonic generation and increased multipath effects, which degrade image\nquality. One compromise is to retain the focusing in elevation while leveraging\nunfocused diverging waves in the lateral direction to reduce the number of\ntransmissions per elevation plane. Reaching the volume rates achieved by full\n3D diverging waves, however, requires dramatically undersampling the number of\nelevation planes. Subsequently, to render the full volume, simple interpolation\ntechniques are applied. This paper introduces a novel approach to 3D ultrasound\nreconstruction from a reduced set of elevation planes by employing diffusion\nmodels (DMs) to achieve increased spatial and temporal resolution. We compare\nboth traditional and supervised deep learning-based interpolation methods on a\n3D cardiac ultrasound dataset. Our results show that DM-based reconstruction\nconsistently outperforms the baselines in image quality and downstream task\nperformance. Additionally, we accelerate inference by leveraging the temporal\nconsistency inherent to ultrasound sequences. Finally, we explore the\nrobustness of the proposed method by exploiting the probabilistic nature of\ndiffusion posterior sampling to quantify reconstruction uncertainty and\ndemonstrate improved recall on out-of-distribution data with synthetic\nanomalies under strong subsampling.", "AI": {"tldr": "This paper presents a new method using diffusion models for 3D ultrasound reconstruction from fewer elevation planes, which improves image quality and downstream task performance.", "motivation": "To address the challenge of achieving both high volume rates and high image quality in 3D ultrasound imaging.", "method": "Employing diffusion models for reconstructing 3D ultrasound images from a reduced set of elevation planes; comparing with traditional and deep learning-based interpolation methods; leveraging temporal consistency for accelerated inference; quantifying reconstruction uncertainty via probabilistic sampling.", "result": "Diffusion model-based reconstruction outperforms baseline methods in image quality and downstream tasks; demonstrates robustness on out-of-distribution data with synthetic anomalies under strong subsampling.", "conclusion": "The proposed diffusion model approach enhances spatial and temporal resolution in 3D ultrasound imaging, providing superior image quality and robustness."}}
{"id": "2505.22094", "pdf": "https://arxiv.org/pdf/2505.22094", "abs": "https://arxiv.org/abs/2505.22094", "authors": ["Tonghe Zhang", "Yu Chao", "Sicang Su", "Yu Wang"], "title": "ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "30 pages, 13 figures, 10 tables", "summary": "We propose ReinFlow, a simple yet effective online reinforcement learning\n(RL) framework that fine-tunes a family of flow matching policies for\ncontinuous robotic control. Derived from rigorous RL theory, ReinFlow injects\nlearnable noise into a flow policy's deterministic path, converting the flow\ninto a discrete-time Markov Process for exact and straightforward likelihood\ncomputation. This conversion facilitates exploration and ensures training\nstability, enabling ReinFlow to fine-tune diverse flow model variants,\nincluding Rectified Flow [35] and Shortcut Models [19], particularly at very\nfew or even one denoising step. We benchmark ReinFlow in representative\nlocomotion and manipulation tasks, including long-horizon planning with visual\ninput and sparse reward. The episode reward of Rectified Flow policies obtained\nan average net growth of 135.36% after fine-tuning in challenging legged\nlocomotion tasks while saving denoising steps and 82.63% of wall time compared\nto state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate\nof the Shortcut Model policies in state and visual manipulation tasks achieved\nan average net increase of 40.34% after fine-tuning with ReinFlow at four or\neven one denoising step, whose performance is comparable to fine-tuned DDIM\npolicies while saving computation time for an average of 23.20%. Project\nWebpage: https://reinflow.github.io/", "AI": {"tldr": "The paper proposes ReinFlow, an online reinforcement learning framework that fine-tunes flow matching policies for continuous robotic control. It injects learnable noise into the deterministic path of a flow policy to convert it into a discrete-time Markov Process. This conversion facilitates exploration and ensures training stability. The method is benchmarked in locomotion and manipulation tasks, showing significant improvements in episode reward and success rate while saving computation time.", "motivation": "To develop an effective online reinforcement learning framework that can fine-tune diverse flow model variants for continuous robotic control, improving both performance and computational efficiency.", "method": "ReinFlow introduces learnable noise into the deterministic path of a flow policy, converting it into a discrete-time Markov Process. This allows for exact likelihood computation, enhancing exploration and ensuring training stability. The framework can fine-tune various flow model variants, including Rectified Flow and Shortcut Models, even with very few denoising steps.", "result": "In challenging legged locomotion tasks, the episode reward of Rectified Flow policies increased by an average of 135.36% after fine-tuning with ReinFlow, saving denoising steps and 82.63% of wall time compared to DPPO. In state and visual manipulation tasks, the success rate of Shortcut Model policies improved by an average of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, with performance comparable to DDIM policies while saving an average of 23.20% computation time.", "conclusion": "ReinFlow effectively fine-tunes flow matching policies for continuous robotic control, achieving significant improvements in performance and computational efficiency in various locomotion and manipulation tasks."}}
{"id": "2505.22503", "pdf": "https://arxiv.org/pdf/2505.22503", "abs": "https://arxiv.org/abs/2505.22503", "authors": ["Yuanfei Wang", "Xinju Huang", "Fangwei Zhong", "Yaodong Yang", "Yizhou Wang", "Yuanpei Chen", "Hao Dong"], "title": "From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments.", "AI": {"tldr": "Embodied agents need to not only execute tasks but also collaborate with human users having vague and implicit goals. This work develops HA-Desire, a home assistance simulation environment integrating an LLM-driven human user agent. A novel framework FAMER is presented for fast desire alignment through mental reasoning, reflection-based communication, and memory persistence.", "motivation": "Real-world applications require embodied agents to go beyond task execution and collaborate effectively with unfamiliar agents and human users who have vague and implicit goals. Thus, there is a need for agents to interpret ambiguous instructions and uncover underlying desires for effective assistance.", "method": "The method involves developing HA-Desire, a home assistance simulation environment with an LLM-driven human user agent that exhibits realistic value-driven goal selection and communication. The ego agent interacts with this proxy user using the FAMER framework, which includes a desire-based mental reasoning mechanism, a reflection-based communication module, and goal-relevant information extraction with memory persistence.", "result": "Extensive experiments show that the FAMER framework significantly improves both task execution and communication efficiency, allowing embodied agents to quickly adapt to user-specific desires in complex environments.", "conclusion": "Fast and accurate desire alignment is crucial for embodied agents. The proposed FAMER framework enhances the ability of embodied agents to interact effectively with human users in complex environments by improving task execution and communication efficiency."}}
{"id": "2505.22099", "pdf": "https://arxiv.org/pdf/2505.22099", "abs": "https://arxiv.org/abs/2505.22099", "authors": ["Wenwen Qiang", "Ziyin Gu", "Lingyu Si", "Jiangmeng Li", "Changwen Zheng", "Fuchun Sun", "Hui Xiong"], "title": "On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we addressed the limitation of relying solely on distribution\nalignment and source-domain empirical risk minimization in Unsupervised Domain\nAdaptation (UDA). Our information-theoretic analysis showed that this standard\nadversarial-based framework neglects the discriminability of target-domain\nfeatures, leading to suboptimal performance. To bridge this\ntheoretical-practical gap, we defined \"good representation learning\" as\nguaranteeing both transferability and discriminability, and proved that an\nadditional loss term targeting target-domain discriminability is necessary.\nBuilding on these insights, we proposed a novel adversarial-based UDA framework\nthat explicitly integrates a domain alignment objective with a\ndiscriminability-enhancing constraint. Instantiated as Domain-Invariant\nRepresentation Learning with Global and Local Consistency (RLGLC), our method\nleverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)\nto address class imbalance and semantic dimension weighting, and employs a\nlocal consistency mechanism to preserve fine-grained target-domain\ndiscriminative information. Extensive experiments across multiple benchmark\ndatasets demonstrate that RLGLC consistently surpasses state-of-the-art\nmethods, confirming the value of our theoretical perspective and underscoring\nthe necessity of enforcing both transferability and discriminability in\nadversarial-based UDA.", "AI": {"tldr": "In this paper, the authors propose a novel adversarial-based Unsupervised Domain Adaptation (UDA) framework called RLGLC. It integrates domain alignment with discriminability-enhancing constraints and uses AR-WWD to address class imbalance and semantic dimension weighting. Experiments show that RLGLC surpasses state-of-the-art methods.", "motivation": "The standard adversarial-based framework in UDA only focuses on distribution alignment and source-domain empirical risk minimization, neglecting the discriminability of target-domain features which leads to suboptimal performance.", "method": "The authors define 'good representation learning' as guaranteeing both transferability and discriminability, then they prove that an additional loss term for target-domain discriminability is necessary. They propose RLGLC method which leverages AR-WWD and a local consistency mechanism.", "result": "Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently outperforms state-of-the-art methods.", "conclusion": "The proposed RLGLC method confirms the value of the theoretical perspective and emphasizes the necessity of enforcing both transferability and discriminability in adversarial-based UDA."}}
{"id": "2505.22513", "pdf": "https://arxiv.org/pdf/2505.22513", "abs": "https://arxiv.org/abs/2505.22513", "authors": ["Bradley Phillips", "Edith Elkind", "Nicholas Teh", "Tomasz W\u0105s"], "title": "Strengthening Proportionality in Temporal Voting", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "We study proportional representation in the framework of temporal voting with\napproval ballots. Prior work adapted basic proportional representation concepts\n-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)\n-- from the multiwinner setting to the temporal setting. Our work introduces\nand examines ways of going beyond EJR. Specifically, we consider stronger\nvariants of JR, PJR, and EJR, and introduce temporal adaptations of more\ndemanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR\n(FPJR), and the Core. For each of these concepts, we investigate its existence\nand study its relationship to existing notions, thereby establishing a rich\nhierarchy of proportionality concepts. Notably, we show that two of our\nproposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable\nin every temporal election.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u65f6\u95f4\u6295\u7968\u6846\u67b6\u4e0b\uff0c\u8d85\u8d8aEJR\u7684\u66f4\u5f3a\u6bd4\u4f8b\u4ee3\u8868\u6743\u6982\u5ff5\u7684\u5b58\u5728\u6027\u548c\u76f8\u4e92\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u6bd4\u4f8b\u4ee3\u8868\u6743\u6982\u5ff5\uff08JR\u3001PJR\u3001EJR\uff09\u5df2\u88ab\u9002\u5e94\u5230\u65f6\u95f4\u6295\u7968\u573a\u666f\uff0c\u4f46\u9700\u8981\u63a2\u7d22\u66f4\u5f3a\u7684\u6bd4\u4f8b\u6027\u6982\u5ff5\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u8868\u793a\u3002", "method": "\u5f15\u5165\u5e76\u7814\u7a76\u4e86\u66f4\u5f3a\u7684JR\u3001PJR\u548cEJR\u53d8\u4f53\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u8d62\u5bb6\u9009\u4e3e\u4e2d\u66f4\u4e25\u683c\u7684\u6982\u5ff5\uff08\u5982EJR+\u3001FJR\u3001FPJR\u548cCore\uff09\u7684\u65f6\u95f4\u9002\u5e94\u7248\u672c\u3002\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u65b0\u6982\u5ff5\u7684\u5b58\u5728\u6027\u548c\u4e0e\u73b0\u6709\u6982\u5ff5\u7684\u5173\u7cfb\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u4e30\u5bcc\u7684\u6bd4\u4f8b\u6027\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u4e2a\u65b0\u63d0\u51fa\u7684\u516c\u7406EJR+\u548cFJR\u80fd\u591f\u5f3a\u5316EJR\uff0c\u540c\u65f6\u5728\u6bcf\u6b21\u65f6\u95f4\u9009\u4e3e\u4e2d\u4ecd\u7136\u53ef\u6ee1\u8db3\u3002", "conclusion": "\u672c\u7814\u7a76\u6269\u5c55\u4e86\u65f6\u95f4\u6295\u7968\u4e2d\u7684\u6bd4\u4f8b\u4ee3\u8868\u6743\u6982\u5ff5\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u548c\u5f3a\u5927\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u548c\u5b9e\u73b0\u6bd4\u4f8b\u6027\u3002"}}
{"id": "2505.22107", "pdf": "https://arxiv.org/pdf/2505.22107", "abs": "https://arxiv.org/abs/2505.22107", "authors": ["Shuhai Zhang", "Zeng You", "Yaofo Chen", "Zhiquan Wen", "Qianyue Wang", "Zhijie Qiu", "Yuanqing Li", "Mingkui Tan"], "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention.", "AI": {"tldr": "Transformer-based LLMs struggle with computational inefficiency due to redundant attention computations. This paper reframes probabilistic sequence modeling as a supervised learning task, analyzes attention sparsity, and proposes Dynamic Group Attention (DGA) to reduce redundancy and computational costs while maintaining performance.", "motivation": "To address the issue of redundant attention computations in long-context modeling, which leads to significant computational inefficiencies despite sparse attention weights.", "method": "Reformulate traditional probabilistic sequence modeling as a supervised learning task, analyze attention sparsity theoretically, formulate attention optimization as a linear coding problem, and propose a group coding strategy leading to the development of Dynamic Group Attention (DGA).", "result": "Empirical results demonstrate that DGA significantly reduces computational costs while maintaining competitive performance.", "conclusion": "Dynamic Group Attention offers an effective solution for reducing redundancy and computational costs in transformer-based models without sacrificing performance."}}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'\u7528\u751f\u6210\u56fe\u50cf\u601d\u8003'\u7684\u65b0\u8303\u5f0f\uff0c\u4f7f\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u4e2d\u95f4\u89c6\u89c9\u601d\u7ef4\u6b65\u9aa4\uff0c\u5728\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u8fdb\u884c\u539f\u751f\u7684\u601d\u8003\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u63a8\u7406\u53d7\u9650\u4e8e\u5904\u7406\u56fa\u5b9a\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u6216\u4ec5\u901a\u8fc7\u57fa\u4e8e\u6587\u672c\u7684\u94fe\u5f0f\u601d\u7ef4\u8fdb\u884c\u63a8\u7406\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u6784\u5efa\u4e2d\u95f4\u89c6\u89c9\u601d\u7ef4\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u673a\u5236\u5b9e\u73b0\uff1a(1) \u5e26\u4e2d\u95f4\u89c6\u89c9\u5b50\u76ee\u6807\u7684\u89c6\u89c9\u751f\u6210\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u9010\u6b65\u751f\u6210\u548c\u96c6\u6210\u7684\u90e8\u5206\uff1b(2) \u5e26\u81ea\u6211\u6279\u8bc4\u7684\u89c6\u89c9\u751f\u6210\uff0c\u751f\u6210\u521d\u59cb\u5047\u8bbe\u5e76\u901a\u8fc7\u6587\u672c\u63a8\u7406\u5206\u6790\u5176\u4e0d\u8db3\u5e76\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u89c6\u89c9\u751f\u6210\u57fa\u51c6\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u591a\u5bf9\u8c61\u573a\u666f\u65f6\uff0c\u76f8\u5bf9\u6539\u8fdb\u9ad8\u8fbe50%\uff08\u4ece38%\u63d0\u5347\u523057%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5141\u8bb8AI\u6a21\u578b\u50cf\u4eba\u7c7b\u4e00\u6837\u8fdb\u884c\u89c6\u89c9\u60f3\u8c61\u548c\u8fed\u4ee3\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\u5982\u751f\u7269\u5316\u5b66\u3001\u5efa\u7b51\u3001\u6cd5\u533b\u5b66\u548c\u7bee\u7403\u7b56\u7565\u7b49\uff0c\u5e76\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.22135", "pdf": "https://arxiv.org/pdf/2505.22135", "abs": "https://arxiv.org/abs/2505.22135", "authors": ["Yuichiro Hoshino", "Hideyuki Tachibana", "Muneyoshi Inahara", "Hiroto Takegawa"], "title": "RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Hybrid models combining Transformers and State Space Models (SSMs) are\npromising for balancing performance and efficiency. However, optimizing these\nhybrid models, particularly by addressing the potential redundancy inherent\nwithin the Transformer components, remains a significant challenge. In this\npaper, we propose RAD (Redundancy-Aware Distillation), a novel framework that\nuses self-speculative decoding as a diagnostic tool to identify redundant\nattention layers within the model. These identified layers are then selectively\nreplaced with SSM components, followed by targeted (self-)distillation.\nSpecifically, RAD focuses knowledge transfer on the components identified as\nredundant, considering architectural changes and specific weight initialization\nstrategies. We experimentally demonstrate that self-distillation using RAD\nsignificantly surpasses the performance of the original base model on\nmathematical and coding tasks. Furthermore, RAD is also effective in standard\nknowledge distillation settings, achieving up to approximately 2x faster\nconvergence compared to baseline methods. Notably, while a baseline model\ndistilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and\n22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and\n28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers\na new pathway for efficient optimization and performance enhancement in the\ndistillation of hybrid models.", "AI": {"tldr": "RAD (Redundancy-Aware Distillation) is a new framework which uses self-speculative decoding to identify and replace redundant attention layers in hybrid models with SSM components, followed by targeted distillation. It surpasses the original model's performance on math and coding tasks, converges faster in standard knowledge distillation settings, and achieves higher scores than baseline methods even when using a smaller teacher model.", "motivation": "Hybrid models combining Transformers and State Space Models (SSMs) show promise for balancing performance and efficiency, but redundancy within Transformer components poses an optimization challenge.", "method": "Propose RAD that employs self-speculative decoding to detect redundant attention layers in hybrid models, replaces them with SSM components, and applies targeted (self-)distillation focusing on the identified redundant components while considering architectural changes and weight initialization strategies.", "result": "Experiments show that self-distillation using RAD outperforms the original base model on mathematical and coding tasks, converges up to approximately 2x faster in standard knowledge distillation settings, and achieves significantly higher scores on GSM8K and CRUX compared to baseline methods even with a much smaller teacher model.", "conclusion": "RAD presents a novel approach for efficient optimization and performance improvement in the distillation of hybrid models."}}
{"id": "2505.22543", "pdf": "https://arxiv.org/pdf/2505.22543", "abs": "https://arxiv.org/abs/2505.22543", "authors": ["Ziheng Jia", "Zicheng Zhang", "Zeyu Zhang", "Yingji Liang", "Xiaorong Zhu", "Chunyi Li", "Jinliang Han", "Haoning Wu", "Bin Wang", "Haoran Zhang", "Guanyu Zhu", "Qiyong Zhao", "Xiaohong Liu", "Guangtao Zhai", "Xiongkuo Min"], "title": "Scaling-up Perceptual Video Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The data scaling law has been shown to significantly enhance the performance\nof large multi-modal models (LMMs) across various downstream tasks. However, in\nthe domain of perceptual video quality assessment (VQA), the potential of\nscaling law remains unprecedented due to the scarcity of labeled resources and\nthe insufficient scale of datasets. To address this, we propose\n\\textbf{OmniVQA}, an efficient framework designed to efficiently build\nhigh-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).\nWe then scale up to create \\textbf{OmniVQA-Chat-400K}, the largest MIDB in the\nVQA field concurrently. Our focus is on the technical and aesthetic quality\ndimensions, with abundant in-context instruction data to provide fine-grained\nVQA knowledge. Additionally, we have built the \\textbf{OmniVQA-MOS-20K} dataset\nto enhance the model's quantitative quality rating capabilities. We then\nintroduce a \\textbf{complementary} training strategy that effectively leverages\nthe knowledge from datasets for quality understanding and quality rating tasks.\nFurthermore, we propose the \\textbf{OmniVQA-FG (fine-grain)-Benchmark} to\nevaluate the fine-grained performance of the models. Our results demonstrate\nthat our models achieve state-of-the-art performance in both quality\nunderstanding and rating tasks.", "AI": {"tldr": "The paper introduces OmniVQA, a framework for building high-quality VQA MIDBs, and creates the largest such database (OmniVQA-Chat-400K). They also build OmniVQA-MOS-20K to improve quality rating, propose a complementary training strategy, and introduce OmniVQA-FG-Benchmark for evaluation. The models achieve state-of-the-art performance in both quality understanding and rating tasks.", "motivation": "To address the lack of labeled resources and insufficient dataset scale in perceptual video quality assessment (VQA), which limits the potential of data scaling law in enhancing model performance.", "method": "1. Propose OmniVQA - an efficient framework for building high-quality human-in-the-loop VQA multi-modal instruction databases (MIDBs).\n2. Scale up to create OmniVQA-Chat-400K, the largest MIDB in the VQA field.\n3. Build OmniVQA-MOS-20K dataset to enhance the model's quantitative quality rating capabilities.\n4. Introduce a complementary training strategy that leverages knowledge from datasets for quality understanding and rating tasks.\n5. Propose OmniVQA-FG-Benchmark to evaluate the fine-grained performance of models.", "result": "Models trained using the proposed framework and datasets achieve state-of-the-art performance in both quality understanding and rating tasks.", "conclusion": "OmniVQA framework and associated datasets effectively address the challenges of limited labeled resources and dataset scale in VQA, demonstrating significant improvements in model performance."}}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552", "abs": "https://arxiv.org/abs/2505.22552", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones.", "AI": {"tldr": "\u5c06\u77e5\u8bc6\u56fe\u8c31(KGs)\u4e0e\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u7ed3\u5408\u4ee5\u589e\u5f3a\u58f0\u660e\u9a8c\u8bc1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u662f\u65b0\u5174\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528KGs\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u73b0\u4ee3LLMs\u5728\u591a\u6b65\u9aa4\u63a8\u7406\u4e0a\u4e5f\u5b58\u5728\u56f0\u96be\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ClaimPKG\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u4e13\u7528LLM\u5c06\u8f93\u5165\u58f0\u660e\u8868\u793a\u4e3a\u4f2a\u5b50\u56fe\uff0c\u5f15\u5bfc\u4e13\u95e8\u7684\u5b50\u56fe\u68c0\u7d22\u6a21\u5757\u8bc6\u522b\u76f8\u5173KG\u5b50\u56fe\uff0c\u518d\u7531\u901a\u7528LLM\u751f\u6210\u6700\u7ec8\u5224\u65ad\u548c\u7406\u7531\u3002\u5b9e\u9a8c\u8868\u660e\uff0cClaimPKG\u5728FactKG\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u4e14\u5728HoVer\u548cFEVEROUS\u7b49\u975e\u7ed3\u6784\u5316\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31(KGs)\u63d0\u4f9b\u4e86\u9002\u5408\u63a8\u7406\u7684\u7ed3\u6784\u5316\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u7136\u800c\u5927\u591a\u6570\u73b0\u6709\u7684\u58f0\u660e\u9a8c\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528KGs\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u6ca1\u6709\u9002\u5e94\u7684\u60c5\u51b5\u4e0b\uff0c\u5904\u7406\u591a\u6b65\u9aa4\u7ba1\u9053\u548c\u5728KGs\u4e0a\u8fdb\u884c\u63a8\u7406\u65f6\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6ClaimPKG\uff0c\u5b83\u65e0\u7f1d\u6574\u5408\u4e86LLM\u63a8\u7406\u4e0e\u6765\u81eaKGs\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u3002\u5177\u4f53\u6765\u8bf4\uff0cClaimPKG\u4f7f\u7528\u8f7b\u91cf\u7ea7\u3001\u4e13\u7528\u7684LLM\u5c06\u8f93\u5165\u58f0\u660e\u8868\u793a\u4e3a\u4f2a\u5b50\u56fe\uff0c\u6307\u5bfc\u4e13\u95e8\u7684\u5b50\u56fe\u68c0\u7d22\u6a21\u5757\u8bc6\u522b\u76f8\u5173\u7684KG\u5b50\u56fe\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u68c0\u7d22\u5230\u7684\u5b50\u56fe\u88ab\u901a\u7528LLM\u5904\u7406\uff0c\u4ee5\u751f\u6210\u6700\u7ec8\u5224\u65ad\u548c\u7406\u7531\u3002", "result": "\u5728FactKG\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cClaimPKG\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u7c7b\u522b\u4e2d\u6bd4\u5f3a\u5927\u57fa\u7ebf\u9ad8\u51fa9%-12%\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0cClaimPKG\u5c55\u793a\u4e86\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u96c6\u5982HoVer\u548cFEVEROUS\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u6765\u81eaKGs\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0eLLM\u63a8\u7406\u3002", "conclusion": "ClaimPKG\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u514b\u670d\u73b0\u6709\u58f0\u660e\u9a8c\u8bc1\u65b9\u6cd5\u5728\u5229\u7528KGs\u548c\u591a\u6b65\u9aa4\u63a8\u7406\u65b9\u9762\u7684\u9650\u5236\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u4e13\u7528\u548c\u901a\u7528LLM\u7684\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u57fa\u4e8eKGs\u7684\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.22564", "pdf": "https://arxiv.org/pdf/2505.22564", "abs": "https://arxiv.org/abs/2505.22564", "authors": ["Jaehyun Choi", "Jiwan Hur", "Gyojin Han", "Jaemyung Yu", "Junmo Kim"], "title": "PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video dataset condensation has emerged as a critical technique for addressing\nthe computational challenges associated with large-scale video data processing\nin deep learning applications. While significant progress has been made in\nimage dataset condensation, the video domain presents unique challenges due to\nthe complex interplay between spatial content and temporal dynamics. This paper\nintroduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for\nvideo dataset condensation, a novel approach that fundamentally reconsiders how\nvideo data should be condensed. Unlike the previous method that separates\nstatic content from dynamic motion, our method preserves the essential\ninterdependence between these elements. Our approach progressively refines and\ninserts frames to fully accommodate the motion in an action while achieving\nbetter performance but less storage, considering the relation of gradients for\neach frame. Extensive experiments across standard video action recognition\nbenchmarks demonstrate that PRISM outperforms existing disentangled approaches\nwhile maintaining compact representations suitable for resource-constrained\nenvironments.", "AI": {"tldr": "PRISM is a new method for video dataset condensation that integrates static content and dynamic motion, offering better performance with reduced storage needs.", "motivation": "To address the unique challenges of video data condensation which involves both spatial content and temporal dynamics, unlike the simpler image data.", "method": "PRISM progressively refines and inserts frames to capture motion in actions while considering frame gradients, preserving the interdependence of static content and dynamic motion.", "result": "Experiments on standard video action recognition benchmarks show PRISM outperforms disentangled approaches with compact representations ideal for resource-limited settings.", "conclusion": "PRISM provides an improved approach to video dataset condensation by integrating static and dynamic elements, leading to better performance and efficient storage."}}
{"id": "2505.22566", "pdf": "https://arxiv.org/pdf/2505.22566", "abs": "https://arxiv.org/abs/2505.22566", "authors": ["Yifan Xie", "Mingyang Li", "Shoujie Li", "Xingting Li", "Guangyu Chen", "Fei Ma", "Fei Richard Yu", "Wenbo Ding"], "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains.", "AI": {"tldr": "VTV-LLM is the first multi-modal large language model for universal Visuo-Tactile Video understanding, incorporating tactile information with natural language. It introduces VTV150K dataset and a three-stage training paradigm, achieving superior performance in tactile video understanding tasks.", "motivation": "Existing approaches fail to effectively incorporate tactile information which is crucial for real-world interaction.", "method": "Developed VTV-LLM with a three-stage training paradigm including VTV enhancement, VTV-text alignment, and text prompt finetuning. Introduced VTV150K dataset with 150,000 video frames from 100 objects across three tactile sensors.", "result": "Achieves superior performance in tactile video understanding tasks, enabling sophisticated tactile reasoning capabilities.", "conclusion": "Establishes a foundation for more intuitive human-machine interaction in tactile domains."}}
{"id": "2505.22238", "pdf": "https://arxiv.org/pdf/2505.22238", "abs": "https://arxiv.org/abs/2505.22238", "authors": ["A. Ploshkin", "V. Tytskiy", "A. Pismenny", "V. Baikalov", "E. Taychinov", "A. Permiakov", "D. Burlakov", "E. Krofto", "N. Savushkin"], "title": "Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "We present Yambda-5B, a large-scale open dataset sourced from the\nYandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item\ninteractions from 1 million users across 9.39 million tracks. The dataset\nincludes two primary types of interactions: implicit feedback (listening\nevents) and explicit feedback (likes, dislikes, unlikes and undislikes). In\naddition, we provide audio embeddings for most tracks, generated by a\nconvolutional neural network trained on audio spectrograms. A key\ndistinguishing feature of Yambda-5B is the inclusion of the is_organic flag,\nwhich separates organic user actions from recommendation-driven events. This\ndistinction is critical for developing and evaluating machine learning\nalgorithms, as Yandex.Music relies on recommender systems to personalize track\nselection for users. To support rigorous benchmarking, we introduce an\nevaluation protocol based on a Global Temporal Split, allowing recommendation\nalgorithms to be assessed in conditions that closely mirror real-world use. We\nreport benchmark results for standard baselines (ItemKNN, iALS) and advanced\nmodels (SANSA, SASRec) using a variety of evaluation metrics. By releasing\nYambda-5B to the community, we aim to provide a readily accessible,\nindustrial-scale resource to advance research, foster innovation, and promote\nreproducible results in recommender systems.", "AI": {"tldr": "Yambda-5B is a large-scale dataset from Yandex.Music with 4.79 billion user-item interactions, including implicit and explicit feedback, audio embeddings, and an 'is_organic' flag to differentiate organic actions from recommendation-driven events.", "motivation": "To provide the research community with an industrial-scale resource for advancing recommender systems research, fostering innovation, and promoting reproducibility.", "method": "The dataset includes implicit (listening events) and explicit (likes/dislikes) feedback, audio embeddings generated by a CNN, and an 'is_organic' flag. An evaluation protocol based on Global Temporal Split is introduced for benchmarking recommendation algorithms.", "result": "Benchmark results are reported for standard baselines (ItemKNN, iALS) and advanced models (SANSA, SASRec) using various evaluation metrics.", "conclusion": "The release of Yambda-5B aims to support rigorous benchmarking and facilitate progress in the development of machine learning algorithms for personalized music recommendations."}}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571", "abs": "https://arxiv.org/abs/2505.22571", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.", "AI": {"tldr": "This paper presents Agent-UniRAG, a trainable agent framework for unified retrieval-augmented LLM systems that enhances effectiveness and interpretability by handling single-hop and multi-hop queries in an end-to-end manner. It also introduces SynAgent-RAG, a synthetic dataset for small open-source LLMs.", "motivation": "To address the limitation of previous works which mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, thus limiting their application to real-world scenarios.", "method": "Proposes a trainable agent framework named Agent-UniRAG for unified retrieval-augmented LLM systems that can handle both single-hop and multi-hop queries in an end-to-end manner. Also introduces SynAgent-RAG, a synthetic dataset designed for small open-source LLMs.", "result": "Achieves comparable performance with closed-source and larger open-source LLMs across various RAG benchmarks.", "conclusion": "The proposed Agent-UniRAG framework and SynAgent-RAG dataset enhance the effectiveness and interpretability of RAG systems, offering potential for further exploitation."}}
{"id": "2505.22243", "pdf": "https://arxiv.org/pdf/2505.22243", "abs": "https://arxiv.org/abs/2505.22243", "authors": ["Bin Li", "Diwei Liu", "Zehong Hu", "Jia Jia"], "title": "UDuo: Universal Dual Optimization Framework for Online Matching", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Online resource allocation under budget constraints critically depends on\nproper modeling of user arrival dynamics. Classical approaches employ\nstochastic user arrival models to derive near-optimal solutions through\nfractional matching formulations of exposed users for downstream allocation\ntasks. However, this is no longer a reasonable assumption when the environment\nchanges dynamically. In this work, We propose the Universal Dual optimization\nframework UDuo, a novel paradigm that fundamentally rethinks online allocation\nthrough three key innovations: (i) a temporal user arrival representation\nvector that explicitly captures distribution shifts in user arrival patterns\nand resource consumption dynamics, (ii) a resource pacing learner with adaptive\nallocation policies that generalize to heterogeneous constraint scenarios, and\n(iii) an online time-series forecasting approach for future user arrival\ndistributions that achieves asymptotically optimal solutions with constraint\nfeasibility guarantees in dynamic environments. Experimental results show that\nUDuo achieves higher efficiency and faster convergence than the traditional\nstochastic arrival model in real-world pricing while maintaining rigorous\ntheoretical validity for general online allocation problems.", "AI": {"tldr": "The paper proposes UDuo, a new framework for online resource allocation that adapts to dynamic environments through innovations in user arrival representation, adaptive allocation policies, and time-series forecasting. It outperforms traditional stochastic models in efficiency and convergence.", "motivation": "Existing methods for online resource allocation rely on stochastic user arrival models which are not suitable for dynamically changing environments.", "method": "UDuo incorporates three key innovations: temporal user arrival representation vector, resource pacing learner with adaptive allocation policies, and online time-series forecasting approach for future user arrivals.", "result": "UDuo demonstrates higher efficiency and faster convergence than traditional stochastic arrival models in real-world pricing scenarios while maintaining theoretical validity.", "conclusion": "UDuo provides a more effective solution for online resource allocation under dynamic conditions compared to classical approaches."}}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572", "abs": "https://arxiv.org/abs/2505.22572", "authors": ["Waldemar Chang", "Alhassan Yasin"], "title": "Fusion Steering: Prompt-Specific Activation Control", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs.", "AI": {"tldr": "Fusion Steering\u662f\u4e00\u79cd\u65b0\u7684\u6fc0\u6d3b\u8f6c\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u5165\u7279\u5b9a\u63d0\u793a\u7684\u6fc0\u6d3b\u5dee\u503c\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5206\u6bb5\u8f6c\u5411\u7b56\u7565\u5728\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u73b0\u6709\u7684\u8f6c\u5411\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u5c42\u6216\u56fa\u5b9a\u5c42\u64cd\u4f5c\uff0c\u65e0\u6cd5\u7075\u6d3b\u5730\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6fc0\u6d3b\u72b6\u6001\u4ee5\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFusion Steering\u7684\u6fc0\u6d3b\u8f6c\u5411\u65b9\u6cd5\uff0c\u5305\u62ec\u5168\u5c42\u8f6c\u5411\u548c\u5206\u6bb5\u8f6c\u5411\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u6ce8\u5165\u7279\u5b9a\u63d0\u793a\u7684\u6fc0\u6d3b\u5dee\u503c\uff08\u7531\u771f\u5b9e\u7b54\u6848\u548c\u6a21\u578b\u751f\u6210\u7684\u89e3\u91ca\u7ec4\u5408\u800c\u6210\uff09\u6765\u5b9e\u73b0\u7075\u6d3b\u7684\u8f6c\u5411\u914d\u7f6e\u3002\u4f7f\u7528Optuna\u4f18\u5316\u6bcf\u4e2a\u63d0\u793a\u7684\u6ce8\u5165\u6743\u91cd\uff0c\u76ee\u6807\u662f\u5e73\u8861\u4e8b\u5b9e\u5bf9\u9f50\u548c\u6d41\u7545\u6027\u3002", "result": "\u5728260\u4e2aSimpleQA\u63d0\u793a\u4e0a\uff0c\u5206\u6bb5\u8f6c\u5411\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u5168\u5c42\u8f6c\u5411\u7b56\u7565\uff0c\u51c6\u786e\u7387\u8fbe\u523025.4%\uff0c\u5e76\u5728\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6807\u51c6\u4e0b\u5c06\u5b8c\u5168\u6b63\u786e\u56de\u7b54\u7684\u6bd4\u4f8b\u4ece0%\u63d0\u5347\u81f313.1%\u3002", "conclusion": "\u5206\u6bb5\u3001\u52a8\u6001\u5e72\u9884\u7b56\u7565\u5c55\u793a\u4e86\u5176\u4f18\u52bf\uff0c\u6bcf\u63d0\u793a\u5168\u7f51\u7edc\u6fc0\u6d3b\u63a7\u5236\u663e\u793a\u51fa\u6f5c\u529b\u3002Fusion Steering\u8fd8\u9002\u7528\u4e8e\u7a00\u758f\u8868\u793a\uff0c\u4e3a\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u6fc0\u6d3b\u7ea7\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.22258", "pdf": "https://arxiv.org/pdf/2505.22258", "abs": "https://arxiv.org/abs/2505.22258", "authors": ["Benjamin Serfling", "Hannes Reichert", "Lorenzo Bayerlein", "Konrad Doll", "Kati Radkhah-Lens"], "title": "LiDAR Based Semantic Perception for Forklifts in Outdoor Environments", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "In this study, we present a novel LiDAR-based semantic segmentation framework\ntailored for autonomous forklifts operating in complex outdoor environments.\nCentral to our approach is the integration of a dual LiDAR system, which\ncombines forward-facing and downward-angled LiDAR sensors to enable\ncomprehensive scene understanding, specifically tailored for industrial\nmaterial handling tasks. The dual configuration improves the detection and\nsegmentation of dynamic and static obstacles with high spatial precision. Using\nhigh-resolution 3D point clouds captured from two sensors, our method employs a\nlightweight yet robust approach that segments the point clouds into\nsafety-critical instance classes such as pedestrians, vehicles, and forklifts,\nas well as environmental classes such as driveable ground, lanes, and\nbuildings. Experimental validation demonstrates that our approach achieves high\nsegmentation accuracy while satisfying strict runtime requirements,\nestablishing its viability for safety-aware, fully autonomous forklift\nnavigation in dynamic warehouse and yard environments.", "AI": {"tldr": "This paper presents a new LiDAR-based semantic segmentation framework for autonomous forklifts in complex outdoor settings, integrating a dual LiDAR system to improve obstacle detection and segmentation with high accuracy and runtime efficiency.", "motivation": "To enhance the safety and autonomy of forklifts operating in dynamic industrial environments by improving the detection and segmentation of obstacles using LiDAR technology.", "method": "The method involves a dual LiDAR system combining forward-facing and downward-angled sensors to capture high-resolution 3D point clouds. A lightweight yet robust approach is then used to segment these point clouds into safety-critical instance classes (e.g., pedestrians, vehicles) and environmental classes (e.g., ground, lanes).", "result": "Experimental results show that the approach achieves high segmentation accuracy while meeting strict runtime requirements, proving its effectiveness for autonomous forklift navigation.", "conclusion": "The presented framework successfully enables safety-aware, fully autonomous forklift navigation in complex outdoor environments, demonstrating its potential for industrial material handling tasks."}}
{"id": "2505.22581", "pdf": "https://arxiv.org/pdf/2505.22581", "abs": "https://arxiv.org/abs/2505.22581", "authors": ["Kartik Kuckreja", "Parul Gupta", "Injy Hamed", "Thamar Solorio", "Muhammad Haris Khan", "Abhinav Dhall"], "title": "Tell me Habibi, is it Real or Fake?", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 2 figures, 12 tables", "summary": "Deepfake generation methods are evolving fast, making fake media harder to\ndetect and raising serious societal concerns. Most deepfake detection and\ndataset creation research focuses on monolingual content, often overlooking the\nchallenges of multilingual and code-switched speech, where multiple languages\nare mixed within the same discourse. Code-switching, especially between Arabic\nand English, is common in the Arab world and is widely used in digital\ncommunication. This linguistic mixing poses extra challenges for deepfake\ndetection, as it can confuse models trained mostly on monolingual data. To\naddress this, we introduce \\textbf{ArEnAV}, the first large-scale\nArabic-English audio-visual deepfake dataset featuring intra-utterance\ncode-switching, dialectal variation, and monolingual Arabic content. It\n\\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our\ndataset is generated using a novel pipeline integrating four Text-To-Speech and\ntwo lip-sync models, enabling comprehensive analysis of multilingual multimodal\ndeepfake detection. We benchmark our dataset against existing monolingual and\nmultilingual datasets, state-of-the-art deepfake detection models, and a human\nevaluation, highlighting its potential to advance deepfake research. The\ndataset can be accessed\n\\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.", "AI": {"tldr": "The paper presents ArEnAV, a large-scale Arabic-English audio-visual deepfake dataset with code-switching and dialectal variation features, containing 387k videos. It aims to address the challenges in multilingual deepfake detection and provides benchmarks for advancing related research.", "motivation": "Deepfake generation methods are rapidly evolving, making fake media harder to detect and raising societal concerns. Current deepfake detection research mainly focuses on monolingual content, neglecting the complexities of multilingual and code-switched speech, especially between Arabic and English which is common in the Arab world. This creates a need for datasets that can better represent these linguistic challenges.", "method": "The authors developed ArEnAV, the first large-scale Arabic-English audio-visual deepfake dataset incorporating intra-utterance code-switching, dialectal variation, and monolingual Arabic content. The dataset contains 387k videos and over 765 hours of real and fake videos. It was created using a novel pipeline integrating four Text-To-Speech models and two lip-sync models to enable comprehensive analysis of multilingual multimodal deepfake detection.", "result": "ArEnAV has been benchmarked against existing monolingual and multilingual datasets, state-of-the-art deepfake detection models, and human evaluations. The results show its potential to advance deepfake detection research by addressing the specific challenges of multilingual and code-switched content.", "conclusion": "ArEnAV is an important contribution to the field of deepfake detection as it addresses the underrepresented area of multilingual and code-switched speech in deepfake research. By providing a large-scale dataset with diverse linguistic features, it offers new opportunities for developing more robust deepfake detection models."}}
{"id": "2505.22583", "pdf": "https://arxiv.org/pdf/2505.22583", "abs": "https://arxiv.org/abs/2505.22583", "authors": ["Tobias Lindenbauer", "Egor Bogomolov", "Yaroslav Zharov"], "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git", "categories": ["cs.SE", "cs.AI"], "comment": "Short Paper, 5 pages", "summary": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,\nhave catalyzed progress in programming capabilities of AI agents. However, they\noverlook critical developer workflows such as Version Control System (VCS)\noperations. To address this issue, we present GitGoodBench, a novel benchmark\nfor evaluating AI agent performance on VCS tasks. GitGoodBench covers three\ncore Git scenarios extracted from permissive open-source Python, Java, and\nKotlin repositories. Our benchmark provides three datasets: a comprehensive\nevaluation suite (900 samples), a rapid prototyping version (120 samples), and\na training corpus (17,469 samples). We establish baseline performance on the\nprototyping version of our benchmark using GPT-4o equipped with custom tools,\nachieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a\ncrucial stepping stone toward truly comprehensive SE agents that go beyond mere\nprogramming.", "AI": {"tldr": "The paper introduces GitGoodBench, a new benchmark for assessing AI agents' performance on Version Control System tasks. It covers three core Git scenarios and provides three datasets. Using GPT-4o, they achieved a 21.11% solve rate on the prototyping version.", "motivation": "Existing benchmarks for AI agents in Software Engineering, like SWE-bench, have promoted advancements in programming capabilities but neglect important developer workflows such as Version Control System operations.", "method": "The authors created GitGoodBench, which evaluates AI agent performance on VCS tasks by covering three core Git scenarios from open-source Python, Java, and Kotlin repositories. They provided three datasets: a full evaluation suite, a rapid prototyping version, and a training corpus.", "result": "Using GPT-4o with custom tools, they established a baseline performance on the prototyping version of GitGoodBench, achieving an overall solve rate of 21.11%.", "conclusion": "GitGoodBench is expected to be a key step towards developing comprehensive SE agents that encompass more than just programming."}}
{"id": "2505.22296", "pdf": "https://arxiv.org/pdf/2505.22296", "abs": "https://arxiv.org/abs/2505.22296", "authors": ["Haosheng Zou", "Xiaowei Lv", "Shousheng Jia", "Xiangzheng Zhang"], "title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training", "categories": ["cs.CL", "cs.LG"], "comment": "code at https://github.com/Qihoo360/360-LLaMA-Factory", "summary": "Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights.", "AI": {"tldr": "The paper introduces 360-LLaMA-Factory, an open-source project that incorporates sequence parallelism into LLaMA-Factory, which has been widely recognized and utilized in various models and training frameworks.", "motivation": "To enhance LLaMA-Factory by incorporating sequence parallelism and sharing the implementation insights with the community.", "method": "Adding sequence parallelism to LLaMA-Factory and open-sourcing it as 360-LLaMA-Factory.", "result": "360-LLaMA-Factory has gained wide recognition and is used in multiple models and large companies' training frameworks.", "conclusion": "This technical report explores different sequence parallel modes within 360-LLaMA-Factory and shares implementation insights."}}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591", "abs": "https://arxiv.org/abs/2505.22591", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization.", "AI": {"tldr": "Although large language models perform well in many areas, they face challenges in mathematical reasoning. This paper introduces Self-Error-Instruct (SEI), a framework that identifies model errors, generates generalized training data based on these errors, and uses this data to fine-tune the model, thereby improving its mathematical reasoning abilities.", "motivation": "To address the issue of large language models struggling with mathematical reasoning and their inability to generalize from isolated bad cases when learning from errors.", "method": "The SEI framework explores a target model on two mathematical datasets to find bad cases, generates error keyphrases using an instructor model, clusters these keyphrases to identify error types, synthesizes additional training data for each error type through a self-instruct approach, refines this data via one-shot learning, and uses the curated data to iteratively fine-tune the target model.", "result": "The application of the SEI framework to various models leads to improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets.", "conclusion": "Self-error instruction is effective in improving LLMs' mathematical reasoning by generalizing errors and synthesizing targeted training data."}}
{"id": "2505.22318", "pdf": "https://arxiv.org/pdf/2505.22318", "abs": "https://arxiv.org/abs/2505.22318", "authors": ["Ishwar B Balappanawar", "Vamshi Krishna Bonagiri", "Anish R Joishy", "Manas Gaur", "Krishnaprasad Thirunarayan", "Ponnurangam Kumaraguru"], "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge.", "AI": {"tldr": "Large Language Models (LLMs) show reasoning capability degradation when context conflicts with their parametric knowledge. The study introduces CounterLogic dataset and proposes Self-Segregate prompting method to improve LLMs' performance in counterfactual scenarios.", "motivation": "To investigate the phenomenon that LLMs struggle when the context conflicts with their parametric knowledge, and to find a solution to enhance their reasoning capabilities in such scenarios.", "method": "Introduced CounterLogic dataset containing 1,800 examples across 9 logical schemas to evaluate logical reasoning through counterfactual scenarios. Systematically evaluated 11 LLMs across 6 different datasets. Proposed Self-Segregate prompting method enabling metacognitive awareness before reasoning.", "result": "Consistent performance degradation of LLMs was observed with accuracies dropping by 27% on average when reasoning through counterfactual information. Self-Segregate method narrowed the average performance gaps from 27% to 11%, while significantly increasing the overall accuracy (+7.5%).", "conclusion": "The findings offer practical insights for understanding and enhancing LLMs reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge."}}
{"id": "2505.22598", "pdf": "https://arxiv.org/pdf/2505.22598", "abs": "https://arxiv.org/abs/2505.22598", "authors": ["Luca Maria Del Bono", "Federico Ricci-Tersenghi", "Francesco Zamponi"], "title": "On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.LG", "physics.comp-ph"], "comment": "16 pages, 9 figures", "summary": "Recent years have seen a rise in the application of machine learning\ntechniques to aid the simulation of hard-to-sample systems that cannot be\nstudied using traditional methods. Despite the introduction of many different\narchitectures and procedures, a wide theoretical understanding is still\nlacking, with the risk of suboptimal implementations. As a first step to\naddress this gap, we provide here a complete analytic study of the widely-used\nSequential Tempering procedure applied to a shallow MADE architecture for the\nCurie-Weiss model. The contribution of this work is twofold: firstly, we give a\ndescription of the optimal weights and of the training under Gradient Descent\noptimization. Secondly, we compare what happens in Sequential Tempering with\nand without the addition of local Metropolis Monte Carlo steps. We are thus\nable to give theoretical predictions on the best procedure to apply in this\ncase. This work establishes a clear theoretical basis for the integration of\nmachine learning techniques into Monte Carlo sampling and optimization.", "AI": {"tldr": "This paper conducts a comprehensive analytic study of the Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model, providing theoretical predictions on the best procedure and establishing a clear theoretical basis for integrating machine learning techniques into Monte Carlo sampling and optimization.", "motivation": "To address the lack of wide theoretical understanding in applying machine learning techniques to aid the simulation of hard-to-sample systems, which may lead to suboptimal implementations.", "method": "Complete analytic study of the Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model, including description of optimal weights and training under Gradient Descent optimization, and comparison with and without local Metropolis Monte Carlo steps.", "result": "Theoretical predictions on the best procedure to apply in this case are provided, contributing to a clear theoretical basis for integrating machine learning techniques into Monte Carlo sampling and optimization.", "conclusion": "This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization."}}
{"id": "2505.22326", "pdf": "https://arxiv.org/pdf/2505.22326", "abs": "https://arxiv.org/abs/2505.22326", "authors": ["James M. Adams", "Gesine Reinert", "Lukasz Szpruch", "Carsten Maple", "Andrew Elliott"], "title": "Individualised Counterfactual Examples Using Conformal Prediction Intervals", "categories": ["stat.ML", "cs.LG"], "comment": "Submitted to Conformal and Probabilistic Predictions With\n  Applications (COPA) 2025", "summary": "Counterfactual explanations for black-box models aim to pr ovide insight into\nan algorithmic decision to its recipient. For a binary classification problem\nan individual counterfactual details which features might be changed for the\nmodel to infer the opposite class. High-dimensional feature spaces that are\ntypical of machine learning classification models admit many possible\ncounterfactual examples to a decision, and so it is important to identify\nadditional criteria to select the most useful counterfactuals. In this paper,\nwe explore the idea that the counterfactuals should be maximally informative\nwhen considering the knowledge of a specific individual about the underlying\nclassifier. To quantify this information gain we explicitly model the knowledge\nof the individual, and assess the uncertainty of predictions which the\nindividual makes by the width of a conformal prediction interval. Regions of\nfeature space where the prediction interval is wide correspond to areas where\nthe confidence in decision making is low, and an additional counterfactual\nexample might be more informative to an individual. To explore and evaluate our\nindividualised conformal prediction interval counterfactuals (CPICFs), first we\npresent a synthetic data set on a hypercube which allows us to fully visualise\nthe decision boundary, conformal intervals via three different methods, and\nresultant CPICFs. Second, in this synthetic data set we explore the impact of a\nsingle CPICF on the knowledge of an individual locally around the original\nquery. Finally, in both our synthetic data set and a complex real world dataset\nwith a combination of continuous and discrete variables, we measure the utility\nof these counterfactuals via data augmentation, testing the performance on a\nheld out set.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2a\u4f53\u5316\u7f6e\u4fe1\u9884\u6d4b\u533a\u95f4\uff08CPICFs\uff09\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u4e2a\u4f53\u77e5\u8bc6\u548c\u8bc4\u4f30\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u9009\u62e9\u6700\u80fd\u63d0\u4f9b\u4fe1\u606f\u7684\u53cd\u4e8b\u5b9e\u793a\u4f8b\u3002\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u591a\u79cd\u53ef\u80fd\u6027\uff0c\u4f46\u672a\u5145\u5206\u8003\u8651\u4e2a\u4f53\u5bf9\u6a21\u578b\u51b3\u7b56\u7684\u7406\u89e3\u7a0b\u5ea6\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6700\u5927\u5316\u4e2a\u4f53\u4fe1\u606f\u83b7\u53d6\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e2a\u4f53\u7684\u77e5\u8bc6\uff0c\u5229\u7528\u7f6e\u4fe1\u9884\u6d4b\u533a\u95f4\u7684\u5bbd\u5ea6\u8861\u91cf\u4e2a\u4f53\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u4e0d\u786e\u5b9a\u6027\u9ad8\u7684\u533a\u57df\u5bfb\u627e\u66f4\u5177\u6709\u4fe1\u606f\u91cf\u7684\u53cd\u4e8b\u5b9e\u793a\u4f8b\u3002\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e2d\uff0c\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u51b3\u7b56\u8fb9\u754c\u3001\u7f6e\u4fe1\u9884\u6d4b\u533a\u95f4\u53ca\u76f8\u5e94\u7684CPICFs\uff1b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u4fdd\u7559\u96c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684CPICFs\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9009\u62e9\u6700\u5177\u6709\u4fe1\u606f\u91cf\u7684\u53cd\u4e8b\u5b9e\u793a\u4f8b\uff0c\u63d0\u5347\u4e2a\u4f53\u5bf9\u6a21\u578b\u51b3\u7b56\u7684\u7406\u89e3\u3002"}}
{"id": "2505.22332", "pdf": "https://arxiv.org/pdf/2505.22332", "abs": "https://arxiv.org/abs/2505.22332", "authors": ["Timo L\u00f6hr", "Paul Hofman", "Felix Mohr", "Eyke H\u00fcllermeier"], "title": "Credal Prediction based on Relative Likelihood", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Predictions in the form of sets of probability distributions, so-called\ncredal sets, provide a suitable means to represent a learner's epistemic\nuncertainty. In this paper, we propose a theoretically grounded approach to\ncredal prediction based on the statistical notion of relative likelihood: The\ntarget of prediction is the set of all (conditional) probability distributions\nproduced by the collection of plausible models, namely those models whose\nrelative likelihood exceeds a specified threshold. This threshold has an\nintuitive interpretation and allows for controlling the trade-off between\ncorrectness and precision of credal predictions. We tackle the problem of\napproximating credal sets defined in this way by means of suitably modified\nensemble learning techniques. To validate our approach, we illustrate its\neffectiveness by experiments on benchmark datasets demonstrating superior\nuncertainty representation without compromising predictive performance. We also\ncompare our method against several state-of-the-art baselines in credal\nprediction.", "AI": {"tldr": "The paper proposes a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood, which provides a suitable means to represent a learner's epistemic uncertainty.", "motivation": "Predictions in the form of sets of probability distributions (credal sets) are useful for representing a learner's epistemic uncertainty. There is a need for a method that can control the trade-off between correctness and precision of these predictions.", "method": "The proposed method defines the target of prediction as the set of all (conditional) probability distributions produced by plausible models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision. The authors use suitably modified ensemble learning techniques to approximate these credal sets.", "result": "The approach was validated through experiments on benchmark datasets, showing superior uncertainty representation without compromising predictive performance. It also compared favorably against several state-of-the-art baselines in credal prediction.", "conclusion": "The proposed approach to credal prediction based on relative likelihood is effective in representing uncertainty while maintaining good predictive performance."}}
{"id": "2505.22608", "pdf": "https://arxiv.org/pdf/2505.22608", "abs": "https://arxiv.org/abs/2505.22608", "authors": ["Haoning Xu", "Zhaoqing Li", "Youjun Chen", "Huimeng Wang", "Guinan Li", "Mengzhe Geng", "Chengxi Deng", "Xunying Liu"], "title": "Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "This paper presents a novel approach for speech foundation models compression\nthat tightly integrates model pruning and parameter update into a single stage.\nHighly compact layer-level tied self-pinching gates each containing only a\nsingle learnable threshold are jointly trained with uncompressed models and\nused in fine-grained neuron level pruning. Experiments conducted on the\nLibriSpeech-100hr corpus suggest that our approach reduces the number of\nparameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%\nrespectively, while incurring no statistically significant word error rate\n(WER) increase on the test-clean dataset. Compared to previously published\nmethods on the same task, our approach not only achieves the lowest WER of\n7.05% on the test-clean dataset under a comparable model compression ratio of\n4.26x, but also operates with at least 25% less model compression time.", "AI": {"tldr": "The paper introduces a new method for compressing speech foundation models by integrating model pruning and parameter update into one stage, achieving significant parameter reduction without affecting performance.", "motivation": "To develop an efficient compression technique that reduces the number of parameters in speech foundation models while maintaining or improving their performance.", "method": "Jointly train compact layer-level tied self-pinching gates with uncompressed models for fine-grained neuron level pruning, integrating model pruning and parameter update into a single stage.", "result": "Reduces parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60% respectively, with no significant WER increase. Achieves lowest WER of 7.05% under comparable compression ratio, with at least 25% less compression time.", "conclusion": "The proposed approach effectively compresses speech models while preserving performance, outperforming previous methods in both compression efficiency and speed."}}
{"id": "2505.22342", "pdf": "https://arxiv.org/pdf/2505.22342", "abs": "https://arxiv.org/abs/2505.22342", "authors": ["Shriram M S", "Xinyue Hao", "Shihao Hou", "Yang Lu", "Laura Sevilla-Lara", "Anurag Arnab", "Shreyank N Gowda"], "title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The success of the machine learning field has reliably depended on training\non large datasets. While effective, this trend comes at an extraordinary cost.\nThis is due to two deeply intertwined factors: the size of models and the size\nof datasets. While promising research efforts focus on reducing the size of\nmodels, the other half of the equation remains fairly mysterious. Indeed, it is\nsurprising that the standard approach to training remains to iterate over and\nover, uniformly sampling the training dataset. In this paper we explore a\nseries of alternative training paradigms that leverage insights from\nhard-data-mining and dropout, simple enough to implement and use that can\nbecome the new training standard. The proposed Progressive Data Dropout reduces\nthe number of effective epochs to as little as 12.4% of the baseline. This\nsavings actually do not come at any cost for accuracy. Surprisingly, the\nproposed method improves accuracy by up to 4.82%. Our approach requires no\nchanges to model architecture or optimizer, and can be applied across standard\ntraining pipelines, thus posing an excellent opportunity for wide adoption.\nCode can be found here: https://github.com/bazyagami/LearningWithRevision", "AI": {"tldr": "The paper proposes Progressive Data Dropout, a new training paradigm that reduces effective epochs to 12.4% of baseline without sacrificing accuracy and even improving it by up to 4.82%. It requires no changes to model architecture or optimizer.", "motivation": "To address the high cost associated with training large machine learning models due to the size of models and datasets, while existing research mainly focuses on reducing model size, this work explores alternative training methods to improve efficiency in dataset usage.", "method": "Progressive Data Dropout is introduced which integrates insights from hard-data-mining and dropout. This method progressively reduces the number of data samples used for training, thereby decreasing the number of effective epochs required.", "result": "Reduces the number of effective epochs to as little as 12.4% of the baseline without any loss in accuracy. In fact, it improves accuracy by up to 4.82%. The approach is easy to implement and can be applied across standard training pipelines.", "conclusion": "Progressive Data Dropout offers an efficient alternative to standard training methods without compromising accuracy, making it a promising candidate for wide adoption in various machine learning training pipelines."}}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.", "AI": {"tldr": "Image recaptioning method RICO refines captions through visual reconstruction using text-to-image models and MLLMs, reducing inaccuracies and improving caption quality. RICO-Flash reduces computational cost. Significant improvements on CapsBench and CompreCap.", "motivation": "Existing image recaptioning methods rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions but suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details.", "method": "Propose RICO, a framework that refines captions through visual reconstruction using a text-to-image model to reconstruct a caption into a reference image and prompting an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. Introduce RICO-Flash, which learns to generate captions like RICO using DPO to mitigate additional computational cost.", "result": "Extensive experiments demonstrate significant improvements in caption accuracy and completeness, outperforming most baselines by approximately 10% on both CapsBench and CompreCap.", "conclusion": "RICO and RICO-Flash effectively improve caption accuracy and completeness while addressing limitations of existing methods."}}
{"id": "2505.22364", "pdf": "https://arxiv.org/pdf/2505.22364", "abs": "https://arxiv.org/abs/2505.22364", "authors": ["Gabriele Visentin", "Patrick Cheridito"], "title": "Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows", "categories": ["stat.ML", "cs.LG", "65K99 (Primary) 68T07, 68T99 (Secondary)"], "comment": null, "summary": "We present a novel method for efficiently computing optimal transport maps\nand Wasserstein barycenters in high-dimensional spaces. Our approach uses\nconditional normalizing flows to approximate the input distributions as\ninvertible pushforward transformations from a common latent space. This makes\nit possible to directly solve the primal problem using gradient-based\nminimization of the transport cost, unlike previous methods that rely on dual\nformulations and complex adversarial optimization. We show how this approach\ncan be extended to compute Wasserstein barycenters by solving a conditional\nvariance minimization problem. A key advantage of our conditional architecture\nis that it enables the computation of barycenters for hundreds of input\ndistributions, which was computationally infeasible with previous methods. Our\nnumerical experiments illustrate that our approach yields accurate results\nacross various high-dimensional tasks and compares favorably with previous\nstate-of-the-art methods.", "AI": {"tldr": "The paper introduces a new method using conditional normalizing flows to compute optimal transport maps and Wasserstein barycenters in high dimensions, showing accuracy and efficiency.", "motivation": "To address the computational challenges of calculating optimal transport maps and Wasserstein barycenters in high-dimensional spaces, which are crucial for comparing and averaging probability distributions.", "method": "Uses conditional normalizing flows to approximate input distributions as invertible transformations from a common latent space, allowing direct minimization of transport cost via gradient-based methods. Extends this approach to compute Wasserstein barycenters through conditional variance minimization.", "result": "Achieves accurate results in various high-dimensional tasks, outperforming previous state-of-the-art methods, especially in computing barycenters for hundreds of input distributions.", "conclusion": "The proposed method provides an efficient and scalable solution for optimal transport problems and Wasserstein barycenter computation, with demonstrated success in high-dimensional scenarios."}}
{"id": "2505.22626", "pdf": "https://arxiv.org/pdf/2505.22626", "abs": "https://arxiv.org/abs/2505.22626", "authors": ["Yu Zhang", "Yuqi Xie", "Huihan Liu", "Rutav Shah", "Michael Wan", "Linxi Fan", "Yuke Zhu"], "title": "SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning advances robot capabilities by enabling the acquisition of\ndiverse behaviors from human demonstrations. However, large-scale datasets used\nfor policy training often introduce substantial variability in quality, which\ncan negatively impact performance. As a result, automatically curating datasets\nby filtering low-quality samples to improve quality becomes essential. Existing\nrobotic curation approaches rely on costly manual annotations and perform\ncuration at a coarse granularity, such as the dataset or trajectory level,\nfailing to account for the quality of individual state-action pairs. To address\nthis, we introduce SCIZOR, a self-supervised data curation framework that\nfilters out low-quality state-action pairs to improve the performance of\nimitation learning policies. SCIZOR targets two complementary sources of\nlow-quality data: suboptimal data, which hinders learning with undesirable\nactions, and redundant data, which dilutes training with repetitive patterns.\nSCIZOR leverages a self-supervised task progress predictor for suboptimal data\nto remove samples lacking task progression, and a deduplication module\noperating on joint state-action representation for samples with redundant\npatterns. Empirically, we show that SCIZOR enables imitation learning policies\nto achieve higher performance with less data, yielding an average improvement\nof 15.4% across multiple benchmarks. More information is available at:\nhttps://ut-austin-rpl.github.io/SCIZOR/", "AI": {"tldr": "SCIZOR is a self-supervised data curation framework that filters out low-quality state-action pairs to enhance imitation learning performance.", "motivation": "Imitation learning in robotics faces challenges due to variability in dataset quality, necessitating methods to filter low-quality samples and improve policy training outcomes.", "method": "SCIZOR uses a self-supervised task progress predictor to identify and remove suboptimal data lacking task progression, and employs a deduplication module to eliminate redundant data patterns based on joint state-action representations.", "result": "Empirical results demonstrate that SCIZOR improves imitation learning policy performance by an average of 15.4% across multiple benchmarks while using less data.", "conclusion": "SCIZOR effectively curates datasets at the state-action pair level, enhancing the quality and efficiency of imitation learning policies."}}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence.", "AI": {"tldr": "This paper presents SKG2Data, a multimodal synthesis approach using spatial knowledge graphs to improve the spatial perception and reasoning abilities of MLLMs.", "motivation": "Multimodal large language models have advanced significantly, but their spatial perception abilities remain limited. There is a need for synthesized data that adheres to spatial common sense to address this limitation.", "method": "The paper introduces SKG2Data, which constructs a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances. This graph is used to guide the synthesis of multimodal data.", "result": "Experiments show that the synthesized data enhances the spatial perception and reasoning abilities of MLLMs and exhibits strong generalization capabilities.", "conclusion": "The authors hope that the concept of knowledge-based data synthesis will contribute to the advancement of spatial intelligence in MLLMs."}}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635", "abs": "https://arxiv.org/abs/2505.22635", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "title": "Learning Composable Chains-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget.", "AI": {"tldr": "\u5728\u6559\u6388\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u901a\u5e38\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6570\u636e\u8bad\u7ec3\uff0c\u4f46\u8fd9\u4e9b\u6807\u6ce8\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3002\u4e3a\u4e86\u5b9e\u73b0\u8d85\u8d8a\u8bad\u7ec3\u5206\u5e03\u7684\u7ec4\u5408\u6cdb\u5316\uff0c\u7814\u7a76\u53d1\u73b0\u901a\u8fc7\u4fee\u6539\u539f\u5b50\u4efb\u52a1\u7684CoT\u683c\u5f0f\u4e3a\u53ef\u7ec4\u5408\u5f62\u5f0f\uff0c\u5e76\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u6216\u6a21\u578b\u5408\u5e76\uff0c\u53ef\u4ee5\u63d0\u9ad8\u96f6\u6837\u672c\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u5730\uff0c\u901a\u8fc7\u5c11\u91cf\u7ec4\u5408\u6570\u636e\u7684\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\uff08RFT\uff09\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7ed9\u5b9a\u8bad\u7ec3\u6570\u636e\u9884\u7b97\u4e0b\uff0c\u4f7f\u7528\u53ef\u7ec4\u5408CoT\u8bad\u7ec3LLMs\u4f18\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u6301\u7eed\u5fae\u8c03\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u6559\u6388\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u95ee\u9898\u7684\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u6602\u8d35\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u672a\u89c1\u7684\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u7ec4\u5408\u539f\u5b50\u63a8\u7406\u6280\u80fd\u89e3\u51b3\u66f4\u590d\u6742\u7684\u3001\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u3002", "method": "1. \u4f7f\u7528\u53ef\u7ec4\u5408\u7684CoT\u683c\u5f0f\u5bf9\u539f\u5b50\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u4fc3\u8fdb\u7ec4\u5408\u6cdb\u5316\u3002\n2. \u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6216\u6a21\u578b\u5408\u5e76\u5c06\u539f\u5b50CoT\u6a21\u578b\u7ec4\u5408\u8d77\u6765\u3002\n3. \u4f7f\u7528\u5c11\u91cf\u7ec4\u5408\u6570\u636e\u8fdb\u884c\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\uff08RFT\uff09\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u5b57\u7b26\u4e32\u64cd\u4f5c\u548c\u81ea\u7136\u8bed\u8a00\u6280\u80fd\u7ec4\u5408\u4efb\u52a1\u4e0a\u7684\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u53ef\u7ec4\u5408CoT\u8bad\u7ec3LLMs\u7684\u6027\u80fd\u4f18\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u6301\u7eed\u5fae\u8c03\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u7ed9\u5b9a\u8bad\u7ec3\u6570\u636e\u9884\u7b97\u4e0b\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u53ef\u7ec4\u5408\u7684CoT\u683c\u5f0f\u3001\u591a\u4efb\u52a1\u5b66\u4e60/\u6a21\u578b\u5408\u5e76\u4ee5\u53ca\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u8fc7\u591a\u6570\u636e\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u6027\u80fd\u3002"}}
{"id": "2505.22454", "pdf": "https://arxiv.org/pdf/2505.22454", "abs": "https://arxiv.org/abs/2505.22454", "authors": ["Mark Danza", "Sonia Lopez Alarcon", "Cory Merkel"], "title": "Depth-Based Matrix Classification for the HHL Quantum Algorithm", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Under the nearing error-corrected era of quantum computing, it is necessary\nto understand the suitability of certain post-NISQ algorithms for practical\nproblems. One of the most promising, applicable and yet difficult to implement\nin practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear\nsystems of equations. An enormous number of problems can be expressed as linear\nsystems of equations, from Machine Learning to fluid dynamics. However, in most\ncases, HHL will not be able to provide a practical, reasonable solution to\nthese problems. This paper's goal inquires about whether problems can be\nlabeled using Machine Learning classifiers as suitable or unsuitable for HHL\nimplementation when some numerical information about the problem is known\nbeforehand. This work demonstrates that training on significantly\nrepresentative data distributions is critical to achieve good classifications\nof the problems based on the numerical properties of the matrix representing\nthe system of equations. Accurate classification is possible through\nMulti-Layer Perceptrons, although with careful design of the training data\ndistribution and classifier parameters.", "AI": {"tldr": "\u5728\u91cf\u5b50\u8ba1\u7b97\u7ea0\u9519\u65f6\u4ee3\u5373\u5c06\u6765\u4e34\u4e4b\u9645\uff0c\u7406\u89e3\u67d0\u4e9b\u540eNISQ\u7b97\u6cd5\u5bf9\u5b9e\u9645\u95ee\u9898\u7684\u9002\u7528\u6027\u662f\u5f88\u5fc5\u8981\u7684\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u662f\u5426\u53ef\u4ee5\u5229\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5728\u5df2\u77e5\u4e00\u4e9b\u6570\u503c\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u95ee\u9898\u6807\u8bb0\u4e3a\u9002\u5408\u6216\u4e0d\u9002\u5408HHL\u7b97\u6cd5\u5b9e\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u65b9\u7a0b\u7ec4\u77e9\u9635\u7684\u6570\u503c\u7279\u6027\u8fdb\u884c\u95ee\u9898\u5206\u7c7b\u65f6\uff0c\u8bad\u7ec3\u5177\u6709\u663e\u8457\u4ee3\u8868\u6027\u7684\u6570\u636e\u5206\u5e03\u662f\u5173\u952e\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u5206\u7c7b\u5668\u53c2\u6570\uff0c\u591a\u5c42\u611f\u77e5\u673a\u53ef\u4ee5\u5b9e\u73b0\u7cbe\u786e\u5206\u7c7b\u3002", "motivation": "\u4e86\u89e3\u540eNISQ\u7b97\u6cd5\uff08\u5982HHL\u7b97\u6cd5\uff09\u5bf9\u4e8e\u5b9e\u9645\u95ee\u9898\u7684\u9002\u7528\u6027\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u8bb8\u591a\u95ee\u9898\u90fd\u53ef\u4ee5\u8868\u793a\u4e3a\u7ebf\u6027\u65b9\u7a0b\u7ec4\u7684\u60c5\u51b5\u4e0b\u3002\u7136\u800c\uff0cHHL\u7b97\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u65e0\u6cd5\u63d0\u4f9b\u5b9e\u9645\u3001\u5408\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002\u56e0\u6b64\u9700\u8981\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\u6765\u5224\u65ad\u54ea\u4e9b\u95ee\u9898\u9002\u5408\u4f7f\u7528HHL\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u6839\u636e\u5df2\u77e5\u7684\u6570\u503c\u4fe1\u606f\uff0c\u5c06\u95ee\u9898\u6807\u8bb0\u4e3a\u9002\u5408\u6216\u4e0d\u9002\u5408HHL\u7b97\u6cd5\u5b9e\u73b0\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u5206\u7c7b\u5668\u53c2\u6570\u7684\u8bbe\u8ba1\u5bf9\u5206\u7c7b\u6548\u679c\u7684\u5f71\u54cd\u3002\u91c7\u7528\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u4f5c\u4e3a\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\u5fc5\u987b\u5177\u6709\u663e\u8457\u7684\u4ee3\u8868\u6027\u624d\u80fd\u83b7\u5f97\u826f\u597d\u7684\u5206\u7c7b\u6548\u679c\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u548c\u5206\u7c7b\u5668\u53c2\u6570\uff0c\u591a\u5c42\u611f\u77e5\u673a\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u7684\u95ee\u9898\u5206\u7c7b\u3002", "conclusion": "\u57fa\u4e8e\u65b9\u7a0b\u7ec4\u77e9\u9635\u7684\u6570\u503c\u7279\u6027\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u53ef\u4ee5\u6709\u6548\u5224\u65ad\u95ee\u9898\u662f\u5426\u9002\u5408\u4f7f\u7528HHL\u7b97\u6cd5\u89e3\u51b3\uff0c\u4f46\u9700\u8981\u786e\u4fdd\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u4ee3\u8868\u6027\u548c\u5206\u7c7b\u5668\u53c2\u6570\u7684\u5408\u7406\u8bbe\u8ba1\u3002"}}
{"id": "2505.22642", "pdf": "https://arxiv.org/pdf/2505.22642", "abs": "https://arxiv.org/abs/2505.22642", "authors": ["Younggyo Seo", "Carmelo Sferrazza", "Haoran Geng", "Michal Nauman", "Zhao-Heng Yin", "Pieter Abbeel"], "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project webpage: https://younggyo.me/fast_td3", "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but\nits complexity and long training times remain major bottlenecks. In this\nreport, we introduce FastTD3, a simple, fast, and capable RL algorithm that\nsignificantly speeds up training for humanoid robots in popular suites such as\nHumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably\nsimple: we train an off-policy TD3 agent with several modifications -- parallel\nsimulation, large-batch updates, a distributional critic, and carefully tuned\nhyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours\non a single A100 GPU, while remaining stable during training. We also provide a\nlightweight and easy-to-use implementation of FastTD3 to accelerate RL research\nin robotics.", "AI": {"tldr": "FastTD3 is a new RL algorithm that accelerates training for humanoid robots.", "motivation": "Reinforcement learning's complexity and long training times are major bottlenecks in robotics.", "method": "Train an off-policy TD3 agent with modifications including parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters.", "result": "FastTD3 solves HumanoidBench tasks in under 3 hours on a single A100 GPU while remaining stable during training.", "conclusion": "A lightweight and easy-to-use implementation of FastTD3 is provided to accelerate RL research in robotics."}}
{"id": "2505.22649", "pdf": "https://arxiv.org/pdf/2505.22649", "abs": "https://arxiv.org/abs/2505.22649", "authors": ["Guoxuan Chen", "Lianghao Xia", "Chao Huang"], "title": "Pre-training for Recommendation Unlearning", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "Accepted to SIGIR 2025 Oral", "summary": "Modern recommender systems powered by Graph Neural Networks (GNNs) excel at\nmodeling complex user-item interactions, yet increasingly face scenarios\nrequiring selective forgetting of training data. Beyond user requests to remove\nspecific interactions due to privacy concerns or preference changes, regulatory\nframeworks mandate recommender systems' ability to eliminate the influence of\ncertain user data from models. This recommendation unlearning challenge\npresents unique difficulties as removing connections within interaction graphs\ncreates ripple effects throughout the model, potentially impacting\nrecommendations for numerous users. Traditional approaches suffer from\nsignificant drawbacks: fragmentation methods damage graph structure and\ndiminish performance, while influence function techniques make assumptions that\nmay not hold in complex GNNs, particularly with self-supervised or random\narchitectures. To address these limitations, we propose a novel model-agnostic\npre-training paradigm UnlearnRec that prepares systems for efficient unlearning\noperations. Our Influence Encoder takes unlearning requests together with\nexisting model parameters and directly produces updated parameters of unlearned\nmodel with little fine-tuning, avoiding complete retraining while preserving\nmodel performance characteristics. Extensive evaluation on public benchmarks\ndemonstrates that our method delivers exceptional unlearning effectiveness\nwhile providing more than 10x speedup compared to retraining approaches. We\nrelease our method implementation at: https://github.com/HKUDS/UnlearnRec.", "AI": {"tldr": "The paper proposes UnlearnRec, a model-agnostic pre-training paradigm that enables efficient unlearning operations in GNN-based recommender systems, providing over 10x speedup compared to retraining while preserving performance.", "motivation": "Modern GNN-powered recommender systems need to handle selective forgetting of training data due to privacy concerns, preference changes, and regulatory requirements. Traditional methods for unlearning either damage graph structure or rely on assumptions that may not hold in complex GNNs.", "method": "The authors propose UnlearnRec, which includes an Influence Encoder that takes unlearning requests and existing model parameters to directly produce updated parameters with minimal fine-tuning, avoiding complete retraining.", "result": "Extensive evaluation on public benchmarks shows that UnlearnRec achieves exceptional unlearning effectiveness while providing more than 10x speedup compared to retraining approaches.", "conclusion": "UnlearnRec addresses the limitations of traditional unlearning methods by offering an efficient, model-agnostic solution for GNN-based recommender systems, maintaining performance while enabling quick unlearning."}}
{"id": "2505.22469", "pdf": "https://arxiv.org/pdf/2505.22469", "abs": "https://arxiv.org/abs/2505.22469", "authors": ["Mohamed R. Elshamy", "Mehdi Elahi", "Ahmad Patooghy", "Abdel-Hameed A. Badawy"], "title": "CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs", "categories": ["cs.PF", "cs.LG"], "comment": null, "summary": "Efficient thermal and power management in modern multiprocessor\nsystems-on-chip (MPSoCs) demands accurate power consumption estimation. One of\nthe state-of-the-art approaches, Alternative Blind Power Identification (ABPI),\ntheoretically eliminates the dependence on steady-state temperatures,\naddressing a major shortcoming of previous approaches. However, ABPI\nperformance has remained unverified in actual hardware implementations. In this\nstudy, we conduct the first empirical validation of ABPI on commercial hardware\nusing the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while\nABPI provides computational efficiency and independence from steady-state\ntemperature, it exhibits considerable accuracy deficiencies in real-world\nscenarios. To overcome these limitations, we introduce a novel approach that\nintegrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying\nthermal model of ABPI. Our approach employs a specialized loss function that\nharmonizes physical principles with data-driven learning, complemented by\nmulti-objective genetic algorithm optimization to balance estimation accuracy\nand computational cost. In experimental validation, CPINN-ABPI achieves a\nreduction of 84.7\\% CPU and 73.9\\% GPU in the mean absolute error (MAE)\nrelative to ABPI, with the weighted mean absolute percentage error (WMAPE)\nimproving from 47\\%--81\\% to $\\sim$12\\%. The method maintains real-time\nperformance with 195.3~$\\mu$s of inference time, with similar 85\\%--99\\%\naccuracy gains across heterogeneous SoCs.", "AI": {"tldr": "The paper conducts the first empirical validation of ABPI on commercial hardware, finding accuracy deficiencies in real-world scenarios. It proposes CPINN-ABPI, integrating physics-informed neural networks with ABPI's thermal model to significantly improve power consumption estimation accuracy while maintaining real-time performance.", "motivation": "Efficient thermal and power management in MPSoCs requires accurate power consumption estimation. While ABPI theoretically eliminates dependence on steady-state temperatures, its actual performance has not been verified on real hardware.", "method": "Empirical validation of ABPI on NVIDIA Jetson Xavier AGX platform revealed accuracy issues. To address this, a novel approach combining Custom Physics-Informed Neural Networks (CPINNs) with ABPI's thermal model was introduced. This includes a specialized loss function and multi-objective genetic algorithm optimization.", "result": "CPINN-ABPI reduces mean absolute error by 84.7% for CPU and 73.9% for GPU compared to ABPI. Weighted mean absolute percentage error improves from 47%-81% to ~12%. The method maintains real-time performance with inference time of 195.3 \u03bcs and shows similar accuracy gains across different SoCs.", "conclusion": "CPINN-ABPI significantly improves power consumption estimation accuracy over ABPI while maintaining computational efficiency, making it suitable for real-time thermal and power management in modern MPSoCs."}}
{"id": "2505.22481", "pdf": "https://arxiv.org/pdf/2505.22481", "abs": "https://arxiv.org/abs/2505.22481", "authors": ["Yiming Xi", "Konstantinos Zygalakis", "Marcelo Pereyra"], "title": "Hypothesis Testing in Imaging Inverse Problems", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper proposes a framework for semantic hypothesis testing tailored to\nimaging inverse problems. Modern imaging methods struggle to support hypothesis\ntesting, a core component of the scientific method that is essential for the\nrigorous interpretation of experiments and robust interfacing with\ndecision-making processes. There are three main reasons why image-based\nhypothesis testing is challenging. First, the difficulty of using a single\nobservation to simultaneously reconstruct an image, formulate hypotheses, and\nquantify their statistical significance. Second, the hypotheses encountered in\nimaging are mostly of semantic nature, rather than quantitative statements\nabout pixel values. Third, it is challenging to control test error\nprobabilities because the null and alternative distributions are often unknown.\nOur proposed approach addresses these difficulties by leveraging concepts from\nself-supervised computational imaging, vision-language models, and\nnon-parametric hypothesis testing with e-values. We demonstrate our proposed\nframework through numerical experiments related to image-based phenotyping,\nwhere we achieve excellent power while robustly controlling Type I errors.", "AI": {"tldr": "This paper proposes a framework for semantic hypothesis testing in imaging inverse problems, using self-supervised computational imaging, vision-language models, and non-parametric hypothesis testing with e-values to achieve excellent power while controlling Type I errors.", "motivation": "Modern imaging methods lack the ability to support hypothesis testing which is crucial for scientific interpretation and decision-making processes. Challenges include reconstructing an image from a single observation, dealing with mostly semantic hypotheses rather than quantitative statements about pixel values, and controlling test error probabilities due to unknown null and alternative distributions.", "method": "The approach leverages concepts from self-supervised computational imaging, vision-language models, and non-parametric hypothesis testing with e-values to address the difficulties in image-based hypothesis testing.", "result": "Through numerical experiments related to image-based phenotyping, the proposed framework achieves excellent power while robustly controlling Type I errors.", "conclusion": "The proposed framework successfully tackles the challenges of semantic hypothesis testing in imaging inverse problems."}}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks.", "AI": {"tldr": "The paper presents 3DMem-Bench, a benchmark for evaluating long-term memory in 3D environments, and proposes 3DLLM-Mem, a model enabling efficient spatial-temporal reasoning and actions in LLMs.", "motivation": "Humans effectively use long-term memory across temporal and spatial experiences to perform complex tasks, whereas current LLMs have difficulty planning and acting in dynamic, multi-room 3D environments due to the lack of proper 3D spatial-temporal memory modeling.", "method": "Introduced 3DMem-Bench, a comprehensive benchmark with over 26,000 trajectories and 2,892 embodied tasks for evaluation. Proposed 3DLLM-Mem, a dynamic memory management and fusion model using working memory tokens as queries to selectively attend to and fuse spatial and temporal features from episodic memory.", "result": "3DLLM-Mem achieves state-of-the-art performance across various tasks, surpassing the strongest baselines by 16.5% in success rate on the most challenging in-the-wild embodied tasks in 3DMem-Bench.", "conclusion": "3DLLM-Mem enables agents to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments."}}
{"id": "2505.22502", "pdf": "https://arxiv.org/pdf/2505.22502", "abs": "https://arxiv.org/abs/2505.22502", "authors": ["Dominic Lowe", "M. S. Kim", "Roberto Bondesan"], "title": "Assessing Quantum Advantage for Gaussian Process Regression", "categories": ["quant-ph", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "Gaussian Process Regression is a well-known machine learning technique for\nwhich several quantum algorithms have been proposed. We show here that in a\nwide range of scenarios these algorithms show no exponential speedup. We\nachieve this by rigorously proving that the condition number of a kernel matrix\nscales at least linearly with the matrix size under general assumptions on the\ndata and kernel. We additionally prove that the sparsity and Frobenius norm of\na kernel matrix scale linearly under similar assumptions. The implications for\nthe quantum algorithms runtime are independent of the complexity of loading\nclassical data on a quantum computer and also apply to dequantised algorithms.\nWe supplement our theoretical analysis with numerical verification for popular\nkernels in machine learning.", "AI": {"tldr": "In this paper, the authors demonstrate that quantum algorithms for Gaussian Process Regression lack exponential speedup in a wide range of scenarios by proving the linear scaling properties of kernel matrices.", "motivation": "The motivation of this paper is to evaluate the effectiveness of quantum algorithms applied to Gaussian Process Regression, specifically focusing on whether these algorithms offer exponential speedups over classical methods.", "method": "The authors rigorously prove that the condition number, sparsity, and Frobenius norm of a kernel matrix scale at least linearly with the matrix size under general assumptions. They also conduct numerical verification using popular kernels from machine learning.", "result": "The results indicate that there is no exponential speedup in the runtime of quantum algorithms for Gaussian Process Regression, independent of the complexity of loading classical data onto a quantum computer. These findings also apply to dequantised algorithms.", "conclusion": "Quantum algorithms for Gaussian Process Regression do not provide exponential speedups in most practical scenarios due to the linear scaling properties of kernel matrices."}}
{"id": "2505.22518", "pdf": "https://arxiv.org/pdf/2505.22518", "abs": "https://arxiv.org/abs/2505.22518", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "title": "IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62F10, 68T07, 62-08"], "comment": "Under review", "summary": "Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications.", "AI": {"tldr": "The paper presents IGNIS Network, a neural framework that estimates parameters for Archimedean copulas more accurately and universally than traditional methods.", "motivation": "Parameter estimation for complex Archimedean copulas (like A1 and A2 families) is challenging due to non-monotonic relationships with dependency measures and numerical instability.", "method": "IGNIS Network, a unified neural framework, learns direct mapping from observable dependency measures to copula parameters. It is trained on simulated data covering five Archimedean copula families and enforces parameter constraints via theory-guided post-processing.", "result": "IGNIS Network reduces estimation errors compared to Method of Moments and demonstrates practical utility through validation on real-world datasets across various domains.", "conclusion": "IGNIS Network offers a transformative approach for robust and accurate dependence modeling in modern applications."}}
{"id": "2505.22527", "pdf": "https://arxiv.org/pdf/2505.22527", "abs": "https://arxiv.org/abs/2505.22527", "authors": ["Agnideep Aich", "Ashit Aich", "Bruce Wade"], "title": "Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling", "categories": ["stat.ML", "cs.LG", "68T07, 37J39, 65P10, 62B10, 53D22, 94A17"], "comment": "Submitted", "summary": "We introduce the Symplectic Generative Network (SGN), a deep generative model\nthat leverages Hamiltonian mechanics to construct an invertible,\nvolume-preserving mapping between a latent space and the data space. By\nendowing the latent space with a symplectic structure and modeling data\ngeneration as the time evolution of a Hamiltonian system, SGN achieves exact\nlikelihood evaluation without incurring the computational overhead of Jacobian\ndeterminant calculations. In this work, we provide a rigorous mathematical\nfoundation for SGNs through a comprehensive theoretical framework that\nincludes: (i) complete proofs of invertibility and volume preservation, (ii) a\nformal complexity analysis with theoretical comparisons to Variational\nAutoencoders and Normalizing Flows, (iii) strengthened universal approximation\nresults with quantitative error bounds, (iv) an information-theoretic analysis\nbased on the geometry of statistical manifolds, and (v) an extensive stability\nanalysis with adaptive integration guarantees. These contributions highlight\nthe fundamental advantages of SGNs and establish a solid foundation for future\nempirical investigations and applications to complex, high-dimensional data.", "AI": {"tldr": "The paper introduces Symplectic Generative Network (SGN), a deep generative model using Hamiltonian mechanics for invertible, volume-preserving mapping. It provides proofs, complexity analysis, universal approximation results, information-theoretic analysis, and stability analysis to highlight SGN's advantages.", "motivation": "To create a deep generative model that can construct an invertible, volume-preserving mapping between latent and data spaces without the computational overhead of Jacobian determinant calculations by leveraging Hamiltonian mechanics.", "method": "Introduce SGN which endows latent space with a symplectic structure and models data generation as time evolution of a Hamiltonian system. Provide theoretical framework including proofs, complexity analysis, universal approximation results, information-theoretic analysis, and stability analysis.", "result": "Achieves exact likelihood evaluation without Jacobian determinant calculations. Establishes fundamental advantages of SGNs through comprehensive theoretical analyses.", "conclusion": "SGNs provide a solid foundation for future empirical investigations and applications to complex, high-dimensional data."}}
{"id": "2505.22535", "pdf": "https://arxiv.org/pdf/2505.22535", "abs": "https://arxiv.org/abs/2505.22535", "authors": ["Mohamad Hakam Shams Eddin", "Yikui Zhang", "Stefan Kollet", "Juergen Gall"], "title": "RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting", "categories": ["cs.CV", "cs.LG"], "comment": "Main paper 10 pages, Appendix 53 pages", "summary": "Recent deep learning approaches for river discharge forecasting have improved\nthe accuracy and efficiency in flood forecasting, enabling more reliable early\nwarning systems for risk management. Nevertheless, existing deep learning\napproaches in hydrology remain largely confined to local-scale applications and\ndo not leverage the inherent spatial connections of bodies of water. Thus,\nthere is a strong need for new deep learning methodologies that are capable of\nmodeling spatio-temporal relations to improve river discharge and flood\nforecasting for scientific and operational applications. To address this, we\npresent RiverMamba, a novel deep learning model that is pretrained with\nlong-term reanalysis data and that can forecast global river discharge and\nfloods on a $0.05^\\circ$ grid up to 7 days lead time, which is of high\nrelevance in early warning. To achieve this, RiverMamba leverages efficient\nMamba blocks that enable the model to capture global-scale channel network\nrouting and enhance its forecast capability for longer lead times. The forecast\nblocks integrate ECMWF HRES meteorological forecasts, while accounting for\ntheir inaccuracies through spatio-temporal modeling. Our analysis demonstrates\nthat RiverMamba delivers reliable predictions of river discharge, including\nextreme floods across return periods and lead times, surpassing both\noperational AI- and physics-based models.", "AI": {"tldr": "The paper introduces RiverMamba, a new deep learning model pretrained with long-term reanalysis data to forecast global river discharge and floods up to 7 days in advance. It uses Mamba blocks for capturing channel network routing and integrates ECMWF HRES forecasts while addressing inaccuracies through spatio-temporal modeling. The model outperforms both operational AI- and physics-based models.", "motivation": "Existing deep learning approaches for hydrology are mostly local-scale and do not leverage the spatial connections of water bodies. There is a need for methodologies capable of modeling spatio-temporal relations for better river discharge and flood forecasting.", "method": "RiverMamba leverages Mamba blocks to capture global-scale channel network routing and enhance forecast capability for longer lead times. Forecast blocks integrate ECMWF HRES meteorological forecasts while accounting for inaccuracies through spatio-temporal modeling.", "result": "RiverMamba delivers reliable predictions of river discharge, including extreme floods across return periods and lead times, surpassing both operational AI- and physics-based models.", "conclusion": "RiverMamba presents a significant advancement in global river discharge and flood forecasting by effectively modeling spatio-temporal relations and leveraging long-term reanalysis data."}}
{"id": "2505.22554", "pdf": "https://arxiv.org/pdf/2505.22554", "abs": "https://arxiv.org/abs/2505.22554", "authors": ["Agnideep Aich", "Md Monzur Murshed", "Amanda Mayeaux", "Sameera Hewage"], "title": "Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 62P10, 68T07"], "comment": "Submitted", "summary": "Accurate diabetes risk prediction relies on identifying key features from\ncomplex health datasets, but conventional methods like mutual information (MI)\nfilters and genetic algorithms (GAs) often overlook extreme dependencies\ncritical for high-risk subpopulations. In this study we introduce a\nfeature-selection framework using the upper-tail dependence coefficient\n({\\lambda}U) of the novel A2 copula, which quantifies how often extreme higher\nvalues of a predictor co-occur with diabetes diagnoses (target variable).\nApplied to the CDC Diabetes Health Indicators dataset (n=253,680), our method\nprioritizes five predictors (self-reported general health, high blood pressure,\nbody mass index, mobility limitations, and high cholesterol levels) based on\nupper tail dependencies. These features match or outperform MI and GA selected\nsubsets across four classifiers (Random Forest, XGBoost, Logistic Regression,\nGradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to\n0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation\nimportance confirms clinical relevance, with BMI and general health driving\naccuracy. To our knowledge, this is the first work to apply a copula's\nupper-tail dependence for supervised feature selection, bridging extreme-value\ntheory and machine learning to deliver a practical toolkit for diabetes\nprevention.", "AI": {"tldr": "This paper presents a new feature-selection framework using the upper-tail dependence coefficient (\u03bbU) of the A2 copula for accurate diabetes risk prediction. Applied to CDC dataset, it prioritizes five predictors based on upper tail dependencies that match or outperform conventional methods across four classifiers with accuracy up to 86.5% and AUC up to 0.806, rivaling the full-feature model.", "motivation": "Conventional methods like mutual information filters and genetic algorithms often overlook extreme dependencies critical for high-risk subpopulations in health datasets.", "method": "The study introduces a feature-selection framework using the upper-tail dependence coefficient (\u03bbU) of the novel A2 copula, which quantifies how often extreme higher values of a predictor co-occur with diabetes diagnoses.", "result": "The method prioritizes five predictors (self-reported general health, high blood pressure, body mass index, mobility limitations, and high cholesterol levels) based on upper tail dependencies. These features achieve accuracy up to 86.5% (XGBoost) and AUC up to 0.806 (Gradient Boosting), rivaling the full 21-feature model.", "conclusion": "This is the first work to apply a copula's upper-tail dependence for supervised feature selection, bridging extreme-value theory and machine learning for practical diabetes prevention."}}
{"id": "2505.22609", "pdf": "https://arxiv.org/pdf/2505.22609", "abs": "https://arxiv.org/abs/2505.22609", "authors": ["Alanna Hazlett", "Naomi Ohashi", "Timothy Rodriguez", "Sodiq Adewole"], "title": "Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In this work, we investigate the performance across multiple classification\nmodels to classify chest X-ray images into four categories of COVID-19,\npneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning\ntechniques with state-of-the-art pre-trained Convolutional Neural Networks\n(CNNs) models. We fine-tuned these pre-trained architectures on a labeled\nmedical x-ray images. The initial results are promising with high accuracy and\nstrong performance in key classification metrics such as precision, recall, and\nF1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for\nmodel interpretability to provide visual explanations for classification\ndecisions, improving trust and transparency in clinical applications.", "AI": {"tldr": "This paper explores the use of transfer learning with pre-trained CNNs to classify chest X-ray images into four categories (COVID-19, pneumonia, tuberculosis, and normal) achieving promising results in accuracy and key classification metrics. Grad-CAM is used for model interpretability.", "motivation": "To effectively classify chest X-ray images into specific health condition categories using advanced machine learning techniques, enhancing diagnostic capabilities in clinical settings.", "method": "Employed transfer learning with state-of-the-art pre-trained CNN models, fine-tuned on labeled medical X-ray images. Used Gradient-weighted Class Activation Mapping (Grad-CAM) for interpreting model decisions.", "result": "The models demonstrated high accuracy and strong performance across key classification metrics like precision, recall, and F1 score.", "conclusion": "Transfer learning with pre-trained CNNs shows great potential in classifying chest X-rays for different diseases, and Grad-CAM enhances the interpretability and trustworthiness of these models."}}
{"id": "2505.22622", "pdf": "https://arxiv.org/pdf/2505.22622", "abs": "https://arxiv.org/abs/2505.22622", "authors": ["Jiawei Ge", "Amanda Wang", "Shange Tang", "Chi Jin"], "title": "Principled Out-of-Distribution Generalization via Simplicity", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Modern foundation models exhibit remarkable out-of-distribution (OOD)\ngeneralization, solving tasks far beyond the support of their training data.\nHowever, the theoretical principles underpinning this phenomenon remain\nelusive. This paper investigates this problem by examining the compositional\ngeneralization abilities of diffusion models in image generation. Our analysis\nreveals that while neural network architectures are expressive enough to\nrepresent a wide range of models -- including many with undesirable behavior on\nOOD inputs -- the true, generalizable model that aligns with human expectations\ntypically corresponds to the simplest among those consistent with the training\ndata.\n  Motivated by this observation, we develop a theoretical framework for OOD\ngeneralization via simplicity, quantified using a predefined simplicity metric.\nWe analyze two key regimes: (1) the constant-gap setting, where the true model\nis strictly simpler than all spurious alternatives by a fixed gap, and (2) the\nvanishing-gap setting, where the fixed gap is replaced by a smoothness\ncondition ensuring that models close in simplicity to the true model yield\nsimilar predictions. For both regimes, we study the regularized maximum\nlikelihood estimator and establish the first sharp sample complexity guarantees\nfor learning the true, generalizable, simple model.", "AI": {"tldr": "\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u672c\u6587\u901a\u8fc7\u7814\u7a76\u6269\u6563\u6a21\u578b\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7b80\u5355\u6027\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u4e3a\u5b66\u4e60\u771f\u6b63\u53ef\u6cdb\u5316\u7684\u7b80\u5355\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cbe\u786e\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u80cc\u540e\u7684\u7406\u8bba\u539f\u7406\u5c1a\u4e0d\u660e\u786e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6df1\u5165\u63a2\u8ba8\u6a21\u578b\u5982\u4f55\u5b9e\u73b0\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u652f\u6301\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u7684\u771f\u6b63\u53ef\u6cdb\u5316\u6a21\u578b\u901a\u5e38\u662f\u4e0e\u8bad\u7ec3\u6570\u636e\u4e00\u81f4\u7684\u6700\u7b80\u5355\u6a21\u578b\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u7b80\u5355\u6027\u4e3a\u6838\u5fc3\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u7b80\u5355\u6027\u5ea6\u91cf\u6765\u91cf\u5316OOD\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u7814\u7a76\u4e86\u4e24\u79cd\u5173\u952e\u573a\u666f\uff1a\u6052\u5b9a\u5dee\u8ddd\u8bbe\u7f6e\u548c\u6d88\u5931\u5dee\u8ddd\u8bbe\u7f6e\u3002", "result": "\u5728\u4e24\u79cd\u4e0d\u540c\u7684\u8bbe\u7f6e\u4e0b\uff0c\u7814\u7a76\u4e86\u6b63\u5219\u5316\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u7684\u6027\u80fd\uff0c\u5e76\u9996\u6b21\u4e3a\u5b66\u4e60\u771f\u6b63\u53ef\u6cdb\u5316\u4e14\u7b80\u5355\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "conclusion": "\u7b80\u5355\u6027\u662f\u5b9e\u73b0OOD\u6cdb\u5316\u7684\u91cd\u8981\u539f\u5219\uff0c\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u53ef\u4ee5\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7b49\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651", "abs": "https://arxiv.org/abs/2505.22651", "authors": ["Yi Ding", "Ruqi Zhang"], "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.", "AI": {"tldr": "Sherlock is a self-correction and self-improvement training framework for reasoning Vision-Language Models (VLMs). It uses trajectory-level self-correction, preference data construction via visual perturbation, and dynamic preference tuning to enhance VLM performance. Built on Llama3.2-Vision-11B, Sherlock achieves an average accuracy of 64.1 with direct generation and 65.4 after self-correction across eight benchmarks.", "motivation": "Reasoning VLMs show promise but are sensitive to errors, require extensive annotated data or verifiers, and struggle to generalize beyond specific domains. This calls for strategies that can enhance their robustness and generalization capabilities without relying heavily on external supervision or large datasets.", "method": "The researchers first analyze the self-correction abilities of reasoning VLMs and identify gaps. They then introduce Sherlock, which incorporates: 1) a trajectory-level self-correction objective; 2) a method for constructing preference data using visual perturbations; and 3) a dynamic beta for preference tuning. The model initially learns from 20k randomly sampled annotated data and subsequently continues to self-improve without further external supervision.", "result": "Sherlock outperforms other models such as LLaVA-CoT, Mulberry, and LlamaV-o1 in terms of accuracy across eight benchmarks while utilizing less than 20% of the annotated data required by these competing models.", "conclusion": "Sherlock demonstrates the potential of self-correction and self-improvement strategies to significantly enhance the performance of reasoning VLMs, achieving higher accuracy with far fewer annotated data."}}
{"id": "2505.22662", "pdf": "https://arxiv.org/pdf/2505.22662", "abs": "https://arxiv.org/abs/2505.22662", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning.", "AI": {"tldr": "AutoL2S proposes a dynamic framework for large language models (LLMs) to adaptively adjust the length of their reasoning paths, reducing unnecessary complexity and improving efficiency without sacrificing performance.", "motivation": "Reasoning-capable LLMs often overthink simple questions by generating overly long reasoning paths, which increases costs. Current approaches manually determine reasoning length but lack flexibility.", "method": "The paper introduces Auto Long-Short Reasoning (AutoL2S), a model-agnostic framework that dynamically compresses reasoning paths based on question complexity. It uses a training data annotation method with <EASY> tokens to signal when shorter reasoning is sufficient.", "result": "Experiments show that AutoL2S reduces reasoning path length by up to 57% while maintaining performance.", "conclusion": "AutoL2S demonstrates an effective way to make LLM reasoning more scalable and efficient by dynamically adjusting reasoning length."}}
