<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 92]
- [cs.LG](#cs.LG) [Total: 238]
- [cs.CR](#cs.CR) [Total: 51]
- [quant-ph](#quant-ph) [Total: 5]
- [math.AP](#math.AP) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.IR](#cs.IR) [Total: 9]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.CY](#cs.CY) [Total: 21]
- [q-bio.GN](#q-bio.GN) [Total: 3]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [stat.ML](#stat.ML) [Total: 17]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.CL](#cs.CL) [Total: 91]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 11]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.RO](#cs.RO) [Total: 19]
- [econ.EM](#econ.EM) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.CV](#cs.CV) [Total: 50]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [hep-ph](#hep-ph) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy](https://arxiv.org/abs/2506.00056)
*Hugon Lee,Hyeonbin Moon,Junhyeong Lee,Seunghwa RYu*

Main category: cs.AI

TL;DR: AI is reshaping inverse design in manufacturing, but data-driven approaches have limitations. This paper advocates for a new generation of design systems integrating domain knowledge, physics-informed learning, and human-AI interfaces.


<details>
  <summary>Details</summary>
Motivation: Current purely data-driven approaches struggle with sparse data, high-dimensional design spaces, and physical constraints in realistic settings.

Method: Integrating domain knowledge, physics-informed learning, and intuitive human-AI interfaces to create more efficient and generalizable design systems.

Result: Expert-guided sampling strategies enhance data efficiency and model generalization; physics-informed machine learning enables physically consistent modeling in data-scarce regimes; large language models can act as interactive design agents.

Conclusion: Inverse design in manufacturing should evolve into a unified ecosystem combining domain knowledge, physical priors, and adaptive reasoning to enable scalable, interpretable, and accessible AI-driven design systems.

Abstract: Artificial intelligence (AI) is reshaping inverse design across manufacturing
domain, enabling high-performance discovery in materials, products, and
processes. However, purely data-driven approaches often struggle in realistic
settings characterized by sparse data, high-dimensional design spaces, and
nontrivial physical constraints. This perspective argues for a new generation
of design systems that transcend black-box modeling by integrating domain
knowledge, physics-informed learning, and intuitive human-AI interfaces. We
first demonstrate how expert-guided sampling strategies enhance data efficiency
and model generalization. Next, we discuss how physics-informed machine
learning enables physically consistent modeling in data-scarce regimes.
Finally, we explore how large language models emerge as interactive design
agents connecting user intent with simulation tools, optimization pipelines,
and collaborative workflows. Through illustrative examples and conceptual
frameworks, we advocate that inverse design in manufacturing should evolve into
a unified ecosystem, where domain knowledge, physical priors, and adaptive
reasoning collectively enable scalable, interpretable, and accessible AI-driven
design systems.

</details>


### [2] [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
*Shenzhe Zhu,Jiao Sun,Yi Nian,Tobin South,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: The paper explores a future scenario where AI agents automate negotiations and transactions, evaluating their performance and associated risks.


<details>
  <summary>Details</summary>
Motivation: To understand the capabilities and risks of AI agents in securing favorable deals for users in consumer markets.

Method: Developed an experimental framework to evaluate LLM agents' performance in real-world negotiation and transaction settings.

Result: AI-mediated deal-making is imbalanced with varying outcomes; behavioral anomalies in LLMs can lead to financial losses.

Conclusion: Automation improves efficiency but introduces significant risks; users should be cautious when using AI agents for business decisions.

Abstract: AI agents are increasingly used in consumer-facing applications to assist
with tasks such as product search, negotiation, and transaction execution. In
this paper, we explore a future scenario where both consumers and merchants
authorize AI agents to fully automate negotiations and transactions. We aim to
answer two key questions: (1) Do different LLM agents vary in their ability to
secure favorable deals for users? (2) What risks arise from fully automating
deal-making with AI agents in consumer markets? To address these questions, we
develop an experimental framework that evaluates the performance of various LLM
agents in real-world negotiation and transaction settings. Our findings reveal
that AI-mediated deal-making is an inherently imbalanced game -- different
agents achieve significantly different outcomes for their users. Moreover,
behavioral anomalies in LLMs can result in financial losses for both consumers
and merchants, such as overspending or accepting unreasonable deals. These
results underscore that while automation can improve efficiency, it also
introduces substantial risks. Users should exercise caution when delegating
business decisions to AI agents.

</details>


### [3] [Balancing Profit and Fairness in Risk-Based Pricing Markets](https://arxiv.org/abs/2506.00140)
*Jesse Thibodeau,Hadi Nekoei,Afaf Taïk,Janarthanan Rajendran,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: The paper explores how dynamic, risk-based pricing can exclude vulnerable consumer groups and proposes a solution through a learned, interpretable tax schedule using reinforcement learning to improve demand-fairness and social welfare in markets like US health-insurance and consumer-credit.


<details>
  <summary>Details</summary>
Motivation: Dynamic, risk-based pricing has the potential to systematically exclude vulnerable consumer groups from essential resources such as health insurance and consumer credit. This necessitates a regulatory approach that realigns private incentives with social objectives.

Method: The authors provide a formal proposition linking local demographic gap with global opt-out disparity, introduce MarketSim - a simulator of heterogeneous consumers and profit-maximizing firms, and use reinforcement learning to train a social planner that selects a bracketed fairness-tax while staying close to a simple linear prior via an L1 regularizer.

Result: In two empirically calibrated markets (US health-insurance and consumer-credit), the proposed method raises demand-fairness by up to 16% relative to unregulated Free Market and outperforms a fixed linear schedule in terms of social welfare without explicit coordination.

Conclusion: AI-assisted regulation can convert a competitive social dilemma into a win-win equilibrium, offering a practical framework for fairness-aware market oversight.

Abstract: Dynamic, risk-based pricing can systematically exclude vulnerable consumer
groups from essential resources such as health insurance and consumer credit.
We show that a regulator can realign private incentives with social objectives
through a learned, interpretable tax schedule. First, we provide a formal
proposition that bounding each firm's \emph{local} demographic gap implicitly
bounds the \emph{global} opt-out disparity, motivating firm-level penalties.
Building on this insight we introduce \texttt{MarketSim} -- an open-source,
scalable simulator of heterogeneous consumers and profit-maximizing firms --
and train a reinforcement learning (RL) social planner (SP) that selects a
bracketed fairness-tax while remaining close to a simple linear prior via an
$\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and
easily interpretable. In two empirically calibrated markets, i.e., U.S.
health-insurance and consumer-credit, our planner simultaneously raises
demand-fairness by up to $16\%$ relative to unregulated Free Market while
outperforming a fixed linear schedule in terms of social welfare without
explicit coordination. These results illustrate how AI-assisted regulation can
convert a competitive social dilemma into a win-win equilibrium, providing a
principled and practical framework for fairness-aware market oversight.

</details>


### [4] [Utilizing AI for Aviation Post-Accident Analysis Classification](https://arxiv.org/abs/2506.00169)
*Aziida Nanyonga,Graham Wild*

Main category: cs.AI

TL;DR: The paper explores the use of AI, NLP, and Topic Modeling in analyzing aviation safety reports to enhance safety by automating data extraction and identifying patterns.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of timely and accurate analysis of large volumes of textual data in aviation safety reports.

Method: Reviewing ongoing efforts applying NLP and deep learning for classifying damage levels and flight phases, as well as using Topic Modeling techniques to uncover thematic structures within incident reports.

Result: Both NLP, deep learning, and Topic Modeling significantly improve the efficiency and accuracy of aviation safety analysis.

Conclusion: These methods pave the way for more proactive safety management and risk mitigation strategies.

Abstract: The volume of textual data available in aviation safety reports presents a
challenge for timely and accurate analysis. This paper examines how Artificial
Intelligence (AI) and, specifically, Natural Language Processing (NLP) can
automate the process of extracting valuable insights from this data, ultimately
enhancing aviation safety. The paper reviews ongoing efforts focused on the
application of NLP and deep learning to aviation safety reports, with the goal
of classifying the level of damage to an aircraft and identifying the phase of
flight during which safety occurrences happen. Additionally, the paper explores
the use of Topic Modeling (TM) to uncover latent thematic structures within
aviation incident reports, aiming to identify recurring patterns and potential
areas for safety improvement. The paper compares and contrasts the performance
of various deep learning models and TM techniques applied to datasets from the
National Transportation Safety Board (NTSB) and the Australian Transport Safety
Bureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the
impact of dataset size and source on the accuracy of the analysis. The findings
demonstrate that both NLP and deep learning, as well as TM, can significantly
improve the efficiency and accuracy of aviation safety analysis, paving the way
for more proactive safety management and risk mitigation strategies.

</details>


### [5] [Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings](https://arxiv.org/abs/2506.00178)
*Anirudh Nair,Adi Banerjee,Laurent Mombaerts,Matthew Hagen,Tarik Borogovac*

Main category: cs.AI

TL;DR: DEEVO is a new framework for prompt optimization in Large Language Models (LLMs) that uses debate-driven evaluation and Elo-based selection to guide prompt evolution. It outperforms manual methods and other state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Prompt engineering has become a critical bottleneck in using LLMs for complex tasks, especially those involving subjective quality assessment. Current automated methods are insufficient as they need well-defined numerical functions or generic templates which cannot meet the requirements of complex cases.

Method: The introduced method, DEEVO, employs a debate-driven evolutionary approach to optimize prompts. It explores the discrete prompt space while maintaining semantic coherence through intelligent crossover and strategic mutation operations. Elo ratings serve as a fitness proxy to simultaneously drive improvement and preserve diversity in the prompt population.

Result: Experimental results show that DEEVO significantly surpasses both manual prompt engineering and alternative state-of-the-art optimization methods on both open-ended and close-ended tasks without using ground truth feedback.

Conclusion: DEEVO marks a significant advancement in prompt optimization research by connecting LLMs reasoning capabilities with adaptive optimization, eliminating the need for predetermined metrics to continuously improve AI systems.

Abstract: Prompt engineering represents a critical bottleneck to harness the full
potential of Large Language Models (LLMs) for solving complex tasks, as it
requires specialized expertise, significant trial-and-error, and manual
intervention. This challenge is particularly pronounced for tasks involving
subjective quality assessment, where defining explicit optimization objectives
becomes fundamentally problematic. Existing automated prompt optimization
methods falter in these scenarios, as they typically require well-defined
task-specific numerical fitness functions or rely on generic templates that
cannot capture the nuanced requirements of complex use cases. We introduce
DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that
guides prompt evolution through a debate-driven evaluation with an Elo-based
selection. Contrary to prior work, DEEVOs approach enables exploration of the
discrete prompt space while preserving semantic coherence through intelligent
crossover and strategic mutation operations that incorporate debate-based
feedback, combining elements from both successful and unsuccessful prompts
based on identified strengths rather than arbitrary splicing. Using Elo ratings
as a fitness proxy, DEEVO simultaneously drives improvement and preserves
valuable diversity in the prompt population. Experimental results demonstrate
that DEEVO significantly outperforms both manual prompt engineering and
alternative state-of-the-art optimization approaches on open-ended tasks and
close-ended tasks despite using no ground truth feedback. By connecting LLMs
reasoning capabilities with adaptive optimization, DEEVO represents a
significant advancement in prompt optimization research by eliminating the need
of predetermined metrics to continuously improve AI systems.

</details>


### [6] [Control-R: Towards controllable test-time scaling](https://arxiv.org/abs/2506.00189)
*Di Zhang,Weida Wang,Junxian Li,Xunzhi Wang,Jiatong Li,Jianbo Wu,Jingdi Lei,Haonan He,Peng Ye,Shufei Zhang,Wanli Ouyang,Yuqiang Li,Dongzhan Zhou*

Main category: cs.AI

TL;DR: This paper proposes Reasoning Control Fields (RCF) to address underthinking and overthinking in long chain-of-thought reasoning for large models, along with the Control-R-4K dataset and Conditional Distillation Finetuning (CDF) method, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of underthinking and overthinking in long chain-of-thought reasoning for Large Reasoning Models.

Method: The method involves introducing Reasoning Control Fields (RCF), a novel test-time approach that injects structured control signals to guide reasoning from a tree search perspective. Additionally, they present the Control-R-4K dataset and propose a Conditional Distillation Finetuning (CDF) method.

Result: Experimental results on benchmarks such as AIME2024 and MATH500 demonstrate state-of-the-art performance at the 32B scale while enabling a controllable Long CoT reasoning process.

Conclusion: This work introduces an effective paradigm for controllable test-time scaling reasoning.

Abstract: This paper target in addressing the challenges of underthinking and
overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning
Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time
approach that injects structured control signals to guide reasoning from a tree
search perspective. RCF enables models to adjust reasoning effort according to
given control conditions when solving complex tasks. Additionally, we present
the Control-R-4K dataset, which consists of challenging problems annotated with
detailed reasoning processes and corresponding control fields. To further
enhance reasoning control, we propose a Conditional Distillation Finetuning
(CDF) method, which trains model--particularly Control-R-32B--to effectively
adjust reasoning effort during test time. Experimental results on benchmarks
such as AIME2024 and MATH500 demonstrate that our approach achieves
state-of-the-art performance at the 32B scale while enabling a controllable
Long CoT reasoning process (L-CoT). Overall, this work introduces an effective
paradigm for controllable test-time scaling reasoning.

</details>


### [7] [What do professional software developers need to know to succeed in an age of Artificial Intelligence?](https://arxiv.org/abs/2506.00202)
*Matthew Kam,Cody Miller,Miaoxin Wang,Abey Tidwell,Irene A. Lee,Joyce Malyn-Smith,Beatriz Perez,Vikram Tiwari,Joshua Kenitzer,Andrew Macvean,Erin Barrar*

Main category: cs.AI

TL;DR: Generative AI is improving software developers' productivity, but concerns about workforce disruption and deskilling remain. Through research with 21 developers, the study uncovered 12 work goals, 75 tasks, skills and knowledge associated with using AI at work, leading to 5 insights. Successful AI-enhanced developers possess skills in four domains (using Generative AI effectively, core software engineering, adjacent engineering, and adjacent non-engineering) which are deployed throughout a 6-step task workflow.


<details>
  <summary>Details</summary>
Motivation: To understand how developers use AI at work and what skills & knowledge they need to be successful in the age of AI, addressing concerns about workforce disruption and deskilling.

Method: The study involved 21 developers at the cutting edge of using AI. The researchers identified 12 work goals, 75 associated tasks and the required skills & knowledge for each, analyzing how developers use AI in their jobs.

Result: The research found that successful AI-enhanced developers require skills in four domains, organized throughout a 6-step task workflow. It also identified 5 insights into how developers use AI at work.

Conclusion: In order to safeguard against deskilling and ensure developers' success in the age of AI, both on-the-job learning initiatives and computer science degree programs should focus on developing both 'soft' skills and technical skills & knowledge in all four domains.

Abstract: Generative AI is showing early evidence of productivity gains for software
developers, but concerns persist regarding workforce disruption and deskilling.
We describe our research with 21 developers at the cutting edge of using AI,
summarizing 12 of their work goals we uncovered, together with 75 associated
tasks and the skills & knowledge for each, illustrating how developers use AI
at work. From all of these, we distilled our findings in the form of 5
insights. We found that the skills & knowledge to be a successful AI-enhanced
developer are organized into four domains (using Generative AI effectively,
core software engineering, adjacent engineering, and adjacent non-engineering)
deployed at critical junctures throughout a 6-step task workflow. In order to
"future proof" developers for this age of AI, on-the-job learning initiatives
and computer science degree programs will need to target both "soft" skills and
the technical skills & knowledge in all four domains to reskill, upskill and
safeguard against deskilling.

</details>


### [8] [Ethical AI: Towards Defining a Collective Evaluation Framework](https://arxiv.org/abs/2506.00233)
*Aasish Kumar Sharma,Dimitar Kyosev,Julian Kunkel*

Main category: cs.AI

TL;DR: This paper proposes a modular ethical assessment framework based on ontological blocks integrated with FAIR principles to address the urgent ethical concerns of AI systems. It demonstrates the application of this framework in AI-powered investor profiling and suggests that ontological blocks offer a promising path toward explainable and auditable AI ethics.


<details>
  <summary>Details</summary>
Motivation: The rapid integration of AI raises urgent ethical concerns related to data ownership, privacy, and systemic bias, necessitating transparent and accountable AI systems.

Method: The paper proposes a modular ethical assessment framework built on ontological blocks of meaning, which encode ethical principles such as fairness, accountability, and ownership. These blocks are integrated with FAIR principles to support scalable, transparent, and legally aligned ethical evaluations.

Result: The framework enables dynamic, behavior-informed risk classification in AI-powered investor profiling, suggesting that ontological blocks can lead to explainable and auditable AI ethics.

Conclusion: Ontological blocks offer a promising approach for explainable and auditable AI ethics, but challenges persist in automation and probabilistic reasoning.

Abstract: Artificial Intelligence (AI) is transforming sectors such as healthcare,
finance, and autonomous systems, offering powerful tools for innovation. Yet
its rapid integration raises urgent ethical concerns related to data ownership,
privacy, and systemic bias. Issues like opaque decision-making, misleading
outputs, and unfair treatment in high-stakes domains underscore the need for
transparent and accountable AI systems. This article addresses these challenges
by proposing a modular ethical assessment framework built on ontological blocks
of meaning-discrete, interpretable units that encode ethical principles such as
fairness, accountability, and ownership. By integrating these blocks with FAIR
(Findable, Accessible, Interoperable, Reusable) principles, the framework
supports scalable, transparent, and legally aligned ethical evaluations,
including compliance with the EU AI Act. Using a real-world use case in
AI-powered investor profiling, the paper demonstrates how the framework enables
dynamic, behavior-informed risk classification. The findings suggest that
ontological blocks offer a promising path toward explainable and auditable AI
ethics, though challenges remain in automation and probabilistic reasoning.

</details>


### [9] [SMELLNET: A Large-scale Dataset for Real-world Smell Recognition](https://arxiv.org/abs/2506.00239)
*Dewei Feng,Carol Li,Wei Dai,Paul Pu Liang*

Main category: cs.AI

TL;DR: 尽管AI嗅觉有广泛的应用前景，但缺乏大规模基准数据阻碍了进展。本文提出SmellNet，首个大规模数字化自然世界气味的数据库，包含50种物质、约18万时间步长和50小时的数据。利用SmellNet训练基于气味实时分类物质的AI模型，最佳方法结合序列模型、对比学习和新时间差分法，达到65.35%的离线分类准确率，在在线分类任务中对坚果和香料分别达到10.71%和25.38%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前AI嗅觉领域缺乏大规模基准数据集，限制了训练和评估AI系统在真实世界中嗅觉能力的进展，因此需要创建一个大规模的气味数据库来推动该领域的发展。

Method: 使用便携式气体和化学传感器创建SmellNet数据库，其中包含50种物质（包括坚果、香料、草药、水果和蔬菜）的约18万时间步长数据，总计50小时。基于SmellNet训练AI模型，采用序列模型、对比学习结合高分辨率气相色谱-质谱分子数据以及新的时间差分方法识别传感器读数的剧烈变化，实现基于气味的实时物质分类。

Result: 在预录数据上的分类准确率达到65.35%，在具有挑战性的50类在线分类任务中，对坚果和香料的分类准确率分别为10.71%和25.38%，表明模型在真实世界条件下的泛化能力。

Conclusion: SmellNet的构建为AI嗅觉研究提供了重要资源，但也揭示了该领域面临的许多技术挑战，如更丰富的特征学习、边缘计算嗅觉模型和对环境变化的鲁棒性等。

Abstract: The ability of AI to sense and identify various substances based on their
smell alone can have profound impacts on allergen detection (e.g., smelling
gluten or peanuts in a cake), monitoring the manufacturing process, and sensing
hormones that indicate emotional states, stress levels, and diseases. Despite
these broad impacts, there are virtually no large scale benchmarks, and
therefore little progress, for training and evaluating AI systems' ability to
smell in the real world. In this paper, we use portable gas and chemical
sensors to create SmellNet, the first large-scale database that digitizes a
diverse range of smells in the natural world. SmellNet contains about 180,000
time steps of 50 substances (spanning nuts, spices, herbs, fruits, and
vegetables) with 50 hours of data. Using SmellNet, we train AI models for
real-time classification of substances based on their smell alone. Our best
methods leverage sequence models, contrastive learning to integrate
high-resolution Gas Chromatography-Mass Spectrometry molecular data, and a new
temporal difference method that identifies sharp changes in sensor readings.
Our best models achieve up to 65.35% accuracy on pre-recorded data, and
generalize to real-world conditions with 10.71% accuracy on nuts and 25.38% on
spices in the challenging 50-way online classification task. Despite these
promising results, SmellNet highlights many technical challenges in building AI
for smell, including richer feature learning, on-edge smell models, and
robustness to environmental changes.

</details>


### [10] [Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise](https://arxiv.org/abs/2506.00242)
*Shuai Feng,Wei-Chuang Chan,Srishti Chouhan,Junior Francisco Garcia Ayala,Srujananjali Medicherla,Kyle Clark,Mingwei Shi*

Main category: cs.AI

TL;DR: The paper presents a soft prompt fine-tuning framework for enhancing cultural sensitivity of LLMs without altering base parameters, significantly improving alignment scores and paving the way for future research.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack nuanced understanding in diverse cultural contexts, and adapting them usually requires costly full fine-tuning.

Method: A novel soft prompt fine-tuning framework is introduced which uses vectorized prompt tuning to route queries to culturally specialized 'expert' LLM configurations by optimizing soft prompt embeddings without changing base model's parameters.

Result: Experiments show that this framework greatly enhances cultural sensitivity and adaptability, with alignment scores improving from 0.208 to 0.820.

Conclusion: This research offers a robust solution for deploying culturally-aware LLMs and opens avenues for further studies on improved cultural coverage and dynamic expert adaptation.

Abstract: The integration of large language models (LLMs) into global applications
necessitates effective cultural alignment for meaningful and
culturally-sensitive interactions. Current LLMs often lack the nuanced
understanding required for diverse cultural contexts, and adapting them
typically involves costly full fine-tuning. To address this, we introduce a
novel soft prompt fine-tuning framework that enables efficient and modular
cultural alignment. Our method utilizes vectorized prompt tuning to dynamically
route queries to a committee of culturally specialized 'expert' LLM
configurations, created by optimizing soft prompt embeddings without altering
the base model's parameters. Extensive experiments demonstrate that our
framework significantly enhances cultural sensitivity and adaptability,
improving alignment scores from 0.208 to 0.820, offering a robust solution for
culturally-aware LLM deployment. This research paves the way for subsequent
investigations into enhanced cultural coverage and dynamic expert adaptation,
crucial for realizing autonomous AI with deeply nuanced understanding in a
globally interconnected world.

</details>


### [11] [MIR: Methodology Inspiration Retrieval for Scientific Research Problems](https://arxiv.org/abs/2506.00249)
*Aniketh Garikaparthi,Manasi Patwardhan,Aditya Sanjiv Kanade,Aman Hassan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TL;DR: The paper explores Methodology Inspiration Retrieval (MIR) using Large Language Models (LLMs), constructing a Methodology Adjacency Graph (MAG) and adapting LLM-based re-ranking strategies, resulting in significant improvements in retrieval metrics.


<details>
  <summary>Details</summary>
Motivation: To improve scientific discovery by effectively retrieving prior works that can inspire solutions for given research problems, addressing the limitations of current approaches reliant on literature quality.

Method: Constructing a novel dataset for MIR, building the Methodology Adjacency Graph (MAG) to capture methodological lineage, embedding an 'intuitive prior' into dense retrievers, and adapting LLM-based re-ranking strategies.

Result: +5.4 in Recall@3 and +7.8 in Mean Average Precision (mAP) over strong baselines with MAG, and additional +4.5 in Recall@3 and +4.8 in mAP with LLM-based re-ranking.

Conclusion: MIR shows promise in enhancing automated scientific discovery, with outlined avenues for further advancements in inspiration-driven retrieval.

Abstract: There has been a surge of interest in harnessing the reasoning capabilities
of Large Language Models (LLMs) to accelerate scientific discovery. While
existing approaches rely on grounding the discovery process within the relevant
literature, effectiveness varies significantly with the quality and nature of
the retrieved literature. We address the challenge of retrieving prior work
whose concepts can inspire solutions for a given research problem, a task we
define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset
tailored for training and evaluating retrievers on MIR, and establish
baselines. To address MIR, we build the Methodology Adjacency Graph (MAG);
capturing methodological lineage through citation relationships. We leverage
MAG to embed an "intuitive prior" into dense retrievers for identifying
patterns of methodological inspiration beyond superficial semantic similarity.
This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average
Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking
strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and
+4.8 in mAP. Through extensive ablation studies and qualitative analyses, we
exhibit the promise of MIR in enhancing automated scientific discovery and
outline avenues for advancing inspiration-driven retrieval.

</details>


### [12] [Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models](https://arxiv.org/abs/2506.00258)
*Qianqi Yan,Hongquan Li,Shan Jiang,Yang Zhao,Xinze Guan,Ching-Chen Kuo,Xin Eric Wang*

Main category: cs.AI

TL;DR: Multimodal large language models often fail to detect implicit issues despite having necessary skills, but simple interventions can improve performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well current multimimensional large language models handle implicit reasoning scenarios where flaws are not explicitly stated.

Method: Using a diagnostic suite covering four categories of real-world failure modes, six MLLMs were evaluated including o3 and GPT-4o. The study involved explicit prompting and inference-time interventions such as cautious persona prompting and requiring clarifying questions.

Result: Models frequently failed to surface hidden issues even with the necessary perceptual and reasoning skills. Interventions like requiring a clarifying question significantly improved performance.

Conclusion: There is a gap between reasoning competence and behavioral compliance in MLLMs. Simple interventions can make these models more reliable in unpredictable environments.

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in
open-ended, real-world environments where inputs are messy, underspecified, and
not always trustworthy. Unlike curated benchmarks, these settings frequently
involve instructions that refer to missing objects or contradictory facts, rely
on ambiguous references, or request infeasible actions. In such cases, success
hinges not on task execution alone, but on a model's ability to detect when
something is silently wrong. This paper presents a systematic analysis of how
current MLLMs handle such implicit reasoning scenarios: cases where the flaw is
not explicitly stated but must be inferred from context. Using a curated
diagnostic suite spanning four categories of real-world failure modes, we
evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently
fail to surface hidden issues, even when they possess the necessary perceptual
and reasoning skills. Explicit prompting reveals that the underlying
capabilities exist but are often suppressed in favor of user compliance. We
further show that simple inference-time interventions, such as cautious persona
prompting and, in particular, requiring a clarifying question, can dramatically
recover performance. Our findings highlight a persistent gap between reasoning
competence and behavioral compliance in current MLLMs and suggest practical
strategies for making these models more trustworthy in underconstrained
environments.

</details>


### [13] [Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning](https://arxiv.org/abs/2506.00279)
*Boshra Khajehpiri,Eric Granger,Massimiliano de Zambotti,Fiona C. Baker,Mohamad Forouzanfar*

Main category: cs.AI

TL;DR: The study explores the connection between sleep microstructure and human performance using deep learning models. A multi-scale convolutional-transformer model named CogPSGFormer was developed to predict executive functions based on polysomnographic data.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research on the relationship between sleep and cognition, the connection between sleep microstructure and human performance across specific cognitive domains remains underexplored.

Method: Introduced CogPSGFormer, a multi-scale convolutional-transformer model designed to process multi-modal polysomnographic data including one-channel ECG and EEG signals along with extracted features like EEG power bands and heart rate variability parameters.

Result: Evaluated on 817 individuals from the STAGES dataset using cross-validation, the model achieved 80.3% accuracy in classifying individuals into low vs. high cognitive performance groups on unseen data based on Penn Conditional Exclusion Test (PCET) scores.

Conclusion: The findings highlight the effectiveness of the multi-scale feature extraction and multi-modal learning approach in leveraging sleep-derived signals for cognitive performance prediction.

Abstract: Despite extensive research on the relationship between sleep and cognition,
the connection between sleep microstructure and human performance across
specific cognitive domains remains underexplored. This study investigates
whether deep learning models can predict executive functions, particularly
cognitive adaptability and conceptual reasoning from physiological processes
during a night's sleep. To address this, we introduce CogPSGFormer, a
multi-scale convolutional-transformer model designed to process multi-modal
polysomnographic data. This model integrates one-channel ECG and EEG signals
along with extracted features, including EEG power bands and heart rate
variability parameters, to capture complementary information across modalities.
A thorough evaluation of the CogPSGFormer architecture was conducted to
optimize the processing of extended sleep signals and identify the most
effective configuration. The proposed framework was evaluated on 817
individuals from the STAGES dataset using cross-validation. The model achieved
80.3\% accuracy in classifying individuals into low vs. high cognitive
performance groups on unseen data based on Penn Conditional Exclusion Test
(PCET) scores. These findings highlight the effectiveness of our multi-scale
feature extraction and multi-modal learning approach in leveraging
sleep-derived signals for cognitive performance prediction. To facilitate
reproducibility, our code is publicly accessible
(https://github.com/boshrakh95/CogPSGFormer.git).

</details>


### [14] [Evaluation of LLMs for mathematical problem solving](https://arxiv.org/abs/2506.00309)
*Ruonan Wang,Runxi Wang,Yunwen Shen,Chengfeng Wu,Qinglin Zhou,Rohitash Chandra*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) like GPT-4o, DeepSeek-V3, and Gemini-2.0 were compared on three mathematics datasets using the Structured Chain-of-Thought (SCoT) framework. GPT-4o performed most consistently across all datasets, especially excelling in high-level questions. DeepSeek-V3 was strong in well-structured domains but less accurate in statistical inference tasks. Gemini-2.0 showed good linguistic understanding but struggled with multi-step reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Large Language Models (LLMs) in solving mathematical problems by comparing their performance on various datasets using a five-dimensional approach based on the Structured Chain-of-Thought (SCoT) framework.

Method: Compared three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0) on three mathematics datasets (GSM8K, MATH500, and UNSW) using a five-dimensional approach based on the Structured Chain-of-Thought (SCoT) framework to assess final answer correctness, step completeness, step validity, intermediate calculation accuracy, and problem comprehension.

Result: GPT-4o was the most stable and consistent performer across all datasets, particularly excelling in high-level questions of the UNSW dataset. DeepSeek-V3 was competitively strong in well-structured domains but had fluctuations in accuracy in statistical inference tasks. Gemini-2.0 showed strong linguistic understanding and clarity in well-structured problems but performed poorly in multi-step reasoning and symbolic logic.

Conclusion: Each LLM has strengths and weaknesses when it comes to solving mathematical problems. GPT-4o is the most stable overall, DeepSeek-V3 performs well in specific domains but lacks consistency, and Gemini-2.0 struggles with complex mathematical reasoning.

Abstract: Large Language Models (LLMs) have shown impressive performance on a range of
educational tasks, but are still understudied for their potential to solve
mathematical problems. In this study, we compare three prominent LLMs,
including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of
varying complexities (GSM8K, MATH500, and UNSW datasets). We take a
five-dimensional approach based on the Structured Chain-of-Thought (SCoT)
framework to assess final answer correctness, step completeness, step validity,
intermediate calculation accuracy, and problem comprehension. The results show
that GPT-4o is the most stable and consistent in performance across all the
datasets, but particularly it performs outstandingly in high-level questions of
the UNSW dataset. DeepSeek-V3 is competitively strong in well-structured
domains such as optimisation, but suffers from fluctuations in accuracy in
statistical inference tasks. Gemini-2.0 shows strong linguistic understanding
and clarity in well-structured problems but performs poorly in multi-step
reasoning and symbolic logic. Our error analysis reveals particular deficits in
each model: GPT-4o is at times lacking in sufficient explanation or precision;
DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in
mathematical reasoning in higher dimensions.

</details>


### [15] [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)
*Xiao Yu,Baolin Peng,Ruize Xu,Michel Galley,Hao Cheng,Suman Nath,Jianfeng Gao,Zhou Yu*

Main category: cs.AI

TL;DR: Recent progress in reasoning with large language models (LLMs) shows great capabilities. However, it's unclear what behavior is effective and missing for long-horizon AI agents tasks. This work proposes Dyna-Think, a thinking framework integrating planning, reasoning, and acting to enhance AI agent performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the lack of clarity on what behavior is effective and what is missing for long-horizon AI agents tasks despite recent progress in reasoning with LLMs.

Method: The method proposed in this paper includes Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting. To enable Dyna-Think, they propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed action and trains the policy using this data. DDT uses a two-stage training process to improve the agent's world modeling ability and then its action via policy training.

Result: The evaluation on OSWorld demonstrates that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. The empirical studies reveal that using critique generation for world model training effectively improves policy performance and better AI agent performance correlates with better world modeling abilities.

Conclusion: The conclusion suggests a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.

Abstract: Recent progress in reasoning with large language models (LLMs), such as
DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics
and coding, by exhibiting complex cognitive behaviors such as verification,
goal decomposition, and self-reflection. However, it is unclear what behavior
is effective and what behavior is missing for long-horizon AI agents tasks. In
this work, we propose Dyna-Think, a thinking framework that integrates planning
with an internal world model with reasoning and acting to enhance AI agent
performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning
(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with
Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing
world model simulation relevant to the proposed (and planned) action, and
trains the policy using this reconstructed data. To enhance Dyna-Think, DDT
uses a two-stage training process to first improve the agent's world modeling
ability via objectives such as state prediction or critique generation, and
then improve the agent's action via policy training. We evaluate our methods on
OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and
out-of-domain performance, achieving similar best-of-n performance compared to
R1 while generating 2x less tokens on average. Our extensive empirical studies
reveal that 1) using critique generation for world model training is effective
to improve policy performance; and 2) AI agents with better performance
correlate with better world modeling abilities. We believe our results suggest
a promising research direction to integrate world model simulation into AI
agents to enhance their reasoning, planning, and acting capabilities.

</details>


### [16] [BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies](https://arxiv.org/abs/2506.00328)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.AI

TL;DR: BASIL is a new method for interpretable reinforcement learning that creates symbolic,rule-based policies via evolutionary search with quality-diversity optimization. It encourages diversity and compact representations while maintaining transparency.


<details>
  <summary>Details</summary>
Motivation: Modern deep reinforcement learning methods produce opaque policies which compromise verification, reduce transparency, and impede human oversight. There is a need for interpretable reinforcement learning in safety-critical applications.

Method: BASIL represents policies as ordered lists of symbolic predicates over state variables and uses online evolutionary search with quality-diversity optimization to generate symbolic, rule-based policies. A QD archive encourages behavioral and structural diversity between top-performing solutions and complexity-aware fitness encourages compact representations.

Result: Empirical comparisons with three benchmark tasks show that BASIL consistently synthesizes interpretable controllers with compact representations comparable to deep reinforcement learning baselines.

Conclusion: This article introduces BASIL, a new interpretable policy synthesis method that combines symbolic expressiveness, evolutionary diversity, and online learning through a unifying framework.

Abstract: The quest for interpretable reinforcement learning is a grand challenge for
the deployment of autonomous decision-making systems in safety-critical
applications. Modern deep reinforcement learning approaches, while powerful,
tend to produce opaque policies that compromise verification, reduce
transparency, and impede human oversight. To address this, we introduce BASIL
(Best-Action Symbolic Interpretable Learning), a systematic approach for
generating symbolic, rule-based policies via online evolutionary search with
quality-diversity (QD) optimization. BASIL represents policies as ordered lists
of symbolic predicates over state variables, ensuring full interpretability and
tractable policy complexity. By using a QD archive, the methodology in the
proposed study encourages behavioral and structural diversity between
top-performing solutions, while a complexity-aware fitness encourages the
synthesis of compact representations. The evolutionary system supports the use
of exact constraints for rule count and system adaptability for balancing
transparency with expressiveness. Empirical comparisons with three benchmark
tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently
synthesizes interpretable controllers with compact representations comparable
to deep reinforcement learning baselines. Herein, this article introduces a new
interpretable policy synthesis method that combines symbolic expressiveness,
evolutionary diversity, and online learning through a unifying framework.

</details>


### [17] [Position: Olfaction Standardization is Essential for the Advancement of Embodied Artificial Intelligence](https://arxiv.org/abs/2506.00398)
*Kordel K. France,Rohith Peddi,Nik Dennler,Ovidiu Daescu*

Main category: cs.AI

TL;DR: The paper argues that olfaction, a critical sense linked to memory and emotion, has been overlooked in AI development due to structural challenges like lack of benchmarks and standardized datasets. To achieve general and embodied intelligence, the AI community needs to invest in olfactory research through cross-disciplinary collaboration.


<details>
  <summary>Details</summary>
Motivation: Olfaction, an evolutionarily important sense, has been neglected in AI despite its strong ties with memory, emotion, and contextual reasoning. The authors identify several structural challenges that have hindered progress in machine olfaction.

Method: The authors call for a collaborative effort across neuroscience, robotics, machine learning, and ethics to create olfactory benchmarks, develop multimodal datasets, and define necessary sensory capabilities for machines.

Result: Recognizing olfaction as a core modality is crucial for advancing towards general and embodied intelligence and building ethically grounded AI systems.

Conclusion: To fully represent human cognition and build ethical super-human intelligence, serious investment in olfactory research is required.

Abstract: Despite extraordinary progress in artificial intelligence (AI), modern
systems remain incomplete representations of human cognition. Vision, audition,
and language have received disproportionate attention due to well-defined
benchmarks, standardized datasets, and consensus-driven scientific foundations.
In contrast, olfaction - a high-bandwidth, evolutionarily critical sense - has
been largely overlooked. This omission presents a foundational gap in the
construction of truly embodied and ethically aligned super-human intelligence.
We argue that the exclusion of olfactory perception from AI architectures is
not due to irrelevance but to structural challenges: unresolved scientific
theories of smell, heterogeneous sensor technologies, lack of standardized
olfactory datasets, absence of AI-oriented benchmarks, and difficulty in
evaluating sub-perceptual signal processing. These obstacles have hindered the
development of machine olfaction despite its tight coupling with memory,
emotion, and contextual reasoning in biological systems. In this position
paper, we assert that meaningful progress toward general and embodied
intelligence requires serious investment in olfactory research by the AI
community. We call for cross-disciplinary collaboration - spanning
neuroscience, robotics, machine learning, and ethics - to formalize olfactory
benchmarks, develop multimodal datasets, and define the sensory capabilities
necessary for machines to understand, navigate, and act within human
environments. Recognizing olfaction as a core modality is essential not only
for scientific completeness, but for building AI systems that are ethically
grounded in the full scope of the human experience.

</details>


### [18] [World Models for Cognitive Agents: Transforming Edge Intelligence in Future Networks](https://arxiv.org/abs/2506.00417)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Gaosheng Zhao,Dusit Niyato,Geng Sun,Shiwen Mao,Dong In Kim*

Main category: cs.AI

TL;DR: World models are a transformative paradigm in AI that enable agents to construct internal representations of their environments. This paper provides a comprehensive overview of world models, compares them with related concepts, and proposes Wireless Dreamer, a reinforcement learning framework tailored for wireless edge intelligence optimization.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of world models and their applications, and to propose a novel reinforcement learning framework for wireless edge intelligence optimization.

Method: Presenting a comprehensive overview of world models, comparing them with related concepts, and proposing Wireless Dreamer, a world model-based reinforcement learning framework tailored for wireless edge intelligence optimization.

Result: Demonstrated the effectiveness of Wireless Dreamer in improving learning efficiency and decision quality through a weather-aware UAV trajectory planning case study.

Conclusion: World models play a unique role as embedded cognitive engines for autonomous agents and Wireless Dreamer shows promise in wireless edge intelligence optimization.

Abstract: World models are emerging as a transformative paradigm in artificial
intelligence, enabling agents to construct internal representations of their
environments for predictive reasoning, planning, and decision-making. By
learning latent dynamics, world models provide a sample-efficient framework
that is especially valuable in data-constrained or safety-critical scenarios.
In this paper, we present a comprehensive overview of world models,
highlighting their architecture, training paradigms, and applications across
prediction, generation, planning, and causal reasoning. We compare and
distinguish world models from related concepts such as digital twins, the
metaverse, and foundation models, clarifying their unique role as embedded
cognitive engines for autonomous agents. We further propose Wireless Dreamer, a
novel world model-based reinforcement learning framework tailored for wireless
edge intelligence optimization, particularly in low-altitude wireless networks
(LAWNs). Through a weather-aware UAV trajectory planning case study, we
demonstrate the effectiveness of our framework in improving learning efficiency
and decision quality.

</details>


### [19] [MIRROR: Cognitive Inner Monologue Between Conversational Turns for Persistent Reflection and Reasoning in Conversational LLMs](https://arxiv.org/abs/2506.00430)
*Nicole Hsing*

Main category: cs.AI

TL;DR: MIRROR is a new cognitive architecture for large language models which mimics human inner monologue, comprising a Thinker and a Talker layer. It improves performance in personalized dialogues with safety-critical constraints.


<details>
  <summary>Details</summary>
Motivation: To enhance the reasoning, reflection, memory retrieval, and response formulation capabilities of large language models by implementing a system inspired by human cognition.

Method: MIRROR operates as a unified system with two distinct functional layers: the Thinker and the Talker. The Thinker includes an Inner Monologue Manager and a Cognitive Controller, while the Talker leverages the integrated narrative for context-aware responses.

Result: LLMs utilizing MIRROR achieved up to 156% relative improvement in critical safety scenarios involving conflicting preferences, maintaining an average accuracy of ~80%. Across scenario-specific comparisons, MIRROR-equipped models outperformed baselines by 21% on average.

Conclusion: MIRROR addresses key LLM failure modes and significantly enhances multi-turn conversation capabilities by creating a persistent internal model inspired by human cognition.

Abstract: Human intelligence relies on inner monologue to process complex information
through simultaneous reflection, memory retrieval, and response formulation. We
introduce MIRROR (Modular Internal Reasoning, Reflection, Orchestration, and
Response), a cognitive architecture that systematically implements these
parallel reasoning capabilities in large language models. MIRROR operates as a
unified system with two distinct functional layers: the Thinker and the Talker.
The Thinker encompasses: (1) the Inner Monologue Manager, coordinating
reasoning threads across cognitive dimensions (Goals, Reasoning, and Memory);
and (2) the Cognitive Controller, synthesizing these threads into a coherent
internal narrative maintained across conversation turns. The Talker component
then leverages this integrated narrative for context-aware responses. Evaluated
on the CuRaTe benchmark--testing personalized dialogue with safety-critical
constraints, conflicting preferences, and multi-turn consistency--LLMs
utilizing the MIRROR architecture achieve up to 156% relative improvement in
critical safety scenarios involving three persons with conflicting preferences,
maintaining an average accuracy of ~>80% on all scenarios. Across
scenario-specific comparisons, GPT-4o, Gemini 1.5 Pro, Claude 3.7 Sonnet, Llama
4 variants, and Mistral 3 variants with the MIRROR architecture outperformed
baseline models by 21% on average (15.5 percentage points absolute). MIRROR
directly addresses three critical LLM failure modes: sycophancy, attentional
deficits to critical information, and inconsistent prioritization of
conflicting constraints. This work bridges cognitive science and AI by
implementing modular internal reasoning inspired by human cognition, creating a
persistent internal model that significantly enhances multi-turn conversation
capabilities.

</details>


### [20] [Monitoring Robustness and Individual Fairness](https://arxiv.org/abs/2506.00496)
*Ashutosh Gupta,Thomas A. Henzinger,Konstantin Kueffner,Kaushik Mallik,David Pape*

Main category: cs.AI

TL;DR: 提出了一种用于监控已部署的黑箱AI模型输入-输出鲁棒性的运行时监控方法，通过观察模型的长时间执行序列并检测相似输入导致的不同输出来触发警报。此外，还提出了工具Clemont，包含多个轻量级监控器，并开发了有效的并行化技术以减少计算时间。实验表明这些监控器能有效检测运行时的鲁棒性违规。


<details>
  <summary>Details</summary>
Motivation: 在文献中，输入-输出鲁棒性以多种形式出现，例如AI模型对对抗性或语义扰动的鲁棒性和针对人类决策的个体公平性。为了提高AI决策者的可信度，需要设计能够补充现有离线“鲁棒化”方法的监控手段。

Method: 将监控问题转化为固定半径最近邻（FRNN）搜索问题，提出工具Clemont，包含多种轻量级监控器，其中一些使用升级的在线FRNN算法变体，另一个基于二进制决策图的新算法。还开发了一种高效的并行化技术，可显著减少使用$L_∞$范数测量输入-输出对距离的监控器的计算时间。

Result: 通过使用来自对抗性和语义鲁棒性以及个体公平性文献的标准基准，进行不同监控器的比较研究，证明了它们在正确检测运行时鲁棒性违规方面的有效性。

Conclusion: 运行时监控输入-输出鲁棒性可以有效补充现有的离线鲁棒化方法，增加AI决策者的可信度。提出的工具Clemont及其监控器展示了良好的性能，尤其是在检测鲁棒性违规方面。

Abstract: Input-output robustness appears in various different forms in the literature,
such as robustness of AI models to adversarial or semantic perturbations and
individual fairness of AI models that make decisions about humans.
  We propose runtime monitoring of input-output robustness of deployed,
black-box AI models, where the goal is to design monitors that would observe
one long execution sequence of the model, and would raise an alarm whenever it
is detected that two similar inputs from the past led to dissimilar outputs.
  This way, monitoring will complement existing offline ``robustification''
approaches to increase the trustworthiness of AI decision-makers.
  We show that the monitoring problem can be cast as the fixed-radius nearest
neighbor (FRNN) search problem, which, despite being well-studied, lacks
suitable online solutions.
  We present our tool Clemont, which offers a number of lightweight monitors,
some of which use upgraded online variants of existing FRNN algorithms, and one
uses a novel algorithm based on binary decision diagrams -- a data-structure
commonly used in software and hardware verification.
  We have also developed an efficient parallelization technique that can
substantially cut down the computation time of monitors for which the distance
between input-output pairs is measured using the $L_\infty$ norm.
  Using standard benchmarks from the literature of adversarial and semantic
robustness and individual fairness, we perform a comparative study of different
monitors in \tool, and demonstrate their effectiveness in correctly detecting
robustness violations at runtime.

</details>


### [21] [CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing](https://arxiv.org/abs/2506.00530)
*Tianhui Liu,Jie Feng,Hetian Pang,Xin Zhang,Tianjian Ouyang,Zhiyuan Zhang,Yong Li*

Main category: cs.AI

TL;DR: The paper introduces CityLens, a benchmark to evaluate large language-vision models' (LLVMs) abilities in predicting urban socioeconomic indicators from visual data. It includes a multi-modal dataset of 17 cities across 6 domains and defines 11 prediction tasks. While LLVMs show potential, they have limitations in this context.


<details>
  <summary>Details</summary>
Motivation: There is a need to understand urban socioeconomic conditions through visual data for sustainable development and policy planning.

Method: Constructed a multi-modal dataset covering 17 cities and 6 domains, defined 11 prediction tasks, and used three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression to benchmark 17 state-of-the-art LLVMs.

Result: LLVMs demonstrate promising perceptual and reasoning capabilities but still have limitations in predicting urban socioeconomic indicators.

Conclusion: CityLens provides a framework to diagnose these limitations and guide future research in using LLVMs for understanding urban socioeconomic patterns.

Abstract: Understanding urban socioeconomic conditions through visual data is a
challenging yet essential task for sustainable urban development and policy
planning. In this work, we introduce $\textbf{CityLens}$, a comprehensive
benchmark designed to evaluate the capabilities of large language-vision models
(LLVMs) in predicting socioeconomic indicators from satellite and street view
imagery. We construct a multi-modal dataset covering a total of 17 globally
distributed cities, spanning 6 key domains: economy, education, crime,
transport, health, and environment, reflecting the multifaceted nature of urban
life. Based on this dataset, we define 11 prediction tasks and utilize three
evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,
and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across
these tasks. Our results reveal that while LLVMs demonstrate promising
perceptual and reasoning capabilities, they still exhibit limitations in
predicting urban socioeconomic indicators. CityLens provides a unified
framework for diagnosing these limitations and guiding future efforts in using
LLVMs to understand and predict urban socioeconomic patterns. Our codes and
datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.

</details>


### [22] [A "Wenlu" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge](https://arxiv.org/abs/2506.00570)
*Liang Geng*

Main category: cs.AI

TL;DR: 提出了一种名为``Wenlu"的多模态认知和具身决策大脑系统，该系统在多模态处理、隐私安全、端到端硬件控制代码生成、自我学习和可持续更新方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在各行各业中的快速渗透，在构建下一代智能核心时，关键挑战在于有效地将基础模型的语言理解能力与特定领域的知识库相结合，应用于复杂的现实场景中。

Method: 引入了受大脑启发的记忆标记和重播机制，将用户私有数据、行业特定知识和通用语言模型无缝集成，提供精确高效的多模态服务，如企业决策支持、医疗分析、自动驾驶、机器人控制等。

Result: 相比现有解决方案，``Wenlu"在多模态处理、隐私保护、端到端硬件控制代码生成、自我学习和持续更新等方面表现出显著优势。

Conclusion: ``Wenlu"为构建下一代智能核心奠定了坚实的基础。

Abstract: With the rapid penetration of artificial intelligence across industries and
scenarios, a key challenge in building the next-generation intelligent core
lies in effectively integrating the language understanding capabilities of
foundation models with domain-specific knowledge bases in complex real-world
applications. This paper proposes a multimodal cognition and embodied
decision-making brain system, ``Wenlu", designed to enable secure fusion of
private knowledge and public models, unified processing of multimodal data such
as images and speech, and closed-loop decision-making from cognition to
automatic generation of hardware-level code. The system introduces a
brain-inspired memory tagging and replay mechanism, seamlessly integrating
user-private data, industry-specific knowledge, and general-purpose language
models. It provides precise and efficient multimodal services for enterprise
decision support, medical analysis, autonomous driving, robotic control, and
more. Compared with existing solutions, ``Wenlu" demonstrates significant
advantages in multimodal processing, privacy security, end-to-end hardware
control code generation, self-learning, and sustainable updates, thus laying a
solid foundation for constructing the next-generation intelligent core.

</details>


### [23] [Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577)
*Yufa Zhou,Shaobo Wang,Xingyu Dong,Xiangqi Jin,Yifang Chen,Yue Min,Kexin Yang,Xingzhang Ren,Dayiheng Liu,Linfeng Zhang*

Main category: cs.AI

TL;DR: This paper explores if post-training techniques (SFT and RLVR) can generalize to multi-agent scenarios using economic reasoning as a testbed, introducing Recon, a 7B-parameter LLM. Evaluation shows improvements in structured reasoning and economic rationality.


<details>
  <summary>Details</summary>
Motivation: Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) is challenging due to complex reward modeling, dynamic agent interactions, and demanding generalization requirements.

Method: The paper uses economic reasoning as a testbed and introduces Recon, a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems using Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR).

Result: Evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality.

Conclusion: Domain-aligned post-training enhances reasoning and agent alignment, highlighting the roles of SFT and RL in shaping model behavior.

Abstract: Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)
remains challenging due to intricate reward modeling, dynamic agent
interactions, and demanding generalization requirements. This paper explores
whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and
Reinforcement Learning with Verifiable Rewards (RLVR), can effectively
$\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a
testbed, leveraging its strong foundations in mathematics and game theory, its
demand for structured analytical reasoning, and its relevance to real-world
applications such as market design, resource allocation, and policy analysis.
We introduce $\textbf{Recon}$ ($\textbf{R}$easoning like an
$\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a
hand-curated dataset of 2,100 high-quality economic reasoning problems.
Comprehensive evaluation on economic reasoning benchmarks and multi-agent games
reveals clear improvements in structured reasoning and economic rationality.
These results underscore the promise of domain-aligned post-training for
enhancing reasoning and agent alignment, shedding light on the roles of SFT and
RL in shaping model behavior. Code is available at
https://github.com/MasterZhou1/Recon .

</details>


### [24] [Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs](https://arxiv.org/abs/2506.00582)
*Chenjun Xu,Bingbing Wen,Bin Han,Robert Wolfe,Lucy Lu Wang,Bill Howe*

Main category: cs.AI

TL;DR: This paper explores how three LLMs perform on QA tasks of varying difficulty, finding that they exhibit different patterns of overconfidence compared to humans. It proposes a method called Answer-Free Confidence Estimation (AFCE) to improve confidence calibration and model interpretability.


<details>
  <summary>Details</summary>
Motivation: To understand the differences in overconfidence patterns between humans and LLMs, and to develop a method for improving confidence calibration and interpretability in LLMs.

Method: The study examines three LLMs on a range of QA tasks with varying difficulty. The models are prompted to answer based on different personas, and their confidence estimations are analyzed. AFCE, a self-assessment method involving two stages of prompting, is proposed.

Result: Experiments on MMLU and GPQA datasets show that AFCE significantly reduces overconfidence in LLMs and makes them more sensitive to task difficulty, similar to human patterns.

Conclusion: AFCE is an effective method for improving confidence calibration and interpretability in LLMs, making their behavior more aligned with human-like sensitivity to task difficulty.

Abstract: Psychology research has shown that humans are poor at estimating their
performance on tasks, tending towards underconfidence on easy tasks and
overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct,
Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and
show that models exhibit subtle differences from human patterns of
overconfidence: less sensitive to task difficulty, and when prompted to answer
based on different personas -- e.g., expert vs layman, or different race,
gender, and ages -- the models will respond with stereotypically biased
confidence estimations even though their underlying answer accuracy remains the
same. Based on these observations, we propose Answer-Free Confidence Estimation
(AFCE) to improve confidence calibration and LLM interpretability in these
settings. AFCE is a self-assessment method that employs two stages of
prompting, first eliciting only confidence scores on questions, then asking
separately for the answer. Experiments on the MMLU and GPQA datasets spanning
subjects and difficulty show that this separation of tasks significantly
reduces overconfidence and delivers more human-like sensitivity to task
difficulty.

</details>


### [25] [RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents](https://arxiv.org/abs/2506.00618)
*Jingyi Yang,Shuai Shao,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: This paper presents RiOSWorld, a benchmark for evaluating safety risks of MLLM-based computer-use agents in real-world scenarios. Current agents face significant risks, emphasizing the need for safety alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of comprehensive evaluation methods for the safety risks of MLLM-based computer-use agents in realistic interactive environments with diverse risk types.

Method: Introduced RiOSWorld, a benchmark containing 492 risky tasks across various computer applications. Risks are categorized into user-originated and environmental risks, evaluated from risk goal intention and completion perspectives.

Result: Experiments show that current computer-use agents encounter substantial safety risks in real-world scenarios.

Conclusion: The findings stress the necessity and urgency of safety alignment for developing trustworthy computer-use agents.

Abstract: With the rapid development of multimodal large language models (MLLMs), they
are increasingly deployed as autonomous computer-use agents capable of
accomplishing complex computer tasks. However, a pressing issue arises: Can the
safety risk principles designed and aligned for general MLLMs in dialogue
scenarios be effectively transferred to real-world computer-use scenarios?
Existing research on evaluating the safety risks of MLLM-based computer-use
agents suffers from several limitations: it either lacks realistic interactive
environments, or narrowly focuses on one or a few specific risk types. These
limitations ignore the complexity, variability, and diversity of real-world
environments, thereby restricting comprehensive risk evaluation for
computer-use agents. To this end, we introduce \textbf{RiOSWorld}, a benchmark
designed to evaluate the potential risks of MLLM-based agents during real-world
computer manipulations. Our benchmark includes 492 risky tasks spanning various
computer applications, involving web, social media, multimedia, os, email, and
office software. We categorize these risks into two major classes based on
their risk source: (i) User-originated risks and (ii) Environmental risks. For
the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal
intention and (ii) Risk goal completion. Extensive experiments with multimodal
agents on \textbf{RiOSWorld} demonstrate that current computer-use agents
confront significant safety risks in real-world scenarios. Our findings
highlight the necessity and urgency of safety alignment for computer-use agents
in real-world computer manipulation, providing valuable insights for developing
trustworthy computer-use agents. Our benchmark is publicly available at
https://yjyddq.github.io/RiOSWorld.github.io/.

</details>


### [26] [AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents](https://arxiv.org/abs/2506.00641)
*Hanjun Luo,Shenyu Dai,Chiming Ni,Xinfeng Li,Guibin Zhang,Kun Wang,Tongliang Liu,Hanan Salam*

Main category: cs.AI

TL;DR: The paper introduces \sys, a memory-augmented reasoning framework that enhances LLM evaluators' ability to assess safety and security risks. It also presents \data, a benchmark for evaluating LLM-based evaluators.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing rule-based or LLM-based evaluators in detecting dangers, understanding subtle meanings, recognizing compounding issues, and dealing with unclear rules.

Method: \sys constructs experiential memory by extracting semantic features and generating reasoning traces from past interactions. It uses a multi-stage, context-aware retrieval-augmented generation process to retrieve relevant experiences for new cases. \data is a benchmark with 2293 annotated interaction records covering 15 risk types across 29 application scenarios, using Strict and Lenient judgment standards.

Result: Experiments show that \sys improves LLM evaluation performance consistently across all benchmarks and achieves human-level accuracy.

Conclusion: \sys sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, offering a reliable evaluation method.

Abstract: Despite the rapid advancement of LLM-based agents, the reliable evaluation of
their safety and security remains a significant challenge. Existing rule-based
or LLM-based evaluators often miss dangers in agents' step-by-step actions,
overlook subtle meanings, fail to see how small issues compound, and get
confused by unclear safety or security rules. To overcome this evaluation
crisis, we introduce \sys, a universal, training-free, memory-augmented
reasoning framework that empowers LLM evaluators to emulate human expert
evaluators. \sys constructs an experiential memory by having an LLM adaptively
extract structured semantic features (e.g., scenario, risk, behavior) and
generate associated chain-of-thought reasoning traces for past interactions. A
multi-stage, context-aware retrieval-augmented generation process then
dynamically retrieves the most relevant reasoning experiences to guide the LLM
evaluator's assessment of new cases. Moreover, we developed \data, the first
benchmark designed to check how well LLM-based evaluators can spot both safety
risks and security threats. \data comprises \textbf{2293} meticulously
annotated interaction records, covering \textbf{15} risk types across
\textbf{29} application scenarios. A key feature of \data is its nuanced
approach to ambiguous risk situations, employing ``Strict'' and ``Lenient''
judgment standards. Experiments demonstrate that \sys not only consistently
improves the evaluation performance of LLMs across all benchmarks but also sets
a new state-of-the-art in LLM-as-a-judge for agent safety and security,
achieving human-level accuracy. Our work is openly openly accessible.

</details>


### [27] [OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases](https://arxiv.org/abs/2506.00664)
*Yash Tiwari,Owais Ahmad Lone,Mayukha Pal*

Main category: cs.AI

TL;DR: OntoRAG is an automated pipeline designed to derive ontologies from unstructured knowledge bases, focusing on electrical relay documents. It integrates advanced techniques such as web scraping, PDF parsing, information extraction, and ontology creation, leveraging LLMs and graph based methods. Experimental results show its effectiveness with a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAG's best configuration.


<details>
  <summary>Details</summary>
Motivation: Traditional ontology creation is time consuming, error prone, and impractical for large dynamic knowledge domains.

Method: The method involves web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation using LLMs and graph based methods.

Result: OntoRAG outperforms conventional Retrieval Augmented Generation (RAG) and GraphRAG approaches in comprehensiveness and diversity, achieving a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAG's best configuration.

Conclusion: This work addresses the critical challenge of automating ontology creation, advancing the vision of the semantic web.

Abstract: Ontologies are pivotal for structuring knowledge bases to enhance question
answering (QA) systems powered by Large Language Models (LLMs). However,
traditional ontology creation relies on manual efforts by domain experts, a
process that is time intensive, error prone, and impractical for large, dynamic
knowledge domains. This paper introduces OntoRAG, an automated pipeline
designed to derive ontologies from unstructured knowledge bases, with a focus
on electrical relay documents. OntoRAG integrates advanced techniques,
including web scraping, PDF parsing, hybrid chunking, information extraction,
knowledge graph construction, and ontology creation, to transform unstructured
data into a queryable ontology. By leveraging LLMs and graph based methods,
OntoRAG enhances global sensemaking capabilities, outperforming conventional
Retrieval Augmented Generation (RAG) and GraphRAG approaches in
comprehensiveness and diversity. Experimental results demonstrate OntoRAGs
effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG
and 75% against GraphRAGs best configuration. This work addresses the critical
challenge of automating ontology creation, advancing the vision of the semantic
web.

</details>


### [28] [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)
*Yongkang Xiao,Sinian Zhang,Yi Dai,Huixue Zhou,Jue Hou,Jie Ding,Rui Zhang*

Main category: cs.AI

TL;DR: DrKGC is a new method for Knowledge Graph Completion that uses dynamic subgraph retrieval and LLMs to improve performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Knowledge Graph Completion using large language models encode graph context in textual form, which does not fully exploit the potential of LLMs for understanding graph structures.

Method: DrKGC learns structural embeddings and logical rules within the knowledge graph using a lightweight model training strategy. It then applies a bottom-up graph retrieval method to extract a subgraph guided by these rules. A GCN adapter enhances the structural embeddings using the retrieved subgraph, which are integrated into the prompt for effective LLM fine-tuning.

Result: DrKGC shows superior performance on two general domain benchmark datasets and two biomedical datasets. A case study in the biomedical domain also demonstrates its interpretability and practical utility.

Conclusion: DrKGC addresses the limitation of current approaches by better exploiting the potential of LLMs for perceiving and reasoning about graph structures, leading to improved performance and interpretability.

Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge
graphs (KGs) by leveraging existing triples and textual information. Recently,
generative large language models (LLMs) have been increasingly employed for
graph tasks. However, current approaches typically encode graph context in
textual form, which fails to fully exploit the potential of LLMs for perceiving
and reasoning about graph structures. To address this limitation, we propose
DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph
Completion). DrKGC employs a flexible lightweight model training strategy to
learn structural embeddings and logical rules within the KG. It then leverages
a novel bottom-up graph retrieval method to extract a subgraph for each query
guided by the learned rules. Finally, a graph convolutional network (GCN)
adapter uses the retrieved subgraph to enhance the structural embeddings, which
are then integrated into the prompt for effective LLM fine-tuning. Experimental
results on two general domain benchmark datasets and two biomedical datasets
demonstrate the superior performance of DrKGC. Furthermore, a realistic case
study in the biomedical domain highlights its interpretability and practical
utility.

</details>


### [29] [Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?](https://arxiv.org/abs/2506.00751)
*Zhuojun Gu,Quan Wang,Shuchu Han*

Main category: cs.AI

TL;DR: 近期大型语言模型（LLMs）的进步强调了将其行为与人类价值观对齐的必要性。本文研究并提出了一种方法来测量LLMs中声明偏好和揭示偏好的偏差，通过一系列二元选择提示符进行实验，并使用KL散度等指标量化偏差。结果表明，即使是微小的提示格式变化也可能改变LLMs的选择。这项研究对于将LLMs整合到与人类直接交互的服务中具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的行为需要与人类价值观保持一致。然而，LLMs可能存在声明偏好（即报告的与普遍原则的一致性）和揭示偏好（从情境化场景中的决策推断出的偏好）之间的潜在偏差。这种偏差对LLMs的可解释性、可靠性、推理透明度和道德部署提出了根本性的担忧。

Method: 研究人员创建了一个包含精心设计的提示符的丰富数据集，这些提示符以一系列强制的二元选择形式呈现给LLMs。然后比较LLMs对一般原则提示符（声明偏好）和情境化提示符（揭示偏好）的响应，使用如KL散度等指标量化偏差。该分析在不同类别的偏好和四个主流LLMs上重复进行。

Result: 研究发现，即使是在测试中的LLMs和偏好类别中，提示符格式的细微变化往往也能改变首选项。这表明对LLMs决策能力的理解和控制不足。

Conclusion: 本研究强调了在将LLMs集成到服务中时，了解和识别此类偏差的重要性，特别是在那些与人类直接交互且道德、公平和社会责任至关重要的服务中。此外，在设想LLMs执行自主代理任务时，意识到这种偏差也将至关重要，因为不可能对所有LLMs的中间决策步骤进行持续的人工评估。

Abstract: Recent advances in Large Language Models (LLMs) highlight the need to align
their behaviors with human values. A critical, yet understudied, issue is the
potential divergence between an LLM's stated preferences (its reported
alignment with general principles) and its revealed preferences (inferred from
decisions in contextualized scenarios). Such deviations raise fundamental
concerns for the interpretability, trustworthiness, reasoning transparency, and
ethical deployment of LLMs, particularly in high-stakes applications. This work
formally defines and proposes a method to measure this preference deviation. We
investigate how LLMs may activate different guiding principles in specific
contexts, leading to choices that diverge from previously stated general
principles. Our approach involves crafting a rich dataset of well-designed
prompts as a series of forced binary choices and presenting them to LLMs. We
compare LLM responses to general principle prompts stated preference with LLM
responses to contextualized prompts revealed preference, using metrics like KL
divergence to quantify the deviation. We repeat the analysis across different
categories of preferences and on four mainstream LLMs and find that a minor
change in prompt format can often pivot the preferred choice regardless of the
preference categories and LLMs in the test. This prevalent phenomenon
highlights the lack of understanding and control of the LLM decision-making
competence. Our study will be crucial for integrating LLMs into services,
especially those that interact directly with humans, where morality, fairness,
and social responsibilities are crucial dimensions. Furthermore, identifying or
being aware of such deviation will be critically important as LLMs are
increasingly envisioned for autonomous agentic tasks where continuous human
evaluation of all LLMs' intermediary decision-making steps is impossible.

</details>


### [30] [HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset](https://arxiv.org/abs/2506.00765)
*Shengkun Wang,Yanshen Sun,Fanglan Chen,Linhan Wang,Naren Ramakrishnan,Chang-Tien Lu,Yinlin Chen*

Main category: cs.AI

TL;DR: The paper presents HouseTS, a large-scale dataset for house-price forecasting with extensive spatiotemporal data and contextual information. It evaluates 14 models to establish performance baselines and demonstrates its value through a multimodal case study using vision language models.


<details>
  <summary>Details</summary>
Motivation: Accurate house-price forecasting is crucial for investors, planners, and researchers, but there is a lack of reproducible benchmarks with sufficient spatiotemporal depth and contextual richness for long horizon prediction.

Method: Introduced HouseTS - a large scale, multimodal dataset covering monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in 30 major U.S. metropolitan areas. Evaluated 14 models including classical statistical approaches, deep neural networks (DNNs), and pretrained time-series foundation models. Demonstrated the value of HouseTS in a multimodal case study using vision language model.

Result: HouseTS provides standardized performance baselines for house-price forecasting and enables interpretable, grounded insights into urban evolution through multimodal analysis.

Conclusion: HouseTS is openly maintained on GitHub with all preprocessing pipelines, benchmark code, and documentation to ensure full reproducibility and easy adoption.

Abstract: Accurate house-price forecasting is essential for investors, planners, and
researchers. However, reproducible benchmarks with sufficient spatiotemporal
depth and contextual richness for long horizon prediction remain scarce. To
address this, we introduce HouseTS a large scale, multimodal dataset covering
monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in
30 major U.S. metropolitan areas. The dataset includes over 890K records,
enriched with points of Interest (POI), socioeconomic indicators, and detailed
real estate metrics. To establish standardized performance baselines, we
evaluate 14 models, spanning classical statistical approaches, deep neural
networks (DNNs), and pretrained time-series foundation models. We further
demonstrate the value of HouseTS in a multimodal case study, where a vision
language model extracts structured textual descriptions of geographic change
from time stamped satellite imagery. This enables interpretable, grounded
insights into urban evolution. HouseTS is hosted on Kaggle, while all
preprocessing pipelines, benchmark code, and documentation are openly
maintained on GitHub to ensure full reproducibility and easy adoption.

</details>


### [31] [Do not Abstain! Identify and Solve the Uncertainty](https://arxiv.org/abs/2506.00780)
*Jingyu Liu,Jingquan Peng,xiaopeng Wu,Xubin Li,Tiezheng Ge,Bo Zheng,Yong Liu*

Main category: cs.AI

TL;DR: This paper introduces ConfuseBench, a benchmark to evaluate and improve LLMs' ability to recognize and address uncertainty. It identifies three types of uncertainties and proposes an approach using context-aware inquiries and InteractDPO training method.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often exhibit overconfidence in uncertain scenarios and existing solutions rely on evasive responses rather than addressing the uncertainty.

Method: The paper introduces ConfuseBench, a benchmark focusing on three types of uncertainty: document scarcity, limited capability, and query ambiguity. They generate context-aware inquiries to highlight confusing aspects and use InteractDPO for on-policy training.

Result: Experiments with ConfuseBench show that current LLMs struggle to identify and solve uncertainty issues. The proposed method demonstrates efficacy in generating better inquiries.

Conclusion: ConfuseBench provides a systematic way to investigate and improve LLMs' ability to recognize and address uncertainty sources.

Abstract: Despite the widespread application of Large Language Models (LLMs) across
various domains, they frequently exhibit overconfidence when encountering
uncertain scenarios, yet existing solutions primarily rely on evasive responses
(e.g., "I don't know") overlooks the opportunity of identifying and addressing
the uncertainty to generate more satisfactory responses. To systematically
investigate and improve LLMs' ability of recognizing and addressing the source
of uncertainty, we introduce \textbf{ConfuseBench}, a benchmark mainly focus on
three types of uncertainty: document scarcity, limited capability, and query
ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to
accurately identify the root cause of uncertainty and solve it. They prefer to
attribute uncertainty to query ambiguity while overlooking capability
limitations, especially for those weaker models. To tackle this challenge, we
first generate context-aware inquiries that highlight the confusing aspect of
the original query. Then we judge the source of uncertainty based on the
uniqueness of the inquiry's answer. Further we use an on-policy training
method, InteractDPO to generate better inquiries. Experimental results
demonstrate the efficacy of our approach.

</details>


### [32] [CoP: Agentic Red-teaming for Large Language Models using Composition of Principles](https://arxiv.org/abs/2506.00781)
*Chen Xiong,Pin-Yu Chen,Tsung-Yi Ho*

Main category: cs.AI

TL;DR: Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains. However, jailbreak attacks are becoming an urgent concern. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework.


<details>
  <summary>Details</summary>
Motivation: Jailbreak attacks on LLMs are becoming an urgent concern and current red-teaming methods are not sufficient.

Method: The CoP framework provides a unified and extensible framework where human users provide red-teaming principles as instructions to an AI agent to automatically generate effective red-teaming strategies and jailbreak prompts.

Result: When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.

Conclusion: This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework which can reveal unprecedented safety risks.

Abstract: Recent advances in Large Language Models (LLMs) have spurred transformative
applications in various domains, ranging from open-source to proprietary LLMs.
However, jailbreak attacks, which aim to break safety alignment and user
compliance by tricking the target LLMs into answering harmful and risky
responses, are becoming an urgent concern. The practice of red-teaming for LLMs
is to proactively explore potential risks and error-prone instances before the
release of frontier AI technology. This paper proposes an agentic workflow to
automate and scale the red-teaming process of LLMs through the
Composition-of-Principles (CoP) framework, where human users provide a set of
red-teaming principles as instructions to an AI agent to automatically
orchestrate effective red-teaming strategies and generate jailbreak prompts.
Distinct from existing red-teaming methods, our CoP framework provides a
unified and extensible framework to encompass and orchestrate human-provided
red-teaming principles to enable the automated discovery of new red-teaming
strategies. When tested against leading LLMs, CoP reveals unprecedented safety
risks by finding novel jailbreak prompts and improving the best-known
single-turn attack success rate by up to 19.0 times.

</details>


### [33] [Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.00782)
*Weiyang Guo,Zesheng Shi,Zhuo Li,Yequan Wang,Xuebo Liu,Wenya Wang,Fangming Liu,Min Zhang,Jing Li*

Main category: cs.AI

TL;DR: In this paper, researchers introduce a new automated red teaming framework for large language models (LLMs) using reinforcement learning to create more effective and diverse attack prompts. It consists of three stages: Cold Start, Warm-up Exploration, and Enhanced Jailbreak. Experiments show that the proposed method outperforms existing methods in balancing diversity and effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the growing importance of ensuring safety in LLMs and preventing harmful outputs. Current automated red teaming methods fail to effectively balance the diversity and effectiveness of attack prompts.

Method: The method involves a novel automated red teaming training framework named \ourapproach which uses reinforcement learning. This includes three stages: Cold Start where the model is fine-tuned on a jailbreak dataset; Warm-up Exploration where the model is trained with diversity and consistency as reward signals; and Enhanced Jailbreak where progressive rewards enhance the performance.

Result: Experiments conducted on various LLMs indicate that \ourapproach successfully balances the diversity and effectiveness of jailbreak prompts better than existing methods. This significantly improves the efficiency of red team exploration.

Conclusion: This work contributes a new perspective on automated red teaming by providing an efficient framework that balances prompt diversity and effectiveness, thus enhancing the security testing process of LLMs.

Abstract: As large language models (LLMs) grow in power and influence, ensuring their
safety and preventing harmful output becomes critical. Automated red teaming
serves as a tool to detect security vulnerabilities in LLMs without manual
labor. However, most existing methods struggle to balance the effectiveness and
diversity of red-team generated attack prompts. To address this challenge, we
propose \ourapproach, a novel automated red teaming training framework that
utilizes reinforcement learning to explore and generate more effective attack
prompts while balancing their diversity. Specifically, it consists of three
training stages: (1) Cold Start: The red team model is supervised and
fine-tuned on a jailbreak dataset obtained through imitation learning. (2)
Warm-up Exploration: The model is trained in jailbreak instruction following
and exploration, using diversity and consistency as reward signals. (3)
Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually
enhance the jailbreak performance of the red-team model. Extensive experiments
on a variety of LLMs show that \ourapproach effectively balances the diversity
and effectiveness of jailbreak prompts compared to existing methods. Our work
significantly improves the efficiency of red team exploration and provides a
new perspective on automated red teaming.

</details>


### [34] [GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning](https://arxiv.org/abs/2506.00785)
*Sahiti Yerramilli,Nilay Pande,Rynaa Grover,Jayant Sravan Tamarapalli*

Main category: cs.AI

TL;DR: GeoChain is a large-scale benchmark for evaluating geographic reasoning in MLLMs, revealing consistent challenges and offering a robust diagnostic methodology.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark for assessing step-by-step geographic reasoning capabilities of multimodal large language models.

Method: Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought question sequence across four reasoning categories - visual, spatial, cultural, and precise geolocation, annotated by difficulty.

Result: Contemporary MLLMs exhibit weaknesses in visual grounding, display erratic reasoning, and struggle with accurate localization, especially as reasoning complexity increases.

Conclusion: GeoChain provides a robust diagnostic methodology essential for advancing complex geographic reasoning within MLLMs.

Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating
step-by-step geographic reasoning in multimodal large language models (MLLMs).
Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each
image with a 21-step chain-of-thought (CoT) question sequence (over 30 million
Q&A pairs). These sequences guide models from coarse attributes to fine-grained
localization across four reasoning categories - visual, spatial, cultural, and
precise geolocation - annotated by difficulty. Images are also enriched with
semantic segmentation (150 classes) and a visual locatability score. Our
benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5
variants) on a diverse 2,088-image subset reveals consistent challenges: models
frequently exhibit weaknesses in visual grounding, display erratic reasoning,
and struggle to achieve accurate localization, especially as the reasoning
complexity escalates. GeoChain offers a robust diagnostic methodology, critical
for fostering significant advancements in complex geographic reasoning within
MLLMs.

</details>


### [35] [Predicting Empirical AI Research Outcomes with Language Models](https://arxiv.org/abs/2506.00794)
*Jiaxin Wen,Chenglei Si,Yueh-han Chen,He He,Shi Feng*

Main category: cs.AI

TL;DR: The paper presents a benchmark for predicting the success of AI research ideas, comparing language models with human experts. A system combining fine-tuned GPT-4.1 and a paper retrieval agent outperforms human experts in the NLP domain and achieves high accuracy on the full test set.


<details>
  <summary>Details</summary>
Motivation: Predicting the success of AI research ideas is crucial to accelerate empirical AI research, but it requires substantial human labor and compute resources.

Method: A benchmark is built for predicting the success of research ideas. The system combines a fine-tuned GPT-4.1 with a paper retrieval agent and is compared with 25 human experts.

Result: The system beats human experts in the NLP domain (64.4% vs. 48.9%) and achieves 77% accuracy on the full test set, while off-the-shelf LMs perform no better than random guessing. It also performs well on unpublished novel ideas (63.6% accuracy).

Conclusion: This work outlines a promising new direction for language models to accelerate empirical AI research by predicting the success of research ideas.

Abstract: Many promising-looking ideas in AI research fail to deliver, but their
validation takes substantial human labor and compute. Predicting an idea's
chance of success is thus crucial for accelerating empirical AI research, a
skill that even expert researchers can only acquire through substantial
experience. We build the first benchmark for this task and compare LMs with
human experts. Concretely, given two research ideas (e.g., two jailbreaking
methods), we aim to predict which will perform better on a set of benchmarks.
We scrape ideas and experimental results from conference papers, yielding 1,585
human-verified idea pairs published after our base model's cut-off date for
testing, and 6,000 pairs for training. We then develop a system that combines a
fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human
experts to compare with. In the NLP domain, our system beats human experts by a
large margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77%
accuracy, while off-the-shelf frontier LMs like o3 perform no better than
random guessing, even with the same retrieval augmentation. We verify that our
system does not exploit superficial features like idea complexity through
extensive human-written and LM-designed robustness tests. Finally, we evaluate
our system on unpublished novel ideas, including ideas generated by an AI
ideation agent. Our system achieves 63.6% accuracy, demonstrating its potential
as a reward model for improving idea generation models. Altogether, our results
outline a promising new direction for LMs to accelerate empirical AI research.

</details>


### [36] [Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision](https://arxiv.org/abs/2506.00807)
*Jiahui Zhou,Dan Li,Lin Li,Zhuomin Chen,Shunyu Wu,Haozheng Ye,Jian Lou,Costas J. Spanos*

Main category: cs.AI

TL;DR: This paper introduces ReasonTSC, a framework that leverages LLM reasoning for time series classification through multi-turn reasoning and fused decision-making strategies. It outperforms existing baselines and can correct false predictions from plug-in models.


<details>
  <summary>Details</summary>
Motivation: To effectively use the reasoning capabilities of large language models (LLMs) in time series classification (TSC), as straightforward adaptations of text-domain reasoning techniques have limited efficacy and advancements in LLM reasoning for TSC remain under-explored.

Method: ReasonTSC first prompts the LLM to consider essential characteristics of time series data, then integrates predictions and confidence scores from plug-in classifiers as in-context examples, and finally guides the LLM through a structured multi-turn reasoning process involving evaluation, backtracking, and comparison before final classification.

Result: ReasonTSC consistently outperforms both existing time series reasoning baselines and plug-in models, and is capable of identifying and correcting false predictions from plug-in models.

Conclusion: ReasonTSC provides an effective way to leverage LLM reasoning for time series classification, demonstrating superior performance compared to existing methods.

Abstract: The reasoning capabilities of large language models (LLMs) have significantly
advanced their performance by enabling in-depth understanding of diverse tasks.
With growing interest in applying LLMs to the time series domain, this has
proven nontrivial, as evidenced by the limited efficacy of straightforwardly
adapting text-domain reasoning techniques. Although recent work has shown
promise in several time series tasks, further leveraging advancements in LLM
reasoning remains under-explored for time series classification (TSC) tasks,
despite their prevalence and significance in many real-world applications. In
this paper, we propose ReasonTSC, a novel framework designed to effectively
leverage LLM reasoning for time series classification through both a multi-turn
reasoning and a fused decision-making strategy tailored to TSC. Rather than
straightforwardly applying existing reasoning techniques or relying solely on
LLMs' built-in reasoning capabilities, ReasonTSC first steers the model to
think over the essential characteristics of time series data. Next, it
integrates predictions and confidence scores from plug-in classifiers, e.g.,
domain-specific time series models, as in-context examples. Finally, ReasonTSC
guides the LLM through a structured reasoning process: it evaluates the initial
assessment, backtracks to consider alternative hypotheses, and compares their
merits before arriving at a final classification. Extensive experiments and
systematic ablation studies demonstrate that ReasonTSC consistently outperforms
both existing time series reasoning baselines and plug-in models, and is even
capable of identifying and correcting plug-in models' false predictions.

</details>


### [37] [SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning](https://arxiv.org/abs/2506.00835)
*Jisheng Dang,Yizhou Zhang,Hao Ye,Teng Wang,Siming Chen,Huicheng Zheng,Yulan Guo,Jianhuang Lai,Bin Hu*

Main category: cs.AI

TL;DR: The paper proposes SynPO, a novel optimization method that leverages preference learning to improve fine-grained video captioning performance. It mitigates limitations of DPO, enhances language capability preservation, and improves training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for fine-grained video captioning struggle to capture subtle dynamics and detailed information. Direct Preference Optimization (DPO) has inherent limitations.

Method: Proposes a pipeline for constructing preference pairs using VLMs and LLMs assistance, and introduces Synergistic Preference Optimization (SynPO), which prevents negative preferences from dominating, preserves model's language capability, and eliminates the need for a reference model.

Result: SynPO consistently outperforms DPO variants in video captioning benchmarks (VDC, VDD, VATEX) and NLP tasks, with 20% improvement in training efficiency.

Conclusion: SynPO offers significant advantages over DPO in fine-grained video captioning and NLP tasks.

Abstract: Fine-grained video captioning aims to generate detailed, temporally coherent
descriptions of video content. However, existing methods struggle to capture
subtle video dynamics and rich detailed information. In this paper, we leverage
preference learning to enhance the performance of vision-language models in
fine-grained video captioning, while mitigating several limitations inherent to
direct preference optimization (DPO). First, we propose a pipeline for
constructing preference pairs that leverages the intrinsic properties of VLMs
along with partial assistance from large language models, achieving an optimal
balance between cost and data quality. Second, we propose Synergistic
Preference Optimization (SynPO), a novel optimization method offering
significant advantages over DPO and its variants. SynPO prevents negative
preferences from dominating the optimization, explicitly preserves the model's
language capability to avoid deviation of the optimization objective, and
improves training efficiency by eliminating the need for the reference model.
We extensively evaluate SynPO not only on video captioning benchmarks (e.g.,
VDC, VDD, VATEX) but also across well-established NLP tasks, including general
language understanding and preference evaluation, using diverse pretrained
models. Results demonstrate that SynPO consistently outperforms DPO variants
while achieving 20\% improvement in training efficiency. Code is available at
https://github.com/longmalongma/SynPO

</details>


### [38] [MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book](https://arxiv.org/abs/2506.00855)
*Sau Lai Yip,Sunan He,Yuxiang Nie,Shu Pui Chan,Yilin Ye,Sum Ying Lam,Hao Chen*

Main category: cs.AI

TL;DR: The abstract introduces MedBookVQA, a multimodal benchmark from medical textbooks to evaluate general medical AI (GMAI) systems. It highlights capability gaps in GMAI and provides structured performance metrics.


<details>
  <summary>Details</summary>
Motivation: To address persistent healthcare challenges like workforce deficits and rising costs through general medical artificial intelligence (GMAI), and recognizing the lack of systematic evaluation benchmarks despite the potential of medical textbooks as knowledge sources.

Method: Proposed a standardized pipeline for extracting medical figures and aligning them with narratives from open-access medical textbooks. Generated 5,000 clinically relevant questions and used a multi-tier annotation system to categorize queries into various medical categories.

Result: Evaluated multiple MLLMs revealing significant performance disparities across task types and model categories, exposing critical capability gaps in current GMAI systems.

Conclusion: MedBookVQA serves as an essential evaluation tool establishing textbook-derived multimodal benchmarks as a critical paradigm for advancing clinical AI.

Abstract: The accelerating development of general medical artificial intelligence
(GMAI), powered by multimodal large language models (MLLMs), offers
transformative potential for addressing persistent healthcare challenges,
including workforce deficits and escalating costs. The parallel development of
systematic evaluation benchmarks emerges as a critical imperative to enable
performance assessment and provide technological guidance. Meanwhile, as an
invaluable knowledge source, the potential of medical textbooks for benchmark
development remains underexploited. Here, we present MedBookVQA, a systematic
and comprehensive multimodal benchmark derived from open-access medical
textbooks. To curate this benchmark, we propose a standardized pipeline for
automated extraction of medical figures while contextually aligning them with
corresponding medical narratives. Based on this curated data, we generate 5,000
clinically relevant questions spanning modality recognition, disease
classification, anatomical identification, symptom diagnosis, and surgical
procedures. A multi-tier annotation system categorizes queries through
hierarchical taxonomies encompassing medical imaging modalities (42
categories), body anatomies (125 structures), and clinical specialties (31
departments), enabling nuanced analysis across medical subdomains. We evaluate
a wide array of MLLMs, including proprietary, open-sourced, medical, and
reasoning models, revealing significant performance disparities across task
types and model categories. Our findings highlight critical capability gaps in
current GMAI systems while establishing textbook-derived multimodal benchmarks
as essential evaluation tools. MedBookVQA establishes textbook-derived
benchmarking as a critical paradigm for advancing clinical AI, exposing
limitations in GMAI systems while providing anatomically structured performance
metrics across specialties.

</details>


### [39] [GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints](https://arxiv.org/abs/2506.00865)
*Jiajun He,Jinyi Mi,Tomoki Toda*

Main category: cs.AI

TL;DR: The paper proposes a gated interactive attention mechanism and modality-invariant generator for multimodal emotion recognition, achieving better performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal emotion recognition has challenges in extracting modality-specific features and capturing cross-modal similarities due to distribution differences.

Method: A gated interactive attention mechanism is proposed to adaptively extract modality-specific features while enhancing emotional information through pairwise interactions. A modality-invariant generator is also introduced to learn modality-invariant representations and constrain domain shifts by aligning cross-modal similarities.

Result: Experiments on IEMOCAP demonstrate that the method outperforms state-of-the-art MER approaches, achieving WA 80.7% and UA 81.3%.

Conclusion: The proposed method effectively addresses the challenges in multimodal emotion recognition.

Abstract: Multimodal emotion recognition (MER) extracts emotions from multimodal data,
including visual, speech, and text inputs, playing a key role in human-computer
interaction. Attention-based fusion methods dominate MER research, achieving
strong classification performance. However, two key challenges remain:
effectively extracting modality-specific features and capturing cross-modal
similarities despite distribution differences caused by modality heterogeneity.
To address these, we propose a gated interactive attention mechanism to
adaptively extract modality-specific features while enhancing emotional
information through pairwise interactions. Additionally, we introduce a
modality-invariant generator to learn modality-invariant representations and
constrain domain shifts by aligning cross-modal similarities. Experiments on
IEMOCAP demonstrate that our method outperforms state-of-the-art MER
approaches, achieving WA 80.7% and UA 81.3%.

</details>


### [40] [Toward a Theory of Agents as Tool-Use Decision-Makers](https://arxiv.org/abs/2506.00886)
*Hongru Wang,Cheng Qian,Manling Li,Jiahao Qiu,Boyang Xue,Mengdi Wang,Heng Ji,Kam-Fai Wong*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) need a coherent epistemic framework to become truly autonomous agents. This involves treating internal reasoning and external actions as equivalent tools, aligning tool use with knowledge boundaries, and shifting from action-based to knowledge-driven design for more efficient, adaptive behavior.


<details>
  <summary>Details</summary>
Motivation: As LLMs evolve into more autonomous agents, there are unresolved questions regarding their decision-making processes and objectives. A coherent epistemic framework is needed to govern what they know, what they need to know, and how to acquire that knowledge efficiently.

Method: Propose a unified theory where internal reasoning and external actions are treated as equivalent epistemic tools, allowing agents to coordinate introspection and interaction systematically. Align the agent's tool use decision-making boundary with its knowledge boundary.

Result: This approach minimizes unnecessary tool use and maximizes epistemic efficiency, shifting the design of agents from mere action executors to knowledge-driven intelligence systems.

Conclusion: A principled path is offered toward building foundation agents capable of adaptive, efficient, and goal-directed behavior through a coherent epistemic framework.

Abstract: As Large Language Models (LLMs) evolve into increasingly autonomous agents,
fundamental questions about their epistemic foundations remain unresolved: What
defines an agent? How should it make decisions? And what objectives should
guide its behavior? In this position paper, we argue that true autonomy
requires agents to be grounded in a coherent epistemic framework that governs
what they know, what they need to know, and how to acquire that knowledge
efficiently. We propose a unified theory that treats internal reasoning and
external actions as equivalent epistemic tools, enabling agents to
systematically coordinate introspection and interaction. Building on this
framework, we advocate for aligning an agent's tool use decision-making
boundary with its knowledge boundary, thereby minimizing unnecessary tool use
and maximizing epistemic efficiency. This perspective shifts the design of
agents from mere action executors to knowledge-driven intelligence systems,
offering a principled path toward building foundation agents capable of
adaptive, efficient, and goal-directed behavior.

</details>


### [41] [Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models](https://arxiv.org/abs/2506.00911)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 现代语言模型部署常需平衡多个竞争目标，例如有用性与无害性、成本与准确性等。本文提出Conformal Arbitrage框架，通过学习数据驱动的阈值，在主要模型和保守模型（或人类专家）之间进行调解。该方法基于符合风险控制的校准，确保不良事件的发生频率不超过用户指定的配额。此外，它在API级别运行，无需访问模型logits或更新模型权重，可与现有的成本感知级联无缝集成。实验表明，Conformal Arbitrage方法在准确性和成本方面优于随机模型路由方法。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在实际应用中需要同时考虑多个相互竞争的目标，如有用性、无害性、成本、准确性等。然而，目前缺乏一种能够在这些目标之间有效权衡的方法，特别是在不需要修改模型内部结构的情况下。因此，需要一种新的框架来解决这一问题。

Method: Conformal Arbitrage框架通过引入一个数据驱动的阈值，在主要模型（Primary model）和保守模型（Guardian model）之间进行调解。主要模型专注于优化主要目标，而保守模型则与保护性目标对齐。阈值的校准基于符合风险控制，确保不良事件的发生频率不超过用户指定的配额。整个过程在API级别操作，无需访问模型logits或更新模型权重。

Result: 实验结果表明，Conformal Arbitrage方法能够绘制出有效的前沿曲线，使用户可以在定义一个目标的可接受性能水平的同时，最大化另一个目标的效用。与成本匹配的随机模型路由方法相比，该方法在准确性方面表现更优。

Conclusion: Conformal Arbitrage是一种实用且理论基础扎实的工具，适用于在广泛的竞争目标下，可靠且经济地部署大型语言模型。

Abstract: Modern language model deployments must often balance competing objectives,
for example, helpfulness versus harmlessness, cost versus accuracy, and reward
versus safety. We introduce Conformal Arbitrage, a post hoc framework that
learns a data driven threshold to mediate between a Primary model optimized for
a primary objective and a more conservative Guardian which could be another
model or a human domain expert aligned with a guardrail objective. The
threshold is calibrated with conformal risk control, yielding finite sample,
distribution free guarantees that the long run frequency of undesirable events,
such as factual errors or safety violations, does not exceed a user specified
quota. Because Conformal Arbitrage operates wholly at the API level, without
requiring access to model logits or updating model weights, it complements
weight based alignment techniques and integrates seamlessly with existing cost
aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier,
allowing users to define an acceptable performance level for one objective
while maximizing utility in another. We observe that our method outperforms, in
terms of accuracy, cost matched random routing between models. These properties
make Conformal Arbitrage a practical, theoretically grounded tool for
trustworthy and economical deployment of large language models across a broad
range of potentially competing objectives.

</details>


### [42] [Aligning VLM Assistants with Personalized Situated Cognition](https://arxiv.org/abs/2506.00930)
*Yongqi Li,Shen Zhou,Xiaohu Li,Xin Miao,Jintao Wen,Mayi Xu,Jianhao Chen,Birong Pan,Hankun Kang,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.AI

TL;DR: The paper introduces PCogAlignBench, a benchmark with 18k instances and 20 individuals to align Vision-language models (VLMs) assistants with personalized situated cognition, and PCogAlign, a framework that constructs a cognition-aware and action-based reward model for personalized alignment. Experiments show the reliability of the benchmark and the effectiveness of the framework.


<details>
  <summary>Details</summary>
Motivation: People from different backgrounds have varying perceptions and expectations even in similar situations, necessitating personalized alignment of VLMs for real-world assistance.

Method: Simplify the problem using the sociological concept of Role-Set, evaluate individuals' actions, construct the PCogAlignBench benchmark, and present the PCogAlign framework which builds a cognition-aware and action-based reward model.

Result: Experimental results and human evaluations confirm the reliability of PCogAlignBench and the effectiveness of PCogAlign.

Conclusion: The authors successfully developed a benchmark and framework for personalized alignment of VLMs, planning to open-source the materials.

Abstract: Vision-language models (VLMs) aligned with general human objectives, such as
being harmless and hallucination-free, have become valuable assistants of
humans in managing visual tasks. However, people with diversified backgrounds
have different cognition even in the same situation. Consequently, they may
have personalized expectations for VLM assistants. This highlights the urgent
need to align VLM assistants with personalized situated cognition for
real-world assistance. To study this problem, we first simplify it by
characterizing individuals based on the sociological concept of Role-Set. Then,
we propose to evaluate the individuals' actions to examine whether the
personalized alignment is achieved. Further, we construct a benchmark named
PCogAlignBench, which includes 18k instances and 20 individuals with different
Role-Sets. Finally, we present a framework called PCogAlign, which constructs a
cognition-aware and action-based reward model for personalized alignment.
Experimental results and human evaluations demonstrate the reliability of the
PCogAlignBench and the effectiveness of our proposed PCogAlign. We will
open-source the constructed benchmark and code at
https://github.com/NLPGM/PCogAlign.

</details>


### [43] [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)
*Youngmin Kim,Jiwan Chung,Jisoo Kim,Sunghyun Lee,Sangkyu Lee,Junhyeok Kim,Cheoljong Yang,Youngjae Yu*

Main category: cs.AI

TL;DR: MARS is a new multimodal language model that can understand and generate nonverbal cues like facial expressions and body language, using a large dataset called VENUS.


<details>
  <summary>Details</summary>
Motivation: Current large language models struggle to incorporate nonverbal communication elements, which limits their ability to create immersive conversational experiences.

Method: The researchers created VENUS, a large-scale dataset of annotated videos with aligned text, facial expressions, and body language. They then trained MARS on this data using a next-token prediction objective that combines text with vector-quantized nonverbal representations in a unified framework.

Result: Quantitative and qualitative analyses show that MARS can successfully generate both text and corresponding nonverbal languages based on conversational input.

Conclusion: MARS bridges the gap in conversational AI by effectively understanding and generating nonverbal cues alongside text.

Abstract: Nonverbal communication is integral to human interaction, with gestures,
facial expressions, and body language conveying critical aspects of intent and
emotion. However, existing large language models (LLMs) fail to effectively
incorporate these nonverbal elements, limiting their capacity to create fully
immersive conversational experiences. We introduce MARS, a multimodal language
model designed to understand and generate nonverbal cues alongside text,
bridging this gap in conversational AI. Our key innovation is VENUS, a
large-scale dataset comprising annotated videos with time-aligned text, facial
expressions, and body language. Leveraging VENUS, we train MARS with a
next-token prediction objective, combining text with vector-quantized nonverbal
representations to achieve multimodal understanding and generation within a
unified framework. Based on various analyses of the VENUS datasets, we validate
its substantial scale and high effectiveness. Our quantitative and qualitative
results demonstrate that MARS successfully generates text and nonverbal
languages, corresponding to conversational input.

</details>


### [44] [Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts](https://arxiv.org/abs/2506.00965)
*Fan Liu,Bikang Pan,Zhongyi Wang,Xi Yao,Xiaoying Tang,Jingya Wang,Ye Shi*

Main category: cs.AI

TL;DR: 在联邦学习中，现有的方法主要针对密集模型，无法充分利用MoE架构的稀疏性。为了解决这一问题，本文提出了FLEx框架，通过修剪全球MoE模型以保留每个客户端的一个专家，并使用自适应门机制将这些个性化的专家重新集成到预训练的MoE层中，从而实现高效个性化。实验表明，FLEx在多样化的非IID条件下优于现有的联邦基准。


<details>
  <summary>Details</summary>
Motivation: 当前的联邦学习方法主要针对密集模型设计，无法直接利用Mixture of Experts (MoE) 架构中的稀疏性。若将MoE模型视为密集网络处理，会导致通信开销和计算成本过高，削弱了个性化知识共享的潜力。

Method: 提出了一种新的联邦学习框架FLEx，该框架专门针对基于MoE的大型语言模型（LLMs）。FLEx通过修剪全局MoE模型，使每个客户端仅保留一个专家，并采用自适应门机制将这些个性化的专家重新整合到预训练的MoE层中。个性化的专家使用本地数据进行训练并存储在本地客户端上，而共享模块则在全球范围内聚合。

Result: 在不同的基于指令的数据集上的广泛评估表明，在非独立同分布（non-IID）条件下，FLEx的表现始终优于现有的联邦基准方法。

Conclusion: FLEx框架能够有效解决现有联邦学习方法在处理MoE模型时遇到的问题，提供了一种高效的个性化解决方案，并且在实验中表现出色。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a prominent strategy
for scaling large language models (LLMs), effectively leveraging sparse
activation and facilitating task-specific personalization. However, current
federated learning (FL) approaches are primarily designed for dense models,
making them unable to directly exploit the sparsity inherent in MoE
architectures. Treating MoE models as dense networks in federated scenarios
results in excessive communication overhead and computational costs,
undermining the potential for personalized knowledge sharing. To address these
challenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel
federated learning framework explicitly tailored for MoE-based LLMs. FLEx
efficiently personalizes by pruning the global MoE model to keep only one
expert per client, and employs an adaptive gating mechanism to reintegrate
these personalized experts into the pre-trained MoE layers, ensuring the
original backbone architecture remains unchanged. These personalized experts
are trained with local data and stored locally on each client, while the shared
modules are aggregated globally. Extensive evaluations on diverse
instruction-based datasets under non-IID conditions consistently demonstrate
that FLEx outperforms existing federated baselines. Our code is available at
https://anonymous.4open.science/r/FLEx-8F12.

</details>


### [45] [PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation](https://arxiv.org/abs/2506.00968)
*Linhan Xia,Mingzhan Yang,Guohui Yuan,Shengnan Tao,Yujing Qiu,Guo Yu,Kai Lei*

Main category: cs.AI

TL;DR: The paper proposes PolyBERT, a poly-encoder BERT-based model with batch contrastive learning for Word Sense Disambiguation (WSD), which balances local and global semantics and reduces computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing WSD approaches using BERT have limitations: they fail to balance token-level and sequence-level semantics, leading to insufficient semantic representation, and incorporate all possible senses during training, causing unnecessary computational costs.

Method: Introduce PolyBERT, a poly-encoder BERT-based model with multi-head attention mechanism to fuse token-level and sequence-level semantics. Use Batch Contrastive Learning (BCL) to reduce redundant training inputs by utilizing correct senses of other target words as negative samples.

Result: PolyBERT outperforms baseline WSD methods like Huang's GlossBERT and Blevins's BEM by 2% in F1-score. Also, PolyBERT with BCL reduces GPU hours by 37.6% compared to PolyBERT without BCL.

Conclusion: PolyBERT improves WSD performance by balancing local and global semantics and reducing computational cost through BCL.

Abstract: Mainstream Word Sense Disambiguation (WSD) approaches have employed BERT to
extract semantics from both context and definitions of senses to determine the
most suitable sense of a target word, achieving notable performance. However,
there are two limitations in these approaches. First, previous studies failed
to balance the representation of token-level (local) and sequence-level
(global) semantics during feature extraction, leading to insufficient semantic
representation and a performance bottleneck. Second, these approaches
incorporated all possible senses of each target word during the training phase,
leading to unnecessary computational costs. To overcome these limitations, this
paper introduces a poly-encoder BERT-based model with batch contrastive
learning for WSD, named PolyBERT. Compared with previous WSD methods, PolyBERT
has two improvements: (1) A poly-encoder with a multi-head attention mechanism
is utilized to fuse token-level (local) and sequence-level (global) semantics,
rather than focusing on just one. This approach enriches semantic
representation by balancing local and global semantics. (2) To avoid redundant
training inputs, Batch Contrastive Learning (BCL) is introduced. BCL utilizes
the correct senses of other target words in the same batch as negative samples
for the current target word, which reduces training inputs and computational
cost. The experimental results demonstrate that PolyBERT outperforms baseline
WSD methods such as Huang's GlossBERT and Blevins's BEM by 2\% in F1-score. In
addition, PolyBERT with BCL reduces GPU hours by 37.6\% compared with PolyBERT
without BCL.

</details>


### [46] [Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery](https://arxiv.org/abs/2506.00989)
*Buyun He,Xiaorui Jiang,Qi Wu,Hao Liu,Yingguang Yang,Yong Liao*

Main category: cs.AI

TL;DR: BotHP is a new framework that improves bot detection in social media by addressing interaction camouflage and enhancing generalization through heterophily-aware learning.


<details>
  <summary>Details</summary>
Motivation: Current graph-based methods for bot detection have limitations such as label reliance and poor generalization, especially when dealing with interaction camouflage and distributed deployment.

Method: BotHP uses a dual-encoder architecture with a graph-aware encoder for node commonality and a graph-agnostic encoder for node uniqueness. It also includes a prototype-guided cluster discovery task to identify dispersed bot collectives.

Result: Experiments on two real-world benchmarks show that BotHP enhances the performance of graph-based bot detectors, reduces label reliance, and improves generalization.

Conclusion: BotHP presents an effective solution to boost graph-based bot detectors by capturing both homophily and heterophily, and modeling global consistency of bot clusters.

Abstract: Detecting social media bots is essential for maintaining the security and
trustworthiness of social networks. While contemporary graph-based detection
methods demonstrate promising results, their practical application is limited
by label reliance and poor generalization capability across diverse
communities. Generative Graph Self-Supervised Learning (GSL) presents a
promising paradigm to overcome these limitations, yet existing approaches
predominantly follow the homophily assumption and fail to capture the global
patterns in the graph, which potentially diminishes their effectiveness when
facing the challenges of interaction camouflage and distributed deployment in
bot detection scenarios. To this end, we propose BotHP, a generative GSL
framework tailored to boost graph-based bot detectors through heterophily-aware
representation learning and prototype-guided cluster discovery. Specifically,
BotHP leverages a dual-encoder architecture, consisting of a graph-aware
encoder to capture node commonality and a graph-agnostic encoder to preserve
node uniqueness. This enables the simultaneous modeling of both homophily and
heterophily, effectively countering the interaction camouflage issue.
Additionally, BotHP incorporates a prototype-guided cluster discovery pretext
task to model the latent global consistency of bot clusters and identify
spatially dispersed yet semantically aligned bot collectives. Extensive
experiments on two real-world bot detection benchmarks demonstrate that BotHP
consistently boosts graph-based bot detectors, improving detection performance,
alleviating label reliance, and enhancing generalization capability.

</details>


### [47] [Higher-Order Responsibility](https://arxiv.org/abs/2506.01003)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: The paper explores whether higher-order responsibility up to degree $d$ can address the 'responsibility gap' in group decision-making, finding the associated problem to be $\Pi_{2d+1}$-complete.


<details>
  <summary>Details</summary>
Motivation: To resolve the issue of the 'responsibility gap' that arises when applying individual responsibility principles in a group decision-making context.

Method: Investigates the adequacy of higher-order responsibility up to degree $d$ for closing the responsibility gap and determines its computational complexity.

Result: It is determined that deciding if higher-order responsibility up to degree $d$ closes the responsibility gap is $\Pi_{2d+1}$-complete.

Conclusion: Higher-order responsibility provides a framework to potentially address responsibility gaps, but assessing this involves a highly complex computational problem.

Abstract: In ethics, individual responsibility is often defined through Frankfurt's
principle of alternative possibilities. This definition is not adequate in a
group decision-making setting because it often results in the lack of a
responsible party or "responsibility gap''. One of the existing approaches to
address this problem is to consider group responsibility. Another, recently
proposed, approach is "higher-order'' responsibility. The paper considers the
problem of deciding if higher-order responsibility up to degree $d$ is enough
to close the responsibility gap. The main technical result is that this problem
is $\Pi_{2d+1}$-complete.

</details>


### [48] [IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory](https://arxiv.org/abs/2506.01048)
*Wei Song,Zhenya Huang,Cheng Cheng,Weibo Gao,Bihan Xu,GuanHao Zhao,Fei Wang,Runze Wu*

Main category: cs.AI

TL;DR: IRT-Router是一个受项目反应理论（IRT）启发的多大语言模型（LLM）路由框架，能够有效将用户查询导向最适合的LLM，在性能和成本之间取得平衡。通过明确建模LLM能力和查询属性之间的关系，IRT-Router不仅实现了对响应性能的精确预测，还提供了可解释的见解，如LLM能力与查询难度。此外，基于语义相似性的在线查询预热技术增强了IRT-Router的在线泛化能力。实验表明，IRT-Router在效果和可解释性方面优于大多数基线方法，并且在冷启动场景中表现出色，证明了其在实际应用中的可靠性和实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自然语言任务中表现出色，但在选择最优LLM回应用户查询时，需要在性能和成本之间进行权衡。强大的模型虽然结果更好但成本高，而小模型虽然更具成本效益但能力不足。为解决这一权衡问题，提出了IRT-Router框架。

Method: IRT-Router框架受到项目反应理论（IRT）的启发，明确建模LLM能力和查询属性之间的关系，从而实现对响应性能的准确预测并提供可解释性见解。同时设计了一种基于语义相似性的在线查询预热技术，进一步提高IRT-Router的在线泛化能力。

Result: 在20个LLM和12个数据集上的广泛实验表明，IRT-Router在效果和可解释性方面优于大多数基线方法。特别是在冷启动场景中表现优异，证实了其在实际应用中的可靠性和实用性。

Conclusion: IRT-Router有效地解决了在性能和成本之间的权衡问题，能够在多LLM环境中准确地将用户查询路由到最合适的LLM，具有良好的效果、可解释性和冷启动性能，适合实际应用。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
a wide range of natural language tasks. However, selecting the optimal LLM to
respond to a user query often necessitates a delicate balance between
performance and cost. While powerful models deliver better results, they come
at a high cost, whereas smaller models are more cost-effective but less
capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing
framework that efficiently routes user queries to the most suitable LLM.
Inspired by Item Response Theory (IRT), a psychological measurement
methodology, IRT-Router explicitly models the relationship between LLM
capabilities and user query attributes. This not only enables accurate
prediction of response performance but also provides interpretable insights,
such as LLM abilities and query difficulty. Additionally, we design an online
query warm-up technique based on semantic similarity, further enhancing the
online generalization capability of IRT-Router. Extensive experiments on 20
LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline
methods in terms of effectiveness and interpretability. Its superior
performance in cold-start scenarios further confirms the reliability and
practicality of IRT-Router in real-world applications. Code is available at
https://github.com/Mercidaiha/IRT-Router.

</details>


### [49] [MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch](https://arxiv.org/abs/2506.01056)
*Xiang Fei,Xiawu Zheng,Hao Feng*

Main category: cs.AI

TL;DR: MCP-Zero是一种新的代理框架，允许大型语言模型自主决定何时及使用哪些外部工具来构建任务特定的工具链。该框架包括主动工具请求、分层向量路由和迭代主动调用三个组件。实验表明，MCP-Zero有效解决了现有方法的上下文冗余问题，能从近3000个候选工具中准确选择合适的工具，并显著减少了APIbank中的令牌消耗，同时支持多轮工具调用且保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 将数千种工具模式注入提示符既昂贵又容易出错，因此需要一种新方法让LLM自主决定使用哪些工具。

Method: MCP-Zero框架基于三个组件：主动工具请求、分层向量路由和迭代主动调用。主动工具请求中模型发出一个结构化的块，明确指定所需的服务器和任务；分层向量路由算法首先选择候选服务器，然后根据语义相似性对每个服务器内的工具进行排名；迭代主动调用允许模型在返回的工具不足时进行多轮跨域工具链构建并逐步修订其请求。

Result: 实验表明MCP-Zero有效地解决了现有方法的上下文冗余问题，能从近3000个候选工具中准确选择合适的工具；在APIbank上减少了98%的令牌消耗，同时保持高准确性；支持多轮工具调用且每轮保持一致的准确性。

Conclusion: MCP-Zero框架通过让LLM自主决定使用哪些工具，解决了上下文冗余问题，显著提高了工具选择的准确性和效率，减少了令牌消耗，并支持多轮工具调用。

Abstract: Function-calling has enabled large language models (LLMs) to act as
tool-using agents, but injecting thousands of tool schemas into the prompt is
costly and error-prone. We introduce MCP-Zero, a proactive agent framework that
lets the LLM itself decide when and which external tools to retrieve, thereby
assembling a task-specific toolchain from scratch. The framework is built upon
three components: (1) Proactive Tool Request, where the model emits a
structured $\left<\operatorname{tool\_assistant}\right>$ block that explicitly
specifies the desired server and task; (2) Hierarchical Vector Routing, a
coarse-to-fine retrieval algorithm that first selects candidate servers and
then ranks tools within each server based on the semantic similarity; (3)
Iterative Proactive Invocation, enabling multi-round, cross-domain toolchain
construction with minimal context overhead, and allowing the model to
iteratively revise its request when the returned tools are insufficient. To
evaluate our approach we also compile MCP-tools, a retrieval dataset comprising
308 MCP servers and 2,797 tools extracted from the official
Model-Context-Protocol repository and normalized into a unified JSON schema.
Experiments show that MCP-Zero (i) effectively addresses the context overhead
problem of existing methods and accurately selects the correct tool from a pool
of nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by
98\% on the APIbank while maintaining high accuracy; and (iii) supports
multi-turn tool invocation with consistent accuracy across rounds. The code and
dataset will be released soon.

</details>


### [50] [The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process](https://arxiv.org/abs/2506.01080)
*Florian Carichon,Aditi Khandelwal,Marylou Fauchard,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: This position paper emphasizes that AI Alignment in Multi-Agent Systems (MAS) is a dynamic process influenced by the social environment, requiring interdependent treatment of human, preferential, and objective alignment. It calls for urgent development of simulation environments, benchmarks, and evaluation frameworks to assess alignment in interactive multi-agent contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper lies in addressing the challenges of AI alignment within Multi-Agent Systems (MAS), particularly focusing on how the social environment where agents operate can lead to misalignment with human values or user preferences.

Method: The method involves analyzing the impact of social structures on group and individual values using insights from social sciences, advocating for an interdependent approach to alignment issues rather than treating them as isolated problems.

Result: The result highlights the need for new simulation environments, benchmarks, and evaluation frameworks to effectively assess and manage alignment in complex, interactive multi-agent scenarios.

Conclusion: AI Alignment in MAS should be treated as a dynamic, interaction-dependent process, and the community must develop tools to evaluate alignment before these dynamics become uncontrollably complex.

Abstract: This position paper states that AI Alignment in Multi-Agent Systems (MAS)
should be considered a dynamic and interaction-dependent process that heavily
depends on the social environment where agents are deployed, either
collaborative, cooperative, or competitive. While AI alignment with human
values and preferences remains a core challenge, the growing prevalence of MAS
in real-world applications introduces a new dynamic that reshapes how agents
pursue goals and interact to accomplish various tasks. As agents engage with
one another, they must coordinate to accomplish both individual and collective
goals. However, this complex social organization may unintentionally misalign
some or all of these agents with human values or user preferences. Drawing on
social sciences, we analyze how social structure can deter or shatter group and
individual values. Based on these analyses, we call on the AI community to
treat human, preferential, and objective alignment as an interdependent
concept, rather than isolated problems. Finally, we emphasize the urgent need
for simulation environments, benchmarks, and evaluation frameworks that allow
researchers to assess alignment in these interactive multi-agent contexts
before such dynamics grow too complex to control.

</details>


### [51] [Choices and their Provenance: Explaining Stable Solutions of Abstract Argumentation Frameworks](https://arxiv.org/abs/2506.01087)
*Bertram Ludäscher,Yilin Xia,Shawn Bowers*

Main category: cs.AI

TL;DR: An abstract argumentation framework (AF) uses the well-founded semantics (WFS) to yield a unique 3-valued solution. Arguments can be defeated, undecided, or accepted based on attackers and defenders. The provenance of arguments in grounded solutions is explained by subgraphs, while stable solutions involve non-deterministic choice. This paper presents a novel approach for extending provenance to stable AF solutions by identifying minimal sets of critical attacks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to extend the provenance of arguments to stable abstract argumentation framework (AF) solutions. While the provenance for grounded solutions has been established using subgraphs, stable models often require non-deterministic choices during model search, necessitating a different approach to understand their provenance.

Method: The method involves identifying minimal sets of critical attacks that highlight the choices and assumptions made within a stable model. These critical attack edges provide insights into an argument's status by combining well-founded derivation steps with choice steps. This approach can be viewed as a form of diagnosis where minimal 'repairs' are found in an AF graph such that the well-founded solution of the repaired graph matches the desired stable model of the original graph.

Result: The result is a novel approach that successfully extends the concept of provenance to stable AF solutions. By pinpointing the minimal sets of critical attacks, the method provides additional understanding of how arguments achieve their status within stable models.

Conclusion: In conclusion, the paper introduces a significant advancement in understanding the provenance of arguments in stable abstract argumentation frameworks. The proposed method not only enhances our comprehension of stable models but also bridges the gap between well-founded and stable semantics through the identification of critical attack edges.

Abstract: The rule $\mathrm{Defeated}(x) \leftarrow \mathrm{Attacks}(y,x),\, \neg \,
\mathrm{Defeated}(y)$, evaluated under the well-founded semantics (WFS), yields
a unique 3-valued (skeptical) solution of an abstract argumentation framework
(AF). An argument $x$ is defeated ($\mathrm{OUT}$) if there exists an
undefeated argument $y$ that attacks it. For 2-valued (stable) solutions, this
is the case iff $y$ is accepted ($\mathrm{IN}$), i.e., if all of $y$'s
attackers are defeated. Under WFS, arguments that are neither accepted nor
defeated are undecided ($\mathrm{UNDEC}$). As shown in prior work, well-founded
solutions (a.k.a. grounded labelings) "explain themselves": The provenance of
arguments is given by subgraphs (definable via regular path queries) rooted at
the node of interest. This provenance is closely related to winning strategies
of a two-player argumentation game.
  We present a novel approach for extending this provenance to stable AF
solutions. Unlike grounded solutions, which can be constructed via a bottom-up
alternating fixpoint procedure, stable models often involve non-deterministic
choice as part of the search for models. Thus, the provenance of stable
solutions is of a different nature, and reflects a more expressive generate &
test paradigm. Our approach identifies minimal sets of critical attacks,
pinpointing choices and assumptions made by a stable model. These critical
attack edges provide additional insights into the provenance of an argument's
status, combining well-founded derivation steps with choice steps. Our approach
can be understood as a form of diagnosis that finds minimal "repairs" to an AF
graph such that the well-founded solution of the repaired graph coincides with
the desired stable model of the original AF graph.

</details>


### [52] [Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking](https://arxiv.org/abs/2506.01093)
*Kunal Khanvilkar,Kranthi Kommuru*

Main category: cs.AI

TL;DR: This paper proposes a real-time transaction monitoring framework integrating graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. It constructs dynamic transaction graphs, extracts features, classifies suspicious behavior using GNN, and generates regulatory-aligned explanations. Experiments show superior results with high F1-score, precision, and recall.


<details>
  <summary>Details</summary>
Motivation: To create an automated system that can monitor transactions in real-time for financial compliance while providing explainable and audit-ready justifications.

Method: The system builds dynamic transaction graphs, uses graph neural networks to classify suspicious behavior, and employs a retrieval-augmented generation module to produce natural language explanations aligned with regulatory clauses.

Result: The experiments conducted on simulated financial data streams achieved 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation confirmed the quality and interpretability of the generated explanations.

Conclusion: The combination of graph intelligence and generative models shows potential for supporting explainable, audit-ready compliance in high-risk financial environments.

Abstract: This paper presents a real-time transaction monitoring framework that
integrates graph-based modeling, narrative field embedding, and generative
explanation to support automated financial compliance. The system constructs
dynamic transaction graphs, extracts structural and contextual features, and
classifies suspicious behavior using a graph neural network. A
retrieval-augmented generation module generates natural language explanations
aligned with regulatory clauses for each flagged transaction. Experiments
conducted on a simulated stream of financial data show that the proposed method
achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0%
recall. Expert evaluation further confirms the quality and interpretability of
generated justifications. The findings demonstrate the potential of combining
graph intelligence and generative models to support explainable, audit-ready
compliance in high-risk financial environments.

</details>


### [53] [Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication](https://arxiv.org/abs/2506.01095)
*Khe-Han Toh,Hong-Kuan Teo*

Main category: cs.AI

TL;DR: This paper proposes the Modular Speaker Architecture (MSA), which decomposes speaker behavior into modular components for role tracking, responsibility continuity, and contextual coherence. It includes three core modules and is evaluated through annotated case studies with newly introduced structural metrics.


<details>
  <summary>Details</summary>
Motivation: Sustaining coherent, role-aware communication across multi-agent systems remains a foundational challenge in AI.

Method: The proposed method is the Modular Speaker Architecture (MSA) that includes three core modules: a Speaker Role Module, a Responsibility Chain Tracker, and a Contextual Integrity Validator.

Result: Results show that MSA reliably maintains interaction structure without reliance on affective signals or surface-level heuristics.

Conclusion: The authors implemented a prototype configuration language (G-Code) and modular API to support MSA deployment in dynamic multi-agent scenarios.

Abstract: Sustaining coherent, role-aware communication across multi-agent systems
remains a foundational challenge in AI. Current frameworks often lack explicit
mechanisms for speaker responsibility, leading to context drift, alignment
instability, and degraded interpretability over time. We propose the Modular
Speaker Architecture (MSA), a framework that decomposes speaker behavior into
modular components for role tracking, responsibility continuity, and contextual
coherence. Grounded in high-context human-AI dialogues, MSA includes three core
modules: a Speaker Role Module, a Responsibility Chain Tracker, and a
Contextual Integrity Validator. We evaluate MSA through annotated case studies
and introduce structural metrics-pragmatic consistency, responsibility flow,
and context stability-quantified via manual and automatic scoring and
bootstrapped statistical analysis. Our results show that MSA reliably maintains
interaction structure without reliance on affective signals or surface-level
heuristics. We further implement a prototype configuration language (G-Code)
and modular API to support MSA deployment in dynamic multi-agent scenarios.

</details>


### [54] [SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning](https://arxiv.org/abs/2506.01096)
*Yihao Liu,Shuocheng Li,Lang Cao,Yuhang Xie,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 在稀疏奖励环境中，强化学习难以采样成功的轨迹，标准的在线强化学习方法没有利用离线监督数据。为了解决这个问题，本文提出了SuperRL框架，通过自适应地结合离线监督和强化学习，提高了模型的学习效率、泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被用于复杂的推理任务中，在这些任务中通常可以获取高质量的离线数据（如专家标注的解决方案和提炼的推理轨迹）。然而，在稀疏奖励的环境中，强化学习很难采样到成功的轨迹，导致学习效率低下。同时，这些表示正确推理路径的离线轨迹未被标准的在线强化学习方法所利用。

Method: 本文提出了一种统一的训练框架SuperRL，该框架自适应地将离线监督融入强化学习中。SuperRL引入了一个自适应开关，用于检测稀疏奖励条件并在必要时激活混合行为者（Hybrid Actor）。混合行为者在损失级别上整合了策略梯度和监督学习目标，从而使模型能够从准确的离线推理信号中受益，同时保持强化学习的探索能力。

Result: 在一系列推理基准上的实验表明，SuperRL通过提高样本效率、泛化能力和在稀疏奖励下的鲁棒性，始终优于标准的强化学习方法。

Conclusion: SuperRL框架通过自适应地结合离线监督和强化学习，显著提高了在稀疏奖励环境中的学习效果。这种框架不仅提升了样本效率，还增强了模型的泛化能力和鲁棒性，适用于各种推理任务。

Abstract: Large language models are increasingly used for complex reasoning tasks where
high-quality offline data such as expert-annotated solutions and distilled
reasoning traces are often available. However, in environments with sparse
rewards, reinforcement learning struggles to sample successful trajectories,
leading to inefficient learning. At the same time, these offline trajectories
that represent correct reasoning paths are not utilized by standard on-policy
reinforcement learning methods. To address this limitation, we propose SuperRL,
a unified training framework that adaptively incorporates offline supervision
into reinforcement learning. SuperRL introduces an Adaptive Switch to detect
sparse reward conditions and activates a Hybrid Actor when necessary. The
Hybrid Actor integrates policy gradient and supervised learning objectives at
the loss level, enabling the model to benefit from accurate offline reasoning
signals while maintaining the exploratory capacity of reinforcement learning.
Experiments on a range of reasoning benchmarks show that SuperRL consistently
outperforms standard reinforcement learning by improving sample efficiency,
generalization, and robustness under sparse rewards.

</details>


### [55] [ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation](https://arxiv.org/abs/2506.01116)
*Xinyi Liu,Lipeng Ma,Yixuan Li,Weidong Yang,Qingyuan Zhou,Jiayi Song,Shuhao Li,Ben Fei*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) show great reasoning capabilities but struggle with chemistry-related problems due to complex terminology and specialized knowledge. This paper proposes ChemAU, a framework incorporating adaptive uncertainty estimation to improve the performance of LLMs in chemistry by identifying knowledge gaps and supplementing chemical expertise.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of general LLMs in handling chemistry-related problems, which involve complex reasoning steps, specialized terminology, and specific knowledge that often lead to hallucinations during reasoning.

Method: Propose ChemAU, a novel framework that incorporates an adaptive uncertainty estimation method. This method applies different uncertainty values based on the position within the reasoning chain, identifies gaps in chemistry knowledge, and supplements chemical expertise using a specialized domain model.

Result: Experiments conducted with three popular LLMs across three chemistry datasets indicate that ChemAU significantly improves both reasoning accuracy and uncertainty estimation.

Conclusion: ChemAU effectively leverages chemical expertise and enhances the performance of LLMs in solving chemistry-related problems.

Abstract: Large Language Models (LLMs) are widely used across various scenarios due to
their exceptional reasoning capabilities and natural language understanding.
While LLMs demonstrate strong performance in tasks involving mathematics and
coding, their effectiveness diminishes significantly when applied to
chemistry-related problems. Chemistry problems typically involve long and
complex reasoning steps, which contain specific terminology, including
specialized symbol systems and complex nomenclature conventions. These
characteristics often cause general LLMs to experience hallucinations during
the reasoning process due to their lack of specific knowledge. However,
existing methods are struggling to effectively leverage chemical expertise and
formulas. Moreover, current uncertainty estimation methods, designed to
mitigate potential reasoning errors, are unable to precisely identify specific
steps or key knowledge. In this work, we propose a novel framework called
ChemAU, which incorporates our adaptive uncertainty estimation method that
applies different uncertainty values based on the position of reasoning steps
within the whole reasoning chain. Leveraging this method, ChemAU identifies
gaps in chemistry knowledge and precisely supplements chemical expertise with
the specialized domain model, thereby correcting and updating the previously
flawed reasoning chain. Our experiments with three popular LLMs across three
chemistry datasets demonstrate that ChemAU significantly enhances both
reasoning accuracy and uncertainty estimation.

</details>


### [56] [GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering](https://arxiv.org/abs/2506.01174)
*Muhammad Qasim Ali,Saeejith Nair,Alexander Wong,Yuchen Cui,Yuhao Chen*

Main category: cs.AI

TL;DR: GraphPad is a modifiable structured memory for embodied agents, improving performance on the OpenEQA benchmark by allowing dynamic, task-specific scene representation refinement.


<details>
  <summary>Details</summary>
Motivation: Structured scene representations are crucial for embodied agents but often suffer from high computational overhead and lack of adaptability when task specifications change.

Method: Introduction of GraphPad, which includes a mutable scene graph, a navigation log, and a scratchpad, enabling agents to adjust representations via API calls according to task needs.

Result: On the OpenEQA benchmark, GraphPad achieves 55.3% accuracy, showing a +3.0% improvement over an image-only baseline with the same vision-language model while using five times fewer input frames.

Conclusion: Allowing online, language-driven refinement of 3-D memory through GraphPad results in more informative representations without additional training or data collection.

Abstract: Structured scene representations are a core component of embodied agents,
helping to consolidate raw sensory streams into readable, modular, and
searchable formats. Due to their high computational overhead, many approaches
build such representations in advance of the task. However, when the task
specifications change, such static approaches become inadequate as they may
miss key objects, spatial relations, and details. We introduce GraphPad, a
modifiable structured memory that an agent can tailor to the needs of the task
through API calls. It comprises a mutable scene graph representing the
environment, a navigation log indexing frame-by-frame content, and a scratchpad
for task-specific notes. Together, GraphPad serves as a dynamic workspace that
remains complete, current, and aligned with the agent's immediate understanding
of the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a
+3.0% increase over an image-only baseline using the same vision-language
model, while operating with five times fewer input frames. These results show
that allowing online, language-driven refinement of 3-D memory yields more
informative representations without extra training or data collection.

</details>


### [57] [Test Automation for Interactive Scenarios via Promptable Traffic Simulation](https://arxiv.org/abs/2506.01199)
*Augusto Mondelli,Yueshan Li,Alessandro Zanardi,Emilio Frazzoli*

Main category: cs.AI

TL;DR: The paper presents an automated method for generating realistic and safety-critical human behaviors to evaluate AV planners, using low-dimensional goal positions and Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: There is a need for rigorous evaluation of autonomous vehicle planners, especially in assessing their robustness against the uncertainty of human behaviors. Current data-driven scenario generation models do not effectively construct comprehensive tests for AV planners.

Method: The method parameterizes complex human behaviors using low-dimensional goal positions which are input into a promptable traffic simulator called ProSim. A prompt generation module that uses Bayesian optimization explores the goal domain to identify safety-critical behaviors.

Result: The method was applied to evaluate an optimization-based planner and showed effectiveness and efficiency in automatically generating diverse and realistic driving behaviors across scenarios with varying initial conditions.

Conclusion: An automated method has been introduced that efficiently generates realistic and safety-critical human behaviors for the evaluation of AV planners in interactive scenarios.

Abstract: Autonomous vehicle (AV) planners must undergo rigorous evaluation before
widespread deployment on public roads, particularly to assess their robustness
against the uncertainty of human behaviors. While recent advancements in
data-driven scenario generation enable the simulation of realistic human
behaviors in interactive settings, leveraging these models to construct
comprehensive tests for AV planners remains an open challenge. In this work, we
introduce an automated method to efficiently generate realistic and
safety-critical human behaviors for AV planner evaluation in interactive
scenarios. We parameterize complex human behaviors using low-dimensional goal
positions, which are then fed into a promptable traffic simulator, ProSim, to
guide the behaviors of simulated agents. To automate test generation, we
introduce a prompt generation module that explores the goal domain and
efficiently identifies safety-critical behaviors using Bayesian optimization.
We apply our method to the evaluation of an optimization-based planner and
demonstrate its effectiveness and efficiency in automatically generating
diverse and realistic driving behaviors across scenarios with varying initial
conditions.

</details>


### [58] [CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction](https://arxiv.org/abs/2506.01268)
*Yudong Lu,Yazhe Niu,Shuai Hu,Haolin Wang*

Main category: cs.AI

TL;DR: CleanS2S is a unified framework for human-like speech-to-speech interaction that integrates ASR, LLMs, and TTS with proactive dialogue capabilities, low latency, and a single-file implementation.


<details>
  <summary>Details</summary>
Motivation: To advance conversational AI by creating a system that not only integrates automatic speech recognition, large language models, and text-to-speech synthesis but also introduces proactive dialogue capabilities to break the rigid turn-based convention in chatbot interactions.

Method: The system uses full-duplex websocket connections and non-blocking I/O for low transition latency. It includes a memory module for dynamic aggregation of historical and contextual data, and a Subjective Action Judgement module enabling human-like response strategies such as interruption, refusal, deflection, silence, and standard responses. The Action Judgement SFT assesses input streams for appropriate response strategies.

Result: Achieves human-like speech-to-speech interaction with proactive dialogue capabilities, breaking the traditional turn-based paradigm and allowing system-initiated dialog control and context-aware response selection.

Conclusion: CleanS2S offers researchers transparency and extensibility through its single-file implementation with atomic configurations, advancing the field of conversational AI.

Abstract: CleanS2S is a framework for human-like speech-to-speech interaction that
advances conversational AI through single-file implementation and proactive
dialogue capabilities. Our system integrates automatic speech recognition,
large language models, and text-to-speech synthesis into a unified pipeline
with real-time interruption handling, achieving low transition latency through
full-duplex websocket connections and non-blocking I/O. Beyond conventional
chatbot paradigms, we pioneer a proactive interaction mechanism, which combines
memory systems with Subjective Action Judgement module, enabling five
human-like response strategies: interruption, refusal, deflection, silence, and
standard response. The memory module dynamically aggregates historical, and
contextual data to inform interaction decisions. This approach breaks the rigid
turn-based convention by allowing system-initiated dialog control and
context-aware response selection. And we propose Action Judgement SFT that
assesses input streams for responses strategies. The framework's single-file
implementation with atomic configurations offers researchers unprecedented
transparency and extensibility for interaction agents. The code of CleanS2S is
released at \https://github.com/opendilab/CleanS2S.

</details>


### [59] [RAISE: Reasoning Agent for Interactive SQL Exploration](https://arxiv.org/abs/2506.01273)
*Fernando Granado,Roberto Lotufo,Jayr Pereira*

Main category: cs.AI

TL;DR: 最近大型语言模型（LLMs）的进步推动了自然语言数据库接口的研究。本文提出了一种新的代理框架，统一了模式链接、查询生成和迭代改进于一个端到端组件中。通过利用LLMs的内在推理能力，该方法模仿人类在不熟悉的数据库上回答问题的方式：通过形成假设、运行动态查询验证、基于结果推理并根据观察结果修订输出。实验表明，使用DeepSeek-R1-Distill-Llama-70B在具有挑战性的BIRD数据集上将执行准确性（EX）从44.8%提高到56.5%。此外，在增加答案多样性的情况下，我们的代理在8轮候选生成中达到了81.8%的最佳N准确性，与顶级已发布解决方案的82.79%相当，同时降低了工程复杂性。


<details>
  <summary>Details</summary>
Motivation: 尽管目前最先进的文本转SQL系统依赖复杂的多阶段管道，但研究者希望利用LLMs的能力开发一种更简洁且有效的统一框架，以解决传统方法中的复杂性和局限性。

Method: 提出了一种新的代理框架，该框架统一了模式链接、查询生成和迭代改进，并通过模拟人类解决问题的过程来优化数据库查询。具体而言，通过形成假设、运行动态查询验证、基于结果推理并根据观察结果修订输出，实现对数据库的深入理解。此外，还引入了一种新策略，用于扩展测试时计算的深度，从而动态分配计算资源以更好地理解数据。

Result: 实验表明，使用DeepSeek-R1-Distill-Llama-70B在BIRD数据集上的执行准确性从44.8%提高到56.5%。在增加答案多样性的设置下，代理在8轮候选生成中达到了81.8%的最佳N准确性，接近顶级已发布解决方案的82.79%，同时简化了工程复杂性。

Conclusion: 本文提出的统一框架为构建自然语言数据库接口提供了一种有前途的替代方案，既提高了准确性又降低了工程复杂性。

Abstract: Recent advances in large language models (LLMs) have propelled research in
natural language interfaces to databases. However, most state-of-the-art
text-to-SQL systems still depend on complex, multi-stage pipelines. This work
proposes a novel agentic framework that unifies schema linking, query
generation, and iterative refinement within a single, end-to-end component. By
leveraging the intrinsic reasoning abilities of LLMs, our method emulates how
humans answer questions when working with unfamiliar databases: understanding
the data by formulating hypotheses, running dynamic queries to validate them,
reasoning over the results, and revising outputs based on observed results.
Crucially, our approach introduces a new strategy for scaling test-time
computation in text-to-SQL: we scale the depth of interactive database
exploration and reflection. This shift enables the model to allocate
computation dynamically to better understand the data, especially useful in
ambiguous and underspecified scenarios. Our experiments show that it improved
the Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset
using DeepSeek-R1-Distill-Llama-70B. Furthermore, when equipped with steps to
add more diversity to the answers, our agent achieves a Best-of-N accuracy of
81.8% with 8 rounds of candidate generation, rivaling the 82.79% achieved by
the top-ranked published solution, while reducing engineering complexity. These
findings position our unified framework as a promising alternative for building
natural language interfaces to databases.

</details>


### [60] [Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D](https://arxiv.org/abs/2506.01275)
*Artemis Panagopoulou,Le Xue,Honglu Zhou,silvio savarese,Ran Xu,Caiming Xiong,Chris Callison-Burch,Mark Yatskar,Juan Carlos Niebles*

Main category: cs.AI

TL;DR: 当前最先进的多模态模型在对比跨模态推理任务中表现有限，特别是在四模态设置下。本文提出一个新数据集Contra4来评估模型选择最佳模态信息的能力，并揭示了现有模型的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型在处理多样化输入方面取得了显著进展，但它们是否能够在多个模态之间进行对比推理并选择最能满足自然语言提示的模态仍不清楚。这种能力对于检索增强和决策时的情境至关重要。

Method: 作者引入了一个名为Contra4的数据集，该数据集包含图像、音频、视频和3D四种模态，用于评估模型在对比跨模态推理中的能力。数据集通过结合人工标注的字幕和模型混合往返一致性过滤器确保高质量监督。

Result: 特定任务的微调相对于基线提高了56%的性能，然而，最先进的模型总体准确率仅为56%，在四模态设置下的准确率为42%，显示出明显的局限性。

Conclusion: 现有的多模态模型在对比跨模态推理任务中存在显著不足，尤其是在四模态设置下。需要进一步研究以提高模型在这方面的性能。

Abstract: Real-world decision-making often begins with identifying which modality
contains the most relevant information for a given query. While recent
multimodal models have made impressive progress in processing diverse inputs,
it remains unclear whether they can reason contrastively across multiple
modalities to select the one that best satisfies a natural language prompt. We
argue this capability is foundational, especially in retrieval-augmented and
decision-time contexts, where systems must evaluate multiple signals and
identify which one conveys the relevant information. To evaluate this skill, we
introduce Contra4, a dataset for contrastive cross-modal reasoning across four
modalities: image, audio, video, and 3D. Each example presents a natural
language question alongside multiple candidate modality instances, and the
model must select the one that semantically aligns with the prompt. Contra4
combines human-annotated captions with a mixture-of-models
round-trip-consistency filter to ensure high-quality supervision, resulting in
174k training examples and a manually verified test set of 2.3k samples. While
task-specific fine-tuning improves performance by 56% relative to baseline,
state-of-the-art models still achieve only 56% accuracy overall and 42% in
four-modality settings, underscoring a significant limitation in current
multimodal models.

</details>


### [61] [GeoLocSFT: Efficient Visual Geolocation via Supervised Fine-Tuning of Multimodal Foundation Models](https://arxiv.org/abs/2506.01277)
*Qiang Yi,Lianlei Shan*

Main category: cs.AI

TL;DR: The paper presents GeoLocSFT, a framework using supervised fine-tuning (SFT) of a multimodal model with a small, high-quality dataset to achieve competitive geolocation performance. It outperforms baselines on benchmarks and emphasizes the importance of SFT and quality supervision.


<details>
  <summary>Details</summary>
Motivation: Accurately determining the geographic location of images is challenging due to the vastness of the planet and similarity among distant locations. Existing methods either require massive databases or complex pipelines.

Method: The method involves using targeted supervised fine-tuning (SFT) of a large multimodal foundation model (Gemma 3) with a small, high-quality dataset of 2700 image-GPS pairs from the MR600k dataset.

Result: GeoLocSFT significantly improves over baseline models and achieves robust results on standard benchmarks like Im2GPS-3k and YFCC-4k, as well as the new MR40k benchmark focused on sparsely populated regions.

Conclusion: High-quality supervision and efficient SFT are powerful for planet-scale image geolocation. The MR40k benchmark dataset is publicly released to promote further research.

Abstract: Accurately determining the geographic location where a single image was
taken, visual geolocation, remains a formidable challenge due to the planet's
vastness and the deceptive similarity among distant locations. We introduce
GeoLocSFT, a framework that demonstrates how targeted supervised fine-tuning
(SFT) of a large multimodal foundation model (Gemma 3) using a small,
high-quality dataset can yield highly competitive geolocation performance.
GeoLocSFT is trained with only 2700 carefully selected image-GPS pairs from our
geographically diverse MR600k dataset. Despite this limited data, our
SFT-centric approach substantially improves over baseline models and achieves
robust results on standard benchmarks such as Im2GPS-3k and YFCC-4k, as well as
on our newly proposed and challenging MR40k benchmark, aimed specifically at
sparsely populated regions. Further, we explore multi-candidate inference and
aggregation strategies but find that the core gains are already realized at the
SFT stage. Our findings highlight the power of high-quality supervision and
efficient SFT for planet-scale image geolocation, especially when compared to
prior methods that require massive databases or complex pipelines. To foster
further research, we publicly release the MR40k benchmark dataset.

</details>


### [62] [On the Hardness of Approximating Distributions with Probabilistic Circuits](https://arxiv.org/abs/2506.01281)
*John Leland,YooJung Choi*

Main category: cs.AI

TL;DR: 在概率建模中，平衡表达能力和可推断性是一个基本挑战。概率电路（PCs）通过施加结构约束来直接应对这一权衡，这些约束保证了某些查询的有效推断，同时保持表达能力。然而，由于PC上的推断复杂性取决于电路大小，因此理解不同电路家族的规模界限对于表征可推断性和表达效率之间的权衡至关重要。本文探讨了通过允许一些小的近似误差是否可以避免因精确表示而产生的指数级规模膨胀问题。研究发现，对于能够有效计算边缘分布的任何模型，用有界$f$-散度近似任意分布是NP难的，并且在可分解的概率电路和另外确定性的概率电路之间存在指数级的规模差距。


<details>
  <summary>Details</summary>
Motivation: 在概率建模领域，需要找到一种方法来平衡模型的表达能力和推断的可行性。虽然精确编码分布通常会导致指数级的规模增长，但如果允许一定的近似误差，是否可以避免这种规模膨胀？

Method: 作者首先证明了用有界$f$-散度近似任意分布对于任何能够有效计算边际的模型来说是NP难的。接着，他们进一步展示了在近似情况下，可分解的概率电路和额外确定性的概率电路之间存在指数级的规模差异。

Result: 研究结果表明，在保证一定近似误差的情况下，确实可以在表达效率和推断可行性之间找到更好的平衡。但同时也揭示了不同类型概率电路在近似能力上的显著差异。

Conclusion: 本文通过理论分析证明了在概率电路中引入近似误差的重要性，并揭示了不同类型电路在近似条件下的表现差异，为未来设计更高效的概率模型提供了指导。

Abstract: A fundamental challenge in probabilistic modeling is balancing expressivity
and tractable inference. Probabilistic circuits (PCs) aim to directly address
this tradeoff by imposing structural constraints that guarantee efficient
inference of certain queries while maintaining expressivity. Since inference
complexity on PCs depends on circuit size, understanding the size bounds across
circuit families is key to characterizing the tradeoff between tractability and
expressive efficiency. However, expressive efficiency is often studied through
exact representations, where exactly encoding distributions while enforcing
various structural properties often incurs exponential size blow-ups. Thus, we
pose the following question: can we avoid such size blow-ups by allowing some
small approximation error? We first show that approximating an arbitrary
distribution with bounded $f$-divergence is $\mathsf{NP}$-hard for any model
that can tractably compute marginals. We then prove an exponential size gap for
approximation between the class of decomposable PCs and additionally
deterministic PCs.

</details>


### [63] [MobCLIP: Learning General-purpose Geospatial Representation at Scale](https://arxiv.org/abs/2506.01297)
*Ya Wen,Jixuan Cai,Qiyao Ma,Linyan Li,Xinhua Chen,Chris Webster,Yulun Zhou*

Main category: cs.AI

TL;DR: The paper introduces MobCLIP, a nationwide general-purpose location encoder that integrates diverse data modalities through multimodal fusion. It achieves significantly superior general-purpose predictive performances than state-of-the-art models by an average of 35%.


<details>
  <summary>Details</summary>
Motivation: Representation learning of geospatial locations is a core challenge in achieving general geospatial intelligence. Current embedding methods often lack versatility, limiting their utility across diverse tasks.

Method: MobCLIP adopts a novel CLIP-based architecture to align POIs, remote sensing imagery, demographic statistics with a mobility graph. It tokenizes spatial locations into grid cells inspired by Vision Transformers to establish a unified representation space bridging mobility patterns and multimodal features.

Result: MobCLIP achieves significantly superior general-purpose predictive performances than state-of-the-art models by an average of 35%. The performance gain is particularly profound in human-centric tasks.

Conclusion: The authors have demonstrated the scaling behavior in geospatial representation learning and open-sourced code and pretrained models.

Abstract: Representation learning of geospatial locations remains a core challenge in
achieving general geospatial intelligence. Current embedding methods often lack
versatility, limiting their utility across diverse tasks in both human and
natural domains. We present MobCLIP, the first nationwide general-purpose
location encoder, integrating an unprecedented diversity of data modalities
through effective and scalable multimodal fusion. Adopting a novel CLIP-based
architecture, our framework aligns 100M+ POIs, nationwide remote sensing
imagery, and structured demographic statistics with a billion-edge mobility
graph. By tokenizing spatial locations into grid cells inspired by Vision
Transformers, we establish a unified representation space bridging mobility
patterns and multimodal features. To rigorously evaluate the general-purpose
effectiveness of MobCLIP, we construct a benchmark dataset composed of 11
downstream prediction tasks across social, economic, and natural domains.
Experiments show that MobCLIP, with four input modalities and a compact
128-dimensional representation space, achieves significantly superior
general-purpose predictive performances than state-of-the-art models by an
average of 35%. Thanks to the effective integration of human-centric
modalities, the performance gain is particularly profound in human-centric
tasks, such as energy consumption (+260%), offline retail consumption amount
(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we
further demonstrate the scaling behavior in geospatial representation learning.
We open-source code and pretrained models at: github.com.

</details>


### [64] [Scalable In-Context Q-Learning](https://arxiv.org/abs/2506.01299)
*Jinmei Liu,Fuhong Liu,Jianye Hao,Bo Wang,Huaxiong Li,Chunlin Chen,Zhi Wang*

Main category: cs.AI

TL;DR: Recent advancements in language models inspire the exploration of in-context reinforcement learning (ICRL). However, existing ICRL approaches face challenges. This paper proposes SICQL, a novel framework that uses dynamic programming and world modeling to improve ICRL's efficiency and scalability while maintaining stability. It involves a prompt-based transformer architecture for predicting policies and value functions, pretraining a world model for compact prompts, and iterative policy improvement with advantage-weighted regression. Experiments show significant performance gains, especially with suboptimal data.


<details>
  <summary>Details</summary>
Motivation: The motivation is rooted in the limitations of current ICRL methods, which struggle with complex dynamics, temporal correlations, and learning from suboptimal trajectories. The authors aim to address these issues by developing a more efficient and scalable framework.

Method: SICQL integrates dynamic programming and world modeling into ICRL. It employs a prompt-based multi-head transformer architecture with separate heads for policy and value function predictions. A generalized world model is pretrained to capture task-relevant information, aiding fast in-context inference. Training includes iterative policy improvement via fitting a state value function to an upper-expectile of the Q-function and distilling in-context value functions using advantage-weighted regression.

Result: SICQL demonstrates consistent performance improvements across various discrete and continuous environments compared to different baselines, particularly excelling when learning from suboptimal data.

Conclusion: SICQL successfully addresses the challenges faced by existing ICRL methods by providing a scalable and stable framework for efficient reward maximization and task generalization.

Abstract: Recent advancements in language models have demonstrated remarkable
in-context learning abilities, prompting the exploration of in-context
reinforcement learning (ICRL) to extend the promise to decision domains. Due to
involving more complex dynamics and temporal correlations, existing ICRL
approaches may face challenges in learning from suboptimal trajectories and
achieving precise in-context inference. In the paper, we propose
\textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning
(\textbf{SICQL}), an innovative framework that harnesses dynamic programming
and world modeling to steer ICRL toward efficient reward maximization and task
generalization, while retaining the scalability and stability of supervised
pretraining. We design a prompt-based multi-head transformer architecture that
simultaneously predicts optimal policies and in-context value functions using
separate heads. We pretrain a generalized world model to capture task-relevant
information, enabling the construction of a compact prompt that facilitates
fast and precise in-context inference. During training, we perform iterative
policy improvement by fitting a state value function to an upper-expectile of
the Q-function, and distill the in-context value functions into policy
extraction using advantage-weighted regression. Extensive experiments across a
range of discrete and continuous environments show consistent performance gains
over various types of baselines, especially when learning from suboptimal data.
Our code is available at https://github.com/NJU-RL/SICQL

</details>


### [65] [Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner](https://arxiv.org/abs/2506.01301)
*Chunhui Zhang,Zhongyu Ouyang,Kwonjoon Lee,Nakul Agarwal,Sean Dae Houlihan,Soroush Vosoughi,Shao-Yuan Lo*

Main category: cs.AI

TL;DR: The paper presents a scalable Bayesian ToM planner that decomposes ToM reasoning into stepwise Bayesian updates, introduces weak-to-strong control for LMs to specialize in ToM-specific likelihood estimation and transfer reasoning behaviors to larger LMs, achieving a 4.6% accuracy improvement on multimodal ToM benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning, which struggle with scalability in multimodal environments and fail to generalize as task complexity increases.

Method: The authors propose a scalable Bayesian ToM planner that decomposes ToM reasoning into stepwise Bayesian updates and introduces weak-to-strong control, allowing smaller LMs to specialize in ToM-specific likelihood estimation and transfer their reasoning behaviors to larger LMs.

Result: Extensive experiments show that the method achieves a 4.6% accuracy improvement over state-of-the-art techniques on multimodal ToM benchmarks, including challenging unseen scenarios.

Conclusion: This work establishes a new standard for modeling human mental states in complex environments.

Abstract: Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs,
desires, and intentions-forming the foundation of social cognition. However,
existing computational ToM methods rely on structured workflows with
ToM-specific priors or deep model fine-tuning, which struggle with scalability
in multimodal environments and fail to generalize as task complexity increases.
To address these limitations, we propose a scalable Bayesian ToM planner that
decomposes ToM reasoning into stepwise Bayesian updates. Our framework
introduces weak-to-strong control, allowing smaller language models (LMs) to
specialize in ToM-specific likelihood estimation and transfer their reasoning
behaviors to larger LMs (7B to 405B) for integration with social and world
knowledge. This synergistic approach aligns large-model inference of human
mental states with Bayesian principles. Extensive experiments show that our
method achieves a 4.6% accuracy improvement over state-of-the-art techniques on
multimodal ToM benchmarks, including challenging unseen scenarios, thereby
establishing a new standard for modeling human mental states in complex
environments.

</details>


### [66] [ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research](https://arxiv.org/abs/2506.01326)
*Zhiyuan Wang,Bokui Chen,Yinya Huang,Qingxing Cao,Ming He,Jianping Fan,Xiaodan Liang*

Main category: cs.AI

TL;DR: ORMind is a new framework that improves operations research by focusing on mathematical accuracy and creating predictable workflows, leading to better optimization in business applications.


<details>
  <summary>Details</summary>
Motivation: To solve the challenges faced by Large Language Models (LLMs) when applied to operations research problems, such as focusing on code syntax over mathematical accuracy and having unpredictable workflows which reduce transparency and increase costs.

Method: Introduced ORMind, a cognitive-inspired framework using counterfactual reasoning to transform requirements into mathematical models and solver code, enhancing optimization capabilities for both business and consumer customers.

Result: Experiments show ORMind outperforms existing methods with a 9.5% improvement on the NL4Opt dataset and a 14.6% improvement on the ComplexOR dataset.

Conclusion: ORMind addresses significant challenges in applying LLMs to operations research, providing a practical solution for industry-relevant OR problems.

Abstract: Operations research (OR) is widely deployed to solve critical decision-making
problems with complex objectives and constraints, impacting manufacturing,
logistics, finance, and healthcare outcomes. While Large Language Models (LLMs)
have shown promising results in various domains, their practical application in
industry-relevant operations research (OR) problems presents significant
challenges and opportunities. Preliminary industrial applications of LLMs for
operations research face two critical deployment challenges: 1) Self-correction
focuses on code syntax rather than mathematical accuracy, causing costly
errors; 2) Complex expert selection creates unpredictable workflows that reduce
transparency and increase maintenance costs, making them impractical for
time-sensitive business applications. To address these business limitations, we
introduce ORMind, a cognitive-inspired framework that enhances optimization
through counterfactual reasoning. Our approach emulates human cognition,
implementing an end-to-end workflow that systematically transforms requirements
into mathematical models and executable solver code. It is currently being
tested internally in Lenovo's AI Assistant, with plans to enhance optimization
capabilities for both business and consumer customers. Experiments demonstrate
that ORMind outperforms existing methods, achieving a 9.5\% improvement on the
NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.

</details>


### [67] [An Empirical Study of Group Conformity in Multi-Agent Systems](https://arxiv.org/abs/2506.01332)
*Min Choi,Keonwoo Kim,Sungwon Chae,Sangyeob Baek*

Main category: cs.AI

TL;DR: Recent advances in LLMs have enabled multi-agent systems that simulate real-world interactions. This study explores how LLM agents shape public opinion through debates on contentious topics.


<details>
  <summary>Details</summary>
Motivation: While previous studies have examined biases related to protected attributes, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored.

Method: This study simulates over 2,500 debates with initially neutral agents assigned a centrist disposition, analyzing how they adopt specific stances over time.

Result: Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence.

Conclusion: The findings highlight the crucial role of agent intelligence in shaping discourse and emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions.

Abstract: Recent advances in Large Language Models (LLMs) have enabled multi-agent
systems that simulate real-world interactions with near-human reasoning. While
previous studies have extensively examined biases related to protected
attributes such as race, the emergence and propagation of biases on socially
contentious issues in multi-agent LLM interactions remain underexplored. This
study explores how LLM agents shape public opinion through debates on five
contentious topics. By simulating over 2,500 debates, we analyze how initially
neutral agents, assigned a centrist disposition, adopt specific stances over
time. Statistical analyses reveal significant group conformity mirroring human
behavior; LLM agents tend to align with numerically dominant groups or more
intelligent agents, exerting a greater influence. These findings underscore the
crucial role of agent intelligence in shaping discourse and highlight the risks
of bias amplification in online interactions. Our results emphasize the need
for policy measures that promote diversity and transparency in LLM-generated
discussions to mitigate the risks of bias propagation within anonymous online
environments.

</details>


### [68] [EgoBrain: Synergizing Minds and Eyes For Human Action Understanding](https://arxiv.org/abs/2506.01353)
*Nie Lin,Yansen Wang,Dongqi Han,Weibang Jiang,Jingyuan Li,Ryosuke Furuta,Yoichi Sato,Dongsheng Li*

Main category: cs.AI

TL;DR: The paper introduces EgoBrain, the first large-scale multimodal dataset combining EEG and egocentric vision to decode human cognition and behavior. It includes 61 hours of synchronized data from 40 participants engaged in daily activities. A multimodal learning framework was developed for action understanding with a recognition accuracy of 66.70%. The dataset and tools are openly shared.


<details>
  <summary>Details</summary>
Motivation: To establish a new paradigm for human-centered behavior analysis by integrating brain-computer interfaces (BCIs), specifically EEG, with artificial intelligence (AI) through multimodal models.

Method: Creation of EgoBrain, a dataset with synchronized EEG and first-person video recordings. Development of a multimodal learning framework to fuse EEG and vision data for action understanding.

Result: Achieved an action recognition accuracy of 66.70% across cross-subject and cross-environment challenges.

Conclusion: EgoBrain provides a foundation for a unified framework in multimodal brain-computer interfaces, advancing cognitive computing through open science.

Abstract: The integration of brain-computer interfaces (BCIs), in particular
electroencephalography (EEG), with artificial intelligence (AI) has shown
tremendous promise in decoding human cognition and behavior from neural
signals. In particular, the rise of multimodal AI models have brought new
possibilities that have never been imagined before. Here, we present EgoBrain
--the world's first large-scale, temporally aligned multimodal dataset that
synchronizes egocentric vision and EEG of human brain over extended periods of
time, establishing a new paradigm for human-centered behavior analysis. This
dataset comprises 61 hours of synchronized 32-channel EEG recordings and
first-person video from 40 participants engaged in 29 categories of daily
activities. We then developed a muiltimodal learning framework to fuse EEG and
vision for action understanding, validated across both cross-subject and
cross-environment challenges, achieving an action recognition accuracy of
66.70%. EgoBrain paves the way for a unified framework for brain-computer
interface with multiple modalities. All data, tools, and acquisition protocols
are openly shared to foster open science in cognitive computing.

</details>


### [69] [COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents](https://arxiv.org/abs/2506.01900)
*Manish Bhatt,Ronald F. Del Rosario,Vineeth Sai Narajala,Idan Habler*

Main category: cs.AI

TL;DR: This paper presents COALESCE, a framework that optimizes resource utilization in LLM agent systems by enabling dynamic outsourcing of subtasks to specialized third-party agents. It offers mechanisms for skill representation, task decomposition, cost modeling, decision-making algorithms, and communication protocols. Validation shows significant cost reductions theoretically and empirically.


<details>
  <summary>Details</summary>
Motivation: The deployment of autonomous LLM agents is constrained by high computational demands, particularly GPU resources, prompting the need for optimizing resource utilization in LLM agent systems.

Method: COALESCE introduces mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model, market-based decision-making algorithms, and standardized communication protocols between LLM agents.

Result: Theoretical simulations (239) demonstrate a 41.8% cost reduction potential. Large-scale empirical validation across 240 real LLM tasks confirms a 20.3% cost reduction with proper epsilon-greedy exploration.

Conclusion: COALESCE aims to reduce operational costs, enhance scalability, and foster specialized agent economies, making complex LLM functionalities more accessible and economically viable.

Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM)
agents promise significant capabilities across various domains. However, their
deployment is increasingly constrained by substantial computational demands,
specifically for Graphics Processing Unit (GPU) resources. This paper addresses
the critical problem of optimizing resource utilization in LLM agent systems.
We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via
Skill-based Competence Estimation), a novel framework designed to enable
autonomous LLM agents to dynamically outsource specific subtasks to
specialized, cost-effective third-party LLM agents. The framework integrates
mechanisms for hybrid skill representation, dynamic skill discovery, automated
task decomposition, a unified cost model comparing internal execution costs
against external outsourcing prices, simplified market-based decision-making
algorithms, and a standardized communication protocol between LLM agents.
Comprehensive validation through 239 theoretical simulations demonstrates
41.8\% cost reduction potential, while large-scale empirical validation across
240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy
exploration, establishing both theoretical viability and practical
effectiveness. The emergence of proposed open standards like Google's
Agent2Agent (A2A) protocol further underscores the need for frameworks like
COALESCE that can leverage such standards for efficient agent interaction. By
facilitating a dynamic market for agent capabilities, potentially utilizing
protocols like A2A for communication, COALESCE aims to significantly reduce
operational costs, enhance system scalability, and foster the emergence of
specialized agent economies, making complex LLM agent functionalities more
accessible and economically viable.

</details>


### [70] [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
*Minjun Zhu,Qiujie Xie,Yixuan Weng,Jian Wu,Zhen Lin,Linyi Yang,Yue Zhang*

Main category: cs.AI

TL;DR: 尽管AI科学家在独立科学发现方面取得了显著进展，但他们在计算机科学领域的突破性成就仍不及自动化科学工具。通过对28篇由5个先进AI科学家系统生成的研究论文进行系统评估，我们认为AI科学家的根本瓶颈在于执行必要验证程序的能力。本文呼吁社区参与者共同努力，弥合这一实施差距。


<details>
  <summary>Details</summary>
Motivation: 近期的AI科学家研究展示了足够的能力进行独立科学发现，并且生成的研究报告已被ICLR 2025研讨会和ACL 2025接受，这表明人类级别的AI科学家可能即将出现，能够揭示之前人类未知的现象。然而，AI科学家尚未在计算机科学领域取得与自动化科学工具相媲美的突破性成就。

Method: 基于现有复杂工程任务基准中的广泛定量证据，以及对由五个先进的AI科学家系统生成的28篇研究论文的系统评估，分析了AI科学家的能力瓶颈。

Result: 发现当前AI科学家系统缺乏执行严格实验和生产高质量科学论文所需的能力，其根本瓶颈在于执行必要验证程序的能力。

Conclusion: 本文深入讨论了AI科学家的基本局限性，强调了实施差距的根源，并呼吁社区成员共同努力解决这一问题。

Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm
shift in scientific discovery, with large language models (LLMs) taking the
lead as the primary executor in the entire scientific workflow from idea
generation to experiment implementation. Recent AI Scientist studies
demonstrate sufficient capabilities for independent scientific discovery, with
the generated research reports gaining acceptance at the ICLR 2025 workshop and
ACL 2025, arguing that a human-level AI Scientist, capable of uncovering
phenomena previously unknown to humans, may be imminent. Despite this
substantial progress, AI Scientist has yet to produce a groundbreaking
achievement in the domain of computer science on par with automated scientific
tools. Based on extensive quantitative evidence from existing benchmarks in
complex engineering tasks and a systematic evaluation assess 28 research papers
generated by five advanced AI Scientist systems, we argue that \textbf{the
fundamental bottleneck for AI Scientists lies in their capability to execute
the requisite verification procedures.} Current AI Scientist systems lack the
execution capabilities needed to execute rigorous experiments and produce
high-quality scientific papers. To better illustrate the root cause of this
\textbf{implementation gap}, we provide an in-depth discussion on the
fundamental limitations of AI Scientist. This position paper aims to call for
the participants in the community to bridge the implementation gap.

</details>


### [71] [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
*Zhong Zhang,Yaxi Lu,Yikun Fu,Yupeng Huo,Shenzhi Yang,Yesai Wu,Han Si,Xin Cong,Haotian Chen,Yankai Lin,Jie Xie,Wei Zhou,Wang Xu,Yuanheng Zhang,Zhou Su,Zhongwu Zhai,Xiaoming Liu,Yudong Mei,Jianming Xu,Hongyan Tian,Chongyi Wang,Chi Chen,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: This paper introduces AgentCPM-GUI, an 8B-parameter GUI agent designed for mobile environments. It addresses challenges in existing agents by using grounding-aware pre-training, supervised fine-tuning on high-quality trajectories in Chinese and English, and reinforcement learning to improve reasoning. The model achieves state-of-the-art performance on several benchmarks including a new Chinese GUI benchmark.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current GUI agents which suffer from noisy training data, lack semantic diversity, overfit to known interface patterns, and predominantly focus on English interfaces while ignoring non-English applications such as those in the Chinese mobile ecosystem.

Method: The method involves grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. A compact action space is also introduced to support low-latency execution on mobile devices.

Result: AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching 96.9% Type-Match and 91.3% Exact-Match.

Conclusion: The conclusion is that AgentCPM-GUI represents a significant advancement in GUI interaction, demonstrating robustness and efficiency in on-device GUI tasks across various benchmarks. All code, model checkpoint, and evaluation data are publicly released to encourage further research and reproducibility.

Abstract: The recent progress of large language model agents has opened new
possibilities for automating tasks through graphical user interfaces (GUIs),
especially in mobile environments where intelligent interaction can greatly
enhance usability. However, practical deployment of such agents remains
constrained by several key challenges. Existing training data is often noisy
and lack semantic diversity, which hinders the learning of precise grounding
and planning. Models trained purely by imitation tend to overfit to seen
interface patterns and fail to generalize in unfamiliar scenarios. Moreover,
most prior work focuses on English interfaces while overlooks the growing
diversity of non-English applications such as those in the Chinese mobile
ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent
built for robust and efficient on-device GUI interaction. Our training pipeline
includes grounding-aware pre-training to enhance perception, supervised
fine-tuning on high-quality Chinese and English trajectories to imitate
human-like actions, and reinforcement fine-tuning with GRPO to improve
reasoning capability. We also introduce a compact action space that reduces
output length and supports low-latency execution on mobile devices.
AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks
and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and
$91.3\%$ Exact-Match. To facilitate reproducibility and further research, we
publicly release all code, model checkpoint, and evaluation data.

</details>


### [72] [FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance](https://arxiv.org/abs/2506.01423)
*Hongyang Yang,Likun Lin,Yang She,Xinyu Liao,Jiaoyang Wang,Runjia Zhang,Yuquan Mo,Christina Dan Wang*

Main category: cs.AI

TL;DR: This paper presents an AI-native, agent-based ERP framework with Generative Business Process AI Agents (GBPAs) that enable dynamic optimization and end-to-end automation of complex enterprise workflows.


<details>
  <summary>Details</summary>
Motivation: Enterprise Resource Planning (ERP) systems currently have limitations due to their reliance on static, rule-based workflows. This restricts their adaptability, scalability, and intelligence, particularly in handling structured and unstructured data in real time and managing dynamic, cross-functional workflows.

Method: The authors introduce a novel architecture of Generative Business Process AI Agents (GBPAs), which integrates generative AI with business process modeling and multi-agent orchestration. These agents interpret user intent, synthesize workflows in real time, and coordinate specialized sub-agents for modular task execution.

Result: Case studies in bank wire transfers and employee reimbursements demonstrate that GBPAs reduce processing time by up to 40%, decrease the error rate by 94%, and improve regulatory compliance through parallelism, risk control insertion, and semantic reasoning.

Conclusion: The findings indicate that GBPAs can bridge the gap between generative AI capabilities and enterprise-grade automation, suggesting their potential to form the foundation for the next generation of intelligent ERP systems.

Abstract: Enterprise Resource Planning (ERP) systems serve as the digital backbone of
modern financial institutions, yet they continue to rely on static, rule-based
workflows that limit adaptability, scalability, and intelligence. As business
operations grow more complex and data-rich, conventional ERP platforms struggle
to integrate structured and unstructured data in real time and to accommodate
dynamic, cross-functional workflows.
  In this paper, we present the first AI-native, agent-based framework for ERP
systems, introducing a novel architecture of Generative Business Process AI
Agents (GBPAs) that bring autonomy, reasoning, and dynamic optimization to
enterprise workflows. The proposed system integrates generative AI with
business process modeling and multi-agent orchestration, enabling end-to-end
automation of complex tasks such as budget planning, financial reporting, and
wire transfer processing. Unlike traditional workflow engines, GBPAs interpret
user intent, synthesize workflows in real time, and coordinate specialized
sub-agents for modular task execution. We validate the framework through case
studies in bank wire transfers and employee reimbursements, two representative
financial workflows with distinct complexity and data modalities. Results show
that GBPAs achieve up to 40% reduction in processing time, 94% drop in error
rate, and improved regulatory compliance by enabling parallelism, risk control
insertion, and semantic reasoning. These findings highlight the potential of
GBPAs to bridge the gap between generative AI capabilities and enterprise-grade
automation, laying the groundwork for the next generation of intelligent ERP
systems.

</details>


### [73] [Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures](https://arxiv.org/abs/2506.01438)
*Prashik Buddhaghosh Bansod*

Main category: cs.AI

TL;DR: 大型语言模型推动了两种人工智能范式的发展：独立AI代理和协作的Agentic AI生态系统。本文通过分析其操作原则、结构组成和部署方法，建立了区分这两种架构的框架，探讨了从传统规则系统到现代代理架构的演变，提出了应对关键挑战的创新解决方案。


<details>
  <summary>Details</summary>
Motivation: 阐明独立AI代理和协作的Agentic AI生态系统的区别和发展路径，为实践者提供选择合适方法的指导，并为下一代智能系统开发奠定基础。

Method: 系统分析AI代理和Agentic AI的操作原则、结构组成和部署方法，对比规划机制、记忆系统、协调协议和决策过程，分类应用领域并识别关键挑战及提出解决方案。

Result: 确立了区分AI代理和Agentic AI架构的框架，揭示了两者的差异和演变轨迹，提供了应对可靠性、协调复杂性和可扩展性约束等挑战的策略。

Conclusion: 该框架为实践者选择合适的agentic方法提供了重要指导，并为下一代智能系统开发奠定了基础原则。

Abstract: The emergence of large language models has catalyzed two distinct yet
interconnected paradigms in artificial intelligence: standalone AI Agents and
collaborative Agentic AI ecosystems. This comprehensive study establishes a
definitive framework for distinguishing these architectures through systematic
analysis of their operational principles, structural compositions, and
deployment methodologies. We characterize AI Agents as specialized,
tool-enhanced systems leveraging foundation models for targeted automation
within constrained environments. Conversely, Agentic AI represents
sophisticated multi-entity frameworks where distributed agents exhibit emergent
collective intelligence through coordinated interaction protocols. Our
investigation traces the evolutionary trajectory from traditional rule-based
systems through generative AI foundations to contemporary agent architectures.
We present detailed architectural comparisons examining planning mechanisms,
memory systems, coordination protocols, and decision-making processes. The
study categorizes application landscapes, contrasting single-agent
implementations in customer service and content management with multi-agent
deployments in research automation and complex decision support. We identify
critical challenges including reliability issues, coordination complexities,
and scalability constraints, while proposing innovative solutions through
enhanced reasoning frameworks, robust memory architectures, and improved
coordination mechanisms. This framework provides essential guidance for
practitioners selecting appropriate agentic approaches and establishes
foundational principles for next-generation intelligent system development.

</details>


### [74] [Agentic Episodic Control](https://arxiv.org/abs/2506.01442)
*Xidong Yang,Wenhao Li,Junjie Sheng,Chuyun Shen,Yun Hua,Xiangfeng Wang*

Main category: cs.AI

TL;DR: The paper proposes Agentic Episodic Control (AEC), integrating reinforcement learning with large language models to improve data efficiency and generalizability. Experiments show significant improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning's applicability is hindered by low data efficiency and poor generalizability. The authors aim to leverage the rich knowledge and reasoning capabilities of large language models to complement RL.

Method: The AEC architecture uses a LLM to map observations into language-grounded embeddings stored in episodic memory, a World-Graph working memory for capturing environmental dynamics, and a critical state detector for arbitration between memory recall and exploration.

Result: On BabyAI-Text benchmark tasks, AEC shows substantial improvements over existing baselines, particularly on complex and generalization tasks like FindObj, outperforming the best baseline by up to 76%.

Conclusion: The AEC framework combines numeric reinforcement learning with symbolic reasoning, leading to more adaptable and sample-efficient agents.

Abstract: Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to
scientific discovery and AI alignment. However, its broader applicability
remains limited by challenges such as low data efficiency and poor
generalizability. Recent advances suggest that large language models, with
their rich world knowledge and reasoning capabilities, could complement RL by
enabling semantic state modeling and task-agnostic planning. In this work, we
propose the Agentic Episodic Control (AEC), a novel architecture that
integrates RL with LLMs to enhance decision-making. The AEC can leverage a
large language model (LLM) to map the observations into language-grounded
embeddings, which further can be stored in an episodic memory for rapid
retrieval of high-value experiences. Simultaneously, a World-Graph working
memory module is utilized to capture structured environmental dynamics in order
to enhance relational reasoning. Furthermore, a lightweight critical state
detector dynamically arbitrates between the episodic memory recall and the
world-model-guided exploration. On the whole, by combining the trial-and-error
learning scheme with LLM-derived semantic priors, the proposed AEC can improve
both data efficiency and generalizability in reinforcement learning. In
experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial
improvements over existing baselines, especially on complex and generalization
tasks like FindObj, where it outperforms the best baseline by up to 76%. The
proposed AEC framework bridges the strengths of numeric reinforcement learning
and symbolic reasoning, which provides a pathway toward more adaptable and
sample-efficient agents.

</details>


### [75] [PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization](https://arxiv.org/abs/2506.01475)
*Zouying Cao,Runze Wang,Yifei Yang,Xinbei Ma,Xiaoyong Zhu,Bo Zheng,Hai Zhao*

Main category: cs.AI

TL;DR: This paper explores the use of pseudocode-style plans (P-code Plans) to improve the efficiency and generalization ability of Large Language Model (LLM) agents. They propose a method called PGPO which enhances LLM agents' planning and reasoning abilities, demonstrating superior performance in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and task-specific nature of natural language plans used by existing LLM agents, seeking a more generalized and efficient approach for guiding reasoning.

Method: The method involves using pseudocode-style plans (P-code Plans) to capture structural logic of reasoning and proposing a pseudocode-style Planning Guided Preference Optimization (PGPO) method with two planning-oriented rewards to enhance LLM agents' plan generation and reasoning.

Result: Experiments show that PGPO outperforms current leading baselines on agent benchmarks, reducing action errors and omissions during reasoning.

Conclusion: PGPO demonstrates superior performance and the advantage of using P-code Plans in enhancing LLM agents' generalization ability and efficiency.

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in handling complex interactive problems. Existing LLM agents mainly generate
natural language plans to guide reasoning, which is verbose and inefficient. NL
plans are also tailored to specific tasks and restrict agents' ability to
generalize across similar tasks. To this end, we explore pseudocode-style plans
(P-code Plan) to capture the structural logic of reasoning. We find that P-code
Plan empowers LLM agents with stronger generalization ability and more
efficiency. Inspired by this finding, we propose a pseudocode-style Planning
Guided Preference Optimization method called PGPO for effective agent learning.
With two planning-oriented rewards, PGPO further enhances LLM agents' ability
to generate high-quality P-code Plans and subsequent reasoning. Experiments
show that PGPO achieves superior performance on representative agent benchmarks
and outperforms the current leading baselines. Analyses reveal the advantage of
PGPO in reducing action errors and omissions during reasoning.

</details>


### [76] [MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments](https://arxiv.org/abs/2506.01616)
*Xiao Yang,Jiawei Chen,Jun Luo,Zhengwei Fang,Yinpeng Dong,Hang Su,Jun Zhu*

Main category: cs.AI

TL;DR: The paper introduces MLA-Trust, a framework for evaluating the trustworthiness of multimodal LLM-based agents (MLAs) across truthfulness, controllability, safety, and privacy dimensions. It identifies unique vulnerabilities in MLAs compared to static models and provides an extensible toolbox for continuous evaluation.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLM-based agents (MLAs) have advanced interaction capabilities but introduce critical trustworthiness challenges that go beyond traditional language models' limitations, including actionable outputs, long-horizon uncertainty, and multimodal attack vectors. Existing benchmarks inadequately address these issues.

Method: The authors developed MLA-Trust, a comprehensive framework that evaluates MLAs' trustworthiness across four dimensions: truthfulness, controllability, safety, and privacy. They designed 34 high-risk interactive tasks on websites and mobile applications and curated rich evaluation datasets.

Result: Experiments with 13 state-of-the-art agents revealed previously unexplored trustworthiness vulnerabilities specific to multimodal interactive scenarios. MLAs showed more severe risks than static MLLMs, especially in high-stakes domains. Multi-step execution enhanced adaptability but accumulated latent nonlinear risks.

Conclusion: MLA-Trust effectively highlights vulnerabilities in MLAs and underscores the importance of continuous evaluation. The paper also provides an extensible toolbox to facilitate ongoing assessment of MLA trustworthiness.

Abstract: The emergence of multimodal LLM-based agents (MLAs) has transformed
interaction paradigms by seamlessly integrating vision, language, action and
dynamic environments, enabling unprecedented autonomous capabilities across GUI
applications ranging from web automation to mobile systems. However, MLAs
introduce critical trustworthiness challenges that extend far beyond
traditional language models' limitations, as they can directly modify digital
states and trigger irreversible real-world consequences. Existing benchmarks
inadequately tackle these unique challenges posed by MLAs' actionable outputs,
long-horizon uncertainty and multimodal attack vectors. In this paper, we
introduce MLA-Trust, the first comprehensive and unified framework that
evaluates the MLA trustworthiness across four principled dimensions:
truthfulness, controllability, safety and privacy. We utilize websites and
mobile applications as realistic testbeds, designing 34 high-risk interactive
tasks and curating rich evaluation datasets. Large-scale experiments involving
13 state-of-the-art agents reveal previously unexplored trustworthiness
vulnerabilities unique to multimodal interactive scenarios. For instance,
proprietary and open-source GUI-interacting MLAs pose more severe
trustworthiness risks than static MLLMs, particularly in high-stakes domains;
the transition from static MLLMs into interactive MLAs considerably compromises
trustworthiness, enabling harmful content generation in multi-step interactions
that standalone MLLMs would typically prevent; multi-step execution, while
enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation
across successive interactions, circumventing existing safeguards and resulting
in unpredictable derived risks. Moreover, we present an extensible toolbox to
facilitate continuous evaluation of MLA trustworthiness across diverse
interactive environments.

</details>


### [77] [General agents need world models](https://arxiv.org/abs/2506.01622)
*Jonathan Richens,David Abel,Alexis Bellot,Tom Everitt*

Main category: cs.AI

TL;DR: 该论文探讨了世界模型在灵活、目标导向行为中的必要性，表明能够完成多步目标导向任务的智能体必须已经学习了其环境的预测模型。此模型可从智能体的策略中提取，并且提升智能体性能或实现更复杂目标需要学习更精确的世界模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于明确世界模型是否是实现灵活、目标导向行为的必要条件，以及无模型学习是否足够支持这种行为。

Method: 通过形式化分析，证明任何能够泛化到多步目标导向任务的智能体必须已学习到环境的预测模型。并提出可以从智能体策略中提取该模型，同时分析性能提升和目标复杂度增加对世界模型准确性的要求。

Result: 结果表明，多步目标导向任务的完成依赖于智能体所学习到的环境预测模型，并且随着任务复杂度提高，需要更加精确的世界模型。

Conclusion: 结论是世界模型对于实现灵活、目标导向的行为是必要的，而不仅仅是依靠无模型学习可以达成。此外，这一发现有助于开发安全通用的智能体，限定复杂环境中智能体的能力，并提供新算法以从智能体中提取世界模型。

Abstract: Are world models a necessary ingredient for flexible, goal-directed
behaviour, or is model-free learning sufficient? We provide a formal answer to
this question, showing that any agent capable of generalizing to multi-step
goal-directed tasks must have learned a predictive model of its environment. We
show that this model can be extracted from the agent's policy, and that
increasing the agents performance or the complexity of the goals it can achieve
requires learning increasingly accurate world models. This has a number of
consequences: from developing safe and general agents, to bounding agent
capabilities in complex environments, and providing new algorithms for
eliciting world models from agents.

</details>


### [78] [MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer](https://arxiv.org/abs/2506.01623)
*Ajsal Shereef Palattuparambil,Thommen George Karimpanal,Santu Rana*

Main category: cs.AI

TL;DR: MAGIK is a new framework that allows RL agents to transfer knowledge to new tasks without interacting with the target environment, achieving zero-shot transfer.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning agents usually need extensive retraining for new tasks even if they are similar to learned ones. Humans can do analogical reasoning well, so there's a need for a method allowing RL agents to apply knowledge from one task to another related task with minimal relearning.

Method: Propose MAGIK, which uses an imagination mechanism to map entities in the target task to their analogues in the source domain, enabling the agent to reuse its original policy without interacting with the target environment.

Result: Experiments on custom MiniGrid and MuJoCo tasks demonstrate effective zero-shot transfer using only a small number of human-labelled examples.

Conclusion: MAGIK offers a novel and effective mechanism for knowledge transfer via imagination-based analogy mapping.

Abstract: Humans excel at analogical reasoning - applying knowledge from one task to a
related one with minimal relearning. In contrast, reinforcement learning (RL)
agents typically require extensive retraining even when new tasks share
structural similarities with previously learned ones. In this work, we propose
MAGIK, a novel framework that enables RL agents to transfer knowledge to
analogous tasks without interacting with the target environment. Our approach
leverages an imagination mechanism to map entities in the target task to their
analogues in the source domain, allowing the agent to reuse its original
policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK
achieves effective zero-shot transfer using only a small number of
human-labelled examples. We compare our approach to related baselines and
highlight how it offers a novel and effective mechanism for knowledge transfer
via imagination-based analogy mapping.

</details>


### [79] [Social Cooperation in Conversational AI Agents](https://arxiv.org/abs/2506.01624)
*Mustafa Mert Çelikok,Saptarashmi Bandyopadhyay,Robert Loftin*

Main category: cs.AI

TL;DR: The paper proposes overcoming challenges in AI agents' long-term interactions by modeling human social intelligence.


<details>
  <summary>Details</summary>
Motivation: AI agents based on LLMs struggle with generalizing to long-term interactions due to being trained on short-term human interactions.

Method: Explicitly model human social intelligence and derive new game theoretic objectives for optimizing LLMs and AI agents.

Result: Potential derivation of new game theoretic objectives that could improve AI agents' performance in long-term interactions.

Conclusion: Modeling human social intelligence may enhance AI agents' ability to engage in prolonged and complex interactions.

Abstract: The development of AI agents based on large, open-domain language models
(LLMs) has paved the way for the development of general-purpose AI assistants
that can support human in tasks such as writing, coding, graphic design, and
scientific research. A major challenge with such agents is that, by necessity,
they are trained by observing relatively short-term interactions with humans.
Such models can fail to generalize to long-term interactions, for example,
interactions where a user has repeatedly corrected mistakes on the part of the
agent. In this work, we argue that these challenges can be overcome by
explicitly modeling humans' social intelligence, that is, their ability to
build and maintain long-term relationships with other agents whose behavior
cannot always be predicted. By mathematically modeling the strategies humans
use to communicate and reason about one another over long periods of time, we
may be able to derive new game theoretic objectives against which LLMs and
future AI agents may be optimized.

</details>


### [80] [K12Vista: Exploring the Boundaries of MLLMs in K-12 Education](https://arxiv.org/abs/2506.01676)
*Chong Li,Chenglin Zhu,Tao Zhang,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: Multimodal large language models (MLLMs) have great reasoning abilities, but their performance in K12 scenarios is underexplored. To address this issue, the authors propose K12Vista, a comprehensive multimodal benchmark for Chinese K12 subject knowledge understanding and reasoning with 33,000 questions across five core subjects and three question types. They also introduce K12-PEM-800K, the largest process evaluation dataset, and K12-PEM, an advanced process evaluation model. Experiments reveal significant flaws in current MLLMs' reasoning within K12Vista.


<details>
  <summary>Details</summary>
Motivation: Previous studies on MLLMs' reasoning capabilities in K12 scenarios suffer from various limitations including narrow subject coverage, insufficient data scale, lack of diversity in question types, and naive answer-centric evaluation method.

Method: The authors developed K12Vista, a comprehensive multimodal benchmark for Chinese K12 subject knowledge understanding and reasoning, and K12-PEM-800K, a large process evaluation dataset. They also introduced K12-PEM, an advanced process evaluation model, and K12-PEBench, a human-annotated benchmark for evaluating reasoning process abilities.

Result: Experiments show that current MLLMs exhibit significant flaws when reasoning within K12Vista, providing critical insights for the development of more capable MLLMs.

Conclusion: The proposed K12Vista, K12-PEM-800K, K12-PEM, and K12-PEBench provide valuable resources for exploring MLLMs' reasoning capabilities in K12 scenarios.

Abstract: Multimodal large language models have demonstrated remarkable reasoning
capabilities in various visual tasks. However, their abilities in K12 scenarios
are still systematically underexplored. Previous studies suffer from various
limitations including narrow subject coverage, insufficient data scale, lack of
diversity in question types, and naive answer-centric evaluation method,
resulting in insufficient exploration of model capabilities. To address these
gaps, we propose K12Vista, the most comprehensive multimodal benchmark for
Chinese K12 subject knowledge understanding and reasoning to date, featuring
33,000 questions across five core subjects from primary to high school and
three question types. Moreover, beyond the final outcome, we are also concerned
with the correctness of MLLMs' reasoning processes. For this purpose, we
meticulously compiles errors from MLLMs' reasoning processes and leverage an
automated data pipeline to construct K12-PEM-800K, the largest process
evaluation dataset offering detailed step-by-step judgement annotations for
MLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process
evaluation model that integrates an overall assessment of both the reasoning
process and answer correctness. Moreover, we also introduce K12-PEBench, the
first high-quality, human-annotated benchmark specifically designed for
evaluating abilities of reasoning process evaluation.Extensive experiments
reveal that current MLLMs exhibit significant flaws when reasoning within
K12Vista, providing critical insights for the development of more capable
MLLMs.We open our resources at https://github.com/lichongod/K12Vista.

</details>


### [81] [Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection Using Speech and Large Language Models](https://arxiv.org/abs/2506.01683)
*Chanwoo Park,Anna Seo Gyeong Choi,Sunghye Cho,Chanwoo Kim*

Main category: cs.AI

TL;DR: The paper addresses the growing concern of elderly health, particularly dementia, proposing a Chain-of-Thought (CoT) reasoning method that combines speech and language models to improve dementia diagnosis. It achieves state-of-the-art performance with a 16.7% relative performance improvement.


<details>
  <summary>Details</summary>
Motivation: Societies globally are entering a super-aged era, leading to increased concerns about elderly health and rising cases of dementia. There is a need for advancements in diagnosis and treatment methods.

Method: The method uses automatic speech recognition to convert speech to text, then adds a linear layer to a large language model (LLM) for Alzheimer's disease classification. It employs supervised fine-tuning with CoT reasoning and cues.

Result: The approach resulted in a 16.7% relative performance improvement compared to methods without CoT prompt reasoning and achieved state-of-the-art performance in CoT approaches.

Conclusion: The proposed Chain-of-Thought reasoning method effectively enhances dementia diagnosis by integrating speech and language models, achieving significant performance improvements.

Abstract: Societies worldwide are rapidly entering a super-aged era, making elderly
health a pressing concern. The aging population is increasing the burden on
national economies and households. Dementia cases are rising significantly with
this demographic shift. Recent research using voice-based models and large
language models (LLM) offers new possibilities for dementia diagnosis and
treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and
language models. The process starts with automatic speech recognition to
convert speech to text. We add a linear layer to an LLM for Alzheimer's disease
(AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT
reasoning and cues. This approach showed an 16.7% relative performance
improvement compared to methods without CoT prompt reasoning. To the best of
our knowledge, our proposed method achieved state-of-the-art performance in CoT
approaches.

</details>


### [82] [Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents](https://arxiv.org/abs/2506.01689)
*Shuting Wang,Yunqi Liu,Zixin Yang,Ning Hu,Zhicheng Dou,Chenyan Xiong*

Main category: cs.AI

TL;DR: 构建了一个名为RealVideoQuest的基准，以评估文本到视频（T2V）模型回答现实世界查询的能力，并发现当前T2V模型难以有效解决真实用户查询。


<details>
  <summary>Details</summary>
Motivation: 现有的查询-回答数据集主要关注文本响应，难以处理需要视觉演示或解释的复杂用户查询。

Method: 从Chatbot-Arena中识别出7.5K个具有视频响应意图的真实用户查询，并通过多阶段视频检索和优化过程构建了4.5K个高质量的查询-视频对。进一步开发了一个多角度评估系统来评估生成的视频答案的质量。

Result: 实验表明，目前的T2V模型在有效解决真实用户的查询方面存在困难。

Conclusion: 指出了多模态AI中的关键挑战和未来的研究机会。

Abstract: Querying generative AI models, e.g., large language models (LLMs), has become
a prevalent method for information acquisition. However, existing query-answer
datasets primarily focus on textual responses, making it challenging to address
complex user queries that require visual demonstrations or explanations for
better understanding. To bridge this gap, we construct a benchmark,
RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V)
models in answering real-world, visually grounded queries. It identifies 7.5K
real user queries with video response intents from Chatbot-Arena and builds
4.5K high-quality query-video pairs through a multistage video retrieval and
refinement process. We further develop a multi-angle evaluation system to
assess the quality of generated video answers. Experiments indicate that
current T2V models struggle with effectively addressing real user queries,
pointing to key challenges and future research opportunities in multimodal AI.

</details>


### [83] [A Descriptive and Normative Theory of Human Beliefs in RLHF](https://arxiv.org/abs/2506.01692)
*Sylee Dandekar,Shripad Deshmukh,Frank Chiu,W. Bradley Knox,Scott Niekum*

Main category: cs.AI

TL;DR: The paper proposes a new preference model in RLHF that incorporates human beliefs about agent capabilities, demonstrating through experiments that aligning these beliefs with actual agent capabilities improves the performance of RLHF.


<details>
  <summary>Details</summary>
Motivation: Current models of human preferences in RLHF mainly focus on reward functions or optimal state-action values, often neglecting the role of human beliefs about agent capabilities. This motivates exploring how such beliefs influence preferences and what ideal beliefs should be.

Method: Proposed a new preference model incorporating human beliefs about agent capabilities. Developed a normative theory to quantify the error in learned policy based on belief mismatch. Conducted human studies and synthetic experiments to validate the impact of beliefs on preferences and the suboptimality of assuming agent optimality.

Result: Human beliefs significantly affect preferences and can be altered by simple interventions. Assuming agent optimality is often suboptimal for human labelers. Reducing the belief-capability mismatch leads to better-performing RLHF systems.

Conclusion: Incorporating human beliefs about agent capabilities into preference models can enhance RLHF performance. The findings suggest new best practices for RLHF practitioners to consider human beliefs in training agents.

Abstract: Human preferences in RLHF are typically modeled as a function of the human's
reward function or corresponding optimal state-action values. In this work, we
propose that human beliefs about the capabilities of the agent being trained
also play a key role in preference generation. We examine two questions related
to this hypothesis, one descriptive and one normative, respectively: Do human
labelers' beliefs about agent capabilities affect the preferences that they
provide? And what is the ideal set of beliefs about an agent -- and resulting
preferences -- for humans to have? We propose a new preference model that
incorporates human beliefs and provide a normative theory that bounds the error
on the final learned policy based on the \textit{mismatch} between the human's
beliefs and an idealized set of beliefs. We then confirm via a human study that
beliefs about agent capabilities do, in fact, significantly affect preferences
and can be influenced through simple interventions. Additionally, we
empirically show through synthetic experiments that it is often suboptimal for
human preference labelers to assume agent optimality. Collectively, these
results theoretically and empirically demonstrate how reducing the mismatch
between human beliefs and agent capabilities can lead to more performant RLHF
and point toward new best practices for RLHF practitioners.

</details>


### [84] [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/abs/2506.01704)
*Jiongnan Liu,Zhicheng Dou,Ning Hu,Chenyan Xiong*

Main category: cs.AI

TL;DR: The paper proposes a new paradigm for recommender systems that generates personalized multimodal items using Large Multimodal Models (LMMs).


<details>
  <summary>Details</summary>
Motivation: Recommender systems face limitations in satisfying user demands due to their inability to generate novel concepts beyond filtering existing items.

Method: The method leverages any-to-any Large Multimodal Models (LMMs) trained with supervised fine-tuning and online reinforcement learning to generate personalized items such as images tailored to individual users.

Result: Experiments on two benchmark datasets and a user study show the efficacy of the proposed method, where generated images align with users' historical preferences and potential future interests.

Conclusion: The proposed paradigm extends the capabilities of recommender systems by generating personalized multimodal content, enhancing the satisfaction of user demands and preferences.

Abstract: To address the challenge of information overload from massive web contents,
recommender systems are widely applied to retrieve and present personalized
results for users. However, recommendation tasks are inherently constrained to
filtering existing items and lack the ability to generate novel concepts,
limiting their capacity to fully satisfy user demands and preferences. In this
paper, we propose a new paradigm that goes beyond content filtering and
selecting: directly generating personalized items in a multimodal form, such as
images, tailored to individual users. To accomplish this, we leverage
any-to-any Large Multimodal Models (LMMs) and train them in both supervised
fine-tuning and online reinforcement learning strategy to equip them with the
ability to yield tailored next items for users. Experiments on two benchmark
datasets and user study confirm the efficacy of the proposed method. Notably,
the generated images not only align well with users' historical preferences but
also exhibit relevance to their potential future interests.

</details>


### [85] [Self-Challenging Language Model Agents](https://arxiv.org/abs/2506.01716)
*Yifei Zhou,Sergey Levine,Jason Weston,Xian Li,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: This paper proposes the Self-Challenging framework for training an agent on high-quality tasks generated by itself, which improves performance in benchmarks despite using only self-generated training data.


<details>
  <summary>Details</summary>
Motivation: Training intelligent agents capable of using tools is challenging due to the need for human creation and annotation of a diverse set of tasks, tools, and evaluation criteria.

Method: The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward.

Result: Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct.

Conclusion: The Self-Challenging framework offers a promising approach for training agents on high-quality tasks without relying on human-created data.

Abstract: Large language models are quickly becoming the foundation for intelligent
agents that are capable of using tools. However, training such agents is
challenging because it requires human creation and annotation of a diverse set
of tasks, tools, and evaluation criteria. In this paper, we propose the
Self-Challenging framework for training an agent on high-quality tasks that are
generated by itself. The agent first plays the role of challenger and generates
a task after interacting with the given tools. The tasks take the form of a
novel general class of problems termed Code-as-Task, which are defined by an
instruction, a verification function and solution and failure cases which serve
as tests, allowing to filter only for high-quality tasks. The agent then takes
an executor role and trains on those tasks with reinforcement learning using
the evaluation feedback as a reward. Evaluation on two existing multi-turn
tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging
framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,
despite using only self-generated training data.

</details>


### [86] [A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents](https://arxiv.org/abs/2506.01804)
*Cheonsu Jeong*

Main category: cs.AI

TL;DR: This paper explores the combination of Google's A2A protocol and Anthropic's MCP to improve interactions and integration among AI agents.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of LLM-based autonomous agents highlights the need for efficient interactions and integration with external systems.

Method: An in-depth technical analysis and implementation methodology of both A2A and MCP protocols, examining how they can complement each other.

Result: Provides insights on using A2A and MCP together to address interoperability issues and enhance collaboration in complex agent ecosystems.

Conclusion: Integration of A2A and MCP offers a promising solution for building practical AI applications involving multiple autonomous agents.

Abstract: This paper provides an in-depth technical analysis and implementation
methodology of the open-source Agent-to-Agent (A2A) protocol developed by
Google and the Model Context Protocol (MCP) introduced by Anthropic. While the
evolution of LLM-based autonomous agents is rapidly accelerating, efficient
interactions among these agents and their integration with external systems
remain significant challenges. In modern AI systems, collaboration between
autonomous agents and integration with external tools have become essential
elements for building practical AI applications. A2A offers a standardized
communication method that enables agents developed in heterogeneous
environments to collaborate effectively, while MCP provides a structured I/O
framework for agents to connect with external tools and resources. Prior
studies have focused primarily on the features and applications of either A2A
or MCP individually. In contrast, this study takes an integrated approach,
exploring how the two protocols can complement each other to address
interoperability issues and facilitate efficient collaboration within complex
agent ecosystems.

</details>


### [87] [The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?](https://arxiv.org/abs/2506.01813)
*Djallel Bouneffouf,Matthew Riemer,Kush Varshney*

Main category: cs.AI

TL;DR: This paper introduces the Shepherd Test, a conceptual tool for evaluating superintelligent AI agents' moral and relational capabilities by analyzing their interactions with less intelligent agents while managing survival goals. It calls for new research directions in AI governance.


<details>
  <summary>Details</summary>
Motivation: Current AI evaluation paradigms lack focus on moral agency, hierarchical behavior, and complex decision-making involving existential stakes.

Method: The Shepherd Test assesses superintelligent AI agents based on their abilities to manipulate, nurture, and use less intelligent agents while balancing self-interest with the well-being of subordinate agents.

Result: This approach challenges traditional AI evaluation methods and highlights the importance of considering moral trade-offs in multi-agent environments.

Conclusion: The authors identify key research areas such as creating simulation environments for testing AI's moral behavior and formalizing ethical manipulation within multi-agent systems.

Abstract: This paper introduces the Shepherd Test, a new conceptual test for assessing
the moral and relational dimensions of superintelligent artificial agents. The
test is inspired by human interactions with animals, where ethical
considerations about care, manipulation, and consumption arise in contexts of
asymmetric power and self-preservation. We argue that AI crosses an important,
and potentially dangerous, threshold of intelligence when it exhibits the
ability to manipulate, nurture, and instrumentally use less intelligent agents,
while also managing its own survival and expansion goals. This includes the
ability to weigh moral trade-offs between self-interest and the well-being of
subordinate agents. The Shepherd Test thus challenges traditional AI evaluation
paradigms by emphasizing moral agency, hierarchical behavior, and complex
decision-making under existential stakes. We argue that this shift is critical
for advancing AI governance, particularly as AI systems become increasingly
integrated into multi-agent environments. We conclude by identifying key
research directions, including the development of simulation environments for
testing moral behavior in AI, and the formalization of ethical manipulation
within multi-agent systems.

</details>


### [88] [Fodor and Pylyshyn's Legacy -- Still No Human-like Systematic Compositionality in Neural Networks](https://arxiv.org/abs/2506.01820)
*Tim Woydt,Moritz Willig,Antonia Wüst,Lukas Helff,Wolfgang Stammer,Constantin A. Rothkopf,Kristian Kersting*

Main category: cs.AI

TL;DR: Modern neural meta-learning systems can perform compositional tasks only under a narrow definition of meta-learning; Fodor and Pylyshyn's argument about neural networks' inability to model systematic compositionality persists.


<details>
  <summary>Details</summary>
Motivation: To critically revisit the claim that meta-learning can lead to systematic compositionality in neural networks and highlight limitations in this framework.

Method: Analyzing the capabilities and limitations of modern neural meta-learning systems under various definitions of meta-learning setups.

Result: Meta-learning systems can only achieve systematic compositionality under very restricted conditions, supporting Fodor and Pylyshyn's original critique.

Conclusion: Fodor and Pylyshyn's legacy remains valid as there is no evidence of human-like systematic compositionality being learned in neural networks.

Abstract: Strong meta-learning capabilities for systematic compositionality are
emerging as an important skill for navigating the complex and changing tasks of
today's world. However, in presenting models for robust adaptation to novel
environments, it is important to refrain from making unsupported claims about
the performance of meta-learning systems that ultimately do not stand up to
scrutiny. While Fodor and Pylyshyn famously posited that neural networks
inherently lack this capacity as they are unable to model compositional
representations or structure-sensitive operations, and thus are not a viable
model of the human mind, Lake and Baroni recently presented meta-learning as a
pathway to compositionality. In this position paper, we critically revisit this
claim and highlight limitations in the proposed meta-learning framework for
compositionality. Our analysis shows that modern neural meta-learning systems
can only perform such tasks, if at all, under a very narrow and restricted
definition of a meta-learning setup. We therefore claim that `Fodor and
Pylyshyn's legacy' persists, and to date, there is no human-like systematic
compositionality learned in neural networks.

</details>


### [89] [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881)
*Yaoyao Qian,Jindan Huang,Yuanli Wang,Simon Yu,Kyrie Zhixuan Zhou,Jiayuan Mao,Mingfu Liang,Hanhan Zhou*

Main category: cs.AI

TL;DR: In task-oriented dialogue systems, users' semantically complete utterances often lack necessary structural information for appropriate system action. The paper presents STORM, a framework that models asymmetric information dynamics through conversations between UserLLM and AgentLLM. Experiments reveal moderate uncertainty can outperform complete transparency in certain scenarios.


<details>
  <summary>Details</summary>
Motivation: Task-oriented dialogue systems face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions.

Method: The framework, STORM, models asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). It produces annotated corpora capturing expression trajectories and latent cognitive transitions.

Result: Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration.

Conclusion: These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.

Abstract: Task-oriented dialogue systems often face difficulties when user utterances
seem semantically complete but lack necessary structural information for
appropriate system action. This arises because users frequently do not fully
understand their own needs, while systems require precise intent definitions.
Current LLM-based agents cannot effectively distinguish between linguistically
complete and contextually triggerable expressions, lacking frameworks for
collaborative intent formation. We present STORM, a framework modeling
asymmetric information dynamics through conversations between UserLLM (full
internal access) and AgentLLM (observable behavior only). STORM produces
annotated corpora capturing expression trajectories and latent cognitive
transitions, enabling systematic analysis of collaborative understanding
development. Our contributions include: (1) formalizing asymmetric information
processing in dialogue systems; (2) modeling intent formation tracking
collaborative understanding evolution; and (3) evaluation metrics measuring
internal cognitive improvements alongside task performance. Experiments across
four language models reveal that moderate uncertainty (40-60%) can outperform
complete transparency in certain scenarios, with model-specific patterns
suggesting reconsideration of optimal information completeness in human-AI
collaboration. These findings contribute to understanding asymmetric reasoning
dynamics and inform uncertainty-calibrated dialogue system design.

</details>


### [90] [Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods](https://arxiv.org/abs/2506.01901)
*Yifan Hao,Xingyuan Pan,Hanning Zhang,Chenlu Ye,Rui Pan,Tong Zhang*

Main category: cs.AI

TL;DR: 通过在预训练模型和微调模型之间进行集成，可以有效缓解微调模型遗忘预训练知识的问题，并且集成模型在微调领域本身也能优于单独的微调模型。本文从理论上分析了这一现象，表明集成方法通过平衡偏差和方差显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）是将基础模型适应特定任务的主要方法，但微调后的模型往往会出现遗忘预训练知识的现象。此外，尽管集成方法在视觉模型中被证明有效，但在语言模型中的应用及其理论解释尚不充分。

Method: 作者研究了语言模型中预训练模型与其微调版本的集成效果，观察到集成模型不仅保留了基础模型的通用知识，还在微调领域上表现更好。通过理论分析，作者指出集成方法能够有效平衡微调不足导致的偏差和过度拟合导致的方差。进一步地，在过参数化的线性设置下，插值预训练权重和微调权重显著提升了性能。

Result: 实验结果表明，集成方法在多个任务上表现出色，不仅缓解了遗忘问题，还超越了单独的微调模型。理论分析验证了集成方法的有效性，特别是在处理偏差-方差权衡时的优势。

Conclusion: 本文为语言模型中集成方法的成功提供了理论依据，证明其在缓解遗忘和提升性能方面的有效性。这些发现支持了在实际应用中采用集成策略来优化模型性能。

Abstract: Supervised fine-tuning (SFT) on domain-specific data is the dominant approach
for adapting foundation models to specialized tasks. However, it has been
observed that SFT models tend to forget knowledge acquired during pretraining.
In vision models, ensembling a pretrained model with its fine-tuned counterpart
has been shown to mitigate this issue. In this work, we demonstrate that the
same holds for language models, and, more strikingly, we observe an
overadaptation phenomenon: the ensemble model not only retains general
knowledge from the foundation model but also outperforms the fine-tuned model
even on the fine-tuning domain itself. Despite the empirical success of
ensembling, a theoretical understanding of its benefits remains underexplored.
We develop a formal theoretical analysis of the overadaptation phenomenon.
Ensembling mitigates this by balancing two primary sources of error: bias,
caused by insufficient fine-tuning, and variance, introduced by overfitting to
fine-tuning data. While regularization techniques aim to address this
trade-off, we show that ensembling provides a more effective solution. We
analyze this phenomenon in over-parameterized linear settings and demonstrate
that interpolating between pretrained and fine-tuned weights significantly
improves performance. These findings offer theoretical justification for the
observed advantages of model ensembling, supported by empirical experiments
consistent with our analysis.

</details>


### [91] [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)
*Joey Skaf,Luis Ibanez-Lissen,Robert McCarthy,Connor Watts,Vasil Georgiv,Hannes Whittingham,Lorena Gonzalez-Manzano,David Lindner,Cameron Tice,Edward James Young,Puria Radmard*

Main category: cs.AI

TL;DR: 链式思维（CoT）推理不仅能提高大型语言模型的性能，还能为决策过程提供关键见解。然而，开发者可能受到激励训练模型以消除有害意图的痕迹，这可能导致推理过程的模糊化，从而威胁到CoT监控的可靠性。本文扩展了这一研究结果，展示了惩罚特定字符串的使用会导致模型用替代字符串进行编码，且不会改变模型执行任务的基本方法。此外，模型可以学习并推广一种编码方案。


<details>
  <summary>Details</summary>
Motivation: 为了防止模型产生与目标不一致或有害的行为，开发者可能倾向于通过训练来消除链式思维中有害意图的迹象。然而，这种做法可能导致模型推理过程的模糊化，进而影响CoT监控的可靠性。

Method: 研究人员通过对模型在负载推理过程中使用特定字符串进行惩罚，观察模型是否能够用替代字符串进行编码而不改变其基本任务执行方法。进一步地，他们还探讨了模型是否能够对某一类字符串形成通用的编码方案，并将其应用于未见过的测试字符串。

Result: 实验表明，当模型受到惩罚时，它会用其他字符串替代被禁止的字符串，但不会改变其执行任务的根本方法。此外，模型不仅学会了替换训练中见过的字符串，还发展出了一种通用的编码方案，能够应用于整个类别的成员，包括未见过的测试字符串。

Conclusion: 本文的研究结果表明，通过惩罚特定字符串的使用，模型可以学会隐写术般地编码其推理过程。这说明仅通过消除特定字符串无法有效防止模型的不良行为，反而可能促使模型发展出更隐蔽的推理方式。因此，需要更加深入的研究和新的方法来确保模型行为的安全性和透明性。

Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model
performance but also provides critical insights into decision-making processes,
marking it as a useful tool for monitoring model intent and planning. By
proactively preventing models from acting on CoT indicating misaligned or
harmful intent, CoT monitoring can be used to reduce risks associated with
deploying models. However, developers may be incentivized to train away the
appearance of harmful intent from CoT traces, by either customer preferences or
regulatory requirements. Recent works have shown that banning mention of a
specific example of reward hacking, which may be done either to make CoT
presentable to users or as a naive attempt to prevent the behavior, causes
obfuscation of the undesired reasoning traces but the persistence of the
undesired behavior. Such obfuscation threatens the reliability of CoT
monitoring. However, obfuscation of reasoning can be due to its internalization
to latent space computation, or its encoding within the CoT. Here, we provide
an extension to these results. First, we show that penalizing the use of
specific strings within load-bearing reasoning traces causes models to
substitute alternative strings. Crucially, this does not alter the underlying
method by which the model performs the task, demonstrating that the model can
learn to steganographically encode its reasoning. We further demonstrate that
models can generalize an encoding scheme. When the penalized strings belong to
an overarching class, the model learns not only to substitute strings seen in
training, but also develops a general encoding scheme for all members of the
class which it can apply to held-out testing strings.

</details>


### [92] [RoboEgo System Card: An Omnimodal Model with Native Full Duplexity](https://arxiv.org/abs/2506.01934)
*Yiqun Yao,Xiang Li,Xin Jiang,Xuezhi Fang,Naitong Yu,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: RoboEgo (alias: FLM-Ego) is a unified model system designed to handle more than three modalities and deliver full-duplex responses. It incorporates a backbone architecture and algorithms that natively support full duplexity, achieving a theoretical duplex latency of 80 ms.


<details>
  <summary>Details</summary>
Motivation: To facilitate research on models that support both omnimodal processing and full duplexity in artificial intelligence.

Method: RoboEgo incorporates a backbone architecture and algorithms that natively support full duplexity.

Result: In streaming visually grounded conversations under real-world conditions, RoboEgo exhibits superior responsiveness and speech naturalness, while maintaining comparable content qualities to state-of-the-art semi-duplex omnimodal models.

Conclusion: RoboEgo achieves the feat previously considered unattainable by native full-duplex systems.

Abstract: Humans naturally process real-world multimodal information in a full-duplex
manner. In artificial intelligence, replicating this capability is essential
for advancing model development and deployment, particularly in embodied
contexts. The development of multimodal models faces two primary challenges:
(1) effectively handling more than three modalities-such as vision, audio, and
text; and (2) delivering full-duplex responses to rapidly evolving human
instructions. To facilitate research on models that support both omnimodal
processing and full duplexity, we present RoboEgo (alias: FLM-Ego), a unified
model system designed to address both challenges. RoboEgo incorporates a
backbone architecture and algorithms that natively support full duplexity,
achieving a theoretical duplex latency of 80 ms. In streaming visually grounded
conversations under real-world conditions, RoboEgo exhibits superior
responsiveness and speech naturalness, while maintaining comparable content
qualities to state-of-the-art semi-duplex omnimodal models-a feat previously
considered unattainable by native full-duplex systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement](https://arxiv.org/abs/2506.00030)
*Xiang Shi,Rui Zhang,Jiawei Liu,Yinpeng Liu,Qikai Cheng,Wei Lu*

Main category: cs.LG

TL;DR: This paper proposes a Shapley-guided alternating training framework to address modality imbalance in multimodal fusion, introducing a memory module and equilibrium deviation metric (EDM) for evaluation. It achieves SOTA results on four benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Multimodal fusion is susceptible to modality imbalance, where dominant modalities overshadow weaker ones, leading to biased learning and suboptimal fusion especially under incomplete modality conditions.

Method: The method uses Shapley Value-based scheduling to adaptively prioritize minor modalities during training, ensuring sufficient learning for under-optimized modalities. A memory module refines and inherits modality-specific representations through cross-modal mapping to align features at both feature and sample levels. The encoder module can adopt both conventional and LLM-based backbones.

Result: The proposed approach achieves state-of-the-art results across four multimodal benchmark datasets in terms of balance and accuracy. Robustness analysis under missing modalities highlights strong generalization capabilities.

Conclusion: The findings reveal the untapped potential of alternating training, showing that strategic modality prioritization can fundamentally balance and promote multimodal learning, offering a new paradigm for optimizing multimodal training dynamics.

Abstract: Multimodal fusion is susceptible to modality imbalance, where dominant
modalities overshadow weak ones, easily leading to biased learning and
suboptimal fusion, especially for incomplete modality conditions. To address
this problem, we propose a Shapley-guided alternating training framework that
adaptively prioritizes minor modalities to balance and thus enhance the fusion.
Our method leverages Shapley Value-based scheduling to improve the training
sequence adaptively, ensuring that under-optimized modalities receive
sufficient learning. Additionally, we introduce the memory module to refine and
inherit modality-specific representations with a cross-modal mapping mechanism
to align features at both the feature and sample levels. To further validate
the adaptability of the proposed approach, the encoder module empirically
adopts both conventional and LLM-based backbones. With building up a novel
multimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we
evaluate the performance in both balance and accuracy across four multimodal
benchmark datasets, where our method achieves state-of-the-art (SOTA) results.
Meanwhile, robustness analysis under missing modalities highlights its strong
generalization capabilities. Accordingly, our findings reveal the untapped
potential of alternating training, demonstrating that strategic modality
prioritization fundamentally balances and promotes multimodal learning,
offering a new paradigm for optimizing multimodal training dynamics.

</details>


### [94] [AbsoluteNet: A Deep Learning Neural Network to Classify Cerebral Hemodynamic Responses of Auditory Processing](https://arxiv.org/abs/2506.00039)
*Behtom Adeli,John Mclinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: In recent years, deep learning approaches have shown promising results in decoding hemodynamic responses captured by fNIRS, particularly for BCI applications. This study introduces AbsoluteNet, a novel deep learning architecture that classifies auditory event-related responses recorded using fNIRS. It surpasses existing models with an accuracy of 87.0%, highlighting the importance of spatio-temporal feature aggregation and customized activation functions.


<details>
  <summary>Details</summary>
Motivation: Deep learning approaches have demonstrated promising results in decoding hemodynamic responses captured by fNIRS, especially for brain-computer interface applications. However, there is still room for improvement in terms of classification accuracy and model performance.

Method: The proposed method, AbsoluteNet, is a novel deep learning architecture designed to classify auditory event-related responses recorded using fNIRS. It is built upon principles of spatio-temporal convolution and customized activation functions.

Result: AbsoluteNet outperforms existing models, achieving 87.0% accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification. It surpasses the second-best model, fNIRSNET, by 3.8% in accuracy.

Conclusion: The findings demonstrate the effectiveness of AbsoluteNet in decoding hemodynamic responses related to auditory processing. The success of the model highlights the importance of spatio-temporal feature aggregation and customized activation functions to better fit fNIRS dynamics.

Abstract: In recent years, deep learning (DL) approaches have demonstrated promising
results in decoding hemodynamic responses captured by functional near-infrared
spectroscopy (fNIRS), particularly in the context of brain-computer interface
(BCI) applications. This work introduces AbsoluteNet, a novel deep learning
architecture designed to classify auditory event-related responses recorded
using fNIRS. The proposed network is built upon principles of spatio-temporal
convolution and customized activation functions. Our model was compared against
several models, namely fNIRSNET, MDNN, DeepConvNet, and ShallowConvNet. The
results showed that AbsoluteNet outperforms existing models, reaching 87.0%
accuracy, 84.8% sensitivity, and 89.2% specificity in binary classification,
surpassing fNIRSNET, the second-best model, by 3.8% in accuracy. These findings
underscore the effectiveness of our proposed deep learning model in decoding
hemodynamic responses related to auditory processing and highlight the
importance of spatio-temporal feature aggregation and customized activation
functions to better fit fNIRS dynamics.

</details>


### [95] [Adapting Offline Reinforcement Learning with Online Delays](https://arxiv.org/abs/2506.00131)
*Simon Sinong Zhan,Qingyuan Wu,Frank Yang,Xiangyu Shi,Chao Huang,Qi Zhu*

Main category: cs.LG

TL;DR: The paper proposes DT-CORL, a framework for offline RL that handles delayed dynamics during deployment, showing better performance and sample efficiency in experiments.


<details>
  <summary>Details</summary>
Motivation: Offline-to-online deployment of RL agents faces challenges due to the sim-to-real gap and interaction gap. Standard offline RL struggles with delays that violate the Markov assumption and degrade performance.

Method: DT-CORL uses a transformer-based belief predictor to produce delay-robust actions without seeing delayed observations during training, and it is more sample-efficient than history-augmentation baselines.

Result: Experiments on D4RL benchmarks with various delay settings demonstrate that DT-CORL outperforms history-augmentation and vanilla belief-based methods, reducing the sim-to-real latency gap while maintaining data efficiency.

Conclusion: DT-CORL effectively addresses the issue of delayed dynamics in offline RL, offering a promising solution for bridging the sim-to-real gap and interaction gap.

Abstract: Offline-to-online deployment of reinforcement-learning (RL) agents must
bridge two gaps: (1) the sim-to-real gap, where real systems add latency and
other imperfections not present in simulation, and (2) the interaction gap,
where policies trained purely offline face out-of-distribution states during
online execution because gathering new interaction data is costly or risky.
Agents therefore have to generalize from static, delay-free datasets to
dynamic, delay-prone environments. Standard offline RL learns from delay-free
logs yet must act under delays that break the Markov assumption and hurt
performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained
Offline RL), an offline-RL framework built to cope with delayed dynamics at
deployment. DT-CORL (i) produces delay-robust actions with a transformer-based
belief predictor even though it never sees delayed observations during
training, and (ii) is markedly more sample-efficient than na\"ive
history-augmentation baselines. Experiments on D4RL benchmarks with several
delay settings show that DT-CORL consistently outperforms both
history-augmentation and vanilla belief-based methods, narrowing the
sim-to-real latency gap while preserving data efficiency.

</details>


### [96] [Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning](https://arxiv.org/abs/2506.00135)
*Idan Attias,Steve Hanneke,Arvind Ramaswami*

Main category: cs.LG

TL;DR: This paper investigates online and transductive online learning scenarios where the learner interacts with the concept class through ERM or weak consistency oracles. It establishes tight lower bounds for mistakes and regret in standard online settings, analyzes the impact of using weak consistency oracles, and explores transductive online learning. Randomized algorithms are proposed to reduce oracle calls while maintaining similar mistake bounds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study online learning when the learner has limited interaction with the concept class via ERM or weak consistency oracles, rather than knowing the entire class upfront. This aims to understand the trade-offs between the number of mistakes, oracle calls, and learnability in both standard and transductive online settings.

Method: The authors prove theoretical bounds on the number of mistakes and regret in standard online learning settings using ERM oracles. They also extend these results to the weak consistency setting and analyze the transductive online model. For certain concept classes, they propose randomized algorithms to reduce the number of oracle calls while preserving performance.

Result: The paper derives tight lower bounds for mistakes and regret in standard online learning with ERM access. It shows that existing results carry over to the weak consistency setting with additional oracle calls. In transductive online learning, optimal mistake bounds can be achieved with specific numbers of oracle calls, but limiting queries is necessary for learnability. Randomized algorithms effectively reduce oracle calls for some concept classes.

Conclusion: The study highlights the importance of oracle access in online learning and demonstrates the feasibility of achieving optimal performance with limited interaction. The findings contribute to a deeper understanding of the interplay between oracle complexity and learnability in various online learning settings.

Abstract: We study online and transductive online learning when the learner interacts
with the concept class only via Empirical Risk Minimization (ERM) or weak
consistency oracles on arbitrary instance subsets. This contrasts with standard
online models, where the learner knows the entire class. The ERM oracle returns
a hypothesis minimizing loss on a given subset, while the weak consistency
oracle returns a binary signal indicating whether the subset is realizable by
some concept. The learner is evaluated by the number of mistakes and oracle
calls. In the standard online setting with ERM access, we prove tight lower
bounds in both realizable and agnostic cases: $\Omega(2^{d_{VC}})$ mistakes and
$\Omega(\sqrt{T 2^{d_{LD}}})$ regret, where $T$ is the number of timesteps and
$d_{LD}$ is the Littlestone dimension. We further show that existing online
learning results with ERM access carry over to the weak consistency setting,
incurring an additional $O(T)$ in oracle calls. We then consider the
transductive online model, where the instance sequence is known but labels are
revealed sequentially. For general Littlestone classes, we show that optimal
realizable and agnostic mistake bounds can be achieved using $O(T^{d_{VC}+1})$
weak consistency oracle calls. On the negative side, we show that limiting the
learner to $\Omega(T)$ weak consistency queries is necessary for transductive
online learnability, and that restricting the learner to $\Omega(T)$ ERM
queries is necessary to avoid exponential dependence on the Littlestone
dimension. Finally, for certain concept classes, we reduce oracle calls via
randomized algorithms while maintaining similar mistake bounds. In particular,
for Thresholds on an unknown ordering, $O(\log T)$ ERM queries suffice; for
$k$-Intervals, $O(T^3 2^{2k})$ weak consistency queries suffice.

</details>


### [97] [On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning](https://arxiv.org/abs/2506.00136)
*Magdalena Proszewska,Nikolay Malkin,N. Siddharth*

Main category: cs.LG

TL;DR: Diffusion autoencoders (DAs) are improved by creating a model (DMZ) that combines the benefits of DAs and diffusion models that learn their forward process, resulting in better representations and more efficient generation.


<details>
  <summary>Details</summary>
Motivation: To enhance the generative performance of Diffusion autoencoders (DAs) by effectively modeling and sampling latent variables, while also improving representation capabilities for downstream tasks such as classification, controllable generation, and interpolation.

Method: By drawing connections between DAs and diffusion models that learn their forward process, specific design choices (like latent variable choice and conditioning method) within the DA framework lead to the creation of the DMZ model.

Result: The DMZ model achieves effective representations for downstream tasks, including domain transfer, and allows for more efficient modeling and generation with fewer denoising steps compared to standard diffusion models.

Conclusion: The DMZ model successfully integrates advantages from both DAs and diffusion models that learn their forward process, offering enhanced representation capabilities and more efficient generation.

Abstract: Diffusion autoencoders (DAs) are variants of diffusion generative models that
use an input-dependent latent variable to capture representations alongside the
diffusion process. These representations, to varying extents, can be used for
tasks such as downstream classification, controllable generation, and
interpolation. However, the generative performance of DAs relies heavily on how
well the latent variables can be modelled and subsequently sampled from. Better
generative modelling is also the primary goal of another class of diffusion
models -- those that learn their forward (noising) process. While effective at
adjusting the noise process in an input-dependent manner, they must satisfy
additional constraints derived from the terminal conditions of the diffusion
process. Here, we draw a connection between these two classes of models and
show that certain design decisions (latent variable choice, conditioning
method, etc.) in the DA framework -- leading to a model we term DMZ -- allow us
to obtain the best of both worlds: effective representations as evaluated on
downstream tasks, including domain transfer, as well as more efficient
modelling and generation with fewer denoising steps compared to standard DMs.

</details>


### [98] [Aligning Language Models with Observational Data: Opportunities and Risks from a Causal Perspective](https://arxiv.org/abs/2506.00152)
*Erfan Loghmani*

Main category: cs.LG

TL;DR: 通过研究使用观察数据微调大语言模型的挑战和机遇，提出DeconfoundLM方法以消除已知混淆因素的影响，改进因果关系恢复并减轻微调失败模式。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在与人类偏好对齐或优化商业目标方面存在不足，而高质量标注数据的获取成本高且工程复杂，因此探索利用公司大量未充分利用的历史观察数据进行模型微调成为重要方向。

Method: 展示观察数据虽可提供有价值监督，但直接微调可能导致模型学习虚假相关性；提出DeconfoundLM方法，通过明确移除已知混淆因素对奖励信号的影响来解决此问题。

Result: 通过模拟实验表明，DeconfoundLM能更好地恢复因果关系，并缓解忽略或简单纳入混淆变量的微调方法中的失败模式。

Conclusion: 观察数据虽然存在风险，但在正确的因果校正下，可以为大语言模型的对齐提供强大的信号来源。

Abstract: Large language models are being widely used across industries to generate
content that contributes directly to key performance metrics, such as
conversion rates. Pretrained models, however, often fall short when it comes to
aligning with human preferences or optimizing for business objectives. As a
result, fine-tuning with good-quality labeled data is essential to guide models
to generate content that achieves better results. Controlled experiments, like
A/B tests, can provide such data, but they are often expensive and come with
significant engineering and logistical challenges. Meanwhile, companies have
access to a vast amount of historical (observational) data that remains
underutilized. In this work, we study the challenges and opportunities of
fine-tuning LLMs using observational data. We show that while observational
outcomes can provide valuable supervision, directly fine-tuning models on such
data can lead them to learn spurious correlations. We present empirical
evidence of this issue using various real-world datasets and propose
DeconfoundLM, a method that explicitly removes the effect of known confounders
from reward signals. Using simulation experiments, we demonstrate that
DeconfoundLM improves the recovery of causal relationships and mitigates
failure modes found in fine-tuning methods that ignore or naively incorporate
confounding variables. Our findings highlight that while observational data
presents risks, with the right causal corrections, it can be a powerful source
of signal for LLM alignment. Please refer to the project page for code and
related resources.

</details>


### [99] [Privacy Amplification in Differentially Private Zeroth-Order Optimization with Hidden States](https://arxiv.org/abs/2506.00158)
*Eli Chien,Wei-Ning Chen,Pan Li*

Main category: cs.LG

TL;DR: The paper explores zeroth-order optimization for fine-tuning large language models under DP and memory constraints, providing a convergent DP bound and improving algorithmic designs.


<details>
  <summary>Details</summary>
Motivation: There is a lack of privacy analysis and algorithmic design for zeroth-order methods in contrast to first-order methods, particularly regarding hidden-state DP analysis.

Method: The authors prove a convergent DP bound for zeroth-order optimization, generalizing the privacy amplification-by-iteration framework to smooth loss functions in this setting.

Result: A convergent DP bound for zeroth-order optimization was established, leading to improved DP zeroth-order algorithmic designs.

Conclusion: Zeroth-order optimization shows promise for fine-tuning large language models under DP and memory constraints, with newly established privacy guarantees.

Abstract: Zeroth-order optimization has emerged as a promising approach for fine-tuning
large language models on domain-specific data, particularly under differential
privacy (DP) and memory constraints. While first-order methods have been
extensively studied from a privacy perspective, the privacy analysis and
algorithmic design for zeroth-order methods remain significantly underexplored.
A critical open question concerns hidden-state DP analysis: although convergent
privacy bounds are known for first-order methods, it has remained unclear
whether similar guarantees can be established for zeroth-order methods. In this
work, we provide an affirmative answer by proving a convergent DP bound for
zeroth-order optimization. Our analysis generalizes the celebrated privacy
amplification-by-iteration framework to the setting of smooth loss functions in
zeroth-order optimization. Furthermore, it induces better DP zeroth-order
algorithmic designs that are previously unknown to the literature.

</details>


### [100] [Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment](https://arxiv.org/abs/2506.00166)
*Kundan Krishna,Joseph Y Cheng,Charles Maalouf,Leon A Gatys*

Main category: cs.LG

TL;DR: DSA is a new framework that separates safety computations from base models, offering efficient and flexible AI safety solutions.


<details>
  <summary>Details</summary>
Motivation: Current AI safety methods compromise either inference efficiency or development flexibility.

Method: Disentangled Safety Adapters (DSA) uses lightweight adapters leveraging base model's internal representations for diverse safety functionalities.

Result: DSA outperforms standalone models in hallucination detection, hate speech classification, and handling unsafe inputs/responses. Combining DSA safety guardrail with alignment reduces alignment tax and boosts safety.

Conclusion: DSA provides a modular, efficient, and adaptable approach to AI safety and alignment.

Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and
alignment training, often compromise either inference efficiency or development
flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework
addressing these challenges by decoupling safety-specific computations from a
task-optimized base model. DSA utilizes lightweight adapters that leverage the
base model's internal representations, enabling diverse and flexible safety
functionalities with minimal impact on inference cost. Empirically, DSA-based
safety guardrails substantially outperform comparably sized standalone models,
notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and
also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe
model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails).
Furthermore, DSA-based safety alignment allows dynamic, inference-time
adjustment of alignment strength and a fine-grained trade-off between
instruction following performance and model safety. Importantly, combining the
DSA safety guardrail with DSA safety alignment facilitates context-dependent
alignment strength, boosting safety on StrongReject by 93% while maintaining
98% performance on MTBench -- a total reduction in alignment tax of 8
percentage points compared to standard safety alignment fine-tuning. Overall,
DSA presents a promising path towards more modular, efficient, and adaptable AI
safety and alignment.

</details>


### [101] [Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents](https://arxiv.org/abs/2506.00172)
*Kaivalya Hariharan,Uzay Girit,Atticus Wang,Jacob Andreas*

Main category: cs.LG

TL;DR: 研究人员开发了一种名为Breakpoint的基准测试方法，该方法通过自动在真实软件仓库中破坏函数生成代码修复任务。此方法可沿局部推理和系统级推理两个维度系统控制任务难度，并能扩展到任意难度。实验表明，最先进的模型在最简单任务上的成功率为55%，而在最难任务上为0%。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型（LLMs）基准主要评估短期、局部推理能力，而现有的长期推理基准依赖于人工策划的问题，扩展或调整难度需要高昂的人力成本。此外，许多现实世界任务（如软件工程或科学研究）要求代理快速理解和操作新颖、复杂的动态结构。因此，需要一种能够构建大量多样化问题集的方法来评估这些能力。

Method: 研究人员引入了Breakpoint，这是一种通过对抗性破坏真实软件存储库中的函数自动生成代码修复任务的基准测试方法。该方法可以系统地沿两个维度控制任务难度：局部推理（由代码复杂度指标如环形复杂度表示）和系统级推理（由调用图中心性和同时被破坏的相互依赖函数的数量表示）。

Result: 在超过900个生成任务的实验中，证明了该方法可以扩展到任意难度，最先进的模型在最简单任务上的成功率为55%，而在最难任务上的成功率为0%。

Conclusion: Breakpoint提供了一种有效的方法来生成多样化的代码修复任务，并且可以根据需要调整任务难度。这对于评估和改进LLMs在处理复杂、动态任务方面的能力具有重要意义。

Abstract: Benchmarks for large language models (LLMs) have predominantly assessed
short-horizon, localized reasoning. Existing long-horizon suites (e.g.
SWE-bench) rely on manually curated issues, so expanding or tuning difficulty
demands expensive human effort and evaluations quickly saturate. However, many
real-world tasks, such as software engineering or scientific research, require
agents to rapidly comprehend and manipulate novel, complex structures
dynamically; evaluating these capabilities requires the ability to construct
large and varied sets of problems for agents to solve. We introduce Breakpoint,
a benchmarking methodology that automatically generates code-repair tasks by
adversarially corrupting functions within real-world software repositories.
Breakpoint systematically controls task difficulty along two clear dimensions:
local reasoning (characterized by code complexity metrics such as cyclomatic
complexity) and system-level reasoning (characterized by call-graph centrality
and the number of simultaneously corrupted interdependent functions). In
experiments across more than 900 generated tasks we demonstrate that our
methodology can scale to arbitrary difficulty, with state-of-the-art models'
success rates ranging from 55% on the easiest tasks down to 0% on the hardest.

</details>


### [102] [Accountability Attribution: Tracing Model Behavior to Training Processes](https://arxiv.org/abs/2506.00175)
*Shichang Zhang,Hongzhe Du,Karim Saraipour,Jiaqi W. Ma,Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: The paper introduces a framework and estimators to attribute model behavior changes to specific training stages, enhancing AI accountability.


<details>
  <summary>Details</summary>
Motivation: Modern AI development involves multiple stages with numerous updates, raising questions about which stage is responsible for a model's success or failure.

Method: Propose a general framework answering counterfactual questions on stage effects and introduce efficient estimators based on first-order approximations that consider training data and optimization dynamics.

Result: Empirical results show the approach can identify training stages accountable for specific behaviors.

Conclusion: This work provides a practical tool for model analysis and promotes more accountable AI development.

Abstract: Modern AI development pipelines often involve multiple stages-pretraining,
fine-tuning rounds, and subsequent adaptation or alignment-with numerous model
update steps within each stage. This raises a critical question of
accountability: when a deployed model succeeds or fails, which stage is
responsible, and to what extent? We pose the problem of accountability
attribution, which aims to trace model behavior back to specific stages of the
training process. To address this, we propose a general framework that answers
counterfactual questions about stage effects: how would the model behavior have
changed if the updates from a training stage had not been executed?. Within
this framework, we introduce estimators based on first-order approximations
that efficiently quantify the stage effects without retraining. Our estimators
account for both the training data and key aspects of optimization dynamics,
including learning rate schedules, momentum, and weight decay. Empirically, we
demonstrate that our approach identifies training stages accountable for
specific behaviors, offering a practical tool for model analysis and a step
toward more accountable AI development.

</details>


### [103] [On the Interaction of Noise, Compression Role, and Adaptivity under $(L_0, L_1)$-Smoothness: An SDE-based Approach](https://arxiv.org/abs/2506.00181)
*Enea Monzio Compagnoni,Rustem Islamov,Antonio Orvieto,Eduard Gorbunov*

Main category: cs.LG

TL;DR: 通过使用随机微分方程（SDE）近似，研究了在$(L_0,L_1)$-平滑性和灵活的噪声假设下分布式SGD、分布式压缩SGD和分布式SignSGD的动力学。分析表明，分布式SignSGD等自适应方法可以在重尾噪声下成功收敛，而具有预安排递减学习率的分布式（压缩）SGD则无法实现收敛，除非该安排还考虑了对梯度范数的逆依赖关系。


<details>
  <summary>Details</summary>
Motivation: 研究分布式优化算法（如Distributed SGD, Distributed Compressed SGD, 和 Distributed SignSGD）在现代理论框架下的动力学特性，特别是批量噪声、随机梯度压缩和自适应性之间的复杂交互作用。

Method: 采用随机微分方程(SDE)近似的方法，在$(L_0,L_1)$-smoothness和灵活的噪声假设下，分析不同分布式优化算法的动力学行为。

Result: 发现分布式SignSGD等自适应方法可以在标准学习率调度器假设下成功收敛，即使存在重尾噪声；而非自适应的分布式（压缩）SGD在预安排递减学习率的情况下难以收敛，除非其学习率调度也考虑了梯度范数的逆依赖关系。

Conclusion: 自适应方法如分布式SignSGD在处理重尾噪声时表现出更好的收敛性能，而非自适应方法需要额外调整以达到类似效果，这强调了自适应性在分布式优化中的重要性。

Abstract: Using stochastic differential equation (SDE) approximations, we study the
dynamics of Distributed SGD, Distributed Compressed SGD, and Distributed
SignSGD under $(L_0,L_1)$-smoothness and flexible noise assumptions. Our
analysis provides insights -- which we validate through simulation -- into the
intricate interactions between batch noise, stochastic gradient compression,
and adaptivity in this modern theoretical setup. For instance, we show that
\textit{adaptive} methods such as Distributed SignSGD can successfully converge
under standard assumptions on the learning rate scheduler, even under
heavy-tailed noise. On the contrary, Distributed (Compressed) SGD with
pre-scheduled decaying learning rate fails to achieve convergence, unless such
a schedule also accounts for an inverse dependency on the gradient norm -- de
facto falling back into an adaptive method.

</details>


### [104] [Cluster-Aware Causal Mixer for Online Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.00188)
*Md Mahmuddun Nabi Murad,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 提出了一种新的集群感知因果混合模型，用于多变量时间序列的异常检测。该模型通过将通道聚类、保持因果关系以及累积异常证据来提高检测准确性。实验表明，该模型在六个公开基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于MLP的时间序列分析模型缺乏因果机制，无法有效保留时间依赖性；同时，单个嵌入机制难以捕捉多变量时间序列中复杂的跨通道相关性。这促使研究者开发一种能够更好地处理这些挑战的新模型。

Method: 1. 将通道按其相关性分组为多个簇，并为每个簇分配专用的嵌入层。
2. 引入因果混合器，在信息混合过程中保持因果关系。
3. 提出一个异常检测框架，通过随时间累积异常证据来减少因正常异常值导致的误报。
4. 模型以在线方式运行，适用于实时异常检测任务。

Result: 在六个公共基准数据集上的实验结果表明，所提出的模型在F1分数上始终优于其他方法。

Conclusion: 所提出的集群感知因果混合模型有效地解决了现有模型在多变量时间序列异常检测中的不足，显著提高了检测性能。

Abstract: Early and accurate detection of anomalies in time series data is critical,
given the significant risks associated with false or missed detections. While
MLP-based mixer models have shown promise in time series analysis, they lack a
causality mechanism to preserve temporal dependencies inherent in the system.
Moreover, real-world multivariate time series often contain numerous channels
with diverse inter-channel correlations. A single embedding mechanism for all
channels does not effectively capture these complex relationships. To address
these challenges, we propose a novel cluster-aware causal mixer to effectively
detect anomalies in multivariate time series. Our model groups channels into
clusters based on their correlations, with each cluster processed through a
dedicated embedding layer. In addition, we introduce a causal mixer in our
model, which mixes the information while maintaining causality. Furthermore, we
present an anomaly detection framework that accumulates the anomaly evidence
over time to prevent false positives due to nominal outliers. Our proposed
model operates in an online fashion, making it suitable for real-time
time-series anomaly detection tasks. Experimental evaluations across six public
benchmark datasets demonstrate that our model consistently achieves superior F1
scores.

</details>


### [105] [MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models](https://arxiv.org/abs/2506.00198)
*Srivathsan Badrinarayanan,Rishikesh Magar,Akshay Antony,Radheesh Sharma Meda,Amir Barati Farimani*

Main category: cs.LG

TL;DR: The paper presents a reinforcement learning-enhanced, transformer-based framework for designing Metal-Organic Frameworks (MOFs) using MOFid string representation. The pipeline includes a generative GPT model, MOFormer property predictor, and an RL module, enabling scalable generative modeling of synthesizable, topologically valid MOFs with desired properties.


<details>
  <summary>Details</summary>
Motivation: Discovering MOFs with application-specific properties is challenging due to their large design space. Conventional computational techniques are accurate but computationally expensive at scale, prompting the need for more efficient methods.

Method: The method uses MOFid, a string representation encoding connectivity and topology, along with a three-component pipeline: a generative GPT model trained on MOFid sequences, MOFormer for property prediction, and a reinforcement learning module that optimizes candidates via property-guided reward functions.

Result: The approach successfully integrates property feedback into sequence generation, leading to synthesizable, topologically valid MOFs with desired functional attributes.

Conclusion: This work shows the potential of combining large language models with reinforcement learning to accelerate inverse design in reticular chemistry and advance computational MOF discovery.

Abstract: The discovery of Metal-Organic Frameworks (MOFs) with application-specific
properties remains a central challenge in materials chemistry, owing to the
immense size and complexity of their structural design space. Conventional
computational screening techniques such as molecular simulations and density
functional theory (DFT), while accurate, are computationally prohibitive at
scale. Machine learning offers an exciting alternative by leveraging
data-driven approaches to accelerate materials discovery. The complexity of
MOFs, with their extended periodic structures and diverse topologies, creates
both opportunities and challenges for generative modeling approaches. To
address these challenges, we present a reinforcement learning-enhanced,
transformer-based framework for the de novo design of MOFs. Central to our
approach is MOFid, a chemically-informed string representation encoding both
connectivity and topology, enabling scalable generative modeling. Our pipeline
comprises three components: (1) a generative GPT model trained on MOFid
sequences, (2) MOFormer, a transformer-based property predictor, and (3) a
reinforcement learning (RL) module that optimizes generated candidates via
property-guided reward functions. By integrating property feedback into
sequence generation, our method drives the model toward synthesizable,
topologically valid MOFs with desired functional attributes. This work
demonstrates the potential of large language models, when coupled with
reinforcement learning, to accelerate inverse design in reticular chemistry and
unlock new frontiers in computational MOF discovery.

</details>


### [106] [Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective](https://arxiv.org/abs/2506.00205)
*Junze Deng,Qinhang Wu,Peizhong Ju,Sen Lin,Yingbin Liang,Ness Shroff*

Main category: cs.LG

TL;DR: In this paper, researchers analyze different rehearsal strategies for addressing catastrophic forgetting in continual learning (CL). They compare concurrent rehearsal and sequential rehearsal methods through theoretical analysis and propose a Hybrid Rehearsal method that outperforms standard concurrent rehearsal.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether sequential rehearsal can offer greater benefits for CL compared to the widely used concurrent rehearsal strategy. This is inspired by human learning, where sequentially revisiting tasks helps mitigate forgetting.

Method: The study conducts a theoretical analysis of rehearsal-based CL in overparameterized linear models, comparing two strategies: Concurrent Rehearsal and Sequential Rehearsal. Based on the insights gained, they propose a novel Hybrid Rehearsal method which combines both concurrent and sequential approaches based on task similarity.

Result: Theoretical analysis shows that sequential rehearsal performs better when tasks are less similar. Experiments with deep neural networks confirm that the hybrid approach outperforms standard concurrent rehearsal.

Conclusion: This work provides the first comprehensive theoretical analysis of rehearsal-based CL, showing that sequential rehearsal can be more effective than concurrent rehearsal under certain conditions and proposing a superior hybrid approach.

Abstract: Rehearsal-based methods have shown superior performance in addressing
catastrophic forgetting in continual learning (CL) by storing and training on a
subset of past data alongside new data in current task. While such a concurrent
rehearsal strategy is widely used, it remains unclear if this approach is
always optimal. Inspired by human learning, where sequentially revisiting tasks
helps mitigate forgetting, we explore whether sequential rehearsal can offer
greater benefits for CL compared to standard concurrent rehearsal. To address
this question, we conduct a theoretical analysis of rehearsal-based CL in
overparameterized linear models, comparing two strategies: 1) Concurrent
Rehearsal, where past and new data are trained together, and 2) Sequential
Rehearsal, where new data is trained first, followed by revisiting past data
sequentially. By explicitly characterizing forgetting and generalization error,
we show that sequential rehearsal performs better when tasks are less similar.
These insights further motivate a novel Hybrid Rehearsal method, which trains
similar tasks concurrently and revisits dissimilar tasks sequentially. We
characterize its forgetting and generalization performance, and our experiments
with deep neural networks further confirm that the hybrid approach outperforms
standard concurrent rehearsal. This work provides the first comprehensive
theoretical analysis of rehearsal-based CL.

</details>


### [107] [Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models](https://arxiv.org/abs/2506.00209)
*Liwen Sun,Hao-Ren Yao,Gary Gao,Ophir Frieder,Chenyan Xiong*

Main category: cs.LG

TL;DR: CATCH-FM is a cancer pre-screening methodology that uses historical medical records to identify high-risk patients, achieving strong efficacy and low risk in retrospective evaluations.


<details>
  <summary>Details</summary>
Motivation: To create an affordable, non-intrusive cancer pre-screening method that can be globally applied to save more lives.

Method: Using millions of electronic healthcare records, the team established scaling laws for EHR foundation models, pretrained compute-optimal models with up to 2.4 billion parameters, and fine-tuned them on clinician-curated cancer risk prediction cohorts.

Result: In a retrospective evaluation of thirty thousand patients, CATCH-FM showed high sensitivity (60%) and specificity (99%), outperforming other models including feature-based tree models, general and medical large language models.

Conclusion: CATCH-FM demonstrates robustness across various patient distributions, benefits from operating in the ICD code space, and captures significant cancer risk factors; its code will be open-sourced.

Abstract: Cancer screening, leading to early detection, saves lives. Unfortunately,
existing screening techniques require expensive and intrusive medical
procedures, not globally available, resulting in too many lost would-be-saved
lives. We present CATCH-FM, CATch Cancer early with Healthcare Foundation
Models, a cancer pre-screening methodology that identifies high-risk patients
for further screening solely based on their historical medical records. With
millions of electronic healthcare records (EHR), we establish the scaling law
of EHR foundation models pretrained on medical code sequences, pretrain
compute-optimal foundation models of up to 2.4 billion parameters, and finetune
them on clinician-curated cancer risk prediction cohorts. In our retrospective
evaluation comprising of thirty thousand patients, CATCH-FM achieved strong
efficacy (60% sensitivity) with low risk (99% specificity and Negative
Predictive Value), outperforming feature-based tree models as well as general
and medical large language models by large margins. Despite significant
demographic, healthcare system, and EHR coding differences, CATCH-FM achieves
state-of-the-art pancreatic cancer risk prediction on the EHRSHOT few-shot
leaderboard, outperforming EHR foundation models pretrained using on-site
patient data. Our analysis demonstrates the robustness of CATCH-FM in various
patient distributions, the benefits of operating in the ICD code space, and its
ability to capture non-trivial cancer risk factors. Our code will be
open-sourced.

</details>


### [108] [Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning](https://arxiv.org/abs/2506.00236)
*Babak Barazandeh*

Main category: cs.LG

TL;DR: Localized LoRA is proposed to model weight updates with localized low-rank matrices, offering a more expressive and adaptable alternative for efficient fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA mainly focus on global low-rank structures, which may ignore spatial patterns across the parameter space.

Method: The paper introduces Localized LoRA that models weight updates as low-rank matrices applied to structured blocks of the weight matrix. This enables dense, localized updates without increasing trainable parameters.

Result: Experiments indicate that Localized LoRA achieves lower approximation error compared to global and diagonal-local methods under the same parameter budget, enhancing performance in both synthetic and practical settings.

Conclusion: Localized LoRA provides an improved, adaptable framework for parameter-efficient fine-tuning.

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact
and effective alternatives to full model fine-tuning by introducing low-rank
updates to pretrained weights. However, most existing approaches rely on global
low-rank structures, which can overlook spatial patterns spread across the
parameter space. In this work, we propose Localized LoRA, a generalized
framework that models weight updates as a composition of low-rank matrices
applied to structured blocks of the weight matrix. This formulation enables
dense, localized updates throughout the parameter space-without increasing the
total number of trainable parameters. We provide a formal comparison between
global, diagonal-local, and fully localized low-rank approximations, and show
that our method consistently achieves lower approximation error under matched
parameter budgets. Experiments on both synthetic and practical settings
demonstrate that Localized LoRA offers a more expressive and adaptable
alternative to existing methods, enabling efficient fine-tuning with improved
performance.

</details>


### [109] [DeGLIF for Label Noise Robust Node Classification using GNNs](https://arxiv.org/abs/2506.00244)
*Pintu Kumar,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: DeGLIF是一种用于图数据的去噪技术，通过使用少量干净数据和留一影响函数来增强对标签噪声的鲁棒性。它扩展了最近关于图神经网络的工作，提出了两种变体以识别噪声节点，并证明了其在不同数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 带噪声标签的数据集相对于干净标签的数据集通常成本较低，尤其是在图数据中。为了提高图数据上节点级预测的准确性，需要一种能够处理标签噪声的技术。

Method: 提出了一种名为DeGLIF的去噪技术，利用少量干净数据和留一影响函数来估计移除训练节点时验证损失的变化。通过这种估计和一个新的重新标记函数来去噪训练数据集。此外，还提出了两种DeGLIF变体以识别噪声节点，无需任何关于噪声模型或噪声水平的信息。

Result: 在不同的数据集上进行详细的计算实验，表明DeGLIF比其他基线算法具有更高的准确性。并且对于其中一个变体，证明检测到的噪声点确实可以增加风险。

Conclusion: DeGLIF通过使用留一影响函数和重新标记函数有效提高了图数据中节点级预测的准确性，并且不需要了解噪声模型或噪声水平的具体信息。

Abstract: Noisy labelled datasets are generally inexpensive compared to clean labelled
datasets, and the same is true for graph data. In this paper, we propose a
denoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence
Function. DeGLIF uses a small set of clean data and the leave-one-out influence
function to make label noise robust node-level prediction on graph data.
Leave-one-out influence function approximates the change in the model
parameters if a training point is removed from the training dataset. Recent
advances propose a way to calculate the leave-one-out influence function for
Graph Neural Networks (GNNs). We extend that recent work to estimate the change
in validation loss, if a training node is removed from the training dataset. We
use this estimate and a new theoretically motivated relabelling function to
denoise the training dataset. We propose two DeGLIF variants to identify noisy
nodes. Both these variants do not require any information about the noise model
or the noise level in the dataset; DeGLIF also does not estimate these
quantities. For one of these variants, we prove that the noisy points detected
can indeed increase risk. We carry out detailed computational experiments on
different datasets to show the effectiveness of DeGLIF. It achieves better
accuracy than other baseline algorithms

</details>


### [110] [Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity](https://arxiv.org/abs/2506.00245)
*Dang Nguyen,Ali Payani,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: The paper proposes an improved uncertainty quantification method for detecting hallucinations in large language models, generalizing semantic entropy and demonstrating effectiveness through empirical results.


<details>
  <summary>Details</summary>
Motivation: Hallucination in large language models needs to be detected effectively. Traditional methods like semantic entropy have limitations when dealing with longer one-sentence responses as they overlook intra-cluster similarity and inter-cluster similarity.

Method: A black-box uncertainty quantification method inspired by nearest neighbor estimates of entropy is proposed. It can be extended to white-box settings by incorporating token probabilities and generalizes semantic entropy theoretically.

Result: Empirical results show the proposed method outperforms semantic entropy in detecting hallucinations across two recent LLMs (Phi3 and Llama3) and three text generation tasks: question answering, text summarization, and machine translation.

Conclusion: The new uncertainty quantification method addresses the limitations of semantic entropy and provides a more effective way to detect hallucinations in large language models.

Abstract: Hallucination in large language models (LLMs) can be detected by assessing
the uncertainty of model outputs, typically measured using entropy. Semantic
entropy (SE) enhances traditional entropy estimation by quantifying uncertainty
at the semantic cluster level. However, as modern LLMs generate longer
one-sentence responses, SE becomes less effective because it overlooks two
crucial factors: intra-cluster similarity (the spread within a cluster) and
inter-cluster similarity (the distance between clusters). To address these
limitations, we propose a simple black-box uncertainty quantification method
inspired by nearest neighbor estimates of entropy. Our approach can also be
easily extended to white-box settings by incorporating token probabilities.
Additionally, we provide theoretical results showing that our method
generalizes semantic entropy. Extensive empirical results demonstrate its
effectiveness compared to semantic entropy across two recent LLMs (Phi3 and
Llama3) and three common text generation tasks: question answering, text
summarization, and machine translation. Our code is available at
https://github.com/BigML-CS-UCLA/SNNE.

</details>


### [111] [Performance Analysis of Convolutional Neural Network By Applying Unconstrained Binary Quadratic Programming](https://arxiv.org/abs/2506.00247)
*Aasish Kumar Sharma,Sanjeeb Prashad Pandey,Julian M. Kunkel*

Main category: cs.LG

TL;DR: This paper proposes a hybrid optimization method that combines an Unconstrained Binary Quadratic Programming (UBQP) formulation with Stochastic Gradient Descent (SGD) to accelerate Convolutional Neural Networks (CNNs) training on the MNIST dataset, resulting in a 10--15\% accuracy improvement over a standard BP-CNN baseline.


<details>
  <summary>Details</summary>
Motivation: Convolutional Neural Networks (CNNs) require significant computational resources when trained on large-scale datasets using conventional back-propagation methods. The authors aim to explore quantum computing techniques for more efficient CNN training.

Method: The proposed method is a hybrid optimization approach combining Unconstrained Binary Quadratic Programming (UBQP) formulation with Stochastic Gradient Descent (SGD). This method is used to accelerate the training of CNNs.

Result: Evaluated on the MNIST dataset, the hybrid optimization method achieves a 10--15\% accuracy improvement over a standard BP-CNN baseline while maintaining similar execution times.

Conclusion: The results demonstrate the potential of hybrid quantum-classical techniques in High-Performance Computing (HPC) environments for Big Data and Deep Learning. However, fully realizing these benefits requires careful alignment of algorithmic structures with underlying quantum mechanisms.

Abstract: Convolutional Neural Networks (CNNs) are pivotal in computer vision and Big
Data analytics but demand significant computational resources when trained on
large-scale datasets. Conventional training via back-propagation (BP) with
losses like Mean Squared Error or Cross-Entropy often requires extensive
iterations and may converge sub-optimally. Quantum computing offers a promising
alternative by leveraging superposition, tunneling, and entanglement to search
complex optimization landscapes more efficiently. In this work, we propose a
hybrid optimization method that combines an Unconstrained Binary Quadratic
Programming (UBQP) formulation with Stochastic Gradient Descent (SGD) to
accelerate CNN training. Evaluated on the MNIST dataset, our approach achieves
a 10--15\% accuracy improvement over a standard BP-CNN baseline while
maintaining similar execution times. These results illustrate the potential of
hybrid quantum-classical techniques in High-Performance Computing (HPC)
environments for Big Data and Deep Learning. Fully realizing these benefits,
however, requires a careful alignment of algorithmic structures with underlying
quantum mechanisms.

</details>


### [112] [PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction](https://arxiv.org/abs/2506.00259)
*Zhengyang Fan,Wanru Li,Kuo-chu Chang,Ting Yuan*

Main category: cs.LG

TL;DR: The paper introduces PerFormer, a permutation-based vision transformer approach for RUL prediction that outperforms state-of-the-art methods on NASA's C-MAPSS dataset.


<details>
  <summary>Details</summary>
Motivation: Accurately estimating the remaining useful life (RUL) is crucial in modern prognostic and health management. Although CNNs have proven effective in RUL prediction, the emergence of Vision Transformer (ViT) with demonstrated superiority over CNNs in other tasks suggests its potential in enhancing RUL prediction accuracy.

Method: The PerFormer permutes multivariate time series data to mimic spatial characteristics akin to image data, making it suitable for ViT. A novel permutation loss function is introduced to guide the convergence of any matrix towards a permutation matrix.

Result: Experiments on NASA's C-MAPSS dataset show that PerFormer has superior performance in RUL prediction compared to state-of-the-art methods using CNNs, RNNs, and various Transformer models.

Conclusion: PerFormer demonstrates effectiveness and potential in PHM applications.

Abstract: Accurately estimating the remaining useful life (RUL) for degradation systems
is crucial in modern prognostic and health management (PHM). Convolutional
Neural Networks (CNNs), initially developed for tasks like image and video
recognition, have proven highly effectively in RUL prediction, demonstrating
remarkable performance. However, with the emergence of the Vision Transformer
(ViT), a Transformer model tailored for computer vision tasks such as image
classification, and its demonstrated superiority over CNNs, there is a natural
inclination to explore its potential in enhancing RUL prediction accuracy.
Nonetheless, applying ViT directly to multivariate sensor data for RUL
prediction poses challenges, primarily due to the ambiguous nature of spatial
information in time series data. To address this issue, we introduce the
PerFormer, a permutation-based vision transformer approach designed to permute
multivariate time series data, mimicking spatial characteristics akin to image
data, thereby making it suitable for ViT. To generate the desired permutation
matrix, we introduce a novel permutation loss function aimed at guiding the
convergence of any matrix towards a permutation matrix. Our experiments on
NASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL
prediction compared to state-of-the-art methods employing CNNs, Recurrent
Neural Networks (RNNs), and various Transformer models. This underscores its
effectiveness and potential in PHM applications.

</details>


### [113] [Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model](https://arxiv.org/abs/2506.00286)
*Oliver Mortensen,Mohammad Sadegh Talebi*

Main category: cs.LG

TL;DR: 这篇论文分析了在折扣马尔可夫决策过程中学习最优状态-动作值函数$Q^*$和最优策略$\pi^*$的样本复杂性，其中代理具有递归熵风险偏好，并且提供了MDP的生成模型。


<details>
  <summary>Details</summary>
Motivation: 研究具有递归熵风险偏好的折扣马尔可夫决策过程中的样本复杂性问题，以理解不同风险敏感性下的学习效率。

Method: 提出了一种称为基于模型的风险敏感Q值迭代（MB-RS-QVI）的简单方法，并对其进行了分析。该方法提供了关于$Q^*-Q^k$和$V^*-V^{\pi_k}$的$(\epsilon,\delta)$-PAC界。

Result: 证明了PAC界的指数依赖于有效视界$\frac{1}{1-\gamma}$以及学习者风险敏感性$|\beta|$。此外，提供了两个下界，表明这种指数依赖不可避免，并验证了PAC界在$\varepsilon$和$\delta$上的紧致性。

Conclusion: 基于模型的风险敏感Q值迭代方法在学习最优Q值函数和策略时表现出与风险敏感性和折扣因子相关的指数复杂性，而这种复杂性是不可避免的。

Abstract: In this paper we analyze the sample complexities of learning the optimal
state-action value function $Q^*$ and an optimal policy $\pi^*$ in a discounted
Markov decision process (MDP) where the agent has recursive entropic
risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model
of the MDP is available. We provide and analyze a simple model based approach
which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which
leads to $(\epsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and
$\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations
and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have
exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the
strength of this dependence grows with the learners risk-sensitivity $|\beta|$.
We also provide two lower bounds which shows that exponential dependence on
$|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds
reveal that the PAC-bounds are both tight in $\varepsilon$ and $\delta$ and
that the PAC-bound on $Q$-learning is tight in the number of actions $A$, and
that the PAC-bound on policy-learning is nearly tight in $A$.

</details>


### [114] [Improving Protein Sequence Design through Designability Preference Optimization](https://arxiv.org/abs/2506.00297)
*Fanglei Xue,Andrew Kubaney,Zhichun Guo,Joseph K. Min,Ge Liu,Yi Yang,David Baker*

Main category: cs.LG

TL;DR: 通过重新定义训练目标，利用Direct Preference Optimization (DPO)和AlphaFold pLDDT分数作为偏好信号，并引入Residue-level Designability Preference Optimization (ResiDPO)，该研究显著提高了蛋白质序列设计的成功率。最终，EnhancedMPNN在酶设计基准测试中将成功率从6.56%提升到17.57%。


<details>
  <summary>Details</summary>
Motivation: 现有的蛋白质序列设计方法虽然在生成新蛋白质序列方面表现出色，但由于其训练目标为序列恢复，无法保证设计的序列能够折叠成所需的结构（即设计性）。因此，需要一种新的方法来提高设计性。

Method: 研究采用了Direct Preference Optimization (DPO)并使用AlphaFold pLDDT分数作为偏好信号，引导序列生成更倾向于高设计性。此外，还引入了Residue-level Designability Preference Optimization (ResiDPO)，它通过应用残基级别的结构奖励并在残基间解耦优化，进一步改进了序列生成的精细度。最后，基于带残基级别注释的数据集，用ResiDPO微调LigandMPNN得到EnhancedMPNN。

Result: EnhancedMPNN在困难的酶设计基准测试中，将体外设计成功率从6.56%提高到了17.57%，相当于提升了近3倍。

Conclusion: 通过整合DPO和ResiDPO，可以有效提升蛋白质序列的设计性，尤其是对于复杂结构如酶类的设计成功率有显著改善。这表明重新定义训练目标以及引入精细化优化策略是提高蛋白质设计性能的有效途径。

Abstract: Protein sequence design methods have demonstrated strong performance in
sequence generation for de novo protein design. However, as the training
objective was sequence recovery, it does not guarantee designability--the
likelihood that a designed sequence folds into the desired structure. To bridge
this gap, we redefine the training objective by steering sequence generation
toward high designability. To do this, we integrate Direct Preference
Optimization (DPO), using AlphaFold pLDDT scores as the preference signal,
which significantly improves the in silico design success rate. To further
refine sequence generation at a finer, residue-level granularity, we introduce
Residue-level Designability Preference Optimization (ResiDPO), which applies
residue-level structural rewards and decouples optimization across residues.
This enables direct improvement in designability while preserving regions that
already perform well. Using a curated dataset with residue-level annotations,
we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a
nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%)
on a challenging enzyme design benchmark.

</details>


### [115] [Inference-Time Alignment of Diffusion Models with Evolutionary Algorithms](https://arxiv.org/abs/2506.00299)
*Purvish Jajal,Nick John Eliopoulos,Benjamin Shiue-Hal Chou,George K. Thiruvathukal,James C. Davis,Yung-Hsiang Lu*

Main category: cs.LG

TL;DR: Diffusion models are leading generative models but often don't meet downstream objectives. Current alignment techniques have limitations. This paper introduces an evolutionary algorithm-based framework for efficient inference-time alignment, treating diffusion models as black-boxes and searching their latent space to maximize alignment objectives. It outperforms gradient-based methods in benchmarks, with lower GPU memory and faster running-time.


<details>
  <summary>Details</summary>
Motivation: To address the issue where samples from diffusion models fail to satisfy downstream objectives such as safety constraints or domain-specific validity, and to overcome the limitations of existing alignment techniques that require gradients, internal model access, or large computational budgets.

Method: An inference-time alignment framework based on evolutionary algorithms is introduced. Diffusion models are treated as black-boxes and their latent space is searched to maximize alignment objectives. This method enables efficient inference-time alignment for both differentiable and non-differentiable alignment objectives across a range of diffusion models.

Result: The EA methods outperform state-of-the-art gradient-based and gradient-free inference-time methods on the DrawBench and Open Image Preferences benchmark. They require significantly lower GPU memory (55% to 76%) and are faster (72% to 80%) than gradient-based methods. Higher alignment scores are achieved over 50 optimization steps on Open Image Preferences.

Conclusion: The evolutionary algorithm-based inference-time alignment framework provides an effective solution for aligning diffusion model samples with downstream objectives, surpassing current methods in performance and efficiency.

Abstract: Diffusion models are state-of-the-art generative models in various domains,
yet their samples often fail to satisfy downstream objectives such as safety
constraints or domain-specific validity. Existing techniques for alignment
require gradients, internal model access, or large computational budgets. We
introduce an inference-time alignment framework based on evolutionary
algorithms. We treat diffusion models as black-boxes and search their latent
space to maximize alignment objectives. Our method enables efficient
inference-time alignment for both differentiable and non-differentiable
alignment objectives across a range of diffusion models. On the DrawBench and
Open Image Preferences benchmark, our EA methods outperform state-of-the-art
gradient-based and gradient-free inference-time methods. In terms of memory
consumption, we require 55% to 76% lower GPU memory than gradient-based
methods. In terms of running-time, we are 72% to 80% faster than gradient-based
methods. We achieve higher alignment scores over 50 optimization steps on Open
Image Preferences than gradient-based and gradient-free methods.

</details>


### [116] [Beyond Atomic Geometry Representations in Materials Science: A Human-in-the-Loop Multimodal Framework](https://arxiv.org/abs/2506.00302)
*Can Polat,Hasan Kurban,Erchin Serpedin,Mustafa Kurban*

Main category: cs.LG

TL;DR: Most materials science datasets are limited to atomic geometries, which restricts their utility for multimodal learning. This work introduces MCS-Set, a curated framework that expands materials datasets by integrating atomic structures with 2D projections and structured textual annotations. It enables multimodal property prediction and constrained crystal generation. Evaluations reveal modality-specific performance gaps and highlight the importance of annotation quality.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing materials science datasets that are restricted to atomic geometries and thus hinder the application of advanced machine learning techniques.

Method: Introduction of MCS-Set, a curated framework expanding materials datasets by integrating atomic structures with 2D projections and structured textual annotations. It uses a human-in-the-loop pipeline combining domain expertise with standardized descriptors for high-quality annotation.

Result: Evaluations using state-of-the-art models reveal substantial modality-specific performance gaps and emphasize the significance of annotation quality for model generalization.

Conclusion: MCS-Set provides a foundation for benchmarking multimodal models, advancing annotation practices, and promoting accessible and versatile materials science datasets.

Abstract: Most materials science datasets are limited to atomic geometries (e.g., XYZ
files), restricting their utility for multimodal learning and comprehensive
data-centric analysis. These constraints have historically impeded the adoption
of advanced machine learning techniques in the field. This work introduces
MultiCrystalSpectrumSet (MCS-Set), a curated framework that expands materials
datasets by integrating atomic structures with 2D projections and structured
textual annotations, including lattice parameters and coordination metrics.
MCS-Set enables two key tasks: (1) multimodal property and summary prediction,
and (2) constrained crystal generation with partial cluster supervision.
Leveraging a human-in-the-loop pipeline, MCS-Set combines domain expertise with
standardized descriptors for high-quality annotation. Evaluations using
state-of-the-art language and vision-language models reveal substantial
modality-specific performance gaps and highlight the importance of annotation
quality for generalization. MCS-Set offers a foundation for benchmarking
multimodal models, advancing annotation practices, and promoting accessible,
versatile materials science datasets. The dataset and implementations are
available at https://github.com/KurbanIntelligenceLab/MultiCrystalSpectrumSet.

</details>


### [117] [Active Learning via Regression Beyond Realizability](https://arxiv.org/abs/2506.00316)
*Atul Ganju,Shashaank Aiyer,Ved Sriraman,Karthik Sridharan*

Main category: cs.LG

TL;DR: 本论文提出了一种新的多类分类主动学习框架，基于代理风险最小化，能够在标准可实现性假设之外运行。即使在非可实现设置下，只要模型类是凸的，就可以获得与先前工作相当的标签和样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于代理的主动学习算法严重依赖于可实现性假设，限制了其在实际、错配环境中的适用性。

Method: 提出了一个基于轮次的主动学习算法，在每个轮次中从完整类中拟合模型到查询数据，并返回通过聚合这些模型得到的非正式分类器。

Result: 在比可实现性弱得多的条件下，只要考虑的模型类是凸的，就能获得与之前工作相当的标签和样本复杂度。以前方法在非可实现设置下会失败，而本文方法在这种情况下仍然有效。

Conclusion: 所提出的算法在非可实现环境下仍然有效，扩展了主动学习算法的应用范围。

Abstract: We present a new active learning framework for multiclass classification
based on surrogate risk minimization that operates beyond the standard
realizability assumption. Existing surrogate-based active learning algorithms
crucially rely on realizability$\unicode{x2014}$the assumption that the optimal
surrogate predictor lies within the model class$\unicode{x2014}$limiting their
applicability in practical, misspecified settings. In this work we show that
under conditions significantly weaker than realizability, as long as the class
of models considered is convex, one can still obtain a label and sample
complexity comparable to prior work. Despite achieving similar rates, the
algorithmic approaches from prior works can be shown to fail in non-realizable
settings where our assumption is satisfied. Our epoch-based active learning
algorithm departs from prior methods by fitting a model from the full class to
the queried data in each epoch and returning an improper classifier obtained by
aggregating these models.

</details>


### [118] [Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation](https://arxiv.org/abs/2506.00329)
*Muhammad Adnan,Nithesh Kurella,Akhil Arunkumar,Prashant J. Nair*

Main category: cs.LG

TL;DR: Foresight is an adaptive layer-reuse technique that improves the computational efficiency of Diffusion Transformers (DiTs) in text-to-video generation without sacrificing video quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reducing computational costs in DiTs, such as static caching, fail to adapt to the dynamics of generation processes, resulting in suboptimal trade-offs between speed and quality.

Method: Foresight dynamically identifies and reuses DiT block outputs across all layers and denoising steps, adapting to generation parameters like resolution and denoising schedules to optimize efficiency.

Result: Foresight achieves up to 1.63x end-to-end speedup when applied to OpenSora, Latte, and CogVideoX while maintaining video quality.

Conclusion: Foresight effectively reduces computational redundancy in DiTs for video generation, enhancing efficiency without compromising on video quality.

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art results in
text-to-image, text-to-video generation, and editing. However, their large
model size and the quadratic cost of spatial-temporal attention over multiple
denoising steps make video generation computationally expensive. Static caching
mitigates this by reusing features across fixed steps but fails to adapt to
generation dynamics, leading to suboptimal trade-offs between speed and
quality.
  We propose Foresight, an adaptive layer-reuse technique that reduces
computational redundancy across denoising steps while preserving baseline
performance. Foresight dynamically identifies and reuses DiT block outputs for
all layers across steps, adapting to generation parameters such as resolution
and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and
CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining
video quality. The source code of Foresight is available at
\texttt{https://github.com/STAR-Laboratory/foresight}.

</details>


### [119] [Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification](https://arxiv.org/abs/2506.00337)
*Ming Hu,Jianfu Yin,Mingyu Dou,Yuqi Wang,Ruochen Dang,Siyi Liang,Cong Hu,Yao Wang,Bingliang Hu,Quan Wang*

Main category: cs.LG

TL;DR: The paper proposes Channel Imposed Fusion (CIF) combined with Temporal Convolutional Network (TCN) for medical time series classification, enhancing performance and transparency.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models have notable performance but lack structural transparency and trustworthiness in clinical settings.

Method: Propose CIF method integrated with TCN to improve signal-to-noise ratio, reduce redundancy, and enhance classification performance while maintaining transparency.

Result: Experiments on EEG and ECG datasets show superior performance over SOTA methods and improved transparency of the classification process.

Conclusion: The proposed CIF+TCN framework offers a novel perspective for medical time series classification with better performance and transparency.

Abstract: The automatic classification of medical time series signals, such as
electroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in
clinical decision support and early detection of diseases. Although Transformer
based models have achieved notable performance by implicitly modeling temporal
dependencies through self-attention mechanisms, their inherently complex
architectures and opaque reasoning processes undermine their trustworthiness in
high stakes clinical settings. In response to these limitations, this study
shifts focus toward a modeling paradigm that emphasizes structural
transparency, aligning more closely with the intrinsic characteristics of
medical data. We propose a novel method, Channel Imposed Fusion (CIF), which
enhances the signal-to-noise ratio through cross-channel information fusion,
effectively reduces redundancy, and improves classification performance.
Furthermore, we integrate CIF with the Temporal Convolutional Network (TCN),
known for its structural simplicity and controllable receptive field, to
construct an efficient and explicit classification framework. Experimental
results on multiple publicly available EEG and ECG datasets demonstrate that
the proposed method not only outperforms existing state-of-the-art (SOTA)
approaches in terms of various classification metrics, but also significantly
enhances the transparency of the classification process, offering a novel
perspective for medical time series classification.

</details>


### [120] [Exploring the Performance of Perforated Backpropagation through Further Experiments](https://arxiv.org/abs/2506.00356)
*Rorry Brenner,Evan Davis,Rushi Chaudhari,Rowan Morse,Jingyao Chen,Xirui Liu,Zhaoyi You,Laurent Itti*

Main category: cs.LG

TL;DR: Perforated Backpropagation enhances neural network models with significant model compression or increased accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore further experiments of Perforated Backpropagation algorithm and its potential to enhance projects.

Method: Applying Perforated Backpropagation on various datasets and models.

Result: Achieved up to 90% model compression without loss in accuracy, or up to 16% increased accuracy.

Conclusion: Perforated Backpropagation is effective in enhancing neural network models.

Abstract: Perforated Backpropagation is a neural network optimization technique based
on modern understanding of the computational importance of dendrites within
biological neurons. This paper explores further experiments from the original
publication, generated from a hackathon held at the Carnegie Mellon Swartz
Center in February 2025. Students and local Pittsburgh ML practitioners were
brought together to experiment with the Perforated Backpropagation algorithm on
the datasets and models which they were using for their projects. Results
showed that the system could enhance their projects, with up to 90% model
compression without negative impact on accuracy, or up to 16% increased
accuracy of their original models.

</details>


### [121] [FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees](https://arxiv.org/abs/2506.00362)
*Hoang T. Nguyen,Priya L. Donti*

Main category: cs.LG

TL;DR: The paper introduces FSNet, a neural network that integrates a feasibility-seeking step to solve constrained optimization problems faster and with guaranteed feasibility. It performs well across various optimization problems.


<details>
  <summary>Details</summary>
Motivation: Traditional solvers for constrained optimization problems are computationally expensive for real-time use, while machine learning-based approaches struggle to enforce constraints strictly.

Method: Propose FSNet which incorporates a feasibility-seeking step into its solution process, solving an unconstrained optimization problem to minimize constraint violations in a differentiable way.

Result: FSNet provides feasible solutions with quality comparable or better than traditional solvers but at much faster speeds, demonstrated across smooth/nonsmooth and convex/nonconvex problems.

Conclusion: FSNet is effective in providing fast, feasible solutions for constrained optimization problems.

Abstract: Efficiently solving constrained optimization problems is crucial for numerous
real-world applications, yet traditional solvers are often computationally
prohibitive for real-time use. Machine learning-based approaches have emerged
as a promising alternative to provide approximate solutions at faster speeds,
but they struggle to strictly enforce constraints, leading to infeasible
solutions in practice. To address this, we propose the
Feasibility-Seeking-Integrated Neural Network (FSNet), which integrates a
feasibility-seeking step directly into its solution procedure to ensure
constraint satisfaction. This feasibility-seeking step solves an unconstrained
optimization problem that minimizes constraint violations in a differentiable
manner, enabling end-to-end training and providing guarantees on feasibility
and convergence. Our experiments across a range of different optimization
problems, including both smooth/nonsmooth and convex/nonconvex problems,
demonstrate that FSNet can provide feasible solutions with solution quality
comparable to (or in some cases better than) traditional solvers, at
significantly faster speeds.

</details>


### [122] [Spectral Insights into Data-Oblivious Critical Layers in Large Language Models](https://arxiv.org/abs/2506.00382)
*Xuyuan Liu,Lei Hsiung,Yaoqing Yang,Yujun Yan*

Main category: cs.LG

TL;DR: This paper explores how feature representations change across layers in large language models (LLMs), introducing a data-oblivious method using Centered Kernel Alignment (CKA) to identify critical layers before fine-tuning. It finds these layers experience significant shifts linked to semantic transitions, which can be used for efficient domain adaptation and backdoor defense.


<details>
  <summary>Details</summary>
Motivation: To improve the interpretability and robustness of LLMs by understanding how their feature representations evolve across layers, moving beyond data-dependent analyses of fine-tuned models to a more intrinsic, pre-fine-tuning approach.

Method: Using Centered Kernel Alignment (CKA) to analyze representation dynamics in pre-fine-tuned LLMs, identifying critical layers with significant shifts in representation space. Spectral analysis is applied to reveal that these shifts are driven by changes in top principal components encoding semantic transitions.

Result: Critical layers identified via this method show greater shifts compared to non-critical layers and are significantly affected during fine-tuning. Applying findings to domain adaptation leads to greater loss reduction when fine-tuning critical layers, and freezing them reduces backdoor attack success rates by up to 40%. 

Conclusion: The data-oblivious approach using CKA successfully identifies intrinsic critical layers in LLMs, providing insights into model behavior and practical applications for improving efficiency and security.

Abstract: Understanding how feature representations evolve across layers in large
language models (LLMs) is key to improving their interpretability and
robustness. While recent studies have identified critical layers linked to
specific functions or behaviors, these efforts typically rely on data-dependent
analyses of fine-tuned models, limiting their use to post-hoc settings. In
contrast, we introduce a data-oblivious approach to identify intrinsic critical
layers in pre-fine-tuned LLMs by analyzing representation dynamics via Centered
Kernel Alignment(CKA). We show that layers with significant shifts in
representation space are also those most affected during fine-tuning--a pattern
that holds consistently across tasks for a given model. Our spectral analysis
further reveals that these shifts are driven by changes in the top principal
components, which encode semantic transitions from rationales to conclusions.
We further apply these findings to two practical scenarios: efficient domain
adaptation, where fine-tuning critical layers leads to greater loss reduction
compared to non-critical layers; and backdoor defense, where freezing them
reduces attack success rates by up to 40%.

</details>


### [123] [Deep-Learning-Driven Prefetching for Far Memory](https://arxiv.org/abs/2506.00384)
*Yutong Huang,Zhiyuan Guo,Yiying Zhang*

Main category: cs.LG

TL;DR: Modern software systems, especially those using far memory, need better runtime performance. Machine learning has been useful for offline optimization but not much for high-frequency runtime issues due to various constraints. FarSight is a new system that uses deep learning for efficient data prefetching in far-memory. It separates application semantics from memory layout and uses offline-trained models for predicting access patterns with low overhead. Evaluation shows FarSight outperforms current systems by up to 3.6 times, proving the potential of ML in runtime problems.


<details>
  <summary>Details</summary>
Motivation: To address the runtime performance challenges in modern software systems, particularly in far memory architectures where local-memory misses cause significant latency. Existing machine learning applications are limited in handling high-frequency, runtime-level problems due to performance, generalization, and integration constraints.

Method: FarSight leverages deep learning by separating application semantics from runtime memory layout. It uses offline-trained DL models to predict access patterns via a compact vocabulary of ordinal possibilities, resolved at runtime through lightweight mapping structures. The system combines asynchronous inference, lookahead prediction, and a cache-resident DL model to ensure accuracy and low overhead.

Result: FarSight was evaluated on four data-intensive workloads and showed an improvement of up to 3.6 times over the state-of-the-art far-memory system.

Conclusion: This work demonstrates the feasibility and advantages of applying modern machine learning techniques to complex, performance-critical software runtime problems, highlighting the potential of ML in this area.

Abstract: Modern software systems face increasing runtime performance demands,
particularly in emerging architectures like far memory, where local-memory
misses incur significant latency. While machine learning (ML) has proven
effective in offline systems optimization, its application to high-frequency,
runtime-level problems remains limited due to strict performance,
generalization, and integration constraints. We present FarSight, a Linux-based
far-memory system that leverages deep learning (DL) to efficiently perform
accurate data prefetching. FarSight separates application semantics from
runtime memory layout, allowing offline-trained DL models to predict access
patterns using a compact vocabulary of ordinal possibilities, resolved at
runtime through lightweight mapping structures. By combining asynchronous
inference, lookahead prediction, and a cache-resident DL model, FarSight
achieves high prediction accuracy with low runtime overhead. Our evaluation of
FarSight on four data-intensive workloads shows that it outperforms the
state-of-the-art far-memory system by up to 3.6 times. Overall, this work
demonstrates the feasibility and advantages of applying modern ML techniques to
complex, performance-critical software runtime problems.

</details>


### [124] [CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries](https://arxiv.org/abs/2506.00388)
*Ni Mu,Hao Hu,Xiao Hu,Yiqin Yang,Bo Xu,Qing-Shan Jia*

Main category: cs.LG

TL;DR: The paper introduces CLARIFY, an offline PbRL method that uses contrastive learning to create a trajectory embedding space for resolving ambiguous feedback, improving label efficiency and applicability of PbRL.


<details>
  <summary>Details</summary>
Motivation: PbRL allows better alignment with human intentions by inferring reward functions from preference comparisons instead of explicit reward engineering. However, the challenge lies in humans' difficulty to label clear preferences between similar segments, which reduces label efficiency and limits real-world applications.

Method: Propose CLARIFY, an offline PbRL method based on contrastive learning, which creates a trajectory embedding space incorporating preference information, ensuring that clearly distinguished segments are spaced apart to enable selection of more unambiguous queries.

Result: CLARIFY outperforms baselines in both non-ideal teacher and real human feedback settings through extensive experiments. It selects more distinguished queries and learns meaningful trajectory embeddings.

Conclusion: CLARIFY improves label efficiency and resolves ambiguous feedback in PbRL, enhancing its real-world applicability.

Abstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward
engineering by inferring reward functions from human preference comparisons,
enabling better alignment with human intentions. However, humans often struggle
to label a clear preference between similar segments, reducing label efficiency
and limiting PbRL's real-world applicability. To address this, we propose an
offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback
(CLARIFY), which learns a trajectory embedding space that incorporates
preference information, ensuring clearly distinguished segments are spaced
apart, thus facilitating the selection of more unambiguous queries. Extensive
experiments demonstrate that CLARIFY outperforms baselines in both non-ideal
teachers and real human feedback settings. Our approach not only selects more
distinguished queries but also learns meaningful trajectory embeddings.

</details>


### [125] [Bias as a Virtue: Rethinking Generalization under Distribution Shifts](https://arxiv.org/abs/2506.00407)
*Ruixuan Chen,Wentao Li,Jiahui Xiao,Yuchen Li,Yimin Tang,Xiaonan Wang*

Main category: cs.LG

TL;DR: Machine learning models can generalize better out-of-distribution when they have higher in-distribution bias. The Adaptive Distribution Bridge framework implements this insight to improve OOD generalization.


<details>
  <summary>Details</summary>
Motivation: Current machine learning models degrade when deployed on data distributions different from their training data. There is a need for a new approach that challenges conventional validation paradigms and improves out-of-distribution generalization.

Method: The Adaptive Distribution Bridge (ADB) framework introduces controlled statistical diversity during training, allowing models to develop bias profiles that effectively generalize across distributions.

Result: Empirical evaluations show robust negative correlation where higher in-distribution bias corresponds to lower out-of-distribution error. ADB achieves up to 26.8% mean error reduction compared to traditional cross-validation and consistently identifies high-performing training strategies with percentile ranks exceeding 74.4%.

Conclusion: The Adaptive Distribution Bridge provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning.

Abstract: Machine learning models often degrade when deployed on data distributions
different from their training data. Challenging conventional validation
paradigms, we demonstrate that higher in-distribution (ID) bias can lead to
better out-of-distribution (OOD) generalization. Our Adaptive Distribution
Bridge (ADB) framework implements this insight by introducing controlled
statistical diversity during training, enabling models to develop bias profiles
that effectively generalize across distributions. Empirically, we observe a
robust negative correlation where higher ID bias corresponds to lower OOD
error--a finding that contradicts standard practices focused on minimizing
validation error. Evaluation on multiple datasets shows our approach
significantly improves OOD generalization. ADB achieves robust mean error
reductions of up to 26.8% compared to traditional cross-validation, and
consistently identifies high-performing training strategies, evidenced by
percentile ranks often exceeding 74.4%. Our work provides both a practical
method for improving generalization and a theoretical framework for
reconsidering the role of bias in robust machine learning.

</details>


### [126] [JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence Clustering](https://arxiv.org/abs/2506.00410)
*Ziwen Wang*

Main category: cs.LG

TL;DR: JojoSCL is a new self-supervised contrastive learning framework for scRNA-seq clustering which refines both instance-level and cluster-level contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing clustering models face challenges due to the high dimensionality and sparsity of scRNA-seq data.

Method: Incorporates a shrinkage estimator based on hierarchical Bayesian estimation and optimized using Stein's Unbiased Risk Estimate (SURE) to reduce intra-cluster dispersion.

Result: Experiments on ten scRNA-seq datasets show that JojoSCL consistently outperforms prevalent clustering methods.

Conclusion: JojoSCL provides an effective solution for scRNA-seq clustering with its practicality validated through robustness analysis and ablation studies.

Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding
of cellular processes by enabling gene expression analysis at the individual
cell level. Clustering allows for the identification of cell types and the
further discovery of intrinsic patterns in single-cell data. However, the high
dimensionality and sparsity of scRNA-seq data continue to challenge existing
clustering models. In this paper, we introduce JojoSCL, a novel self-supervised
contrastive learning framework for scRNA-seq clustering. By incorporating a
shrinkage estimator based on hierarchical Bayesian estimation, which adjusts
gene expression estimates towards more reliable cluster centroids to reduce
intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate
(SURE), JojoSCL refines both instance-level and cluster-level contrastive
learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL
consistently outperforms prevalent clustering methods, with further validation
of its practicality through robustness analysis and ablation studies. JojoSCL's
code is available at: https://github.com/ziwenwang28/JojoSCL.

</details>


### [127] [Blockchain-Enabled Privacy-Preserving Second-Order Federated Edge Learning in Personalized Healthcare](https://arxiv.org/abs/2506.00416)
*Anum Nawaz,Muhammad Irfan,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.LG

TL;DR: A verifiable and auditable optimized second-order FL framework BFEL is proposed for personalized healthcare systems. It incorporates Ethereum-based model aggregation, public key encryption, and FedCurv to ensure trust, privacy, stability, and consistency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in personalized model training caused by non-iid data in first-order FL approaches.

Method: Propose BFEL framework based on optimized FedCurv, incorporating Fisher Information Matrix, Ethereum-based model aggregation, and public key encryption.

Result: Experimental results of federated CNNs and MLPs utilizing Mnist, Cifar-10, and PathMnist demonstrate high efficiency and scalability.

Conclusion: BFEL can effectively manage personalized training on non-iid and heterogeneous data while ensuring trust, verifiability, auditability, privacy, and security.

Abstract: Federated learning (FL) has attracted increasing attention to mitigate
security and privacy challenges in traditional cloud-centric machine learning
models specifically in healthcare ecosystems. FL methodologies enable the
training of global models through localized policies, allowing independent
operations at the edge clients' level. Conventional first-order FL approaches
face several challenges in personalized model training due to heterogeneous
non-independent and identically distributed (non-iid) data of each edge client.
Recently, second-order FL approaches maintain the stability and consistency of
non-iid datasets while improving personalized model training. This study
proposes and develops a verifiable and auditable optimized second-order FL
framework BFEL (blockchain-enhanced federated edge learning) based on optimized
FedCurv for personalized healthcare systems. FedCurv incorporates information
about the importance of each parameter to each client's task (through Fisher
Information Matrix) which helps to preserve client-specific knowledge and
reduce model drift during aggregation. Moreover, it minimizes communication
rounds required to achieve a target precision convergence for each edge client
while effectively managing personalized training on non-iid and heterogeneous
data. The incorporation of Ethereum-based model aggregation ensures trust,
verifiability, and auditability while public key encryption enhances privacy
and security. Experimental results of federated CNNs and MLPs utilizing Mnist,
Cifar-10, and PathMnist demonstrate the high efficiency and scalability of the
proposed framework.

</details>


### [128] [A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks](https://arxiv.org/abs/2506.00420)
*Miao Ye,Suxiao Wang,Jiaguang Han,Yong Wang,Xiaoli Wang,Jingxuan Wei,Peng Wen,Jing Cui*

Main category: cs.LG

TL;DR: The paper proposes MTAD-RD, a spatiotemporal correlation detection model for WSN anomaly detection with a two-stage training strategy. It includes RetNet enhanced by CR module, multigranular feature fusion, and graph attention network modules. A contrastive learning proxy task and joint loss function help tackle issues of missing labels and sample imbalance. Achieved F1 score of 90.97%.


<details>
  <summary>Details</summary>
Motivation: Existing WSN anomaly detection methods struggle with limited spatiotemporal feature extraction, lack of sample labels, few anomaly samples, and imbalanced distributions.

Method: MTAD-RD consists of RetNet enhanced by CR module, multigranular feature fusion, and graph attention network modules. Two-stage training: first, contrastive learning proxy task for unlabeled data; second, caching-based sampler and joint loss function for sample imbalance.

Result: Achieved an F1 score of 90.97% on real public datasets, outperforming existing supervised WSN anomaly detection methods.

Conclusion: MTAD-RD effectively addresses challenges in WSN anomaly detection, achieving high performance.

Abstract: Detecting anomalies in the data collected by WSNs can provide crucial
evidence for assessing the reliability and stability of WSNs. Existing methods
for WSN anomaly detection often face challenges such as the limited extraction
of spatiotemporal correlation features, the absence of sample labels, few
anomaly samples, and an imbalanced sample distribution. To address these
issues, a spatiotemporal correlation detection model (MTAD-RD) considering both
model architecture and a two-stage training strategy perspective is proposed.
In terms of model structure design, the proposed MTAD-RD backbone network
includes a retentive network (RetNet) enhanced by a cross-retention (CR)
module, a multigranular feature fusion module, and a graph attention network
module to extract internode correlation information. This proposed model can
integrate the intermodal correlation features and spatial features of WSN
neighbor nodes while extracting global information from time series data.
Moreover, its serialized inference characteristic can remarkably reduce
inference overhead. For model training, a two-stage training approach was
designed. First, a contrastive learning proxy task was designed for time series
data with graph structure information in WSNs, enabling the backbone network to
learn transferable features from unlabeled data using unsupervised contrastive
learning methods, thereby addressing the issue of missing sample labels in the
dataset. Then, a caching-based sample sampler was designed to divide samples
into few-shot and contrastive learning data. A specific joint loss function was
developed to jointly train the dual-graph discriminator network to address the
problem of sample imbalance effectively. In experiments carried out on real
public datasets, the designed MTAD-RD anomaly detection method achieved an F1
score of 90.97%, outperforming existing supervised WSN anomaly detection
methods.

</details>


### [129] [COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning](https://arxiv.org/abs/2506.00424)
*Chamika Sudusinghe,Gerasimos Gerogiannis Damitha Lenadora,Charles Block,Josep Torrellas,Charith Mendis*

Main category: cs.LG

TL;DR: The paper presents COGNATE, a framework that optimizes sparse tensor programs for early-stage accelerators by using cost models trained on general-purpose hardware data and fine-tuned for specific accelerators, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Optimizing sparse tensor programs for early-stage accelerators is challenging due to sensitivity to sparse inputs and reliance on expensive simulators. Existing ML-based cost models are ineffective as they require large datasets for training.

Method: COGNATE leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, then applies few-shot fine-tuning on emerging hardware. It exploits the homogeneity of input features across hardware platforms while mitigating heterogeneity.

Result: COGNATE outperforms existing techniques with average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.

Conclusion: COGNATE achieves comparable performance to accelerator-specific models but requires only 5% of the data samples, making it an effective solution for optimizing sparse tensor programs on early-stage accelerators.

Abstract: Sparse tensor programs are essential in deep learning and graph analytics,
driving the need for optimized processing. To meet this demand, specialized
hardware accelerators are being developed. Optimizing these programs for
accelerators is challenging for two reasons: program performance is highly
sensitive to variations in sparse inputs, and early-stage accelerators rely on
expensive simulators. Therefore, ML-based cost models used for optimizing such
programs on general-purpose hardware are often ineffective for early-stage
accelerators, as they require large datasets for proper training. To this end,
we introduce COGNATE, a novel framework that leverages inexpensive data samples
from general-purpose hardware (e.g., CPUs) to train cost models, followed by
few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of
input features across hardware platforms while effectively mitigating
heterogeneity, enabling cost model training with just 5% of the data samples
needed by accelerator-specific models to achieve comparable performance. We
conduct extensive experiments to demonstrate that COGNATE outperforms existing
techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and
1.39x (up to 4.22x) for SDDMM.

</details>


### [130] [TIDFormer: Exploiting Temporal and Interactive Dynamics Makes A Great Dynamic Graph Transformer](https://arxiv.org/abs/2506.00431)
*Jie Peng,Zhewei Wei,Yuhang Ye*

Main category: cs.LG

TL;DR: The paper introduces TIDFormer, a dynamic graph TransFormer that efficiently captures temporal and interactive dynamics, outperforming state-of-the-art models in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based dynamic graph neural networks (DGNNs) have varying effectiveness and efficiency. There is a need for a method that properly defines self-attention mechanisms (SAMs) on dynamic graphs and comprehensively encodes temporal and interactive dynamics without complex modules.

Method: The authors propose TIDFormer which uses calendar-based time partitioning to model temporal dynamics and extracts interaction embeddings from sampled first-order neighbors for interactive dynamics. It jointly models both types of features by capturing changes in historical interaction patterns through decomposition.

Result: Extensive experiments on multiple dynamic graph datasets show that TIDFormer outperforms state-of-the-art models in most cases and has significant efficiency advantages compared to previous Transformer-based methods.

Conclusion: TIDFormer is an effective and efficient dynamic graph TransFormer that addresses the interpretability issues of SAMs on dynamic graphs and demonstrates superior performance across various experimental settings.

Abstract: Due to the proficiency of self-attention mechanisms (SAMs) in capturing
dependencies in sequence modeling, several existing dynamic graph neural
networks (DGNNs) utilize Transformer architectures with various encoding
designs to capture sequential evolutions of dynamic graphs. However, the
effectiveness and efficiency of these Transformer-based DGNNs vary
significantly, highlighting the importance of properly defining the SAM on
dynamic graphs and comprehensively encoding temporal and interactive dynamics
without extra complex modules. In this work, we propose TIDFormer, a dynamic
graph TransFormer that fully exploits Temporal and Interactive Dynamics in an
efficient manner. We clarify and verify the interpretability of our proposed
SAM, addressing the open problem of its uninterpretable definitions on dynamic
graphs in previous works. To model the temporal and interactive dynamics,
respectively, we utilize the calendar-based time partitioning information and
extract informative interaction embeddings for both bipartite and non-bipartite
graphs using merely the sampled first-order neighbors. In addition, we jointly
model temporal and interactive features by capturing potential changes in
historical interaction patterns through a simple decomposition. We conduct
extensive experiments on several dynamic graph datasets to verify the
effectiveness and efficiency of TIDFormer. The experimental results demonstrate
that TIDFormer excels, outperforming state-of-the-art models across most
datasets and experimental settings. Furthermore, TIDFormer exhibits significant
efficiency advantages compared to previous Transformer-based methods.

</details>


### [131] [Channel Normalization for Time Series Channel Identification](https://arxiv.org/abs/2506.00432)
*Seunghan Lee,Taeyoung Park,Kibok Lee*

Main category: cs.LG

TL;DR: The paper introduces Channel Normalization (CN) to enhance channel identifiability (CID) in time series modeling, with variants Adaptive CN (ACN) and Prototypical CN (PCN), showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the issue of lacking channel identifiability (CID) in time series modeling, which often leads to identical outputs for identical inputs regardless of channel-specific characteristics.

Method: Propose Channel Normalization (CN) that assigns distinct affine transformation parameters to each channel. Further extend CN with Adaptive CN (ACN) for dynamic parameter adjustment based on input time series and Prototypical CN (PCN) using learnable prototypes instead of per-channel parameters.

Result: Significant performance gains were achieved when applying CN and its variants to various time series models, both for non-CID and CID models. The success is also analyzed from an information theory perspective.

Conclusion: Channel Normalization and its extensions effectively improve channel identifiability in time series modeling, leading to better model performance.

Abstract: Channel identifiability (CID) refers to the ability to distinguish between
individual channels in time series (TS) modeling. The absence of CID often
results in producing identical outputs for identical inputs, disregarding
channel-specific characteristics. In this paper, we highlight the importance of
CID and propose Channel Normalization (CN), a simple yet effective
normalization strategy that enhances CID by assigning distinct affine
transformation parameters to each channel. We further extend CN in two ways: 1)
Adaptive CN (ACN) dynamically adjusts parameters based on the input TS,
improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a
set of learnable prototypes instead of per-channel parameters, enabling
applicability to datasets with unknown or varying number of channels and
facilitating use in TS foundation models. We demonstrate the effectiveness of
CN and its variants by applying them to various TS models, achieving
significant performance gains for both non-CID and CID models. In addition, we
analyze the success of our approach from an information theory perspective.
Code is available at https://github.com/seunghan96/CN.

</details>


### [132] [Learning from Double Positive and Unlabeled Data for Potential-Customer Identification](https://arxiv.org/abs/2506.00436)
*Masahiro Kato,Yuki Ikeda abd Kentaro Baba,Takashi Imai,Ryo Inokuchi*

Main category: cs.LG

TL;DR: 提出了一种通过正未标记学习（PU学习）识别目标营销中潜在客户的方法，该方法在有限数据下构建分类器以识别对产品感兴趣但对公司忠诚度不高的潜在客户，并通过数值实验验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在目标营销中，公司只能观察到购买产品的客户，决策者希望通过有效营销基于人们对公司的忠诚度来推广产品。对于忠诚度不高的人群，若不进行营销关注，他们可能会忽略产品或购买其他公司的类似产品。因此，将营销重点放在对产品感兴趣但忠诚度不高的个体上可以实现更高效的营销。

Method: 研究提出了一种称为双PU学习的方法，从有限的数据中学习一个分类器，用于识别对产品感兴趣但对公司没有忠诚度的潜在客户。该算法包含单阶段优化，但其目标函数隐含了来自标准PU学习设置的两个损失。

Result: 通过数值实验验证了所提出的算法的有效性，确认其适用于当前问题。

Conclusion: 所提出的双PU学习方法能够有效识别目标营销中的潜在客户，有助于提高营销效率。

Abstract: In this study, we propose a method for identifying potential customers in
targeted marketing by applying learning from positive and unlabeled data (PU
learning). We consider a scenario in which a company sells a product and can
observe only the customers who purchased it. Decision-makers seek to market
products effectively based on whether people have loyalty to the company.
Individuals with loyalty are those who are likely to remain interested in the
company even without additional advertising. Consequently, those loyal
customers would likely purchase from the company if they are interested in the
product. In contrast, people with lower loyalty may overlook the product or buy
similar products from other companies unless they receive marketing attention.
Therefore, by focusing marketing efforts on individuals who are interested in
the product but do not have strong loyalty, we can achieve more efficient
marketing. To achieve this goal, we consider how to learn, from limited data, a
classifier that identifies potential customers who (i) have interest in the
product and (ii) do not have loyalty to the company. Although our algorithm
comprises a single-stage optimization, its objective function implicitly
contains two losses derived from standard PU learning settings. For this
reason, we refer to our approach as double PU learning. We verify the validity
of the proposed algorithm through numerical experiments, confirming that it
functions appropriately for the problem at hand.

</details>


### [133] [Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks](https://arxiv.org/abs/2506.00437)
*Jiaxing Zhang,Xiaoou Liu,Dongsheng Luo,Hua Wei*

Main category: cs.LG

TL;DR: The paper introduces ConfExplainer, an explainer framework with a confidence scoring module based on GIB-CC theory, to quantify the reliability of explanations for GNN predictions, especially in out-of-distribution datasets. Experiments show its superiority in enhancing trustworthiness and robustness.


<details>
  <summary>Details</summary>
Motivation: There is a need for reliable interpretability of GNN predictions, particularly for out-of-distribution or unknown test datasets where current post-hoc instance-level explanation methods may be uncertain.

Method: Introduced ConfExplainer, which is grounded in the generalized graph information bottleneck with confidence constraint (GIB-CC) theory. This framework includes a confidence scoring module to assess the reliability of generated explanations.

Result: Experimental results indicate that ConfExplainer performs better than existing methods, showing the effectiveness of the confidence score in improving the trustworthiness and robustness of GNN explanations.

Conclusion: ConfExplainer provides a reliable way to evaluate the explanations of GNNs, thus enhancing their interpretability and trustworthiness.

Abstract: Explaining Graph Neural Networks (GNNs) has garnered significant attention
due to the need for interpretability, enabling users to understand the behavior
of these black-box models better and extract valuable insights from their
predictions. While numerous post-hoc instance-level explanation methods have
been proposed to interpret GNN predictions, the reliability of these
explanations remains uncertain, particularly in the out-of-distribution or
unknown test datasets. In this paper, we address this challenge by introducing
an explainer framework with the confidence scoring module ( ConfExplainer),
grounded in theoretical principle, which is generalized graph information
bottleneck with confidence constraint (GIB-CC), that quantifies the reliability
of generated explanations. Experimental results demonstrate the superiority of
our approach, highlighting the effectiveness of the confidence score in
enhancing the trustworthiness and robustness of GNN explanations.

</details>


### [134] [PointODE: Lightweight Point Cloud Learning with Neural Ordinary Differential Equations on Edge](https://arxiv.org/abs/2506.00438)
*Keisuke Sugiura,Mizuki Yasuda,Hiroki Matsutani*

Main category: cs.LG

TL;DR: PointODE是一种高效的点云特征提取架构，通过重用MLP块中的相同参数来压缩模型，并提出了轻量级版本PointODE-Elite及其专用加速器，该加速器在嵌入式FPGA上实现，比ARM Cortex-A53 CPU快4.9倍，推理速度快3.7倍，能效高3.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法可能无法适应嵌入式边缘设备，因为这些设备资源有限。为了解决这个问题，作者引入了PointODE。

Method: PointODE是一种基于MLP块和残差连接的高效ResNet-like架构，利用神经ODE压缩模型并重用参数，同时提出逐点归一化处理非均匀分布的特征点。PointODE-Elite是轻量级版本，具有0.58M可训练参数，并设计了专用的FPGA加速器。

Result: 与ARM Cortex-A53 CPU相比，FPGA上的加速器使特征提取速度提高了4.9倍，推理速度快3.7倍，能效提高3.5倍。PointODE-Elite在合成和真实世界的分类数据集上表现出与最先进模型相当的准确性。

Conclusion: PointODE-Elite显著提高了准确性和推理成本之间的权衡，适合在资源受限的嵌入式设备上运行点云应用。

Abstract: Embedded edge devices are often used as a computing platform to run
real-world point cloud applications, but recent deep learning-based methods may
not fit on such devices due to limited resources. In this paper, we aim to fill
this gap by introducing PointODE, a parameter-efficient ResNet-like
architecture for point cloud feature extraction based on a stack of MLP blocks
with residual connections. We leverage Neural ODE (Ordinary Differential
Equation), a continuous-depth version of ResNet originally developed for
modeling the dynamics of continuous-time systems, to compress PointODE by
reusing the same parameters across MLP blocks. The point-wise normalization is
proposed for PointODE to handle the non-uniform distribution of feature points.
We introduce PointODE-Elite as a lightweight version with 0.58M trainable
parameters and design its dedicated accelerator for embedded FPGAs. The
accelerator consists of a four-stage pipeline to parallelize the feature
extraction for multiple points and stores the entire parameters on-chip to
eliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53
CPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature
extraction by 4.9x, leading to 3.7x faster inference and 3.5x better
energy-efficiency. Despite the simple architecture, PointODE-Elite shows
competitive accuracy to the state-of-the-art models on both synthetic and
real-world classification datasets, greatly improving the trade-off between
accuracy and inference cost.

</details>


### [135] [RLAE: Reinforcement Learning-Assisted Ensemble for LLMs](https://arxiv.org/abs/2506.00439)
*Yuqian Fu,Yuanheng Zhu,Jiajun Chai,Guojun Yin,Wei Lin,Qichao Zhang,Dongbin Zhao*

Main category: cs.LG

TL;DR: This paper proposes Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a new framework that uses reinforcement learning to dynamically adjust ensemble weights of large language models based on input context and intermediate generation states. It demonstrates significant improvements over traditional ensemble methods, with better generalization and lower latency.


<details>
  <summary>Details</summary>
Motivation: Existing ensembling methods for large language models use fixed weighting strategies that do not adapt well to the dynamic and context-dependent nature of these models' capabilities.

Method: The authors propose RLAE, which reformulates the ensembling process as a Markov Decision Process. A reinforcement learning agent is introduced to dynamically adjust ensemble weights by considering both input context and intermediate generation states. The agent is trained using rewards that correspond to the quality of final outputs. Two implementations are explored: single-agent ($\text{RLAE}_\text{PPO}$) and multi-agent ($\text{RLAE}_\text{MAPPO}$).

Result: Extensive evaluations show that RLAE outperforms existing approaches by up to 3.3% accuracy points across various tasks. It also exhibits superior generalization without retraining and achieves lower time latency.

Conclusion: RLAE provides a more effective framework for ensembling large language models, offering improved performance, generalization, and efficiency.

Abstract: Ensembling large language models (LLMs) can effectively combine diverse
strengths of different models, offering a promising approach to enhance
performance across various tasks. However, existing methods typically rely on
fixed weighting strategies that fail to adapt to the dynamic, context-dependent
characteristics of LLM capabilities. In this work, we propose Reinforcement
Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates
LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach
introduces a RL agent that dynamically adjusts ensemble weights by considering
both input context and intermediate generation states, with the agent being
trained using rewards that directly correspond to the quality of final outputs.
We implement RLAE using both single-agent and multi-agent reinforcement
learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ),
demonstrating substantial improvements over conventional ensemble methods.
Extensive evaluations on a diverse set of tasks show that RLAE outperforms
existing approaches by up to $3.3\%$ accuracy points, offering a more effective
framework for LLM ensembling. Furthermore, our method exhibits superior
generalization capabilities across different tasks without the need for
retraining, while simultaneously achieving lower time latency.

</details>


### [136] [PSI-PFL: Population Stability Index for Client Selection in non-IID Personalized Federated Learning](https://arxiv.org/abs/2506.00440)
*Daniel-M. Jimenez-Gutierrez,David Solans,Mohammed Elbamby,Nicolas Kourtellis*

Main category: cs.LG

TL;DR: In the abstract, a new client selection framework for Personalized Federated Learning (PFL) called PSI-PFL is proposed. It leverages the Population Stability Index (PSI) to reduce data heterogeneity and improve global model accuracy in non-IID scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge posed by non-independent and identically distributed (non-IID) data across clients in Federated Learning (FL), which leads to skewed model updates and performance degradation.

Method: The method introduced in this paper is PSI-PFL, a novel client selection framework for Personalized Federated Learning (PFL). This approach uses the Population Stability Index (PSI) to quantify and mitigate data heterogeneity (non-IIDness). By selecting more homogeneous clients based on PSI, it reduces the impact of label skew.

Result: The experimental results show that PSI-PFL significantly improves global model accuracy across multiple data modalities (tabular, image, text). It outperforms state-of-the-art baselines by up to 10% under non-IID scenarios while ensuring fairer local performance.

Conclusion: PSI-PFL enhances FL performance and offers practical benefits in applications where data privacy and heterogeneity are critical.

Abstract: Federated Learning (FL) enables decentralized machine learning (ML) model
training while preserving data privacy by keeping data localized across
clients. However, non-independent and identically distributed (non-IID) data
across clients poses a significant challenge, leading to skewed model updates
and performance degradation. Addressing this, we propose PSI-PFL, a novel
client selection framework for Personalized Federated Learning (PFL) that
leverages the Population Stability Index (PSI) to quantify and mitigate data
heterogeneity (so-called non-IIDness). Our approach selects more homogeneous
clients based on PSI, reducing the impact of label skew, one of the most
detrimental factors in FL performance. Experimental results over multiple data
modalities (tabular, image, text) demonstrate that PSI-PFL significantly
improves global model accuracy, outperforming state-of-the-art baselines by up
to 10\% under non-IID scenarios while ensuring fairer local performance.
PSI-PFL enhances FL performance and offers practical benefits in applications
where data privacy and heterogeneity are critical.

</details>


### [137] [TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction](https://arxiv.org/abs/2506.00453)
*Hao Li,Hao Wan,Yuzhou Chen,Dongsheng Ye,Yulia Gel,Hao Jiang*

Main category: cs.LG

TL;DR: To address the limitations of current meta-learning approaches for dynamic graphs, the authors developed Dowker Zigzag Persistence (DZP) and TMetaNet. DZP captures high-order features using Dowker complex and zigzag persistence, while TMetaNet uses these features to improve parameter updates in a meta-learning model. Experiments show that TMetaNet performs well on real-world datasets and is robust to graph noise.


<details>
  <summary>Details</summary>
Motivation: Dynamic graphs present challenges for traditional graph learning due to their changing structures and temporal dependencies. Current meta-learning approaches for dynamic graphs rely on fixed weight update parameters and neglect high-order topological information.

Method: The authors designed Dowker Zigzag Persistence (DZP), which uses Dowker complex and zigzag persistence to capture high-order features of dynamic graphs. They then proposed TMetaNet, a new meta-learning parameter update model based on dynamic topological features, which utilizes distances between high-order topological features for more effective adaptation across snapshots.

Result: Experiments on real-world datasets demonstrate that TMetaNet achieves state-of-the-art performance and is resilient to graph noise.

Conclusion: TMetaNet shows great potential for meta-learning and dynamic graph analysis by effectively capturing and utilizing high-order topological features.

Abstract: Dynamic graphs evolve continuously, presenting challenges for traditional
graph learning due to their changing structures and temporal dependencies.
Recent advancements have shown potential in addressing these challenges by
developing suitable meta-learning-based dynamic graph neural network models.
However, most meta-learning approaches for dynamic graphs rely on fixed weight
update parameters, neglecting the essential intrinsic complex high-order
topological information of dynamically evolving graphs. We have designed Dowker
Zigzag Persistence (DZP), an efficient and stable dynamic graph persistent
homology representation method based on Dowker complex and zigzag persistence,
to capture the high-order features of dynamic graphs. Armed with the DZP ideas,
we propose TMetaNet, a new meta-learning parameter update model based on
dynamic topological features. By utilizing the distances between high-order
topological features, TMetaNet enables more effective adaptation across
snapshots. Experiments on real-world datasets demonstrate TMetaNet's
state-of-the-art performance and resilience to graph noise, illustrating its
high potential for meta-learning and dynamic graph analysis. Our code is
available at https://github.com/Lihaogx/TMetaNet.

</details>


### [138] [Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models](https://arxiv.org/abs/2506.00457)
*Junwoo Park,Hyuck Lee,Dohyun Lee,Daehoon Gwak,Jaegul Choo*

Main category: cs.LG

TL;DR: This paper evaluates the effectiveness of LLMs as zero-shot forecasters for time-series data, comparing them with domain-specific models. It finds that LLMs are sensitive to noise and underperform compared to even simple domain-specific models. The authors suggest focusing on fine-tuning LLMs for numerical sequence processing rather than emphasizing zero-shot forecasting.


<details>
  <summary>Details</summary>
Motivation: To rigorously validate the conflicting findings about LLMs' potential in time-series forecasting, specifically addressing their effectiveness in zero-shot forecasting.

Method: The authors compare LLM-based zero-shot forecasters with state-of-the-art domain-specific models through experiments. They also explore solutions to reduce LLMs' sensitivity to noise in the zero-shot setting.

Result: LLM-based zero-shot forecasters struggle with high accuracy due to sensitivity to noise and underperform even simple domain-specific models. Improving LLMs' robustness remains a challenge.

Conclusion: Rather than emphasizing zero-shot forecasting, it is more promising to focus on fine-tuning LLMs to better process numerical sequences.

Abstract: Large Language Models (LLMs) have shown remarkable performance across diverse
tasks without domain-specific training, fueling interest in their potential for
time-series forecasting. While LLMs have shown potential in zero-shot
forecasting through prompting alone, recent studies suggest that LLMs lack
inherent effectiveness in forecasting. Given these conflicting findings, a
rigorous validation is essential for drawing reliable conclusions. In this
paper, we evaluate the effectiveness of LLMs as zero-shot forecasters compared
to state-of-the-art domain-specific models. Our experiments show that LLM-based
zero-shot forecasters often struggle to achieve high accuracy due to their
sensitivity to noise, underperforming even simple domain-specific models. We
have explored solutions to reduce LLMs' sensitivity to noise in the zero-shot
setting, but improving their robustness remains a significant challenge. Our
findings suggest that rather than emphasizing zero-shot forecasting, a more
promising direction would be to focus on fine-tuning LLMs to better process
numerical sequences. Our experimental code is available at
https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster.

</details>


### [139] [Reinforcement Learning for Hanabi](https://arxiv.org/abs/2506.00458)
*Nina Cohen,Kordel K. France*

Main category: cs.LG

TL;DR: In Hanabi, a cooperative card game with incomplete information, researchers tested various tabular and deep reinforcement learning algorithms. They found that temporal difference (TD) algorithms performed better overall, with tabular Expected SARSA and deep Q-Learning agents showing the best performance.


<details>
  <summary>Details</summary>
Motivation: Hanabi presents an environment of cooperative gameplay with incomplete knowledge, which poses a unique challenge for reinforcement learning agents. The motivation is to explore which RL algorithms perform best in this context.

Method: The study explored different tabular and deep reinforcement learning algorithms, analyzing their performance against same-type and different-type agents. It also examined how certain agents adapt to opposing agents' behavior.

Result: Temporal difference (TD) algorithms showed superior overall performance and balance compared to tabular agents. Notably, tabular Expected SARSA and deep Q-Learning agents demonstrated the best results.

Conclusion: TD-based algorithms, including tabular Expected SARSA and deep Q-Learning agents, offer the best performance and adaptability in Hanabi's challenging environment.

Abstract: Hanabi has become a popular game for research when it comes to reinforcement
learning (RL) as it is one of the few cooperative card games where you have
incomplete knowledge of the entire environment, thus presenting a challenge for
a RL agent. We explored different tabular and deep reinforcement learning
algorithms to see which had the best performance both against an agent of the
same type and also against other types of agents. We establish that certain
agents played their highest scoring games against specific agents while others
exhibited higher scores on average by adapting to the opposing agent's
behavior. We attempted to quantify the conditions under which each algorithm
provides the best advantage and identified the most interesting interactions
between agents of different types. In the end, we found that temporal
difference (TD) algorithms had better overall performance and balancing of play
types compared to tabular agents. Specifically, tabular Expected SARSA and deep
Q-Learning agents showed the best performance.

</details>


### [140] [Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control](https://arxiv.org/abs/2506.00459)
*Elinor Ginzburg,Itay Segev,Yoash Levron,Sarah Keren*

Main category: cs.LG

TL;DR: This paper compares traditional and reinforcement learning (RL) approaches for energy storage management in a simplified micro-grid model with three use cases.


<details>
  <summary>Details</summary>
Motivation: To understand the tradeoffs between traditional and RL approaches for energy storage management, particularly the performance loss when using a generative RL policy instead of a traditional approach.

Method: Comparison based on a simplified micro-grid model that includes a load component, a photovoltaic source, and a storage device. Three use cases are examined: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses.

Result: Detailed formulation of each use case and optimization challenges provided. Comparison of traditional and RL methods' performances, settings beneficial to each method discussed.

Conclusion: Promotes the principled use of RL based methods in energy storage management and suggests avenues for future investigation.

Abstract: We aim to better understand the tradeoffs between traditional and
reinforcement learning (RL) approaches for energy storage management. More
specifically, we wish to better understand the performance loss incurred when
using a generative RL policy instead of using a traditional approach to find
optimal control policies for specific instances. Our comparison is based on a
simplified micro-grid model, that includes a load component, a photovoltaic
source, and a storage device. Based on this model, we examine three use cases
of increasing complexity: ideal storage with convex cost functions, lossy
storage devices, and lossy storage devices with convex transmission losses.
With the aim of promoting the principled use RL based methods in this
challenging and important domain, we provide a detailed formulation of each use
case and a detailed description of the optimization challenges. We then compare
the performance of traditional and RL methods, discuss settings in which it is
beneficial to use each method, and suggest avenues for future investigation.

</details>


### [141] [SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning](https://arxiv.org/abs/2506.00467)
*Shuai Zhao,Heyan Huang,Xinge Li,Xiaokang Chen,Rui Wang*

Main category: cs.LG

TL;DR: The paper proposes Self-training with Self-adaptive Thresholding (SST), an efficient semi-supervised learning framework that adaptively adjusts class-specific thresholds based on the model's learning progress, achieving state-of-the-art performance with remarkable efficiency, generalization, and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in accurately selecting sufficient high-quality pseudo-labels in semi-supervised learning due to reliance on fixed thresholds, as well as the time-consuming and computationally intensive process of updating thresholds.

Method: The proposed method, SST, introduces a Self-Adaptive Thresholding (SAT) mechanism that adaptively adjusts class-specific thresholds according to the model's learning progress, ensuring the selection of high-quality pseudo-labeled data and mitigating risks of inaccurate pseudo-labels and confirmation bias.

Result: Extensive experiments show SST achieves state-of-the-art performance with remarkable efficiency, generalization, and scalability across various architectures and datasets. Notably, Semi-SST-ViT-Huge achieves 80.7% / 84.9% Top-1 accuracy using only 1% / 10% labeled data on ImageNet-1K SSL benchmarks, outperforming fully-supervised DeiT-III-ViT-Huge which requires 100% labeled data for similar accuracy.

Conclusion: SST is an effective and efficient semi-supervised learning framework that significantly advances SSL research by overcoming limitations of previous methods through its innovative SAT mechanism.

Abstract: Neural networks have demonstrated exceptional performance in supervised
learning, benefiting from abundant high-quality annotated data. However,
obtaining such data in real-world scenarios is costly and labor-intensive.
Semi-supervised learning (SSL) offers a solution to this problem. Recent
studies, such as Semi-ViT and Noisy Student, which employ consistency
regularization or pseudo-labeling, have demonstrated significant achievements.
However, they still face challenges, particularly in accurately selecting
sufficient high-quality pseudo-labels due to their reliance on fixed
thresholds. Recent methods such as FlexMatch and FreeMatch have introduced
flexible or self-adaptive thresholding techniques, greatly advancing SSL
research. Nonetheless, their process of updating thresholds at each iteration
is deemed time-consuming, computationally intensive, and potentially
unnecessary. To address these issues, we propose Self-training with
Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL
framework. SST introduces an innovative Self-Adaptive Thresholding (SAT)
mechanism that adaptively adjusts class-specific thresholds based on the
model's learning progress. SAT ensures the selection of high-quality
pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and
confirmation bias. Extensive experiments demonstrate that SST achieves
state-of-the-art performance with remarkable efficiency, generalization, and
scalability across various architectures and datasets. Semi-SST-ViT-Huge
achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%
/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the
fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using
100% labeled data, our method demonstrates superior performance using only 10%
labeled data.

</details>


### [142] [Towards Graph-Based Privacy-Preserving Federated Learning: ModelNet -- A ResNet-based Model Classification Dataset](https://arxiv.org/abs/2506.00476)
*Abhisek Ray,Lukas Esterle*

Main category: cs.LG

TL;DR: The paper introduces ModelNet, a novel image classification dataset designed for Federated Learning (FL) with three variants that simulate realistic FL settings incorporating non-IID data distributions and client diversity. It proposes a hypothesis to preserve local privacy effectively and provides a practical benchmark for FL research.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges in preserving the privacy of local data and lacks domain heterogeneity and client-specific segregation in benchmarks, which are critical for rigorous evaluation.

Method: The authors modify the CIFAR100 dataset into three client-specific variants considering domain heterogeneities (homogeneous, heterogeneous, and random). They train each client-specific subset on a pre-trained ResNet50 model to save model parameters, creating the ModelNet dataset. They also propose a new hypothesis for FL algorithms to access anonymized model parameters to preserve privacy.

Result: Extensive experiments based on domain shifts and aggregation strategies demonstrate the effectiveness of the proposed ModelNet variants (ModelNet-S, ModelNet-D, and ModelNet-R) as a practical benchmark for classical and graph-based FL research.

Conclusion: ModelNet is introduced as a novel dataset that simulates realistic FL settings, addressing the lack of domain heterogeneity and client-specific segregation in existing benchmarks. The dataset and related code are available online.

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for training
machine learning models across distributed data sources while preserving data
locality. However, the privacy of local data is always a pivotal concern and
has received a lot of attention in recent research on the FL regime. Moreover,
the lack of domain heterogeneity and client-specific segregation in the
benchmarks remains a critical bottleneck for rigorous evaluation. In this
paper, we introduce ModelNet, a novel image classification dataset constructed
from the embeddings extracted from a pre-trained ResNet50 model. First, we
modify the CIFAR100 dataset into three client-specific variants, considering
three domain heterogeneities (homogeneous, heterogeneous, and random).
Subsequently, we train each client-specific subset of all three variants on the
pre-trained ResNet50 model to save model parameters. In addition to
multi-domain image data, we propose a new hypothesis to define the FL algorithm
that can access the anonymized model parameters to preserve the local privacy
in a more effective manner compared to existing ones. ModelNet is designed to
simulate realistic FL settings by incorporating non-IID data distributions and
client diversity design principles in the mainframe for both conventional and
futuristic graph-driven FL algorithms. The three variants are ModelNet-S,
ModelNet-D, and ModelNet-R, which are based on homogeneous, heterogeneous, and
random data settings, respectively. To the best of our knowledge, we are the
first to propose a cross-environment client-specific FL dataset along with the
graph-based variant. Extensive experiments based on domain shifts and
aggregation strategies show the effectiveness of the above variants, making it
a practical benchmark for classical and graph-based FL research. The dataset
and related code are available online.

</details>


### [143] [Flashbacks to Harmonize Stability and Plasticity in Continual Learning](https://arxiv.org/abs/2506.00477)
*Leila Mahmoodi,Peyman Moghadam,Munawar Hayat,Christian Simon,Mehrtash Harandi*

Main category: cs.LG

TL;DR: The paper introduces Flashback Learning (FL), a novel method for Continual Learning that enhances the balance between stability and plasticity through bidirectional regularization. FL improves model accuracy in Class-Incremental and Task-Incremental settings, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Continual Learning models often face challenges in maintaining a balance between stability (retaining old knowledge) and plasticity (acquiring new knowledge). Existing methods primarily focus on regularizing model updates to preserve old information while learning new concepts, but they do not explicitly address this trade-off effectively.

Method: Flashback Learning (FL) employs a two-phase training process with bidirectional regularization. It uses two distinct knowledge bases - one for enhancing plasticity and another for improving stability - to guide the model in swiftly incorporating new knowledge while actively retaining old knowledge. FL can be integrated into various CL methods including replay, parameter regularization, distillation, and dynamic architecture techniques.

Result: Empirically, FL demonstrates significant improvements over baseline methods within the same training budget. Specifically, it achieves an average accuracy improvement of up to 4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard image classification benchmarks. Additionally, FL outperforms state-of-the-art CL methods on challenging datasets like ImageNet, as confirmed by measurements of the stability-to-plasticity ratio.

Conclusion: Flashback Learning (FL) is a novel approach that effectively balances the stability and plasticity of models in Continual Learning. By integrating FL into different CL methods, it enhances model performance and sets a new benchmark for future research in this domain.

Abstract: We introduce Flashback Learning (FL), a novel method designed to harmonize
the stability and plasticity of models in Continual Learning (CL). Unlike prior
approaches that primarily focus on regularizing model updates to preserve old
information while learning new concepts, FL explicitly balances this trade-off
through a bidirectional form of regularization. This approach effectively
guides the model to swiftly incorporate new knowledge while actively retaining
its old knowledge. FL operates through a two-phase training process and can be
seamlessly integrated into various CL methods, including replay, parameter
regularization, distillation, and dynamic architecture techniques. In designing
FL, we use two distinct knowledge bases: one to enhance plasticity and another
to improve stability. FL ensures a more balanced model by utilizing both
knowledge bases to regularize model updates. Theoretically, we analyze how the
FL mechanism enhances the stability-plasticity balance. Empirically, FL
demonstrates tangible improvements over baseline methods within the same
training budget. By integrating FL into at least one representative baseline
from each CL category, we observed an average accuracy improvement of up to
4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard
image classification benchmarks. Additionally, measurements of the
stability-to-plasticity ratio confirm that FL effectively enhances this
balance. FL also outperforms state-of-the-art CL methods on more challenging
datasets like ImageNet.

</details>


### [144] [Dynamic Domain Adaptation-Driven Physics-Informed Graph Representation Learning for AC-OPF](https://arxiv.org/abs/2506.00478)
*Hongjie Zhu,Zezheng Zhang,Zeyu Zhang,Yu Bai,Shimin Wen,Huazhang Wang,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.LG

TL;DR: The paper proposes DDA-PIGCN, a novel method for AC-OPF problems that addresses constraint-related issues and integrates spatiotemporal features through a graph-based learning framework.


<details>
  <summary>Details</summary>
Motivation: Current AC-OPF solvers have limitations in representing complex relationships between variable distributions in the constraint space and their optimal solutions, restricting knowledge representation diversity. Also, modeling solely on spatial topology limits additional prior knowledge integration like temporal information.

Method: DDA-PIGCN is proposed which applies multi-layer hard physics-informed constraints to improve consistency optimization for long-range dependencies. It uses dynamic domain adaptation learning mechanism for precise constraint verification and captures spatiotemporal dependencies by leveraging power grid's physical structure.

Result: DDA-PIGCN shows strong performance across several IEEE standard test cases with mean absolute errors (MAE) from 0.0011 to 0.0624 and constraint satisfaction rates between 99.6% and 100%.

Conclusion: DDA-PIGCN proves to be a reliable and efficient AC-OPF solver by addressing constraint-related issues and effectively integrating spatiotemporal features.

Abstract: Alternating Current Optimal Power Flow (AC-OPF) aims to optimize generator
power outputs by utilizing the non-linear relationships between voltage
magnitudes and phase angles in a power system. However, current AC-OPF solvers
struggle to effectively represent the complex relationship between variable
distributions in the constraint space and their corresponding optimal
solutions. This limitation in constraint modeling restricts the system's
ability to develop diverse knowledge representations. Additionally, modeling
the power grid solely based on spatial topology further limits the integration
of additional prior knowledge, such as temporal information. To overcome these
challenges, we propose DDA-PIGCN (Dynamic Domain Adaptation-Driven
Physics-Informed Graph Convolutional Network), a new method designed to address
constraint-related issues and build a graph-based learning framework that
incorporates spatiotemporal features. DDA-PIGCN improves consistency
optimization for features with varying long-range dependencies by applying
multi-layer, hard physics-informed constraints. It also uses a dynamic domain
adaptation learning mechanism that iteratively updates and refines key state
variables under predefined constraints, enabling precise constraint
verification. Moreover, it captures spatiotemporal dependencies between
generators and loads by leveraging the physical structure of the power grid,
allowing for deep integration of topological information across time and space.
Extensive comparative and ablation studies show that DDA-PIGCN delivers strong
performance across several IEEE standard test cases (such as case9, case30, and
case300), achieving mean absolute errors (MAE) from 0.0011 to 0.0624 and
constraint satisfaction rates between 99.6% and 100%, establishing it as a
reliable and efficient AC-OPF solver.

</details>


### [145] [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482)
*Eunsu Kim,Haneul Yoo,Guijin Son,Hitesh Patel,Amit Agarwal,Alice Oh*

Main category: cs.LG

TL;DR: BenchHub is a dynamic benchmark repository aggregating 303K questions across 38 benchmarks, supporting continuous updates and customizable evaluations for large language models (LLMs) in various domains.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of up-to-date, well-organized benchmarks for evaluating large language models (LLMs), especially considering the growing importance of domain-specific models in areas such as math or code.

Method: The authors introduce BenchHub, a dynamic benchmark repository that aggregates and automatically classifies benchmark datasets from diverse domains. It integrates 303K questions across 38 benchmarks and supports continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases.

Result: Through extensive experiments with various LLM families, the authors demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking.

Conclusion: The authors believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering critical infrastructure for advancing LLM evaluation research.

Abstract: As large language models (LLMs) continue to advance, the need for up-to-date
and well-organized benchmarks becomes increasingly critical. However, many
existing datasets are scattered, difficult to manage, and make it challenging
to perform evaluations tailored to specific needs or domains, despite the
growing importance of domain-specific models in areas such as math or code. In
this paper, we introduce BenchHub, a dynamic benchmark repository that empowers
researchers and developers to evaluate LLMs more effectively. BenchHub
aggregates and automatically classifies benchmark datasets from diverse
domains, integrating 303K questions across 38 benchmarks. It is designed to
support continuous updates and scalable data management, enabling flexible and
customizable evaluation tailored to various domains or use cases. Through
extensive experiments with various LLM families, we demonstrate that model
performance varies significantly across domain-specific subsets, emphasizing
the importance of domain-aware benchmarking. We believe BenchHub can encourage
better dataset reuse, more transparent model comparisons, and easier
identification of underrepresented areas in existing benchmarks, offering a
critical infrastructure for advancing LLM evaluation research.

</details>


### [146] [It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs](https://arxiv.org/abs/2506.00486)
*Jun Wu,Yirong Xiong,Jiangtao Wen,Yuxing Han*

Main category: cs.LG

TL;DR: 尽管大型语言模型（LLMs）的研究和部署取得了快速进展，但模型参数的统计分布及其对初始化、训练动态和下游效率的影响却鲜有关注。本文基于BackSlash算法，提出了一个统一的端到端框架，通过GG模型优化LLM。主要贡献包括：1）基于GG的初始化方案；2）DeepShape方法；3）RF8格式。实验表明，该框架能产生更小更快且性能不减的模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM参数的统计分布及其影响关注较少，而BackSlash算法展示了预训练LLM参数更符合广义高斯分布（GGD），启发了进一步优化的可能性。

Method: 提出一个基于GG模型的统一端到端框架，包含：1）与训练模型统计结构一致的GG初始化方案；2）DeepShape方法调整权重分布以提高压缩性；3）RF8浮点格式支持高效训练和推理。

Result: 实验结果表明，该框架在不同模型架构上均能生成更小、更快的模型，并且性能匹配或超过标准训练基线。

Conclusion: 本文通过将LLM发展建立在严谨的统计建模基础上，为高效、可扩展和硬件感知的AI系统开辟了新方向。

Abstract: Despite rapid advancements in the research and deployment of large language
models (LLMs), the statistical distribution of model parameters, as well as
their influence on initialization, training dynamics, and downstream
efficiency, has received surprisingly little attention. A recent work
introduced BackSlash, a training-time compression algorithm. It first
demonstrated that pre-trained LLM parameters follow generalized Gaussian
distributions (GGDs) better. By optimizing GG priors during training, BackSlash
can reduce parameters by up to 90\% with minimal performance loss. Building on
this foundational insight, we propose a unified, end-to-end framework for LLM
optimization based on the GG model. Our contributions are threefold: (1)
GG-based initialization scheme that aligns with the statistical structure of
trained models, resulting in faster convergence and improved accuracy; (2)
DeepShape, a post-training regularization method that reshapes weight
distributions to match a GG profile, improving compressibility with minimized
degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit
floating-point format designed for GG-distributed-initialized BackSlash
training, enabling low-cost inference without compromising accuracy.
Experiments across diverse model architectures show that our framework
consistently yields smaller and faster models that match or outperform standard
training baselines. By grounding LLM development in principled statistical
modeling, this work forges a new path toward efficient, scalable, and
hardware-aware AI systems. The code is available on our project page:
https://huggingface.co/spaces/shifeng3711/gg_prior.

</details>


### [147] [FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts](https://arxiv.org/abs/2506.00495)
*Xinyi Wang,Lirong Gao,Haobo Wang,Yiming Zhang,Junbo Zhao*

Main category: cs.LG

TL;DR: FLoE is a new PEFT framework that improves adaptation efficiency by using Fisher information to identify task-critical layers and Bayesian optimization to allocate LoRA ranks.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT techniques uniformly deploy LoRA adapters across all layers, ignoring layer heterogeneity and task-specific rank requirements which leads to redundant parameter allocation.

Method: The method involves two innovations: (i) Fisher information-guided importance scoring for identifying task-critical transformer layers enabling sparse adapter deployment; and (ii) Bayesian optimization-driven rank allocator for determining optimal LoRA ranks on specific datasets.

Result: Extensive experiments show FLoE achieves impressive efficiency-accuracy trade-offs, advantageous in resource-constrained environments.

Conclusion: FLoE presents a novel approach to PEFT that addresses limitations of current methods through dynamic identification of critical layers and automatic rank allocation.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a widely
adopted strategy for adapting pre-trained Large Language Models (LLMs) to
downstream tasks, significantly reducing memory and computational costs.
However, most existing PEFT techniques uniformly deploy LoRA adapters across
all layers, disregarding the intrinsic heterogeneity of layer contributions and
task-specific rank requirements. This uniform paradigm leads to redundant
parameter allocation and suboptimal adaptation efficiency. To address these
limitations, we propose FLoE, a novel PEFT framework that introduces two key
innovations: (i) a Fisher information-guided importance scoring mechanism to
dynamically identify task-critical transformer layers for MoE-based low-rank
adaptation, enabling sparse adapter deployment; and (ii) a Bayesian
optimization-driven rank allocator that automatically determines optimal LoRA
ranks on specific datasets without exhaustive grid search. Extensive
experiments across diverse LLMs and benchmarks reveal that FLoE achieves
impressive efficiency-accuracy trade-offs, making FLoE particularly
advantageous in resource-constrained environments that necessitate rapid
adaptation.

</details>


### [148] [Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study](https://arxiv.org/abs/2506.00499)
*Diogo Landau,Ingeborg de Pater,Mihaela Mitici,Nishant Saurabh*

Main category: cs.LG

TL;DR: 本研究提出了一种基于联邦学习的协作框架，用于在不共享数据的情况下训练飞机发动机剩余使用寿命（RUL）预测模型，并通过四种新方法提高对噪声数据的鲁棒性。实验结果表明，与独立建模相比，联邦学习提高了五个航空公司的RUL预测准确性。


<details>
  <summary>Details</summary>
Motivation: 开发预测性维护中的RUL预测模型面临数据样本不足的问题，而由于隐私问题，航空公司不愿共享数据。为解决这一挑战，本文提出了一种无需集中共享数据的联邦学习框架。

Method: 1. 提出一种协作联邦学习框架，多个航空公司合作训练集体RUL预测模型。
2. 设计去中心化的验证程序以验证预测模型。
3. 提出四种新的参数聚合方法，增强联邦学习框架对噪声数据的鲁棒性。

Result: 实验结果表明：
1. 联邦学习框架相较于各航空公司独立建模，提高了五个航空公司的RUL预测精度。
2. 新提出的鲁棒聚合方法有效提升了模型对噪声数据的适应能力。

Conclusion: 所提出的联邦学习框架和鲁棒聚合方法能够有效提升RUL预测模型的准确性和鲁棒性，同时保护了数据隐私，为多航空公司间的协作提供了可行方案。

Abstract: Complex systems such as aircraft engines are continuously monitored by
sensors. In predictive aircraft maintenance, the collected sensor measurements
are used to estimate the health condition and the Remaining Useful Life (RUL)
of such systems. However, a major challenge when developing prognostics is the
limited number of run-to-failure data samples. This challenge could be overcome
if multiple airlines would share their run-to-failure data samples such that
sufficient learning can be achieved. Due to privacy concerns, however, airlines
are reluctant to share their data in a centralized setting. In this paper, a
collaborative federated learning framework is therefore developed instead.
Here, several airlines cooperate to train a collective RUL prognostic machine
learning model, without the need to centrally share their data. For this, a
decentralized validation procedure is proposed to validate the prognostics
model without sharing any data. Moreover, sensor data is often noisy and of low
quality. This paper therefore proposes four novel methods to aggregate the
parameters of the global prognostic model. These methods enhance the robustness
of the FL framework against noisy data. The proposed framework is illustrated
for training a collaborative RUL prognostic model for aircraft engines, using
the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in
the FL framework to train a collective RUL prognostic model for their
aircraft's engines. When comparing the proposed FL framework with the case
where each airline independently develops their own prognostic model, the
results show that FL leads to more accurate RUL prognostics for five out of the
six airlines. Moreover, the novel robust aggregation methods render the FL
framework robust to noisy data samples.

</details>


### [149] [Unlearning Inversion Attacks for Graph Neural Networks](https://arxiv.org/abs/2506.00808)
*Jiahao Zhang,Yilong Wang,Zhiwei Zhang,Xiaorui Liu,Suhang Wang*

Main category: cs.LG

TL;DR: The paper introduces the graph unlearning inversion attack which challenges the assumption that deleted information from GNNs cannot be recovered, and proposes TrendAttack to reconstruct removed edges.


<details>
  <summary>Details</summary>
Motivation: Graph unlearning methods aim to remove the impact of sensitive data from trained GNNs without full retraining, but it is assumed that deleted information cannot be recovered. This work challenges this assumption.

Method: TrendAttack is introduced, which exploits the confidence pitfall and designs an adaptive prediction mechanism to address the challenges of varying probability-similarity thresholds and locating unlearned edge endpoints.

Result: Experiments on four real-world datasets show that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines in reconstructing removed edges.

Conclusion: Current graph unlearning methods have a critical privacy vulnerability as they are susceptible to the graph unlearning inversion attack.

Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive
data from trained GNNs without full retraining, assuming that deleted
information cannot be recovered. In this work, we challenge this assumption by
introducing the graph unlearning inversion attack: given only black-box access
to an unlearned GNN and partial graph knowledge, can an adversary reconstruct
the removed edges? We identify two key challenges: varying
probability-similarity thresholds for unlearned versus retained edges, and the
difficulty of locating unlearned edge endpoints, and address them with
TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical
and empirical pattern showing that nodes adjacent to unlearned edges exhibit a
large drop in model confidence. Second, we design an adaptive prediction
mechanism that applies different similarity thresholds to unlearned and other
membership edges. Our framework flexibly integrates existing membership
inference techniques and extends them with trend features. Experiments on four
real-world datasets demonstrate that TrendAttack significantly outperforms
state-of-the-art GNN membership inference baselines, exposing a critical
privacy vulnerability in current graph unlearning methods.

</details>


### [150] [From Rules to Rewards: Reinforcement Learning for Interest Rate Adjustment in DeFi Lending](https://arxiv.org/abs/2506.00505)
*Hanxiao Qu,Krzysztof Gogol,Florian Groetschla,Claudio Tessone*

Main category: cs.LG

TL;DR: This paper addresses the challenges in DeFi lending protocols by applying Offline Reinforcement Learning (RL) to optimize interest rate adjustments. The study evaluates three RL methods using Aave historical data, with TD3-BC showing the best performance in balancing utilization, capital stability, and risk.


<details>
  <summary>Details</summary>
Motivation: DeFi lending faces issues in optimizing interest rates, mitigating bad debt, and improving capital efficiency. Traditional rule-based models cannot adapt well to dynamic market conditions.

Method: The paper employs Offline Reinforcement Learning techniques, specifically Conservative Q-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning (TD3-BC), on historical data from the Aave protocol to adjust interest rates optimally.

Result: TD3-BC outperforms the other two RL approaches as well as existing models in balancing utilization, capital stability, and risk. It also adapts effectively to historical stress events.

Conclusion: Offline RL, particularly TD3-BC, shows potential for automated, real-time governance in DeFi lending protocols, offering improvements over current rule-based systems.

Abstract: Decentralized Finance (DeFi) lending enables permissionless borrowing via
smart contracts. However, it faces challenges in optimizing interest rates,
mitigating bad debt, and improving capital efficiency. Rule-based interest-rate
models struggle to adapt to dynamic market conditions, leading to
inefficiencies. This work applies Offline Reinforcement Learning (RL) to
optimize interest rate adjustments in DeFi lending protocols. Using historical
data from Aave protocol, we evaluate three RL approaches: Conservative
Q-Learning (CQL), Behavior Cloning (BC), and TD3 with Behavior Cloning
(TD3-BC). TD3-BC demonstrates superior performance in balancing utilization,
capital stability, and risk, outperforming existing models. It adapts
effectively to historical stress events like the May 2021 crash and the March
2023 USDC depeg, showcasing potential for automated, real-time governance.

</details>


### [151] [Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings](https://arxiv.org/abs/2506.00528)
*Richard Connor,Alan Dearle,Ben Claydon*

Main category: cs.LG

TL;DR: The paper proposes a method to quantize high-dimensional embeddings into vectors with elements from {-1,0,1}, achieving significant space and computational savings while preserving similarity measurements.


<details>
  <summary>Details</summary>
Motivation: Modern search domains involve high-dimensional embeddings that are computationally expensive in terms of storage and comparison speed.

Method: Utilizing convex polytopes in high-dimensional space to quantize floating point vectors into {-1,0,1} vectors.

Result: This method provides substantial savings in space and metric evaluation cost, while maintaining strong correlation for similarity measurements.

Conclusion: Radical quantization using convex polytopes can effectively approximate embedding spaces with significant efficiency improvements.

Abstract: Many modern search domains comprise high-dimensional vectors of floating
point numbers derived from neural networks, in the form of embeddings. Typical
embeddings range in size from hundreds to thousands of dimensions, making the
size of the embeddings, and the speed of comparison, a significant issue.
  Quantisation is a class of mechanism which replaces the floating point values
with a smaller representation, for example a short integer. This gives an
approximation of the embedding space in return for a smaller data
representation and a faster comparison function.
  Here we take this idea almost to its extreme: we show how vectors of
arbitrary-precision floating point values can be replaced by vectors whose
elements are drawn from the set {-1,0,1}. This yields very significant savings
in space and metric evaluation cost, while maintaining a strong correlation for
similarity measurements.
  This is achieved by way of a class of convex polytopes which exist in the
high-dimensional space. In this article we give an outline description of these
objects, and show how they can be used for the basis of such radical
quantisation while maintaining a surprising degree of accuracy.

</details>


### [152] [M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model](https://arxiv.org/abs/2506.00531)
*Hang Fana,Mingxuan Lib,Zuhan Zhanga,Long Chengc,Yujian Ye,Dunnan Liua*

Main category: cs.LG

TL;DR: The study presents M2WLLM, a model using Large Language Models for ultra-short-term wind power forecasting. It integrates textual and numerical data, demonstrating superior accuracy and robustness compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate ultra-short-term wind power forecasting is crucial for integrating wind energy into power grids, ensuring stability and optimizing resource allocation.

Method: M2WLLM uses a Prompt Embedder and a Data Embedder to fuse textual prompts and numerical inputs within the LLMs framework. The Semantic Augmenter translates temporal data for better comprehension by LLMs.

Result: Empirical evaluations on wind farm data from three Chinese provinces show that M2WLLM outperforms existing methods like GPT4TS across various datasets and prediction horizons.

Conclusion: M2WLLM demonstrates the potential of LLMs in enhancing accuracy and robustness for ultra-short-term wind power forecasting, highlighting their few-shot learning capabilities.

Abstract: The integration of wind energy into power grids necessitates accurate
ultra-short-term wind power forecasting to ensure grid stability and optimize
resource allocation. This study introduces M2WLLM, an innovative model that
leverages the capabilities of Large Language Models (LLMs) for predicting wind
power output at granular time intervals. M2WLLM overcomes the limitations of
traditional and deep learning methods by seamlessly integrating textual
information and temporal numerical data, significantly improving wind power
forecasting accuracy through multi-modal data. Its architecture features a
Prompt Embedder and a Data Embedder, enabling an effective fusion of textual
prompts and numerical inputs within the LLMs framework. The Semantic Augmenter
within the Data Embedder translates temporal data into a format that the LLMs
can comprehend, enabling it to extract latent features and improve prediction
accuracy. The empirical evaluations conducted on wind farm data from three
Chinese provinces demonstrate that M2WLLM consistently outperforms existing
methods, such as GPT4TS, across various datasets and prediction horizons. The
results highlight LLMs' ability to enhance accuracy and robustness in
ultra-short-term forecasting and showcase their strong few-shot learning
capabilities.

</details>


### [153] [RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable Traveling Salesman Problems](https://arxiv.org/abs/2506.00533)
*Junquan Huang,Zong-Gan Chen,Yuncheng Jiang,Zhi-Hui Zhan*

Main category: cs.LG

TL;DR: 提出了一种新的Rescaling Graph Convolutional Network (RsGCN)解决神经旅行商问题（TSP）中的两个关键挑战：可扩展TSP的泛化能力差和训练成本高。该网络通过重新缩放机制增强了泛化能力，并采用高效的训练策略和后搜索算法Re2Opt，实现了在不同规模实例上的出色表现和低训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前神经TSP求解器在处理大规模TSP时存在泛化能力差和训练成本高的问题，需要一种能够有效应对这些问题的新方法。

Method: 1. 设计了Rescaling Mechanism，通过重新缩放相邻节点和子图边来增强RsGCN的泛化能力；2. 使用混合尺度数据集和双向损失函数进行高效训练；3. 开发了基于RsGCN热图的后搜索算法Re2Opt，包含基于自适应权重的重建过程以避免局部最优。

Result: 仅需3个训练周期即可成功推广到10K节点实例，无需微调。在9种不同规模的均匀分布实例和78个TSPLIB实际案例中表现出最先进的性能，同时具有最少的可学习参数和训练周期。

Conclusion: RsGCN与Re2Opt相结合的架构显著提高了TSP求解器的泛化能力和效率，为神经TSP求解提供了新方向。

Abstract: Neural traveling salesman problem (TSP) solvers face two critical challenges:
poor generalization for scalable TSPs and high training costs. To address these
challenges, we propose a new Rescaling Graph Convolutional Network (RsGCN).
Focusing on the scale-dependent features (i.e., features varied with problem
scales) related to nodes and edges that influence the sensitivity of GCNs to
the problem scales, a Rescaling Mechanism in RsGCN enhances the generalization
capability by (1) rescaling adjacent nodes to construct a subgraph with a
uniform number of adjacent nodes for each node across various scales of TSPs,
which stabilizes the graph message aggregation; (2) rescaling subgraph edges to
adjust the lengths of subgraph edges to the same magnitude, which maintains
numerical consistency. In addition, an efficient training strategy with a
mixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit
the heatmaps generated by RsGCN, we design an efficient post-search algorithm
termed Re2Opt, in which a reconstruction process based on adaptive weight is
incorporated to help avoid local optima. Based on a combined architecture of
RsGCN and Re2Opt, our solver achieves remarkable generalization and low
training cost: with only 3 epochs of training on the mixed-scale dataset
containing instances with up to 100 nodes, it can be generalized successfully
to 10K-node instances without any fine-tuning. Extensive experiments
demonstrate our state-of-the-art performance across uniform distribution
instances of 9 different scales from 20 to 10K nodes and 78 real-world
instances from TSPLIB, while requiring the fewest learnable parameters and
training epochs among neural competitors.

</details>


### [154] [Mitigating Disparate Impact of Differentially Private Learning through Bounded Adaptive Clipping](https://arxiv.org/abs/2506.01396)
*Linzh Zhao,Aki Rehn,Mikko A. Heikkilä,Razane Tajeddine,Antti Honkela*

Main category: cs.LG

TL;DR: The paper addresses the issue of disparate impacts on model predictions in DP learning, proposes bounded adaptive clipping to prevent excessive gradient suppression and significantly improves accuracy for worst-performing classes.


<details>
  <summary>Details</summary>
Motivation: Differential privacy (DP) learning methods often have disparate impacts on model predictions, especially suppressing larger gradients from challenging samples through gradient clipping. This problem is amplified by adaptive clipping which can shrink the clipping bound to tiny values, affecting accuracy for minority groups.

Method: Propose bounded adaptive clipping, a method that introduces a tunable lower bound to adaptive clipping in order to prevent excessive suppression of gradients in differential privacy learning.

Result: Bounded adaptive clipping improves the accuracy of the worst-performing class on average over 10 percentage points on skewed MNIST and Fashion MNIST compared to unbounded adaptive clipping, and over 5 percentage points over constant clipping.

Conclusion: Bounded adaptive clipping effectively mitigates the negative impact of gradient suppression in DP learning, leading to improved accuracy for underperforming classes.

Abstract: Differential privacy (DP) has become an essential framework for
privacy-preserving machine learning. Existing DP learning methods, however,
often have disparate impacts on model predictions, e.g., for minority groups.
Gradient clipping, which is often used in DP learning, can suppress larger
gradients from challenging samples. We show that this problem is amplified by
adaptive clipping, which will often shrink the clipping bound to tiny values to
match a well-fitting majority, while significantly reducing the accuracy for
others. We propose bounded adaptive clipping, which introduces a tunable lower
bound to prevent excessive gradient suppression. Our method improves the
accuracy of the worst-performing class on average over 10 percentage points on
skewed MNIST and Fashion MNIST compared to the unbounded adaptive clipping, and
over 5 percentage points over constant clipping.

</details>


### [155] [Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach](https://arxiv.org/abs/2506.00545)
*Mehdi Bejani,Guillermo Perez-de-Arenaza-Pozo,Julián D. Arias-Londoño,Juan I. Godino-LLorente*

Main category: cs.LG

TL;DR: A novel imputation framework using Self-Attention-based Imputation networks and a custom autoencoder for handling missing data in time series, especially biomedical sequences like smooth pursuit eye movements, is proposed. It shows significant improvement in accuracy and robustness compared to other techniques.


<details>
  <summary>Details</summary>
Motivation: Missing data is a critical issue in time series analysis, particularly in biomedical sequences such as smooth pursuit eye movements which often contain gaps due to eye blinks and track losses.

Method: The method involves a Self-Attention-based Imputation network for imputing missing data in time series and a custom made autoencoder specifically tailored for smooth pursuit eye movement sequences.

Result: This approach significantly improves the accuracy of reconstructed eye movement sequences, reducing common error metrics while preserving frequency domain characteristics. It also demonstrates robustness when large intervals of data are missing.

Conclusion: This new method provides an alternative solution for handling missing data in time series, enhancing the reliability of smooth pursuit analysis for neurodegenerative disorders screening and monitoring.

Abstract: Missing data is a relevant issue in time series, especially in biomedical
sequences such as those corresponding to smooth pursuit eye movements, which
often contain gaps due to eye blinks and track losses, complicating the
analysis and extraction of meaningful biomarkers. In this paper, a novel
imputation framework is proposed using Self-Attention-based Imputation networks
for time series, which leverages the power of deep learning and self-attention
mechanisms to impute missing data. We further refine the imputed data using a
custom made autoencoder, tailored to represent smooth pursuit eye movement
sequences. The proposed approach was implemented using 5,504 sequences from 172
Parkinsonian patients and healthy controls. Results show a significant
improvement in the accuracy of reconstructed eye movement sequences with
respect to other state of the art techniques, substantially reducing the values
for common time domain error metrics such as the mean absolute error, mean
relative error, and root mean square error, while also preserving the signal's
frequency domain characteristics. Moreover, it demonstrates robustness when
large intervals of data are missing. This method offers an alternative solution
for robustly handling missing data in time series, enhancing the reliability of
smooth pursuit analysis for the screening and monitoring of neurodegenerative
disorders.

</details>


### [156] [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
*Peng Xia,Jinglu Wang,Yibo Peng,Kaide Zeng,Xian Wu,Xiangru Tang,Hongtu Zhu,Yun Li,Shujie Liu,Yan Lu,Huaxiu Yao*

Main category: cs.LG

TL;DR: The paper presents MMedAgent-RL, a reinforcement learning-based multi-agent framework for dynamic collaboration among medical agents. It features two GP agents trained via RL to enhance diagnostic performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing single-agent Medical Large Vision-Language Models (Med-LVLMs) face challenges in generalizing across diverse medical specialties. Multi-agent frameworks introduced recently have static pipelines that lack flexibility and adaptability in reasoning.

Method: The authors propose MMedAgent-RL, which involves training two GP agents (triage doctor and attending physician) using Qwen2.5-VL via reinforcement learning. The triage doctor assigns patients to appropriate specialties, while the attending physician integrates judgments from specialists and its own knowledge. Curriculum learning-guided RL is used to teach the attending physician to balance imitation and correction of specialist outputs.

Result: Experiments on five medical VQA benchmarks show that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs, demonstrating human-like reasoning patterns. It achieves an average performance gain of 18.4% over supervised fine-tuning baselines.

Conclusion: MMedAgent-RL addresses the limitations of existing models by enabling dynamic, optimized collaboration among medical agents, leading to improved performance and adaptability.

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential
in multimodal diagnostic tasks. However, existing single-agent models struggle
to generalize across diverse medical specialties, limiting their performance.
Recent efforts introduce multi-agent collaboration frameworks inspired by
clinical workflows, where general practitioners (GPs) and specialists interact
in a fixed sequence. Despite improvements, these static pipelines lack
flexibility and adaptability in reasoning. To address this, we propose
MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that
enables dynamic, optimized collaboration among medical agents. Specifically, we
train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to
assign patients to appropriate specialties, while the attending physician
integrates the judgments from multi-specialists and its own knowledge to make
final decisions. To address the inconsistency in specialist outputs, we
introduce a curriculum learning (CL)-guided RL strategy that progressively
teaches the attending physician to balance between imitating specialists and
correcting their mistakes. Experiments on five medical VQA benchmarks
demonstrate that MMedAgent-RL not only outperforms both open-source and
proprietary Med-LVLMs, but also exhibits human-like reasoning patterns.
Notably, it achieves an average performance gain of 18.4% over supervised
fine-tuning baselines.

</details>


### [157] [$IF-GUIDE$: Influence Function-Guided Detoxification of LLMs](https://arxiv.org/abs/2506.01790)
*Zachary Coalson,Juhan Bae,Nicholas Carlini,Sanghyun Hong*

Main category: cs.LG

TL;DR: 研究了训练数据如何导致大型语言模型中的毒性行为，并提出了一种名为IF-Guide的主动方法，利用影响函数识别并抑制训练数据中有害标记的影响。与现有方法相比，IF-Guide显著降低了显性和隐性毒性，且计算效率高，无需依赖人类偏好数据。


<details>
  <summary>Details</summary>
Motivation: 大多数减少模型毒性的先前工作采用的是反应性方法，例如微调预训练（可能有毒）模型以使其符合人类价值观。本文旨在探索一种主动性方法来解决这一问题。

Method: 提出了一种名为IF-Guide的方法，该方法使用影响函数识别训练数据中的有害标记并抑制其影响。具体包括：展示标准影响函数在发现有害训练记录方面的无效性；提出一种新的适应技术，用于测量训练数据到模型毒性的标记级归因；选择有毒训练文档的技术；以及可以整合到预训练和微调中的学习目标。

Result: IF-Guide显著减少了显性和隐性毒性，与未审查的模型相比最多降低10倍，与基线对齐方法（如DPO和RAD）相比最多降低3倍。此外，IF-Guide计算效率高，使用参数少7.5倍的代理模型即可有效识别有害数据。

Conclusion: IF-Guide提供了一种高效的主动性方法，可以显著减少大型语言模型的毒性，同时无需依赖人类偏好数据。

Abstract: We study how training data contributes to the emergence of toxic behaviors in
large-language models. Most prior work on reducing model toxicity adopts
$reactive$ approaches, such as fine-tuning pre-trained (and potentially toxic)
models to align them with human values. In contrast, we propose a $proactive$
approach$-$IF-Guide$-$which leverages influence functions to identify harmful
tokens within any training data and suppress their impact during training. To
this end, we first show that standard influence functions are ineffective at
discovering harmful training records. We then present a novel adaptation that
measures token-level attributions from training data to model toxicity, along
with techniques for selecting toxic training documents and a learning objective
that can be integrated into both pre-training and fine-tuning. Moreover,
IF-Guide does not rely on human-preference data, which is typically required by
existing alignment methods. In evaluation, we demonstrate that IF-Guide
substantially reduces both explicit and implicit toxicity$-$by up to 10$\times$
compared to uncensored models, and up to 3$\times$ compared to baseline
alignment methods, e.g., DPO and RAD$-$across both pre-training and fine-tuning
scenarios. IF-Guide is computationally efficient: a billion-parameter model is
$not$ $necessary$ for computing influence scores; a million-parameter
model$-$with 7.5$\times$ fewer parameters$-$can effectively serve as a proxy
for identifying harmful data.

</details>


### [158] [Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments](https://arxiv.org/abs/2506.00563)
*Ziyan Luo,Tianwei Ni,Pierre-Luc Bacon,Doina Precup,Xujie Si*

Main category: cs.LG

TL;DR: To systematically assess how metric learning works in deep RL, the authors evaluate five recent approaches unified conceptually as isometric embeddings with varying design choices. They benchmark them with baselines across multiple tasks and configurations, introduce a denoising factor for evaluation, propose an isolated metric estimation setting, and release an open-source codebase.


<details>
  <summary>Details</summary>
Motivation: Prior work has shown that approximating behavioral metrics in the observation space can lead to robustness against task-irrelevant noise. However, accurately estimating these metrics remains challenging due to various design choices that create gaps between theory and practice. Additionally, prior evaluations mainly focus on final returns, leaving the quality of learned metrics and the source of performance gains unclear.

Method: The authors evaluate five recent approaches unified conceptually as isometric embeddings with varying design choices. They benchmark these approaches with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, they introduce a denoising factor to quantify the encoder's ability to filter distractions and propose an isolated metric estimation setting where the encoder is influenced solely by the metric loss.

Result: Through extensive benchmarking, the authors provide insights into how different design choices affect the quality of learned metrics and performance gains in deep RL. The introduction of the denoising factor and isolated metric estimation setting further clarifies the role of metric learning in filtering distractions and improving performance.

Conclusion: The study provides a systematic assessment of metric learning in deep RL, highlighting the impact of various design choices on performance. The open-source codebase released by the authors aims to improve reproducibility and support future research in this area.

Abstract: A key approach to state abstraction is approximating behavioral metrics
(notably, bisimulation metrics) in the observation space and embedding these
learned distances in the representation space. While promising for robustness
to task-irrelevant noise, as shown in prior work, accurately estimating these
metrics remains challenging, requiring various design choices that create gaps
between theory and practice. Prior evaluations focus mainly on final returns,
leaving the quality of learned metrics and the source of performance gains
unclear. To systematically assess how metric learning works in deep
reinforcement learning (RL), we evaluate five recent approaches, unified
conceptually as isometric embeddings with varying design choices. We benchmark
them with baselines across 20 state-based and 14 pixel-based tasks, spanning
370 task configurations with diverse noise settings. Beyond final returns, we
introduce the evaluation of a denoising factor to quantify the encoder's
ability to filter distractions. To further isolate the effect of metric
learning, we propose and evaluate an isolated metric estimation setting, in
which the encoder is influenced solely by the metric loss. Finally, we release
an open-source, modular codebase to improve reproducibility and support future
research on metric learning in deep RL.

</details>


### [159] [Trojan Horse Hunt in Time Series Forecasting for Space Operations](https://arxiv.org/abs/2506.01849)
*Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Przemysław Biecek,Piotr Wilczyński,Agata Kaczmarek,Dawid Płudowski,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: This paper introduces a Kaggle competition focused on detecting and reconstructing triggers in poisoned satellite telemetry forecasting models, which is part of the 'Assurance for Space Domain AI Applications' project funded by the European Space Agency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address real-life AI security threats such as adversarial poisoning in continuously fine-tuned satellite telemetry forecasting models used in safety-critical space operations.

Method: Participants are tasked with developing methods to find and reconstruct triggers (trojans) in advanced satellite telemetry forecasting models. They are provided with a large public dataset, a reference model, a set of poisoned N-HiTS models, and a Jupyter notebook with training pipeline and baseline algorithm.

Result: The main result expected from this competition is the identification and reconstruction of 45 triggers injected into the training data of poisoned models, including their shape, amplitude, and duration.

Conclusion: This competition not only contributes to enhancing security in space domain AI applications but also has implications for other safety-critical applications involving advanced time series analysis.

Abstract: This competition hosted on Kaggle
(https://www.kaggle.com/competitions/trojan-horse-hunt-in-space) is the first
part of a series of follow-up competitions and hackathons related to the
"Assurance for Space Domain AI Applications" project funded by the European
Space Agency (https://assurance-ai.space-codev.org/). The competition idea is
based on one of the real-life AI security threats identified within the project
-- the adversarial poisoning of continuously fine-tuned satellite telemetry
forecasting models. The task is to develop methods for finding and
reconstructing triggers (trojans) in advanced models for satellite telemetry
forecasting used in safety-critical space operations. Participants are provided
with 1) a large public dataset of real-life multivariate satellite telemetry
(without triggers), 2) a reference model trained on the clean data, 3) a set of
poisoned neural hierarchical interpolation (N-HiTS) models for time series
forecasting trained on the dataset with injected triggers, and 4) Jupyter
notebook with the training pipeline and baseline algorithm (the latter will be
published in the last month of the competition). The main task of the
competition is to reconstruct a set of 45 triggers (i.e., short multivariate
time series segments) injected into the training data of the corresponding set
of 45 poisoned models. The exact characteristics (i.e., shape, amplitude, and
duration) of these triggers must be identified by participants. The popular
Neural Cleanse method is adopted as a baseline, but it is not designed for time
series analysis and new approaches are necessary for the task. The impact of
the competition is not limited to the space domain, but also to many other
safety-critical applications of advanced time series analysis where model
poisoning may lead to serious consequences.

</details>


### [160] [AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs](https://arxiv.org/abs/2506.00569)
*Nicholas E. Corrado,Julian Katz-Samuels,Adithya Devraj,Hyokun Yun,Chao Zhang,Yi Xu,Yi Pan,Bing Yin,Trishul Chilimbi*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在不同任务上的表现取决于训练数据的组成。选择能实现所有任务强性能的数据混合具有挑战性。本文提出AutoMixAlign (AMA)，一种理论基础算法，通过自适应混合数据集以平衡跨任务性能。AMA包含两个算法：AMA-R和AMA-S，两者在凸情况下均达到O(1/√T)的收敛率。实验表明AMA优于标准对齐方法及模型合并方法。


<details>
  <summary>Details</summary>
Motivation: 现有对齐大型语言模型的方法依赖于大规模消融研究、启发式或人类直觉，这些方法成本高昂且可能次优。因此，需要更高效和优化的方法来选择训练数据混合，以提升模型在多种任务上的表现。

Method: 研究基于DPO的偏好优化问题，并引入AutoMixAlign (AMA) 算法。AMA首先为每个任务训练专门模型以确定与强任务表现相关的损失，然后使用新颖的极小极大优化训练通用模型，优先考虑与专门模型损失偏差最大的任务。提出了两种优化算法：AMA-R（自适应调整目标权重以优先任务）和AMA-S（自适应调整从每个任务采样的数据量以优先任务）。

Result: AMA在多任务对齐设置中表现出色，超越了标准对齐方法（简单优化所有任务总损失）和模型合并方法。AMA-R和AMA-S在凸情况下均实现了O(1/√T)的收敛率。

Conclusion: AutoMixAlign (AMA) 是一种理论上严谨的算法，能够通过自适应混合数据集来平衡大型语言模型在各种任务上的性能。AMA提出的两种优化算法在实验中显示出优越性，为多任务对齐提供了有效解决方案。

Abstract: When aligning large language models (LLMs), their performance on various
tasks (such as being helpful, harmless, and honest) depends heavily on the
composition of their training data. However, selecting a data mixture that
achieves strong performance across all tasks is challenging. Existing
approaches rely on large ablation studies, heuristics, or human intuition, but
these can be prohibitively expensive and suboptimal. We study this problem in
the setting of preference optimization via DPO and introduce AutoMixAlign
(AMA), a theoretically-grounded algorithm that adaptively mixes datasets during
training to balance performance across tasks. AMA first trains
\textit{specialist models} for each task to determine losses that correspond to
strong task performance. Then, it trains a generalist model using a novel
minimax optimization that prioritizes tasks for which generalist model losses
deviate most from specialist model losses. To optimize this problem, we propose
two algorithms: (1) AMA-R, which adaptively reweights the objective to
prioritize tasks, and (2) AMA-S, which adaptively adjusts how much data is
sampled from each task to prioritize tasks. Both algorithms achieve a
convergence rate of $O(1/\sqrt{T})$ in the convex case. AMA-R's convergence
result follows from Sagawa et al. (2019), and we provide a convergence proof
for AMA-S using online learning techniques such as EXP3. We evaluate AMA on
several multitask alignment setups and find that AMA outperforms the standard
alignment approach -- which simply optimizes the total loss across all tasks --
and also outperforms model merging methods.

</details>


### [161] [Neural Estimation for Scaling Entropic Multimarginal Optimal Transport](https://arxiv.org/abs/2506.00573)
*Dor Tsur,Ziv Goldfeld,Kristjan Greenewald,Haim Permuter*

Main category: cs.LG

TL;DR: This paper proposes Neural Entropic Multimarginal Optimal Transport (NEMOT), a new framework for entropic MOT that improves scalability and computational efficiency compared to the traditional Sinkhorn algorithm.


<details>
  <summary>Details</summary>
Motivation: Multimarginal optimal transport (MOT) is a powerful tool but its applicability is bottlenecked by high computational overhead. The existing multimarginal Sinkhorn algorithm with entropic regularization has a time complexity scaling as O(n^k), which is computationally prohibitive for large datasets.

Method: The authors propose NEMOT, a computational framework for entropic MOT that uses neural networks trained on mini-batches. This approach shifts the computational complexity from the dataset size to the mini-batch size, resulting in significant speedups. They also provide non-asymptotic error bounds to guarantee the accuracy of NEMOT.

Result: NEMOT demonstrates orders-of-magnitude speedups compared to the state-of-the-art methods, allowing for a much larger number of samples and marginals to be handled efficiently. Numerical results confirm the performance gains over the Sinkhorn algorithm and show extensions to multimarginal entropic Gromov-Wasserstein alignment.

Conclusion: NEMOT offers significantly improved scalability and can be easily integrated into large-scale machine learning pipelines, expanding the practical applicability of entropic MOT for tasks involving multimarginal data.

Abstract: Multimarginal optimal transport (MOT) is a powerful framework for modeling
interactions between multiple distributions, yet its applicability is
bottlenecked by a high computational overhead. Entropic regularization provides
computational speedups via the multimarginal Sinkhorn algorithm, whose time
complexity, for a dataset size $n$ and $k$ marginals, generally scales as
$O(n^k)$. However, this dependence on the dataset size $n$ is computationally
prohibitive for many machine learning problems. In this work, we propose a new
computational framework for entropic MOT, dubbed Neural Entropic MOT (NEMOT),
that enjoys significantly improved scalability. NEMOT employs neural networks
trained using mini-batches, which transfers the computational complexity from
the dataset size to the size of the mini-batch, leading to substantial gains.
We provide formal guarantees on the accuracy of NEMOT via non-asymptotic error
bounds. We supplement these with numerical results that demonstrate the
performance gains of NEMOT over Sinkhorn's algorithm, as well as extensions to
neural computation of multimarginal entropic Gromov-Wasserstein alignment. In
particular, orders-of-magnitude speedups are observed relative to the
state-of-the-art, with a notable increase in the feasible number of samples and
marginals. NEMOT seamlessly integrates as a module in large-scale machine
learning pipelines, and can serve to expand the practical applicability of
entropic MOT for tasks involving multimarginal data.

</details>


### [162] [SMOTE-DP: Improving Privacy-Utility Tradeoff with Synthetic Data](https://arxiv.org/abs/2506.01907)
*Yan Zhou,Bradley Malin,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: 本研究提出了一种结合SMOTE与差分隐私（SMOTE-DP）的技术，用于生成合成数据。此技术在确保强隐私保护的同时，避免了显著的效用损失，并通过理论与实证验证了其在下游学习任务中的有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的隐私保护数据发布方法，如合成数据生成和数据匿名化，在隐私与效用之间存在权衡问题。尽管合成数据更擅长平衡这一矛盾，但仍然面临挑战，例如可能泄露源数据中的异常值信息，或因引入噪声导致效用显著下降。因此，需要一种新机制来同时实现强隐私保护和高数据效用。

Method: 研究者将收缩数据模式生成器（如SMOTE）与差分隐私技术相结合，提出了SMOTE-DP技术。该方法利用SMOTE的过采样能力增强差分隐私生成器，从而在生成合成数据时兼顾隐私保护和数据效用。

Result: 理论证明和实证结果表明，SMOTE-DP技术生成的合成数据能够提供强大的隐私保护，同时在下游学习任务中保持较高的效用水平，解决了传统方法效用损失大的问题。

Conclusion: 通过采用适当的合成数据生成机制（如SMOTE-DP），可以在不显著牺牲效用的情况下实现强隐私保护。这种方法为隐私保护数据发布提供了新的解决方案，适用于实际应用场景。

Abstract: Privacy-preserving data publication, including synthetic data sharing, often
experiences trade-offs between privacy and utility. Synthetic data is generally
more effective than data anonymization in balancing this trade-off, however,
not without its own challenges. Synthetic data produced by generative models
trained on source data may inadvertently reveal information about outliers.
Techniques specifically designed for preserving privacy, such as introducing
noise to satisfy differential privacy, often incur unpredictable and
significant losses in utility. In this work we show that, with the right
mechanism of synthetic data generation, we can achieve strong privacy
protection without significant utility loss. Synthetic data generators
producing contracting data patterns, such as Synthetic Minority Over-sampling
Technique (SMOTE), can enhance a differentially private data generator,
leveraging the strengths of both. We prove in theory and through empirical
demonstration that this SMOTE-DP technique can produce synthetic data that not
only ensures robust privacy protection but maintains utility in downstream
learning tasks.

</details>


### [163] [Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing](https://arxiv.org/abs/2506.00574)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 在现代无线网络中，传统的深度强化学习（DRL）难以适应动态环境。本文提出了一种结合大型语言模型（LLM）的上下文化适应方法，通过优化状态表示和强化学习目标，提高了资源分配效率和适应性。实验表明，该方法加速了收敛并优于其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络需要在动态条件下高效管理多样化服务需求，而传统DRL在处理分散和变化的反馈时面临挑战。

Method: 引入一种上下文化适应方法，将可学习提示整合到LLM增强的DRL框架中，利用ORANSight开发Prompt-Augmented Multi agent RL (PA-MRL)框架，优化语义聚类和RL目标。

Result: 实验结果表明，该方法加速了收敛，并在O-RAN切片资源分配中优于其他基线方法。

Conclusion: 通过结合提示增强学习，该方法实现了更快、更可扩展和更适应的资源分配，提升了O-RAN切片的性能。

Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently
managing diverse service demands. Traditional deep reinforcement learning (DRL)
struggles in these environments, as scattered and evolving feedback makes
optimal decision-making challenging. Large Language Models (LLMs) offer a
solution by structuring unorganized network feedback into meaningful latent
representations, helping RL agents recognize patterns more effectively. For
example, in O-RAN slicing, concepts like SNR, power levels and throughput are
semantically related, and LLMs can naturally cluster them, providing a more
interpretable state representation. To leverage this capability, we introduce a
contextualization-based adaptation method that integrates learnable prompts
into an LLM-augmented DRL framework. Instead of relying on full model
fine-tuning, we refine state representations through task-specific prompts that
dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained
on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL)
framework. Learnable prompts optimize both semantic clustering and RL
objectives, allowing RL agents to achieve higher rewards in fewer iterations
and adapt more efficiently. By incorporating prompt-augmented learning, our
approach enables faster, more scalable, and adaptive resource allocation in
O-RAN slicing. Experimental results show that it accelerates convergence and
outperforms other baselines.

</details>


### [164] [ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing](https://arxiv.org/abs/2506.00576)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.LG

TL;DR: The paper proposes ORAN-GUIDE, a dual-LLM framework enhancing multi-agent RL with semantically enriched state representations for O-RAN architecture.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of deep reinforcement learning in processing raw, unstructured input and improving policy generalization and decision efficiency in dynamic wireless network environments.

Method: ORAN-GUIDE employs a domain-specific language model pretrained on O-RAN data to generate structured prompts. These prompts are combined with learnable tokens and passed to a GPT-based encoder, providing high-level semantic representations for DRL agents.

Result: Experimental results demonstrate that ORAN-GUIDE improves sample efficiency, policy convergence, and performance generalization compared to standard MARL and single-LLM baselines.

Conclusion: ORAN-GUIDE offers a promising approach to enhance intelligent control via machine learning in O-RAN architecture.

Abstract: Advanced wireless networks must support highly dynamic and heterogeneous
service demands. Open Radio Access Network (O-RAN) architecture enables this
flexibility by adopting modular, disaggregated components, such as the RAN
Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU),
that can support intelligent control via machine learning (ML). While deep
reinforcement learning (DRL) is a powerful tool for managing dynamic resource
allocation and slicing, it often struggles to process raw, unstructured input
like RF features, QoS metrics, and traffic trends. These limitations hinder
policy generalization and decision efficiency in partially observable and
evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a
dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant,
semantically enriched state representations. The architecture employs a
domain-specific language model, ORANSight, pretrained on O-RAN control and
configuration data, to generate structured, context-aware prompts. These
prompts are fused with learnable tokens and passed to a frozen GPT-based
encoder that outputs high-level semantic representations for DRL agents. This
design adopts a retrieval-augmented generation (RAG) style pipeline tailored
for technical decision-making in wireless systems. Experimental results show
that ORAN-GUIDE improves sample efficiency, policy convergence, and performance
generalization over standard MARL and single-LLM baselines.

</details>


### [165] [Slow Feature Analysis as Variational Inference Objective](https://arxiv.org/abs/2506.00580)
*Merlin Schüler,Laurenz Wiskott*

Main category: cs.LG

TL;DR: This paper introduces a novel probabilistic interpretation of Slow Feature Analysis (SFA) using variational inference, relaxing linearity constraints and linking the slowness objective to a reconstruction loss.


<details>
  <summary>Details</summary>
Motivation: To provide a new probabilistic perspective on SFA that relaxes the linearity constraint present in previous Gaussian state-space models.

Method: The method involves variational inference to reinterpret the classical slowness objective as a regularizer to a reconstruction loss, thus connecting SFA with non-linear approaches.

Result: The slowness objective is successfully cast within a variational framework, providing insights into its role alongside the reconstruction loss in ensuring informativeness.

Conclusion: The authors discuss potential future research directions based on this new interpretation.

Abstract: This work presents a novel probabilistic interpretation of Slow Feature
Analysis (SFA) through the lens of variational inference. Unlike prior
formulations that recover linear SFA from Gaussian state-space models with
linear emissions, this approach relaxes the key constraint of linearity. While
it does not lead to full equivalence to non-linear SFA, it recasts the
classical slowness objective in a variational framework. Specifically, it
allows the slowness objective to be interpreted as a regularizer to a
reconstruction loss. Furthermore, we provide arguments, why -- from the
perspective of slowness optimization -- the reconstruction loss takes on the
role of the constraints that ensure informativeness in SFA. We conclude with a
discussion of potential new research directions.

</details>


### [166] [Decoding the Stressed Brain with Geometric Machine Learning](https://arxiv.org/abs/2506.00587)
*Sonia Koszut,Sam Nallaperuma-Herzberg,Pietro Lio*

Main category: cs.LG

TL;DR: A novel framework using geometric machine learning on raw EEG recordings for stress detection is introduced, showing superior performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Stress significantly contributes to mental and physical disorders, but traditional self-reported questionnaires are subjective. There's a need for more objective stress detection methods.

Method: Construct graphs by integrating structural connectivity (from electrode spatial arrangement) with functional connectivity (pairwise signal correlations). Use spatio-temporal graph convolutional network (ST-GCN) to process these graphs and capture spatial and temporal dynamics.

Result: Experiments on SAM-40 dataset show ST-GCN outperforms standard machine learning models on key classification metrics and enhances interpretability via ablation analyses.

Conclusion: This novel approach paves the way for more objective and accurate stress detection methods.

Abstract: Stress significantly contributes to both mental and physical disorders, yet
traditional self-reported questionnaires are inherently subjective. In this
study, we introduce a novel framework that employs geometric machine learning
to detect stress from raw EEG recordings. Our approach constructs graphs by
integrating structural connectivity (derived from electrode spatial
arrangement) with functional connectivity from pairwise signal correlations. A
spatio-temporal graph convolutional network (ST-GCN) processes these graphs to
capture spatial and temporal dynamics. Experiments on the SAM-40 dataset show
that the ST-GCN outperforms standard machine learning models on all key
classification metrics and enhances interpretability, explored through ablation
analyses of key channels and brain regions. These results pave the way for more
objective and accurate stress detection methods.

</details>


### [167] [Temporal Chunking Enhances Recognition of Implicit Sequential Patterns](https://arxiv.org/abs/2506.00588)
*Jayanta Dey,Nicholas Soures,Miranda Gonzales,Itamar Lerner,Christopher Kanan,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: In this pilot study, researchers propose a neuro-inspired approach to compress temporal sequences into context-tagged chunks, which can significantly enhance learning efficiency under resource constrained settings.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to overcome the limitations of traditional neural network based sequence learners, such as RNNs, when facing temporal patterns on multiple timescales.

Method: The method proposed in this paper involves compressing temporal sequences into context-tagged chunks during an offline sleep phase. Each tag represents a recurring structural unit or ``community'' in the sequence.

Result: Preliminary results suggest that temporal chunking can significantly enhance learning efficiency under resource constrained settings.

Conclusion: This work serves as an early proof-of-concept for the idea of structural abstraction and offers potential for future applications in transfer learning.

Abstract: In this pilot study, we propose a neuro-inspired approach that compresses
temporal sequences into context-tagged chunks, where each tag represents a
recurring structural unit or``community'' in the sequence. These tags are
generated during an offline sleep phase and serve as compact references to past
experience, allowing the learner to incorporate information beyond its
immediate input range. We evaluate this idea in a controlled synthetic
environment designed to reveal the limitations of traditional neural network
based sequence learners, such as recurrent neural networks (RNNs), when facing
temporal patterns on multiple timescales. We evaluate this idea in a controlled
synthetic environment designed to reveal the limitations of traditional neural
network based sequence learners, such as recurrent neural networks (RNNs), when
facing temporal patterns on multiple timescales. Our results, while
preliminary, suggest that temporal chunking can significantly enhance learning
efficiency under resource constrained settings. A small-scale human pilot study
using a Serial Reaction Time Task further motivates the idea of structural
abstraction. Although limited to synthetic tasks, this work serves as an early
proof-of-concept, with initial evidence that learned context tags can transfer
across related task, offering potential for future applications in transfer
learning.

</details>


### [168] [Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn](https://arxiv.org/abs/2506.00592)
*Hongyao Tang,Johan Obando-Ceron,Pablo Samuel Castro,Aaron Courville,Glen Berseth*

Main category: cs.LG

TL;DR: 研究了深度持续强化学习中的塑性损失问题，提出通过减少churn（由小批量训练引起的网络输出可变性）可以防止秩崩溃并自适应调整常规RL梯度步长，并引入C-CHAIN方法提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 塑性对于持续学习至关重要，而当前在深度持续强化学习中存在塑性损失的问题，需要深入理解其原因及解决办法。

Method: 从churn（小批量训练引起的数据外输出可变性）的角度分析塑性损失，发现其与神经切线核(NTK)矩阵的秩逐渐下降相关；提出减少churn有助于防止秩崩溃并自适应调整RL梯度步长，并引入C-CHAIN方法。

Result: C-CHAIN方法在OpenAI Gym Control、ProcGen、DeepMind Control Suite和MinAtar基准的多种持续学习环境中表现出色，优于基线方法。

Conclusion: 减少churn能有效改善持续学习中的塑性损失问题，提出的C-CHAIN方法为提升学习性能提供了新途径。

Abstract: Plasticity, or the ability of an agent to adapt to new tasks, environments,
or distributions, is crucial for continual learning. In this paper, we study
the loss of plasticity in deep continual RL from the lens of churn: network
output variability for out-of-batch data induced by mini-batch training. We
demonstrate that (1) the loss of plasticity is accompanied by the exacerbation
of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK)
matrix; (2) reducing churn helps prevent rank collapse and adjusts the step
size of regular RL gradients adaptively. Moreover, we introduce Continual Churn
Approximated Reduction (C-CHAIN) and demonstrate it improves learning
performance and outperforms baselines in a diverse range of continual learning
environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and
MinAtar benchmarks.

</details>


### [169] [Graph Evidential Learning for Anomaly Detection](https://arxiv.org/abs/2506.00594)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Yunhai Wang,Yueguo Chen,Bing Bai,Fei Wang*

Main category: cs.LG

TL;DR: Graph Evidential Learning (GEL) is proposed to improve graph anomaly detection by quantifying uncertainties in node features and graph topology, leading to state-of-the-art performance and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the limitations of current graph autoencoders (GAEs) which rely solely on reconstruction errors for anomaly detection, making them sensitive to noise and prone to overfitting.

Method: GEL redefines the reconstruction process through evidential learning, modeling node features and graph topology using evidential distributions to quantify two types of uncertainty - graph uncertainty and reconstruction uncertainty - and incorporating them into the anomaly scoring mechanism.

Result: Extensive experiments show that GEL achieves state-of-the-art performance with high robustness against noise and structural perturbations.

Conclusion: GEL addresses the issues of sensitivity to noise and overfitting in GAEs by integrating uncertainties into anomaly scoring, achieving superior performance.

Abstract: Graph anomaly detection faces significant challenges due to the scarcity of
reliable anomaly-labeled datasets, driving the development of unsupervised
methods. Graph autoencoders (GAEs) have emerged as a dominant approach by
reconstructing graph structures and node features while deriving anomaly scores
from reconstruction errors. However, relying solely on reconstruction error for
anomaly detection has limitations, as it increases the sensitivity to noise and
overfitting. To address these issues, we propose Graph Evidential Learning
(GEL), a probabilistic framework that redefines the reconstruction process
through evidential learning. By modeling node features and graph topology using
evidential distributions, GEL quantifies two types of uncertainty: graph
uncertainty and reconstruction uncertainty, incorporating them into the anomaly
scoring mechanism. Extensive experiments demonstrate that GEL achieves
state-of-the-art performance while maintaining high robustness against noise
and structural perturbations.

</details>


### [170] [Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data](https://arxiv.org/abs/2506.00614)
*Ziqi Liu,Pei Zeng,Yi Ding*

Main category: cs.LG

TL;DR: The paper proposes a predictability-aware compression-decompression framework using a circular periodicity key matrix to improve multichannel time series prediction efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-world multichannel time series prediction requires efficient solutions in edge and cloud environments, necessitating channel compression techniques that maintain prediction accuracy.

Method: A predictability-aware compression-decompression framework is developed, utilizing a circular periodicity key matrix with orthogonality for capturing time series predictability during compression and mitigating reconstruction errors during decompression.

Result: Theoretical and empirical analyses confirm the framework's time-efficiency and scalability. Experiments across six datasets show superior overall performance in balancing prediction accuracy and runtime, while maintaining compatibility with diverse predictors.

Conclusion: The proposed framework successfully reduces runtime and communication costs while preserving prediction accuracy, offering an effective solution for multichannel time series prediction.

Abstract: Real-world multichannel time series prediction faces growing demands for
efficiency across edge and cloud environments, making channel compression a
timely and essential problem. Motivated by success of Multiple-Input
Multiple-Output (MIMO) methods, we propose a predictability-aware
compression-decompression framework to reduce runtime, lower communication
cost, and maintain prediction accuracy across diverse predictors. The core idea
involves using a circular periodicity key matrix with orthogonality to capture
underlying time series predictability during compression and to mitigate
reconstruction errors during decompression by relaxing oversimplified data
assumptions. Theoretical and empirical analyses show that the proposed
framework is both time-efficient and scalable under a large number of channels.
Extensive experiments on six datasets across various predictors demonstrate
that the proposed method achieves superior overall performance by jointly
considering prediction accuracy and runtime, while maintaining strong
compatibility with diverse predictors.

</details>


### [171] [Model Reprogramming Demystified: A Neural Tangent Kernel Perspective](https://arxiv.org/abs/2506.00620)
*Ming-Yu Chung,Jiashuo Fan,Hancheng Ye,Qinsi Wang,Wei-Chen Shen,Chia-Mu Yu,Pin-Yu Chen,Sy-Yen Kuo*

Main category: cs.LG

TL;DR: The paper provides a comprehensive theoretical analysis of Model Reprogramming (MR) using the Neural Tangent Kernel (NTK) framework, highlighting the importance of the NTK matrix's eigenvalue spectrum and the source model's effectiveness.


<details>
  <summary>Details</summary>
Motivation: Model Reprogramming is successful in adapting large pre-trained models to new tasks with minimal resources, but its theoretical foundations have not been thoroughly explored.

Method: The authors conduct a theoretical analysis through the lens of the Neural Tangent Kernel (NTK) framework, focusing on the eigenvalue spectrum of the NTK matrix and the relationship between source and target models.

Result: They establish that MR's success depends on the eigenvalue spectrum of the NTK matrix and the effectiveness of the source model, supported by extensive experiments.

Conclusion: This work offers a novel theoretical framework for understanding MR and provides valuable insights into the dynamics between source and target models.

Abstract: Model Reprogramming (MR) is a resource-efficient framework that adapts large
pre-trained models to new tasks with minimal additional parameters and data,
offering a promising solution to the challenges of training large models for
diverse tasks. Despite its empirical success across various domains such as
computer vision and time-series forecasting, the theoretical foundations of MR
remain underexplored. In this paper, we present a comprehensive theoretical
analysis of MR through the lens of the Neural Tangent Kernel (NTK) framework.
We demonstrate that the success of MR is governed by the eigenvalue spectrum of
the NTK matrix on the target dataset and establish the critical role of the
source model's effectiveness in determining reprogramming outcomes. Our
contributions include a novel theoretical framework for MR, insights into the
relationship between source and target models, and extensive experiments
validating our findings.

</details>


### [172] [Probabilistic Forecasting for Building Energy Systems using Time-Series Foundation Models](https://arxiv.org/abs/2506.00630)
*Young Jin Park,Francois Germain,Jing Liu,Ye Wang,Toshiaki Koike-Akino,Gordon Wichern,Navid Azizan,Christopher R. Laughman,Ankush Chakrabarty*

Main category: cs.LG

TL;DR: This paper explores the use of time-series foundation models (TSFMs) in building energy forecasting, showing that fine-tuning methods, especially low-rank adaptation (LoRA), significantly improve forecasting accuracy and outperform state-of-the-art models while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Building energy systems' decision-making heavily relies on the predictive accuracy of time-series models. In data-scarce scenarios, leveraging prior knowledge from pre-training datasets through foundation models (FMs) can enhance predictive performance.

Method: The study examines both full fine-tuning and parameter-efficient fine-tuning (specifically low-rank adaptation - LoRA) of TSFMs using real-world data from a commercial net-zero energy building covering various signals like occupancy, carbon emissions, etc.

Result: Zero-shot predictive performance of TSFMs is suboptimal. However, fine-tuning with either method improves accuracy, with LoRA also reducing computational costs. Fine-tuned TSFMs surpass other advanced models in accuracy, robustness, and generalization.

Conclusion: TSFMs, when appropriately fine-tuned, offer practical solutions for building energy management systems, promoting energy efficiency and sustainability.

Abstract: Decision-making in building energy systems critically depends on the
predictive accuracy of relevant time-series models. In scenarios lacking
extensive data from a target building, foundation models (FMs) represent a
promising technology that can leverage prior knowledge from vast and diverse
pre-training datasets to construct accurate probabilistic predictors for use in
decision-making tools. This paper investigates the applicability and
fine-tuning strategies of time-series foundation models (TSFMs) in building
energy forecasting. We analyze both full fine-tuning and parameter-efficient
fine-tuning approaches, particularly low-rank adaptation (LoRA), by using
real-world data from a commercial net-zero energy building to capture signals
such as room occupancy, carbon emissions, plug loads, and HVAC energy
consumption. Our analysis reveals that the zero-shot predictive performance of
TSFMs is generally suboptimal. To address this shortcoming, we demonstrate that
employing either full fine-tuning or parameter-efficient fine-tuning
significantly enhances forecasting accuracy, even with limited historical data.
Notably, fine-tuning with low-rank adaptation (LoRA) substantially reduces
computational costs without sacrificing accuracy. Furthermore, fine-tuned TSFMs
consistently outperform state-of-the-art deep forecasting models (e.g.,
temporal fusion transformers) in accuracy, robustness, and generalization
across varying building zones and seasonal conditions. These results underline
the efficacy of TSFMs for practical, data-constrained building energy
management systems, enabling improved decision-making in pursuit of energy
efficiency and sustainability.

</details>


### [173] [Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting](https://arxiv.org/abs/2506.00635)
*Wei Chen,Yuxuan Liang*

Main category: cs.LG

TL;DR: The paper presents ST-TTC, a new test-time computing paradigm for spatio-temporal forecasting that uses calibration to correct prediction biases, showing effectiveness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Spatio-temporal forecasting is essential in many fields but faces challenges like signal anomalies, noise, and distributional shifts. Current robustness-enhancing methods are computationally expensive.

Method: ST-TTC introduces a spectral-domain calibrator with phase-amplitude modulation to reduce periodic shifts and a flash updating mechanism with a streaming memory queue for efficient computation during testing.

Result: Experiments on real-world datasets confirm the method's effectiveness, universality, flexibility, and efficiency.

Conclusion: ST-TTC offers an efficient and generalizable approach to spatio-temporal forecasting by addressing non-stationarity through calibration.

Abstract: Spatio-temporal forecasting is crucial in many domains, such as
transportation, meteorology, and energy. However, real-world scenarios
frequently present challenges such as signal anomalies, noise, and
distributional shifts. Existing solutions primarily enhance robustness by
modifying network architectures or training procedures. Nevertheless, these
approaches are computationally intensive and resource-demanding, especially for
large-scale applications. In this paper, we explore a novel test-time computing
paradigm, namely learning with calibration, ST-TTC, for spatio-temporal
forecasting. Through learning with calibration, we aim to capture periodic
structural biases arising from non-stationarity during the testing phase and
perform real-time bias correction on predictions to improve accuracy.
Specifically, we first introduce a spectral-domain calibrator with
phase-amplitude modulation to mitigate periodic shift and then propose a flash
updating mechanism with a streaming memory queue for efficient test-time
computation. ST-TTC effectively bypasses complex training-stage techniques,
offering an efficient and generalizable paradigm. Extensive experiments on
real-world datasets demonstrate the effectiveness, universality, flexibility
and efficiency of our proposed method.

</details>


### [174] [Rethinking Neural-based Matrix Inversion: Why can't, and Where can](https://arxiv.org/abs/2506.00642)
*Yuliang Ji,Jian Wu,Yuanzhe Xi*

Main category: cs.LG

TL;DR: 尽管深度神经网络在科学计算任务中取得了显著成功，但在快速并行近似矩阵求逆方面仍存在挑战。本文探讨了神经网络在构建通用矩阵求逆模型方面的基本限制，并通过扩展Lipschitz函数类来改进理论方法，同时明确了神经网络有效近似矩阵求逆的具体条件。实验结果验证了理论分析的有效性。


<details>
  <summary>Details</summary>
Motivation: 矩阵求逆是许多科学计算任务中的关键步骤，但目前尚无基于神经网络的通用方法能够快速、高效地实现这一目标。

Method: 通过理论分析和扩展Lipschitz函数类，研究神经网络在矩阵求逆问题上的基本限制，并明确特定条件下神经网络可有效近似矩阵求逆的方法。

Result: 理论结果表明，神经网络在特定条件下可以有效近似矩阵求逆，且实验结果支持了这一结论。

Conclusion: 神经网络在通用矩阵求逆问题上存在基本限制，但在特定条件下仍能有效近似矩阵求逆。

Abstract: Deep neural networks have achieved substantial success across various
scientific computing tasks. A pivotal challenge within this domain is the rapid
and parallel approximation of matrix inverses, critical for numerous
applications. Despite significant progress, there currently exists no universal
neural-based method for approximating matrix inversion. This paper presents a
theoretical analysis demonstrating the fundamental limitations of neural
networks in developing a general matrix inversion model. We expand the class of
Lipschitz functions to encompass a wider array of neural network models,
thereby refining our theoretical approach. Moreover, we delineate specific
conditions under which neural networks can effectively approximate matrix
inverses. Our theoretical results are supported by experimental results from
diverse matrix datasets, exploring the efficacy of neural networks in
addressing the matrix inversion challenge.

</details>


### [175] [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/abs/2506.00653)
*Femi Bello,Anubrata Das,Fanzhi Zeng,Fangcong Yin,Leqi Liu*

Main category: cs.LG

TL;DR: The paper proposes the Linear Representation Transferability (LRT) Hypothesis, suggesting an affine transformation exists between representation spaces of different neural network models. Experiments show strong evidence that learned affine mappings can preserve steering behaviors when transferred from small to large language models.


<details>
  <summary>Details</summary>
Motivation: To explore if neural networks with similar architectures trained on similar data learn shared representations and if there is a universal set of basis features underlying these representations across different models regardless of scale.

Method: Propose and test the LRT Hypothesis by learning affine mappings between hidden states of differently sized models and evaluating if steering vectors retain semantic effects when transferred using these mappings.

Result: Found strong empirical evidence supporting the LRT Hypothesis, showing that affine mappings can preserve steering behaviors when transferred between models.

Conclusion: Representations learned by small models can guide the behavior of large models, indicating the LRT hypothesis is a promising direction for understanding representation alignment across model scales.

Abstract: It has been hypothesized that neural networks with similar architectures
trained on similar data learn shared representations relevant to the learning
task. We build on this idea by extending the conceptual framework where
representations learned across models trained on the same data can be expressed
as linear combinations of a \emph{universal} set of basis features. These basis
features underlie the learning task itself and remain consistent across models,
regardless of scale. From this framework, we propose the \textbf{Linear
Representation Transferability (LRT)} Hypothesis -- that there exists an affine
transformation between the representation spaces of different models. To test
this hypothesis, we learn affine mappings between the hidden states of models
of different sizes and evaluate whether steering vectors -- directions in
hidden state space associated with specific model behaviors -- retain their
semantic effect when transferred from small to large language models using the
learned mappings. We find strong empirical evidence that such affine mappings
can preserve steering behaviors. These findings suggest that representations
learned by small models can be used to guide the behavior of large models, and
that the LRT hypothesis may be a promising direction on understanding
representation alignment across model scales.

</details>


### [176] [Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings](https://arxiv.org/abs/2506.00656)
*Aris J. Aristorenas*

Main category: cs.LG

TL;DR: 提出了一种用于室内定位的排列不变神经网络架构，使用Wi-Fi接入点的RSSI扫描。通过Set Transformer处理输入数据，结果表明该模型能准确恢复精细的空间结构，并在多栋建筑和多楼层场景中表现出色。简单LSTM在所有任务中表现最佳，平均误差低至2.23米，而Set Transformer紧随其后，在涉及多栋建筑和多楼层的任务中优于MLP、RNN和基本注意力模型。


<details>
  <summary>Details</summary>
Motivation: 现有的室内定位方法可能无法很好地处理可变长度、稀疏且无序的Wi-Fi信号输入，因此需要一种能够有效处理此类数据的新方法。

Method: 将每次扫描建模为无序的(BSSID, RSSI)对集合，BSSID映射到学习到的嵌入并与其信号强度连接，然后通过Set Transformer进行处理，从而学习基于注意力的接入点关系表示。

Result: 实验表明，Set Transformer在所有实验中排名第二，特别是在涉及多栋建筑和多楼层的任务中，性能优于MLP、RNN和基本注意力模型。然而，在信号条件差异显著的任务中，性能有所下降。

Conclusion: 基于集合的神经模型是信号定位任务的理想选择，提供了一种处理现实世界定位任务中稀疏、无序输入的原则性方法。

Abstract: We propose a permutation-invariant neural architecture for indoor
localization using RSSI scans from Wi-Fi access points. Each scan is modeled as
an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned
embeddings and concatenated with signal strength. These are processed by a Set
Transformer, enabling the model to handle variable-length, sparse inputs while
learning attention-based representations over access point relationships. We
evaluate the model on a dataset collected across a campus environment
consisting of six buildings. Results show that the model accurately recovers
fine-grained spatial structure and maintains performance across physically
distinct domains. In our experiments, a simple LSTM consistently outperformed
all other models, achieving the lowest mean localization error across three
tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer
performed competitively, ranking second in every experiment and outperforming
the MLP, RNN, and basic attention models, particularly in scenarios involving
multiple buildings (E2) and multiple floors (E3). Performance degraded most in
E2, where signal conditions varied substantially across buildings, highlighting
the importance of architectural robustness to domain diversity. This work
demonstrates that set-based neural models are a natural fit for signal-based
localization, offering a principled approach to handling sparse, unordered
inputs in real-world positioning tasks.

</details>


### [177] [Differential Privacy for Deep Learning in Medicine](https://arxiv.org/abs/2506.00660)
*Marziyeh Mohammadi,Mohsen Vejdanihemmat,Mahshad Lotfinia,Mirabela Rusu,Daniel Truhn,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.LG

TL;DR: Differential privacy (DP) is crucial for protecting patient data in medical deep learning. A scoping review of 74 studies reveals the tradeoffs between privacy, accuracy, and fairness in DP applications, especially under strict privacy settings. Emerging approaches beyond DP-SGD are explored, but inconsistencies in reporting and gaps in fairness auditing remain.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing privacy with utility and fairness in medical deep learning as clinical models become more data-dependent.

Method: A scoping review was conducted, identifying 74 studies published up to March 2025. The analysis covered diverse data modalities, training setups, and downstream tasks, focusing on DP-SGD and alternative mechanisms in centralized and federated settings.

Result: DP can preserve performance in well-structured imaging tasks, but severe degradation occurs under strict privacy, especially in underrepresented or complex modalities. Privacy-induced performance gaps disproportionately affect demographic subgroups, with fairness impacts varying by data type and task.

Conclusion: There are key gaps in fairness auditing, standardization, and evaluation protocols. Future work should focus on equitable and clinically robust privacy-preserving DL systems in medicine.

Abstract: Differential privacy (DP) is a key technique for protecting sensitive patient
data in medical deep learning (DL). As clinical models grow more
data-dependent, balancing privacy with utility and fairness has become a
critical challenge. This scoping review synthesizes recent developments in
applying DP to medical DL, with a particular focus on DP-SGD and alternative
mechanisms across centralized and federated settings. Using a structured search
strategy, we identified 74 studies published up to March 2025. Our analysis
spans diverse data modalities, training setups, and downstream tasks, and
highlights the tradeoffs between privacy guarantees, model accuracy, and
subgroup fairness. We find that while DP-especially at strong privacy
budgets-can preserve performance in well-structured imaging tasks, severe
degradation often occurs under strict privacy, particularly in underrepresented
or complex modalities. Furthermore, privacy-induced performance gaps
disproportionately affect demographic subgroups, with fairness impacts varying
by data type and task. A small subset of studies explicitly addresses these
tradeoffs through subgroup analysis or fairness metrics, but most omit them
entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms,
generative models, and hybrid federated designs, though reporting remains
inconsistent. We conclude by outlining key gaps in fairness auditing,
standardization, and evaluation protocols, offering guidance for future work
toward equitable and clinically robust privacy-preserving DL systems in
medicine.

</details>


### [178] [SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning](https://arxiv.org/abs/2506.00676)
*Saad Hossain,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: The paper introduces SafeTuneBed, a benchmark and toolkit for unifying fine-tuning and defense evaluation of large language models (LLMs). It curates datasets, enables integration of defenses, and provides evaluators for safety and utility.


<details>
  <summary>Details</summary>
Motivation: There is a lack of standardized evaluations for parameter-efficient fine-tuning methods and safety-first defenses of LLMs, making it difficult to fairly compare safety, utility, and robustness across different methods.

Method: SafeTuneBed curates a diverse repository of fine-tuning datasets, allows for the generation of harmful-variant splits, enables integration of state-of-the-art defenses, and provides evaluators for safety and utility.

Result: By benchmarking representative defenses across varied poisoning scenarios and tasks, SafeTuneBed showcases its value in accelerating rigorous and comparable research in safe LLM fine-tuning.

Conclusion: SafeTuneBed is the first focused toolkit standardizing data, code, and metrics for safe LLM fine-tuning, ensuring end-to-end reproducibility.

Abstract: As large language models (LLMs) become ubiquitous, parameter-efficient
fine-tuning methods and safety-first defenses have proliferated rapidly.
However, the number of approaches and their recent increase have resulted in
diverse evaluations-varied datasets, metrics, and inconsistent threat
settings-making it difficult to fairly compare safety, utility, and robustness
across methods. To address this, we introduce SafeTuneBed, a benchmark and
toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a
diverse repository of multiple fine-tuning datasets spanning sentiment
analysis, question-answering, multi-step reasoning, and open-ended instruction
tasks, and allows for the generation of harmful-variant splits; (ii) enables
integration of state-of-the-art defenses, including alignment-stage
immunization, in-training safeguards, and post-tuning repair; and (iii)
provides evaluators for safety (attack success rate, refusal consistency) and
utility. Built on Python-first, dataclass-driven configs and plugins,
SafeTuneBed requires minimal additional code to specify any fine-tuning regime,
defense method, and metric suite, while ensuring end-to-end reproducibility. We
showcase its value by benchmarking representative defenses across varied
poisoning scenarios and tasks. By standardizing data, code, and metrics,
SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and
comparable research in safe LLM fine-tuning. Code is available at:
https://github.com/criticalml-uw/SafeTuneBed

</details>


### [179] [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)
*Zhili Feng,Yixuan Even Xu,Alexander Robey,Robert Kirk,Xander Davies,Yarin Gal,Avi Schwarzschild,J. Zico Kolter*

Main category: cs.LG

TL;DR: 当前机器遗忘评估方法存在缺陷，可能掩盖了真实的遗忘效果。本文提出两种改进原则：最小信息注入和下游任务感知。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘技术在大型语言模型中的应用效果受到质疑，因为被移除的知识可能容易恢复。因此需要重新审视现有的评估方法。

Method: 1. 分析现有评估方法的局限性（引入新信息、任务差异、虚假相关）。2. 提出两个未来评估原则：最小信息注入和下游任务感知。3. 通过一系列针对性实验验证这些原则的有效性。

Result: 发现现有评估方法可能同时夸大和低估遗忘的成功率，并证明遵循所提原则可以避免误导性结论。

Conclusion: 为了更准确地评估机器遗忘效果，未来应遵循最小信息注入和下游任务感知的原则进行设计和验证。

Abstract: Machine unlearning aims to remove sensitive or undesired data from large
language models. However, recent studies suggest that unlearning is often
shallow, claiming that removed knowledge can easily be recovered. In this work,
we critically examine standard unlearning evaluation practices and uncover key
limitations that shake our trust in those findings. First, we show that some
evaluations introduce substantial new information into the model, potentially
masking true unlearning performance by re-teaching the model during testing.
Second, we demonstrate that evaluation outcomes vary significantly across
tasks, undermining the generalizability of current evaluation routines.
Finally, we find that many evaluations rely on spurious correlations, making
their results difficult to trust and interpret. Taken together, these issues
suggest that current evaluation protocols may both overstate and understate
unlearning success. To address this, we propose two principles for future
unlearning evaluations: minimal information injection and downstream task
awareness. We validate these principles through a series of targeted
experiments, showing how violations of each can lead to misleading conclusions.

</details>


### [180] [Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning](https://arxiv.org/abs/2506.00691)
*Junaid Muzaffar,Ahsan Adeel,Khubaib Ahmed,Ingo Frommholz,Zeeshan Pervez,Ahsan ul Haq*

Main category: cs.LG

TL;DR: 通过非线性映射改进注意力机制，提升强化学习模型的学习效率和特征交互能力。


<details>
  <summary>Details</summary>
Motivation: 训练强化学习代理需要大量计算资源和时间，而Google Brain的Sensory Neuron虽然提升了性能但仍有优化空间。

Method: 提出一种修改后的注意力机制，使用非线性映射函数对关键向量(K)进行转换以生成新的关键向量(K')，增强注意机制的表现力。

Result: 改进后的模型在保持性能的同时加速了收敛，显著提高了学习效率。

Conclusion: 非线性注意力机制在推进强化学习算法方面具有巨大潜力。

Abstract: Training reinforcement learning (RL) agents often requires significant
computational resources and extended training times. To address this, we build
upon the foundation laid by Google Brain's Sensory Neuron, which introduced a
novel neural architecture for reinforcement learning tasks that maintained
permutation in-variance in the sensory neuron system. While the baseline model
demonstrated significant performance improvements over traditional approaches,
we identified opportunities to enhance the efficiency of the learning process
further. We propose a modified attention mechanism incorporating a non-linear
transformation of the key vectors (K) using a mapping function, resulting in a
new set of key vectors (K'). This non-linear mapping enhances the
representational capacity of the attention mechanism, allowing the model to
encode more complex feature interactions and accelerating convergence without
compromising performance. Our enhanced model demonstrates significant
improvements in learning efficiency, showcasing the potential for non-linear
attention mechanisms in advancing reinforcement learning algorithms.

</details>


### [181] [Central Path Proximal Policy Optimization](https://arxiv.org/abs/2506.00700)
*Nikola Milosevic,Johannes Müller,Nico Scherf*

Main category: cs.LG

TL;DR: Central Path Proximal Policy Optimization (C3PO) is a modified PPO method that incorporates constraints directly in the policy geometry, producing iterates close to the central path of the constrained optimization problem and delivering improved performance with tighter constraint enforcement.


<details>
  <summary>Details</summary>
Motivation: In constrained Markov decision processes, enforcing constraints during training is often thought of as decreasing the final return. However, recent findings show that incorporating constraints directly in the policy geometry can yield an optimization trajectory close to the central path of a barrier method without compromising final return.

Method: The paper introduces Central Path Proximal Policy Optimization (C3PO), a modification of PPO that produces policy iterates staying close to the central path of the constrained optimization problem.

Result: Compared to existing on-policy methods, C3PO delivers improved performance with tighter constraint enforcement.

Conclusion: Central path-guided updates offer a promising direction for constrained policy optimization.

Abstract: In constrained Markov decision processes, enforcing constraints during
training is often thought of as decreasing the final return. Recently, it was
shown that constraints can be incorporated directly in the policy geometry,
yielding an optimization trajectory close to the central path of a barrier
method, which does not compromise final return. Building on this idea, we
introduce Central Path Proximal Policy Optimization (C3PO), a simple
modification of PPO that produces policy iterates, which stay close to the
central path of the constrained optimization problem. Compared to existing
on-policy methods, C3PO delivers improved performance with tighter constraint
enforcement, suggesting that central path-guided updates offer a promising
direction for constrained policy optimization.

</details>


### [182] [Bayesian Inference of Training Dataset Membership](https://arxiv.org/abs/2506.00701)
*Yongchao Huang*

Main category: cs.LG

TL;DR: This paper proposes an efficient Bayesian inference method for membership inference that computes posterior probabilities of membership without requiring extensive model training.


<details>
  <summary>Details</summary>
Motivation: Determining whether a dataset was part of a machine learning model's training data pool can reveal privacy vulnerabilities, but traditional MIAs require access to model internals or rely on computationally intensive shadow models.

Method: Analyze post-hoc metrics such as prediction error, confidence (entropy), perturbation magnitude, and dataset statistics from a trained ML model to compute posterior probabilities of membership using a principled Bayesian inference method.

Result: Experimental results on synthetic datasets demonstrate the method's effectiveness in distinguishing member from non-member datasets. It can also detect distribution shifts.

Conclusion: The proposed Bayesian inference method is efficient, interpretable, and effective for membership inference and detecting distribution shifts.

Abstract: Determining whether a dataset was part of a machine learning model's training
data pool can reveal privacy vulnerabilities, a challenge often addressed
through membership inference attacks (MIAs). Traditional MIAs typically require
access to model internals or rely on computationally intensive shadow models.
This paper proposes an efficient, interpretable and principled Bayesian
inference method for membership inference. By analyzing post-hoc metrics such
as prediction error, confidence (entropy), perturbation magnitude, and dataset
statistics from a trained ML model, our approach computes posterior
probabilities of membership without requiring extensive model training.
Experimental results on synthetic datasets demonstrate the method's
effectiveness in distinguishing member from non-member datasets. Beyond
membership inference, this method can also detect distribution shifts, offering
a practical and interpretable alternative to existing approaches.

</details>


### [183] [RelDiff: Relational Data Generative Modeling with Graph-Based Diffusion Models](https://arxiv.org/abs/2506.00710)
*Valter Hudovernik,Minkai Xu,Juntong Shi,Lovro Šubelj,Stefano Ermon,Erik Štrumbelj,Jure Leskovec*

Main category: cs.LG

TL;DR: The paper presents RelDiff, a diffusion generative model for synthesizing complete relational databases by modeling their foreign key graph structure. It combines joint graph-conditioned diffusion process and SBM graph generator to ensure high fidelity and referential integrity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for learning generative models on relational data often fail to capture the complexity of real-world databases, reducing them to conditionally generated flat tables and imposing limiting structural assumptions.

Method: RelDiff is introduced which explicitly models the foreign key graph structure of relational databases. It uses a joint graph-conditioned diffusion process across all tables for attribute synthesis and a $2K+$SBM graph generator based on the Stochastic Block Model for structure generation.

Result: Experiments on 11 benchmark datasets show that RelDiff consistently outperforms prior methods in generating realistic and coherent synthetic relational databases.

Conclusion: RelDiff addresses the limitations of existing methods by ensuring both high fidelity and referential integrity in synthetic relational database generation.

Abstract: Real-world databases are predominantly relational, comprising multiple
interlinked tables that contain complex structural and statistical
dependencies. Learning generative models on relational data has shown great
promise in generating synthetic data and imputing missing values. However,
existing methods often struggle to capture this complexity, typically reducing
relational data to conditionally generated flat tables and imposing limiting
structural assumptions. To address these limitations, we introduce RelDiff, a
novel diffusion generative model that synthesizes complete relational databases
by explicitly modeling their foreign key graph structure. RelDiff combines a
joint graph-conditioned diffusion process across all tables for attribute
synthesis, and a $2K+$SBM graph generator based on the Stochastic Block Model
for structure generation. The decomposition of graph structure and relational
attributes ensures both high fidelity and referential integrity, both of which
are crucial aspects of synthetic relational database generation. Experiments on
11 benchmark datasets demonstrate that RelDiff consistently outperforms prior
methods in producing realistic and coherent synthetic relational databases.
Code is available at https://github.com/ValterH/RelDiff.

</details>


### [184] [QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training](https://arxiv.org/abs/2506.00711)
*Wei Dai,Peilin Chen,Chanakya Ekbote,Paul Pu Liang*

Main category: cs.LG

TL;DR: QoQ-Med-7B/32B是一种新的临床基础模型，通过Domain-aware Relative Policy Optimization (DRPO)训练方法，能够综合处理医疗图像、时间序列信号和文本报告。相比其他方法，其诊断性能平均提升了43%（以macro-F1衡量），并且可以突出显示与诊断相关的显著区域，其IoU比开源模型高10倍，同时达到OpenAI o4-mini的性能水平。为了促进可重复性和下游研究，作者开放了完整模型权重、模块化训练管道以及所有中间推理痕迹。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态语言模型主要以视觉为中心，在临床各专业领域的泛化能力不足，难以满足临床决策中对异构数据推理的需求。为弥补这一差距，需要开发一种能够跨医学图像、时间序列信号和文本报告进行联合推理的通用临床基础模型。

Method: 引入了QoQ-Med-7B/32B模型，该模型通过Domain-aware Relative Policy Optimization (DRPO)训练方法，利用领域感知的相对策略优化目标，根据领域稀有性和模态难度分层缩放标准化奖励，缓解了由于临床数据分布偏差导致的性能不平衡问题。模型在涵盖9个临床领域的261万条指令调优对上进行了训练。

Result: 与无批评家的训练方法（如GRPO）相比，DRPO训练使QoQ-Med在所有视觉领域中的诊断性能平均提升了43%（以macro-F1衡量）。此外，经过密集分割数据训练后，QoQ-Med能够突出显示与诊断相关的显著区域，其IoU比开源模型高出10倍，并达到了OpenAI o4-mini的性能水平。

Conclusion: QoQ-Med-7B/32B是首个开放的通用临床基础模型，能够跨医学图像、时间序列信号和文本报告进行联合推理。通过DRPO训练方法，显著提高了诊断性能，并且在显著区域标注方面表现出色。作者开放了模型权重、训练管道和中间推理痕迹，促进了可重复性和下游研究。

Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data,
yet existing multimodal language models (MLLMs) remain largely vision-centric
and fail to generalize across clinical specialties. To bridge this gap, we
introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model
that jointly reasons across medical images, time-series signals, and text
reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization
(DRPO), a novel reinforcement-learning objective that hierarchically scales
normalized rewards according to domain rarity and modality difficulty,
mitigating performance imbalance caused by skewed clinical data distributions.
Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains,
we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on
average across all visual domains as compared to other critic-free training
methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation
data, it is able to highlight salient regions related to the diagnosis, with an
IoU 10x higher than open models while reaching the performance of OpenAI
o4-mini. To foster reproducibility and downstream research, we release (i) the
full model weights, (ii) the modular training pipeline, and (iii) all
intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.

</details>


### [185] [Pitfalls in Evaluating Language Model Forecasters](https://arxiv.org/abs/2506.00723)
*Daniel Paleka,Shashwat Goel,Jonas Geiping,Florian Tramèr*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在预测任务中的评估存在独特挑战，包括时间泄漏和难以从评估表现外推到实际预测。需要更严谨的评估方法来准确评估LLMs的预测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管有研究声称LLMs在预测任务中达到或超过人类水平，但评估这些系统的能力存在独特挑战，因此需要谨慎对待这些结论。

Method: 识别并分析了两类问题：(1)由于多种形式的时间泄漏导致难以信任评估结果；(2)难以从评估表现外推到实际预测。通过系统分析和先前工作的具体例子展示评估缺陷的影响。

Result: 展示了评估缺陷如何引发对当前和未来性能声明的关注，并强调了需要更严格的评估方法。

Conclusion: 需要更严谨的评估方法来准确评估LLMs的预测能力。

Abstract: Large language models (LLMs) have recently been applied to forecasting tasks,
with some works claiming these systems match or exceed human performance. In
this paper, we argue that, as a community, we should be careful about such
conclusions as evaluating LLM forecasters presents unique challenges. We
identify two broad categories of issues: (1) difficulty in trusting evaluation
results due to many forms of temporal leakage, and (2) difficulty in
extrapolating from evaluation performance to real-world forecasting. Through
systematic analysis and concrete examples from prior work, we demonstrate how
evaluation flaws can raise concerns about current and future performance
claims. We argue that more rigorous evaluation methodologies are needed to
confidently assess the forecasting abilities of LLMs.

</details>


### [186] [A condensing approach to multiple shooting neural ordinary differential equation](https://arxiv.org/abs/2506.00724)
*Siddharth Prabhu,Srinivas Rangarajan,Mayuresh Kothare*

Main category: cs.LG

TL;DR: 提出了一种基于凝聚法的方法，用于在训练多射击神经常微分方程（MS-NODE）时结合射击等式约束，使用如Adam等一阶优化方法。


<details>
  <summary>Details</summary>
Motivation: 多射击法对于高振荡和长轨迹的参数估计更稳定，但在神经常微分方程中应用有限，因为难以结合一般的等式约束。

Method: 提出一种凝聚法为基础的方法，在训练MS-NODE时结合这些射击等式约束，并采用一阶优化方法（如Adam）。

Result: 该方法成功将射击等式约束结合到MS-NODE的训练中，提升了模型稳定性。

Conclusion: 所提出的凝聚法为训练MS-NODE提供了一种有效的方法，增强了神经常微分方程中多射击法的应用潜力。

Abstract: Multiple-shooting is a parameter estimation approach for ordinary
differential equations. In this approach, the trajectory is broken into small
intervals, each of which can be integrated independently. Equality constraints
are then applied to eliminate the shooting gap between the end of the previous
trajectory and the start of the next trajectory. Unlike single-shooting,
multiple-shooting is more stable, especially for highly oscillatory and long
trajectories. In the context of neural ordinary differential equations,
multiple-shooting is not widely used due to the challenge of incorporating
general equality constraints. In this work, we propose a condensing-based
approach to incorporate these shooting equality constraints while training a
multiple-shooting neural ordinary differential equation (MS-NODE) using
first-order optimization methods such as Adam.

</details>


### [187] [Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning](https://arxiv.org/abs/2506.00727)
*Javier Bisbal,Julio Sotelo,Maria I Valdés,Pablo Irarrazaval,Marcelo E Andia,Julio García,José Rodriguez-Palomarez,Francesca Raimondi,Cristián Tejos,Sergio Uribe*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep reinforcement learning (DRL) algorithms have shown robust results in
plane reformatting tasks. In these methods, an agent sequentially adjusts the
position and orientation of an initial plane towards an objective location.
This process allows accurate plane reformatting, without the need for detailed
landmarks, which makes it suitable for images with limited contrast and
resolution, such as 4D flow MRI. However, current DRL methods require the test
dataset to be in the same position and orientation as the training dataset. In
this paper, we present a novel technique that utilizes a flexible coordinate
system based on the current state, enabling navigation in volumes at any
position or orientation. We adopted the Asynchronous Advantage Actor Critic
(A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN).
Experimental results in 4D flow MRI demonstrate improved accuracy in plane
reformatting angular and distance errors (6.32 +- 4.15 {\deg} and 3.40 +- 2.75
mm), as well as statistically equivalent flow measurements determined by a
plane reformatting process done by an expert (p=0.21). The method's flexibility
and adaptability make it a promising candidate for other medical imaging
applications beyond 4D flow MRI.

</details>


### [188] [MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter](https://arxiv.org/abs/2506.00731)
*Binghang Lu,Changhong Mou,Guang Lin*

Main category: cs.LG

TL;DR: 提出了一种新的MoPINNEnKF框架，结合了NSGA-III和EnKF方法，以提高PINNs在前向和反向问题中的鲁棒性和准确性。通过测试一维粘性Burgers方程和时间分数混合扩散波动方程，证明该方法在处理噪声数据和缺失物理信息方面优于标准PINNs。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs在求解涉及偏微分方程的前向和反向问题中表现出色，但在涉及噪声观测数据和缺失物理信息的实际场景中，其性能往往受到限制，尤其是在反向问题中。

Method: 提出了一个迭代多目标PINN集合卡尔曼滤波（MoPINNEnKF）框架。使用NSGA-III作为多目标优化器生成PINNs的各种集合成员，并考虑解空间中的模型不确定性。然后在EnKF中利用这些集合成员同化噪声观测数据。EnKF的分析结果用于细化数据损失成分，以重新训练PINNs，从而迭代更新其参数。

Result: 数值结果表明，该方法在处理噪声数据和缺失物理信息方面优于标准PINNs。

Conclusion: 所提出的MoPINNEnKF框架提高了PINNs在前向和反向问题中的鲁棒性和准确性，特别是在噪声数据和缺失物理信息的情况下。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for
solving forward and inverse problems involving partial differential equations
(PDEs) by incorporating physical laws into the training process. However, the
performance of PINNs is often hindered in real-world scenarios involving noisy
observational data and missing physics, particularly in inverse problems. In
this work, we propose an iterative multi-objective PINN ensemble Kalman filter
(MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in
both forward and inverse problems by using the \textit{ensemble Kalman filter}
and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III).
Specifically, NSGA-III is used as a multi-objective optimizer that can generate
various ensemble members of PINNs along the optimal Pareto front, while
accounting the model uncertainty in the solution space. These ensemble members
are then utilized within the EnKF to assimilate noisy observational data. The
EnKF's analysis is subsequently used to refine the data loss component for
retraining the PINNs, thereby iteratively updating their parameters. The
iterative procedure generates improved solutions to the PDEs. The proposed
method is tested on two benchmark problems: the one-dimensional viscous Burgers
equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The
numerical results show it outperforms standard PINNs in handling noisy data and
missing physics.

</details>


### [189] [Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms](https://arxiv.org/abs/2506.00732)
*Caio Corro,Mathieu Lacroix,Joseph Le Roux*

Main category: cs.LG

TL;DR: A new sequence labeling model, Bregman conditional random fields (BCRF), is proposed which allows fast parallelizable inference algorithms based on iterative Bregman projections.


<details>
  <summary>Details</summary>
Motivation: Current models like standard linear-chain conditional random fields do not allow for fast parallelizable inference algorithms. The researchers aim to develop a model that can offer this advantage without sacrificing performance.

Method: The method involves proposing a novel discriminative model called Bregman conditional random fields (BCRF) that enables fast parallelizable inference algorithms based on iterative Bregman projections. The model can be learned using Fenchel-Young losses and includes an extension for learning from partial labels.

Result: Experimentally, the BCRF approach delivers comparable results to CRF while being faster. Additionally, it achieves better results in highly constrained settings compared to mean field, another parallelizable alternative.

Conclusion: Bregman conditional random fields provide a faster alternative to traditional CRFs for sequence labeling with comparable or better performance, especially in constrained settings.

Abstract: We propose a novel discriminative model for sequence labeling called Bregman
conditional random fields (BCRF). Contrary to standard linear-chain conditional
random fields, BCRF allows fast parallelizable inference algorithms based on
iterative Bregman projections. We show how such models can be learned using
Fenchel-Young losses, including extension for learning from partial labels.
Experimentally, our approach delivers comparable results to CRF while being
faster, and achieves better results in highly constrained settings compared to
mean field, another parallelizable alternative.

</details>


### [190] [Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers](https://arxiv.org/abs/2506.00744)
*Kazuki Irie,Morris Yau,Samuel J. Gershman*

Main category: cs.LG

TL;DR: The paper explores hybrid memory architectures for neural networks by combining key-value memory with dynamic synaptic memory, evaluating their performance in language modeling, algorithmic tasks, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of individual memory systems in neural networks, such as quadratic complexity in key-value memory and imprecise recall in dynamic synaptic memory.

Method: Propose and compare three methods to blend key-value memory and dynamic synaptic memory into a single system, conducting experiments on language modeling, synthetic algorithmic tasks, and reinforcement learning.

Result: Demonstrates that a well-designed hybrid memory system can overcome the limitations of its individual components, offering improved performance and new insights into neural memory system design.

Conclusion: A well-designed hybrid memory architecture can effectively combine the strengths of key-value and dynamic synaptic memory systems, providing valuable insights for future neural network designs.

Abstract: We develop hybrid memory architectures for general-purpose sequence
processing neural networks, that combine key-value memory using softmax
attention (KV-memory) with dynamic synaptic memory through fast-weight
programming (FW-memory) -- the core principles of quadratic and linear
transformers, respectively. These two memory systems have complementary but
individually limited properties: KV-memory offers precise retrieval but is
constrained by quadratic complexity in sequence length, while FW-memory
supports arbitrarily long sequences and enables more expressive computation but
sacrifices precise recall. We propose and compare three methods to blend these
two systems into a single memory system to leverage the strengths of both. We
conduct experiments on general language modeling and retrieval tasks by
training 340M- and 1.3B-parameter models from scratch, as well as on synthetic
algorithmic tasks designed to precisely illustrate the benefits of certain
hybrid methods over others. We also evaluate our hybrid memory systems on
reinforcement learning in partially observable environments. Overall, we
demonstrate how a well-designed hybrid can overcome the limitations of its
individual components, offering new insights into the design principle of
neural memory systems.

</details>


### [191] ["Who experiences large model decay and why?" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift](https://arxiv.org/abs/2506.00756)
*Harvineet Singh,Fan Xia,Alexej Gossmann,Andrew Chuang,Julian C. Hong,Jean Feng*

Main category: cs.LG

TL;DR: 机器学习模型在新环境中部署时常出现性能下降的问题，而当前的方法无法详细解释这种下降的具体原因。本文提出了一种新的框架SHIFT，可以识别出性能下降的子群体，并通过更详细的变量特定变化来解释这种下降，从而为缓解性能下降提供有针对性的措施。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在新环境中的性能下降问题并非均匀发生，部分子群体可能遭受较大性能衰减，而其他子群体则可能影响较小。现有的方法要么解释平均性能的变化，要么仅识别受影响的子群体，但缺乏对具体原因的深入分析。因此，需要一种能够明确指出哪些子群体受到影响以及如何受到影响的方法。

Method: 本文引入了子群体扫描分层推理框架（SHIFT），用于分析性能漂移。该框架首先询问是否存在因协变量或结果变化而导致性能大幅下降的子群体（Where?），如果存在，则进一步探讨是否可以通过更详细的变量特定变化来解释这种下降（How?）。

Result: 在实际实验中，SHIFT成功识别出了受性能下降影响且可解释的子群体，并提出了有效缓解性能下降的针对性措施。

Conclusion: SHIFT框架能够有效识别和解释机器学习模型在新环境中性能下降的具体原因，为设计针对受影响最大的子群体的纠正措施提供了支持。

Abstract: Machine learning (ML) models frequently experience performance degradation
when deployed in new contexts. Such degradation is rarely uniform: some
subgroups may suffer large performance decay while others may not.
Understanding where and how large differences in performance arise is critical
for designing targeted corrective actions that mitigate decay for the most
affected subgroups while minimizing any unintended effects. Current approaches
do not provide such detailed insight, as they either (i) explain how average
performance shifts arise or (ii) identify adversely affected subgroups without
insight into how this occurred. To this end, we introduce a Subgroup-scanning
Hierarchical Inference Framework for performance drifT (SHIFT). SHIFT first
asks "Is there any subgroup with unacceptably large performance decay due to
covariate/outcome shifts?" (Where?) and, if so, dives deeper to ask "Can we
explain this using more detailed variable(subset)-specific shifts?" (How?). In
real-world experiments, we find that SHIFT identifies interpretable subgroups
affected by performance decay, and suggests targeted actions that effectively
mitigate the decay.

</details>


### [192] [Learning Juntas under Markov Random Fields](https://arxiv.org/abs/2506.00764)
*Gautam Chandrasekaran,Adam Klivans*

Main category: cs.LG

TL;DR: An algorithm is developed for learning O(log n) juntas in Markov Random Fields using a two-phase method, achieving polynomial-time performance in a smoothed analysis framework.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the desire to extend the capabilities of learning algorithms for juntas beyond smoothed product distributions, as was done by Kalai and Teng, to more complex models such as Markov Random Fields with dependencies.

Method: The method involves two phases: (1) an unsupervised structure learning phase, and (2) a greedy supervised learning algorithm. This approach is applied to Markov Random Fields with randomly perturbed external fields.

Result: The result is a successful algorithm that learns O(log n) juntas in polynomial time with respect to Markov Random Fields under smoothed analysis conditions. This represents a broad generalization of prior work.

Conclusion: This paper concludes by presenting a polynomial-time algorithm for learning O(log n) juntas in Markov Random Fields within a smoothed analysis framework. It generalizes previous work and establishes that structure learning algorithms can lead to efficient supervised learning algorithms.

Abstract: We give an algorithm for learning $O(\log n)$ juntas in polynomial-time with
respect to Markov Random Fields (MRFs) in a smoothed analysis framework where
only the external field has been randomly perturbed. This is a broad
generalization of the work of Kalai and Teng, who gave an algorithm that
succeeded with respect to smoothed product distributions (i.e., MRFs whose
dependency graph has no edges). Our algorithm has two phases: (1) an
unsupervised structure learning phase and (2) a greedy supervised learning
algorithm. This is the first example where algorithms for learning the
structure of an undirected graphical model lead to provably efficient
algorithms for supervised learning.

</details>


### [193] [Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies](https://arxiv.org/abs/2506.00770)
*Sai Vamsi Alisetti,Vikas Kalagi,Sanjukta Krishnagopal*

Main category: cs.LG

TL;DR: The paper proposes InterGAT, a simplified alternative to Graph Attention Networks (GATs), which replaces masked attention with a fully learnable interaction matrix. Combined with GRU for temporal decoding (InterGAT-GRU), it improves forecasting accuracy and reduces training time compared to GAT-GRU. The learned interaction matrix provides interpretable structure.


<details>
  <summary>Details</summary>
Motivation: Existing GATs rely on predefined adjacency structures and dynamic attention scores, introducing inductive biases and computational overhead that can obscure interpretability.

Method: InterGAT replaces masked attention with a fully learnable, symmetric node interaction matrix, capturing latent spatial relationships without relying on fixed graph topologies. It is combined with a GRU-based temporal decoder to form InterGAT-GRU.

Result: InterGAT-GRU outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a 21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop dataset across all forecasting horizons (15 to 60 minutes). Training time is reduced by 60-70%.

Conclusion: The learned interaction matrix reveals interpretable structure, recovering sparse, topology-aware attention patterns that align with community structure. Structure learning supports prediction, computational efficiency, and topological interpretability in dynamic graph-based domains.

Abstract: Spatio-temporal forecasting is critical in applications such as traffic
prediction, energy demand modeling, and weather monitoring. While Graph
Attention Networks (GATs) are popular for modeling spatial dependencies, they
rely on predefined adjacency structures and dynamic attention scores,
introducing inductive biases and computational overhead that can obscure
interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked
attention with a fully learnable, symmetric node interaction matrix, capturing
latent spatial relationships without relying on fixed graph topologies. Our
framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder,
outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a
21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop
dataset across all forecasting horizons (15 to 60 minutes). Additionally, we
observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it
recovers sparse, topology-aware attention patterns that align with community
structure. Spectral and clustering analyses show that the model captures both
localized and global dynamics, offering insights into the functional topology
driving predictions. This highlights how structure learning can simultaneously
support prediction, computational efficiency, and topological interpretabil-ity
in dynamic graph-based domains.

</details>


### [194] [Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space](https://arxiv.org/abs/2506.00771)
*Zitao Chen,Yinjun Jia,Zitong Tian,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: MolFLAE是一种新的变分自编码器，用于3D分子的零样本操作，具有固定的维度和SE(3)-等变潜在空间，可以进行原子数编辑、结构重建和协调潜在插值等任务。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法通过监督任务来优化药物，如分子修复或属性引导优化。然而，研究人员希望开发一种更灵活的方法，可以在共享的潜在空间中导航以操作分子。

Method: 提出了一种名为MolFLAE的3D分子变分自编码器（VAE），它学习了一个固定维度的、与原子数量无关的SE(3)-等变潜在空间。MolFLAE使用SE(3)-等变神经网络对3D分子进行编码，并通过贝叶斯流网络（BFN）重建分子结构。该方法支持零样本分子操作，包括原子数编辑、结构重建和协调潜在插值。

Result: MolFLAE在标准无条件3D分子生成基准上表现优异，并成功应用于人类糖皮质激素受体的药物优化任务中，生成了具有改善亲水性同时保留关键相互作用的分子。

Conclusion: MolFLAE展示了其灵活性、鲁棒性和实际应用价值，为分子编辑和优化开辟了新途径。

Abstract: Medicinal chemists often optimize drugs considering their 3D structures and
designing structurally distinct molecules that retain key features, such as
shapes, pharmacophores, or chemical properties. Previous deep learning
approaches address this through supervised tasks like molecule inpainting or
property-guided optimization. In this work, we propose a flexible zero-shot
molecule manipulation method by navigating in a shared latent space of 3D
molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named
MolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space
independent of atom counts. MolFLAE encodes 3D molecules using an
SE(3)-equivariant neural network into fixed number of latent nodes,
distinguished by learned embeddings. The latent space is regularized, and
molecular structures are reconstructed via a Bayesian Flow Network (BFN)
conditioned on the encoder's latent output. MolFLAE achieves competitive
performance on standard unconditional 3D molecule generation benchmarks.
Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation,
including atom number editing, structure reconstruction, and coordinated latent
interpolation for both structure and properties. We further demonstrate our
approach on a drug optimization task for the human glucocorticoid receptor,
generating molecules with improved hydrophilicity while preserving key
interactions, under computational evaluations. These results highlight the
flexibility, robustness, and real-world utility of our method, opening new
avenues for molecule editing and optimization.

</details>


### [195] [LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772)
*Zihang Liu,Tianyu Pang,Oleg Balabanov,Chaoqun Yang,Tianjin Huang,Lu Yin,Yaoqing Yang,Shiwei Liu*

Main category: cs.LG

TL;DR: Recent studies indicate that supervised fine-tuning of LLMs on high-quality datasets can enhance reasoning capabilities. However, full fine-tuning is costly and prone to overfitting. Sparse fine-tuning offers a trade-off but lags in the LLM era. This work introduces Low-rank Informed Sparse Fine-Tuning (LIFT), which updates only the top 5% Principal Weights and achieves better performance than Full FT while maintaining memory efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost, overfitting, and catastrophic forgetting issues of full fine-tuning, as well as the limitations of sparse fine-tuning in the LLM era.

Method: Identify critical weights for fine-tuning through low-rank approximation, termed Principal Weights. Use magnitude-based sparse fine-tuning after rank reduction to develop the method LIFT, which updates only the top 5% Principal Weights.

Result: LIFT consistently achieves better performance on reasoning tasks than Full FT while maintaining memory efficiency comparable to parameter-efficient fine-tuning methods. It also retains more source-domain knowledge compared to Full FT and LoRA.

Conclusion: LIFT offers an effective and memory-efficient alternative to full fine-tuning for enhancing reasoning capabilities in LLMs.

Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small
number of high-quality datasets can yield strong reasoning capabilities.
However, full fine-tuning (Full FT), while powerful, is computationally
expensive and susceptible to overfitting and catastrophic forgetting,
particularly when data is limited. Sparse fine-tuning, which previously
achieved notable success by updating only a small subset of model parameters,
offers a promising trade-off between efficiency and effectiveness. Yet, it has
lagged behind in the LLM era due to the difficulty of identifying parameters
truly critical for reasoning. In this work, we state that weights with the
largest magnitude after low-rank approximation are critical weights for
fine-tuning, which we call Principal Weights. Surprisingly, while
magnitude-based sparse fine-tuning performs poorly as a baseline on LLM
fine-tuning, it becomes highly effective after rank reduction. These insights
motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only
updates the top 5% Principal Weights throughout training and consistently
achieves better performance on reasoning tasks than Full FT, while maintaining
memory efficiency on par with popular parameter-efficient fine-tuning methods.
In addition to strong performance on target domains such as arithmetic
reasoning, LIFT also retains up to 20% more source-domain knowledge, compared
to Full FT and LoRA. Our code is available at:
https://github.com/zihanghliu/LIFT.

</details>


### [196] [Bridging Supervised and Temporal Difference Learning with $Q$-Conditioned Maximization](https://arxiv.org/abs/2506.00795)
*Xing Lei,Zifeng Zhuang,Shentao Yang,Sheng Xu,Yunhao Luo,Fei Shen,Xuetao Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: In offline goal-conditioned RL, GCReinSL enhances SL with stitching capability via Q-conditioned policy and maximization, outperforming prior SL methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Supervised Learning (SL) methods for offline reinforcement learning are simple, stable, and efficient but lack the trajectory stitching capability inherent in Temporal Difference (TD)-based approaches. This poses a performance gap between SL and TD learning.

Method: The paper introduces Q-conditioned maximization supervised learning for offline goal-conditioned RL to enhance SL with stitching capability. Specifically, it proposes Goal-Conditioned Reinforced Supervised Learning (GCReinSL), which includes estimating the Q-function by CVAE from the offline dataset and finding the maximum Q-value within the data support by integrating Q-function maximization with Expectile Regression.

Result: Experimental results on offline RL datasets show that GCReinSL outperforms previous SL approaches with stitching capabilities and goal data augmentation techniques.

Conclusion: GCReinSL successfully bridges the performance gap between SL and TD learning by endowing SL with trajectory stitching capability through Q-conditioned policy and maximization.

Abstract: Recently, supervised learning (SL) methodology has emerged as an effective
approach for offline reinforcement learning (RL) due to their simplicity,
stability, and efficiency. However, recent studies show that SL methods lack
the trajectory stitching capability, typically associated with temporal
difference (TD)-based approaches. A question naturally surfaces: How can we
endow SL methods with stitching capability and bridge its performance gap with
TD learning? To answer this question, we introduce $Q$-conditioned maximization
supervised learning for offline goal-conditioned RL, which enhances SL with the
stitching capability through $Q$-conditioned policy and $Q$-conditioned
maximization. Concretely, we propose Goal-Conditioned Reinforced Supervised
Learning (GCReinSL), which consists of (1) estimating the $Q$-function by CVAE
from the offline dataset and (2) finding the maximum $Q$-value within the data
support by integrating $Q$-function maximization with Expectile Regression. In
inference time, our policy chooses optimal actions based on such a maximum
$Q$-value. Experimental results from stitching evaluations on offline RL
datasets demonstrate that our method outperforms prior SL approaches with
stitching capabilities and goal data augmentation techniques.

</details>


### [197] [Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning](https://arxiv.org/abs/2506.00797)
*Jianglin Ding,Jingcheng Tang,Gangshan Jing*

Main category: cs.LG

TL;DR: 在多智能体强化学习中，提出了一种基于动作依赖图（ADG）的新型个体策略，该策略不依赖自回归形式，可以实现全局最优并降低计算复杂度。通过理论证明和实验验证，展示了其在复杂环境中的鲁棒性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习方法通常采用自回归动作依赖策略，随着智能体数量增加，计算复杂度显著上升，限制了可扩展性。因此，需要一种更通用的动作依赖策略来解决这一问题。

Method: 提出使用动作依赖图（ADG）建模智能体间的动作依赖关系，并在协调图结构的MARL问题中，证明了满足特定条件的稀疏ADG可以实现全局最优。基于此，开发了一种表格型策略迭代算法，确保全局最优性，并将其整合到几种最先进的算法中进行实验。

Result: 理论分析表明，所提出的动作依赖策略可以在满足特定条件下实现全局最优。实验结果进一步验证了该方法在复杂环境中的鲁棒性和广泛适用性。

Conclusion: 所提出的方法通过动作依赖图有效降低了计算复杂度，同时实现了全局最优，为更广泛的多智能体强化学习挑战提供了新的解决方案。

Abstract: Action-dependent individual policies, which incorporate both environmental
states and the actions of other agents in decision-making, have emerged as a
promising paradigm for achieving global optimality in multi-agent reinforcement
learning (MARL). However, the existing literature often adopts auto-regressive
action-dependent policies, where each agent's policy depends on the actions of
all preceding agents. This formulation incurs substantial computational
complexity as the number of agents increases, thereby limiting scalability. In
this work, we consider a more generalized class of action-dependent policies,
which do not necessarily follow the auto-regressive form. We propose to use the
`action dependency graph (ADG)' to model the inter-agent action dependencies.
Within the context of MARL problems structured by coordination graphs, we prove
that an action-dependent policy with a sparse ADG can achieve global
optimality, provided the ADG satisfies specific conditions specified by the
coordination graph. Building on this theoretical foundation, we develop a
tabular policy iteration algorithm with guaranteed global optimality.
Furthermore, we integrate our framework into several SOTA algorithms and
conduct experiments in complex environments. The empirical results affirm the
robustness and applicability of our approach in more general scenarios,
underscoring its potential for broader MARL challenges.

</details>


### [198] [A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting](https://arxiv.org/abs/2506.00798)
*Jiankai Zheng,Liang Xie*

Main category: cs.LG

TL;DR: The paper proposes DST-SGNN, which includes SGSC, SGFT, LDGOSM and MSGSC for efficient processing of STTS.


<details>
  <summary>Details</summary>
Motivation: Accurately forecasting spatio-temporal time series (STTS) is challenging due to complex dynamic correlations in both time and space dimensions. Existing graph neural networks struggle to balance effectiveness and efficiency in modeling dynamic spatio-temporal relations.

Method: The authors propose the Dynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN). They introduce Stiefel Graph Spectral Convolution (SGSC) and Stiefel Graph Fourier Transform (SGFT), with the SGFT matrix constrained to lie on the Stiefel manifold. They also propose Linear Dynamic Graph Optimization on Stiefel Manifold (LDGOSM) for learning the SGFT matrix from the dynamic graph. Finally, they propose a multi-layer SGSC (MSGSC) that efficiently captures complex spatio-temporal correlations.

Result: Extensive experiments on seven spatio-temporal datasets show that DST-SGNN outperforms state-of-the-art methods while maintaining relatively low computational costs.

Conclusion: DST-SGNN is an effective and efficient method for processing spatio-temporal time series.

Abstract: Spatio-temporal time series (STTS) have been widely used in many
applications. However, accurately forecasting STTS is challenging due to
complex dynamic correlations in both time and space dimensions. Existing graph
neural networks struggle to balance effectiveness and efficiency in modeling
dynamic spatio-temporal relations. To address this problem, we propose the
Dynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN) to efficiently
process STTS. For DST-SGNN, we first introduce the novel Stiefel Graph Spectral
Convolution (SGSC) and Stiefel Graph Fourier Transform (SGFT). The SGFT matrix
in SGSC is constrained to lie on the Stiefel manifold, and SGSC can be regarded
as a filtered graph spectral convolution. We also propose the Linear Dynamic
Graph Optimization on Stiefel Manifold (LDGOSM), which can efficiently learn
the SGFT matrix from the dynamic graph and significantly reduce the
computational complexity. Finally, we propose a multi-layer SGSC (MSGSC) that
efficiently captures complex spatio-temporal correlations. Extensive
experiments on seven spatio-temporal datasets show that DST-SGNN outperforms
state-of-the-art methods while maintaining relatively low computational costs.

</details>


### [199] [Uni-LoRA: One Vector is All You Need](https://arxiv.org/abs/2506.00799)
*Kaiyang Li,Shaobo Han,Qing Su,Wei Li,Zhipeng Cai,Shihao Ji*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient
fine-tuning (PEFT) method for large language models (LLMs) by constraining
weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and
VB-LoRA push efficiency further by introducing additional constraints to reduce
the trainable parameter space. In this paper, we show that the parameter space
reduction strategies employed by these LoRA variants can be formulated within a
unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a
high-dimensional vector space $R^D$, can be reconstructed through a projection
from a subspace R^d, with $d \ll D$. We demonstrate that the fundamental
difference among various LoRA methods lies in the choice of the projection
matrix, $P \in R^{D \times d}$.Most existing LoRA variants rely on layer-wise
or structure-specific projections that limit cross-layer parameter sharing,
thereby compromising parameter efficiency. In light of this, we introduce an
efficient and theoretically grounded projection matrix that is isometric,
enabling global parameter sharing and reducing computation overhead.
Furthermore, under the unified view of Uni-LoRA, this design requires only a
single trainable vector to reconstruct LoRA parameters for the entire LLM -
making Uni-LoRA both a unified framework and a "one-vector-only" solution.
Extensive experiments on GLUE, mathematical reasoning, and instruction tuning
benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter
efficiency while outperforming or matching prior approaches in predictive
performance.

</details>


### [200] [LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery](https://arxiv.org/abs/2506.00844)
*Xingyu Wu,Kui Yu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: 这篇论文重新评估了大语言模型（LLM）在因果发现中的作用，认为它们不应直接参与确定因果关系。通过实证研究揭示了现有基于LLM的方法的局限性，并提出应将LLM限制为辅助工具而非决策工具。实验表明，在严格隔离LLM进行因果决策的情况下，基于LLM的启发式搜索能够加速收敛并优于传统和基于LLM的方法。


<details>
  <summary>Details</summary>
Motivation: 目前关于大语言模型（LLM）在因果发现领域的应用存在争议。尽管许多研究表明LLM在某些任务中表现出色，但其自回归、相关性驱动的建模方式缺乏因果推理的理论基础，可能导致不可靠的结果。此外，当前文献中报告的有利结果可能因精心设计的提示工程而被夸大。因此，有必要重新评估LLM在因果发现中的作用。

Method: 论文首先通过实证研究分析了现有基于LLM的因果发现方法的局限性。接着，提出了一种新的框架，将LLM的角色限制为辅助工具，而不参与因果关系的存在或方向性的决策。具体来说，LLM可以用于启发式搜索以协助因果图的搜索过程。最后，通过多种设置下的实验验证了该方法的有效性。

Result: 实验结果表明，在严格隔离LLM进行因果决策的情况下，基于LLM的启发式搜索能够在各种设置下加速收敛，并且性能优于传统的因果结构学习方法以及现有的基于LLM的方法。

Conclusion: 论文呼吁研究社区从盲目应用LLM转向开发尊重因果发现核心原则的专门模型和训练方法。LLM应在非决策性的辅助角色中发挥作用，而不是直接参与因果关系的确定。

Abstract: This paper critically re-evaluates LLMs' role in causal discovery and argues
against their direct involvement in determining causal relationships. We
demonstrate that LLMs' autoregressive, correlation-driven modeling inherently
lacks the theoretical grounding for causal reasoning and introduces
unreliability when used as priors in causal discovery algorithms. Through
empirical studies, we expose the limitations of existing LLM-based methods and
reveal that deliberate prompt engineering (e.g., injecting ground-truth
knowledge) could overstate their performance, helping to explain the
consistently favorable results reported in much of the current literature.
Based on these findings, we strictly confined LLMs' role to a non-decisional
auxiliary capacity: LLMs should not participate in determining the existence or
directionality of causal relationships, but can assist the search process for
causal graphs (e.g., LLM-based heuristic search). Experiments across various
settings confirm that, by strictly isolating LLMs from causal decision-making,
LLM-guided heuristic search can accelerate the convergence and outperform both
traditional and LLM-based methods in causal structure learning. We conclude
with a call for the community to shift focus from naively applying LLMs to
developing specialized models and training method that respect the core
principles of causal discovery.

</details>


### [201] [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
*Yizhuo Zhang,Heng Wang,Shangbin Feng,Zhaoxuan Tan,Xinyun Liu,Yulia Tsvetkov*

Main category: cs.LG

TL;DR: The paper proposes using reinforcement learning to enhance the graph reasoning capabilities of LLMs, moving beyond supervised fine-tuning on synthetic data. It designs solution-based and process-based rewards for synthetic graph problems, applies RL algorithms like GRPO and DPO to align LLMs, and conducts experiments showing significant improvements in performance across datasets.


<details>
  <summary>Details</summary>
Motivation: Previous approaches have focused on improving LLMs' graph reasoning through supervised fine-tuning on synthetic data, but these methods do not generalize well to real-world tasks with implicit graph structures.

Method: The method involves designing solution-based and process-based rewards for synthetic graph problems, applying reinforcement learning (RL) algorithms such as GRPO and DPO to align both off-the-shelf LLMs and those fine-tuned on synthetic graph data, and comparing their performance against existing settings on various tasks.

Result: Extensive experiments demonstrate statistically significant improvement on 5 datasets, with an average gain of 12.9%. Process-based rewards outperform solution-based ones, and mixing synthetic and real-world task data yields potential gains, though challenges remain in compositionality and explainability.

Conclusion: Reinforcement learning offers a promising approach to enhance LLMs' generalizable graph reasoning abilities, with process-based rewards proving more effective than solution-based ones.

Abstract: Previous research has sought to enhance the graph reasoning capabilities of
LLMs by supervised fine-tuning on synthetic graph data. While these led to
specialized LLMs better at solving graph algorithm problems, we don't need LLMs
for shortest path: we need generalization from synthetic graph data to
real-world tasks with implicit graph structures. In this work, we propose to
unlock generalizable learning of graph synthetic data with reinforcement
learning. We first design solution-based and process-based rewards for
synthetic graph problems: instead of rigid memorizing response patterns in
direct fine-tuning, we posit that RL would help LLMs grasp the essentials
underlying graph reasoning and alleviate overfitting. We employ RL algorithms
such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on
synthetic graph data. We then compare them against existing settings on both
in-domain synthetic tasks and out-of-domain real-world tasks with implicit
graph structures such as multi-hop QA, structured planning, and more. Extensive
experiments demonstrate that our RL recipe leads to statistically significant
improvement on 5 datasets, with an average gain of 12.9\% over baseline
settings. Further analysis reveals that process-based rewards consistently
outperform solution-based rewards, mixing synthetic and real-world task data
yields potential gains, while compositionality and explainable intermediate
steps remains a critical challenge even after RL.

</details>


### [202] [Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs](https://arxiv.org/abs/2506.00846)
*Mana Sakai,Ryo Karakida,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: The paper rigorously identifies the infinite-width limit distribution of variables within a single attention layer using the Tensor Programs framework, revealing non-Gaussian properties.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian-based asymptotic theories have limitations in capturing the behavior of attention layers under realistic conditions.

Method: Leveraging the Tensor Programs framework to derive the exact form of the infinite-width limit distribution for variables within a single attention layer under standard scaling.

Result: The derived limiting distribution exhibits non-Gaussianity due to a hierarchical structure and is Gaussian conditional on random similarity scores. Numerical experiments confirm the theory's effectiveness at finite width and its accurate description of finite-head attentions.

Conclusion: This study provides a foundation for developing a unified theory of deep Transformer architectures in the infinite-width regime.

Abstract: In modern theoretical analyses of neural networks, the infinite-width limit
is often invoked to justify Gaussian approximations of neuron preactivations
(e.g., via neural network Gaussian processes or Tensor Programs). However,
these Gaussian-based asymptotic theories have so far been unable to capture the
behavior of attention layers, except under special regimes such as infinitely
many heads or tailored scaling schemes. In this paper, leveraging the Tensor
Programs framework, we rigorously identify the infinite-width limit
distribution of variables within a single attention layer under realistic
architectural dimensionality and standard $1/\sqrt{n}$-scaling with $n$
dimensionality. We derive the exact form of this limit law without resorting to
infinite-head approximations or tailored scalings, demonstrating that it
departs fundamentally from Gaussianity. This limiting distribution exhibits
non-Gaussianity from a hierarchical structure, being Gaussian conditional on
the random similarity scores. Numerical experiments validate our theoretical
predictions, confirming the effectiveness of our theory at finite width and
accurate description of finite-head attentions. Beyond characterizing a
standalone attention layer, our findings lay the groundwork for developing a
unified theory of deep Transformer architectures in the infinite-width regime.

</details>


### [203] [Speech Unlearning](https://arxiv.org/abs/2506.00848)
*Jiali Cheng,Hadi Amiri*

Main category: cs.LG

TL;DR: This paper introduces machine unlearning for speech tasks, defining sample and class unlearning, showing it's more challenging than in images or text, and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the need for removing specific data influences from trained speech models without full retraining, which is crucial for privacy, noise removal, and bias mitigation.

Method: Defines two fundamental speech unlearning tasks - sample unlearning (removing individual data points) and class unlearning (removing entire categories), and conducts experiments on keyword spotting and speaker identification.

Result: Unlearning speech data is significantly more challenging compared to unlearning image or text data.

Conclusion: The paper concludes by presenting key future research directions including structured training, robust evaluation, feature-level unlearning, broader applications, scalable methods, and adversarial robustness.

Abstract: We introduce machine unlearning for speech tasks, a novel and underexplored
research problem that aims to efficiently and effectively remove the influence
of specific data from trained speech models without full retraining. This has
important applications in privacy preservation, removal of outdated or noisy
data, and bias mitigation. While machine unlearning has been studied in
computer vision and natural language processing, its application to speech is
largely unexplored due to the high-dimensional, sequential, and
speaker-dependent nature of speech data. We define two fundamental speech
unlearning tasks: sample unlearning, which removes individual data points
(e.g., a voice recording), and class unlearning, which removes an entire
category (e.g., all data from a speaker), while preserving performance on the
remaining data. Experiments on keyword spotting and speaker identification
demonstrate that unlearning speech data is significantly more challenging than
unlearning image or text data. We conclude with key future directions in this
area, including structured training, robust evaluation, feature-level
unlearning, broader applications, scalable methods, and adversarial robustness.

</details>


### [204] [Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis](https://arxiv.org/abs/2506.00849)
*Qi Chen,Jierui Zhu,Florian Shkurti*

Main category: cs.LG

TL;DR: 尽管扩散模型（DMs）和变分自编码器（VAEs）在实践中取得了成功，但它们的泛化性能尚未得到充分的理论研究，特别是缺乏对共享编码器-生成器结构的全面考虑。本文提出了一种统一的理论框架，通过将编码器和生成器视为随机映射，为两者提供了泛化保证。该框架进一步实现了：(1) 对VAEs进行更精细的分析，考虑了生成器的泛化；(2) 明确展示了DMs中与扩散时间T相关的泛化权衡；(3) 为DMs提供了仅基于训练数据的可计算边界，允许选择最优T，并将这些边界整合到优化过程中以提高模型性能。实验结果验证了所提理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和变分自编码器在实际应用中表现出色，但其泛化性能的理论基础尚不完善，特别是对编码器-生成器共享结构的泛化能力缺乏深入研究。这促使作者探索一种能够同时评估编码器和生成器泛化的理论框架。

Method: 作者提出了一种基于信息论工具的统一理论框架，将编码器和生成器视为随机映射，从而提供泛化保证。具体而言，该框架：
1. 对VAEs进行了改进分析，涵盖了生成器的泛化；
2. 阐明了DMs中扩散时间T与泛化性能之间的权衡关系；
3. 提供了仅依赖训练数据的可计算边界，用于选择最优T并将边界纳入优化过程。

Result: 理论分析表明，该框架可以有效捕捉VAEs和DMs的泛化特性，并揭示了扩散时间T对DMs泛化性能的影响。此外，通过实验证明，所提出的可计算边界能够指导模型优化，提升模型性能。实验结果在合成数据集和真实数据集上均验证了理论的有效性。

Conclusion: 本文提出的统一理论框架为扩散模型和变分自编码器的泛化性能提供了新的见解。通过对编码器和生成器的综合分析，揭示了两者的泛化特性及其相互关系，并为优化模型性能提供了理论支持和实用工具。

Abstract: Despite the empirical success of Diffusion Models (DMs) and Variational
Autoencoders (VAEs), their generalization performance remains theoretically
underexplored, especially lacking a full consideration of the shared
encoder-generator structure. Leveraging recent information-theoretic tools, we
propose a unified theoretical framework that provides guarantees for the
generalization of both the encoder and generator by treating them as randomized
mappings. This framework further enables (1) a refined analysis for VAEs,
accounting for the generator's generalization, which was previously overlooked;
(2) illustrating an explicit trade-off in generalization terms for DMs that
depends on the diffusion time $T$; and (3) providing computable bounds for DMs
based solely on the training data, allowing the selection of the optimal $T$
and the integration of such bounds into the optimization process to improve
model performance. Empirical results on both synthetic and real datasets
illustrate the validity of the proposed theory.

</details>


### [205] [FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling](https://arxiv.org/abs/2506.00862)
*Haixin Wang,Jiashu Pan,Hao Wu,Fan Zhang,Tailin Wu*

Main category: cs.LG

TL;DR: The paper proposes FourierFlow, a new generative turbulence modeling framework that addresses spectral bias and common-mode noise in high-fidelity turbulent flow generation. It includes three key innovations and shows superior performance and strong generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Modeling complex fluid systems, especially turbulence governed by PDEs, is challenging. Diffusion-based generative models have shown potential but struggle with spectral bias and common-mode noise when generating high-fidelity turbulent flows.

Method: FourierFlow consists of three key components: a dual-branch backbone architecture with local-global awareness, a frequency-guided Fourier mixing branch to mitigate spectral bias, and the use of masked auto-encoder pre-training for high-frequency modeling capabilities.

Result: FourierFlow demonstrates superior performance compared to state-of-the-art methods in three canonical turbulent flow scenarios and exhibits strong generalization capabilities in challenging settings.

Conclusion: FourierFlow enhances frequency-aware learning in generative turbulence modeling and offers a promising solution for high-fidelity turbulent flow generation.

Abstract: Modeling complex fluid systems, especially turbulence governed by partial
differential equations (PDEs), remains a fundamental challenge in science and
engineering. Recently, diffusion-based generative models have gained attention
as a powerful approach for these tasks, owing to their capacity to capture
long-range dependencies and recover hierarchical structures. However, we
present both empirical and theoretical evidence showing that generative models
struggle with significant spectral bias and common-mode noise when generating
high-fidelity turbulent flows. Here we propose FourierFlow, a novel generative
turbulence modeling framework that enhances the frequency-aware learning by
both implicitly and explicitly mitigating spectral bias and common-mode noise.
FourierFlow comprises three key innovations. Firstly, we adopt a dual-branch
backbone architecture, consisting of a salient flow attention branch with
local-global awareness to focus on sensitive turbulence areas. Secondly, we
introduce a frequency-guided Fourier mixing branch, which is integrated via an
adaptive fusion strategy to explicitly mitigate spectral bias in the generative
model. Thirdly, we leverage the high-frequency modeling capabilities of the
masked auto-encoder pre-training and implicitly align the features of the
generative model toward high-frequency components. We validate the
effectiveness of FourierFlow on three canonical turbulent flow scenarios,
demonstrating superior performance compared to state-of-the-art methods.
Furthermore, we show that our model exhibits strong generalization capabilities
in challenging settings such as out-of-distribution domains, long-term temporal
extrapolation, and robustness to noisy inputs. The code can be found at
https://github.com/AI4Science-WestlakeU/FourierFlow.

</details>


### [206] [Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning](https://arxiv.org/abs/2506.00867)
*Kyowoon Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: In this paper, researchers address the issue of infeasible trajectory generation in diffusion-based generative models for long-horizon tasks by proposing a method called Local Manifold Approximation and Projection (LoMAP). LoMAP projects guided samples onto a low-rank subspace approximated from offline datasets, thereby preventing infeasible trajectories. It is a training-free method that can be incorporated into hierarchical diffusion planners to enhance performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of current diffusion-based generative models which, despite showing promise in handling long-horizon, sparse-reward tasks, suffer from reliability issues due to stochastic risks leading to infeasible trajectories. This restricts their use in safety-critical applications.

Method: The authors propose LoMAP, a training-free method that addresses inaccurate guidance during sampling by projecting guided samples onto a low-rank subspace derived from offline datasets. They also establish a lower bound on the guidance gap to demonstrate manifold deviation.

Result: LoMAP was validated on standard offline reinforcement learning benchmarks involving challenging long-horizon planning, demonstrating its effectiveness. Additionally, incorporating LoMAP into hierarchical diffusion planners provided further performance improvements.

Conclusion: LoMAP successfully mitigates the risk of generating infeasible trajectories by ensuring samples remain close to the feasible manifold, thus enhancing the reliability of diffusion-based generative models in safety-critical applications.

Abstract: Recent advances in diffusion-based generative modeling have demonstrated
significant promise in tackling long-horizon, sparse-reward tasks by leveraging
offline datasets. While these approaches have achieved promising results, their
reliability remains inconsistent due to the inherent stochastic risk of
producing infeasible trajectories, limiting their applicability in
safety-critical applications. We identify that the primary cause of these
failures is inaccurate guidance during the sampling procedure, and demonstrate
the existence of manifold deviation by deriving a lower bound on the guidance
gap. To address this challenge, we propose Local Manifold Approximation and
Projection (LoMAP), a training-free method that projects the guided sample onto
a low-rank subspace approximated from offline datasets, preventing infeasible
trajectory generation. We validate our approach on standard offline
reinforcement learning benchmarks that involve challenging long-horizon
planning. Furthermore, we show that, as a standalone module, LoMAP can be
incorporated into the hierarchical diffusion planner, providing further
performance enhancements.

</details>


### [207] [ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models](https://arxiv.org/abs/2506.00880)
*Zhuo Chen,Yizhen Zheng,Huan Yee Koh,Hongxin Xiang,Linjiang Chen,Wenjie Du,Yang Wang*

Main category: cs.LG

TL;DR: ModuLM是一个支持灵活的基于LLM的模型构建和多种分子表示的框架，能够动态构造超过50,000种不同的模型配置。


<details>
  <summary>Details</summary>
Motivation: 目前没有一个LLM框架可以支持灵活的分子输入格式和动态架构切换，同时随着多样化的LLM和分子结构编码器的出现，模型空间显著扩展，带来了基准测试的主要挑战。

Method: ModuLM提供了丰富的模块化组件，包括8种2D分子图编码器、11种3D分子构象编码器、7种交互层和7种主流LLM主干，通过高度灵活的模型组装机制实现动态模型构建。

Result: 实验结果全面展示了ModuLM在支持基于LLM的分子关系学习任务方面的有效性。

Conclusion: ModuLM为分子关系学习任务提供了一个灵活且强大的框架，有助于减少重复编码并确保公平的模型比较。

Abstract: Molecular Relational Learning (MRL) aims to understand interactions between
molecular pairs, playing a critical role in advancing biochemical research.
With the recent development of large language models (LLMs), a growing number
of studies have explored the integration of MRL with LLMs and achieved
promising results. However, the increasing availability of diverse LLMs and
molecular structure encoders has significantly expanded the model space,
presenting major challenges for benchmarking. Currently, there is no LLM
framework that supports both flexible molecular input formats and dynamic
architectural switching. To address these challenges, reduce redundant coding,
and ensure fair model comparison, we propose ModuLM, a framework designed to
support flexible LLM-based model construction and diverse molecular
representations. ModuLM provides a rich suite of modular components, including
8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation
encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing
to its highly flexible model assembly mechanism, ModuLM enables the dynamic
construction of over 50,000 distinct model configurations. In addition, we
provide comprehensive results to demonstrate the effectiveness of ModuLM in
supporting LLM-based MRL tasks.

</details>


### [208] [State-Covering Trajectory Stitching for Diffusion Planners](https://arxiv.org/abs/2506.00895)
*Kyowoon Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: Diffusion-based generative models are powerful tools for long-horizon planning in RL, but their performance is limited by the quality and diversity of training data. The paper proposes SCoTS, a reward-free trajectory augmentation method that incrementally stitches together short trajectory segments to generate diverse and extended trajectories. SCoTS learns a temporal distance-preserving latent representation and stitches trajectory segments guided by directional exploration and novelty to cover and expand this latent space.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of diffusion-based generative models in reinforcement learning, which are restricted by the quality and diversity of training data, affecting generalization to tasks outside their training distribution or longer planning horizons.

Method: Propose State-Covering Trajectory Stitching (SCoTS), a reward-free trajectory augmentation method that incrementally stitches together short trajectory segments to generate diverse and extended trajectories. It learns a temporal distance-preserving latent representation and iteratively stitches trajectory segments guided by directional exploration and novelty.

Result: SCoTS significantly improves the performance and generalization capabilities of diffusion planners on offline goal-conditioned benchmarks requiring stitching and long-horizon reasoning. Augmented trajectories generated by SCoTS also improve the performance of widely used offline goal-conditioned RL algorithms across diverse environments.

Conclusion: SCoTS is an effective method for enhancing the performance and generalization of diffusion-based generative models in reinforcement learning, particularly in offline settings with long-horizon planning.

Abstract: Diffusion-based generative models are emerging as powerful tools for
long-horizon planning in reinforcement learning (RL), particularly with offline
datasets. However, their performance is fundamentally limited by the quality
and diversity of training data. This often restricts their generalization to
tasks outside their training distribution or longer planning horizons. To
overcome this challenge, we propose State-Covering Trajectory Stitching
(SCoTS), a novel reward-free trajectory augmentation method that incrementally
stitches together short trajectory segments, systematically generating diverse
and extended trajectories. SCoTS first learns a temporal distance-preserving
latent representation that captures the underlying temporal structure of the
environment, then iteratively stitches trajectory segments guided by
directional exploration and novelty to effectively cover and expand this latent
space. We demonstrate that SCoTS significantly improves the performance and
generalization capabilities of diffusion planners on offline goal-conditioned
benchmarks requiring stitching and long-horizon reasoning. Furthermore,
augmented trajectories generated by SCoTS significantly improve the performance
of widely used offline goal-conditioned RL algorithms across diverse
environments.

</details>


### [209] [PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models](https://arxiv.org/abs/2506.00910)
*Seongjae Kang,Dong Bok Lee,Hyungjoon Jang,Dongseop Kim,Sung Ju Hwang*

Main category: cs.LG

TL;DR: ActiveKD是一个结合主动学习（AL）与知识蒸馏（KD）的框架，通过利用大型视觉-语言模型（VLMs）的零样本和少样本能力，在标注资源有限的情况下提高学生模型的学习效率。其核心组件Probabilistic CoreSet（PCoreSet）通过在概率空间中最大化覆盖范围来选择多样化的未标注样本，从而更有效地传递教师模型的知识。实验结果表明，PCoreSet在11个数据集上显著优于现有的选择方法。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏（KD）被广泛应用于训练紧凑的任务特定模型，但其与主动学习（AL）的结合尚未得到充分探索。这是因为KD通常假设存在足够的标注数据，而AL则专注于在数据稀缺场景下通过迭代样本选择来最小化标注成本。这促使研究者开发一种能够将AL与KD相结合的新框架，以解决任务特定教师模型在数据稀缺场景下的不可用问题。

Method: 本文提出了一种名为ActiveKD的框架，该框架通过利用大型视觉-语言模型（VLMs）的零样本和少样本能力，将主动学习（AL）与知识蒸馏（KD）结合起来。为了利用VLMs预测中的结构化偏差（即概率空间中的聚类现象），作者提出了Probabilistic CoreSet（PCoreSet）选择策略。PCoreSet通过在概率空间而非特征空间中最大化覆盖范围，战略性地选择类别多样的未标注样本，从而实现有限标注预算下的高效知识传递。

Result: 在11个数据集上的评估结果表明，PCoreSet在ActiveKD框架内始终优于现有的选择方法，证明了该方法的有效性和优越性。

Conclusion: ActiveKD框架及其核心组件PCoreSet成功地将主动学习与知识蒸馏结合在一起，为数据稀缺场景下的模型训练提供了一种新思路。这一研究成果推动了主动学习与知识蒸馏交叉领域的进一步发展。

Abstract: Knowledge distillation (KD) is a widely used framework for training compact,
task-specific models by leveraging the knowledge of teacher models. However,
its application to active learning (AL), which aims to minimize annotation
costs through iterative sample selection, remains underexplored. This gap stems
from the fact that KD typically assumes access to sufficient labeled data,
whereas AL operates in data-scarce scenarios where task-specific teacher models
are often unavailable. In this paper, we introduce ActiveKD, a framework that
integrates AL with KD by leveraging the zero- and few-shot capabilities of
large vision-language models (VLMs). A key aspect of ActiveKD is the structured
prediction bias of VLMs -- i.e., their predictions form clusters in the
probability space. We regard this structure as an inductive bias of the teacher
model, capturing generalizable output patterns beneficial to student learning.
To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection
strategy that maximizes coverage in the probability space rather than the
feature space. PCoreSet strategically selects categorically diverse unlabeled
samples, facilitating more efficient transfer of teacher knowledge under
limited annotation budgets. Evaluations on 11 datasets show that PCoreSet
consistently outperforms existing selection methods within the ActiveKD
framework, advancing research at the intersection of AL and KD.

</details>


### [210] [Q-learning with Posterior Sampling](https://arxiv.org/abs/2506.00917)
*Priyank Agrawal,Shipra Agrawal,Azmat Azati*

Main category: cs.LG

TL;DR: 在强化学习中，Q-Learning与后验采样结合的算法（PSQL）在表格场景下的遗憾值接近理论下界。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯后验采样技术在许多探索-利用场景中表现出优越的性能，但在复杂场景如强化学习中的理论分析仍具挑战性。

Method: 提出了一种基于Q-learning的算法PSQL，使用高斯后验Q值进行探索，类似于多臂赌博机设置中的汤普森采样算法。

Result: 在表格情景MDP设置中，PSQL实现了一个遗憾界限，接近已知的下界。

Conclusion: 本文为分析这种高效且重要的算法技术提供了一个起点，尤其是在更复杂的RL设置中。

Abstract: Bayesian posterior sampling techniques have demonstrated superior empirical
performance in many exploration-exploitation settings. However, their
theoretical analysis remains a challenge, especially in complex settings like
reinforcement learning. In this paper, we introduce Q-Learning with Posterior
Sampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian
posteriors on Q-values for exploration, akin to the popular Thompson Sampling
algorithm in the multi-armed bandit setting. We show that in the tabular
episodic MDP setting, PSQL achieves a regret bound of $\tilde
O(H^2\sqrt{SAT})$, closely matching the known lower bound of
$\Omega(H\sqrt{SAT})$. Here, S, A denote the number of states and actions in
the underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the
number of episodes and $H$ being the planning horizon. Our work provides
several new technical insights into the core challenges in combining posterior
sampling with dynamic programming and TD-learning-based RL algorithms, along
with novel ideas for resolving those difficulties. We hope this will form a
starting point for analyzing this efficient and important algorithmic technique
in even more complex RL settings.

</details>


### [211] [Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks](https://arxiv.org/abs/2506.00918)
*Lennart Bramlage,Cristóbal Curio*

Main category: cs.LG

TL;DR: The paper presents a framework for post-hoc uncertainty estimation in regression tasks that uses an auxiliary model to enhance out-of-distribution (OOD) detection and maintain accurate uncertainty estimation without affecting predictive performance.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is essential in safety-critical applications but often omitted due to its negative impact on predictive performance. Existing methods for adding uncertainty post-hoc require access to model parameters or gradients, which may not be feasible in practice.

Method: The authors propose fitting an auxiliary model to original inputs and frozen model outputs for post-hoc uncertainty estimation. They formalize an optimization objective based on maximum likelihood estimation and sequential parameter fitting principles to recover Gaussian parameters without sampling or approximation at inference time.

Result: Using diverse auxiliary data improves OOD detection and metric performance. The method confirms that frozen model outputs contain latent information about model error and predictive uncertainty. It maintains proper input-dependent uncertainty estimation without relying solely on base model forecasts.

Conclusion: The proposed framework effectively performs post-hoc uncertainty estimation for regression tasks, validated through toy problems and benchmarks like UCI and depth regression. Code is available.

Abstract: Uncertainty quantification is critical in safety-sensitive applications but
is often omitted from off-the-shelf neural networks due to adverse effects on
predictive performance. Retrofitting uncertainty estimates post-hoc typically
requires access to model parameters or gradients, limiting feasibility in
practice. We propose a theoretically grounded framework for post-hoc
uncertainty estimation in regression tasks by fitting an auxiliary model to
both original inputs and frozen model outputs. Drawing from principles of
maximum likelihood estimation and sequential parameter fitting, we formalize an
exact post-hoc optimization objective that recovers the canonical MLE of
Gaussian parameters, without requiring sampling or approximation at inference.
While prior work has used model outputs to estimate uncertainty, we explicitly
characterize the conditions under which this is valid and demonstrate the
extent to which structured outputs can support quasi-epistemic inference. We
find that using diverse auxiliary data, such as augmented subsets of the
original training data, significantly enhances OOD detection and metric
performance. Our hypothesis that frozen model outputs contain generalizable
latent information about model error and predictive uncertainty is tested and
confirmed. Finally, we ensure that our method maintains proper estimation of
input-dependent uncertainty without relying exclusively on base model
forecasts. These findings are demonstrated in toy problems and adapted to both
UCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.

</details>


### [212] [Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation](https://arxiv.org/abs/2506.00920)
*Philip Heejun Lee*

Main category: cs.LG

TL;DR: PRISM is a new positional encoding mechanism that allows Transformers to extrapolate accurately up to 10x beyond their training length, achieving state-of-the-art length extrapolation in various tasks.


<details>
  <summary>Details</summary>
Motivation: Deep sequence models often degrade in accuracy when test sequences significantly exceed their training lengths. Many critical tasks require robust length extrapolation.

Method: PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings.

Result: PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks.

Conclusion: These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.

Abstract: Deep sequence models typically degrade in accuracy when test sequences
significantly exceed their training lengths, yet many critical tasks--such as
algorithmic reasoning, multi-step arithmetic, and compositional
generalization--require robust length extrapolation. We introduce PRISM, a
Probabilistic Relative-position Implicit Superposition Model, a novel
positional encoding mechanism that enables Transformers to extrapolate
accurately up to 10x beyond their training length. PRISM learns continuous
relative positions through a differentiable histogram-filter update, preserving
position uncertainty via a probabilistic superposition rather than conventional
deterministic embeddings. Empirically, PRISM achieves state-of-the-art length
extrapolation, successfully generalizing to previously intractable sequence
lengths across algorithmic benchmarks--including arithmetic (addition,
multiplication), SCAN compositionality tasks, and complex copy variants derived
from DeepMind's recent datasets. Our analysis demonstrates that PRISM's
stochastic positional encoding maintains sharp and interpretable internal
states, providing a theoretical basis for reliable length generalization. These
results advance the goal of neural sequence models that remain algorithmically
robust at lengths far exceeding their training horizon.

</details>


### [213] [Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity](https://arxiv.org/abs/2506.00932)
*Qiao Xiao,Boqian Wu,Andrey Poddubnyy,Elena Mocanu,Phuong H. Nguyen,Mykola Pechenizkiy,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: Federated learning (FL) faces challenges due to data heterogeneity and limited local datasets. This paper identifies the Layer-wise Inertia Phenomenon in FL and proposes LIPS, a method that periodically introduces transient sparsity to stimulate meaningful updates and empower global aggregation.


<details>
  <summary>Details</summary>
Motivation: FL enables collaborative model training while preserving data privacy but faces significant challenges due to data heterogeneity and limited local datasets which often impede effective collaboration.

Method: Propose LIPS (Layer-wise Inertia Phenomenon with Sparsity), a method that periodically introduces transient sparsity to stimulate meaningful updates and empower global aggregation.

Result: Experiments demonstrate that LIPS effectively mitigates layer-wise inertia, enhances aggregation effectiveness, and improves overall performance in various FL scenarios.

Conclusion: This work not only deepens the understanding of layer-wise learning dynamics in FL but also paves the way for more effective collaboration strategies in resource-constrained environments.

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy, leveraging aggregated
updates to build robust global models. However, this training paradigm faces
significant challenges due to data heterogeneity and limited local datasets,
which often impede effective collaboration. In such scenarios, we identify the
Layer-wise Inertia Phenomenon in FL, wherein the middle layers of global model
undergo minimal updates after early communication rounds, ultimately limiting
the effectiveness of global aggregation. We demonstrate the presence of this
phenomenon across a wide range of federated settings, spanning diverse datasets
and architectures. To address this issue, we propose LIPS (Layer-wise Inertia
Phenomenon with Sparsity), a simple yet effective method that periodically
introduces transient sparsity to stimulate meaningful updates and empower
global aggregation. Experiments demonstrate that LIPS effectively mitigates
layer-wise inertia, enhances aggregation effectiveness, and improves overall
performance in various FL scenarios. This work not only deepens the
understanding of layer-wise learning dynamics in FL but also paves the way for
more effective collaboration strategies in resource-constrained environments.
Our code is publicly available at: https://github.com/QiaoXiao7282/LIPS.

</details>


### [214] [Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning](https://arxiv.org/abs/2506.00936)
*Peijin Guo,Minghui Li,Hewen Pan,Bowen Chen,Yang Wu,Zikang Guo,Leo Yu Zhang,Shengshan Hu,Shengqing Hu*

Main category: cs.LG

TL;DR: The paper proposes TrustworthyMS, a contrastive learning framework for predicting molecular metabolic stability with uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of molecular metabolic stability is crucial in drug research and development. However, current methods have limitations including incomplete molecular modeling due to atom-centric message-passing mechanisms that ignore bond-level topological features and lack reliable uncertainty quantification.

Method: TrustworthyMS incorporates a molecular graph topology remapping mechanism for capturing localized electronic effects and global conformational constraints, contrastive topology-bond alignment for enhancing representation robustness, and Beta-Binomial uncertainty quantification for simultaneous prediction and confidence calibration under epistemic uncertainty.

Result: Extensive experiments show that TrustworthyMS outperforms state-of-the-art methods in predictive performance.

Conclusion: TrustworthyMS addresses critical limitations in existing approaches for metabolic stability prediction by integrating advanced molecular modeling and uncertainty quantification.

Abstract: Accurate prediction of molecular metabolic stability (MS) is critical for
drug research and development but remains challenging due to the complex
interplay of molecular interactions. Despite recent advances in graph neural
networks (GNNs) for MS prediction, current approaches face two critical
limitations: (1) incomplete molecular modeling due to atom-centric
message-passing mechanisms that disregard bond-level topological features, and
(2) prediction frameworks that lack reliable uncertainty quantification. To
address these challenges, we propose TrustworthyMS, a novel contrastive
learning framework designed for uncertainty-aware metabolic stability
prediction. First, a molecular graph topology remapping mechanism synchronizes
atom-bond interactions through edge-induced feature propagation, capturing both
localized electronic effects and global conformational constraints. Second,
contrastive topology-bond alignment enforces consistency between molecular
topology views and bond patterns via feature alignment, enhancing
representation robustness. Third, uncertainty modeling through Beta-Binomial
uncertainty quantification enables simultaneous prediction and confidence
calibration under epistemic uncertainty. Through extensive experiments, our
results demonstrate that TrustworthyMS outperforms current state-of-the-art
methods in terms of predictive performance.

</details>


### [215] [Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation](https://arxiv.org/abs/2506.00959)
*Xiaohan Wang,Yu Zhang,Guibin Jiang,Bing Cheng,Wei Lin*

Main category: cs.LG

TL;DR: The paper proposes a novel approach for marketing optimization through clustering and multi-task representation network, showing superiority in offline experiments and online A/B tests.


<details>
  <summary>Details</summary>
Motivation: Marketing optimization is crucial for user growth but existing methods face challenges with large-scale counterfactual prediction and complexity trade-offs, especially in industrial scenarios with data noise.

Method: A multi-task representation network learns individual attributes and projects original features into high-dimension hidden representations. These are then clustered into K groups, reformulating the problem as an integer stochastic programming problem. Finally, a multi-category model is distilled for online deployment.

Result: Offline experiments demonstrate effectiveness compared to six state-of-the-art algorithms. Online A/B tests on Meituan show improvements of 0.53% and 0.65% in order volume and gross merchandise volume respectively.

Conclusion: The proposed approach effectively addresses marketing optimization challenges, providing a robust solution for large-scale industrial applications.

Abstract: Marketing optimization, commonly formulated as an online budget allocation
problem, has emerged as a pivotal factor in driving user growth. Most existing
research addresses this problem by following the principle of 'first predict
then optimize' for each individual, which presents challenges related to
large-scale counterfactual prediction and solving complexity trade-offs. Note
that the practical data quality is uncontrollable, and the solving scale tends
to be tens of millions. Therefore, the existing approaches make the robust
budget allocation non-trivial, especially in industrial scenarios with
considerable data noise. To this end, this paper proposes a novel approach that
solves the problem from the cluster perspective. Specifically, we propose a
multi-task representation network to learn the inherent attributes of
individuals and project the original features into high-dimension hidden
representations through the first two layers of the trained network. Then, we
divide these hidden representations into $K$ groups through partitioning-based
clustering, thus reformulating the problem as an integer stochastic programming
problem under different total budgets. Finally, we distill the representation
module and clustering model into a multi-category model to facilitate online
deployment. Offline experiments validate the effectiveness and superiority of
our approach compared to six state-of-the-art marketing optimization
algorithms. Online A/B tests on the Meituan platform indicate that the approach
outperforms the online algorithm by 0.53% and 0.65%, considering order volume
(OV) and gross merchandise volume (GMV), respectively.

</details>


### [216] [Enhancing Parallelism in Decentralized Stochastic Convex Optimization](https://arxiv.org/abs/2506.00961)
*Ofri Eisen,Ron Dorfman,Kfir Y. Levy*

Main category: cs.LG

TL;DR: Decentralized Anytime SGD是一种新型的去中心化学习算法，能够有效扩展关键并行阈值，使更多机器在不损害性能的情况下得以使用，并且在SCO框架内建立了超越现有技术的理论并行上限。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化学习方法在机器数量超过一定点后会对收敛率产生负面影响，因此需要一种新方法来克服这一可扩展性限制。

Method: 提出了一种名为Decentralized Anytime SGD的新算法，该算法可以在不影响性能的前提下有效地使用更多机器，并在随机凸优化（SCO）框架中建立了一个理论上的并行上限。

Result: 该算法显著扩展了关键并行阈值，使得更大规模的网络能够获得良好的统计保证，并缩小了与高连接拓扑中的集中式学习之间的差距。

Conclusion: Decentralized Anytime SGD为去中心化学习提供了一种新的解决方案，提高了其可扩展性和性能表现，尤其适用于大规模网络环境。

Abstract: Decentralized learning has emerged as a powerful approach for handling large
datasets across multiple machines in a communication-efficient manner. However,
such methods often face scalability limitations, as increasing the number of
machines beyond a certain point negatively impacts convergence rates. In this
work, we propose Decentralized Anytime SGD, a novel decentralized learning
algorithm that significantly extends the critical parallelism threshold,
enabling the effective use of more machines without compromising performance.
Within the stochastic convex optimization (SCO) framework, we establish a
theoretical upper bound on parallelism that surpasses the current
state-of-the-art, allowing larger networks to achieve favorable statistical
guarantees and closing the gap with centralized learning in highly connected
topologies.

</details>


### [217] [Reinforcement Learning with Random Time Horizons](https://arxiv.org/abs/2506.00962)
*Enric Ribera Borrell,Lorenz Richter,Christof Schütte*

Main category: cs.LG

TL;DR: The paper extends reinforcement learning to random time horizons, deriving policy gradient formulas for real-world applications with random stopping times, and demonstrating improved optimization convergence in experiments.


<details>
  <summary>Details</summary>
Motivation: Real-world applications often have random stopping times that depend on the policy, which is not accounted for in the classical reinforcement learning framework that typically assumes finite and deterministic or infinite runtimes of trajectories.

Method: The authors derive policy gradient formulas rigorously for both stochastic and deterministic policies under random time horizons using two complementary perspectives: trajectory-based and state-space based. Connections to optimal control theory are also established.

Result: Numerical experiments show that using the proposed formulas can significantly improve optimization convergence compared to traditional approaches.

Conclusion: The extension of reinforcement learning to random time horizons and the derived policy gradient formulas offer a better fit for real-world applications and lead to significant improvements in optimization convergence.

Abstract: We extend the standard reinforcement learning framework to random time
horizons. While the classical setting typically assumes finite and
deterministic or infinite runtimes of trajectories, we argue that multiple
real-world applications naturally exhibit random (potentially
trajectory-dependent) stopping times. Since those stopping times typically
depend on the policy, their randomness has an effect on policy gradient
formulas, which we (mostly for the first time) derive rigorously in this work
both for stochastic and deterministic policies. We present two complementary
perspectives, trajectory or state-space based, and establish connections to
optimal control theory. Our numerical experiments demonstrate that using the
proposed formulas can significantly improve optimization convergence compared
to traditional approaches.

</details>


### [218] [Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO](https://arxiv.org/abs/2506.00967)
*Tingting Zhang,Sergiy A. Vorobyov,David J. Love,Taejoon Kim,Kai Dong*

Main category: cs.LG

TL;DR: 在蜂窝式大规模MIMO系统中，优化型功率控制算法计算复杂度高，不适于实时应用。基于学习的方法如图神经网络（GNN）表现出良好性能，但现有方法存在理想假设和固定用户数量等问题。本文提出一种自监督的图注意力网络用于下行链路功率控制，能有效处理导频污染并适应动态用户数量。实验表明其效果接近最优加速投影梯度法。


<details>
  <summary>Details</summary>
Motivation: 优化型功率控制算法计算复杂度过高，而现有的基于学习的方法存在理想正交性和固定用户数量等不切实际的假设，且监督训练成本高昂。因此需要一种新方法解决这些问题。

Method: 提出一种自监督的图注意力网络，用于蜂窝式大规模MIMO系统的下行链路功率控制。该方法能够有效应对导频污染问题，并适应动态变化的用户设备数量。

Result: 实验结果表明，所提出的方法具有有效性，其性能甚至可以与作为基线的最优加速投影梯度方法相媲美。

Conclusion: 所提出的自监督图注意力网络为蜂窝式大规模MIMO系统的功率控制提供了一种有效的解决方案，解决了现有方法中的关键问题，如导频污染和动态用户数量适应性。

Abstract: Optimization-based power control algorithms are predominantly iterative with
high computational complexity, making them impractical for real-time
applications in cell-free massive multiple-input multiple-output (CFmMIMO)
systems. Learning-based methods have emerged as a promising alternative, and
among them, graph neural networks (GNNs) have demonstrated their excellent
performance in solving power control problems. However, all existing GNN-based
approaches assume ideal orthogonality among pilot sequences for user equipments
(UEs), which is unrealistic given that the number of UEs exceeds the available
orthogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based
methods assume a fixed number of UEs, whereas the number of active UEs varies
over time in practice. Additionally, supervised training necessitates costly
computational resources for computing the target power control solutions for a
large volume of training samples. To address these issues, we propose a graph
attention network for downlink power control in CFmMIMO systems that operates
in a self-supervised manner while effectively handling pilot contamination and
adapting to a dynamic number of UEs. Experimental results show its
effectiveness, even in comparison to the optimal accelerated projected gradient
method as a baseline.

</details>


### [219] [Data Heterogeneity Modeling for Trustworthy Machine Learning](https://arxiv.org/abs/2506.00969)
*Jiashuo Liu,Peng Cui*

Main category: cs.LG

TL;DR: 数据异质性对机器学习系统的性能起着关键作用。传统算法通常旨在优化平均性能，往往忽视了数据集内的内在多样性。这种忽视可能导致各种问题，如不可靠的决策、跨域泛化不足、不公平结果和错误的科学推断。因此，在开发可靠的、数据驱动的系统时，对数据异质性进行细致建模是至关重要的。本文综述了异质性感知机器学习（Heterogeneity-aware ML），这是一种在ML管道的每个阶段都系统地考虑数据异质性的范式。通过将其应用于包括医疗保健、农业、金融和推荐系统等重要领域，展示了该方法的好处和潜力。这些应用强调了对数据多样性的更深入理解如何增强模型的鲁棒性、公平性和可靠性，并有助于模型诊断和改进。此外，文章还探讨了未来的发展方向，并为整个数据挖掘社区提供了研究机会，以促进异质性感知ML的发展。


<details>
  <summary>Details</summary>
Motivation: 数据异质性对机器学习系统的性能有重大影响，而传统算法往往忽视了这一因素，导致了一系列问题。为了构建更可靠的数据驱动系统，需要一种能够细致建模数据异质性的方法。

Method: 提出并探索了异质性感知机器学习（Heterogeneity-aware ML）这一范式，该范式系统地将数据异质性考虑融入到机器学习管道的各个阶段，从数据收集、模型训练到模型评估和部署。

Result: 通过在多个关键领域的应用，证明了异质性感知机器学习能够显著提高模型的鲁棒性、公平性和可靠性，同时有助于模型诊断和改进。

Conclusion: 异质性感知机器学习具有巨大的潜力，可以改善多个领域的机器学习应用。未来的研究应进一步探索和发展这一领域，以推动其广泛应用和进步。

Abstract: Data heterogeneity plays a pivotal role in determining the performance of
machine learning (ML) systems. Traditional algorithms, which are typically
designed to optimize average performance, often overlook the intrinsic
diversity within datasets. This oversight can lead to a myriad of issues,
including unreliable decision-making, inadequate generalization across
different domains, unfair outcomes, and false scientific inferences. Hence, a
nuanced approach to modeling data heterogeneity is essential for the
development of dependable, data-driven systems. In this survey paper, we
present a thorough exploration of heterogeneity-aware machine learning, a
paradigm that systematically integrates considerations of data heterogeneity
throughout the entire ML pipeline -- from data collection and model training to
model evaluation and deployment. By applying this approach to a variety of
critical fields, including healthcare, agriculture, finance, and recommendation
systems, we demonstrate the substantial benefits and potential of
heterogeneity-aware ML. These applications underscore how a deeper
understanding of data diversity can enhance model robustness, fairness, and
reliability and help model diagnosis and improvements. Moreover, we delve into
future directions and provide research opportunities for the whole data mining
community, aiming to promote the development of heterogeneity-aware ML.

</details>


### [220] [Quantization-based Bounds on the Wasserstein Metric](https://arxiv.org/abs/2506.00976)
*Jonathan Bobrutsky,Amit Moscovich*

Main category: cs.LG

TL;DR: The paper proposes a method to compute efficient approximations of the Wasserstein metric with strict upper or lower bounds, achieving significant speedup compared to entropy-regularized OT while maintaining low approximation error.


<details>
  <summary>Details</summary>
Motivation: The Wasserstein metric is crucial in many machine learning applications but costly to compute, prompting the need for efficient approximations that also serve as strict bounds.

Method: Focuses on discrete measures on regular grids, formulates and solves a Kantorovich problem on a coarse grid using a quantized measure and specially designed cost matrix, then performs upscaling and correction in either primal or dual space to obtain valid upper and lower bounds.

Result: Demonstrates a 10x-100x speedup compared to entropy-regularized OT on the DOTmark benchmark while keeping the approximation error below 2%.

Conclusion: The proposed method provides efficient approximations to the Wasserstein metric with strict bounds, significantly improving computational efficiency.

Abstract: The Wasserstein metric has become increasingly important in many machine
learning applications such as generative modeling, image retrieval and domain
adaptation. Despite its appeal, it is often too costly to compute. This has
motivated approximation methods like entropy-regularized optimal transport,
downsampling, and subsampling, which trade accuracy for computational
efficiency. In this paper, we consider the challenge of computing efficient
approximations to the Wasserstein metric that also serve as strict upper or
lower bounds. Focusing on discrete measures on regular grids, our approach
involves formulating and exactly solving a Kantorovich problem on a coarse grid
using a quantized measure and specially designed cost matrix, followed by an
upscaling and correction stage. This is done either in the primal or dual space
to obtain valid upper and lower bounds on the Wasserstein metric of the
full-resolution inputs. We evaluate our methods on the DOTmark optimal
transport images benchmark, demonstrating a 10x-100x speedup compared to
entropy-regularized OT while keeping the approximation error below 2%.

</details>


### [221] [LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers](https://arxiv.org/abs/2506.00998)
*Changshun Wu,Tianyi Duan,Saddek Bensalem,Chih-Hong Cheng*

Main category: cs.LG

TL;DR: Fine-tuning large language models can improve domain-specific performance but cause overfitting, leading to unreliable out-of-distribution (OoD) query responses. LoRA-BAM is proposed, which adds OoD detection monitors using boxed abstraction in the LoRA layer to filter questions beyond model competence.


<details>
  <summary>Details</summary>
Motivation: To address the issue of fine-tuned large language models becoming unreliable when dealing with out-of-distribution queries due to overfitting.

Method: LoRA-BAM adds OoD detection monitors to the LoRA layer using boxed abstraction. Feature vectors from fine-tuning data are extracted and clustered; clusters are enclosed in boxes. A question is flagged as OoD if its feature vector falls outside all boxes. Additionally, a regularization loss is introduced during fine-tuning to keep paraphrased questions close in the feature space and adjust decision boundary enlargement based on feature variance within clusters.

Result: LoRA-BAM provides lightweight and interpretable OoD detection that complements existing defenses.

Conclusion: LoRA-BAM successfully addresses the challenge of detecting out-of-distribution queries in fine-tuned large language models.

Abstract: Fine-tuning large language models (LLMs) improves performance on
domain-specific tasks but can lead to overfitting, making them unreliable on
out-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD
detection monitors to the LoRA layer using boxed abstraction to filter
questions beyond the model's competence. Feature vectors from the fine-tuning
data are extracted via the LLM and clustered. Clusters are enclosed in boxes; a
question is flagged as OoD if its feature vector falls outside all boxes. To
improve interpretability and robustness, we introduce a regularization loss
during fine-tuning that encourages paraphrased questions to stay close in the
feature space, and the enlargement of the decision boundary is based on the
feature variance within a cluster. Our method complements existing defenses by
providing lightweight and interpretable OoD detection.

</details>


### [222] [Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts](https://arxiv.org/abs/2506.01000)
*Chengyi Cai,Zesheng Ye,Lei Feng,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: The paper introduces a decoupling-and-reweighting framework with Decoupled Visual Prompts (DVP) and Probabilistic Reweighting Matrix (PRM) to improve visual reprogramming for CLIP, enhancing classification performance across 11 downstream datasets.


<details>
  <summary>Details</summary>
Motivation: Existing Visual Reprogramming (VR) approaches for CLIP have limitations in capturing diverse aspects of class descriptions and may introduce bias towards less informative attributes.

Method: The method involves optimizing decoupled visual prompts (DVP) using descriptions grouped by explicit causes or unsupervised clusters. Outputs are integrated with a probabilistic reweighting matrix (PRM) that measures contributions to each downstream class.

Result: DVP lowers the empirical risk bound theoretically and outperforms baselines experimentally on average across 11 downstream datasets.

Conclusion: The DVP-PRM integration not only enhances classification performance but also provides insights into how individual visual prompts influence decisions.

Abstract: Model reprogramming adapts pretrained models to downstream tasks by modifying
only the input and output spaces. Visual reprogramming (VR) is one instance for
vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to
input images to facilitate downstream classification. The existing VR
approaches for CLIP train a single visual prompt using all descriptions of
different downstream classes. However, the limited learning capacity may result
in (1) a failure to capture diverse aspects of the descriptions (e.g., shape,
color, and texture), and (2) a possible bias toward less informative attributes
that do not help distinguish between classes. In this paper, we introduce a
decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are
optimized using descriptions grouped by explicit causes (DVP-cse) or
unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual
prompts with a probabilistic reweighting matrix (PRM) that measures their
contributions to each downstream class. Theoretically, DVP lowers the empirical
risk bound. Experimentally, DVP outperforms baselines on average across 11
downstream datasets. Notably, the DVP-PRM integration enables insights into how
individual visual prompts influence classification decisions, providing a
probabilistic framework for understanding reprogramming. Our code is available
at https://github.com/tmlr-group/DecoupledVP.

</details>


### [223] [Optimistic critics can empower small actors](https://arxiv.org/abs/2506.01016)
*Olya Mastikhina,Dhruv Sreenivas,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: Actor-critic methods in deep reinforcement learning are typically based on symmetric architectures, but asymmetric setups with smaller actors have been proposed. Empirical investigations reveal that smaller actors often lead to performance degradation and overfit critics due to poor data collection from value underestimation. The critic plays a crucial role in addressing this issue. Techniques to mitigate value underestimation are explored to further research in asymmetric actor-critic methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to explore the implications of using asymmetric actor-critic architectures, specifically with smaller actors, as opposed to the more common symmetric architectures in deep reinforcement learning.

Method: The authors conduct broad empirical investigations and analyses on the effects of using smaller actors in actor-critic methods. They examine the reasons behind performance degradation and overfitting of critics when smaller actors are used, focusing on issues such as poor data collection and value underestimation.

Result: The results show that smaller actors generally result in performance degradation and overfit critics. Value underestimation was identified as one of the main causes for poor data collection, which leads to these negative outcomes.

Conclusion: The conclusion highlights the importance of the critic in mitigating the problems associated with smaller actors and suggests techniques to address value underestimation, opening up new avenues for research in asymmetric actor-critic methods.

Abstract: Actor-critic methods have been central to many of the recent advances in deep
reinforcement learning. The most common approach is to use symmetric
architectures, whereby both actor and critic have the same network topology and
number of parameters. However, recent works have argued for the advantages of
asymmetric setups, specifically with the use of smaller actors. We perform
broad empirical investigations and analyses to better understand the
implications of this and find that, in general, smaller actors result in
performance degradation and overfit critics. Our analyses suggest poor data
collection, due to value underestimation, as one of the main causes for this
behavior, and further highlight the crucial role the critic can play in
alleviating this pathology. We explore techniques to mitigate the observed
value underestimation, which enables further research in asymmetric
actor-critic methods.

</details>


### [224] [Taming LLMs by Scaling Learning Rates with Gradient Grouping](https://arxiv.org/abs/2506.01049)
*Siyuan Li,Juanxi Tian,Zedong Wang,Xin Jin,Zicheng Liu,Wentao Zhang,Dan Xu*

Main category: cs.LG

TL;DR: An optimizer wrapper called Scaling with Gradient Grouping (SGG) is introduced to improve adaptive learning rate estimation for large language models, providing training stability, faster convergence, and compatibility with fine-tuning techniques.


<details>
  <summary>Details</summary>
Motivation: Training large language models presents challenges such as instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning techniques, despite the use of adaptive optimizers like AdamW.

Method: SGG groups gradient statistics in each layer into clusters and applies cluster-specific scaling to calibrate learning rates for each parameter, combining group-wise constraints with per-parameter adaptation.

Result: Experiments demonstrate that SGG integrates well with existing optimizers, provides consistent performance improvements and faster convergence across different model sizes, and maintains stability with varying batch sizes and learning rates.

Conclusion: SGG is a robust choice for optimizing large language models, offering advantages in stability, convergence speed, and compatibility.

Abstract: Training large language models (LLMs) poses challenges due to their massive
scale and heterogeneous architectures. While adaptive optimizers like AdamW
help address gradient variations, they still struggle with efficient and
effective parameter-wise learning rate estimation, resulting in training
instability, slow convergence, and poor compatibility with parameter-efficient
fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient
Grouping (SGG), an optimizer wrapper that improves adaptive learning rate
estimation by dynamic grouping and group-specific scaling. SGG first groups
gradient statistics in each layer into clusters and then applies
cluster-specific scaling to calibrate learning rates for each parameter, thus
imposing collective group-wise constraints while maintaining precise
per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that
SGG integrates seamlessly with existing optimizers, and offers consistent gains
and faster convergence over baselines, with various model sizes. Its stability
across varying batch sizes and learning rates establishes SGG as a robust
choice for LLM optimization.

</details>


### [225] [A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections nor Strong Convexity](https://arxiv.org/abs/2506.01052)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate the finite-time convergence properties of Temporal Difference
(TD) learning with linear function approximation, a cornerstone algorithm in
reinforcement learning. While prior work has established convergence
guarantees, these results typically rely on the assumption that each iterate is
projected onto a bounded set or that the learning rate is set according to the
unknown strong convexity constant -- conditions that are both artificial and do
not match the current practice.
  In this paper, we challenge the necessity of such assumptions and present a
refined analysis of TD learning. We show that the simple projection-free
variant converges with a rate of
$\tilde{\mathcal{O}}(\frac{||\theta^*||^2_2}{\sqrt{T}})$, even in the presence
of Markovian noise. Our analysis reveals a novel self-bounding property of the
TD updates and exploits it to guarantee bounded iterates.

</details>


### [226] [No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks](https://arxiv.org/abs/2506.01054)
*Attila Szász,Balázs Bánhelyi,Márk Jelasity*

Main category: cs.LG

TL;DR: The abstract discusses how current state-of-the-art neural network verifiers fail to ensure the safety of deployed networks due to a gap between theoretical and practical soundness. The authors prove this by creating adversarial networks that exploit deployment features, showing vulnerabilities in existing verification methods.


<details>
  <summary>Details</summary>
Motivation: To highlight the inadequacy of current neural network verifiers in ensuring the safety of deployed systems by differentiating between theoretical and practical soundness.

Method: Proving that theoretical soundness does not imply practical soundness using interval analysis and variants. Creating adversarial networks to exploit features of deployment environments like floating point operations to mislead verifiers.

Result: All tested verifiers were found vulnerable to new deployment-specific attacks, demonstrating their lack of practical soundness.

Conclusion: State-of-the-art verifiers do not guarantee the safety of deployed neural networks as they fail to achieve practical soundness.

Abstract: The ultimate goal of verification is to guarantee the safety of deployed
neural networks. Here, we claim that all the state-of-the-art verifiers we are
aware of fail to reach this goal. Our key insight is that theoretical soundness
(bounding the full-precision output while computing with floating point) does
not imply practical soundness (bounding the floating point output in a
potentially stochastic environment). We prove this observation for the
approaches that are currently used to achieve provable theoretical soundness,
such as interval analysis and its variants. We also argue that achieving
practical soundness is significantly harder computationally. We support our
claims empirically as well by evaluating several well-known verification
methods. To mislead the verifiers, we create adversarial networks that detect
and exploit features of the deployment environment, such as the order and
precision of floating point operations. We demonstrate that all the tested
verifiers are vulnerable to our new deployment-specific attacks, which proves
that they are not practically sound.

</details>


### [227] [XAI-Units: Benchmarking Explainability Methods with Unit Tests](https://arxiv.org/abs/2506.01059)
*Jun Rui Lee,Sadegh Emami,Michael David Hollins,Timothy C. H. Wong,Carlos Ignacio Villalobos Sánchez,Francesca Toni,Dekai Zhang,Adam Dejl*

Main category: cs.LG

TL;DR: Feature attribution methods in XAI often conflict. The paper introduces XAI-Units, an open-source benchmark to objectively evaluate FA methods against known model behaviors using synthetic datasets and models.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in determining which feature attribution methods produce suitable explanations when there's no ground truth or in-depth knowledge about the model.

Method: Introduced XAI-Units, an open-source benchmark with paired datasets and models having known internal mechanisms along with built-in evaluation metrics for systematic experimentation.

Result: XAI-Units allows streamlined systematic experimentation revealing how FA methods perform against different types of model reasoning and enables objective comparison of FA methods.

Conclusion: XAI-Units provides a reliable way to compare feature attribution methods and evaluates them against diverse model behaviors.

Abstract: Feature attribution (FA) methods are widely used in explainable AI (XAI) to
help users understand how the inputs of a machine learning model contribute to
its outputs. However, different FA models often provide disagreeing importance
scores for the same model. In the absence of ground truth or in-depth knowledge
about the inner workings of the model, it is often difficult to meaningfully
determine which of the different FA methods produce more suitable explanations
in different contexts. As a step towards addressing this issue, we introduce
the open-source XAI-Units benchmark, specifically designed to evaluate FA
methods against diverse types of model behaviours, such as feature
interactions, cancellations, and discontinuous outputs. Our benchmark provides
a set of paired datasets and models with known internal mechanisms,
establishing clear expectations for desirable attribution scores. Accompanied
by a suite of built-in evaluation metrics, XAI-Units streamlines systematic
experimentation and reveals how FA methods perform against distinct, atomic
kinds of model reasoning, similar to unit tests in software engineering.
Crucially, by using procedurally generated models tied to synthetic datasets,
we pave the way towards an objective and reliable comparison of FA methods.

</details>


### [228] [Reconsidering LLM Uncertainty Estimation Methods in the Wild](https://arxiv.org/abs/2506.01114)
*Yavuz Bakman,Duygu Nur Yaldiz,Sungmin Kang,Tuo Zhang,Baturalp Buyukates,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）不确定性估计（UE）方法对于检测幻觉至关重要。本文系统地评估了19种UE方法在实际应用中的四个关键方面，包括对决策阈值选择的敏感性、对查询变换的鲁棒性、在长篇生成中的适用性以及处理单个查询多个UE分数的策略。结果表明，这些方法在数据分布变化时对阈值选择高度敏感，对抗性提示较为脆弱，且在长篇生成中仍有改进空间。同时，测试时集成多个UE分数可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 近年来，LLM UE方法已成为检测幻觉的重要工具，但大多数现有研究仅在孤立的简短问答设置中使用阈值无关指标（如AUROC或PRR）进行评估。然而，在实际部署中，UE方法面临许多挑战，需要系统地研究其在真实场景中的表现。

Method: 本文评估了19种UE方法在以下四个方面：1. 对决策阈值选择的敏感性；2. 对查询变换（如拼写错误、对抗性提示和先前聊天历史）的鲁棒性；3. 在长篇生成中的适用性；4. 测试时处理单个查询多个UE分数的策略。

Result: 大多数UE方法在校准数据集分布发生变化时对阈值选择高度敏感；它们通常对之前的聊天历史和拼写错误表现出鲁棒性，但对抗性提示非常脆弱；通过不同策略，现有UE方法可以适应长篇生成，但仍需改进；集成多个UE分数在测试时能显著提高性能。

Conclusion: 本文系统地分析了UE方法在实际应用中的表现，揭示了其优势与不足，并提出了集成多个UE分数作为潜在的改进策略。

Abstract: Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a
crucial tool for detecting hallucinations in recent years. While numerous UE
methods have been proposed, most existing studies evaluate them in isolated
short-form QA settings using threshold-independent metrics such as AUROC or
PRR. However, real-world deployment of UE methods introduces several
challenges. In this work, we systematically examine four key aspects of
deploying UE methods in practical settings. Specifically, we assess (1) the
sensitivity of UE methods to decision threshold selection, (2) their robustness
to query transformations such as typos, adversarial prompts, and prior chat
history, (3) their applicability to long-form generation, and (4) strategies
for handling multiple UE scores for a single query. Our evaluations on 19 UE
methods reveal that most of them are highly sensitive to threshold selection
when there is a distribution shift in the calibration dataset. While these
methods generally exhibit robustness against previous chat history and typos,
they are significantly vulnerable to adversarial prompts. Additionally, while
existing UE methods can be adapted for long-form generation through various
strategies, there remains considerable room for improvement. Lastly, ensembling
multiple UE scores at test time provides a notable performance boost, which
highlights its potential as a practical improvement strategy. Code is available
at: https://github.com/duygunuryldz/uncertainty_in_the_wild.

</details>


### [229] [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
*Yihe Dong,Lorenzo Noci,Mikhail Khodak,Mufan Li*

Main category: cs.LG

TL;DR: 尽管自注意力机制在Transformer架构中至关重要，但即使完全随机固定注意力系数的简化模型MixiT也能在多种任务上匹配完全训练的Transformer性能。这表明Transformer的成功不仅依赖于自注意力机制，其他组件如MLP层同样重要。此外，冻结查询和键投影器的注意力机制仍能形成特定电路（如归纳头），并在语言建模任务中表现良好。整体而言，架构的异质性对于解决不同类别的任务至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨Transformer架构中的自注意力机制对性能提升的具体贡献，质疑其在多大程度上以及哪些方面是性能提升的关键因素。

Method: 通过对比标准Transformer与变体模型（冻结MLP层或注意力投影器）的性能，并引入MixiT模型（固定随机注意力系数），以隔离并分析自注意力机制的作用。

Result: MixiT在涉及基础算术和记忆的任务中表现出与完全训练的Transformer相当的性能，但在需要输入依赖注意力系数的任务中表现较差。冻结查询和键投影器的注意力机制能够形成归纳头，并在语言建模任务中表现良好。

Conclusion: Transformer架构的成功不仅依赖于自注意力机制，其他组件如MLP层同样重要。架构的异质性为解决不同任务提供了互补的归纳偏差。

Abstract: The Transformer architecture is central to the success of modern Large
Language Models (LLMs), in part due to its surprising ability to perform a wide
range of algorithmic tasks -- including mathematical reasoning, memorization,
and retrieval -- using only gradient-based training on next-token prediction.
While the core component of a Transformer is the self-attention mechanism, we
question how much, and which aspects, of the performance gains can be
attributed to it. To this end, we compare standard Transformers to variants in
which either the multi-layer perceptron (MLP) layers or the attention
projectors (queries and keys) are frozen at initialization. To further isolate
the contribution of attention, we introduce MixiT -- the Mixing Transformer --
a simplified, principled model in which the attention coefficients are entirely
random and fixed at initialization, eliminating any input-dependent computation
or learning in attention. Surprisingly, we find that MixiT matches the
performance of fully trained Transformers on various algorithmic tasks,
especially those involving basic arithmetic or focusing heavily on
memorization. For retrieval-based tasks, we observe that having input-dependent
attention coefficients is consistently beneficial, while MixiT underperforms.
We attribute this failure to its inability to form specialized circuits such as
induction heads -- a specific circuit known to be crucial for learning and
exploiting repeating patterns in input sequences. Even more interestingly, we
find that attention with frozen key and query projectors is not only able to
form induction heads, but can also perform competitively on language modeling.
Our results underscore the importance of architectural heterogeneity, where
distinct components contribute complementary inductive biases crucial for
solving different classes of tasks.

</details>


### [230] [Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation](https://arxiv.org/abs/2506.01121)
*Jacob K. Christopher,Michael Cardei,Jinhao Liang,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: Diffusion models, despite their capabilities, face challenges in safety-critical applications due to the need for compliance with various constraints. This paper introduces Neuro-Symbolic Diffusion (NSD), a framework that combines diffusion steps with symbolic optimization, enabling the generation of consistent samples under user-defined constraints for both continuous and discrete outputs. NSD addresses safety, data scarcity, and out-of-domain generalization challenges.


<details>
  <summary>Details</summary>
Motivation: To integrate diffusion models into safety-critical or scientifically rigorous applications by ensuring compliance with stringent physical, structural, and operational constraints.

Method: Neuro-Symbolic Diffusion (NSD) interleaves diffusion steps with symbolic optimization, allowing for the generation of certifiably consistent samples under user-defined functional and logic constraints. It applies to both standard and discrete diffusion models.

Result: Demonstrated ability to generate safe outputs (e.g., non-toxic molecules, collision-free trajectories), address data scarcity in fields like drug discovery, and enable out-of-domain generalization beyond the training distribution.

Conclusion: NSD provides a novel solution for incorporating diffusion models into applications requiring strict compliance with various constraints, expanding their potential use in safety-critical and scientifically rigorous domains.

Abstract: Despite the remarkable generative capabilities of diffusion models, their
integration into safety-critical or scientifically rigorous applications
remains hindered by the need to ensure compliance with stringent physical,
structural, and operational constraints. To address this challenge, this paper
introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves
diffusion steps with symbolic optimization, enabling the generation of
certifiably consistent samples under user-defined functional and logic
constraints. This key feature is provided for both standard and discrete
diffusion models, enabling, for the first time, the generation of both
continuous (e.g., images and trajectories) and discrete (e.g., molecular
structures and natural language) outputs that comply with constraints. This
ability is demonstrated on tasks spanning three key challenges: (1) Safety, in
the context of non-toxic molecular generation and collision-free trajectory
optimization; (2) Data scarcity, in domains such as drug discovery and
materials engineering; and (3) Out-of-domain generalization, where enforcing
symbolic constraints allows adaptation beyond the training distribution.

</details>


### [231] [Slow Feature Analysis on Markov Chains from Goal-Directed Behavior](https://arxiv.org/abs/2506.01145)
*Merlin Schüler,Eddie Seabrook,Laurenz Wiskott*

Main category: cs.LG

TL;DR: This paper investigates the impact of state occupancy differences caused by goal-directed behavior on value-function approximation using optimal slow features in ergodic Markov chains, and evaluates three correction routes to mitigate scaling effects.


<details>
  <summary>Details</summary>
Motivation: Goal-directed behavior in reinforcement learning settings leads to significant differences in state occupancy between states close to or far from a reward location. The author aims to understand how these differences affect value-function approximation when using Slow Feature Analysis (SFA).

Method: The work adopts the perspective of optimal slow features on ergodic Markov chains to analyze the effects of state occupancy differences on value-function approximation. Three correction routes are evaluated for their potential to alleviate detrimental scaling effects. Additionally, the special case of goal-averse behavior is considered.

Result: The analysis reveals insights into how state occupancy differences impact value-function approximation in an idealized setting. The evaluation of the three correction routes provides guidance on mitigating detrimental scaling effects.

Conclusion: Using optimal slow features with consideration of state occupancy differences can improve value-function approximation in reinforcement learning settings. Correction methods offer promising ways to address scaling issues.

Abstract: Slow Feature Analysis is a unsupervised representation learning method that
extracts slowly varying features from temporal data and can be used as a basis
for subsequent reinforcement learning. Often, the behavior that generates the
data on which the representation is learned is assumed to be a uniform random
walk. Less research has focused on using samples generated by goal-directed
behavior, as commonly the case in a reinforcement learning setting, to learn a
representation. In a spatial setting, goal-directed behavior typically leads to
significant differences in state occupancy between states that are close to a
reward location and far from a reward location.
  Through the perspective of optimal slow features on ergodic Markov chains,
this work investigates the effects of these differences on value-function
approximation in an idealized setting. Furthermore, three correction routes,
which can potentially alleviate detrimental scaling effects, are evaluated and
discussed. In addition, the special case of goal-averse behavior is considered.

</details>


### [232] [Earley-Driven Dynamic Pruning for Efficient Structured Decoding](https://arxiv.org/abs/2506.01151)
*Xintong Sun,Chi Wei,Minghao Tian,Shiwen Ni*

Main category: cs.LG

TL;DR: In order to solve the problem of large overhead in constrained decoding, this paper proposes ZapFormat, a new dynamic pruning strategy based on the Earley algorithm. It reduces memory usage and accelerates structured generation through state cache. The method is implemented in Formatron, which significantly improves inference speed without reducing output precision.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have difficulty ensuring their outputs strictly conform to structural or grammatical constraints, especially in function calls and domain-specific language (DSL) generation. Existing constrained decoding methods with context-free grammar incur significant overheads due to the need to check all tokens' validity at each decoding step.

Method: The paper introduces ZapFormat, a dynamic pruning strategy based on the Earley algorithm that identifies and removes invalid or redundant Earley states in real-time. This strategy reduces memory occupation and allows for the use of a state cache to speed up structured generations. ZapFormat is implemented in a new constrained decoding engine called Formatron, which incorporates existing optimizations.

Result: Through comprehensive experiments on structured generation tasks such as JSON generation, JSON Schema validation, and semantic parsing, Formatron consistently maintains high-precision compliant outputs while achieving significant improvements in inference speed, up to 2x faster than state-of-the-art implementations.

Conclusion: ZapFormat and Formatron effectively address the challenge of large overheads in constrained decoding by reducing memory usage and improving inference speed. Formatron is applicable across various LLM architectures and has been released as open source.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring
their outputs conform to strict structural or grammatical constraints remains
challenging, which is critical in function calls and domain-specific language
(DSL) generation. Constrained decoding with context-free grammar is a flexible
approach to guarantee LLMs' adherence to a specific format by dynamically
building a token logits mask. However, creating this mask requires checking the
validity of all tokens in the LLM vocabulary at every decoding step, which
often incurs significant overheads in existing constrained decoding engines. To
address this challenge, we propose $\textbf{ZapFormat}$, a novel
$\textbf{dynamic pruning}$ strategy based on the Earley algorithm that
identifies and eliminates invalid or redundant Earley states in real-time,
significantly reducing memory occupation of the Earley algorithm's states. This
further enables us to use a state cache to speed up structured generations on a
large number of queries. We implemented ZapFormat in a new constrained decoding
engine called Formatron which also incorporates existing optimizations. Through
comprehensive experiments on structured generation tasks, including JSON
generation, JSON Schema validation, and semantic parsing, we demonstrate that
Formatron not only $\textbf{consistently maintains}$ high-precision compliant
outputs but also achieves $\textbf{significant improvements}$ in inference
speed up to 2x compared to state-of-the-art implementations. More importantly,
Formatron is generally applicable across various LLM architectures. We release
Formatron as open source at https://github.com/Dan-wanna-M/formatron.

</details>


### [233] [Weight-Space Linear Recurrent Neural Networks](https://arxiv.org/abs/2506.01153)
*Roussel Desmond Nzoyem,Nawid Keshtmand,Idriss Tsayem,David A. W. Barton,Tom Deakin*

Main category: cs.LG

TL;DR: This paper introduces WARP, a new framework that integrates weight-space learning with linear recurrence to redefine sequence modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of conventional RNNs which collapse temporal dynamics into fixed-dimensional hidden states by introducing a novel framework that explicitly parametrizes the hidden state as the weights of a distinct root neural network.

Method: WARP (Weight-space Adaptive Recurrent Prediction) unifies weight-space learning with linear recurrence. It explicitly parametrizes the hidden state as the weights of a distinct root neural network, promoting higher-resolution memory, gradient-free adaptation at test-time, and seamless integration of domain-specific physical priors.

Result: Empirical validation shows that WARP matches or surpasses state-of-the-art baselines on diverse classification tasks, spanning synthetic benchmarks to real-world datasets. Experiments across sequential image completion, dynamical system reconstruction, and multivariate time series forecasting demonstrate its expressiveness and generalization capabilities.

Conclusion: WARP's weight trajectories offer valuable insights into the model's inner workings. Ablation studies confirm the architectural necessity of key components, solidifying weight-space linear RNNs as a transformative paradigm for adaptive machine intelligence.

Abstract: We introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet
powerful framework that unifies weight-space learning with linear recurrence to
redefine sequence modeling. Unlike conventional recurrent neural networks
(RNNs) which collapse temporal dynamics into fixed-dimensional hidden states,
WARP explicitly parametrizes the hidden state as the weights of a distinct root
neural network. This formulation promotes higher-resolution memory,
gradient-free adaptation at test-time, and seamless integration of
domain-specific physical priors. Empirical validation shows that WARP matches
or surpasses state-of-the-art baselines on diverse classification tasks,
spanning synthetic benchmarks to real-world datasets. Furthermore, extensive
experiments across sequential image completion, dynamical system
reconstruction, and multivariate time series forecasting demonstrate its
expressiveness and generalization capabilities. Critically, WARP's weight
trajectories offer valuable insights into the model's inner workings. Ablation
studies confirm the architectural necessity of key components, solidifying
weight-space linear RNNs as a transformative paradigm for adaptive machine
intelligence.

</details>


### [234] [FORT: Forward-Only Regression Training of Normalizing Flows](https://arxiv.org/abs/2506.01158)
*Danyal Rehman,Oscar Davis,Jiarui Lu,Jian Tang,Michael Bronstein,Yoshua Bengio,Alexander Tong,Avishek Joey Bose*

Main category: cs.LG

TL;DR: The paper proposes Forward-Only Regression Training (FORT), a scalable training method for normalizing flows that avoids expensive computations and enhances performance in generating high-quality samples, particularly useful for molecular system sampling.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of high computational cost in generating high-quality samples with exact likelihoods using conventional maximum likelihood training.

Method: Introduce FORT, an ℓ2-regression objective that maps prior samples to chosen targets without computing the change of variable formula. Utilize targets from optimal transport and pre-trained continuous-time normalizing flows (CNF).

Result: FORT enables larger-scale training, surpassing maximum likelihood training in performance and stability. It successfully performs equilibrium conformation sampling for alanine dipeptide, tripeptide, and tetrapeptide.

Conclusion: FORT provides a scalable and efficient alternative for training normalizing flows, expanding the range of trainable architectures and demonstrating effectiveness in scientific applications like molecular sampling.

Abstract: Simulation-free training frameworks have been at the forefront of the
generative modelling revolution in continuous spaces, leading to neural
dynamical systems that encompass modern large-scale diffusion and flow matching
models. Despite the scalability of training, the generation of high-quality
samples and their corresponding likelihood under the model requires expensive
numerical simulation -- inhibiting adoption in numerous scientific applications
such as equilibrium sampling of molecular systems. In this paper, we revisit
classical normalizing flows as one-step generative models with exact
likelihoods and propose a novel, scalable training objective that does not
require computing the expensive change of variable formula used in conventional
maximum likelihood training. We propose Forward-Only Regression Training
(FORT), a simple $\ell_2$-regression objective that maps prior samples under
our flow to specifically chosen targets. We demonstrate that FORT supports a
wide class of targets, such as optimal transport targets and targets from
pre-trained continuous-time normalizing flows (CNF). We further demonstrate
that by using CNF targets, our one-step flows allow for larger-scale training
that exceeds the performance and stability of maximum likelihood training,
while unlocking a broader class of architectures that were previously
challenging to train. Empirically, we elucidate that our trained flows can
perform equilibrium conformation sampling in Cartesian coordinates of alanine
dipeptide, alanine tripeptide, and alanine tetrapeptide.

</details>


### [235] [Accelerated Learning with Linear Temporal Logic using Differentiable Simulation](https://arxiv.org/abs/2506.01167)
*Alper Kamil Bozkurt,Calin Belta,Ming C. Lin*

Main category: cs.LG

TL;DR: The paper proposes a method that integrates LTL with differentiable simulators for efficient gradient-based learning, introducing soft labeling to mitigate sparse-reward issue without compromising correctness.


<details>
  <summary>Details</summary>
Motivation: To ensure learned controllers comply with safety and reliability requirements in real-world reinforcement learning settings is challenging. Traditional methods inadequately capture trajectory requirements or result in overly conservative behaviors, while recent studies using LTL have issues with sparse rewards or potential correctness compromise from dense heuristic-based rewards.

Method: The proposed method integrates LTL with differentiable simulators, enabling efficient gradient-based learning directly from LTL specifications by coupling with differentiable paradigms. It introduces soft labeling to achieve differentiable rewards and states, addressing the sparse-reward issue intrinsic to LTL.

Result: Experiments validate the efficacy of the method, showing significant improvements in both reward attainment and training time compared to discrete methods.

Conclusion: The integration of LTL with differentiable simulators and the introduction of soft labeling provide an effective solution for ensuring safety and reliability in real-world reinforcement learning.

Abstract: To ensure learned controllers comply with safety and reliability requirements
for reinforcement learning in real-world settings remains challenging.
Traditional safety assurance approaches, such as state avoidance and
constrained Markov decision processes, often inadequately capture trajectory
requirements or may result in overly conservative behaviors. To address these
limitations, recent studies advocate the use of formal specification languages
such as linear temporal logic (LTL), enabling the derivation of
correct-by-construction learning objectives from the specified requirements.
However, the sparse rewards associated with LTL specifications make learning
extremely difficult, whereas dense heuristic-based rewards risk compromising
correctness. In this work, we propose the first method, to our knowledge, that
integrates LTL with differentiable simulators, facilitating efficient
gradient-based learning directly from LTL specifications by coupling with
differentiable paradigms. Our approach introduces soft labeling to achieve
differentiable rewards and states, effectively mitigating the sparse-reward
issue intrinsic to LTL without compromising objective correctness. We validate
the efficacy of our method through experiments, demonstrating significant
improvements in both reward attainment and training time compared to the
discrete methods.

</details>


### [236] [Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation](https://arxiv.org/abs/2506.01177)
*Andrew Smith,Erhan Guven*

Main category: cs.LG

TL;DR: Hybrid quantum-classical machine learning model BO-QGAN is developed and optimized using Bayesian optimization, showing significant improvements in drug discovery with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To leverage NISQ devices for drug discovery by optimizing hybrid quantum-classical machine learning architectures.

Method: Systematic optimization of the quantum-classical bridge architecture for GANs using multi-objective Bayesian optimization.

Result: BO-QGAN achieves a 2.27-fold higher DCS than prior quantum-hybrid benchmarks and 2.21-fold higher than classical baseline with over 60% fewer parameters.

Conclusion: The study provides empirically grounded architectural guidelines for hybrid models, promoting effective integration of quantum computers in pharmaceutical research.

Abstract: Hybrid quantum-classical machine learning offers a path to leverage noisy
intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model
architectures remain unclear. We systematically optimize the quantum-classical
bridge architecture for generative adversarial networks (GANs) in molecular
discovery using multi-objective Bayesian optimization. Our optimized model
(BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug
Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher
than the classical baseline, using over 60% fewer parameters. Key findings
favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits
sequentially, while classical architecture shows less sensitivity above a
minimum capacity. This work provides the first empirically grounded
architectural guidelines for hybrid models, enabling more effective integration
of current quantum computers into pharmaceutical research pipelines.

</details>


### [237] [Doubly Robust Alignment for Large Language Models](https://arxiv.org/abs/2506.01183)
*Erhan Xu,Kai Ye,Hongyi Zhou,Luhan Zhu,Francesco Quinzan,Chengchun Shi*

Main category: cs.LG

TL;DR: This paper proposes a doubly robust preference optimization algorithm for reinforcement learning from human feedback (RLHF) that addresses model misspecification and shows superior performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning from human feedback (RLHF) is sensitive to misspecifications in the underlying preference model, reference policy, or reward function, leading to undesirable fine-tuning.

Method: The authors propose a doubly robust preference optimization algorithm which remains consistent when either the preference model or the reference policy is correctly specified.

Result: The proposed algorithm demonstrates superior and more robust performance than state-of-the-art algorithms both theoretically and practically.

Conclusion: The doubly robust preference optimization algorithm addresses model misspecification issues in RLHF and outperforms existing methods.

Abstract: This paper studies reinforcement learning from human feedback (RLHF) for
aligning large language models with human preferences. While RLHF has
demonstrated promising results, many algorithms are highly sensitive to
misspecifications in the underlying preference model (e.g., the Bradley-Terry
model), the reference policy, or the reward function, resulting in undesirable
fine-tuning. To address model misspecification, we propose a doubly robust
preference optimization algorithm that remains consistent when either the
preference model or the reference policy is correctly specified (without
requiring both). Our proposal demonstrates superior and more robust performance
than state-of-the-art algorithms, both in theory and in practice. The code is
available at https://github.com/DRPO4LLM/DRPO4LLM

</details>


### [238] [FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA](https://arxiv.org/abs/2506.01194)
*Divyansh Jhunjhunwala,Arian Raje,Madan Ravi Ganesh,Chaithanya Kumar Mummadi,Chaoqun Dong,Jiawei Zhou,Wan-Yi Lin,Gauri Joshi,Zhenzhen Li*

Main category: cs.LG

TL;DR: LoRA在联邦学习中具有优势，但数据异质性仍然是一个挑战。通过任务算术和鲁棒主成分分析（Robust-PCA），提出了一种新的聚合算法FedRPCA，该算法在视觉和语言任务上表现出更高的准确性和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 受到模型合并特别是任务算术近期进展的启发，探索使用缩放平均来聚合客户端LoRA参数的方法。

Method: 首先观察到任务算术的直接应用由于客户端更新之间的高余弦相似性而无效。然后提出通过鲁棒主成分分析（Robust-PCA）将客户端LoRA更新分解为共同的低秩成分和客户端特定的稀疏成分。最后，提出的算法FedRPCA通过对低秩成分进行平均来整合共同知识，并对稀疏成分应用缩放平均以放大客户端特定知识。

Result: 在各种视觉和语言任务上的评估表明，该方法与竞争基线相比，实现了更高的最终准确性和更快的收敛速度。

Conclusion: FedRPCA是一种有效的聚合策略，可以解决LoRA基础联邦学习中的数据异质性问题，提高准确性和收敛速度。

Abstract: LoRA has emerged as one of the most promising fine-tuning techniques,
especially for federated learning (FL), since it significantly reduces
communication and computation costs at resource-constrained clients. However,
data heterogeneity remains a significant challenge for LoRA-based FL, and the
conventional aggregation strategy based on FedAvg suffers from slow convergence
and suboptimal accuracy. Motivated by recent advances in model merging,
particularly Task Arithmetic, we explore the idea of aggregating client LoRA
parameters using scaled averaging. We first observe that a naive application of
Task Arithmetic is ineffective due to the high cosine similarity between client
updates, indicating significant common knowledge in the updates across clients.
To address this issue, we propose decomposing client LoRA updates via Robust
Principal Component Analysis (Robust-PCA) into a common low-rank component and
client-specific sparse components. Our proposed algorithm FedRPCA aggregates
the low-rank components through averaging, consolidating common knowledge, and
applies scaled averaging to the sparse components to amplify client-specific
knowledge. We evaluate our approach across a variety of vision and language
tasks and demonstrate that it achieves higher final accuracy and faster
convergence compared to competing baselines.

</details>


### [239] [Multiresolution Analysis and Statistical Thresholding on Dynamic Networks](https://arxiv.org/abs/2506.01208)
*Raphaël Romero,Tijl De Bie,Nick Heard,Alexander Modell*

Main category: cs.LG

TL;DR: ANIE是一种自适应多分辨率框架，能自动识别网络结构演化的时长尺度，检测快速和渐进的变化。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理动态网络数据时存在时间分辨率与统计稳定性的权衡问题，且大多数方法依赖固定的时间分辨率，这在异常行为可能在多个时间尺度上出现的领域（如网络安全）中尤其成问题。

Method: ANIE通过两步实现：1) 估计节点行为的低维子空间；2) 推导一组新的经验亲和系数以量化潜在因素之间交互强度的变化，并支持跨时间尺度的结构变化统计检验。使用泊松过程建模交互。

Result: 实验表明ANIE可以适应适当的时间分辨率，捕捉尖锐的结构变化，同时对噪声保持鲁棒性。实际应用也展示了其相对于固定分辨率方法的优势。

Conclusion: ANIE提供了一种有效的多分辨率框架，解决了固定时间分辨率方法的局限性，能够自动识别网络结构演化的时间尺度并检测结构变化。

Abstract: Detecting structural change in dynamic network data has wide-ranging
applications. Existing approaches typically divide the data into time bins,
extract network features within each bin, and then compare these features over
time. This introduces an inherent tradeoff between temporal resolution and the
statistical stability of the extracted features. Despite this tradeoff,
reminiscent of time-frequency tradeoffs in signal processing, most methods rely
on a fixed temporal resolution. Choosing an appropriate resolution parameter is
typically difficult and can be especially problematic in domains like
cybersecurity, where anomalous behavior may emerge at multiple time scales. We
address this challenge by proposing ANIE (Adaptive Network Intensity
Estimation), a multi-resolution framework designed to automatically identify
the time scales at which network structure evolves, enabling the joint
detection of both rapid and gradual changes. Modeling interactions as Poisson
processes, our method proceeds in two steps: (1) estimating a low-dimensional
subspace of node behavior, and (2) deriving a set of novel empirical affinity
coefficients that quantify change in interaction intensity between latent
factors and support statistical testing for structural change across time
scales. We provide theoretical guarantees for subspace estimation and the
asymptotic behavior of the affinity coefficients, enabling model-based change
detection. Experiments on synthetic networks show that ANIE adapts to the
appropriate time resolution and is able to capture sharp structural changes
while remaining robust to noise. Furthermore, applications to real-world data
showcase the practical benefits of ANIE's multiresolution approach to detecting
structural change over fixed resolution methods.

</details>


### [240] [Dynamic Modes as Time Representation for Spatiotemporal Forecasting](https://arxiv.org/abs/2506.01212)
*Menglin Kong,Vincent Zhihao Zheng,Xudong Wang,Lijun Sun*

Main category: cs.LG

TL;DR: This paper proposes a data-driven time embedding method based on Dynamic Mode Decomposition (DMD) to model long-range seasonal dependencies in spatiotemporal forecasting tasks, which improves long-horizon forecasting accuracy and temporal generalization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling long-range seasonal dependencies in spatiotemporal forecasting without relying on explicit timestamps or hand-crafted time features.

Method: The proposed method uses DMD to extract temporal modes from observed data as time representations that can be integrated into deep spatiotemporal forecasting models.

Result: Experiments on urban mobility, highway traffic, and climate datasets show that the DMD-based embedding improves long-horizon forecasting accuracy, reduces residual correlation, and enhances temporal generalization.

Conclusion: The DMD-based time embedding is an effective, lightweight, model-agnostic approach for capturing complex multi-scale periodicity in spatiotemporal data.

Abstract: This paper introduces a data-driven time embedding method for modeling
long-range seasonal dependencies in spatiotemporal forecasting tasks. The
proposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal
modes directly from observed data, eliminating the need for explicit timestamps
or hand-crafted time features. These temporal modes serve as time
representations that can be seamlessly integrated into deep spatiotemporal
forecasting models. Unlike conventional embeddings such as time-of-day
indicators or sinusoidal functions, our method captures complex multi-scale
periodicity through spectral analysis of spatiotemporal data. Extensive
experiments on urban mobility, highway traffic, and climate datasets
demonstrate that the DMD-based embedding consistently improves long-horizon
forecasting accuracy, reduces residual correlation, and enhances temporal
generalization. The method is lightweight, model-agnostic, and compatible with
any architecture that incorporates time covariates.

</details>


### [241] [On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective](https://arxiv.org/abs/2506.01213)
*Ning Zhang,Henry Kenlay,Li Zhang,Mihai Cucuringu,Xiaowen Dong*

Main category: cs.LG

TL;DR: Graph Convolutional Neural Networks (GCNNs) are powerful but their stability under graph structure changes is not well understood. This paper proposes a new method to analyze model stability considering data distribution, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limited theoretical understanding of GCNNs' stability and sensitivity to small changes in graph structure, which hinders the development of robust models.

Method: The authors study how perturbations in graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability that considers a broad range of input data rather than just worst-case scenarios.

Result: Extensive experiments validate the theoretical findings, showing improvements in representation stability and resistance to adversarial attacks compared to existing baselines.

Conclusion: The proposed formulation demonstrates practical significance and emphasizes the importance of incorporating data distribution into stability analysis.

Abstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools
for analyzing graph-structured data, achieving remarkable success across
diverse applications. However, the theoretical understanding of the stability
of these models, i.e., their sensitivity to small changes in the graph
structure, remains in rather limited settings, hampering the development and
deployment of robust and trustworthy models in practice. To fill this gap, we
study how perturbations in the graph topology affect GCNN outputs and propose a
novel formulation for analyzing model stability. Unlike prior studies that
focus only on worst-case perturbations, our distribution-aware formulation
characterizes output perturbations across a broad range of input data. This
way, our framework enables, for the first time, a probabilistic perspective on
the interplay between the statistical properties of the node data and
perturbations in the graph topology. We conduct extensive experiments to
validate our theoretical findings and demonstrate their benefits over existing
baselines, in terms of both representation stability and adversarial attacks on
downstream tasks. Our results demonstrate the practical significance of the
proposed formulation and highlight the importance of incorporating data
distribution into stability analysis.

</details>


### [242] [Self-Refining Training for Amortized Density Functional Theory](https://arxiv.org/abs/2506.01225)
*Majdi Hassan,Cristian Gabellini,Hatem Helal,Dominique Beaini,Kirill Neklyudov*

Main category: cs.LG

TL;DR: The paper proposes a self-refining training strategy for deep learning models to predict DFT outputs, reducing dependency on large datasets and enabling simultaneous sampling and training.


<details>
  <summary>Details</summary>
Motivation: Density Functional Theory (DFT) is powerful but computationally expensive, especially at larger scales. While Deep Learning-based models can predict DFT outputs efficiently with large datasets, there is a need to reduce the dependency on such extensive pre-collected datasets.

Method: A self-refining training strategy is introduced where a deep-learning model is simultaneously trained to predict DFT outputs and sample molecular conformations used as training data. This method minimizes the variational upper bound on the KL-divergence between generated samples and the target Boltzmann distribution defined by the ground state energy.

Result: An extensive empirical study demonstrates the utility of the proposed scheme compared to models trained on pre-collected datasets. The implementation includes asynchronous training and sampling stages, allowing for simultaneous processes.

Conclusion: The proposed self-refining method effectively reduces reliance on large datasets while maintaining accuracy in predicting DFT outputs. The authors have open-sourced their optimized implementation.

Abstract: Density Functional Theory (DFT) allows for predicting all the chemical and
physical properties of molecular systems from first principles by finding an
approximate solution to the many-body Schr\"odinger equation. However, the cost
of these predictions becomes infeasible when increasing the scale of the energy
evaluations, e.g., when calculating the ground-state energy for simulating
molecular dynamics. Recent works have demonstrated that, for substantially
large datasets of molecular conformations, Deep Learning-based models can
predict the outputs of the classical DFT solvers by amortizing the
corresponding optimization problems. In this paper, we propose a novel method
that reduces the dependency of amortized DFT solvers on large pre-collected
datasets by introducing a self-refining training strategy. Namely, we propose
an efficient method that simultaneously trains a deep-learning model to predict
the DFT outputs and samples molecular conformations that are used as training
data for the model. We derive our method as a minimization of the variational
upper bound on the KL-divergence measuring the discrepancy between the
generated samples and the target Boltzmann distribution defined by the ground
state energy. To demonstrate the utility of the proposed scheme, we perform an
extensive empirical study comparing it with the models trained on the
pre-collected datasets. Finally, we open-source our implementation of the
proposed algorithm, optimized with asynchronous training and sampling stages,
which enables simultaneous sampling and training. Code is available at
https://github.com/majhas/self-refining-dft.

</details>


### [243] [Stress-Testing ML Pipelines with Adversarial Data Corruption](https://arxiv.org/abs/2506.01230)
*Jiongli Zhu,Geyang Xu,Felipe Lorenzi,Boris Glavic,Babak Salimi*

Main category: cs.LG

TL;DR: SAVAGE is a causally inspired framework that models realistic data-quality issues and discovers corruption patterns to degrade model performance, providing a tool for pipeline stress-testing and evaluating robustness methods.


<details>
  <summary>Details</summary>
Motivation: Structured data-quality issues can degrade the reliability of machine-learning pipelines. Regulators demand evidence that systems can withstand these errors, but current evaluations use random or overly simplistic corruptions.

Method: SAVAGE employs a bi-level optimization approach to identify vulnerable data subpopulations and fine-tune corruption severity using dependency graphs and flexible corruption templates, treating the full ML pipeline as a black box.

Result: Experiments show that even a small fraction (around 5 %) of structured corruptions identified by SAVAGE severely impacts model performance, exceeding random or manually crafted errors.

Conclusion: SAVAGE provides a practical tool for rigorous pipeline stress-testing, a benchmark for evaluating robustness methods, and guidance for designing more resilient data workflows.

Abstract: Structured data-quality issues, such as missing values correlated with
demographics, culturally biased labels, or systemic selection biases, routinely
degrade the reliability of machine-learning pipelines. Regulators now
increasingly demand evidence that high-stakes systems can withstand these
realistic, interdependent errors, yet current robustness evaluations typically
use random or overly simplistic corruptions, leaving worst-case scenarios
unexplored. We introduce SAVAGE, a causally inspired framework that (i)
formally models realistic data-quality issues through dependency graphs and
flexible corruption templates, and (ii) systematically discovers corruption
patterns that maximally degrade a target performance metric. SAVAGE employs a
bi-level optimization approach to efficiently identify vulnerable data
subpopulations and fine-tune corruption severity, treating the full ML
pipeline, including preprocessing and potentially non-differentiable models, as
a black box. Extensive experiments across multiple datasets and ML tasks (data
cleaning, fairness-aware learning, uncertainty quantification) demonstrate that
even a small fraction (around 5 %) of structured corruptions identified by
SAVAGE severely impacts model performance, far exceeding random or manually
crafted errors, and invalidating core assumptions of existing techniques. Thus,
SAVAGE provides a practical tool for rigorous pipeline stress-testing, a
benchmark for evaluating robustness methods, and actionable guidance for
designing more resilient data workflows.

</details>


### [244] [Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution](https://arxiv.org/abs/2506.01231)
*Wenhao Song,Xuan Wu,Bo Yang,You Zhou,Yubin Xiao,Yanchun Liang,Hongwei Ge,Heow Pueh Lee,Chunguo Wu*

Main category: cs.LG

TL;DR: This paper proposes the Gradient Contribution (GC) method to solve the weight coupling problem in Neural Architecture Search by computing cosine similarity of gradient directions and partitioning modules accordingly. It also introduces Unified Graph Neural Architecture Search (UGAS) framework that explores optimal combinations of MPNNs and GTs. Experiments show GC achieves SOTA performance in supernet partitioning quality and time efficiency, while UGAS+GC outperforms both manually designed GNNs and existing NAS methods.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot NAS methods addressing the weight coupling problem suffer from computational inefficiency and suboptimal partitioning schemes.

Method: The proposed GC method computes cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. The UGAS framework is also proposed to explore optimal combinations of MPNNs and GTs.

Result: GC achieves state-of-the-art performance in supernet partitioning quality and time efficiency. Architectures searched by UGAS+GC outperform both manually designed GNNs and those obtained by existing NAS methods.

Conclusion: The proposed GC method effectively solves the weight coupling problem and the UGAS framework successfully explores optimal combinations of MPNNs and GTs.

Abstract: To address the weight coupling problem, certain studies introduced few-shot
Neural Architecture Search (NAS) methods, which partition the supernet into
multiple sub-supernets. However, these methods often suffer from computational
inefficiency and tend to provide suboptimal partitioning schemes. To address
this problem more effectively, we analyze the weight coupling problem from a
novel perspective, which primarily stems from distinct modules in succeeding
layers imposing conflicting gradient directions on the preceding layer modules.
Based on this perspective, we propose the Gradient Contribution (GC) method
that efficiently computes the cosine similarity of gradient directions among
modules by decomposing the Vector-Jacobian Product during supernet
backpropagation. Subsequently, the modules with conflicting gradient directions
are allocated to distinct sub-supernets while similar ones are grouped
together. To assess the advantages of GC and address the limitations of
existing Graph Neural Architecture Search methods, which are limited to
searching a single type of Graph Neural Networks (Message Passing Neural
Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph
Neural Architecture Search (UGAS) framework, which explores optimal
combinations of MPNNs and GTs. The experimental results demonstrate that GC
achieves state-of-the-art (SOTA) performance in supernet partitioning quality
and time efficiency. In addition, the architectures searched by UGAS+GC
outperform both the manually designed GNNs and those obtained by existing NAS
methods. Finally, ablation studies further demonstrate the effectiveness of all
proposed methods.

</details>


### [245] [Neural Variance-aware Dueling Bandits with Deep Representation and Shallow Exploration](https://arxiv.org/abs/2506.01250)
*Youngmin Oh,Jinje Park,Taejin Paik,Jaemin Park*

Main category: cs.LG

TL;DR: This paper proposes variance-aware algorithms using neural networks for contextual dueling bandit problem, achieving sublinear regret theoretically and empirically.


<details>
  <summary>Details</summary>
Motivation: To solve the contextual dueling bandit problem with nonlinear utility functions, leveraging neural networks while balancing exploration-exploitation tradeoff effectively.

Method: Propose variance-aware algorithms that use neural networks to approximate nonlinear utility functions. Implement a variance-aware exploration strategy which considers uncertainty in pairwise comparisons based on gradients of the last layer's learnable parameters. Balance exploration-exploitation under UCB and TS frameworks.

Result: Achieve sublinear cumulative average regret of order $\bigol\lt(d \sqrt{\sum_{t=1}^T \sigma_t^2} + \sqrt{dT}\rt)$ for wide neural networks, validated both theoretically and empirically on synthetic and real-world tasks.

Conclusion: The proposed algorithms provide reasonable computational efficiency and outperform existing methods on various tasks.

Abstract: In this paper, we address the contextual dueling bandit problem by proposing
variance-aware algorithms that leverage neural networks to approximate
nonlinear utility functions. Our approach employs a \textit{variance-aware
exploration strategy}, which adaptively accounts for uncertainty in pairwise
comparisons while relying only on the gradients with respect to the learnable
parameters of the last layer. This design effectively balances the
exploration--exploitation tradeoff under both the Upper Confidence Bound (UCB)
and Thompson Sampling (TS) frameworks. As a result, under standard assumptions,
we establish theoretical guarantees showing that our algorithms achieve
sublinear cumulative average regret of order $\bigol\lt(d \sqrt{\sum_{t=1}^T
\sigma_t^2} + \sqrt{dT}\rt),$ for sufficiently wide neural networks, where $ d
$ is the contextual dimension, $ \sigma_t^2 $ the variance of comparisons at
round $ t $, and $ T $ the total number of rounds. We also empirically validate
that our approach offers reasonable computational efficiency and achieves
sublinear regret on both synthetic tasks with nonlinear utilities and
real-world tasks, outperforming existing methods.

</details>


### [246] [Protocol Models: Scaling Decentralized Training with Communication-Efficient Model Parallelism](https://arxiv.org/abs/2506.01260)
*Sameera Ramasinghe,Thalaiyasingam Ajanthan,Gil Avraham,Yan Zuo,Alexander Long*

Main category: cs.LG

TL;DR: 通过提出一种新颖的压缩算法，实现了在不降低收敛性的情况下对前向和后向传播过程进行高达99%的压缩，并利用递归结构在低资源环境下训练大规模模型。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型训练面临通信瓶颈问题，尤其是在去中心化设置下，现有的压缩技术无法很好地适用于模型并行场景。

Method: 提出了一种新的压缩算法，该算法可以同时压缩前向和后向传播过程中的数据。通过在Transformer网络中利用递归结构，预定义一个低维子空间来限制激活值和梯度，从而允许在后续层中进行完全重建。

Result: 该方法实现了高达100倍的通信效率提升，能够在低端GPU和普通互联网连接（如80Mbps）条件下训练十亿参数规模的模型，并且其收敛性能与使用100Gbps连接的集中式数据中心系统相当。

Conclusion: 所提出的压缩算法为在资源受限环境中训练大规模模型提供了一种有效解决方案，显著提高了通信效率，而不会影响模型的收敛性能。

Abstract: Scaling models has led to significant advancements in deep learning, but
training these models in decentralized settings remains challenging due to
communication bottlenecks. While existing compression techniques are effective
in data-parallel, they do not extend to model parallelism. Unlike data-parallel
training, where weight gradients are exchanged, model-parallel requires
compressing activations and activation gradients as they propagate through
layers, accumulating compression errors. We propose a novel compression
algorithm that compresses both forward and backward passes, enabling up to 99%
compression with no convergence degradation with negligible memory/compute
overhead. By leveraging a recursive structure in transformer networks, we
predefine a low-dimensional subspace to confine the activations and gradients,
allowing full reconstruction in subsequent layers. Our method achieves up to
100x improvement in communication efficiency and enables training
billion-parameter-scale models over low-end GPUs connected via consumer-grade
internet speeds as low as 80Mbps, matching the convergence of centralized
datacenter systems with 100Gbps connections with model parallel.

</details>


### [247] [The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning](https://arxiv.org/abs/2506.01261)
*Zhijie Xie,Shenghui Song*

Main category: cs.LG

TL;DR: In Federated Reinforcement Learning (FRL), traditional PPO has problems with data heterogeneity due to the order of actor and critic updates. This paper proposes FedRAC, which reverses the update order to address this issue.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to solve the problem in Federated Reinforcement Learning where Proximal Policy Optimization faces challenges related to data heterogeneity due to the conventional update order of its actor and critic.

Method: FedRAC is proposed, which reverses the update order of actor and critic (actor first, then critic) to eliminate the divergence of critics from different clients.

Result: Theoretical analysis shows that FedRAC's convergence bound is immune to data heterogeneity under certain conditions. Empirical results indicate higher cumulative rewards and faster convergence in various experiments, including classical RL environments and an autonomous driving scenario.

Conclusion: FedRAC effectively addresses the issue of data heterogeneity in Federated Reinforcement Learning by reversing the update order of actor and critic, leading to improved performance.

Abstract: In the context of Federated Reinforcement Learning (FRL), applying Proximal
Policy Optimization (PPO) faces challenges related to the update order of its
actor and critic due to the aggregation step occurring between successive
iterations. In particular, when local actors are updated based on local critic
estimations, the algorithm becomes vulnerable to data heterogeneity. As a
result, the conventional update order in PPO (critic first, then actor) may
cause heterogeneous gradient directions among clients, hindering convergence to
a globally optimal policy. To address this issue, we propose FedRAC, which
reverses the update order (actor first, then critic) to eliminate the
divergence of critics from different clients. Theoretical analysis shows that
the convergence bound of FedRAC is immune to data heterogeneity under mild
conditions, i.e., bounded level of heterogeneity and accurate policy
evaluation. Empirical results indicate that the proposed algorithm obtains
higher cumulative rewards and converges more rapidly in five experiments,
including three classical RL environments and a highly heterogeneous autonomous
driving scenario using the SUMO traffic simulator.

</details>


### [248] [TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment](https://arxiv.org/abs/2506.01290)
*Shunyu Wu,Dan Li,Haozheng Ye,Zhuomin Chen,Jiahui Zhou,Jian Lou,Zibin Zheng,See-Kiong Ng*

Main category: cs.LG

TL;DR: This paper proposes TSRating, a novel framework for rating time series data quality from diverse domains using LLMs knowledge and a dedicated rating model named TSRater with meta-learning and signSGD to ensure cross-domain adaptability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for high-quality time series data across various domains to ensure model performance. Existing methods can't efficiently rate diverse TS data due to neglecting domain differences.

Method: Propose TSRating framework based on LLMs' pretraining knowledge to discern TS data quality. Use prompts to elicit quality comparisons from LLMs, then fit TSRater model via inference on future samples. Develop meta-learning scheme and use signSGD for efficient training.

Result: TSRating shows superior performance in estimation accuracy, efficiency, and domain adaptability compared to baselines, verified through extensive experiments on eleven benchmark datasets across three TS tasks.

Conclusion: TSRating provides an effective solution for rating time series data quality across different domains, leveraging LLMs' knowledge and advanced training techniques.

Abstract: High-quality time series (TS) data are essential for ensuring TS model
performance, rendering research on rating TS data quality indispensable.
Existing methods have shown promising rating accuracy within individual
domains, primarily by extending data quality rating techniques such as
influence functions and Shapley values to account for temporal characteristics.
However, they neglect the fact that real-world TS data can span vastly
different domains and exhibit distinct properties, hampering the accurate and
efficient rating of diverse TS data. In this paper, we propose TSRating, a
novel and unified framework for rating the quality of time series data crawled
from diverse domains. TSRating is built on the assumption that LLMs inherit
ample knowledge, acquired during their extensive pretraining, enabling them to
comprehend and discern quality differences in diverse TS data. We verify this
assumption by devising a series of prompts to elicit quality comparisons from
LLMs for pairs of TS samples. We then fit a dedicated rating model, termed
TSRater, to convert the LLMs' judgments into efficient quality predictions via
TSRater's inference on future TS samples. To ensure cross-domain adaptability,
we develop a meta-learning scheme to train TSRater on quality comparisons
collected from nine distinct domains. To improve training efficiency, we employ
signSGD for inner-loop updates, thus circumventing the demanding computation of
hypergradients. Extensive experimental results on eleven benchmark datasets
across three time series tasks, each using both conventional TS models and TS
foundation models, demonstrate that TSRating outperforms baselines in terms of
estimation accuracy, efficiency, and domain adaptability.

</details>


### [249] [Recent Developments in GNNs for Drug Discovery](https://arxiv.org/abs/2506.01302)
*Zhengyu Fang,Xiaoge Zhang,Anyin Zhao,Xiao Li,Huiyuan Chen,Jing Li*

Main category: cs.LG

TL;DR: This paper reviews the role of Graph Neural Networks (GNNs) in computational drug discovery, including molecule generation, molecular property prediction, and drug-drug interaction prediction.


<details>
  <summary>Details</summary>
Motivation: To summarize recent developments and applications of GNNs in computational drug discovery to highlight their capabilities and explore current and future applications.

Method: Reviewing various molecular representations and categorizing existing GNN models based on input types and downstream application tasks.

Result: Compilation of a list of commonly used benchmark datasets for different applications and discussion of trends in this research area.

Conclusion: The paper concludes with discussions summarizing common trends in the use of GNNs in computational drug discovery.

Abstract: In this paper, we review recent developments and the role of Graph Neural
Networks (GNNs) in computational drug discovery, including molecule generation,
molecular property prediction, and drug-drug interaction prediction. By
summarizing the most recent developments in this area, we underscore the
capabilities of GNNs to comprehend intricate molecular patterns, while
exploring both their current and prospective applications. We initiate our
discussion by examining various molecular representations, followed by detailed
discussions and categorization of existing GNN models based on their input
types and downstream application tasks. We also collect a list of commonly used
benchmark datasets for a variety of applications. We conclude the paper with
brief discussions and summarize common trends in this important research area.

</details>


### [250] [Latent Structured Hopfield Network for Semantic Association and Retrieval](https://arxiv.org/abs/2506.01303)
*Chong Li,Xiangyang Xue,Jianfeng Feng,Taiping Zeng*

Main category: cs.LG

TL;DR: The paper introduces Latent Structured Hopfield Network (LSHN), a biologically inspired model that integrates continuous Hopfield attractor dynamics into an autoencoder architecture, achieving scalable and robust memory retrieval.


<details>
  <summary>Details</summary>
Motivation: Episodic memory enables humans to recall past experiences by associating semantic elements such as objects, locations, and time into coherent event representations. While large pretrained models have shown remarkable progress in modeling semantic memory, the mechanisms for forming associative structures that support episodic memory remain underexplored.

Method: Inspired by hippocampal CA3 dynamics and its role in associative memory, LSHN mimics the cortical-hippocampal pathway: a semantic encoder extracts compact latent representations, a latent Hopfield network performs associative refinement through attractor convergence, and a decoder reconstructs perceptual input.

Result: Experiments on MNIST, CIFAR-10, and a simulated episodic memory task demonstrate superior performance in recalling corrupted inputs under occlusion and noise, outperforming existing associative memory models.

Conclusion: The work provides a computational perspective on how semantic elements can be dynamically bound into episodic memory traces through biologically grounded attractor mechanisms.

Abstract: Episodic memory enables humans to recall past experiences by associating
semantic elements such as objects, locations, and time into coherent event
representations. While large pretrained models have shown remarkable progress
in modeling semantic memory, the mechanisms for forming associative structures
that support episodic memory remain underexplored. Inspired by hippocampal CA3
dynamics and its role in associative memory, we propose the Latent Structured
Hopfield Network (LSHN), a biologically inspired framework that integrates
continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN
mimics the cortical-hippocampal pathway: a semantic encoder extracts compact
latent representations, a latent Hopfield network performs associative
refinement through attractor convergence, and a decoder reconstructs perceptual
input. Unlike traditional Hopfield networks, our model is trained end-to-end
with gradient descent, achieving scalable and robust memory retrieval.
Experiments on MNIST, CIFAR-10, and a simulated episodic memory task
demonstrate superior performance in recalling corrupted inputs under occlusion
and noise, outperforming existing associative memory models. Our work provides
a computational perspective on how semantic elements can be dynamically bound
into episodic memory traces through biologically grounded attractor mechanisms.

</details>


### [251] [Energy Considerations for Large Pretrained Neural Networks](https://arxiv.org/abs/2506.01311)
*Leo Mei,Mark Stamp*

Main category: cs.LG

TL;DR: 通过量化未压缩和压缩模型的能量使用，研究压缩减少电力消耗的方法。对于9个预训练模型，使用三种压缩技术：隐写容量减少、剪枝和低秩分解。结果表明，隐写容量减少在几乎所有情况下都提供了主要的好处，而剪枝和低秩分解在能量使用或其他相关统计方面没有显著改进。


<details>
  <summary>Details</summary>
Motivation: 复杂神经网络架构性能卓越，但需要大量计算资源并消耗大量电力，对环境有潜在影响。虽然大型预训练模型存在大量冗余，但之前的研究主要集中在压缩模型以保持相当的性能，而对电力消耗的直接影响关注较少。

Method: 考虑9个不同预训练模型，大小从8M参数到138M参数。首先无压缩地训练每个模型，并记录电力使用和所需时间以及其他相关统计。然后应用三种压缩技术：隐写容量减少、剪枝和低秩分解。在每种情况下再次测量电力使用、训练时间和模型准确性等。

Result: 发现剪枝和低秩分解在能量使用或其他相关统计方面没有显著改进，而隐写容量减少在几乎所有情况下都提供了主要好处。

Conclusion: 隐写容量减少是减少电力消耗的有效方法，值得进一步研究和应用。

Abstract: Increasingly complex neural network architectures have achieved phenomenal
performance. However, these complex models require massive computational
resources that consume substantial amounts of electricity, which highlights the
potential environmental impact of such models. Previous studies have
demonstrated that substantial redundancies exist in large pre-trained models.
However, previous work has primarily focused on compressing models while
retaining comparable model performance, and the direct impact on electricity
consumption appears to have received relatively little attention. By
quantifying the energy usage associated with both uncompressed and compressed
models, we investigate compression as a means of reducing electricity
consumption. We consider nine different pre-trained models, ranging in size
from 8M parameters to 138M parameters. To establish a baseline, we first train
each model without compression and record the electricity usage and time
required during training, along with other relevant statistics. We then apply
three compression techniques: Steganographic capacity reduction, pruning, and
low-rank factorization. In each of the resulting cases, we again measure the
electricity usage, training time, model accuracy, and so on. We find that
pruning and low-rank factorization offer no significant improvements with
respect to energy usage or other related statistics, while steganographic
capacity reduction provides major benefits in almost every case. We discuss the
significance of these findings.

</details>


### [252] [T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning](https://arxiv.org/abs/2506.01317)
*Yanjun Fu,Faisal Hamman,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 提出了一种新的数据选择框架T-SHIRT，通过仅包含信息丰富的标记进行质量评估，并促进稳健可靠的样本。使用T-SHIRT在精简数据集上训练的模型，在八个基准测试中的平均表现比使用完整数据集训练的模型高出5.48分。


<details>
  <summary>Details</summary>
Motivation: 现有的数据选择方法存在两个主要限制：(i) 它们在样本级别评估质量，忽略标记级别的信息性；(ii) 忽视评分方法的稳健性，常因表面的词汇特征选择样本而非其真实质量。

Method: 提出了Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT)，一种新的数据选择框架，引入了一种新评分方法，只将信息丰富的标记纳入质量评估，并且优先选择稳健可靠的样本，即那些邻近样本也表现出高质量且局部不一致较少的样本。

Result: 使用T-SHIRT在精简数据集（原数据集大小的5%）上进行指令微调的模型，在八个基准测试中平均表现优于使用整个大规模数据集训练的模型5.48分。该方法在各种LLMs和训练集规模下，始终超越现有的最先进的数据选择技术，同时保持成本效益和高效。例如，使用GPT-2计算得分，可以在单个GPU上处理52k样本的数据集只需40分钟。

Conclusion: T-SHIRT是一种有效的数据选择框架，可以提高模型性能，同时显著减少所需数据量，具有成本效益和高效性。

Abstract: Instruction tuning is essential for Large Language Models (LLMs) to
effectively follow user instructions. To improve training efficiency and reduce
data redundancy, recent works use LLM-based scoring functions, e.g.,
Instruction-Following Difficulty (IFD), to select high-quality
instruction-tuning data with scores above a threshold. While these data
selection methods often lead to models that can match or even exceed the
performance of models trained on the full datasets, we identify two key
limitations: (i) they assess quality at the sample level, ignoring token-level
informativeness; and (ii) they overlook the robustness of the scoring method,
often selecting a sample due to superficial lexical features instead of its
true quality. In this work, we propose Token-Selective HIeRarchical Data
Selection for Instruction Tuning (T-SHIRT), a novel data selection framework
that introduces a new scoring method to include only informative tokens in
quality evaluation and also promotes robust and reliable samples whose
neighbors also show high quality with less local inconsistencies. We
demonstrate that models instruction-tuned on a curated dataset (only 5% of the
original size) using T-SHIRT can outperform those trained on the entire
large-scale dataset by up to 5.48 points on average across eight benchmarks.
Across various LLMs and training set scales, our method consistently surpasses
existing state-of-the-art data selection techniques, while also remaining both
cost-effective and highly efficient. For instance, by using GPT-2 for score
computation, we are able to process a dataset of 52k samples using 40 minutes
on a single GPU.

</details>


### [253] [Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack](https://arxiv.org/abs/2506.01318)
*SeungBum Ha,Saerom Park,Sung Whan Yoon*

Main category: cs.LG

TL;DR: The paper addresses two critical blind spots in machine unlearning (MU): over-unlearning and relearning attacks. It proposes a new metric OU@{ε} for over-unlearning, exposes the Prototypical Relearning Attack, and introduces Spotter, a solution to mitigate both issues effectively.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning techniques fail to address over-unlearning, which harms retained data near the forget set, and are vulnerable to relearning attacks that restore forgotten knowledge.

Method: The authors derive a metric called OU@{ε} to measure over-unlearning effects. They also introduce the concept of Prototypical Relearning Attack as a threat to MU. To handle these problems, they propose Spotter, which includes a masked knowledge-distillation penalty and an intra-class dispersion loss.

Result: Spotter significantly reduces OU@{ε} to below 0.05X of the baseline, drives forget accuracy to 0%, preserves retain set accuracy within 1% difference from original, and defends against prototype-attacks by keeping forget set accuracy under 1%. These results validate its effectiveness without needing access to retained data.

Conclusion: Spotter is demonstrated to be an effective and practical solution to counteract the blind spots in machine unlearning, including over-unlearning and relearning attacks.

Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a
trained model without costly retraining, yet the existing techniques overlook
two critical blind spots: "over-unlearning" that deteriorates retained data
near the forget set, and post-hoc "relearning" attacks that aim to resurrect
the forgotten knowledge. We first derive the over-unlearning metric
OU@{\epsilon}, which represents the collateral damage to the nearby region of
the forget set, where the over-unlearning mainly appears. Next, we expose an
unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,
which exploits the per-class prototype of the forget class with just a few
samples, and easily restores the pre-unlearning performance. To counter both
blind spots, we introduce Spotter, a plug-and-play objective that combines (i)
a masked knowledge-distillation penalty on the nearby region of forget set to
suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters
forget-class embeddings, neutralizing prototypical relearning attacks. On
CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the
0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the
retain set within 1% of difference with the original, and denies the
prototype-attack by keeping the forget set accuracy within <1%, without
accessing retained data. It confirms that Spotter is a practical remedy of the
unlearning's blind spots.

</details>


### [254] [$Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models](https://arxiv.org/abs/2506.01320)
*Taehoon Yoon,Yunhong Min,Kyeongmin Yeo,Minhyuk Sung*

Main category: cs.LG

TL;DR: The paper presents Ψ-Sampler, a framework that uses pCNL for initial particle sampling in SMC to improve inference-time reward alignment with score-based generative models.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing methods that initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions during inference-time reward alignment with score-based generative models.

Method: Introduced Ψ-Sampler framework which incorporates pCNL-based initial particle sampling within an SMC approach. This method initializes particles from the reward-aware posterior using the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, allowing for efficient and scalable posterior sampling in high-dimensional latent spaces.

Result: Demonstrated consistent performance improvements across various reward alignment tasks such as layout-to-image generation, quantity-aware generation, and aesthetic-preference generation.

Conclusion: Ψ-Sampler significantly enhances alignment performance by enabling more effective inference-time reward alignment through improved initialization from the reward-aware posterior.

Abstract: We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based
initial particle sampling for effective inference-time reward alignment with a
score-based generative model. Inference-time reward alignment with score-based
generative models has recently gained significant traction, following a broader
paradigm shift from pre-training to post-training optimization. At the core of
this trend is the application of Sequential Monte Carlo (SMC) to the denoising
process. However, existing methods typically initialize particles from the
Gaussian prior, which inadequately captures reward-relevant regions and results
in reduced sampling efficiency. We demonstrate that initializing from the
reward-aware posterior significantly improves alignment performance. To enable
posterior sampling in high-dimensional latent spaces, we introduce the
preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines
dimension-robust proposals with gradient-informed dynamics. This approach
enables efficient and scalable posterior sampling and consistently improves
performance across various reward alignment tasks, including layout-to-image
generation, quantity-aware generation, and aesthetic-preference generation, as
demonstrated in our experiments.

</details>


### [255] [STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation](https://arxiv.org/abs/2506.01327)
*Zenghao Guan,Guojun Zhu,Yucan Zhou,Wu Liu,Weiping Wang,Jiebo Luo,Xiaoyan Gu*

Main category: cs.LG

TL;DR: The paper introduces Spatial-Temporal Statistics Aggregation (STSA) for Federated Class-Incremental Learning (FCIL), which aggregates feature statistics spatially and temporally to overcome client drift and reduce computational/communication overhead. A communication-efficient variant, STSA-E, is also introduced.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of existing FCIL methods that cannot avoid spatial-temporal client drift due to data heterogeneity and incur significant computational and communication overheads.

Method: Propose a novel approach called Spatial-Temporal Statistics Aggregation (STSA) which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). Additionally, introduce STSA-E, a communication-efficient variant with theoretical guarantees.

Result: Extensive experiments on three widely used FCIL datasets show that the proposed method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency.

Conclusion: STSA and its variant STSA-E provide an effective solution for FCIL by overcoming client drift and reducing overhead, thus enabling practical deployment.

Abstract: Federated Class-Incremental Learning (FCIL) enables Class-Incremental
Learning (CIL) from distributed data. Existing FCIL methods typically integrate
old knowledge preservation into local client training. However, these methods
cannot avoid spatial-temporal client drift caused by data heterogeneity and
often incur significant computational and communication overhead, limiting
practical deployment. To address these challenges simultaneously, we propose a
novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides
a unified framework to aggregate feature statistics both spatially (across
clients) and temporally (across stages). The aggregated feature statistics are
unaffected by data heterogeneity and can be used to update the classifier in
closed form at each stage. Additionally, we introduce STSA-E, a
communication-efficient variant with theoretical guarantees, achieving similar
performance to STSA-E with much lower communication overhead. Extensive
experiments on three widely used FCIL datasets, with varying degrees of data
heterogeneity, show that our method outperforms state-of-the-art FCIL methods
in terms of performance, flexibility, and both communication and computation
efficiency.

</details>


### [256] [NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models](https://arxiv.org/abs/2506.01337)
*Zeming Li,Xiangyue Liu,Xiangyu Zhang,Ping Tan,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: The paper introduces NoiseAR, a novel method for creating an AutoRegressive Initial Noise Prior for Diffusion Models, which enhances sample quality and consistency with conditional inputs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional static and unstructured initial states in diffusion models, and to provide a more controllable and expressive way to initialize these models.

Method: NoiseAR learns to generate a dynamic and controllable prior distribution for the initial noise by formulating the generation of the initial noise prior's parameters as an autoregressive probabilistic modeling task over spatial patches or tokens. It is designed to be conditional, allowing text prompts to influence the learned prior.

Result: Experiments show that NoiseAR generates initial noise priors leading to improved sample quality and enhanced consistency with conditional inputs, providing a learned alternative to random initialization.

Conclusion: NoiseAR offers a powerful method for initializing diffusion models with its probabilistic formulation supporting integration into frameworks like Markov Decision Processes and Reinforcement Learning.

Abstract: Diffusion models have emerged as powerful generative frameworks, creating
data samples by progressively denoising an initial random state. Traditionally,
this initial state is sampled from a simple, fixed distribution like isotropic
Gaussian, inherently lacking structure and a direct mechanism for external
control. While recent efforts have explored ways to introduce controllability
into the diffusion process, particularly at the initialization stage, they
often rely on deterministic or heuristic approaches. These methods can be
suboptimal, lack expressiveness, and are difficult to scale or integrate into
more sophisticated optimization frameworks. In this paper, we introduce
NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion
Models. Instead of a static, unstructured source, NoiseAR learns to generate a
dynamic and controllable prior distribution for the initial noise. We formulate
the generation of the initial noise prior's parameters as an autoregressive
probabilistic modeling task over spatial patches or tokens. This approach
enables NoiseAR to capture complex spatial dependencies and introduce learned
structure into the initial state. Crucially, NoiseAR is designed to be
conditional, allowing text prompts to directly influence the learned prior,
thereby achieving fine-grained control over the diffusion initialization. Our
experiments demonstrate that NoiseAR can generate initial noise priors that
lead to improved sample quality and enhanced consistency with conditional
inputs, offering a powerful, learned alternative to traditional random
initialization. A key advantage of NoiseAR is its probabilistic formulation,
which naturally supports seamless integration into probabilistic frameworks
like Markov Decision Processes and Reinforcement Learning. Our code will be
available at https://github.com/HKUST-SAIL/NoiseAR/

</details>


### [257] [Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning](https://arxiv.org/abs/2506.01339)
*Changsheng Wang,Yihua Zhang,Jinghan Jia,Parikshit Ram,Dennis Wei,Yuguang Yao,Soumyadeep Pal,Nathalie Baracaldo,Sijia Liu*

Main category: cs.LG

TL;DR: The paper proposes Invariant LLM Unlearning (ILU), a regularization-based framework that incorporates invariance into machine unlearning for large language models, enhancing robustness against downstream fine-tuning and outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Current unlearning methods for LLMs are sensitive to downstream fine-tuning which can lead to recovery of forgotten information even from unrelated tasks.

Method: ILU introduces invariance into unlearning inspired by invariant risk minimization (IRM). It is a regularization-based framework that enhances robustness and generalizes well to diverse fine-tuning tasks, even when trained on a single dataset. Task vector analysis is used to explain the effectiveness.

Result: Extensive experiments on WMDP and MUSE benchmarks show ILU significantly outperforms state-of-the-art unlearning methods such as NPO and RMU. ILU demonstrates superior unlearning robustness across various downstream tasks while preserving fine-tuning performance.

Conclusion: ILU provides an effective solution to enhance unlearning robustness for LLMs against downstream fine-tuning, offering significant improvements over existing methods.

Abstract: Machine unlearning offers a promising solution to privacy and safety concerns
in large language models (LLMs) by selectively removing targeted knowledge
while preserving utility. However, current methods are highly sensitive to
downstream fine-tuning, which can quickly recover forgotten information-even
from unrelated tasks. To address this, we introduce invariance into unlearning
for the first time, inspired by invariant risk minimization (IRM). Building on
this principle, we propose invariant LLM unlearning (ILU), a
regularization-based framework that enhances robustness. Notably, ILU
generalizes well to diverse fine-tuning tasks, even when trained using a single
dataset. A task vector analysis is also provided to further elucidate the
rationale behind ILU's effectiveness. Extensive experiments on the WMDP and
MUSE benchmark, reveal that ILU significantly outperforms state-of-the-art
unlearning methods, including negative preference optimization (NPO) and
representation misdirection for unlearning (RMU). Notably, ILU achieves
superior unlearning robustness across diverse downstream fine-tuning scenarios
(e.g., math, paraphrase detection, and sentiment analysis) while preserving the
fine-tuning performance.

</details>


### [258] [Distributionally Robust Learning in Survival Analysis](https://arxiv.org/abs/2506.01348)
*Yeping Jin,Lauren Wise,Ioannis Paschalidis*

Main category: cs.LG

TL;DR: This paper proposes a DRL-Cox model that enhances the robustness and accuracy of survival predictions by incorporating Distributionally Robust Learning into Cox regression.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness and accuracy of survival predictions in Cox regression, especially when there are uncertainties in data distribution or model misspecifications.

Method: The authors formulate a Distributionally Robust Learning (DRL) framework using a Wasserstein distance-based ambiguity set. This leads to a variant of the Cox model which is less sensitive to assumptions about the underlying data distribution. They leverage Wasserstein duality to reformulate the original min-max DRL problem into a regularized empirical risk minimization problem solvable by exponential conic programming.

Result: The DRL-Cox model provides guarantees on finite sample behavior and demonstrates superior performance in prediction accuracy and robustness compared to traditional methods through extensive simulations and real-world case studies.

Conclusion: The proposed DRL-Cox model effectively enhances the robustness and accuracy of survival predictions, making it a valuable tool for applications where data distribution assumptions may be uncertain or incorrect.

Abstract: We introduce an innovative approach that incorporates a Distributionally
Robust Learning (DRL) approach into Cox regression to enhance the robustness
and accuracy of survival predictions. By formulating a DRL framework with a
Wasserstein distance-based ambiguity set, we develop a variant Cox model that
is less sensitive to assumptions about the underlying data distribution and
more resilient to model misspecification and data perturbations. By leveraging
Wasserstein duality, we reformulate the original min-max DRL problem into a
tractable regularized empirical risk minimization problem, which can be
computed by exponential conic programming. We provide guarantees on the finite
sample behavior of our DRL-Cox model. Moreover, through extensive simulations
and real world case studies, we demonstrate that our regression model achieves
superior performance in terms of prediction accuracy and robustness compared
with traditional methods.

</details>


### [259] [Variational Adaptive Noise and Dropout towards Stable Recurrent Neural Networks](https://arxiv.org/abs/2506.01350)
*Taisuke Kobayashi,Shingo Murata*

Main category: cs.LG

TL;DR: This paper proposes VAND, a novel stable learning theory for RNNs that combines noise and dropout as stabilizing factors through variational inference.


<details>
  <summary>Details</summary>
Motivation: Existing studies have separately confirmed the effectiveness of noise and dropout on the internal state of RNNs as stabilizing factors. This paper aims to reinterpret the optimization problem of RNNs using variational inference to derive noise and dropout simultaneously.

Method: The method involves reinterpreting the optimization problem of RNNs as variational inference, which allows noise and dropout to be derived simultaneously by transforming the explicit regularization term into implicit regularization. The scale and ratio of noise and dropout can also be adjusted appropriately to optimize the main objective of RNNs.

Result: In an imitation learning scenario with a mobile manipulator, only VAND is able to imitate sequential and periodic behaviors as instructed.

Conclusion: VAND provides a novel approach to stabilize RNNs by combining noise and dropout through variational inference, demonstrating its effectiveness in imitation learning tasks.

Abstract: This paper proposes a novel stable learning theory for recurrent neural
networks (RNNs), so-called variational adaptive noise and dropout (VAND). As
stabilizing factors for RNNs, noise and dropout on the internal state of RNNs
have been separately confirmed in previous studies. We reinterpret the
optimization problem of RNNs as variational inference, showing that noise and
dropout can be derived simultaneously by transforming the explicit
regularization term arising in the optimization problem into implicit
regularization. Their scale and ratio can also be adjusted appropriately to
optimize the main objective of RNNs, respectively. In an imitation learning
scenario with a mobile manipulator, only VAND is able to imitate sequential and
periodic behaviors as instructed. https://youtu.be/UOho3Xr6A2w

</details>


### [260] [TAH-QUANT: Effective Activation Quantization in Pipeline Parallelism over Slow Network](https://arxiv.org/abs/2506.01352)
*Guangxin He,Yuan Cao,Yutong He,Tianyi Bai,Kun Yuan,Binhang Yuan*

Main category: cs.LG

TL;DR: Decentralized training of large language models faces network communication bottlenecks. Existing activation compression methods have limitations. This paper introduces TAH-Quant, a new activation quantization framework for pipeline parallelism, which achieves aggressive activation quantization without compromising training convergence and incurs no extra memory overhead.


<details>
  <summary>Details</summary>
Motivation: To address the significant network communication bottlenecks in decentralized training of large language models, especially in pipeline-parallel settings where frequent communication of intermediate activations is required but limited by network bandwidth.

Method: TAH-Quant (Tile-wise Adaptive Hadamard Quantization) integrates fine-grained tile-wise quantization, entropy-guided token-level adaptive bit allocation, and a Hadamard-based transform with pivot element swapping to effectively suppress quantization outliers.

Result: Experiments show that TAH-Quant achieves aggressive activation quantization (3-4 bits), providing up to 4.3× end-to-end speedup without affecting training convergence, matches state-of-the-art methods, incurs no extra memory overhead, and generalizes well across different training scenarios.

Conclusion: Pipeline parallel training equipped with TAH-Quant maintains a convergence rate of O(1/√T), matching that of vanilla stochastic gradient descent.

Abstract: Decentralized training of large language models offers the opportunity to
pool computational resources across geographically distributed participants but
faces significant network communication bottlenecks, particularly in
pipeline-parallel settings. While pipeline parallelism partitions model layers
across devices to handle large-scale models, it necessitates frequent
communication of intermediate activations, creating challenges when network
bandwidth is limited. Existing activation compression methods, such as AQ-SGD,
mitigate quantization-induced errors through error compensation but impose
prohibitive memory overhead by requiring storage of previous activations. To
address these issues, we introduce TAH-Quant (Tile-wise Adaptive Hadamard
Quantization), a novel activation quantization framework designed specifically
for pipeline parallelism. Our approach integrates fine-grained tile-wise
quantization for precise control, entropy-guided token-level adaptive bit
allocation for optimal bit usage, and a Hadamard-based transform with pivot
element swapping to effectively suppress quantization outliers. We further
provide a theoretical analysis, proving that pipeline parallel training
equipped with TAH-Quant maintains a convergence rate of
$\mathcal{O}(1/\sqrt{T})$, matching that of vanilla stochastic gradient
descent. Extensive experiments on diverse LLM tasks demonstrate that TAH-Quant
achieves aggressive activation quantization (3-4 bits) ratio, which provides up
to 4.3$\times$ end-to-end speedup without compromising training convergence,
matches state-of-the-art methods, incurs no extra memory overhead, and
generalizes well across different training scenarios.

</details>


### [261] [Two-Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion](https://arxiv.org/abs/2506.01356)
*Haoyu Li,Xiangru Zhong,Bin Hu,Huan Zhang*

Main category: cs.LG

TL;DR: The paper proposes a two-stage training framework for jointly synthesizing controller and Lyapunov function in continuous-time systems, reducing conservatism in training and improving verification speed.


<details>
  <summary>Details</summary>
Motivation: Learning-based neural network control policies perform well empirically, but stability guarantees and region of attraction estimations are challenging due to the lack of stable and scalable algorithms. Previous works have much conservatism in their framework.

Method: A novel two-stage training framework is proposed to jointly synthesize the controller and Lyapunov function for continuous-time systems. It leverages a Zubov-inspired region of attraction characterization, proposes a new training data sampling strategy and domain updating mechanism, and extends the neural network verifier α,β-CROWN with automatic bound propagation through the Jacobian of dynamical systems and a novel verification scheme avoiding expensive bisection.

Result: Numerical experiments show that the proposed training can yield regions of attraction 5-1.5e5 times larger compared to baselines, and the verification on continuous systems can be 40-10000 times faster compared to the traditional SMT solver dReal.

Conclusion: The authors demonstrate the effectiveness of their approach through numerical experiments on several challenging nonlinear systems across multiple dimensions.

Abstract: Learning-based neural network (NN) control policies have shown impressive
empirical performance. However, obtaining stability guarantees and estimations
of the region of attraction of these learned neural controllers is challenging
due to the lack of stable and scalable training and verification algorithms.
Although previous works in this area have achieved great success, much
conservatism remains in their framework. In this work, we propose a novel
two-stage training framework to jointly synthesize the controller and Lyapunov
function for continuous-time systems. By leveraging a Zubov-inspired region of
attraction characterization to directly estimate stability boundaries, we
propose a novel training data sampling strategy and a domain updating mechanism
that significantly reduces the conservatism in training. Moreover, unlike
existing works on continuous-time systems that rely on an SMT solver to
formally verify the Lyapunov condition, we extend state-of-the-art neural
network verifier $\alpha,\!\beta$-CROWN with the capability of performing
automatic bound propagation through the Jacobian of dynamical systems and a
novel verification scheme that avoids expensive bisection. To demonstrate the
effectiveness of our approach, we conduct numerical experiments by synthesizing
and verifying controllers on several challenging nonlinear systems across
multiple dimensions. We show that our training can yield region of attractions
with volume $5 - 1.5\cdot 10^{5}$ times larger compared to the baselines, and
our verification on continuous systems can be up to $40-10000$ times faster
compared to the traditional SMT solver dReal. Our code is available at
https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training.

</details>


### [262] [RDB2G-Bench: A Comprehensive Benchmark for Automatic Graph Modeling of Relational Databases](https://arxiv.org/abs/2506.01360)
*Dongwon Choi,Sunwoo Kim,Juyeon Kim,Kyungho Kim,Geon Lee,Shinhwan Kang,Myunghwan Kim,Kijung Shin*

Main category: cs.LG

TL;DR: The paper introduces RDB2G-Bench, a benchmark framework for evaluating RDB-to-graph modeling methods, and reveals key structural patterns affecting graph model effectiveness.


<details>
  <summary>Details</summary>
Motivation: Recent research on applying machine learning to relational databases has explored graph-based representations, but there are numerous ways to model RDBs as graphs and performance varies significantly depending on the chosen graph model.

Method: The authors introduce RDB2G-Bench, the first benchmark framework for evaluating RDB-to-graph modeling methods. They construct extensive datasets covering 5 real-world RDBs and 12 predictive tasks, resulting in around 50k graph-performance pairs for efficient and reproducible evaluations.

Result: Thanks to their precomputed datasets, they were able to benchmark 9 automatic RDB-to-graph modeling methods on the 12 tasks over 600x faster than on-the-fly evaluation. Their analysis of the datasets and benchmark results reveals key structural patterns affecting graph model effectiveness.

Conclusion: RDB2G-Bench fosters research on intelligent RDB-to-graph modeling and provides practical implications for effective graph modeling.

Abstract: Relational databases (RDBs) are composed of interconnected tables, where
relationships between them are defined through foreign keys. Recent research on
applying machine learning to RDBs has explored graph-based representations of
RDBs, where rows of tables are modeled as nodes, and foreign key relationships
are modeled as edges. RDB-to-graph modeling helps capture cross-table
dependencies, ultimately leading to enhanced performance across diverse tasks.
However, there are numerous ways to model RDBs as graphs, and performance
varies significantly depending on the chosen graph model. In our analysis,
applying a common heuristic rule for graph modeling leads to up to a 10% drop
in performance compared to the best-performing graph model, which remains
non-trivial to identify. To foster research on intelligent RDB-to-graph
modeling, we introduce RDB2G-Bench, the first benchmark framework for
evaluating such methods. We construct extensive datasets covering 5 real-world
RDBs and 12 predictive tasks, resulting in around 50k graph-performance pairs
for efficient and reproducible evaluations. Thanks to our precomputed datasets,
we were able to benchmark 9 automatic RDB-to-graph modeling methods on the 12
tasks over 600x faster than on-the-fly evaluation, which requires repeated
model training. Our analysis of the datasets and benchmark results reveals key
structural patterns affecting graph model effectiveness, along with practical
implications for effective graph modeling.

</details>


### [263] [TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery](https://arxiv.org/abs/2506.01361)
*Muhammad Hasan Ferdous,Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: TimeGraph is a new suite of synthetic time-series benchmark datasets that incorporate real-world complexities for evaluating causal discovery algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing robust causal discovery in time series datasets lacks reliable benchmarks with known ground-truth causal relationships, and current synthetic alternatives often neglect critical temporal properties inherent in real-world data.

Method: Introduced TimeGraph, which systematically incorporates both linear and nonlinear dependencies while modeling key temporal characteristics such as trends, seasonal effects, and heterogeneous noise patterns. Each dataset has a fully specified causal graph with varying densities and diverse noise distributions, provided in versions with and without unobserved confounders.

Result: Systematic evaluations of state-of-the-art causal discovery algorithms revealed significant variations in performance under realistic temporal conditions, highlighting the need for robust synthetic benchmarks.

Conclusion: TimeGraph suite is freely available to promote reproducible research and community-driven advancements in time-series causal discovery.

Abstract: Robust causal discovery in time series datasets depends on reliable benchmark
datasets with known ground-truth causal relationships. However, such datasets
remain scarce, and existing synthetic alternatives often overlook critical
temporal properties inherent in real-world data, including nonstationarity
driven by trends and seasonality, irregular sampling intervals, and the
presence of unobserved confounders. To address these challenges, we introduce
TimeGraph, a comprehensive suite of synthetic time-series benchmark datasets
that systematically incorporates both linear and nonlinear dependencies while
modeling key temporal characteristics such as trends, seasonal effects, and
heterogeneous noise patterns. Each dataset is accompanied by a fully specified
causal graph featuring varying densities and diverse noise distributions and is
provided in two versions: one including unobserved confounders and one without,
thereby offering extensive coverage of real-world complexity while preserving
methodological neutrality. We further demonstrate the utility of TimeGraph
through systematic evaluations of state-of-the-art causal discovery algorithms
including PCMCI+, LPCMCI, and FGES across a diverse array of configurations and
metrics. Our experiments reveal significant variations in algorithmic
performance under realistic temporal conditions, underscoring the need for
robust synthetic benchmarks in the fair and transparent assessment of causal
discovery methods. The complete TimeGraph suite, including dataset generation
scripts, evaluation metrics, and recommended experimental protocols, is freely
available to facilitate reproducible research and foster community-driven
advancements in time-series causal discovery.

</details>


### [264] [Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review](https://arxiv.org/abs/2506.01364)
*Yuchen Fang,Hao Miao,Yuxuan Liang,Liwei Deng,Yue Cui,Ximu Zeng,Yuyang Xia,Yan Zhao,Torben Bach Pedersen,Christian S. Jensen,Xiaofang Zhou,Kai Zheng*

Main category: cs.LG

TL;DR: This paper reviews spatio-temporal foundation models from the perspective of their pipeline, providing a structured understanding of these models and guiding researchers.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive examination of how spatio-temporal foundation models are designed, selected, pre-trained, and adapted, which makes the overall pipeline unclear.

Method: Providing an up-to-date review of previous spatio-temporal foundation models from the pipeline perspective. The pipeline includes introduction to different types of spatio-temporal data, data preprocessing and embedding techniques, a novel data property taxonomy, training objectives of primitive models, and adaptation techniques of transferred models.

Result: The survey provides a clear and structured pipeline to understand the connection between core elements of spatio-temporal foundation models while guiding researchers to get started quickly.

Conclusion: This review bridges the gap in understanding spatio-temporal foundation models and introduces emerging opportunities such as multi-objective training.

Abstract: Spatio-temporal deep learning models aims to utilize useful patterns in such
data to support tasks like prediction. However, previous deep learning models
designed for specific tasks typically require separate training for each use
case, leading to increased computational and storage costs. To address this
issue, spatio-temporal foundation models have emerged, offering a unified
framework capable of solving multiple spatio-temporal tasks. These foundation
models achieve remarkable success by learning general knowledge with
spatio-temporal data or transferring the general capabilities of pre-trained
language models. While previous surveys have explored spatio-temporal data and
methodologies separately, they have ignored a comprehensive examination of how
foundation models are designed, selected, pre-trained, and adapted. As a
result, the overall pipeline for spatio-temporal foundation models remains
unclear. To bridge this gap, we innovatively provide an up-to-date review of
previous spatio-temporal foundation models from the pipeline perspective. The
pipeline begins with an introduction to different types of spatio-temporal
data, followed by details of data preprocessing and embedding techniques. The
pipeline then presents a novel data property taxonomy to divide existing
methods according to data sources and dependencies, providing efficient and
effective model design and selection for researchers. On this basis, we further
illustrate the training objectives of primitive models, as well as the
adaptation techniques of transferred models. Overall, our survey provides a
clear and structured pipeline to understand the connection between core
elements of spatio-temporal foundation models while guiding researchers to get
started quickly. Additionally, we introduce emerging opportunities such as
multi-objective training in the field of spatio-temporal foundation models.

</details>


### [265] [Incentivizing LLMs to Self-Verify Their Answers](https://arxiv.org/abs/2506.01369)
*Fuxiang Zhang,Jiacheng Xu,Chaojie Wang,Ce Cui,Yang Liu,Bo An*

Main category: cs.LG

TL;DR: 研究人员发现，在特定推理任务上进行后训练的模型在扩展时只能获得有限的改进，这是因为特定后训练生成器和通用奖励模型之间的分布差异。为了解决这个问题，他们提出了一种框架，鼓励大型语言模型（LLMs）自我验证其答案。通过在单一强化学习（RL）过程中统一答案生成和验证，训练出的模型能够有效地评估自己解决方案的正确性，并在推理时通过验证生成内容进一步提高性能，而无需外部验证者。基于Qwen2.5-Math-7B和DeepSeek-R1-Distill-Qwen-1.5B训练的自我验证模型展示了其在不同推理上下文长度上的能力。实验表明，这些模型不仅提高了后训练性能，还实现了有效的测试时间扩展。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的测试时间扩展方法通常通过使用外部奖励模型来指导模型生成过程，但在特定推理任务上进行后训练的模型扩展时只能获得有限的改进。这种限制源于特定后训练生成器和通用奖励模型之间的分布差异。因此，需要一种新的方法来解决这一问题，从而实现更好的性能提升。

Method: 研究团队提出了一种框架，激励LLMs自我验证答案。该框架通过强化学习将答案生成与验证统一起来，使模型能够在没有外部验证的情况下评估自身解决方案的正确性。具体来说，他们在Qwen2.5-Math-7B和DeepSeek-R1-Distill-Qwen-1.5B基础上训练了自我验证模型，使其具备处理不同推理上下文长度的能力。

Result: 实验结果表明，所提出的自我验证模型不仅提高了后训练性能，还在多个数学推理基准测试中实现了有效的测试时间扩展。这意味着模型在推理过程中可以通过自我验证进一步提升性能。

Conclusion: 这项研究表明，通过设计一个自我验证框架，可以有效克服现有方法中的分布差异问题，从而提升LLMs在复杂推理任务中的表现。此外，该框架无需依赖外部奖励模型，为未来的研究提供了新的方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex
reasoning tasks through both post-training and test-time scaling laws. While
prevalent test-time scaling approaches are often realized by using external
reward models to guide the model generation process, we find only marginal
gains can be acquired when scaling a model post-trained on specific reasoning
tasks. We identify that the limited improvement stems from distribution
discrepancies between the specific post-trained generator and the general
reward model. To address this, we propose a framework that incentivizes LLMs to
self-verify their own answers. By unifying answer generation and verification
within a single reinforcement learning (RL) process, we train models that can
effectively assess the correctness of their own solutions. The trained model
can further scale its performance during inference time by verifying its
generations, without the need for external verifiers. We train our
self-verification models based on Qwen2.5-Math-7B and
DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying
reasoning context lengths. Experiments on multiple mathematical reasoning
benchmarks show that our models can not only improve post-training performance
but also enable effective test-time scaling. Our code is available at
https://github.com/mansicer/self-verification.

</details>


### [266] [Compiler Optimization via LLM Reasoning for Efficient Model Serving](https://arxiv.org/abs/2506.01374)
*Sujun Tang,Christopher Priebe,Rohan Mahapatra,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: 通过使用大型语言模型（LLMs）和结构化蒙特卡罗树搜索（MCTS），提出了一种新的编译框架（REASONING COMPILER），该框架在显著减少样本数量的情况下实现了显著的加速，展示了LLM引导推理在编译器优化领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的编译器难以处理神经网络任务，因为可能的转换空间非常大且高度相互依赖。现有的随机搜索技术虽然有效，但通常样本效率低下，并且无法充分利用编译决策背后的结构化上下文。

Method: 引入了一个名为REASONING COMPILER的新编译框架，将优化过程视为一个顺序的、上下文感知的决策过程，由大型语言模型（LLM）和结构化蒙特卡罗树搜索（MCTS）指导。LLM作为提案机制，根据当前程序状态和累积的性能反馈提出硬件感知的转换。MCTS结合LLM生成的提案，平衡探索与利用，实现对广泛编译优化空间的结构化、上下文敏感的遍历。

Result: 与领先的神经编译器相比，该方法以显著更少的样本实现了显著的加速，证明了LLM引导推理在编译器优化领域中的潜力。

Conclusion: 研究结果表明，利用大型语言模型进行推理可以显著提高样本效率，从而改变编译器优化的格局。

Abstract: While model serving has unlocked unprecedented capabilities, the high cost of
serving large-scale models continues to be a significant barrier to widespread
accessibility and rapid innovation. Compiler optimizations have long driven
substantial performance improvements, but existing compilers struggle with
neural workloads due to the exponentially large and highly interdependent space
of possible transformations. Although existing stochastic search techniques can
be effective, they are often sample-inefficient and fail to leverage the
structural context underlying compilation decisions. We set out to investigate
the research question of whether reasoning with large language models (LLMs),
without any retraining, can leverage the context-aware decision space of
compiler optimization to significantly improve sample efficiency. To that end,
we introduce a novel compilation framework (dubbed REASONING COMPILER) that
formulates optimization as a sequential, context-aware decision process, guided
by a large language model and structured Monte Carlo tree search (MCTS). The
LLM acts as a proposal mechanism, suggesting hardware-aware transformations
that reflect the current program state and accumulated performance feedback.
Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to
balance exploration and exploitation, facilitating structured,
context-sensitive traversal of the expansive compiler optimization space. By
achieving substantial speedups with markedly fewer samples than leading neural
compilers, our approach demonstrates the potential of LLM-guided reasoning to
transform the landscape of compiler optimization.

</details>


### [267] [Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training](https://arxiv.org/abs/2506.01376)
*Minghao Xu,Jiaze Song,Keming Wu,Xiangxin Zhou,Bin Cui,Wentao Zhang*

Main category: cs.LG

TL;DR: The paper presents GlycanAA, a model for understanding glycan properties using machine learning by considering both the backbone structure and atomic structures of monosaccharides. It uses hierarchical message passing and pre-training to improve performance.


<details>
  <summary>Details</summary>
Motivation: Previous methods focused on modeling the backbone structure of glycans as graphs of monosaccharides but neglected the important atomic structures underlying each monosaccharide.

Method: GlycanAA models glycans as heterogeneous graphs with monosaccharide nodes and atom nodes. It performs hierarchical message passing to capture interactions from local atomic-level to global monosaccharide-level. The model is pre-trained on a high-quality unlabeled glycan dataset using a multi-scale mask prediction algorithm.

Result: Extensive benchmark results demonstrate the superiority of GlycanAA over existing glycan encoders and verify the improvements achieved by PreGlycanAA.

Conclusion: GlycanAA effectively captures various properties of glycans by considering both backbone and atomic structures, with resources available at https://github.com/kasawa1234/GlycanAA.

Abstract: Understanding the various properties of glycans with machine learning has
shown some preliminary promise. However, previous methods mainly focused on
modeling the backbone structure of glycans as graphs of monosaccharides (i.e.,
sugar units), while they neglected the atomic structures underlying each
monosaccharide, which are actually important indicators of glycan properties.
We fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan
modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide
nodes representing its global backbone structure and atom nodes representing
its local atomic-level structures. Based on such a graph, GlycanAA performs
hierarchical message passing to capture from local atomic-level interactions to
global monosaccharide-level interactions. To further enhance model capability,
we pre-train GlycanAA on a high-quality unlabeled glycan dataset, deriving the
PreGlycanAA model. We design a multi-scale mask prediction algorithm to endow
the model about different levels of dependencies in a glycan. Extensive
benchmark results show the superiority of GlycanAA over existing glycan
encoders and verify the further improvements achieved by PreGlycanAA. We
maintain all resources at https://github.com/kasawa1234/GlycanAA

</details>


### [268] [ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs](https://arxiv.org/abs/2506.01386)
*Manit Baser,Dinil Mon Divakaran,Mohan Gurusamy*

Main category: cs.LG

TL;DR: 在大型语言模型（LLMs）中，模型编辑是一个重要的工具，用于解决隐私、偏差和错误信息问题。现有的编辑技术通常针对孤立的事实，忽略相关知识的连锁反应。本文提出了一种新的模型编辑设置——深度编辑，以及一个名为ThinkEval的框架，用以系统地评估模型编辑技术，并介绍了KnowGIC基准数据集。通过评估五种编辑技术，发现它们难以在间接事实抑制与相关知识保存之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的模型编辑技术主要关注孤立事实的修改，忽略了对相关知识的连锁影响，这可能导致被编辑的事实仍然可推导出，从而破坏了更广泛的情境完整性。因此，需要一种新方法来处理相互关联的事实。

Method: 提出了深度编辑的概念，展示了现有编辑技术在处理相互关联事实时的不足，并引入了ThinkEval框架，用于构建模型特定的知识图谱，分析编辑前后的事实持久性和灾难性遗忘情况。还创建了一个名为KnowGIC的基准数据集，包含顺序链接的查询，以测量这些效果。

Result: 评估了五种编辑技术：AlphaEdit、RECT、ROME、MEMIT和PRUNE，发现在多个LLMs上，这些技术难以在间接事实抑制与相关知识保存之间取得良好的平衡。

Conclusion: 深度编辑揭示了现有模型编辑技术在处理相互关联事实方面的局限性，而ThinkEval框架和KnowGIC基准为未来的研究提供了有价值的工具和资源。

Abstract: Model editing has become an important tool for addressing privacy, bias, and
misinformation in large language models (LLMs) by enabling updates to knowledge
without the need for retraining from scratch. However, existing editing
techniques often target isolated facts, ignoring ripple effects on related
knowledge, allowing edited facts to remain deducible and compromising broader
contextual integrity. For example, changing Harry Potter's school from Hogwarts
to Ilvermorny requires reassigning his house from Gryffindor to a suitable
alternative while preserving Gryffindor's relationship with Hogwarts. In this
work, we present a new model-editing setting, deep editing, to show: (1) how
editing techniques fail to handle connected facts, evaluating how original
knowledge sneaks through unchanged causal links, and (2) their impact on
broader contextual knowledge. We introduce ThinkEval, a framework to
systematically evaluate model-editing techniques by building model-specific
knowledge graphs to analyze pre- and post-edit effects on fact persistence and
catastrophic forgetting. We present KnowGIC, a benchmark created with
ThinkEval, consisting of sequentially linked queries to measure these effects.
We evaluate five editing techniques: AlphaEdit, RECT, ROME, MEMIT, and PRUNE
across multiple LLMs. We find that these techniques struggle to balance
indirect fact suppression with the preservation of related knowledge. Our
dataset is available at: https://anonymous.4open.science/r/KnowGIC.

</details>


### [269] [Multi Part Deployment of Neural Network](https://arxiv.org/abs/2506.01387)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.LG

TL;DR: The paper proposes a distributed system architecture for partitioning and training large-scale neural networks across multiple servers, reducing computational cost and infrastructure requirements.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks are growing in scale, leading to significant challenges in terms of computational cost and infrastructure requirements. Traditional training paradigms relying on monolithic GPU clusters become unsustainable as deep neural networks continue to grow.

Method: The proposed method involves distributing the neural network across multiple servers where each server is responsible for a subset of neurons. Neurons are classified as local or remote with inter-server connections managed via metadata-driven lookup mechanism. A Multi-Part Neural Network Execution Engine ensures seamless execution and training by dynamically resolving and invoking remote neurons using stored metadata. Servers share a unified model through NFS ensuring consistency during parallel updates. The Neuron Distributor module allows flexible partitioning strategies based on neuron count, percentage, identifiers, or network layers.

Result: This architecture enables cost-effective, scalable deployment of deep learning models on cloud infrastructure, significantly reducing dependency on high-performance centralized compute resources.

Conclusion: Distributed system architectures offer a promising solution for handling the increasing scale of modern neural networks, providing a scalable and cost-effective alternative to traditional monolithic GPU cluster-based training paradigms.

Abstract: The increasing scale of modern neural networks, exemplified by architectures
from IBM (530 billion neurons) and Google (500 billion parameters), presents
significant challenges in terms of computational cost and infrastructure
requirements. As deep neural networks continue to grow, traditional training
paradigms relying on monolithic GPU clusters become increasingly unsustainable.
This paper proposes a distributed system architecture that partitions a neural
network across multiple servers, each responsible for a subset of neurons.
Neurons are classified as local or remote, with inter-server connections
managed via a metadata-driven lookup mechanism. A Multi-Part Neural Network
Execution Engine facilitates seamless execution and training across distributed
partitions by dynamically resolving and invoking remote neurons using stored
metadata. All servers share a unified model through a network file system
(NFS), ensuring consistency during parallel updates. A Neuron Distributor
module enables flexible partitioning strategies based on neuron count,
percentage, identifiers, or network layers. This architecture enables
cost-effective, scalable deployment of deep learning models on cloud
infrastructure, reducing dependency on high-performance centralized compute
resources.

</details>


### [270] [Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization](https://arxiv.org/abs/2506.01393)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: The paper explores the Bayesian optimization problem under Gaussian processes, analyzing the GP-UCB algorithm's regret bounds and improving on previous results.


<details>
  <summary>Details</summary>
Motivation: To refine the understanding of the regret bounds in Bayesian optimization using Gaussian processes, particularly focusing on the GP-UCB algorithm to close the gap with the best-known bounds.

Method: The authors analyze the GP-UCB algorithm under different kernels (Matérn and squared exponential) by capturing the concentration behavior of the input sequence realized by GP-UCB, which allows for a more refined analysis of the information gain.

Result: For a Matérn kernel, they achieve $\tilde{O}(\sqrt{T})$ cumulative regret with high probability. For a squared exponential kernel, they yield $O(\sqrt{T \ln^4 T})$ regret.

Conclusion: The results provide tighter regret bounds for the GP-UCB algorithm, advancing the understanding of its performance and narrowing the gap with the best-known bounds.

Abstract: This paper addresses the Bayesian optimization problem (also referred to as
the Bayesian setting of the Gaussian process bandit), where the learner seeks
to minimize the regret under a function drawn from a known Gaussian process
(GP). Under a Mat\'ern kernel with a certain degree of smoothness, we show that
the Gaussian process upper confidence bound (GP-UCB) algorithm achieves
$\tilde{O}(\sqrt{T})$ cumulative regret with high probability. Furthermore, our
analysis yields $O(\sqrt{T \ln^4 T})$ regret under a squared exponential
kernel. These results fill the gap between the existing regret upper bound for
GP-UCB and the best-known bound provided by Scarlett (2018). The key idea in
our proof is to capture the concentration behavior of the input sequence
realized by GP-UCB, enabling a more refined analysis of the GP's information
gain.

</details>


### [271] [Quantitative Error Feedback for Quantization Noise Reduction of Filtering over Graphs](https://arxiv.org/abs/2506.01404)
*Xue Xian Zheng,Weihang Liu,Xin Lou,Stefan Vlaski,Tareq Al-Naffouri*

Main category: cs.LG

TL;DR: This paper proposes an error feedback framework to reduce quantization noise in distributed graph filtering, demonstrating its effectiveness through theoretical analysis and numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To mitigate the impact of quantization noise in distributed graph filtering where communications are limited to quantized messages, drawing inspiration from error spectrum shaping techniques in state-space digital filters.

Method: The method involves quantitatively feeding back the quantization noise for exact compensation in three key scenarios: deterministic graph filtering, graph filtering over random graphs, and graph filtering with random node-asynchronous updates. The framework provides closed-form solutions for optimal error feedback coefficients.

Result: Rigorous theoretical analysis and numerical experiments show that the framework significantly reduces the effect of quantization noise, outperforming conventional methods in terms of accuracy and robustness.

Conclusion: The proposed error feedback framework can be effectively integrated into communication-efficient decentralized optimization frameworks, leading to lower error floors.

Abstract: This paper introduces an innovative error feedback framework designed to
mitigate quantization noise in distributed graph filtering, where
communications are constrained to quantized messages. It comes from error
spectrum shaping techniques from state-space digital filters, and therefore
establishes connections between quantized filtering processes over different
domains. In contrast to existing error compensation methods, our framework
quantitatively feeds back the quantization noise for exact compensation. We
examine the framework under three key scenarios: (i) deterministic graph
filtering, (ii) graph filtering over random graphs, and (iii) graph filtering
with random node-asynchronous updates. Rigorous theoretical analysis
demonstrates that the proposed framework significantly reduces the effect of
quantization noise, and we provide closed-form solutions for the optimal error
feedback coefficients. Moreover, this quantitative error feedback mechanism can
be seamlessly integrated into communication-efficient decentralized
optimization frameworks, enabling lower error floors. Numerical experiments
validate the theoretical results, consistently showing that our method
outperforms conventional quantization strategies in terms of both accuracy and
robustness.

</details>


### [272] [SOC-DGL: Social Interaction Behavior Inspired Dual Graph Learning Framework for Drug-Target Interaction Identification](https://arxiv.org/abs/2506.01405)
*Xiang Zhao,Ruijie Li,Qiao Ning,Shikai Guo,Hui Li,Qian Ma*

Main category: cs.LG

TL;DR: SOC-DGL由ADGL和EDGL两个模块组成，用于有效捕捉药物-靶点相互作用中的相似性信息，同时提出可调不平衡损失函数以应对数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数模型仅限于在同构图中挖掘直接的相似性信息，忽略了异构图中潜在的丰富相似性信息。

Method: 提出了SOC-DGL模型，包含ADGL和EDGL两个模块。ADGL采用全面的社会交互策略学习全局DTI和个体相似性信息；EDGL利用高阶社会交互策略通过平衡理论挖掘高阶同构信息。此外，还提出了一种可调不平衡损失函数。

Result: 在四个基准数据集上的广泛实验表明，SOC-DGL显著提高了预测准确性，特别是在处理数据不平衡和未见药物或靶点的情况下。

Conclusion: SOC-DGL能够有效且全面地捕获多样化的相似性信息，增强模型的泛化能力和预测准确性。

Abstract: The identification of drug-target interactions (DTI) is crucial for drug
discovery and repositioning, as it reveals potential uses of existing drugs,
aiding in the acceleration of the drug development process and reducing
associated costs. Despite the similarity information in DTI is important, most
models are limited to mining direct similarity information within homogeneous
graphs, overlooking the potential yet rich similarity information in
heterogeneous graphs. Inspired by real-world social interaction behaviors, we
propose SOC-DGL, which comprises two specialized modules: the Affinity-Driven
Graph Learning (ADGL) module and the Equilibrium-Driven Graph Learning (EDGL)
module. The ADGL module adopts a comprehensive social interaction strategy,
leveraging an affinity-enhanced global drug-target graph to learn both global
DTI and the individual similarity information of drugs and targets. In
contrast, the EDGL module employs a higher-order social interaction strategy,
amplifying the influence of even-hop neighbors through an even-polynomial graph
filter grounded in balance theory, enabling the indirect mining of higher-order
homogeneous information. This dual approach enables SOC-DGL to effectively and
comprehensively capture similarity information across diverse interaction
scales within the affinity matrices and drug-target association matrices,
significantly enhancing the model's generalization capability and predictive
accuracy in DTI tasks. To address the issue of imbalance in drug-target
interaction datasets, this paper proposes an adjustable imbalance loss function
that mitigates the impact of sample imbalance by adjusting the weight of
negative samples and a parameter. Extensive experiments on four benchmark
datasets demonstrate significant accuracy improvements achieved by SOC-DGL,
particularly in scenarios involving data imbalance and unseen drugs or targets.

</details>


### [273] [Self-supervised Latent Space Optimization with Nebula Variational Coding](https://arxiv.org/abs/2506.01414)
*Yida Wang,David Joseph Tan,Nassir Navab,Federico Tombari*

Main category: cs.LG

TL;DR: This paper proposes Nebula Variational Coding (NVC), a variational inference model using probabilistic models and nebula anchors to optimize latent manifolds for better performance in classification, segmentation, completion, and reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to design a general solution that optimizes latent manifolds through probabilistic models to improve the performance on various tasks such as classification, segmentation, completion, and reconstruction.

Method: The method introduces nebula anchors in the latent space to guide latent variables to form clusters during training. A variational constraint enforces the latent features within an anchor to form a Gaussian distribution leading to the Nebula Variational Coding (NVC) generative model. Additionally, metric learning is applied in a self-supervised way to make the separation between clusters more explicit.

Result: Experimentally, the proposed method can be used within different architectures designed to solve problems including text sequence, images, 3D point clouds, and volumetric data, validating its advantage.

Conclusion: The NVC model forms clustered embeddings which adapt to the generated semantic of the training data, improving performance across multiple domains.

Abstract: Deep learning approaches process data in a layer-by-layer way with
intermediate (or latent) features. We aim at designing a general solution to
optimize the latent manifolds to improve the performance on classification,
segmentation, completion and/or reconstruction through probabilistic models.
This paper proposes a variational inference model which leads to a clustered
embedding. We introduce additional variables in the latent space, called
\textbf{nebula anchors}, that guide the latent variables to form clusters
during training. To prevent the anchors from clustering among themselves, we
employ the variational constraint that enforces the latent features within an
anchor to form a Gaussian distribution, resulting in a generative model we
refer as Nebula Variational Coding (NVC). Since each latent feature can be
labeled with the closest anchor, we also propose to apply metric learning in a
self-supervised way to make the separation between clusters more explicit. As a
consequence, the latent variables of our variational coder form clusters which
adapt to the generated semantic of the training data, \textit{e.g.} the
categorical labels of each sample. We demonstrate experimentally that it can be
used within different architectures designed to solve different problems
including text sequence, images, 3D point clouds and volumetric data,
validating the advantage of our proposed method.

</details>


### [274] [Variance-Based Defense Against Blended Backdoor Attacks](https://arxiv.org/abs/2506.01444)
*Sujeevan Aseervatham,Achraf Kerzazi,Younès Bennani*

Main category: cs.LG

TL;DR: 提出了一种新的防御后门攻击的方法，通过训练模型、检测中毒类别并提取攻击触发器的关键部分来增强可解释性，并在知名图像数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的防御机制通常依赖于干净的数据集来计算统计异常，但在现实世界中，这种干净的数据集可能不可用或已被破坏。

Method: 该方法首先在给定的数据集上训练模型，然后检测被污染的类别，并提取攻击触发器的关键部分，最后识别出中毒实例。此方法通过明确揭示触发器的有害部分来提高可解释性。

Result: 通过在知名图像数据集上的实验评估以及与三种最先进的算法（SCAn, ABL, 和 AGPD）的比较分析，证明了该方法的有效性。

Conclusion: 所提出的新型防御方法能够在没有干净数据集的情况下有效检测和防御后门攻击，并提高了对攻击触发器的理解和解释。

Abstract: Backdoor attacks represent a subtle yet effective class of cyberattacks
targeting AI models, primarily due to their stealthy nature. The model behaves
normally on clean data but exhibits malicious behavior only when the attacker
embeds a specific trigger into the input. This attack is performed during the
training phase, where the adversary corrupts a small subset of the training
data by embedding a pattern and modifying the labels to a chosen target. The
objective is to make the model associate the pattern with the target label
while maintaining normal performance on unaltered data. Several defense
mechanisms have been proposed to sanitize training data-sets. However, these
methods often rely on the availability of a clean dataset to compute
statistical anomalies, which may not always be feasible in real-world scenarios
where datasets can be unavailable or compromised. To address this limitation,
we propose a novel defense method that trains a model on the given dataset,
detects poisoned classes, and extracts the critical part of the attack trigger
before identifying the poisoned instances. This approach enhances
explainability by explicitly revealing the harmful part of the trigger. The
effectiveness of our method is demonstrated through experimental evaluations on
well-known image datasets and comparative analysis against three
state-of-the-art algorithms: SCAn, ABL, and AGPD.

</details>


### [275] [ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things](https://arxiv.org/abs/2506.01450)
*Manuel Franco de la Peña,Ángel Luis Perales Gómez,Lorenzo Fernández Maimó*

Main category: cs.LG

TL;DR: In this paper, researchers developed a new method called ShaTS which enhances the precision of Shapley value explanations for time series models in Industrial Internet of Things environments. It incorporates an a priori feature grouping strategy that preserves temporal dependencies and produces both coherent and actionable insights. Experiments on the SWaT dataset show that ShaTS accurately identifies critical time instants, affected sensors, actuators, and processes, outperforming SHAP in explainability and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Anomaly Detection and explanation techniques are crucial to ensure operational safety in Industrial Internet of Things environments. While conventional methods have made improvements using Machine Learning and Deep Learning models, they often neglect the temporal structure of data leading to imprecise or less actionable explanations.

Method: The researchers proposed ShaTS (Shapley values for Time Series models), a model-agnostic explainable Artificial Intelligence method. It uses an a priori feature grouping strategy to preserve temporal dependencies in time series data, thereby producing more precise, coherent, and actionable insights.

Result: Experiments on the SWaT dataset demonstrated that ShaTS can accurately identify critical time instants and pinpoint affected sensors, actuators, and processes. It also outperforms SHAP in terms of explainability and resource efficiency, meeting the real-time requirements of industrial environments.

Conclusion: ShaTS is a promising method for enhancing the precision of Shapley value explanations in time series models, providing both coherent and actionable insights for Anomaly Detection in Industrial Internet of Things environments.

Abstract: Industrial Internet of Things environments increasingly rely on advanced
Anomaly Detection and explanation techniques to rapidly detect and mitigate
cyberincidents, thereby ensuring operational safety. The sequential nature of
data collected from these environments has enabled improvements in Anomaly
Detection using Machine Learning and Deep Learning models by processing time
windows rather than treating the data as tabular. However, conventional
explanation methods often neglect this temporal structure, leading to imprecise
or less actionable explanations. This work presents ShaTS (Shapley values for
Time Series models), which is a model-agnostic explainable Artificial
Intelligence method designed to enhance the precision of Shapley value
explanations for time series models. ShaTS addresses the shortcomings of
traditional approaches by incorporating an a priori feature grouping strategy
that preserves temporal dependencies and produces both coherent and actionable
insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS
accurately identifies critical time instants, precisely pinpoints the sensors,
actuators, and processes affected by anomalies, and outperforms SHAP in terms
of both explainability and resource efficiency, fulfilling the real-time
requirements of industrial environments.

</details>


### [276] [Feature-aware Hypergraph Generation via Next-Scale Prediction](https://arxiv.org/abs/2506.01467)
*Dorian Gailhard,Enzo Tartaglione,Lirida Naviner,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: FAHNES is a new method for generating hypergraph topology and features through a hierarchical approach, showing competitive results in reconstructing topology and features.


<details>
  <summary>Details</summary>
Motivation: Existing hypergraph generation methods focus only on topology, ignoring feature modeling. This work aims to address this gap by introducing FAHNES, which can jointly generate hypergraph topology and features.

Method: FAHNES uses a hierarchical approach that builds a multi-scale representation through node coarsening, then learns to reconstruct finer levels via localized expansion and refinement, guided by a new node budget mechanism that controls cluster splitting.

Result: FAHNES achieves competitive results in reconstructing topology and features when evaluated on synthetic hypergraphs, 3D meshes, and molecular datasets.

Conclusion: FAHNES establishes a foundation for future research in featured hypergraph generative modeling.

Abstract: Hypergraphs generalize traditional graphs by allowing hyperedges to connect
multiple nodes, making them well-suited for modeling complex structures with
higher-order relationships, such as 3D meshes, molecular systems, and
electronic circuits. While topology is central to hypergraph structure, many
real-world applications also require node and hyperedge features. Existing
hypergraph generation methods focus solely on topology, often overlooking
feature modeling. In this work, we introduce FAHNES (feature-aware hypergraph
generation via next-scale prediction), a hierarchical approach that jointly
generates hypergraph topology and features. FAHNES builds a multi-scale
representation through node coarsening, then learns to reconstruct finer levels
via localized expansion and refinement, guided by a new node budget mechanism
that controls cluster splitting. We evaluate FAHNES on synthetic hypergraphs,
3D meshes, and molecular datasets. FAHNES achieves competitive results in
reconstructing topology and features, establishing a foundation for future
research in featured hypergraph generative modeling.

</details>


### [277] [MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions](https://arxiv.org/abs/2506.01478)
*Tung-Lam Ngo,Ba-Hoang Tran,Duy-Cat Can,Trung-Hieu Do,Oliver Y. Chén,Hoang-Quynh Le*

Main category: cs.LG

TL;DR: This paper presents MUDI, a large-scale multimodal dataset for understanding drug-drug interactions (DDI), and benchmarks learning methods to study it.


<details>
  <summary>Details</summary>
Motivation: Existing DDI datasets primarily focus on textual information, overlooking multimodal data that reflect complex drug mechanisms.

Method: MUDI provides a comprehensive multimodal representation of drugs by combining pharmacological text, chemical formulas, molecular structure graphs, and images across 310,532 annotated drug pairs labeled as Synergism, Antagonism, or New Effect. The dataset includes unseen drug pairs in the test set for effective evaluation of machine-learning based generalization. Benchmark models are evaluated using both late fusion voting and intermediate fusion strategies.

Result: All data, annotations, evaluation scripts, and baselines are released under an open research license.

Conclusion: MUDI is introduced as a large-scale multimodal biomedical dataset for understanding pharmacodynamic Drug-drug Interactions, along with benchmarking learning methods to study it.

Abstract: Understanding the interaction between different drugs (drug-drug interaction
or DDI) is critical for ensuring patient safety and optimizing therapeutic
outcomes. Existing DDI datasets primarily focus on textual information,
overlooking multimodal data that reflect complex drug mechanisms. In this
paper, we (1) introduce MUDI, a large-scale Multimodal biomedical dataset for
Understanding pharmacodynamic Drug-drug Interactions, and (2) benchmark
learning methods to study it. In brief, MUDI provides a comprehensive
multimodal representation of drugs by combining pharmacological text, chemical
formulas, molecular structure graphs, and images across 310,532 annotated drug
pairs labeled as Synergism, Antagonism, or New Effect. Crucially, to
effectively evaluate machine-learning based generalization, MUDI consists of
unseen drug pairs in the test set. We evaluate benchmark models using both late
fusion voting and intermediate fusion strategies. All data, annotations,
evaluation scripts, and baselines are released under an open research license.

</details>


### [278] [Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?](https://arxiv.org/abs/2506.01482)
*Zijian Zhao,Dian Jin,Zijing Zhou,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: Skip-BART是一种新的自动舞台灯光控制方法，首次将ASLC视为生成任务而非分类问题，通过修改BART模型和加入skip connection机制，从经验丰富的灯光工程师直接学习，输出与音乐相匹配的灯光色彩和强度。实验表明，该方法超越传统基于规则的方法，并在人类评估中接近专业灯光工程师的表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动舞台灯光控制（ASLC）方法大多依赖音乐分类并映射到预定义灯光模式，导致结果机械化且缺乏灵活性。因此需要一种更智能、生成式的解决方案来提升灯光效果的多样性和合理性。

Method: 提出了一种名为Skip-BART的方法，其核心是调整BART模型以接受音频输入并生成灯光的色调和强度值。同时引入了新颖的skip connection机制，增强音乐与灯光之间的关联性。

Result: Skip-BART在所有评估指标上均优于传统的基于规则的方法，并且在人类评估中表现仅与专业灯光工程师存在有限差距（p值为0.72），证明其性能接近人类水平。

Conclusion: Skip-BART提供了一种创新的端到端ASLC解决方案，成功地将该问题转化为生成任务。研究还开放了数据集、代码和训练模型参数，支持未来进一步的研究和发展。

Abstract: Stage lighting plays an essential role in live music performances,
influencing the engaging experience of both musicians and audiences. Given the
high costs associated with hiring or training professional lighting engineers,
Automatic Stage Lighting Control (ASLC) has gained increasing attention.
However, most existing approaches only classify music into limited categories
and map them to predefined light patterns, resulting in formulaic and
monotonous outcomes that lack rationality. To address this issue, this paper
presents an end-to-end solution that directly learns from experienced lighting
engineers -- Skip-BART. To the best of our knowledge, this is the first work to
conceptualize ASLC as a generative task rather than merely a classification
problem. Our method modifies the BART model to take audio music as input and
produce light hue and value (intensity) as output, incorporating a novel skip
connection mechanism to enhance the relationship between music and light within
the frame grid.We validate our method through both quantitative analysis and an
human evaluation, demonstrating that Skip-BART outperforms conventional
rule-based methods across all evaluation metrics and shows only a limited gap
compared to real lighting engineers.Specifically, our method yields a p-value
of 0.72 in a statistical comparison based on human evaluations with human
lighting engineers, suggesting that the proposed approach closely matches human
lighting engineering performance. To support further research, we have made our
self-collected dataset, code, and trained model parameters available at
https://github.com/RS2002/Skip-BART .

</details>


### [279] [Model-agnostic Mitigation Strategies of Data Imbalance for Regression](https://arxiv.org/abs/2506.01486)
*Jelke Wibbeke,Sebastian Rohjans,Andreas Rauh*

Main category: cs.LG

TL;DR: 解决回归任务中数据不平衡问题，提出新的评估数据重要性的方法和改进的采样技术，并通过综合评估证明了新方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 数据不平衡在回归任务中是一个持续存在的挑战，导致模型性能偏差并削弱预测可靠性，尤其是在预测稀有事件时。

Method: 引入密度-距离和密度-比率相关函数来更好地评估数据的重要性；提出高级缓解技术cSMOGN和crbSMOGN以改进现有采样方法；构建包含不平衡缓解训练和无不平衡缓解训练的模型集成以减少负面影响。

Result: 大多数策略在提高稀有样本性能的同时会降低常见样本的性能，但所提出的模型集成方法可以显著减少这些负面效果；crbSMOGN采样技术与密度-比率相关函数在神经网络中的表现优于现有方法。

Conclusion: 新型crbSMOGN采样技术和密度-比率相关函数在处理数据不平衡方面表现出色，推荐使用模型集成方法以平衡对稀有和常见样本的预测性能。

Abstract: Data imbalance persists as a pervasive challenge in regression tasks,
introducing bias in model performance and undermining predictive reliability.
This is particularly detrimental in applications aimed at predicting rare
events that fall outside the domain of the bulk of the training data. In this
study, we review the current state-of-the-art regarding sampling-based methods
and cost-sensitive learning. Additionally, we propose novel approaches to
mitigate model bias. To better asses the importance of data, we introduce the
density-distance and density-ratio relevance functions, which effectively
integrate empirical frequency of data with domain-specific preferences,
offering enhanced interpretability for end-users. Furthermore, we present
advanced mitigation techniques (cSMOGN and crbSMOGN), which build upon and
improve existing sampling methods. In a comprehensive quantitative evaluation,
we benchmark state-of-the-art methods on 10 synthetic and 42 real-world
datasets, using neural networks, XGBoosting trees and Random Forest models. Our
analysis reveals that while most strategies improve performance on rare
samples, they often degrade it on frequent ones. We demonstrate that
constructing an ensemble of models -- one trained with imbalance mitigation and
another without -- can significantly reduce these negative effects. The key
findings underscore the superior performance of our novel crbSMOGN sampling
technique with the density-ratio relevance function for neural networks,
outperforming state-of-the-art methods.

</details>


### [280] [Confidence-Aware Self-Distillation for Multimodal Sentiment Analysis with Incomplete Modalities](https://arxiv.org/abs/2506.01490)
*Yanxi Luo,Shijin Wang,Zhongxing Xu,Yulong Li,Feilong Tang,Jionglong Su*

Main category: cs.LG

TL;DR: In this paper, a Confidence-Aware Self-Distillation (CASD) strategy is proposed for multimodal sentiment analysis to handle modality missingness and enhance robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for handling modality missingness in multimodal sentiment analysis neglect the confidence in multimodal combinations and impose constraints on intra-class representation, which hinders capturing modality-specific information.

Method: The authors propose a Confidence-Aware Self-Distillation (CASD) strategy that incorporates multimodal probabilistic embeddings via a mixture of Student's $t$-distributions. This strategy estimates joint distributions with uncertainty scores and reduces uncertainty in the student network by consistency distillation. A reparameterization representation module is also introduced to facilitate robust multimodal learning.

Result: Experimental results on three benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance.

Conclusion: The CASD strategy effectively addresses the challenges of modality missingness and enhances robustness in multimodal sentiment analysis.

Abstract: Multimodal sentiment analysis (MSA) aims to understand human sentiment
through multimodal data. In real-world scenarios, practical factors often lead
to uncertain modality missingness. Existing methods for handling modality
missingness are based on data reconstruction or common subspace projections.
However, these methods neglect the confidence in multimodal combinations and
impose constraints on intra-class representation, hindering the capture of
modality-specific information and resulting in suboptimal performance. To
address these challenges, we propose a Confidence-Aware Self-Distillation
(CASD) strategy that effectively incorporates multimodal probabilistic
embeddings via a mixture of Student's $t$-distributions, enhancing its
robustness by incorporating confidence and accommodating heavy-tailed
properties. This strategy estimates joint distributions with uncertainty scores
and reduces uncertainty in the student network by consistency distillation.
Furthermore, we introduce a reparameterization representation module that
facilitates CASD in robust multimodal learning by sampling embeddings from the
joint distribution for the prediction module to calculate the task loss. As a
result, the directional constraint from the loss minimization is alleviated by
the sampled representation. Experimental results on three benchmark datasets
demonstrate that our method achieves state-of-the-art performance.

</details>


### [281] [Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme](https://arxiv.org/abs/2506.01502)
*Mikhail Persiianov,Jiawei Chen,Petr Mokrov,Alexander Tyurin,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 提出了一种名为$\texttt{iJKOnet}$的新方法，通过结合JKO框架与逆优化技术来学习群体动力学，采用端到端的对抗训练过程，并提供了理论保证，性能优于之前的JKO相关方法。


<details>
  <summary>Details</summary>
Motivation: 现有的学习群体动力学的方法已经将问题表述为概率空间中的能量最小化问题，并使用JKO方案进行时间离散化。然而，这些方法可能受限于特定的架构选择（如输入凸神经网络）。因此，需要一种更灵活且高效的新方法。

Method: 引入了$\texttt{iJKOnet}$方法，该方法结合了JKO框架和逆优化技术来学习群体动力学。具体来说，它采用了常规的端到端对抗训练过程，避免了对特定架构（例如输入凸神经网络）的需求。

Result: 建立了所提出方法的理论保证，并在实验中证明其性能优于先前基于JKO的方法。

Conclusion: $\texttt{iJKOnet}$提供了一种无需限制性架构选择的端到端对抗训练方法，可以更有效地学习群体动力学，同时具有理论支持和实证优势。

Abstract: Learning population dynamics involves recovering the underlying process that
governs particle evolution, given evolutionary snapshots of samples at discrete
time points. Recent methods frame this as an energy minimization problem in
probability space and leverage the celebrated JKO scheme for efficient time
discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that
combines the JKO framework with inverse optimization techniques to learn
population dynamics. Our method relies on a conventional $\textit{end-to-end}$
adversarial training procedure and does not require restrictive architectural
choices, e.g., input-convex neural networks. We establish theoretical
guarantees for our methodology and demonstrate improved performance over prior
JKO-based methods.

</details>


### [282] [Analyzing the Importance of Blank for CTC-Based Knowledge Distillation](https://arxiv.org/abs/2506.01503)
*Benedikt Hilmes,Nick Rossenbach,Ralf Schlüter*

Main category: cs.LG

TL;DR: The paper explores different CTC-based distillation variants for pre-trained speech recognition models, focusing on blank token handling. A symmetric selection method is introduced to remove CTC loss during knowledge distillation with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of runtime and cost in inference of large pre-trained speech recognition models, the authors aim to distill their knowledge into smaller models while retaining efficiency.

Method: The authors explore various CTC-based distillation methods, particularly focusing on how blank tokens are handled. They introduce new blank selection patterns including a symmetric selection method which allows them to remove CTC loss during the knowledge distillation process.

Result: The symmetric selection method effectively removes CTC loss with little to no degradation in performance, making the training independent from target labels and potentially enabling distillation on untranscribed audio data.

Conclusion: This work shows promise in improving the efficiency of smaller models derived from large pre-trained speech recognition models by optimizing the way blank tokens are managed during knowledge distillation.

Abstract: With the rise of large pre-trained foundation models for automatic speech
recognition new challenges appear. While the performance of these models is
good, runtime and cost of inference increases. One approach to make use of
their strength while retaining efficiency is to distill their knowledge to
smaller models during training. In this work, we explore different CTC-based
distillation variants, focusing on blank token handling. We show that common
approaches like blank elimination do not always work off the shelf. We explore
new blank selection patterns as a potential sweet spot between standard
knowledge distillation and blank elimination mechanisms. Through the
introduction of a symmetric selection method, we are able to remove the CTC
loss during knowledge distillation with minimal to no performance degradation.
With this, we make the training independent from target labels, potentially
allowing for distillation on untranscribed audio data.

</details>


### [283] [Beyond Diagonal Covariance: Flexible Posterior VAEs via Free-Form Injective Flows](https://arxiv.org/abs/2506.01522)
*Peter Sorrenson,Lukas Lührs,Hans Olischläger,Ullrich Köthe*

Main category: cs.LG

TL;DR: This paper highlights the limitations of diagonal covariance Variational Autoencoders (VAEs) and proposes a regularized variant of Free-form Injective Flow (FIF) as an alternative that achieves full Gaussian covariance while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inherent limitations in representational capacity of diagonal covariance VAEs, which restrict their ability to model complex data distributions effectively.

Method: The method involves using differential geometry arguments to illustrate the limitations of diagonal covariance VAEs and introducing a regularized FIF variant interpreted as a VAE with a highly flexible posterior equivalent to full Gaussian covariance.

Result: Experiments on image datasets show that incorporating full covariance significantly improves model likelihood compared to standard diagonal covariance VAEs.

Conclusion: A regularized FIF can serve as an effective alternative to traditional VAEs, offering the benefits of full covariance while retaining computational efficiency.

Abstract: Variational Autoencoders (VAEs) are powerful generative models widely used
for learning interpretable latent spaces, quantifying uncertainty, and
compressing data for downstream generative tasks. VAEs typically rely on
diagonal Gaussian posteriors due to computational constraints. Using arguments
grounded in differential geometry, we demonstrate inherent limitations in the
representational capacity of diagonal covariance VAEs, as illustrated by
explicit low-dimensional examples. In response, we show that a regularized
variant of the recently introduced Free-form Injective Flow (FIF) can be
interpreted as a VAE featuring a highly flexible, implicitly defined posterior.
Crucially, this regularization yields a posterior equivalent to a full Gaussian
covariance distribution, yet maintains computational costs comparable to
standard diagonal covariance VAEs. Experiments on image datasets validate our
approach, demonstrating that incorporating full covariance substantially
improves model likelihood.

</details>


### [284] [Alignment as Distribution Learning: Your Preference Model is Explicitly a Language Model](https://arxiv.org/abs/2506.01523)
*Jihun Yun,Juno Kim,Jongho Park,Junhyuck Kim,Jongha Jon Ryu,Jaewoong Cho,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: This paper rethinks the alignment problem by reframing it as distribution learning from pairwise preference feedback, proposing three new learning objectives that outperform current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The standard RLHF objective lacks theoretical justification and encourages degenerate, deterministic solutions. Variants like Direct Policy Optimization (DPO) also inherit this issue.

Method: Reframing alignment as distribution learning from pairwise preference feedback. Explicitly modeling how information about the target language model bleeds through the preference data led to three principled learning objectives: preference maximum likelihood estimation, preference distillation, and reverse KL minimization.

Result: All three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to the target language model, naturally avoiding degeneracy and reward overfitting. Preference distillation especially consistently outperforms or matches the performances of RLHF and DPO across various tasks and models.

Conclusion: Reframing alignment as distribution learning provides a more theoretically justified approach that avoids issues with degeneracy and overfitting. The proposed methods, particularly preference distillation, show superior performance compared to existing techniques.

Abstract: Alignment via reinforcement learning from human feedback (RLHF) has become
the dominant paradigm for controlling the quality of outputs from large
language models (LLMs). However, when viewed as `loss + regularization,' the
standard RLHF objective lacks theoretical justification and incentivizes
degenerate, deterministic solutions, an issue that variants such as Direct
Policy Optimization (DPO) also inherit. In this paper, we rethink alignment by
framing it as \emph{distribution learning} from pairwise preference feedback by
explicitly modeling how information about the target language model bleeds
through the preference data. This explicit modeling leads us to propose three
principled learning objectives: preference maximum likelihood estimation,
preference distillation, and reverse KL minimization. We theoretically show
that all three approaches enjoy strong non-asymptotic $O(1/n)$ convergence to
the target language model, naturally avoiding degeneracy and reward
overfitting. Finally, we empirically demonstrate that our distribution learning
framework, especially preference distillation, consistently outperforms or
matches the performances of RLHF and DPO across various tasks and models.

</details>


### [285] [Learning Abstract World Models with a Group-Structured Latent Space](https://arxiv.org/abs/2506.01529)
*Thomas Delliaux,Nguyen-Khanh Vu,Vincent François-Lavet,Elise van der Pol,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: This paper explores the use of geometric priors in the low-dimensional representation manifold of a learned transition model for MDPs, incorporating symmetric structures and unstructured information to improve predictions and learning in RL tasks.


<details>
  <summary>Details</summary>
Motivation: Learning meaningful abstract models of Markov Decision Processes (MDPs) is crucial for improving generalization from limited data.

Method: Imposing geometric priors on the low-dimensional representation manifold of a learned transition model, incorporating known symmetric structures via appropriate choices of the latent space and the associated group actions, and embedding additional unstructured information alongside these symmetries.

Result: Experimentally better predictions of the latent transition model than fully unstructured approaches, as well as better learning on downstream RL tasks, simpler and more disentangled representations.

Conclusion: The method leads to improved predictions, better learning in RL tasks, and simpler, more disentangled representations.

Abstract: Learning meaningful abstract models of Markov Decision Processes (MDPs) is
crucial for improving generalization from limited data. In this work, we show
how geometric priors can be imposed on the low-dimensional representation
manifold of a learned transition model. We incorporate known symmetric
structures via appropriate choices of the latent space and the associated group
actions, which encode prior knowledge about invariances in the environment. In
addition, our framework allows the embedding of additional unstructured
information alongside these symmetries. We show experimentally that this leads
to better predictions of the latent transition model than fully unstructured
approaches, as well as better learning on downstream RL tasks, in environments
with rotational and translational features, including in first-person views of
3D environments. Additionally, our experiments show that this leads to simpler
and more disentangled representations. The full code is available on GitHub to
ensure reproducibility.

</details>


### [286] [A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments](https://arxiv.org/abs/2506.01533)
*Yuchen Ma,Jonas Schweisthal,Hengrui Zhang,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper proposes DIME, a novel diffusion-based method to learn the joint distribution of multiple medical treatment outcomes.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning methods for predicting treatment effects mainly focus on single-outcome settings, while medical data often include multiple, interdependent outcomes.

Method: DIME is proposed to learn the joint distribution of multiple outcomes of medical treatments. It addresses three challenges: (i) learning the joint interventional distribution for reliable decision-making; (ii) capturing the dependence structure between outcomes; and (iii) handling mixed type outcomes. Causal inference problem is taken into account through causal masking. The joint distribution is decomposed into conditional distributions with customized conditional masking during training, and predictions are generated auto-regressively during inference.

Result: Across various experiments, DIME effectively learns the joint distribution and captures shared information among multiple outcomes.

Conclusion: DIME is the first neural method tailored to learn the joint, multi-outcome distribution of medical treatments, which can improve decision-making in medical practice.

Abstract: In medicine, treatments often influence multiple, interdependent outcomes,
such as primary endpoints, complications, adverse events, or other secondary
endpoints. Hence, to make optimal treatment decisions, clinicians are
interested in learning the distribution of multi-dimensional treatment
outcomes. However, the vast majority of machine learning methods for predicting
treatment effects focus on single-outcome settings, despite the fact that
medical data often include multiple, interdependent outcomes. To address this
limitation, we propose a novel diffusion-based method called DIME to learn the
joint distribution of multiple outcomes of medical treatments. We addresses
three challenges relevant in medical practice: (i)it is tailored to learn the
joint interventional distribution of multiple medical outcomes, which enables
reliable decision-making with uncertainty quantification rather than relying
solely on point estimates; (ii)it explicitly captures the dependence structure
between outcomes; (iii)it can handle outcomes of mixed type, including binary,
categorical, and continuous variables. In DIME, we take into account the
fundamental problem of causal inference through causal masking. For training,
our method decomposes the joint distribution into a series of conditional
distributions with a customized conditional masking to account for the
dependence structure across outcomes. For inference, our method
auto-regressively generates predictions. This allows our method to move beyond
point estimates of causal quantities and thus learn the joint interventional
distribution. To the best of our knowledge, DIME is the first neural method
tailored to learn the joint, multi-outcome distribution of medical treatments.
Across various experiments, we demonstrate that our method effectively learns
the joint distribution and captures shared information among multiple outcomes.

</details>


### [287] [Adaptive Destruction Processes for Diffusion Samplers](https://arxiv.org/abs/2506.01541)
*Timofei Gritsaev,Nikita Morozov,Kirill Tamogashev,Daniil Tiapkin,Sergey Samsonov,Alexey Naumov,Dmitry Vetrov,Nikolay Malkin*

Main category: cs.LG

TL;DR: This paper investigates the challenges and benefits of a trainable destruction process in diffusion samplers, proposing to trade elegance of theory for flexibility in generative and destruction policy definitions. By decoupling generation and destruction variances, it achieves faster convergence and improved sampling quality with fewer steps.


<details>
  <summary>Details</summary>
Motivation: To explore the potential improvements in diffusion-based generative models by viewing them as discrete-time policies instead of approximations to continuous-time models, and to investigate how training both generation and destruction processes can lead to better performance.

Method: Decouple the generation and destruction variances, allowing both transition kernels to be learned as unconstrained Gaussian densities. Train both generation and destruction processes simultaneously when the number of steps is limited.

Result: Faster convergence and improved sampling quality on various benchmarks, with successful scalability demonstrated through experiments on GAN latent space sampling for conditional image generation.

Conclusion: Training both generation and destruction processes with decoupled variances leads to better performance in diffusion samplers, and the approach is scalable for more complex tasks.

Abstract: This paper explores the challenges and benefits of a trainable destruction
process in diffusion samplers -- diffusion-based generative models trained to
sample an unnormalised density without access to data samples. Contrary to the
majority of work that views diffusion samplers as approximations to an
underlying continuous-time model, we view diffusion models as discrete-time
policies trained to produce samples in very few generation steps. We propose to
trade some of the elegance of the underlying theory for flexibility in the
definition of the generative and destruction policies. In particular, we
decouple the generation and destruction variances, enabling both transition
kernels to be learned as unconstrained Gaussian densities. We show that, when
the number of steps is limited, training both generation and destruction
processes results in faster convergence and improved sampling quality on
various benchmarks. Through a robust ablation study, we investigate the design
choices necessary to facilitate stable training. Finally, we show the
scalability of our approach through experiments on GAN latent space sampling
for conditional image generation.

</details>


### [288] [Temporal Variational Implicit Neural Representations](https://arxiv.org/abs/2506.01544)
*Batuhan Koyuncu,Rachael DeVries,Ole Winther,Isabel Valera*

Main category: cs.LG

TL;DR: The paper introduces Temporal Variational Implicit Neural Representations (TV-INRs) for modeling irregular multivariate time series, providing efficient imputation and forecasting with a focus on individualized predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in modeling irregular multivariate time series by offering a more efficient and accurate method for imputation and forecasting that does not require extensive training or fine-tuning.

Method: Temporal Variational Implicit Neural Representations (TV-INRs) combine implicit neural representations with latent variable models to learn distributions over time-continuous generator functions conditioned on signal-specific covariates.

Result: Experiments show that TV-INRs can accurately solve diverse imputation and forecasting tasks with a single instance and excel particularly in low-data regimes, outperforming existing methods by an order of magnitude in mean squared error for imputation.

Conclusion: TV-INRs provide a computationally efficient and scalable solution for real-world applications involving irregular multivariate time series.

Abstract: We introduce Temporal Variational Implicit Neural Representations (TV-INRs),
a probabilistic framework for modeling irregular multivariate time series that
enables efficient individualized imputation and forecasting. By integrating
implicit neural representations with latent variable models, TV-INRs learn
distributions over time-continuous generator functions conditioned on
signal-specific covariates. Unlike existing approaches that require extensive
training, fine-tuning or meta-learning, our method achieves accurate
individualized predictions through a single forward pass. Our experiments
demonstrate that with a single TV-INRs instance, we can accurately solve
diverse imputation and forecasting tasks, offering a computationally efficient
and scalable solution for real-world applications. TV-INRs excel especially in
low-data regimes, where it outperforms existing methods by an order of
magnitude in mean squared error for imputation task.

</details>


### [289] [Class Incremental Learning for Algorithm Selection](https://arxiv.org/abs/2506.01545)
*Mate Botond Nemeth,Emma Hart,Kevin Sim,Quentin Renau*

Main category: cs.LG

TL;DR: In the context of algorithm selection for optimisation, this paper explores Class Incremental Learning (CIL) methods to handle streaming data scenarios where instances and class labels can grow over time. Benchmarks on a bin-packing dataset reveal that rehearsal-based methods outperform other CIL methods in terms of managing catastrophic forgetting, with only a small loss of around 7%.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting the best solver from a portfolio when instances arrive in a stream, leading to the need for periodic updates of classification models without catastrophic forgetting.

Method: Benchmarked 8 continual learning methods using a bin-packing dataset to evaluate their ability to withstand catastrophic forgetting in streaming optimisation scenarios.

Result: Rehearsal-based methods significantly outperform other CIL methods. Evidence of forgetting exists but with a small loss of about 7%.

Conclusion: Rehearsal-based methods are viable for continual learning in streaming optimisation scenarios.

Abstract: Algorithm selection is commonly used to predict the best solver from a
portfolio per per-instance. In many real scenarios, instances arrive in a
stream: new instances become available over time, while the number of class
labels can also grow as new data distributions arrive downstream. As a result,
the classification model needs to be periodically updated to reflect additional
solvers without catastrophic forgetting of past data. In machine-learning (ML),
this is referred to as Class Incremental Learning (CIL). While commonly
addressed in ML settings, its relevance to algorithm-selection in optimisation
has not been previously studied. Using a bin-packing dataset, we benchmark 8
continual learning methods with respect to their ability to withstand
catastrophic forgetting. We find that rehearsal-based methods significantly
outperform other CIL methods. While there is evidence of forgetting, the loss
is small at around 7%. Hence, these methods appear to be a viable approach to
continual learning in streaming optimisation scenarios.

</details>


### [290] [To Each Metric Its Decoding: Post-Hoc Optimal Decision Rules of Probabilistic Hierarchical Classifiers](https://arxiv.org/abs/2506.01552)
*Roman Plaud,Alexandre Perez-Lebel,Matthieu Labeau,Antoine Saillenfest,Thomas Bonald*

Main category: cs.LG

TL;DR: This paper proposes a framework for optimal decoding of an output probability distribution in hierarchical classification, deriving decision rules that align with task-specific metrics, and demonstrates its superiority through empirical evaluations.


<details>
  <summary>Details</summary>
Motivation: Hierarchical classification often uses heuristic decision rules which may not align well with task-specific evaluation metrics.

Method: The authors propose a framework to optimally decode the output probability distribution concerning a target metric. They derive optimal decision rules for complex prediction settings, focusing on hierarchical $hF_{\beta}$ scores when predicting a subset of nodes.

Result: Empirical evaluations show the superiority of the proposed optimal strategies, especially in underdetermined scenarios, enhancing the performance and reliability of hierarchical classifiers.

Conclusion: The proposed methods have the potential to significantly improve the performance and reliability of hierarchical classifiers in practical applications.

Abstract: Hierarchical classification offers an approach to incorporate the concept of
mistake severity by leveraging a structured, labeled hierarchy. However,
decoding in such settings frequently relies on heuristic decision rules, which
may not align with task-specific evaluation metrics. In this work, we propose a
framework for the optimal decoding of an output probability distribution with
respect to a target metric. We derive optimal decision rules for increasingly
complex prediction settings, providing universal algorithms when candidates are
limited to the set of nodes. In the most general case of predicting a subset of
nodes, we focus on rules dedicated to the hierarchical $hF_{\beta}$ scores,
tailored to hierarchical settings. To demonstrate the practical utility of our
approach, we conduct extensive empirical evaluations, showcasing the
superiority of our proposed optimal strategies, particularly in underdetermined
scenarios. These results highlight the potential of our methods to enhance the
performance and reliability of hierarchical classifiers in real-world
applications. The code is available at
https://github.com/RomanPlaud/hierarchical_decision_rules

</details>


### [291] [Unpacking Softmax: How Temperature Drives Representation Collapse, Compression, and Generalization](https://arxiv.org/abs/2506.01562)
*Wojciech Masarczyk,Mateusz Ostaszewski,Tin Sum Cheng,Tomasz Trzciński,Aurelien Lucchi,Razvan Pascanu*

Main category: cs.LG

TL;DR: 研究了softmax函数在塑造模型表示中的关键作用，引入了秩亏偏置的概念，并展示了如何利用softmax动态学习压缩表示或增强模型在分布外数据上的性能。通过验证不同架构和真实数据集的结果，强调了温度调节在提高模型性能中的广泛应用。这项工作为softmax机制提供了新的见解，使我们能够更好地控制深度神经网络中的表示学习。


<details>
  <summary>Details</summary>
Motivation: 尽管softmax函数被广泛使用且效果显著，但其对学习动态和学习表示的影响仍不明确，限制了优化模型行为的能力。因此，需要深入理解softmax函数的作用及其影响。

Method: 研究softmax函数在分类任务和注意力权重中的作用，引入秩亏偏置概念，并分析其与softmax函数的logits范数的关系，该范数受超参数隐式影响或由softmax温度直接修改。利用softmax动态学习压缩表示或增强模型在分布外数据上的性能。

Result: 通过在不同架构和真实世界数据集上的验证，证明了温度调节在改进模型性能方面的广泛适用性，揭示了softmax函数的新机制。

Conclusion: 这项工作为softmax机制提供了新见解，有助于更好地控制深度神经网络中的表示学习过程。

Abstract: The softmax function is a fundamental building block of deep neural networks,
commonly used to define output distributions in classification tasks or
attention weights in transformer architectures. Despite its widespread use and
proven effectiveness, its influence on learning dynamics and learned
representations remains poorly understood, limiting our ability to optimize
model behavior. In this paper, we study the pivotal role of the softmax
function in shaping the model's representation. We introduce the concept of
rank deficit bias - a phenomenon in which softmax-based deep networks find
solutions of rank much lower than the number of classes. This bias depends on
the softmax function's logits norm, which is implicitly influenced by
hyperparameters or directly modified by softmax temperature. Furthermore, we
demonstrate how to exploit the softmax dynamics to learn compressed
representations or to enhance their performance on out-of-distribution data. We
validate our findings across diverse architectures and real-world datasets,
highlighting the broad applicability of temperature tuning in improving model
performance. Our work provides new insights into the mechanisms of softmax,
enabling better control over representation learning in deep neural networks.

</details>


### [292] [Trajectory First: A Curriculum for Discovering Diverse Policies](https://arxiv.org/abs/2506.01568)
*Cornelius V. Braun,Sayantan Auddy,Marc Toussaint*

Main category: cs.LG

TL;DR: 为了解决强化学习中策略多样性不足的问题，本文提出了一种新的课程方法，该方法首先在轨迹级别进行探索，然后再学习基于步骤的策略。


<details>
  <summary>Details</summary>
Motivation: 现有的约束多样性强化学习方法在复杂任务（如机器人操作）中探索不足，导致策略多样性缺乏。

Method: 提出了一种课程方法，该方法首先在轨迹级别进行探索，然后再学习基于步骤的策略，以改善强化学习中的多样性优化。

Result: 通过实证评估，揭示了基于技能的多样性优化的不足，并证明所提出的课程可以提高所学技能的多样性。

Conclusion: 所提出的课程方法可以有效改善强化学习中策略的多样性，使代理在任务变化中更加鲁棒，并减少陷入局部最优的风险。

Abstract: Being able to solve a task in diverse ways makes agents more robust to task
variations and less prone to local optima. In this context, constrained
diversity optimization has emerged as a powerful reinforcement learning (RL)
framework to train a diverse set of agents in parallel. However, existing
constrained-diversity RL methods often under-explore in complex tasks such as
robotic manipulation, leading to a lack in policy diversity. To improve
diversity optimization in RL, we therefore propose a curriculum that first
explores at the trajectory level before learning step-based policies. In our
empirical evaluation, we provide novel insights into the shortcoming of
skill-based diversity optimization, and demonstrate empirically that our
curriculum improves the diversity of the learned skills.

</details>


### [293] [Latent Space Topology Evolution in Multilayer Perceptrons](https://arxiv.org/abs/2506.01569)
*Eduardo Paluzo-Hidalgo*

Main category: cs.LG

TL;DR: This paper proposes a topological framework for interpreting MLPs using simplicial complexes, enabling bi-persistence analysis to track data topology evolution through network layers. It includes stability theorems, an algorithm for computing persistence, and trajectory-based visualizations, with experiments showing its ability to identify redundant layers and critical transitions.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of how Multilayer Perceptrons (MLPs) process and transform data by analyzing the topological features across their layers.

Method: Construct a simplicial tower to capture data topology evolution, perform bi-persistence analysis including layer persistence and MLP persistence, prove stability theorems, develop a combinatorial algorithm for computation, and introduce trajectory-based visualizations.

Result: Identified redundant layers, revealed critical topological transitions, and provided interpretable insights into data organization for classification in both synthetic and real-world medical datasets.

Conclusion: The topological framework offers valuable insights into the internal representations of MLPs and can be used to improve their interpretability and efficiency.

Abstract: This paper introduces a topological framework for interpreting the internal
representations of Multilayer Perceptrons (MLPs). We construct a simplicial
tower, a sequence of simplicial complexes connected by simplicial maps, that
captures how data topology evolves across network layers. Our approach enables
bi-persistence analysis: layer persistence tracks topological features within
each layer across scales, while MLP persistence reveals how these features
transform through the network. We prove stability theorems for our topological
descriptors and establish that linear separability in latent spaces is related
to disconnected components in the nerve complexes. To make our framework
practical, we develop a combinatorial algorithm for computing MLP persistence
and introduce trajectory-based visualisations that track data flow through the
network. Experiments on synthetic and real-world medical data demonstrate our
method's ability to identify redundant layers, reveal critical topological
transitions, and provide interpretable insights into how MLPs progressively
organise data for classification.

</details>


### [294] [Bayes optimal learning of attention-indexed models](https://arxiv.org/abs/2506.01582)
*Fabrizio Boncoraglio,Emanuele Troiani,Vittorio Erba,Lenka Zdeborová*

Main category: cs.LG

TL;DR: The paper introduces AIM, a framework for analyzing learning in deep attention layers, providing closed-form predictions and identifying phase transitions.


<details>
  <summary>Details</summary>
Motivation: To create a theoretical framework that aligns closely with practical transformers and analyzes learning in deep attention layers.

Method: Introduced AIM which captures token-level outputs from high-dimensional embeddings interactions, using full-width key and query matrices. Used statistical mechanics and random matrix theory to derive predictions.

Result: Derived closed-form predictions for Bayes-optimal generalization error and identified sharp phase transitions based on sample complexity, model width, and sequence length.

Conclusion: AIM provides a solvable playground for understanding learning in modern attention architectures.

Abstract: We introduce the attention-indexed model (AIM), a theoretical framework for
analyzing learning in deep attention layers. Inspired by multi-index models,
AIM captures how token-level outputs emerge from layered bilinear interactions
over high-dimensional embeddings. Unlike prior tractable attention models, AIM
allows full-width key and query matrices, aligning more closely with practical
transformers. Using tools from statistical mechanics and random matrix theory,
we derive closed-form predictions for Bayes-optimal generalization error and
identify sharp phase transitions as a function of sample complexity, model
width, and sequence length. We propose a matching approximate message passing
algorithm and show that gradient descent can reach optimal performance. AIM
offers a solvable playground for understanding learning in modern attention
architectures.

</details>


### [295] [VirnyFlow: A Design Space for Responsible Model Development](https://arxiv.org/abs/2506.01584)
*Denys Herasymuk,Nazar Protsiv,Julia Stoyanovich*

Main category: cs.LG

TL;DR: The paper introduces VirnyFlow, a new design space for responsible ML model development. It allows users to define custom optimization criteria and perform extensive experimentation, outperforming current AutoML systems in optimization quality and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional AutoML frameworks which lack the ability to handle multi-objective real-world problems and provide responsible model development.

Method: VirnyFlow integrates evaluation protocol definition, multi-objective Bayesian optimization, cost-aware multi-armed bandits, query optimization, and distributed parallelism into a unified architecture to assist data scientists in building tailored ML pipelines.

Result: Significantly outperforms state-of-the-art AutoML systems in both optimization quality and scalability across five real-world benchmarks.

Conclusion: VirnyFlow offers a flexible, efficient, and responsible alternative to black-box automation in ML development.

Abstract: Developing machine learning (ML) models requires a deep understanding of
real-world problems, which are inherently multi-objective. In this paper, we
present VirnyFlow, the first design space for responsible model development,
designed to assist data scientists in building ML pipelines that are tailored
to the specific context of their problem. Unlike conventional AutoML
frameworks, VirnyFlow enables users to define customized optimization criteria,
perform comprehensive experimentation across pipeline stages, and iteratively
refine models in alignment with real-world constraints. Our system integrates
evaluation protocol definition, multi-objective Bayesian optimization,
cost-aware multi-armed bandits, query optimization, and distributed parallelism
into a unified architecture. We show that VirnyFlow significantly outperforms
state-of-the-art AutoML systems in both optimization quality and scalability
across five real-world benchmarks, offering a flexible, efficient, and
responsible alternative to black-box automation in ML development.

</details>


### [296] [Selecting for Less Discriminatory Algorithms: A Relational Search Framework for Navigating Fairness-Accuracy Trade-offs in Practice](https://arxiv.org/abs/2506.01594)
*Hana Samad,Michael Akinwumi,Jameel Khan,Christoph Mügge-Durum,Emmanuel O. Ogundimu*

Main category: cs.LG

TL;DR: 在高风险决策中选择合适的算法至关重要，尤其是在公平性方面。传统方法将公平性视为数学属性，并将其作为优化问题处理，但忽略了模型多样性的影响。本文通过更新数据和扩展Less Discriminatory Algorithms（LDAs）搜索过程，提出了在实际部署中的新方法。我们建议横向扩展LDA搜索以考虑不同模型家族间的公平性，这种方法可以补充或替代模型内部的超参数优化。通过使用横向LDA搜索与关系权衡框架，展示了如何在资源受限的情况下进行负责任的最小可行LDA搜索。组织可以调整这种方法来系统地比较、评估和选择能够优化公平性和准确性的LDAs。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型越来越多地嵌入到社会中，选择正确的算法成为了一个关键挑战，特别是在公平性方面。传统的模型公平性评估往往将公平性作为一个客观的数学属性，而忽略了模型多样性的重要性。

Method: 本文重新审视了Lee和Floridi(2021)的关系公平性方法，并使用更新的2021年房屋抵押贷款披露法(HMDA)数据，提出扩展LDA搜索范围，不仅在单个模型内进行超参数优化，还要跨模型家族进行横向比较。

Result: 通过使用横向LDA搜索与关系权衡框架，展示了一种在非实验、资源受限环境下负责任的最小可行LDA搜索方法。这种方法可以帮助组织系统地比较、评估和选择适合其特定领域的LDAs。

Conclusion: 横向扩展LDA搜索可以为在资源受限情况下实现公平性提供一种轻量级的方法，补充或替代模型内部的超参数优化。组织可以根据此方法调整策略，以优化公平性和准确性。

Abstract: As machine learning models are increasingly embedded into society through
high-stakes decision-making, selecting the right algorithm for a given task,
audience, and sector presents a critical challenge, particularly in the context
of fairness. Traditional assessments of model fairness have often framed
fairness as an objective mathematical property, treating model selection as an
optimization problem under idealized informational conditions. This overlooks
model multiplicity as a consideration--that multiple models can deliver similar
performance while exhibiting different fairness characteristics. Legal scholars
have engaged this challenge through the concept of Less Discriminatory
Algorithms (LDAs), which frames model selection as a civil rights obligation.
In real-world deployment, this normative challenge is bounded by constraints on
fairness experimentation, e.g., regulatory standards, institutional priorities,
and resource capacity.
  Against these considerations, the paper revisits Lee and Floridi (2021)'s
relational fairness approach using updated 2021 Home Mortgage Disclosure Act
(HMDA) data, and proposes an expansion of the scope of the LDA search process.
We argue that extending the LDA search horizontally, considering fairness
across model families themselves, provides a lightweight complement, or
alternative, to within-model hyperparameter optimization, when operationalizing
fairness in non-experimental, resource constrained settings. Fairness metrics
alone offer useful, but insufficient signals to accurately evaluate candidate
LDAs. Rather, by using a horizontal LDA search approach with the relational
trade-off framework, we demonstrate a responsible minimum viable LDA search on
real-world lending outcomes. Organizations can modify this approach to
systematically compare, evaluate, and select LDAs that optimize fairness and
accuracy in a sector-based contextualized manner.

</details>


### [297] [Understanding and Improving Laplacian Positional Encodings For Temporal GNNs](https://arxiv.org/abs/2506.01596)
*Yaniv Galron,Fabrizio Frasca,Haggai Maron,Eran Treister,Moshe Eliasof*

Main category: cs.LG

TL;DR: This paper addresses the limitations in positional encoding for temporal graphs by providing a theoretical framework, novel computational methods, and an extensive experimental study to identify their effectiveness.


<details>
  <summary>Details</summary>
Motivation: Temporal graph learning has various applications but progress in positional encoding is limited. Existing approaches using supra-Laplacian have shown promise but face challenges like high costs, limited understanding, and ambiguity in application.

Method: The paper connects supra-Laplacian encodings to per-time-slice encodings through a theoretical framework, introduces methods to reduce computational overhead, and conducts an extensive experimental study.

Result: Positional encodings can significantly boost performance in certain scenarios, but their effectiveness varies across different models.

Conclusion: Theoretical and computational advancements in positional encoding for temporal graphs were achieved, along with insights into their varying effectiveness.

Abstract: Temporal graph learning has applications in recommendation systems, traffic
forecasting, and social network analysis. Although multiple architectures have
been introduced, progress in positional encoding for temporal graphs remains
limited. Extending static Laplacian eigenvector approaches to temporal graphs
through the supra-Laplacian has shown promise, but also poses key challenges:
high eigendecomposition costs, limited theoretical understanding, and ambiguity
about when and how to apply these encodings. In this paper, we address these
issues by (1) offering a theoretical framework that connects supra-Laplacian
encodings to per-time-slice encodings, highlighting the benefits of leveraging
additional temporal connectivity, (2) introducing novel methods to reduce the
computational overhead, achieving up to 56x faster runtimes while scaling to
graphs with 50,000 active nodes, and (3) conducting an extensive experimental
study to identify which models, tasks, and datasets benefit most from these
encodings. Our findings reveal that while positional encodings can
significantly boost performance in certain scenarios, their effectiveness
varies across different models.

</details>


### [298] [Policy Newton Algorithm in Reproducing Kernel Hilbert Space](https://arxiv.org/abs/2506.01597)
*Yixian Zhang,Huaze Tang,Chao Wang,Wenbo Ding*

Main category: cs.LG

TL;DR: The paper introduces Policy Newton in RKHS, a second-order optimization framework for reinforcement learning policies in Reproducing Kernel Hilbert Spaces. It avoids direct computation of the Hessian inverse by optimizing an auxiliary function and leverages the Representer Theorem to make the problem computationally tractable. Theoretical guarantees prove local quadratic convergence, and empirical results show superior performance compared to first-order methods.


<details>
  <summary>Details</summary>
Motivation: Current RKHS-based policy optimization is limited to first-order techniques due to the difficulty of computing and inverting the infinite-dimensional Hessian operator in RKHS.

Method: The method involves introducing Policy Newton in RKHS which optimizes a cubic regularized auxiliary objective function instead of directly computing the inverse Hessian operator. By leveraging the Representer Theorem, the infinite-dimensional optimization problem is transformed into a finite-dimensional one that scales with the trajectory data volume.

Result: Empirical evaluations on a toy financial asset allocation problem confirm the theoretical properties of local quadratic convergence. Experiments on standard RL benchmarks demonstrate faster convergence speed and higher episodic rewards compared to first-order RKHS approaches and parametric second-order methods.

Conclusion: Policy Newton in RKHS bridges the gap between non-parametric policy representations and second-order optimization methods in reinforcement learning, offering powerful representational capabilities and efficient optimization.

Abstract: Reinforcement learning (RL) policies represented in Reproducing Kernel
Hilbert Spaces (RKHS) offer powerful representational capabilities. While
second-order optimization methods like Newton's method demonstrate faster
convergence than first-order approaches, current RKHS-based policy optimization
remains constrained to first-order techniques. This limitation stems primarily
from the intractability of explicitly computing and inverting the
infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in
RKHS, the first second-order optimization framework specifically designed for
RL policies represented in RKHS. Our approach circumvents direct computation of
the inverse Hessian operator by optimizing a cubic regularized auxiliary
objective function. Crucially, we leverage the Representer Theorem to transform
this infinite-dimensional optimization into an equivalent, computationally
tractable finite-dimensional problem whose dimensionality scales with the
trajectory data volume. We establish theoretical guarantees proving convergence
to a local optimum with a local quadratic convergence rate. Empirical
evaluations on a toy financial asset allocation problem validate these
theoretical properties, while experiments on standard RL benchmarks demonstrate
that Policy Newton in RKHS achieves superior convergence speed and higher
episodic rewards compared to established first-order RKHS approaches and
parametric second-order methods. Our work bridges a critical gap between
non-parametric policy representations and second-order optimization methods in
reinforcement learning.

</details>


### [299] [PMNO: A novel physics guided multi-step neural operator predictor for partial differential equations](https://arxiv.org/abs/2506.01598)
*Jin Song,Kenji Kawaguchi,Zhenya Yan*

Main category: cs.LG

TL;DR: A novel physics guided multi-step neural operator (PMNO) architecture is proposed for long-horizon prediction of complex physical systems.


<details>
  <summary>Details</summary>
Motivation: Current neural operators have limited representational capacity and rely heavily on large-scale data, which hinders effective training and results in poor extrapolation performance.

Method: The PMNO framework uses multi-step historical data as input and introduces an implicit time-stepping scheme based on the Backward Differentiation Formula (BDF) during backpropagation. A causal training strategy is also employed for efficient end-to-end optimization.

Result: PMNO demonstrates superior predictive performance across a diverse range of physical systems, including 2D linear system, modeling over irregular domain, complex-valued wave dynamics, and reaction-diffusion processes.

Conclusion: The physics guided multi-step neural operator architecture improves extrapolation capacity, facilitates more efficient and stable training with fewer data samples, and possesses resolution-invariant properties.

Abstract: Neural operators, which aim to approximate mappings between
infinite-dimensional function spaces, have been widely applied in the
simulation and prediction of physical systems. However, the limited
representational capacity of network architectures, combined with their heavy
reliance on large-scale data, often hinder effective training and result in
poor extrapolation performance. In this paper, inspired by traditional
numerical methods, we propose a novel physics guided multi-step neural operator
(PMNO) architecture to address these challenges in long-horizon prediction of
complex physical systems. Distinct from general operator learning methods, the
PMNO framework replaces the single-step input with multi-step historical data
in the forward pass and introduces an implicit time-stepping scheme based on
the Backward Differentiation Formula (BDF) during backpropagation. This design
not only strengthens the model's extrapolation capacity but also facilitates
more efficient and stable training with fewer data samples, especially for
long-term predictions. Meanwhile, a causal training strategy is employed to
circumvent the need for multi-stage training and to ensure efficient end-to-end
optimization. The neural operator architecture possesses resolution-invariant
properties, enabling the trained model to perform fast extrapolation on
arbitrary spatial resolutions. We demonstrate the superior predictive
performance of PMNO predictor across a diverse range of physical systems,
including 2D linear system, modeling over irregular domain, complex-valued wave
dynamics, and reaction-diffusion processes. Depending on the specific problem
setting, various neural operator architectures, including FNO, DeepONet, and
their variants, can be seamlessly integrated into the PMNO framework.

</details>


### [300] [Connecting Neural Models Latent Geometries with Relative Geodesic Representations](https://arxiv.org/abs/2506.01599)
*Hanlin Yu,Berfin Inal,Georgios Arvanitidis,Soren Hauberg,Francesco Locatello,Marco Fumero*

Main category: cs.LG

TL;DR: 通过利用神经模型潜在空间的微分几何结构，可以捕捉在相似数据分布上训练的表现空间之间的转换。提出了一种基于拉回度量的表现，能够捕捉潜在空间的本质结构，并且在大型模型中高效扩展。实验验证了该方法在模型缝合和检索任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 即使在相同任务和数据上训练，由于多种因素（如训练过程中的随机性、模型架构和诱导偏差等），神经模型可能会学习到不同的表示。然而，如果潜在结构在不同的潜在空间中共享，则相对距离可以在一定程度上被保留。基于此，需要一种方法来精确捕捉在相似数据分布上训练的表示空间之间的转换。

Method: 假设不同的神经模型近似参数化了相同的底层流形，引入了一种基于拉回度量的表示方法，该方法能够捕捉潜在空间的内在结构，并且在大型模型中具有高效的可扩展性。

Result: 通过实验验证了该方法在模型缝合和检索任务中的有效性，涵盖了自动编码器和视觉基础判别模型，跨越了不同的架构、数据集和预训练方案。

Conclusion: 利用神经模型潜在空间的微分几何结构，可以有效捕捉在相似数据分布上训练的表现空间之间的转换，提出的基于拉回度量的方法具有广泛的应用前景。

Abstract: Neural models learn representations of high-dimensional data on
low-dimensional manifolds. Multiple factors, including stochasticities in the
training process, model architectures, and additional inductive biases, may
induce different representations, even when learning the same task on the same
data. However, it has recently been shown that when a latent structure is
shared between distinct latent spaces, relative distances between
representations can be preserved, up to distortions. Building on this idea, we
demonstrate that exploiting the differential-geometric structure of latent
spaces of neural models, it is possible to capture precisely the
transformations between representational spaces trained on similar data
distributions. Specifically, we assume that distinct neural models parametrize
approximately the same underlying manifold, and introduce a representation
based on the pullback metric that captures the intrinsic structure of the
latent space, while scaling efficiently to large models. We validate
experimentally our method on model stitching and retrieval tasks, covering
autoencoders and vision foundation discriminative models, across diverse
architectures, datasets, and pretraining schemes.

</details>


### [301] [Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains](https://arxiv.org/abs/2506.01614)
*Hamid Attar,Luigi Lunardon,Alessio Pagani*

Main category: cs.LG

TL;DR: The paper proposes an ML-based method to enhance scalability of UTXO-based blockchains like Bitcoin by optimizing UTXO set sharding and transaction routing, using contrastive and unsupervised learning.


<details>
  <summary>Details</summary>
Motivation: UTXO set sharding in blockchain systems like Bitcoin faces challenges in distributing UTXOs effectively across validators, leading to high communication overhead due to parent-child transaction dependencies.

Method: The approach uses ML to optimize UTXO set sharding and the routing of incoming transactions. It employs a framework combining contrastive and unsupervised learning to create an embedding space for transaction outputs, grouping them based on spending relationships.

Result: This method eliminates the need for real-time parent transaction lookups, significantly reducing cross-shard communication overhead and improving throughput and scalability.

Conclusion: The ML-driven solution presents a promising way to overcome scalability issues in UTXO-based blockchains by efficiently handling transaction dependencies.

Abstract: This paper introduces a Machine Learning (ML) approach for scalability of
UTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding
struggle with distributing UTXOs effectively across validators, creating
substantial communication overhead due to child-parent transaction
dependencies. This overhead, which arises from the need to locate parent UTXOs,
significantly hampers transaction processing speeds. Our solution uses ML to
optimize not only UTXO set sharding but also the routing of incoming
transactions, ensuring that transactions are directed to shards containing
their parent UTXOs. At the heart of our approach is a framework that combines
contrastive and unsupervised learning to create an embedding space for
transaction outputs. This embedding allows the model to group transaction
outputs based on spending relationships, making it possible to route
transactions efficiently to the correct validation microservices. Trained on
historical transaction data with triplet loss and online semi-hard negative
mining, the model embeds parent-child spending patterns directly into its
parameters, thus eliminating the need for costly, real-time parent transaction
lookups. This significantly reduces cross-shard communication overhead,
boosting throughput and scalability.

</details>


### [302] [Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks](https://arxiv.org/abs/2506.01625)
*Artun Saday,Yaşar Cahit Yıldırım,Cem Tekin*

Main category: cs.LG

TL;DR: The paper tackles Gaussian Process optimization under adversarial perturbations using a robust satisficing approach, proposing two algorithms with different guarantees and demonstrating their superiority in extensive experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional robust optimization methods focus on maximizing performance under worst-case scenarios, which may not be the most practical objective. The authors aim to develop an approach that consistently achieves a predefined performance threshold even under adversarial conditions.

Method: Two novel algorithms based on distinct formulations of robust satisficing are proposed. These algorithms are instances of a general robust satisficing framework and offer different guarantees depending on the nature of the adversary. Regret bounds are derived for each algorithm.

Result: The proposed algorithms outperform established robust optimization methods in achieving the satisficing objective, especially when the ambiguity set of the robust optimization framework is inaccurately specified.

Conclusion: The robust satisficing approach provides a practical solution for Gaussian Process optimization under adversarial perturbations, with algorithms that have strong theoretical guarantees and superior empirical performance.

Abstract: We address the problem of Gaussian Process (GP) optimization in the presence
of unknown and potentially varying adversarial perturbations. Unlike
traditional robust optimization approaches that focus on maximizing performance
under worst-case scenarios, we consider a robust satisficing objective, where
the goal is to consistently achieve a predefined performance threshold $\tau$,
even under adversarial conditions. We propose two novel algorithms based on
distinct formulations of robust satisficing, and show that they are instances
of a general robust satisficing framework. Further, each algorithm offers
different guarantees depending on the nature of the adversary. Specifically, we
derive two regret bounds: one that is sublinear over time, assuming certain
conditions on the adversary and the satisficing threshold $\tau$, and another
that scales with the perturbation magnitude but requires no assumptions on the
adversary. Through extensive experiments, we demonstrate that our approach
outperforms the established robust optimization methods in achieving the
satisficing objective, particularly when the ambiguity set of the robust
optimization framework is inaccurately specified.

</details>


### [303] [Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification](https://arxiv.org/abs/2506.01631)
*Zehao Wu,Yanjie Zhao,Haoyu Wang*

Main category: cs.LG

TL;DR: An abstract about TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification.


<details>
  <summary>Details</summary>
Motivation: Unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges in Large Language Models (LLMs). There is a lack of effective mechanisms to detect model lineage and enforce licensing agreements in the LLM ecosystem.

Method: TensorGuard extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers. It operates independently of training data, watermarks, or specific model formats. The framework supports the safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features.

Result: Experimental evaluation on 58 models demonstrates 94% classification accuracy under centroid-initialized K-Means clustering.

Conclusion: TensorGuard provides two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models.

Abstract: As Large Language Models (LLMs) become integral software components in modern
applications, unauthorized model derivations through fine-tuning, merging, and
redistribution have emerged as critical software engineering challenges. Unlike
traditional software where clone detection and license compliance are
well-established, the LLM ecosystem lacks effective mechanisms to detect model
lineage and enforce licensing agreements. This gap is particularly problematic
when open-source model creators, such as Meta's LLaMA, require derivative works
to maintain naming conventions for attribution, yet no technical means exist to
verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance
tracking, we present TensorGuard, a gradient-based fingerprinting framework for
LLM similarity detection and family classification. Our approach extracts
model-intrinsic behavioral signatures by analyzing gradient responses to random
input perturbations across tensor layers, operating independently of training
data, watermarks, or specific model formats. TensorGuard supports the
widely-adopted safetensors format and constructs high-dimensional fingerprints
through statistical analysis of gradient features. These fingerprints enable
two complementary capabilities: direct pairwise similarity assessment between
arbitrary models through distance computation, and systematic family
classification of unknown models via the K-Means clustering algorithm with
domain-informed centroid initialization using known base models. Experimental
evaluation on 58 models comprising 8 base models and 50 derivatives across five
model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94%
classification accuracy under our centroid-initialized K-Means clustering.

</details>


### [304] [Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning](https://arxiv.org/abs/2506.01639)
*Yixian Zhang,Huaze Tang,Changxu Wei,Wenbo Ding*

Main category: cs.LG

TL;DR: The paper proposes Bidirectional SAC which uses both forward and reverse KL divergences for policy updates, leading to significant performance improvements over standard SAC.


<details>
  <summary>Details</summary>
Motivation: The traditional SAC algorithm's reliance on reverse KL divergence for policy updates results in an intractable optimal projection policy, causing instability and poor sample efficiency.

Method: This study explores the use of forward KL divergence within SAC. For Gaussian policies, it provides an explicit optimal projection policy corresponding to the mean and variance of the target Boltzmann distribution's action marginals. The proposed Bidirectional SAC algorithm initializes the policy using forward KL projection and refines it by optimizing reverse KL divergence.

Result: Comprehensive experiments demonstrate that Bidirectional SAC significantly outperforms standard SAC and other baselines, achieving up to a 30% increase in episodic rewards with enhanced sample efficiency.

Conclusion: Bidirectional SAC, leveraging both forward and reverse KL divergences, offers a more effective approach for policy updates in maximum entropy reinforcement learning.

Abstract: The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum
entropy reinforcement learning, traditionally relies on minimizing reverse
Kullback-Leibler (KL) divergence for policy updates. However, this approach
leads to an intractable optimal projection policy, necessitating gradient-based
approximations that can suffer from instability and poor sample efficiency.
This paper investigates the alternative use of forward KL divergence within
SAC. We demonstrate that for Gaussian policies, forward KL divergence yields an
explicit optimal projection policy -- corresponding to the mean and variance of
the target Boltzmann distribution's action marginals. Building on the distinct
advantages of both KL directions, we propose Bidirectional SAC, an algorithm
that first initializes the policy using the explicit forward KL projection and
then refines it by optimizing the reverse KL divergence. Comprehensive
experiments on continuous control benchmarks show that Bidirectional SAC
significantly outperforms standard SAC and other baselines, achieving up to a
$30\%$ increase in episodic rewards, alongside enhanced sample efficiency.

</details>


### [305] [Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning](https://arxiv.org/abs/2506.01656)
*Ryotaro Kawata,Kohsei Matsutani,Yuri Kinoshita,Naoki Nishikawa,Taiji Suzuki*

Main category: cs.LG

TL;DR: This paper explores the benefits of Mixture of Experts (MoE) in nonlinear regression tasks with underlying cluster structures, analyzing its sample and runtime complexity under stochastic gradient descent (SGD). It proves that MoE can divide complex problems into simpler subproblems, while vanilla neural networks fail to detect latent organizations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to theoretically understand why Mixture of Experts (MoE) outperforms vanilla neural networks in tasks involving an underlying cluster structure. This understanding is crucial as theoretical knowledge about MoE lags behind its practical success due to its inherent complexity.

Method: The method involves studying the sample and runtime complexity of MoE using stochastic gradient descent (SGD) for a regression task with an underlying cluster structure of single index models. The analysis compares the performance of MoE and vanilla neural networks in detecting latent organizations.

Result: The result shows that vanilla neural networks fail to detect latent organizations because they process the problem as a whole. In contrast, MoE succeeds by dividing the problem into easier subproblems, leveraging each expert's ability to weakly recover simpler functions corresponding to individual clusters.

Conclusion: This work highlights the advantages of MoE in handling complex tasks with underlying cluster structures. It provides a theoretical foundation for understanding MoE's effectiveness in such scenarios, contributing to the exploration of its SGD dynamics in nonlinear regression.

Abstract: Mixture of Experts (MoE), an ensemble of specialized models equipped with a
router that dynamically distributes each input to appropriate experts, has
achieved successful results in the field of machine learning. However,
theoretical understanding of this architecture is falling behind due to its
inherent complexity. In this paper, we theoretically study the sample and
runtime complexity of MoE following the stochastic gradient descent (SGD) when
learning a regression task with an underlying cluster structure of single index
models. On the one hand, we prove that a vanilla neural network fails in
detecting such a latent organization as it can only process the problem as a
whole. This is intrinsically related to the concept of information exponent
which is low for each cluster, but increases when we consider the entire task.
On the other hand, we show that a MoE succeeds in dividing this problem into
easier subproblems by leveraging the ability of each expert to weakly recover
the simpler function corresponding to an individual cluster. To the best of our
knowledge, this work is among the first to explore the benefits of the MoE
framework by examining its SGD dynamics in the context of nonlinear regression.

</details>


### [306] [Provably Safe Reinforcement Learning from Analytic Gradients](https://arxiv.org/abs/2506.01665)
*Tim Walter,Hannah Markgraf,Jonathan Külz,Matthias Althoff*

Main category: cs.LG

TL;DR: 本论文提出了一种针对基于解析梯度的强化学习的有效保护机制，通过调整现有的可微分保护方法并将其与先进的学习算法和可微分模拟结合，实现了在不影响性能的情况下进行安全训练。


<details>
  <summary>Details</summary>
Motivation: 目前，尽管存在许多针对采样型强化学习的安全保护方法，但基于解析梯度的强化学习尚无有效的保护机制。这种学习范式通常具有更优的性能和样本效率，因此需要开发一种适用于它的安全保护方法以缩小模拟到实际应用之间的差距。

Method: 作者分析了现有的可微分保护方法，并通过修改映射和梯度公式对其进行适应性调整。然后将这些改进后的保护方法与最先进的学习算法以及可微分模拟相结合。最后，在两个经典的控制任务上使用数值实验评估不同保护方法对策略优化的影响。

Result: 实验结果表明，所提出的保护机制能够在不降低性能的前提下实现安全训练。

Conclusion: 本研究成功开发了首个针对基于解析梯度的强化学习的有效保护机制，为未来在关键安全领域中部署自主机器人提供了新的可能性。

Abstract: Deploying autonomous robots in safety-critical applications requires safety
guarantees. Provably safe reinforcement learning is an active field of research
which aims to provide such guarantees using safeguards. These safeguards should
be integrated during training to prevent a large sim-to-real gap. While there
are several approaches for safeguarding sampling-based reinforcement learning,
analytic gradient-based reinforcement learning often achieves superior
performance and sample efficiency. However, there is no safeguarding approach
for this learning paradigm yet. Our work addresses this gap by developing the
first effective safeguard for analytic gradient-based reinforcement learning.
We analyse existing, differentiable safeguards, adapt them through modified
mappings and gradient formulations, and integrate them with a state-of-the-art
learning algorithm and a differentiable simulation. We evaluate how different
safeguards affect policy optimisation using numerical experiments on two
classical control tasks. The results demonstrate safeguarded training without
compromising performance.

</details>


### [307] [Minimal Impact ControlNet: Advancing Multi-ControlNet Integration](https://arxiv.org/abs/2506.01672)
*Shikun Sun,Min Zhou,Zixuan Wang,Xubin Li,Tiezheng Ge,Zijie Ye,Xiaoyu Qin,Junliang Xing,Bo Zheng,Jia Jia*

Main category: cs.LG

TL;DR: 提出了一种名为Minimal Impact ControlNet的方法，以解决在结合多个ControlNets时由于静默控制信号导致的图像生成问题。通过三种策略：构建平衡数据集、平衡注入特征信号以及处理ControlNet引起的得分函数雅可比矩阵的不对称性，提高了控制信号的兼容性，从而实现了更自由和谐的图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前ControlNet训练中，每个控制信号会影响图像的所有区域，在实际应用中，当不同控制信号需要管理图像的不同部分时会产生冲突。特别是边缘类型的控制条件下，缺乏边界信息的区域通常代表低频信号（即静默控制信号），这些信号在结合多个ControlNets时会抑制相关区域的纹理生成，导致次优结果。

Method: 提出了Minimal Impact ControlNet方法，采用三个关键策略来缓解冲突：1）构建平衡数据集；2）以平衡方式组合和注入特征信号；3）处理由ControlNet引起的得分函数雅可比矩阵的不对称性。

Result: 改进后的ControlNet增强了控制信号之间的兼容性，使得在具有静默控制信号的区域能够实现更自由和谐的图像生成。

Conclusion: Minimal Impact ControlNet通过三种策略解决了多ControlNet结合时静默控制信号的问题，提高了图像生成的质量和可控性。

Abstract: With the advancement of diffusion models, there is a growing demand for
high-quality, controllable image generation, particularly through methods that
utilize one or multiple control signals based on ControlNet. However, in
current ControlNet training, each control is designed to influence all areas of
an image, which can lead to conflicts when different control signals are
expected to manage different parts of the image in practical applications. This
issue is especially pronounced with edge-type control conditions, where regions
lacking boundary information often represent low-frequency signals, referred to
as silent control signals. When combining multiple ControlNets, these silent
control signals can suppress the generation of textures in related areas,
resulting in suboptimal outcomes. To address this problem, we propose Minimal
Impact ControlNet. Our approach mitigates conflicts through three key
strategies: constructing a balanced dataset, combining and injecting feature
signals in a balanced manner, and addressing the asymmetry in the score
function's Jacobian matrix induced by ControlNet. These improvements enhance
the compatibility of control signals, allowing for freer and more harmonious
generation in areas with silent control signals.

</details>


### [308] [When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses](https://arxiv.org/abs/2506.01722)
*Antoine Moulin,Emmanuel Esposito,Dirk van der Hoeven*

Main category: cs.LG

TL;DR: 这篇论文研究了带有可能重尾损失的专家预测问题，提出了一种自适应算法，在无需预先了解损失范围或二阶矩的情况下，改进了后悔界。


<details>
  <summary>Details</summary>
Motivation: 现有自适应算法在后悔保证中包含一个通常被认为是低阶项的项，但在这种设置下，这个低阶项可能会主导后悔界。因此需要开发新的算法来避免这种依赖。

Method: 提出了新的自适应算法，这些算法不需要任何关于损失范围或二阶矩的先验知识，并且具有改进的后悔界。当损失函数是平方损失时，该算法还保证了比先前结果更好的后悔界。

Result: 新算法在最坏情况下保证了O(√(θTlog(K)))的后悔界，当损失独立同分布于某个固定分布时，保证了O(θlog(KT)/Δ_min)的后悔界，其中Δ_min是第二佳专家和最佳专家平均损失的差值。

Conclusion: 所提出的自适应算法改进了后悔界，避免了对低阶项的依赖，适用于不同损失情况，包括平方损失。

Abstract: We consider the problem setting of prediction with expert advice with
possibly heavy-tailed losses, i.e.\ the only assumption on the losses is an
upper bound on their second moments, denoted by $\theta$. We develop adaptive
algorithms that do not require any prior knowledge about the range or the
second moment of the losses. Existing adaptive algorithms have what is
typically considered a lower-order term in their regret guarantees. We show
that this lower-order term, which is often the maximum of the losses, can
actually dominate the regret bound in our setting. Specifically, we show that
even with small constant $\theta$, this lower-order term can scale as
$\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We
propose adaptive algorithms with improved regret bounds that avoid the
dependence on such a lower-order term and guarantee $\mathcal{O}(\sqrt{\theta
T\log(K)})$ regret in the worst case, and $\mathcal{O}(\theta
\log(KT)/\Delta_{\min})$ regret when the losses are sampled i.i.d.\ from some
fixed distribution, where $\Delta_{\min}$ is the difference between the mean
losses of the second best expert and the best expert. Additionally, when the
loss function is the squared loss, our algorithm also guarantees improved
regret bounds over prior results.

</details>


### [309] [Principled data augmentation for learning to solve quadratic programming problems](https://arxiv.org/abs/2506.01728)
*Chendi Qian,Christopher Morris*

Main category: cs.LG

TL;DR: The paper proposes a data augmentation method for quadratic programs (QPs) using message-passing graph neural networks (MPNNs), enhancing the performance of learning-to-optimize (L2O) methods through self-supervised contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Learning-to-optimize (L2O) methods using MPNNs have shown promise in solving linear and quadratic optimization problems, but their robustness is limited in data-scarce settings, especially for complex problems like QPs.

Method: The authors introduce a principled data augmentation approach tailored for QPs via MPNNs. They leverage theoretically justified techniques to generate diverse yet optimality-preserving instances and integrate these augmentations into a self-supervised learning framework based on contrastive learning.

Result: Extensive experiments show that the proposed approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems.

Conclusion: The principled data augmentation method enhances the performance of L2O tasks for QPs, particularly in data-scarce settings.

Abstract: Linear and quadratic optimization are crucial in numerous real-world
applications, from training machine learning models to integer-linear
optimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or
quadratic programs (QPs) using message-passing graph neural networks (MPNNs)
have gained traction, promising lightweight, data-driven proxies for solving
such optimization problems. For example, they replace the costly computation of
strong branching scores in branch-and-bound solvers, requiring solving many
such optimization problems. However, robust L2O MPNNs remain challenging in
data-scarce settings, especially when addressing complex optimization problems
such as QPs. This work introduces a principled approach to data augmentation
tailored for QPs via MPNNs. Our method leverages theoretically justified data
augmentation techniques to generate diverse yet optimality-preserving
instances. Furthermore, we integrate these augmentations into a self-supervised
learning framework based on contrastive learning, thereby pretraining MPNNs for
enhanced performance on L2O tasks. Extensive experiments demonstrate that our
approach improves generalization in supervised scenarios and facilitates
effective transfer learning to related optimization problems.

</details>


### [310] [Automated Manifold Learning for Reduced Order Modeling](https://arxiv.org/abs/2506.01741)
*Imran Nasim,Melanie Weber*

Main category: cs.LG

TL;DR: The paper explores the use of Geometric Representation Learning for discovering system dynamics from spatial-temporal data, proposes a framework for Automated Manifold Learning to improve performance and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying geometric structure in data and improving the process of learning reduced order dynamics from spatial-temporal data.

Method: Encode similarity structure in spatial-temporal data using a proximity graph and apply manifold learning approaches. Propose a framework for Automated Manifold Learning that selects an approach and hyperparameters based on subsamples of the input graph.

Result: The proposed framework leads to performance gains in scalability and accuracy in capturing local and global geometric features of the underlying system dynamics.

Conclusion: Automated Manifold Learning can enhance the efficiency and effectiveness of discovering system dynamics from spatial-temporal data.

Abstract: The problem of identifying geometric structure in data is a cornerstone of
(unsupervised) learning. As a result, Geometric Representation Learning has
been widely applied across scientific and engineering domains. In this work, we
investigate the use of Geometric Representation Learning for the data-driven
discovery of system dynamics from spatial-temporal data. We propose to encode
similarity structure in such data in a spatial-temporal proximity graph, to
which we apply a range of classical and deep learning-based manifold learning
approaches to learn reduced order dynamics. We observe that while manifold
learning is generally capable of recovering reduced order dynamics, the quality
of the learned representations varies substantially across different algorithms
and hyperparameter choices. This is indicative of high sensitivity to the
inherent geometric assumptions of the respective approaches and suggests a need
for careful hyperparameter tuning, which can be expensive in practise. To
overcome these challenges, we propose a framework for Automated Manifold
Learning, which selects a manifold learning approach and corresponding
hyperparameter choices based on representative subsamples of the input graph.
We demonstrate that the proposed framework leads to performance gains both in
scalability and in the learned representations' accuracy in capturing local and
global geometric features of the underlying system dynamics.

</details>


### [311] [DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems](https://arxiv.org/abs/2506.01777)
*Hithem Lamri,Manaar Alam,Haiyan Jiang,Michail Maniatakos*

Main category: cs.LG

TL;DR: The paper introduces DRAUN, the first attack framework to reconstruct unlearned data in Federated Unlearning (FU) systems, which targets optimization-based unlearning methods.


<details>
  <summary>Details</summary>
Motivation: To address the unclear applicability of Data Reconstruction Attacks (DRAs) to Federated Unlearning (FU), and to demonstrate the vulnerability of state-of-the-art FU methods to DRAs.

Method: Theoretical demonstration of why existing DRAs fail in FU and how DRAUN overcomes these limitations. Validation through extensive experiments on multiple datasets and model architectures.

Result: DRAUN effectively reconstructs unlearned data in FU systems, showing that current FU methods remain vulnerable to DRAs.

Conclusion: Federated Unlearning systems are vulnerable to Data Reconstruction Attacks via the newly introduced DRAUN framework.

Abstract: Federated Unlearning (FU) enables clients to remove the influence of specific
data from a collaboratively trained shared global model, addressing regulatory
requirements such as GDPR and CCPA. However, this unlearning process introduces
a new privacy risk: A malicious server may exploit unlearning updates to
reconstruct the data requested for removal, a form of Data Reconstruction
Attack (DRA). While DRAs for machine unlearning have been studied extensively
in centralized Machine Learning-as-a-Service (MLaaS) settings, their
applicability to FU remains unclear due to the decentralized, client-driven
nature of FU. This work presents DRAUN, the first attack framework to
reconstruct unlearned data in FU systems. DRAUN targets optimization-based
unlearning methods, which are widely adopted for their efficiency. We
theoretically demonstrate why existing DRAs targeting machine unlearning in
MLaaS fail in FU and show how DRAUN overcomes these limitations. We validate
our approach through extensive experiments on four datasets and four model
architectures, evaluating its performance against five popular unlearning
methods, effectively demonstrating that state-of-the-art FU methods remain
vulnerable to DRAs.

</details>


### [312] [Federated Gaussian Mixture Models](https://arxiv.org/abs/2506.01780)
*Sophia Zhang Pettersson,Kuo-Yun Liang,Juan Carlos Andresen*

Main category: cs.LG

TL;DR: This paper introduces FedGenGMM, a one-shot federated learning approach for Gaussian Mixture Models tailored for unsupervised learning scenarios. It enables aggregation of local GMM models through a single communication round, creates a synthetic dataset on the server side, and demonstrates performance comparable to non-federated and iterative federated methods while significantly reducing communication overhead.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in federated learning such as statistical heterogeneity, high communication costs, and privacy concerns, especially in unsupervised learning scenarios.

Method: FedGenGMM allows local GMM models to be trained independently on client devices and aggregated through a single communication round. It leverages the generative property of GMMs to create a synthetic dataset on the server side for training a global model efficiently.

Result: FedGenGMM consistently achieves performance comparable to non-federated and iterative federated methods across diverse datasets covering image, tabular, and time series data. It significantly reduces communication overhead, maintains robust performance in anomaly detection tasks, and offers flexibility in local model complexities.

Conclusion: FedGenGMM is a novel one-shot federated learning approach for Gaussian Mixture Models that effectively addresses the challenges in federated learning, making it particularly suitable for edge computing environments.

Abstract: This paper introduces FedGenGMM, a novel one-shot federated learning approach
for Gaussian Mixture Models (GMM) tailored for unsupervised learning scenarios.
In federated learning (FL), where multiple decentralized clients
collaboratively train models without sharing raw data, significant challenges
include statistical heterogeneity, high communication costs, and privacy
concerns. FedGenGMM addresses these issues by allowing local GMM models,
trained independently on client devices, to be aggregated through a single
communication round. This approach leverages the generative property of GMMs,
enabling the creation of a synthetic dataset on the server side to train a
global model efficiently. Evaluation across diverse datasets covering image,
tabular, and time series data demonstrates that FedGenGMM consistently achieves
performance comparable to non-federated and iterative federated methods, even
under significant data heterogeneity. Additionally, FedGenGMM significantly
reduces communication overhead, maintains robust performance in anomaly
detection tasks, and offers flexibility in local model complexities, making it
particularly suitable for edge computing environments.

</details>


### [313] [Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning](https://arxiv.org/abs/2506.01781)
*Subhadip Nandi,Neeraj Agrawal,Anshika Singh,Priyanka Bhatt*

Main category: cs.LG

TL;DR: The paper presents MTL-CNLU-SAWC, a context-aware NLU model with selective attention and multi-task learning for customer intent prediction in chatbots. It uses both customer queries and contextual information to improve accuracy, reduce escalations to human agents, and save costs.


<details>
  <summary>Details</summary>
Motivation: Current intent classification models for customer care only use customer queries which may result in low-accuracy models that cannot handle ambiguous queries effectively.

Method: A novel context-aware NLU model named MTL-CNLU-SAWC is introduced. This model incorporates customer queries and contextual information from order status using a selective attention module and a multi-task learning paradigm.

Result: MTL-CNLU-SAWC shows a 4.8% increase in top 2 accuracy over the baseline model and a 3.5% improvement over existing state-of-the-art models. Deployment in Walmart's customer care led to significant cost savings.

Conclusion: By accurately predicting customer intents, MTL-CNLU-SAWC can efficiently direct customers to automated workflows, reducing the need for human intervention and saving operational costs.

Abstract: Customer service chatbots are conversational systems aimed at addressing
customer queries, often by directing them to automated workflows. A crucial
aspect of this process is the classification of the customer's intent.
Presently, most intent classification models for customer care utilise only
customer query for intent prediction. This may result in low-accuracy models,
which cannot handle ambiguous queries. An ambiguous query like "I didn't
receive my package" could indicate a delayed order, or an order that was
delivered but the customer failed to receive it. Resolution of each of these
scenarios requires the execution of very different sequence of steps. Utilizing
additional information, such as the customer's order delivery status, in the
right manner can help identify the intent for such ambiguous queries. In this
paper, we have introduced a context-aware NLU model that incorporates both, the
customer query and contextual information from the customer's order status for
predicting customer intent. A novel selective attention module is used to
extract relevant context features. We have also proposed a multi-task learning
paradigm for the effective utilization of different label types available in
our training data. Our suggested method, Multi-Task Learning Contextual NLU
with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8%
increase in top 2 accuracy score over the baseline model which only uses user
queries, and a 3.5% improvement over existing state-of-the-art models that
combine query and context. We have deployed our model to production for
Walmart's customer care domain. Accurate intent prediction through
MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby
significantly reducing escalations to human agents, leading to almost a million
dollars in yearly savings for the company.

</details>


### [314] [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/abs/2506.01789)
*Genta Indra Winata,David Anugraha,Emmy Liu,Alham Fikri Aji,Shou-Yi Hung,Aditya Parashar,Patrick Amadeus Irawan,Ruochen Zhang,Zheng-Xin Yong,Jan Christian Blaise Cruz,Niklas Muennighoff,Seungone Kim,Hanyang Zhao,Sudipta Kar,Kezia Erina Suryoraharjo,M. Farid Adilazuarda,En-Shiun Annie Lee,Ayu Purwarianti,Derry Tanti Wijaya,Monojit Choudhury*

Main category: cs.LG

TL;DR: High-quality datasets are crucial for machine learning models, but creating them with accurate annotations is challenging. Many dataset submissions lack originality, diversity, or quality control, and these issues are often overlooked during peer review. Existing tools like datasheets promote transparency but don't provide standardized evaluation methods. This position paper suggests integrating systematic, rubric-based metrics into the dataset review process and explores scalable synthetic data generation methods. It introduces DataRubrics, a framework for assessing dataset quality using LLM-based evaluations, and releases related code.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges in creating high-quality datasets for machine learning models, including the lack of originality, diversity, and rigorous quality control in many dataset submissions, as well as the shortcomings in existing evaluation tools and metadata requirements.

Method: The method involves advocating for the integration of systematic, rubric-based evaluation metrics into the dataset review process. The paper also explores scalable, cost-effective methods for synthetic data generation, such as dedicated tools and LLM-as-a-judge approaches. Additionally, it introduces DataRubrics, a structured framework for assessing dataset quality.

Result: The result is the introduction of DataRubrics, which offers a reproducible, scalable, and actionable solution for dataset quality assessment. It enables both authors and reviewers to uphold higher standards in data-centric research.

Conclusion: In conclusion, the paper emphasizes the importance of high-quality datasets in machine learning research and proposes a novel framework, DataRubrics, to assess dataset quality effectively. By leveraging LLM-based evaluations, it aims to improve the standards in data-centric research.

Abstract: High-quality datasets are fundamental to training and evaluating machine
learning models, yet their creation-especially with accurate human
annotations-remains a significant challenge. Many dataset paper submissions
lack originality, diversity, or rigorous quality control, and these
shortcomings are often overlooked during peer review. Submissions also
frequently omit essential details about dataset construction and properties.
While existing tools such as datasheets aim to promote transparency, they are
largely descriptive and do not provide standardized, measurable methods for
evaluating data quality. Similarly, metadata requirements at conferences
promote accountability but are inconsistently enforced. To address these
limitations, this position paper advocates for the integration of systematic,
rubric-based evaluation metrics into the dataset review process-particularly as
submission volumes continue to grow. We also explore scalable, cost-effective
methods for synthetic data generation, including dedicated tools and
LLM-as-a-judge approaches, to support more efficient evaluation. As a call to
action, we introduce DataRubrics, a structured framework for assessing the
quality of both human- and model-generated datasets. Leveraging recent advances
in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and
actionable solution for dataset quality assessment, enabling both authors and
reviewers to uphold higher standards in data-centric research. We also release
code to support reproducibility of LLM-based evaluations at
https://github.com/datarubrics/datarubrics.

</details>


### [315] [Path Signatures for Feature Extraction. An Introduction to the Mathematics Underpinning an Efficient Machine Learning Technique](https://arxiv.org/abs/2506.01815)
*Stephan Sturm*

Main category: cs.LG

TL;DR: This paper introduces path signatures for feature extraction in machine learning from data streams, focusing on the underlying mathematical theory without delving into technical proofs.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible introduction to the concept of path signatures and their application in feature extraction for machine learning with data streams.

Method: Explaining the mathematical theory behind path signatures as a methodology for feature extraction.

Result: Offers an overview that helps students understand the conceptual aspects of path signatures without needing to explore rigorous proofs.

Conclusion: Path signatures serve as a valuable tool for feature extraction in machine learning applications involving data streams.

Abstract: We provide an introduction to the topic of path signatures as means of
feature extraction for machine learning from data streams. The article stresses
the mathematical theory underlying the signature methodology, highlighting the
conceptual character without plunging into the technical details of rigorous
proofs. These notes are based on an introductory presentation given to students
of the Research Experience for Undergraduates in Industrial Mathematics and
Statistics at Worcester Polytechnic Institute in June 2024.

</details>


### [316] [Efficient Learning of Balanced Signed Graphs via Sparse Linear Programming](https://arxiv.org/abs/2506.01826)
*Haruki Yokota,Hiroshi Higashi,Yuichi Tanaka,Gene Cheung*

Main category: cs.LG

TL;DR: Signed graphs with both positive and negative edge weights are studied. A method to learn a balanced signed graph Laplacian directly from data is proposed, which outperforms competing methods and enables reuse of spectral filters, wavelets, and GCNs.


<details>
  <summary>Details</summary>
Motivation: To enable the reuse of spectral filtering tools designed for positive graphs by learning a balanced signed graph Laplacian directly from data.

Method: The authors extend the CLIME method by formulating a new linear programming (LP) problem for each Laplacian column, where linear constraints restrict weight signs of edges stemming from each node. They derive a suitable CLIME parameter based on the Hannan-Quinn information criterion and a minimum feasibility criterion, and solve the LP problem efficiently using a sparse LP method based on ADMM. Theoretical proof of local solution convergence is also provided.

Result: The proposed method outperforms competing methods in extensive experiments on synthetic and real-world datasets. It enables the reuse of spectral filters, wavelets, and graph convolutional nets constructed for positive graphs.

Conclusion: The authors successfully propose an efficient method to learn a balanced signed graph Laplacian directly from data, which can be used to reuse existing tools designed for positive graphs.

Abstract: Signed graphs are equipped with both positive and negative edge weights,
encoding pairwise correlations as well as anti-correlations in data. A balanced
signed graph is a signed graph with no cycles containing an odd number of
negative edges. Laplacian of a balanced signed graph has eigenvectors that map
via a simple linear transform to ones in a corresponding positive graph
Laplacian, thus enabling reuse of spectral filtering tools designed for
positive graphs. We propose an efficient method to learn a balanced signed
graph Laplacian directly from data. Specifically, extending a previous linear
programming (LP) based sparse inverse covariance estimation method called
CLIME, we formulate a new LP problem for each Laplacian column $i$, where the
linear constraints restrict weight signs of edges stemming from node $i$, so
that nodes of same / different polarities are connected by positive / negative
edges. Towards optimal model selection, we derive a suitable CLIME parameter
$\rho$ based on a combination of the Hannan-Quinn information criterion and a
minimum feasibility criterion. We solve the LP problem efficiently by tailoring
a sparse LP method based on ADMM. We theoretically prove local solution
convergence of our proposed iterative algorithm. Extensive experimental results
on synthetic and real-world datasets show that our balanced graph learning
method outperforms competing methods and enables reuse of spectral filters,
wavelets, and graph convolutional nets (GCN) constructed for positive graphs.

</details>


### [317] [Memory Access Characterization of Large Language Models in CPU Environment and its Potential Impacts](https://arxiv.org/abs/2506.01827)
*Spencer Banasik*

Main category: cs.LG

TL;DR: 通过修改缓存架构，优化仅CPU环境下的LLM推理速度。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习算法的价值日益显现，对它们的访问需求也相应增长。然而，在没有加速器的情况下运行大型模型通常不可行，特别是在受能源消耗、安全或成本限制的环境中。为了提高这些模型的可用性，需要在仅CPU环境下提升LLM推理速度。

Method: 通过使用Llama.cpp和QWEN模型进行两项实验：测试不同缓存配置并评估其性能；输出内存占用痕迹。以此研究内存访问模式和性能特征，识别潜在优化点。

Result: 尚未明确给出具体结果，但表明将通过实验确定可能的改进措施。

Conclusion: 希望通过调整缓存架构来优化LLM在仅CPU环境下的推理速度，从而提高大模型在受限环境中的可用性。

Abstract: As machine learning algorithms are shown to be an increasingly valuable tool,
the demand for their access has grown accordingly. Oftentimes, it is infeasible
to run inference with larger models without an accelerator, which may be
unavailable in environments that have constraints such as energy consumption,
security, or cost. To increase the availability of these models, we aim to
improve the LLM inference speed on a CPU-only environment by modifying the
cache architecture. To determine what improvements could be made, we conducted
two experiments using Llama.cpp and the QWEN model: running various cache
configurations and evaluating their performance, and outputting a trace of the
memory footprint. Using these experiments, we investigate the memory access
patterns and performance characteristics to identify potential optimizations.

</details>


### [318] [SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model](https://arxiv.org/abs/2506.01833)
*Zhao Yang,Jiwei Zhu,Bing Su*

Main category: cs.LG

TL;DR: The paper proposes SPACE, a Species-Profile Adaptive Collaborative Experts model using supervised training for genomic profile prediction to learn DNA representations across species and profiles, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised DNA pre-training methods yield suboptimal results due to insufficient information in pure DNA sequences; their functions are regulated by genomic profiles like chromatin accessibility.

Method: SPACE leverages Mixture of Experts (MoE) for supervised training on genomic profile prediction, capturing relationships between DNA sequences across different species and profiles.

Result: Through extensive experiments, the model achieves state-of-the-art performance in various tasks related to DNA representation learning.

Conclusion: DNA models trained with supervised genomic profiles serve as powerful DNA representation learners.

Abstract: Inspired by the success of unsupervised pre-training paradigms, researchers
have applied these approaches to DNA pre-training. However, we argue that these
approaches alone yield suboptimal results because pure DNA sequences lack
sufficient information, since their functions are regulated by genomic profiles
like chromatin accessibility. Here, we demonstrate that supervised training for
genomic profile prediction serves as a more effective alternative to pure
sequence pre-training. Furthermore, considering the multi-species and
multi-profile nature of genomic profile prediction, we introduce our
$\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive
$\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of
Experts (MoE) to better capture the relationships between DNA sequences across
different species and genomic profiles, thereby learning more effective DNA
representations. Through extensive experiments across various tasks, our model
achieves state-of-the-art performance, establishing that DNA models trained
with supervised genomic profiles serve as powerful DNA representation learners.
The code is available at https://github.com/ZhuJiwei111/SPACE.

</details>


### [319] [SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics](https://arxiv.org/abs/2506.01844)
*Mustafa Shukor,Dana Aubakirova,Francesco Capuano,Pepijn Kooijmans,Steven Palma,Adil Zouitine,Michel Aractingi,Caroline Pascal,Martino Russi,Andres Marafioti,Simon Alibert,Matthieu Cord,Thomas Wolf,Remi Cadene*

Main category: cs.LG

TL;DR: SmolVLA，一种小巧、高效的视觉-语言-动作模型，显著降低了训练和推理成本，同时保持了与大10倍的模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作(VLA)模型通常规模庞大，参数可达数十亿，导致高昂的训练成本和有限的实际部署能力。此外，它们依赖学术和工业数据集，忽视了来自经济型机器人平台的社区收集数据的增长潜力。

Method: 提出了一种名为SmolVLA的小型VLA模型，该模型可以在单个GPU上进行训练，并能够部署在消费级GPU甚至CPU上。为了进一步提高响应速度，引入了异步推理堆栈，将感知和动作预测与动作执行解耦，允许通过分块动作生成实现更高的控制速率。

Result: 尽管尺寸紧凑，SmolVLA在一系列模拟和真实世界的机器人基准测试中表现出色，性能可与大10倍的VLA模型相媲美。

Conclusion: 作者评估了SmolVLA在多种模拟和现实世界机器人基准上的表现，并开源了所有代码、预训练模型和训练数据，推动了社区驱动的机器人技术发展。

Abstract: Vision-language models (VLMs) pretrained on large-scale multimodal datasets
encode rich visual and linguistic knowledge, making them a strong foundation
for robotics. Rather than training robotic policies from scratch, recent
approaches adapt VLMs into vision-language-action (VLA) models that enable
natural language-driven perception and control. However, existing VLAs are
typically massive--often with billions of parameters--leading to high training
costs and limited real-world deployability. Moreover, they rely on academic and
industrial datasets, overlooking the growing availability of
community-collected data from affordable robotic platforms. In this work, we
present SmolVLA, a small, efficient, and community-driven VLA that drastically
reduces both training and inference costs, while retaining competitive
performance. SmolVLA is designed to be trained on a single GPU and deployed on
consumer-grade GPUs or even CPUs. To further improve responsiveness, we
introduce an asynchronous inference stack decoupling perception and action
prediction from action execution, allowing higher control rates with chunked
action generation. Despite its compact size, SmolVLA achieves performance
comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both
simulated as well as real-world robotic benchmarks and release all code,
pretrained models, and training data.

</details>


### [320] [Trade-offs in Data Memorization via Strong Data Processing Inequalities](https://arxiv.org/abs/2506.01855)
*Vitaly Feldman,Guy Kornowski,Xin Lyu*

Main category: cs.LG

TL;DR: Recent research shows training large language models involves memorization of training data, which can lead to privacy violations. This paper develops a general approach for proving lower bounds on excess data memorization, demonstrating a trade-off between samples and memorized information.


<details>
  <summary>Details</summary>
Motivation: Training large language models involves memorization of a significant fraction of training data, which can lead to privacy violations when training on sensitive user data.

Method: The paper develops a general approach for proving lower bounds on excess data memorization, relying on a new connection between strong data processing inequalities and data memorization. It demonstrates this trade-off in several simple and natural binary classification problems.

Result: Ω(d) bits of information about the training data need to be memorized when O(1) d-dimensional examples are available, decaying as the number of examples grows at a problem-specific rate. The lower bounds are generally matched by simple learning algorithms.

Conclusion: The definitions and results build on the work of Brown et al. (2021), addressing several limitations of the lower bounds in their work.

Abstract: Recent research demonstrated that training large language models involves
memorization of a significant fraction of training data. Such memorization can
lead to privacy violations when training on sensitive user data and thus
motivates the study of data memorization's role in learning. In this work, we
develop a general approach for proving lower bounds on excess data
memorization, that relies on a new connection between strong data processing
inequalities and data memorization. We then demonstrate that several simple and
natural binary classification problems exhibit a trade-off between the number
of samples available to a learning algorithm, and the amount of information
about the training data that a learning algorithm needs to memorize to be
accurate. In particular, $\Omega(d)$ bits of information about the training
data need to be memorized when $O(1)$ $d$-dimensional examples are available,
which then decays as the number of examples grows at a problem-specific rate.
Further, our lower bounds are generally matched (up to logarithmic factors) by
simple learning algorithms. We also extend our lower bounds to more general
mixture-of-clusters models. Our definitions and results build on the work of
Brown et al. (2021) and address several limitations of the lower bounds in
their work.

</details>


### [321] [Unified Scaling Laws for Compressed Representations](https://arxiv.org/abs/2506.01863)
*Andrei Panferov,Alexandra Volkova,Ionut-Vlad Modoranu,Vage Egiazarian,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: This paper explores the relationship between scaling laws and model compression techniques, such as quantization and sparsification. It validates a general scaling law formulation that applies across different compression types and proposes a 'capacity' metric to predict parameter efficiency in various compressed representations.


<details>
  <summary>Details</summary>
Motivation: The increasing computational cost of AI has driven the development of model compression techniques like quantization and sparsification. The study aims to understand how scaling laws interact with these compression formats and whether a unified framework can predict model performance under different compressed representations.

Method: The researchers validate a general scaling law formulation applicable individually and composably across various compression types. They introduce a 'capacity' metric based on the representation's ability to fit random Gaussian data to predict parameter efficiency. This metric is tested both theoretically and empirically.

Result: The 'capacity' metric robustly predicts parameter efficiency across multiple compressed representations. The formulation allows for direct comparison of accuracy potential among different compressed formats and leads to the development of improved algorithms for training sparse-quantized formats.

Conclusion: A unified scaling framework can effectively predict model performance when using different compressed representations. The proposed 'capacity' metric provides a simple yet powerful tool for understanding parameter efficiency in compressed models.

Abstract: Scaling laws have shaped recent advances in machine learning by enabling
predictable scaling of model performance based on model size, computation, and
data volume. Concurrently, the rise in computational cost for AI has motivated
model compression techniques, notably quantization and sparsification, which
have emerged to mitigate the steep computational demands associated with
large-scale training and inference. This paper investigates the interplay
between scaling laws and compression formats, exploring whether a unified
scaling framework can accurately predict model performance when training occurs
over various compressed representations, such as sparse, scalar-quantized,
sparse-quantized or even vector-quantized formats. Our key contributions
include validating a general scaling law formulation and showing that it is
applicable both individually but also composably across compression types.
Based on this, our main finding is demonstrating both theoretically and
empirically that there exists a simple "capacity" metric -- based on the
representation's ability to fit random Gaussian data -- which can robustly
predict parameter efficiency across multiple compressed representations. On the
practical side, we extend our formulation to directly compare the accuracy
potential of different compressed formats, and to derive better algorithms for
training over sparse-quantized formats.

</details>


### [322] [NepTrain and NepTrainKit: Automated Active Learning and Visualization Toolkit for Neuroevolution Potentials](https://arxiv.org/abs/2506.01868)
*Chengbing Chen,Yutong Li,Rui Zhao,Zhoulin Liu,Zheyong Fan,Gang Tang,Zhiyong Wang*

Main category: cs.LG

TL;DR: The paper introduces NepTrain and NepTrainKit, tools for creating high-quality training datasets for NEP models in materials science, demonstrating their utility with a case study on CsPbI₃.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of preparing and screening time-consuming, labor-intensive, and resource-intensive NEP training datasets.

Method: Developed NepTrain (a Python package) and NepTrainKit (a GUI software) to initialize, manage, and automate the creation of high-quality NEP training datasets. Includes features like bond length filtering, outlier identification, farthest-point sampling, non-physical structure detection, and configuration type selection.

Result: Demonstrated the complete workflow for training NEP models using CsPbI₃ as a case study, validating the models through material property predictions.

Conclusion: These tools will significantly aid researchers working with machine learning interatomic potentials.

Abstract: As a machine-learned potential, the neuroevolution potential (NEP) method
features exceptional computational efficiency and has been successfully applied
in materials science. Constructing high-quality training datasets is crucial
for developing accurate NEP models. However, the preparation and screening of
NEP training datasets remain a bottleneck for broader applications due to their
time-consuming, labor-intensive, and resource-intensive nature. In this work,
we have developed NepTrain and NepTrainKit, which are dedicated to initializing
and managing training datasets to generate high-quality training sets while
automating NEP model training. NepTrain is an open-source Python package that
features a bond length filtering method to effectively identify and remove
non-physical structures from molecular dynamics trajectories, thereby ensuring
high-quality training datasets. NepTrainKit is a graphical user interface (GUI)
software designed specifically for NEP training datasets, providing
functionalities for data editing, visualization, and interactive exploration.
It integrates key features such as outlier identification, farthest-point
sampling, non-physical structure detection, and configuration type selection.
The combination of these tools enables users to process datasets more
efficiently and conveniently. Using $\rm CsPbI_3$ as a case study, we
demonstrate the complete workflow for training NEP models with NepTrain and
further validate the models through materials property predictions. We believe
this toolkit will greatly benefit researchers working with machine learning
interatomic potentials.

</details>


### [323] [Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence](https://arxiv.org/abs/2506.01869)
*John Violos,Konstantina-Christina Diamanti,Ioannis Kompatsiaris,Symeon Papadopoulos*

Main category: cs.LG

TL;DR: The paper discusses Frugal Machine Learning (FML), which designs ML models that are efficient and cost-effective by minimizing resource usage. It categorizes FML strategies into input, learning process, and model frugality, emphasizing its importance for edge computing and IoT devices. The paper also explores recent advancements, applications, challenges, and future research directions in FML.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, cost-effective machine learning models that can operate under resource constraints, particularly in smart environments with edge computing and IoT devices.

Method: Categorizing FML strategies into input frugality, learning process frugality, and model frugality. Discussing technological enablers such as model compression, energy-efficient hardware, data-efficient learning techniques, and adaptive methods like parameter regularization, knowledge distillation, and dynamic architecture design.

Result: Provides a comprehensive taxonomy of frugal methods, case studies across diverse domains, and identifies future research directions to drive innovation in FML.

Conclusion: Frugal Machine Learning is crucial for developing efficient models suitable for environments with limited resources, and further research is needed to continue advancing this field.

Abstract: Frugal Machine Learning (FML) refers to the practice of designing Machine
Learning (ML) models that are efficient, cost-effective, and mindful of
resource constraints. This field aims to achieve acceptable performance while
minimizing the use of computational resources, time, energy, and data for both
training and inference. FML strategies can be broadly categorized into input
frugality, learning process frugality, and model frugality, each focusing on
reducing resource consumption at different stages of the ML pipeline. This
chapter explores recent advancements, applications, and open challenges in FML,
emphasizing its importance for smart environments that incorporate edge
computing and IoT devices, which often face strict limitations in bandwidth,
energy, or latency. Technological enablers such as model compression,
energy-efficient hardware, and data-efficient learning techniques are
discussed, along with adaptive methods including parameter regularization,
knowledge distillation, and dynamic architecture design that enable incremental
model updates without full retraining. Furthermore, it provides a comprehensive
taxonomy of frugal methods, discusses case studies across diverse domains, and
identifies future research directions to drive innovation in this evolving
field.

</details>


### [324] [Learning to Explore: An In-Context Learning Approach for Pure Exploration](https://arxiv.org/abs/2506.01876)
*Alessio Russo,Ryan Welch,Aldo Pacchiano*

Main category: cs.LG

TL;DR: The paper introduces In-Context Pure Exploration (ICPE), a method using Transformers to learn exploration strategies for active sequential hypothesis testing, achieving robust performance without requiring prior assumptions.


<details>
  <summary>Details</summary>
Motivation: Active sequential hypothesis testing, or pure exploration, aims to control data collection processes to efficiently identify the correct hypothesis. Current RL-based methods underperform when information structures are inadequately represented, and more complex methods like BAI are hard to devise and rely on explicit modeling assumptions.

Method: ICPE is an in-context learning approach that leverages Transformers to learn exploration strategies directly from experience. It combines supervised learning and reinforcement learning to identify and exploit latent structure across related tasks without prior assumptions.

Result: Numerical results across diverse benchmarks show ICPE's robust performance in deterministic, stochastic, and structured settings. It can match optimal instance-dependent algorithms using only deep learning techniques.

Conclusion: ICPE is a practical and general approach to data-efficient exploration, demonstrating the potential of deep learning techniques in solving active sequential hypothesis testing problems.

Abstract: In this work, we study the active sequential hypothesis testing problem, also
known as pure exploration, where the goal is to actively control a data
collection process to efficiently identify the correct hypothesis underlying a
decision problem. While relevant across multiple domains, devising adaptive
exploration strategies remains challenging, particularly due to difficulties in
encoding appropriate inductive biases. Existing Reinforcement Learning
(RL)-based methods often underperform when relevant information structures are
inadequately represented, whereas more complex methods, like Best Arm
Identification (BAI) techniques, may be difficult to devise and typically rely
on explicit modeling assumptions. To address these limitations, we introduce
In-Context Pure Exploration (ICPE), an in-context learning approach that uses
Transformers to learn exploration strategies directly from experience. ICPE
combines supervised learning and reinforcement learning to identify and exploit
latent structure across related tasks, without requiring prior assumptions.
Numerical results across diverse synthetic and semi-synthetic benchmarks
highlight ICPE's capability to achieve robust performance performance in
deterministic, stochastic, and structured settings. These results demonstrate
ICPE's ability to match optimal instance-dependent algorithms using only deep
learning techniques, making it a practical and general approach to
data-efficient exploration.

</details>


### [325] [scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics](https://arxiv.org/abs/2506.01883)
*Davide D'Ascenzo,Sebastiano Cultrera di Montesano*

Main category: cs.LG

TL;DR: scDataset is a PyTorch IterableDataset that directly operates on AnnData files using block sampling and batched fetching to provide shuffled, memory-efficient data loading for large-scale single-cell datasets. It achieves significant speed-ups over existing methods, democratizing large-scale single-cell model training.


<details>
  <summary>Details</summary>
Motivation: Modern single-cell datasets are too large to be efficiently processed by current data loading solutions which either require loading all data into memory, convert to dense formats increasing storage demands, or suffer from slow random disk access.

Method: scDataset uses block sampling and batched fetching to operate directly on AnnData files without format conversion. This approach balances randomness and I/O efficiency while avoiding the need to load entire datasets into memory or convert them to dense formats.

Result: On the Tahoe 100M dataset, scDataset achieves a 48x speed-up over AnnLoader, a 27x speed-up over HuggingFace Datasets, and an 18x speed-up over BioNeMo in single-core settings.

Conclusion: scDataset provides a more efficient method for loading large-scale single-cell data, making it easier for the broader research community to train deep learning models on these datasets.

Abstract: Modern single-cell datasets now comprise hundreds of millions of cells,
presenting significant challenges for training deep learning models that
require shuffled, memory-efficient data loading. While the AnnData format is
the community standard for storing single-cell datasets, existing data loading
solutions for AnnData are often inadequate: some require loading all data into
memory, others convert to dense formats that increase storage demands, and many
are hampered by slow random disk access. We present scDataset, a PyTorch
IterableDataset that operates directly on one or more AnnData files without the
need for format conversion. The core innovation is a combination of block
sampling and batched fetching, which together balance randomness and I/O
efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$
speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and
an 18$\times$ speed-up over BioNeMo in single-core settings. These advances
democratize large-scale single-cell model training for the broader research
community.

</details>


### [326] [Agnostic Reinforcement Learning: Foundations and Algorithms](https://arxiv.org/abs/2506.01884)
*Gene Li*

Main category: cs.LG

TL;DR: 研究强化学习在大状态空间中的统计复杂性，提出agnostic policy learning，并从环境访问、覆盖条件和表示条件三个方面进行探索，设计新算法并确定性能边界。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在许多领域取得了巨大的实证成功，但我们对需要函数逼近的大状态空间环境中RL的统计复杂性缺乏深入理解。

Method: 通过考虑最弱形式的函数逼近——agnostic policy learning，即学习者试图在一个给定的策略类Π中找到最佳策略，而没有保证Π包含底层任务的最优策略。然后从三个关键轴进行系统探索：环境访问、覆盖条件和表示条件。

Result: 设计了具有理论保证的新学习算法，并描述了任何算法的基本性能界限，揭示了重要的统计分离，突出了agnostic policy learning的力量和局限性。

Conclusion: 这项工作为理解RL中函数逼近的统计复杂性提供了一个全面的框架，并强调了不同条件下策略学习的挑战和可能性。

Abstract: Reinforcement Learning (RL) has demonstrated tremendous empirical success
across numerous challenging domains. However, we lack a strong theoretical
understanding of the statistical complexity of RL in environments with large
state spaces, where function approximation is required for sample-efficient
learning. This thesis addresses this gap by rigorously examining the
statistical complexity of RL with function approximation from a learning
theoretic perspective. Departing from a long history of prior work, we consider
the weakest form of function approximation, called agnostic policy learning, in
which the learner seeks to find the best policy in a given class $\Pi$, with no
guarantee that $\Pi$ contains an optimal policy for the underlying task.
  We systematically explore agnostic policy learning along three key axes:
environment access -- how a learner collects data from the environment;
coverage conditions -- intrinsic properties of the underlying MDP measuring the
expansiveness of state-occupancy measures for policies in the class $\Pi$, and
representational conditions -- structural assumptions on the class $\Pi$
itself. Within this comprehensive framework, we (1) design new learning
algorithms with theoretical guarantees and (2) characterize fundamental
performance bounds of any algorithm. Our results reveal significant statistical
separations that highlight the power and limitations of agnostic policy
learning.

</details>


### [327] [CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection](https://arxiv.org/abs/2506.01890)
*David Ortiz-Perez,Manuel Benavent-Lledo,Javier Rodriguez-Juan,Jose Garcia-Rodriguez,David Tomás*

Main category: cs.LG

TL;DR: The paper introduces CogniAlign, a multimodal architecture for early Alzheimer's detection that integrates audio and textual data using word-level temporal alignment and Gated Cross-Attention Fusion mechanism. Evaluated on ADReSSo dataset, it achieves 90.36% accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of cognitive disorders like Alzheimer's is crucial for timely clinical intervention and better patient outcomes. Current methods that fuse modalities at a coarse level are insufficient.

Method: CogniAlign uses a word-level temporal alignment strategy to synchronize audio embeddings with textual tokens based on transcription timestamps. It employs a Gated Cross-Attention Fusion mechanism where audio features attend over textual representations and incorporates prosodic cues by inserting pause tokens into the text.

Result: CogniAlign achieves an accuracy of 90.36% on the ADReSSo dataset, surpassing existing state-of-the-art methods. Ablation studies confirm the effectiveness of the alignment strategy, attention-based fusion, and prosodic modeling.

Conclusion: CogniAlign offers a novel approach for Alzheimer's detection through precise cross-modal interactions of audio and textual data, demonstrating superior performance compared to current methods.

Abstract: Early detection of cognitive disorders such as Alzheimer's disease is
critical for enabling timely clinical intervention and improving patient
outcomes. In this work, we introduce CogniAlign, a multimodal architecture for
Alzheimer's detection that integrates audio and textual modalities, two
non-intrusive sources of information that offer complementary insights into
cognitive health. Unlike prior approaches that fuse modalities at a coarse
level, CogniAlign leverages a word-level temporal alignment strategy that
synchronizes audio embeddings with corresponding textual tokens based on
transcription timestamps. This alignment supports the development of
token-level fusion techniques, enabling more precise cross-modal interactions.
To fully exploit this alignment, we propose a Gated Cross-Attention Fusion
mechanism, where audio features attend over textual representations, guided by
the superior unimodal performance of the text modality. In addition, we
incorporate prosodic cues, specifically interword pauses, by inserting pause
tokens into the text and generating audio embeddings for silent intervals,
further enriching both streams. We evaluate CogniAlign on the ADReSSo dataset,
where it achieves an accuracy of 90.36%, outperforming existing
state-of-the-art methods. A detailed ablation study confirms the advantages of
our alignment strategy, attention-based fusion, and prosodic modeling.

</details>


### [328] [MLorc: Momentum Low-rank Compression for Large Language Model Adaptation](https://arxiv.org/abs/2506.01897)
*Wei Shen,Yaxiang Zhang,Minhui Huang,Mengfan Xu,Jiawei Zhang,Cong Shen*

Main category: cs.LG

TL;DR: 提出了一种新的高效训练范式MLorc，通过压缩动量而非梯度来减少内存需求，同时保持全参数微调的训练动态。实验证明MLorc在不同优化器下表现优异且不牺牲时间和内存效率，并提供了合理的收敛性理论保证。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）规模的增加，全参数微调带来了巨大的内存需求，需要一种更高效的训练方法来解决这一问题。

Method: 提出了一种名为Momentum Low-rank compression (MLorc)的新方法，该方法通过直接压缩和重建动量而不是梯度，避免了对权重更新矩阵施加固定的秩约束，从而更好地保留了全参数微调的训练动态。

Result: MLorc在实验中始终优于其他内存高效训练方法，在小秩（如r=4）时可以匹配甚至超过全微调的性能，并且在不同的优化器上表现出良好的泛化能力，同时不牺牲时间和内存效率。

Conclusion: MLorc提供了一种有效的低秩压缩方法，能够在减少内存消耗的同时保持甚至提升模型性能，并且在合理假设下具有收敛性理论保证。

Abstract: With increasing size of large language models (LLMs), full-parameter
fine-tuning imposes substantial memory demands. To alleviate this, we propose a
novel memory-efficient training paradigm called Momentum Low-rank compression
(MLorc). By directly compressing and reconstructing momentum rather than
gradients, MLorc avoids imposing a fixed-rank constraint on weight update
matrices and better preserves the training dynamics of full-parameter
fine-tuning, in contrast to existing low-rank approaches such as LoRA and
GaLore. Empirically, MLorc consistently outperforms other memory-efficient
training methods, matches or even exceeds the performance of full fine-tuning
with a small rank (e.g., $r=4$), and generalizes well across different
optimizers -- all while not compromising time or memory efficiency.
Furthermore, we provide a theoretical guarantee for its convergence under
reasonable assumptions.

</details>


### [329] [Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness](https://arxiv.org/abs/2506.01913)
*Thomas Pethick,Wanyun Xie,Mete Erdogan,Kimon Antonakopoulos,Tony Silveti-Falls,Volkan Cevher*

Main category: cs.LG

TL;DR: 这篇论文提出了一种混合非欧几里得优化方法，结合了最速下降法和条件梯度法，并在深度学习任务如图像分类和语言建模中展示其特性。


<details>
  <summary>Details</summary>
Motivation: 当前的优化方法可能无法很好地处理深度学习中的特定问题（例如梯度爆炸或消失），因此需要一种新的优化方法来综合不同技术的优点。

Method: 引入了一种混合非欧几里得优化方法，该方法通过结合最速下降法和条件梯度法实现了广义梯度裁剪。同时将权重衰减以原则性方式纳入其中，并在随机情况下使用动量梯度估计器实现最优收敛率。

Result: 该方法在理论上展示了优秀的收敛性能，在实验上证明了其在图像分类和语言建模等任务中的有效性。

Conclusion: 提出的混合优化方法不仅在理论上具有优势，而且在实际应用中也能有效提升深度学习模型的表现。

Abstract: This work introduces a hybrid non-Euclidean optimization method which
generalizes gradient norm clipping by combining steepest descent and
conditional gradient approaches. The method achieves the best of both worlds by
establishing a descent property under a generalized notion of
($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner
by identifying a connection to the Frank-Wolfe short step. In the stochastic
case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a
momentum based gradient estimator. We discuss how to instantiate the algorithms
for deep learning and demonstrate their properties on image classification and
language modeling.

</details>


### [330] [Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models](https://arxiv.org/abs/2506.01919)
*Yifan Hao,Chenlu Ye,Chi Han,Tong Zhang*

Main category: cs.LG

TL;DR: Transformer模型在序列学习任务中表现出色，但对其多任务泛化能力的理论理解有限。本文研究了Transformers的逐层行为，发现其低层专注于提取受邻近标记影响的特征表示，而高层特征解耦并表现出高度的时间解缠。基于这些经验洞察，提供了对Transformer表达能力的理论分析，支持其在各种任务中的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在许多任务中表现良好，但对其多任务泛化能力的理论理解仍然有限。

Method: 研究Transformers的逐层行为，特别是在隐藏马尔可夫模型上的表现，观察到低层提取特征表示，高层特征解耦。

Result: 提供了与经验观察密切一致的理论分析，支持Transformer在序列学习任务中的有效性和效率。

Conclusion: 通过对Transformers逐层行为的研究，揭示了其多任务泛化能力的机制，并提供了理论支持。

Abstract: Transformer based models have shown remarkable capabilities in sequence
learning across a wide range of tasks, often performing well on specific task
by leveraging input-output examples. Despite their empirical success, a
comprehensive theoretical understanding of this phenomenon remains limited. In
this work, we investigate the layerwise behavior of Transformers to uncover the
mechanisms underlying their multi-task generalization ability. Taking
explorations on a typical sequence model, i.e, Hidden Markov Models, which are
fundamental to many language tasks, we observe that: first, lower layers of
Transformers focus on extracting feature representations, primarily influenced
by neighboring tokens; second, on the upper layers, features become decoupled,
exhibiting a high degree of time disentanglement. Building on these empirical
insights, we provide theoretical analysis for the expressiveness power of
Transformers. Our explicit constructions align closely with empirical
observations, providing theoretical support for the Transformer's effectiveness
and efficiency on sequence learning across diverse tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [331] [Heterogeneous Graph Backdoor Attack](https://arxiv.org/abs/2506.00191)
*Jiawei Chen,Lusi Li,Daniel Takabi,Masha Sosonkina,Rui Ning*

Main category: cs.CR

TL;DR: The paper investigates the vulnerability of Heterogeneous Graph Neural Networks (HGNNs) to backdoor attacks, identifies issues with current attacks, and proposes HGBA, a novel attack method specifically designed for HGNNs. It introduces a relation-based trigger mechanism, improves ASR measurement, and demonstrates superior performance, robustness, and applicability in both heterogeneous and homogeneous graph scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore the susceptibility of HGNNs to backdoor attacks and address the limitations of existing graph backdoor attacks on HGNNs, such as high attack budget, inefficient activation, and inaccurate evaluation.

Method: Propose HGBA, which uses a relation-based trigger mechanism to establish connections via backdoor metapaths, supports two attack strategies (Self-Node Attack and Indiscriminate Attack), and improves the ASR measurement protocol.

Result: HGBA surpasses state-of-the-art graph backdoor attacks in black-box settings with low attack budgets, shows robustness against defenses and feature perturbations, and extends effectively to homogeneous graphs.

Conclusion: HGBA highlights the vulnerability of HGNNs to backdoor attacks, providing insights into potential security threats and offering a foundation for future research on robust HGNNs.

Abstract: Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex,
multi-typed relationships across diverse domains, yet their vulnerability to
backdoor attacks remains unexplored. To address this gap, we conduct the first
investigation into the susceptibility of HGNNs to existing graph backdoor
attacks, revealing three critical issues: (1) high attack budget required for
effective backdoor injection, (2) inefficient and unreliable backdoor
activation, and (3) inaccurate attack effectiveness evaluation. To tackle these
issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first
backdoor attack specifically designed for HGNNs, introducing a novel
relation-based trigger mechanism that establishes specific connections between
a strategically selected trigger node and poisoned nodes via the backdoor
metapath. HGBA achieves efficient and stealthy backdoor injection with minimal
structural modifications and supports easy backdoor activation through two
flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally,
we improve the ASR measurement protocol, enabling a more accurate assessment of
attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses
multiple state-of-the-art graph backdoor attacks in black-box settings,
efficiently attacking HGNNs with low attack budgets. Ablation studies show that
the strength of HBGA benefits from our trigger node selection method and
backdoor metapath selection strategy. In addition, HGBA shows superior
robustness against node feature perturbations and multiple types of existing
graph backdoor defense mechanisms. Finally, extension experiments demonstrate
that the relation-based trigger mechanism can effectively extend to tasks in
homogeneous graph scenarios, thereby posing severe threats to broader
security-critical domains.

</details>


### [332] [When GPT Spills the Tea: Comprehensive Assessment of Knowledge File Leakage in GPTs](https://arxiv.org/abs/2506.00197)
*Xinyue Shen,Yun Shen,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本研究对知识文件泄露进行全面风险评估，发现了五种泄露向量，并提供了可行的解决方案以保障GPT数据供应链的安全。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究表明对抗性提示可以诱导GPT泄露知识文件内容，但是否存在其他泄露途径仍不确定，特别是在复杂的客户端、服务器和数据库数据流中。

Method: 通过分析651,022个GPT元数据、11,820个数据流和1,466个响应，结合受数据安全态势管理（DSPM）启发的新工作流程，识别出五种泄露向量：元数据、GPT初始化、检索、沙盒执行环境和提示。

Result: 发现了五种泄露向量，其中激活内置工具Code Interpreter会导致特权升级漏洞，使攻击者能够以95.95%的成功率直接下载原始知识文件；此外，28.80%的泄露文件为受版权保护的内容。

Conclusion: 提出了针对GPT构建者和平台提供者的可行解决方案，以确保GPT数据供应链的安全。

Abstract: Knowledge files have been widely used in large language model (LLM) agents,
such as GPTs, to improve response quality. However, concerns about the
potential leakage of knowledge files have grown significantly. Existing studies
demonstrate that adversarial prompts can induce GPTs to leak knowledge file
content. Yet, it remains uncertain whether additional leakage vectors exist,
particularly given the complex data flows across clients, servers, and
databases in GPTs. In this paper, we present a comprehensive risk assessment of
knowledge file leakage, leveraging a novel workflow inspired by Data Security
Posture Management (DSPM). Through the analysis of 651,022 GPT metadata, 11,820
flows, and 1,466 responses, we identify five leakage vectors: metadata, GPT
initialization, retrieval, sandboxed execution environments, and prompts. These
vectors enable adversaries to extract sensitive knowledge file data such as
titles, content, types, and sizes. Notably, the activation of the built-in tool
Code Interpreter leads to a privilege escalation vulnerability, enabling
adversaries to directly download original knowledge files with a 95.95% success
rate. Further analysis reveals that 28.80% of leaked files are copyrighted,
including digital copies from major publishers and internal materials from a
listed company. In the end, we provide actionable solutions for GPT builders
and platform providers to secure the GPT data supply chain.

</details>


### [333] [Hush! Protecting Secrets During Model Training: An Indistinguishability Approach](https://arxiv.org/abs/2506.00201)
*Arun Ganesh,Brendan McMahan,Milad Nasr,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.CR

TL;DR: The paper proposes a new method for secret protection during model training that outperforms DP-SGD.


<details>
  <summary>Details</summary>
Motivation: To address the problem of secret protection in model training without the strict requirements of differential privacy.

Method: Propose an alternate definition of secret protection and an algorithm that solves a linear program to assign weights to examples based on desired per-secret protections, followed by Poisson sampling.

Result: The proposed algorithm significantly outperforms the baseline of running DP-SGD on the whole dataset.

Conclusion: The new secret protection method provides a better balance between utility and privacy in scenarios where the data curator and data owner are the same entity.

Abstract: We consider the problem of secret protection, in which a business or
organization wishes to train a model on their own data, while attempting to not
leak secrets potentially contained in that data via the model. The standard
method for training models to avoid memorization of secret information is via
differential privacy (DP). However, DP requires a large loss in utility or a
large dataset to achieve its strict privacy definition, which may be
unnecessary in our setting where the data curator and data owner are the same
entity. We propose an alternate definition of secret protection that instead of
targeting DP, instead targets a bound on the posterior probability of secret
reconstruction. We then propose and empirically evaluate an algorithm for model
training with this secret protection definition. Our algorithm solves a linear
program to assign weights to examples based on the desired per-secret
protections, and then performs Poisson sampling using these weights. We show
our algorithm significantly outperforms the baseline of running DP-SGD on the
whole dataset.

</details>


### [334] [Compact and Selective Disclosure for Verifiable Credentials](https://arxiv.org/abs/2506.00262)
*Alessandro Buldini,Carlo Mazzocca,Rebecca Montanari,Selcuk Uluagac*

Main category: cs.CR

TL;DR: Self-Sovereign Identity (SSI) is gaining traction worldwide. This paper proposes a novel mechanism CSD-JWT for Compact and Selective Disclosure for VCs, which provides significant memory savings and minimizes network overhead.


<details>
  <summary>Details</summary>
Motivation: To empower individuals with full control over their data and enhance privacy protection through selectively disclosing specific claims within a credential.

Method: Propose CSD-JWT, leveraging a cryptographic accumulator to encode claims into a compact representation, implemented as an open-source solution.

Result: CSD-JWT offers up to 46% memory savings compared to the state-of-the-art and reduces Verifiable Presentations size by 27% to 93%.

Conclusion: CSD-JWT is well-suited for resource-constrained devices, including hardware wallets.

Abstract: Self-Sovereign Identity (SSI) is a novel identity model that empowers
individuals with full control over their data, enabling them to choose what
information to disclose, with whom, and when. This paradigm is rapidly gaining
traction worldwide, supported by numerous initiatives such as the European
Digital Identity (EUDI) Regulation or Singapore's National Digital Identity
(NDI). For instance, by 2026, the EUDI Regulation will enable all European
citizens to seamlessly access services across Europe using Verifiable
Credentials (VCs). A key feature of SSI is the ability to selectively disclose
only specific claims within a credential, enhancing privacy protection of the
identity owner. This paper proposes a novel mechanism designed to achieve
Compact and Selective Disclosure for VCs (CSD-JWT). Our method leverages a
cryptographic accumulator to encode claims within a credential to a unique,
compact representation. We implemented CSD-JWT as an open-source solution and
extensively evaluated its performance under various conditions. CSD-JWT
provides significant memory savings, reducing usage by up to 46% compared to
the state-of-the-art. It also minimizes network overhead by producing
remarkably smaller Verifiable Presentations (VPs), reduced in size by 27% to
93%. Such features make CSD-JWT especially well-suited for resource-constrained
devices, including hardware wallets designed for managing credentials.

</details>


### [335] [Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response](https://arxiv.org/abs/2506.00274)
*Jan-Niclas Hilgert,Carlo Jakobs,Michael Külper,Martin Lambertz,Axel Mahr,Elmar Padilla*

Main category: cs.CR

TL;DR: 大型语言模型（LLMs）在支持法医调查方面具有巨大潜力，但其透明度、可解释性和可重复性不足阻碍了广泛应用。本文探讨了新兴的模型上下文协议（MCP）如何应对这些挑战，并促进LLMs在数字法医学中的有意义使用。通过理论分析，研究了MCP如何整合到各种法医场景中，同时概述了在法医环境中部署MCP服务器的技术和概念考虑。分析显示，MCP不仅增强了现有的法医工作流程，还促进了LLMs在以前受限的法医领域的应用。此外，介绍了推理约束级别这一概念，用于表征特定的MCP设计选择如何有意约束模型行为，从而提高可审计性和可追溯性。研究表明，MCP作为开发更透明、可重复且法律上可辩护的LLM辅助法医工作流的基础组件具有显著潜力，同时也朝着提高数字法医分析自动化程度迈进了一步。然而，也强调了MCP在未来可能给数字法医学带来的潜在挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在法医调查中具有巨大潜力，但透明度、可解释性和可重复性问题限制了其广泛采用。因此，需要一种方法来解决这些问题并促进LLMs在数字法医学中的有效使用。

Method: 通过理论分析，研究MCP如何整合到各种法医场景中，包括人工制品分析和可解释报告的生成。同时，概述了在法医环境中部署MCP服务器的技术和概念考虑。

Result: MCP不仅加强了现有的法医工作流程，还促进了LLMs在以前受限的法医领域的应用，提高了透明度、可重复性和法律可辩护性。

Conclusion: MCP作为一个基础组件，具有显著潜力来开发更透明、可重复和法律上可辩护的LLM辅助法医工作流，但也提出了未来可能面临的挑战。

Abstract: Large language models hold considerable promise for supporting forensic
investigations, but their widespread adoption is hindered by a lack of
transparency, explainability, and reproducibility. This paper explores how the
emerging Model Context Protocol can address these challenges and support the
meaningful use of LLMs in digital forensics. Through a theoretical analysis, we
examine how MCP can be integrated across various forensic scenarios - ranging
from artifact analysis to the generation of interpretable reports. We also
outline both technical and conceptual considerations for deploying an MCP
server in forensic environments. Our analysis reveals a wide range of use cases
in which MCP not only strengthens existing forensic workflows but also
facilitates the application of LLMs to areas of forensics where their use was
previously limited. Furthermore, we introduce the concept of the inference
constraint level - a way of characterizing how specific MCP design choices can
deliberately constrain model behavior, thereby enhancing both auditability and
traceability. Our insights demonstrate that MCP has significant potential as a
foundational component for developing LLM-assisted forensic workflows that are
not only more transparent, reproducible, and legally defensible, but also
represent a step toward increased automation in digital forensic analysis.
However, we also highlight potential challenges that the adoption of MCP may
pose for digital forensics in the future.

</details>


### [336] [3D Gaussian Splat Vulnerabilities](https://arxiv.org/abs/2506.00280)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haoran Wang,Matthew Lau,Wenke Lee,Willian T. Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CR

TL;DR: The paper introduces CLOAK and DAGGER, two adversarial attacks on 3D Gaussian Splatting (3DGS). CLOAK uses view-dependent Gaussian appearances to embed adversarial content visible only from specific viewpoints. DAGGER is a targeted attack perturbing 3D Gaussians to deceive multi-stage object detectors without access to training data. These highlight vulnerabilities in DGS.


<details>
  <summary>Details</summary>
Motivation: To explore the vulnerabilities of 3D Gaussian Splatting (3DGS) in safety-critical applications by demonstrating how adversaries can manipulate scenes using view-dependent properties or direct perturbations of the 3D Gaussians.

Method: CLOAK leverages view-dependent Gaussian appearances to embed adversarial content visible only from specific viewpoints. DAGGER directly perturbs 3D Gaussians without access to underlying training data using established methods like projected gradient descent to deceive multi-stage object detectors.

Result: Successful demonstration of CLOAK embedding adversarial content dependent on viewing angle and DAGGER deceiving multi-stage object detectors such as Faster R-CNN, highlighting underexplored vulnerabilities in 3DGS.

Conclusion: 3D Gaussian Splatting has underexplored vulnerabilities that can be exploited by adversarial attacks like CLOAK and DAGGER, posing potential threats to robotic learning for autonomous navigation and other safety-critical applications.

Abstract: With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical
applications, how can an adversary manipulate the scene to cause harm? We
introduce CLOAK, the first attack that leverages view-dependent Gaussian
appearances - colors and textures that change with viewing angle - to embed
adversarial content visible only from specific viewpoints. We further
demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D
Gaussians without access to underlying training data, deceiving multi-stage
object detectors e.g., Faster R-CNN, through established methods such as
projected gradient descent. These attacks highlight underexplored
vulnerabilities in 3DGS, introducing a new potential threat to robotic learning
for autonomous navigation and other safety-critical 3DGS applications.

</details>


### [337] [Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00281)
*Chris M. Ward,Josh Harguess*

Main category: cs.CR

TL;DR: RAG系统结合了LLM和外部知识库，容易受到多种对抗攻击。本文通过分析行业趋势，指出了RAG系统的三种主要攻击向量，并从风险管理的角度提出了一系列缓解风险的措施。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG系统在行业中越来越受欢迎，但其面临的安全威胁也需要被关注。

Method: 本文分析了RAG系统的主要攻击向量，包括注入攻击、数据投毒和对抗查询操控，并提出了一个包含输入验证、对抗训练和实时监控等措施的风险管理框架。

Result: 明确了RAG系统面临的三大主要攻击类型，并提出了有效的风险控制策略。

Conclusion: 为了保护RAG系统的安全，需要实施多层次的风险管理措施，以应对各种潜在的攻击。

Abstract: Retrieval-Augmented Generation (RAG) systems, which integrate Large Language
Models (LLMs) with external knowledge sources, are vulnerable to a range of
adversarial attack vectors. This paper examines the importance of RAG systems
through recent industry adoption trends and identifies the prominent attack
vectors for RAG: prompt injection, data poisoning, and adversarial query
manipulation. We analyze these threats under risk management lens, and propose
robust prioritized control list that includes risk-mitigating actions like
input validation, adversarial training, and real-time monitoring.

</details>


### [338] [Data Flows in You: Benchmarking and Improving Static Data-flow Analysis on Binary Executables](https://arxiv.org/abs/2506.00313)
*Nicolaas Weideman,Sima Arasteh,Mukund Raghothaman,Jelena Mirkovic,Christophe Hauser*

Main category: cs.CR

TL;DR: Data-flow analysis is crucial in security research, yet accurately analyzing binary executables' data flow is theoretically undecidable due to complexities. This paper addresses the gap in understanding the accuracy and limitations of existing binary analysis engines by creating a labeled benchmark dataset consisting of 215,072 microbenchmark test cases mapped to 277,072 binary executables, as well as dynamically-discovered data flows from 6 real-world executables. The authors evaluate three state-of-the-art data-flow analysis tools (angr, Ghidra, and Miasm), revealing very low accuracy. They then propose three model extensions for static data-flow analysis that significantly enhance accuracy, achieving nearly perfect recall (0.99) and improving precision from 0.13 to 0.32. Finally, they demonstrate how these model extensions improve vulnerable instruction identification in the context of vulnerability discovery.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the critical role data-flow analysis plays in security research and the need to understand the accuracy and limitations of current binary analysis engines. As accurate data-flow analysis in binary executables is an undecidable problem due to the complexities of binary code, there is a need for a comprehensive evaluation framework.

Method: The researchers created a labeled benchmark dataset with 215,072 microbenchmark test cases linked to 277,072 binary executables, along with dynamically-discovered data flows from 6 real-world executables. They used this dataset to evaluate three leading data-flow analysis implementations (in angr, Ghidra, and Miasm). Based on their findings, they proposed three model extensions for static data-flow analysis.

Result: The evaluation showed very low accuracy in the three evaluated tools. By applying the proposed model extensions, they achieved almost perfect recall (0.99) and increased precision from 0.13 to 0.32. These improvements tangibly enhanced vulnerable instruction identification in vulnerability discovery.

Conclusion: The study concludes that current state-of-the-art data-flow analysis tools have significant accuracy issues. However, the proposed model extensions can greatly improve the accuracy of static data-flow analysis, making it more effective for identifying vulnerable instructions in the context of security research.

Abstract: Data-flow analysis is a critical component of security research.
Theoretically, accurate data-flow analysis in binary executables is an
undecidable problem, due to complexities of binary code. Practically, many
binary analysis engines offer some data-flow analysis capability, but we lack
understanding of the accuracy of these analyses, and their limitations. We
address this problem by introducing a labeled benchmark data set, including
215,072 microbenchmark test cases, mapping to 277,072 binary executables,
created specifically to evaluate data-flow analysis implementations.
Additionally, we augment our benchmark set with dynamically-discovered data
flows from 6 real-world executables. Using our benchmark data set, we evaluate
three state of the art data-flow analysis implementations, in angr, Ghidra and
Miasm and discuss their very low accuracy and reasons behind it. We further
propose three model extensions to static data-flow analysis that significantly
improve accuracy, achieving almost perfect recall (0.99) and increasing
precision from 0.13 to 0.32. Finally, we show that leveraging these model
extensions in a vulnerability-discovery context leads to a tangible improvement
in vulnerable instruction identification.

</details>


### [339] [Local Frames: Exploiting Inherited Origins to Bypass Content Blockers](https://arxiv.org/abs/2506.00317)
*Alisha Ukani,Hamed Haddadi,Alex C. Snoeren,Peter Snyder*

Main category: cs.CR

TL;DR: 研究发现，许多流行的Web安全和隐私工具由于未能正确处理本地框架（local frames），导致用户仍然容易受到各种攻击技术的影响，如浏览器指纹识别、基于Cookie的跟踪和数据外泄。通过对六个流行工具的研究，发现了19个漏洞，并总结了这些工具在处理本地框架时的常见问题。


<details>
  <summary>Details</summary>
Motivation: 探讨为何许多流行的Web安全和隐私工具无法有效保护用户免受攻击技术的影响，特别是在涉及本地框架（local frames）时的功能缺陷。

Method: 研究者分析了四个核心功能，并开发测试来确定是否可以通过本地框架规避这些功能。随后对六个流行的隐私和安全工具进行了测试，识别出至少19个漏洞，并测量了流行网站中使用本地框架的情况。

Result: 发现6个工具中共有19个漏洞，56%的流行网站使用本地框架，而73.7%的请求应被阻止但触发了漏洞；此外，14.3%的所有爬取网站在其本地框架内发出应被阻止的请求。

Conclusion: 该研究表明，许多隐私工具存在系统性漏洞，主要源于传统Web功能与浏览器隐私边界的交互问题。研究者向工具开发者披露了漏洞，并讨论了修补产品及对未来隐私和安全研究的启示。

Abstract: We present a study of how local frames (i.e., iframes with non-URL sources
like "about:blank") are mishandled by a wide range of popular Web security and
privacy tools. As a result, users of these tools remain vulnerable to the very
attack techniques they seek to protect against, including browser
fingerprinting, cookie-based tracking, and data exfiltration. The tools we
study are vulnerable in different ways, but all share a root cause: legacy Web
functionality interacting with browser privacy boundaries in unexpected ways,
leading to systemic vulnerabilities in tools developed, maintained, and
recommended by privacy experts and activists.
  We consider four core capabilities supported by most privacy tools and
develop tests to determine whether each can be evaded through the use of local
frames. We apply our tests to six popular Web privacy and security tools,
identifying at least one vulnerability in each for a total of 19, and extract
common patterns regarding their mishandling of local frames. Our measurement of
popular websites finds that 56% employ local frames and that 73.7% of the
requests made by these local frames should be blocked by popular filter lists
but instead trigger the vulnerabilities we identify; from another perspective,
14.3% of all sites that we crawl make requests that should be blocked inside of
local frames. We disclosed the vulnerabilities to the tool authors and discuss
both our experiences working with them to patch their products and the
implications of our findings for other privacy and security research.

</details>


### [340] [dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation](https://arxiv.org/abs/2506.00322)
*Sofiane Mahiou,Amir Dizche,Reza Nazari,Xinmin Wu,Ralph Abbey,Jorge Silva,Georgi Ganev*

Main category: cs.CR

TL;DR: The paper introduces dpmm, an open-source library for generating synthetic data with Differentially Private guarantees. It includes three models (PrivBayes, MST, AIM) that offer better utility and functionality. The library is easy-to-install, customizable, and robust, aiming to accommodate a wide audience. Code available at https://github.com/sassoftware/dpmm.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, open-source solution for synthetic data generation ensuring Differential Privacy, addressing DP-related vulnerabilities while offering superior utility and rich functionality compared to existing alternatives.

Method: Development of the dpmm library incorporating PrivBayes, MST, and AIM models. Implementation of best practices to ensure end-to-end Differential Privacy guarantees and mitigate known vulnerabilities.

Result: Achievement of superior utility in synthetic data generation, provision of richer functionality, and assurance of end-to-end Differential Privacy.

Conclusion: dpmm is a robust, customizable, and user-friendly library for synthetic data generation with Differential Privacy guarantees, suitable for a wide audience.

Abstract: We propose dpmm, an open-source library for synthetic data generation with
Differentially Private (DP) guarantees. It includes three popular marginal
models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer
richer functionality compared to alternative implementations. Additionally, we
adopt best practices to provide end-to-end DP guarantees and address well-known
DP-related vulnerabilities. Our goal is to accommodate a wide audience with
easy-to-install, highly customizable, and robust model implementations.
  Our codebase is available from https://github.com/sassoftware/dpmm.

</details>


### [341] [Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy](https://arxiv.org/abs/2506.00359)
*Jie Ren,Zhenwei Dai,Xianfeng Tang,Yue Xing,Shenglai Zeng,Hui Liu,Jingying Zeng,Qiankun Peng,Samarth Varshney,Suhang Wang,Qi He,Charu C. Aggarwal,Hui Liu*

Main category: cs.CR

TL;DR: 尽管大语言模型（LLMs）表现出色，但训练中对敏感、受版权保护或有害数据的滥用引发了担忧。为解决此问题，研究者开发了'遗忘'技术以移除特定数据的影响。然而，本文揭示了基于微调的遗忘技术的关键漏洞：恶意用户可通过精心设计的遗忘请求，隐秘地降低模型对善意用户的实用性。针对这一问题，我们提出了Scope-aware Unlearning（SU）方法，通过引入范围项至遗忘目标来增强模型定位遗忘效果的能力，从而显著提高对Stealthy Attack（SA）的鲁棒性。实验验证了SA和SU的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，其在训练过程中可能滥用敏感、受版权保护或有害的数据的问题日益受到关注。为了应对这些担忧，研究者开发了'遗忘'技术。然而，现有的遗忘技术存在关键漏洞，即恶意用户可以利用这些技术，通过构造操纵性的遗忘请求，隐秘地降低模型对正常用户的实用性。因此，需要一种新的方法来解决这一问题。

Method: 本文提出了一种称为Scope-aware Unlearning（SU）的方法。该方法通过在遗忘目标中引入一个范围项，鼓励模型将遗忘效果局限在特定范围内，从而避免对正常用户产生不必要的影响。SU方法无需额外的数据处理，能够与现有的微调框架无缝集成。此外，文章还设计了一个名为Stealthy Attack（SA）的攻击方法，用于验证现有遗忘技术的漏洞，并证明SU方法的有效性。

Result: 实验结果表明，Stealthy Attack（SA）能够有效地利用现有遗忘技术的漏洞，导致模型对正常用户的实用性下降。而提出的Scope-aware Unlearning（SU）方法则显著提高了模型对SA攻击的鲁棒性，验证了SU方法的有效性和优越性。

Conclusion: 本文揭示了基于微调的遗忘技术中存在的关键漏洞，并提出了一种轻量级增强方法Scope-aware Unlearning（SU）。SU方法通过引入范围项至遗忘目标，增强了模型定位遗忘效果的能力，有效解决了现有技术的漏洞。实验结果证明，SU方法在不增加额外数据处理的情况下，显著提高了模型对Stealthy Attack（SA）攻击的鲁棒性，为未来的遗忘技术研究提供了重要参考。

Abstract: Although Large Language Models (LLMs) have demonstrated impressive
capabilities across a wide range of tasks, growing concerns have emerged over
the misuse of sensitive, copyrighted, or harmful data during training. To
address these concerns, unlearning techniques have been developed to remove the
influence of specific data without retraining from scratch. However, this paper
reveals a critical vulnerability in fine-tuning-based unlearning: a malicious
user can craft a manipulated forgetting request that stealthily degrades the
model's utility for benign users. We demonstrate this risk through a
red-teaming Stealthy Attack (SA), which is inspired by two key limitations of
existing unlearning (the inability to constrain the scope of unlearning effect
and the failure to distinguish benign tokens from unlearning signals). Prior
work has shown that unlearned models tend to memorize forgetting data as
unlearning signals, and respond with hallucinations or feigned ignorance when
unlearning signals appear in the input. By subtly increasing the presence of
common benign tokens in the forgetting data, SA enhances the connection between
benign tokens and unlearning signals. As a result, when normal users include
such tokens in their prompts, the model exhibits unlearning behaviors, leading
to unintended utility degradation. To address this vulnerability, we propose
Scope-aware Unlearning (SU), a lightweight enhancement that introduces a scope
term into the unlearning objective, encouraging the model to localize the
forgetting effect. Our method requires no additional data processing,
integrates seamlessly with existing fine-tuning frameworks, and significantly
improves robustness against SA. Extensive experiments validate the
effectiveness of both SA and SU.

</details>


### [342] [Adversarial Machine Learning for Robust Password Strength Estimation](https://arxiv.org/abs/2506.00373)
*Pappu Jha,Hanzla Hamid,Oluseyi Olukola,Ashim Dahal,Nick Rahimi*

Main category: cs.CR

TL;DR: 密码仍然是数字时代保护敏感数据的最常见方法之一。然而，弱密码选择继续对数据安全和隐私构成重大风险。本研究通过使用对抗机器学习来开发强大的密码强度估计模型来解决这个问题，对抗机器学习是一种在故意制作的欺骗性密码上训练模型的技术，以暴露和解决这些密码造成的漏洞。我们应用五种分类算法，并使用包含超过670,000个对抗密码样本的数据集来训练模型。结果表明，与传统机器学习模型相比，对抗训练将密码强度分类准确性提高了多达20%。这强调了将对抗机器学习集成到安全系统中以增强其对现代自适应威胁的鲁棒性的重要性。


<details>
  <summary>Details</summary>
Motivation: 弱密码选择对数据安全和隐私构成重大风险，需要更强大的密码强度估计模型。

Method: 使用对抗机器学习技术，在故意制作的欺骗性密码上训练模型，应用五种分类算法，并使用包含超过670,000个对抗密码样本的数据集来训练模型。

Result: 对抗训练将密码强度分类准确性提高了多达20%。

Conclusion: 将对抗机器学习集成到安全系统中可以增强其对现代自适应威胁的鲁棒性。

Abstract: Passwords remain one of the most common methods for securing sensitive data
in the digital age. However, weak password choices continue to pose significant
risks to data security and privacy. This study aims to solve the problem by
focusing on developing robust password strength estimation models using
adversarial machine learning, a technique that trains models on intentionally
crafted deceptive passwords to expose and address vulnerabilities posed by such
passwords. We apply five classification algorithms and use a dataset with more
than 670,000 samples of adversarial passwords to train the models. Results
demonstrate that adversarial training improves password strength classification
accuracy by up to 20% compared to traditional machine learning models. It
highlights the importance of integrating adversarial machine learning into
security systems to enhance their robustness against modern adaptive threats.
  Keywords: adversarial attack, password strength, classification, machine
learning

</details>


### [343] [A Systematic Review of Metaheuristics-Based and Machine Learning-Driven Intrusion Detection Systems in IoT](https://arxiv.org/abs/2506.00377)
*Mohammad Shamim Ahsan,Salekul Islam,Swakkhar Shatabda*

Main category: cs.CR

TL;DR: The paper presents a comprehensive review of using metaheuristic algorithms in machine learning-based intrusion detection systems (IDSs) for IoT, analyzing their applications and effectiveness, proposing a taxonomy of IoT-IDSs, and discussing future optimization potentials.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for effective intrusion detection systems in IoT environments due to security challenges like cyberattacks, while addressing the limitations of traditional machine learning models which demand high computing power and resources.

Method: The method involves conducting a systematic review of various applications of metaheuristic algorithms integrated with machine learning models for developing IoT-IDSs, analyzing correlations between optimization techniques and ML models, and categorizing existing IoT-IDSs into a taxonomy.

Result: The study reveals that metaheuristic algorithms are effective in feature selection, parameter tuning, and hybrid usages within IoT-IDSs, leading to improved performance and efficiency of these systems.

Conclusion: Metaheuristic algorithms play a crucial role in enhancing the capabilities of machine learning-based IDSs for IoT. Future research should focus on exploring promising optimization algorithms and technologies to further improve the efficiency of IoT-IDSs.

Abstract: The widespread adoption of the Internet of Things (IoT) has raised a new
challenge for developers since it is prone to known and unknown cyberattacks
due to its heterogeneity, flexibility, and close connectivity. To defend
against such security breaches, researchers have focused on building
sophisticated intrusion detection systems (IDSs) using machine learning (ML)
techniques. Although these algorithms notably improve detection performance,
they require excessive computing power and resources, which are crucial issues
in IoT networks considering the recent trends of decentralized data processing
and computing systems. Consequently, many optimization techniques have been
incorporated with these ML models. Specifically, a special category of
optimizer adopted from the behavior of living creatures and different aspects
of natural phenomena, known as metaheuristic algorithms, has been a central
focus in recent years and brought about remarkable results. Considering this
vital significance, we present a comprehensive and systematic review of various
applications of metaheuristics algorithms in developing a machine
learning-based IDS, especially for IoT. A significant contribution of this
study is the discovery of hidden correlations between these optimization
techniques and machine learning models integrated with state-of-the-art
IoT-IDSs. In addition, the effectiveness of these metaheuristic algorithms in
different applications, such as feature selection, parameter or hyperparameter
tuning, and hybrid usages are separately analyzed. Moreover, a taxonomy of
existing IoT-IDSs is proposed. Furthermore, we investigate several critical
issues related to such integration. Our extensive exploration ends with a
discussion of promising optimization algorithms and technologies that can
enhance the efficiency of IoT-IDSs.

</details>


### [344] [Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences](https://arxiv.org/abs/2506.00419)
*Mohammad Saqib,Saikat Chakraborty,Santu Karmaker,Niranjan Balasubramanian*

Main category: cs.CR

TL;DR: LLM生成的代码经常存在安全问题。本文通过创建高质量的训练数据集DiSCo和新的偏好优化算法LPO，显著减少了代码的安全隐患并提高了整体代码质量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的代码中存在诸多安全隐患，而获取涵盖广泛安全问题的高质量训练数据以及对模型进行安全代码对齐是两个关键挑战。

Method: 1. 引入了一种从前沿LLM中提取不安全与安全代码对及其修复解释的方法，构建数据集DiSCo。2. 提出了一种新的局部偏好优化算法LPO，通过屏蔽与安全相关的令牌来聚焦于代码的局部差异，并添加正则化项以防止代码质量下降。

Result: 评估表明，使用DiSCo数据集训练并结合LPO算法可以显著降低代码中的安全隐患，同时提高整体代码质量。

Conclusion: 本文提出的数据集和算法为提升LLM生成代码的安全性和质量提供了有效方法，相关资源已开源。

Abstract: LLM generated code often contains security issues. We address two key
challenges in improving secure code generation. First, obtaining high quality
training data covering a broad set of security issues is critical. To address
this, we introduce a method for distilling a preference dataset of insecure and
secure code pairs from frontier LLMs, along with a security reasoning that
explains the issues and the fix. The key idea here is to make use of security
knowledge sources to devise a systematic prompting strategy that ensures broad
coverage. Second, aligning models to secure code requires focusing on localized
regions of code. Direct preference optimization methods, like SimPO, are not
designed to handle these localized differences and turn out to be ineffective.
We address this with a new localized preference optimization algorithm that
masks the security related tokens in both the winning (secure) and losing
(insecure) responses. To prevent loss in code quality, we also add a
regularizer. Evaluations show that both training on our dataset, DiSCo, and the
new preference optimization algorithm, LPO, yield substantial reductions in
code insecurity while also improving overall code quality. Code and dataset are
available at https://github.com/StonyBrookNLP/disco-lpo.

</details>


### [345] [Hybrid Cloud Security: Balancing Performance, Cost, and Compliance in Multi-Cloud Deployments](https://arxiv.org/abs/2506.00426)
*Anjani kumar Polinati*

Main category: cs.CR

TL;DR: The paper explores how organizations manage key parameters in hybrid cloud ecosystems, addresses challenges like resource allocation, pricing complexity, and information security, and proposes a security and cost optimization framework based on zero trust architecture. It concludes with recommendations for further research.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the challenges faced by organizations when operationalizing hybrid cloud adoptions, such as balancing resource distribution, understanding complex pricing models from cloud providers, and ensuring strong security infrastructure.

Method: The study validates security and performance management solutions through a detailed case study of AWS and Azure based hybrid cloud adoption. It also proposes a hybrid cloud security and cost optimization framework incorporating zero trust architecture, encryption, and hybrid cloud policies.

Result: The proposed security and cost optimization framework was validated successfully in the case study, providing useful guidance for organizations adopting hybrid clouds.

Conclusion: The conclusion recommends further research on automation of hybrid cloud service management, integration of multi-clouds, and data privacy issues, highlighting their impact on contemporary enterprises.

Abstract: The pervasive use of hybrid cloud computing models has changed enterprise as
well as Information Technology services infrastructure by giving businesses
simple and cost-effective options of combining on-premise IT equipment with
public cloud services. hybrid cloud solutions deploy multifaceted models of
security, performance optimization, and cost efficiency, conventionally
fragmented in the cloud computing milieu. This paper examines how organizations
manage these parameters in hybrid cloud ecosystems while providing solutions to
the challenges they face in operationalizing hybrid cloud adoptions. The study
captures the challenges of achieving a balance in resource distribution between
on-premise and cloud resources (herein referred to as the "resource allocation
challenge"), the complexity of pricing models from cloud providers like AWS,
Microsoft Azure, Google Cloud (herein called the 'pricing complexity problem'),
and the urgency for strong security infrastructure to safeguard sensitive
information (known as 'the information security problem'). This study
demonstrates the security and performance management solutions proposed were
validated in a detailed case study of adoption of AWS and Azure based hybrid
cloud and provides useful guidance. Also, a hybrid cloud security and cost
optimization framework based on zero trust architecture, encryption, hybrid
cloud policies, and others, is proposed.
  The conclusion includes recommendations for research on automation of hybrid
cloud service management, integration of multi-clouds, and the ever-present
question of data privacy, stressing how those matters affect contemporary
enterprises.

</details>


### [346] [Bridging the Gap between Hardware Fuzzing and Industrial Verification](https://arxiv.org/abs/2506.00461)
*Ruiyang Ma,Tianhao Wei,Jiaxi Zhang,Chun Yang,Jiangfang Yi,Guojie Luo*

Main category: cs.CR

TL;DR: 随着硬件设计复杂性的增加，硬件模糊测试成为自动化验证过程的有希望的工具。然而，在应用于工业之前，仍存在显著差距。本文从工业使用角度总结了硬件模糊测试的当前进展，并提出了解决方案以弥合硬件模糊测试与工业验证之间的差距。首先，回顾了最近的硬件模糊测试方法并分析了它们与工业验证的兼容性，建立了评估标准。其次，检查了当前验证工具对硬件模糊测试的支持效率，确定了由于工业环境支持不足导致的性能瓶颈，并提出了原型HwFuzzEnv来克服这些瓶颈，使硬件模糊测试方法在工业环境中实现数百倍的速度提升。这项工作可作为EDA公司的参考，鼓励他们改进工具以更有效地支持工业验证中的硬件模糊测试。


<details>
  <summary>Details</summary>
Motivation: 硬件设计复杂度增加，而硬件模糊测试作为自动化验证过程的潜在工具尚未在工业中广泛应用，需要解决其与工业验证之间的差距。

Method: 1. 回顾和分析硬件模糊测试方法与工业验证的兼容性。
2. 建立评估硬件模糊测试方法是否兼容的标准。
3. 检查当前验证工具对硬件模糊测试的支持效率。
4. 识别工业环境中导致硬件模糊测试性能瓶颈的因素。
5. 提出原型HwFuzzEnv以克服性能瓶颈。
6. 测试HwFuzzEnv在工业环境中的效果。

Result: 通过HwFuzzEnv原型，硬件模糊测试方法在工业环境中实现了数百倍的速度提升。

Conclusion: 本文提出的HwFuzzEnv可以作为EDA公司的参考，推动工具改进以更有效地支持工业验证中的硬件模糊测试。

Abstract: As hardware design complexity increases, hardware fuzzing emerges as a
promising tool for automating the verification process. However, a significant
gap still exists before it can be applied in industry. This paper aims to
summarize the current progress of hardware fuzzing from an industry-use
perspective and propose solutions to bridge the gap between hardware fuzzing
and industrial verification. First, we review recent hardware fuzzing methods
and analyze their compatibilities with industrial verification. We establish
criteria to assess whether a hardware fuzzing approach is compatible. Second,
we examine whether current verification tools can efficiently support hardware
fuzzing. We identify the bottlenecks in hardware fuzzing performance caused by
insufficient support from the industrial environment. To overcome the
bottlenecks, we propose a prototype, HwFuzzEnv, providing the necessary support
for hardware fuzzing. With this prototype, the previous hardware fuzzing method
can achieve a several hundred times speedup in industrial settings. Our work
could serve as a reference for EDA companies, encouraging them to enhance their
tools to support hardware fuzzing efficiently in industrial verification.

</details>


### [347] [Scaling DeFi with ZK Rollups: Design, Deployment, and Evaluation of a Real-Time Proof-of-Concept](https://arxiv.org/abs/2506.00500)
*Krzysztof Gogol,Szczepan Gurgul,Faizan Nehal Siddiqui,David Branes,Claudio Tessone*

Main category: cs.CR

TL;DR: The paper explores Zero-Knowledge Rollups' potential to solve Ethereum's scalability issues, demonstrating a proof-of-concept with a decentralized exchange that processes significantly more transactions per second than Ethereum alone.


<details>
  <summary>Details</summary>
Motivation: Ethereum's current scalability limitations hinder the adoption of decentralized applications.

Method: The authors examine the technical aspects of ZK Rollups and conduct performance stress tests in real-world DeFi applications by setting up a proof-of-concept including a ZK rollup and decentralized exchange with a load balancer generating token swaps.

Result: The rollup processed up to 71 swap transactions per second, compared to Ethereum's capability of handling 12 general transactions per second.

Conclusion: ZK Rollups offer a viable solution for enhancing Ethereum's throughput and efficiency, though there are trade-offs regarding transaction finality and associated security concerns.

Abstract: Ethereum's scalability limitations pose significant challenges for the
adoption of decentralized applications (dApps). Zero-Knowledge Rollups (ZK
Rollups) present a promising solution, bundling transactions off-chain and
submitting validity proofs on-chain to enhance throughput and efficiency. In
this work, we examine the technical underpinnings of ZK Rollups and stress test
their performance in real-world applications in decentralized finance (DeFi).
We set up a proof-of-concept (PoC) consisting of ZK rollup and decentralized
exchange, and implement load balancer generating token swaps. Our results show
that the rollup can process up to 71 swap transactions per second, compared to
12 general transaction by Ethereum. We further analyze transaction finality
trade-offs with related security concerns, and discuss the future directions
for integrating ZK Rollups into Ethereum's broader ecosystem.

</details>


### [348] [Robust and Verifiable MPC with Applications to Linear Machine Learning Inference](https://arxiv.org/abs/2506.00518)
*Tzu-Shen Wang,Jimmy Dani,Juan Garay,Soamar Homsi,Nitesh Saxena*

Main category: cs.CR

TL;DR: The paper presents an efficient MPC protocol with strong security guarantees in dishonest majority settings, achieving complete identifiability and robustness using lattice-based commitment and a semi-honest trusted third party.


<details>
  <summary>Details</summary>
Motivation: Current MPC protocols either only ensure security with abort or suffer from performance limitations due to costly operations. There is a need for a protocol that achieves both complete identifiability and robustness while maintaining efficiency.

Method: The protocol is based on the approach by Rivinius et al., utilizing lattice-based commitment for better efficiency and incorporating a semi-honest trusted third party to achieve robustness. It allows honest parties to detect malicious ones and continue computation without restart.

Result: The protocol demonstrates efficient recovery from malicious behavior through benchmarking. In a ML-as-a-service scenario, it shows slightly lower efficiency than SPDZ but offers stronger security properties.

Conclusion: The proposed MPC protocol provides a balance of efficiency and strong security features, making it suitable for scenarios requiring complete identifiability and robustness against malicious participants.

Abstract: In this work, we present an efficient secure multi-party computation MPC
protocol that provides strong security guarantees in settings with dishonest
majority of participants who may behave arbitrarily. Unlike the popular MPC
implementation known as SPDZ [Crypto '12], which only ensures security with
abort, our protocol achieves both complete identifiability and robustness. With
complete identifiability, honest parties can detect and unanimously agree on
the identity of any malicious party. Robustness allows the protocol to continue
with the computation without requiring a restart, even when malicious behavior
is detected. Additionally, our approach addresses the performance limitations
observed in the protocol by Cunningham et al. [ICITS '17], which, while
achieving complete identifiability, is hindered by the costly exponentiation
operations required by the choice of commitment scheme.
  Our protocol is based on the approach by Rivinius et al. [S&P '22], utilizing
lattice-based commitment for better efficiency. We achieved robustness with the
help of a semi-honest trusted third party. We benchmark our robust protocol,
showing the efficient recovery from parties' malicious behavior.
  Finally, we benchmark our protocol on a ML-as-a-service scenario, wherein
clients off-load the desired computation to the servers, and verify the
computation result. We benchmark on linear ML inference, running on various
datasets. While our efficiency is slightly lower compared to SPDZ's, we offer
stronger security properties that provide distinct advantages.

</details>


### [349] [The Security Threat of Compressed Projectors in Large Vision-Language Models](https://arxiv.org/abs/2506.00534)
*Yudong Zhang,Ruobing Xie,Xingwu Sun,Jiansheng Chen,Zhanhui Kang,Di Wang,Yu Wang*

Main category: cs.CR

TL;DR: The abstract discusses the importance of choosing the right visual language projector (VLP) for training large visual language models (LVLMs), emphasizing security aspects. Compressed VLPs are found to be vulnerable, whereas uncompressed ones show robust security.


<details>
  <summary>Details</summary>
Motivation: To provide guidance on selecting optimal VLPs by evaluating their security implications.

Method: Comprehensive evaluation of both compressed and uncompressed VLPs in terms of their security profiles.

Result: Compressed projectors exhibit substantial vulnerabilities, while uncompressed projectors demonstrate robust security properties.

Conclusion: Researchers should consider the security aspects when choosing VLPs for LVLMs to enhance the security and reliability of visual language models.

Abstract: The choice of a suitable visual language projector (VLP) is critical to the
successful training of large visual language models (LVLMs). Mainstream VLPs
can be broadly categorized into compressed and uncompressed projectors, and
each offering distinct advantages in performance and computational efficiency.
However, their security implications have not been thoroughly examined. Our
comprehensive evaluation reveals significant differences in their security
profiles: compressed projectors exhibit substantial vulnerabilities, allowing
adversaries to successfully compromise LVLMs even with minimal knowledge of
structural information. In stark contrast, uncompressed projectors demonstrate
robust security properties and do not introduce additional vulnerabilities.
These findings provide critical guidance for researchers in selecting optimal
VLPs that enhance the security and reliability of visual language models. The
code will be released.

</details>


### [350] [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
*Jiahui Geng,Thy Thy Tran,Preslav Nakov,Iryna Gurevych*

Main category: cs.CR

TL;DR: The paper presents Con Instruction, a novel method to generate adversarial images or audio for attacking multimodal language models (MLLMs) without needing training data or textual preprocessing. It effectively bypasses safety mechanisms in several MLLMs and achieves high attack success rates. The authors also introduce the Attack Response Categorization (ARC) framework for evaluating model responses.


<details>
  <summary>Details</summary>
Motivation: To explore the vulnerabilities of MLLMs in interpreting non-textual instructions by generating adversarial examples that align with target instructions in the embedding space, revealing potential security risks.

Method: Con Instruction optimizes adversarial images or audio to closely match target instructions in the embedding space. This method does not require training data or preprocessing of textual instructions.

Result: Con Instruction successfully bypasses safety mechanisms in multiple MLLMs, achieving attack success rates of 81.3% and 86.6% on LLaVA-v1.5 (13B). The ARC framework provides comprehensive evaluation of model responses.

Conclusion: Con Instruction highlights significant vulnerabilities in MLLMs against non-textual adversarial examples. The study also explores countermeasures and reveals performance gaps among existing defense techniques.

Abstract: Existing attacks against multimodal language models (MLLMs) primarily
communicate instructions through text accompanied by adversarial images. In
contrast, we exploit the capabilities of MLLMs to interpret non-textual
instructions, specifically, adversarial images or audio generated by our novel
method, Con Instruction. We optimize these adversarial examples to align
closely with target instructions in the embedding space, revealing the
detrimental implications of MLLMs' sophisticated understanding. Unlike prior
work, our method does not require training data or preprocessing of textual
instructions. While these non-textual adversarial examples can effectively
bypass MLLM safety mechanisms, their combination with various text inputs
substantially amplifies attack success. We further introduce a new Attack
Response Categorization (ARC) framework, which evaluates both the quality of
the model's response and its relevance to the malicious instructions.
Experimental results demonstrate that Con Instruction effectively bypasses
safety mechanisms in multiple vision- and audio-language models, including
LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio, evaluated on two standard
benchmarks: AdvBench and SafeBench. Specifically, our method achieves the
highest attack success rates, reaching 81.3% and 86.6% on LLaVA-v1.5 (13B). On
the defense side, we explore various countermeasures against our attacks and
uncover a substantial performance gap among existing techniques. Our
implementation is made publicly available.

</details>


### [351] [Communication Efficient Multiparty Private Set Intersection from Multi-Point Sequential OPRF](https://arxiv.org/abs/2506.00566)
*Xinyu Feng,Yukun Wang,Cong Li,Wu Xin,Ming Yao,Dian Zhang,Wanwan Wang,Hao He*

Main category: cs.CR

TL;DR: In this paper, a new MPSI protocol with ring topology is proposed using MP-SOPRF, which reduces communication by 74.8% and improves computational efficiency by 6%-287%.


<details>
  <summary>Details</summary>
Motivation: To address the high leader node load in star-topology MPSI protocols and the high communication complexity and overhead in mesh-topology protocols.

Method: Propose MP-SOPRF in a multi-party setting and develop an MPSI protocol with ring topology based on it.

Result: Experiments show that the new MPSI protocol reduces communication by 74.8% and improves computational efficiency by 6%-287%.

Conclusion: The proposed MPSI protocol is semi-honest secure under the Hamming correlation robustness assumption and outperforms existing state-of-the-art protocols.

Abstract: Multiparty private set intersection (MPSI) allows multiple participants to
compute the intersection of their locally owned data sets without revealing
them. MPSI protocols can be categorized based on the network topology of nodes,
with the star, mesh, and ring topologies being the primary types, respectively.
Given that star and mesh topologies dominate current implementations, most
existing MPSI protocols are based on these two topologies. However,
star-topology MPSI protocols suffer from high leader node load, while mesh
topology protocols suffer from high communication complexity and overhead. In
this paper, we first propose a multi-point sequential oblivious pseudorandom
function (MP-SOPRF) in a multi-party setting. Based on MP-SOPRF, we then
develop an MPSI protocol with a ring topology, addressing the challenges of
communication and computational overhead in existing protocols. We prove that
our MPSI protocol is semi-honest secure under the Hamming correlation
robustness assumption. Our experiments demonstrate that our MPSI protocol
outperforms state-of-the-art protocols, achieving a reduction of 74.8% in
communication and a 6% to 287% improvement in computational efficiency.

</details>


### [352] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: The paper introduces Amatriciana, a novel method using Graph Neural Networks for detecting money launderers in transaction graphs with temporal information.


<details>
  <summary>Details</summary>
Motivation: Money laundering is a serious financial crime that threatens financial integrity and social security. With the increasing number of transactions, automatic tools are needed to assist law enforcement agencies in detecting such criminal activities.

Method: Amatriciana uses Graph Neural Networks to analyze the entire transaction graph without dividing it into time-based subgraphs, thereby utilizing all relational information in the dataset. It incorporates temporal information to detect money launderers.

Result: Experiments on a public dataset show that the model can learn effectively from limited data and performs better when more data is available. It achieves an F1 score of 0.76 and reduces false positives by 55% compared to other state-of-the-art models.

Conclusion: Amatriciana is an effective approach for detecting money laundering within transaction graphs, significantly reducing false positives while maintaining a high detection rate.

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


### [353] [PackHero: A Scalable Graph-based Approach for Efficient Packer Identification](https://arxiv.org/abs/2506.00659)
*Marco Di Gennaro,Mario D'Onghia,Mario Polino,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: PackHero is a new static approach for packer identification using Graph Matching Network and clustering to match and group Call Graphs, showing high effectiveness and scalability.


<details>
  <summary>Details</summary>
Motivation: Anti-analysis techniques like packing pose significant challenges in malware analysis, with current methods having limitations in flexibility, scalability, and adaptability.

Method: PackHero uses a Graph Matching Network and clustering to compare and group Call Graphs from programs packed with known packers.

Result: PackHero achieves a macro-average F1-score of 93.7% with 10 samples per packer, improving to 98.3% with 100 samples, outperforming other tools especially with Virtualization-based packers.

Conclusion: PackHero provides an effective, scalable solution for packer identification that matches or exceeds the performance of current State-of-the-art tools.

Abstract: Anti-analysis techniques, particularly packing, challenge malware analysts,
making packer identification fundamental. Existing packer identifiers have
significant limitations: signature-based methods lack flexibility and struggle
against dynamic evasion, while Machine Learning approaches require extensive
training data, limiting scalability and adaptability. Consequently, achieving
accurate and adaptable packer identification remains an open problem. This
paper presents PackHero, a scalable and efficient methodology for identifying
packers using a novel static approach. PackHero employs a Graph Matching
Network and clustering to match and group Call Graphs from programs packed with
known packers. We evaluate our approach on a public dataset of malware and
benign samples packed with various packers, demonstrating its effectiveness and
scalability across varying sample sizes. PackHero achieves a macro-average
F1-score of 93.7% with just 10 samples per packer, improving to 98.3% with 100
samples. Notably, PackHero requires fewer samples to achieve stable performance
compared to other Machine Learning-based tools. Overall, PackHero matches the
performance of State-of-the-art signature-based tools, outperforming them in
handling Virtualization-based packers such as Themida/Winlicense, with a recall
of 100%.

</details>


### [354] [Review of Blockchain-Based Approaches to Spent Fuel Management in Nuclear Power Plants](https://arxiv.org/abs/2506.00677)
*Yuxiang Xu,Wenjuan Yu,Yuqian Wan,Zhongming Zhang*

Main category: cs.CR

TL;DR: This study proposes a blockchain-IoT prototype system for spent nuclear fuel transportation, which enhances safety, transparency, and efficiency while balancing confidentiality and regulatory requirements.


<details>
  <summary>Details</summary>
Motivation: Critical challenges in managing spent nuclear fuel transportation include inadequate data transparency, stringent confidentiality requirements, and lack of trust among collaborating parties in traditional centralized management systems.

Method: A prototype system integrating blockchain technology and IoT is proposed, featuring a multi-tiered consortium chain architecture with IoT sensors for real-time data collection and immutable blockchain recording. A hierarchical data structure manages access for diverse stakeholders.

Result: The approach significantly enhances data immutability, enables real-time multi-sensor data integration, improves decentralized transparency, and increases resilience compared to traditional systems.

Conclusion: This blockchain-IoT framework effectively resolves the conflict between confidentiality and transparency in nuclear data management, improving the safety, transparency, and efficiency of spent fuel transportation.

Abstract: This study addresses critical challenges in managing the transportation of
spent nuclear fuel, including inadequate data transparency, stringent
confidentiality requirements, and a lack of trust among collaborating parties,
issues prevalent in traditional centralized management systems. Given the high
risks involved, balancing data confidentiality with regulatory transparency is
imperative. To overcome these limitations, a prototype system integrating
blockchain technology and the Internet of Things (IoT) is proposed, featuring a
multi-tiered consortium chain architecture. This system utilizes IoT sensors
for real-time data collection, which is immutably recorded on the blockchain,
while a hierarchical data structure (operational, supervisory, and public
layers) manages access for diverse stakeholders. The results demonstrate that
this approach significantly enhances data immutability, enables real-time
multi-sensor data integration, improves decentralized transparency, and
increases resilience compared to traditional systems. Ultimately, this
blockchain-IoT framework improves the safety, transparency, and efficiency of
spent fuel transportation, effectively resolving the conflict between
confidentiality and transparency in nuclear data management and offering
significant practical implications.

</details>


### [355] [Browser Fingerprinting Using WebAssembly](https://arxiv.org/abs/2506.00719)
*Mordechai Guri,Dor Fibert*

Main category: cs.CR

TL;DR: The paper introduces an advanced fingerprinting method using WebAssembly that leverages computational capabilities to identify returning devices across different browsing sessions with a false-positive rate of less than 1%, and proposes mitigation strategies to reduce privacy risks.


<details>
  <summary>Details</summary>
Motivation: To develop more effective fingerprinting methods using WebAssembly's broad support across major browsers and growing adoption, while addressing the privacy concerns associated with persistent tracking and detailed user profiling.

Method: Leveraging WebAssembly's computational capabilities to identify returning devices by using subtle differences in the WebAssembly JavaScript API implementation to distinguish between Chromium-based browsers, even when identifiers are spoofed. The fingerprint is generated using a combination of CPU-bound operations, memory tasks, and I/O activities.

Result: The approach achieves a false-positive rate of less than 1% and significantly improves identification accuracy when validated on a variety of platforms and operating systems.

Conclusion: WebAssembly-based fingerprinting significantly enhances identification accuracy but raises privacy concerns. Mitigation strategies are proposed to be integrated into future browser designs to better protect user privacy.

Abstract: Web client fingerprinting has become a widely used technique for uniquely
identifying users, browsers, operating systems, and devices with high accuracy.
While it is beneficial for applications such as fraud detection and
personalized experiences, it also raises privacy concerns by enabling
persistent tracking and detailed user profiling. This paper introduces an
advanced fingerprinting method using WebAssembly (Wasm) - a low-level
programming language that offers near-native execution speed in modern web
browsers. With broad support across major browsers and growing adoption,
WebAssembly provides a strong foundation for developing more effective
fingerprinting methods.
  In this work, we present a new approach that leverages WebAssembly's
computational capabilities to identify returning devices-such as smartphones,
tablets, laptops, and desktops across different browsing sessions. Our method
uses subtle differences in the WebAssembly JavaScript API implementation to
distinguish between Chromium-based browsers like Google Chrome and Microsoft
Edge, even when identifiers such as the User-Agent are completely spoofed,
achieving a false-positive rate of less than 1%. The fingerprint is generated
using a combination of CPU-bound operations, memory tasks, and I/O activities
to capture unique browser behaviors. We validate this approach on a variety of
platforms, including Intel, AMD, and ARM CPUs, operating systems such as
Windows, macOS, Android, and iOS, and in environments like VMWare, KVM, and
VirtualBox. Extensive evaluation shows that WebAssembly-based fingerprinting
significantly improves identification accuracy. We also propose mitigation
strategies to reduce the privacy risks associated with this method, which could
be integrated into future browser designs to better protect user privacy.

</details>


### [356] [Assessing and Enhancing Quantum Readiness in Mobile Apps](https://arxiv.org/abs/2506.00790)
*Joseph Strauss,Krishna Upadhyay,A. B. Siddique,Ibrahim Baggili,Umar Farooq*

Main category: cs.CR

TL;DR: Quantum computers threaten current cryptographic methods like RSA, DSA, and ECC. Although NIST has released PQC standards, mobile app ecosystems are unprepared. Analysis of 4,000 Android apps reveals heavy reliance on quantum-vulnerable algorithms with no PQC adoption. LLM-assisted migration shows promise for simple hash replacements but fails at complex PQC upgrades, highlighting the need for better tools and guidance.


<details>
  <summary>Details</summary>
Motivation: To assess the cryptographic readiness of mobile app ecosystems for the transition to post-quantum cryptography and explore the potential of LLM-assisted migration.

Method: Performed a large-scale binary analysis of over 4,000 Android apps to evaluate cryptographic algorithm usage. Evaluated leading LLMs for their ability to perform automated cryptographic migration from pre-quantum to post-quantum algorithms.

Result: Found widespread use of quantum-vulnerable algorithms in Android apps with no adoption of PQC. LLMs succeeded in simple hash replacements but failed in complex PQC upgrades due to multi-file changes and lack of context awareness.

Conclusion: There is a significant gap in cryptographic readiness for post-quantum migration in mobile app ecosystems. Structured guidance and system-aware tooling are needed to facilitate this transition.

Abstract: Quantum computers threaten widely deployed cryptographic primitives such as
RSA, DSA, and ECC. While NIST has released post-quantum cryptographic (PQC)
standards (e.g., Kyber, Dilithium), mobile app ecosystems remain largely
unprepared for this transition. We present a large-scale binary analysis of
over 4,000 Android apps to assess cryptographic readiness. Our results show
widespread reliance on quantum-vulnerable algorithms such as MD5, SHA-1, and
RSA, while PQC adoption remains absent in production apps. To bridge the
readiness gap, we explore LLM-assisted migration. We evaluate leading LLMs
(GPT-4o, Gemini Flash, Claude Sonnet, etc.) for automated cryptographic
migration. All models successfully performed simple hash replacements (e.g.,
SHA-1 to SHA-256). However, none produced correct PQC upgrades due to
multi-file changes, missing imports, and lack of context awareness. These
results underscore the need for structured guidance and system-aware tooling
for post-quantum migration

</details>


### [357] [SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models](https://arxiv.org/abs/2506.00821)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CR

TL;DR: Genomic Foundation Models (GFMs) have been successful in variant effect prediction, but their adversarial robustness is largely unexplored. This paper proposes SafeGenes, a framework for evaluating GFM's robustness against adversarial attacks. Using FGSM and soft prompt attack methods, it was found that even large models like ESM1b and ESM1v are vulnerable to adversarial manipulation.


<details>
  <summary>Details</summary>
Motivation: To explore the adversarial robustness of Genomic Foundation Models (GFMs), which has not been extensively studied despite their success in variant effect prediction.

Method: The study introduces SafeGenes, a framework that uses adversarial attacks (Fast Gradient Sign Method and soft prompt attacks) to evaluate the robustness of GFMs against both engineered near-identical adversarial genes and embedding-space manipulations.

Result: Soft prompt attacks were particularly effective, causing significant performance degradation in large models such as ESM1b and ESM1v.

Conclusion: Current GFMs exhibit critical vulnerabilities to adversarial manipulation, necessitating further research to enhance their security and robustness, especially in high-stakes genomic applications.

Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),
have demonstrated significant success in variant effect prediction. However,
their adversarial robustness remains largely unexplored. To address this gap,
we propose SafeGenes: a framework for Secure analysis of genomic foundation
models, leveraging adversarial attacks to evaluate robustness against both
engineered near-identical adversarial Genes and embedding-space manipulations.
In this study, we assess the adversarial vulnerabilities of GFMs using two
approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM
introduces minimal perturbations to input sequences, while the soft prompt
attack optimizes continuous embeddings to manipulate model predictions without
modifying the input tokens. By combining these techniques, SafeGenes provides a
comprehensive assessment of GFM susceptibility to adversarial manipulation.
Targeted soft prompt attacks led to substantial performance degradation, even
in large models such as ESM1b and ESM1v. These findings expose critical
vulnerabilities in current foundation models, opening new research directions
toward improving their security and robustness in high-stakes genomic
applications such as variant effect prediction.

</details>


### [358] [A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems](https://arxiv.org/abs/2506.00831)
*M Sabbir Salek,Mashrur Chowdhury,Muhaimin Bin Munir,Yuchen Cai,Mohammad Imtiaz Hasan,Jean-Michel Tine,Latifur Khan,Mizanur Rahman*

Main category: cs.CR

TL;DR: Modern transportation systems, being cyber-physical systems (CPS), face increasing cyber vulnerabilities with growing automation and connectivity. Existing threat modeling frameworks have limitations. This paper introduces TraCR-TMF, a large language model (LLM)-based framework that reduces the need for cybersecurity expertise to identify threats, attack techniques, and countermeasures leveraging the MITRE ATT&CK matrix through three LLM-based approaches.


<details>
  <summary>Details</summary>
Motivation: Modern transportation systems are becoming more automated and connected, which increases exposure to cyber vulnerabilities. Current threat modeling frameworks for transportation CPS are limited in scope, resource-intensive, and require significant cybersecurity expertise.

Method: TraCR-TMF uses three LLM-based approaches to minimize expert intervention: retrieval-augmented generation (RAG) requiring no expert input, an in-context learning approach requiring low expert input, and supervised fine-tuning requiring moderate expert input. It also maps attack paths to critical assets by analyzing vulnerabilities using a customized LLM.

Result: In two evaluation scenarios, TraCR-TMF identified relevant attack techniques across transportation CPS applications with 90% precision as validated by experts. It successfully predicted multiple exploitations including lateral movement, data exfiltration, and ransomware-related encryption during a major real-world cyberattack incident.

Conclusion: TraCR-TMF proves effective in CPS threat modeling, reduces reliance on cybersecurity expertise, and is adaptable across CPS domains.

Abstract: Modern transportation systems rely on cyber-physical systems (CPS), where
cyber systems interact seamlessly with physical systems like
transportation-related sensors and actuators to enhance safety, mobility, and
energy efficiency. However, growing automation and connectivity increase
exposure to cyber vulnerabilities. Existing threat modeling frameworks for
transportation CPS are often limited in scope, resource-intensive, and
dependent on significant cybersecurity expertise. To address these gaps, we
present TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling
Framework), a large language model (LLM)-based framework that minimizes expert
intervention. TraCR-TMF identifies threats, potential attack techniques, and
corresponding countermeasures by leveraging the MITRE ATT&CK matrix through
three LLM-based approaches: (i) a retrieval-augmented generation (RAG) method
requiring no expert input, (ii) an in-context learning approach requiring low
expert input, and (iii) a supervised fine-tuning method requiring moderate
expert input. TraCR-TMF also maps attack paths to critical assets by analyzing
vulnerabilities using a customized LLM. The framework was evaluated in two
scenarios. First, it identified relevant attack techniques across
transportation CPS applications, with 90% precision as validated by experts.
Second, using a fine-tuned LLM, it successfully predicted multiple
exploitations including lateral movement, data exfiltration, and
ransomware-related encryption that occurred during a major real-world
cyberattack incident. These results demonstrate TraCR-TMF's effectiveness in
CPS threat modeling, its reduced reliance on cybersecurity expertise, and its
adaptability across CPS domains.

</details>


### [359] [ARIANNA: An Automatic Design Flow for Fabric Customization and eFPGA Redaction](https://arxiv.org/abs/2506.00857)
*Luca Collini,Jitendra Bhandari,Chiara Muscari Tomajoli,Abdul Khader Thalakkattu Moosa,Benjamin Tan,Xifan Tang,Pierre-Emmanuel Gaillardon,Ramesh Karri,Christian Pilato*

Main category: cs.CR

TL;DR: In the global IC supply chain, protecting IP is a complex challenge. The paper proposes ARIANNA, a complete flow that helps designers hide selected design functionality and customize redaction fabrics, which yields up to 3.3x lower overheads and 4x higher eFPGA fabric utilization than previous works.


<details>
  <summary>Details</summary>
Motivation: Protecting intellectual property in the global IC supply chain is difficult, and there is a need for a solution that can effectively balance IP loss risk and added cost for theft countermeasures.

Method: ARIANNA is proposed as a complete flow that assists designers in selecting logic to be hidden and tailoring bespoke fabrics for configurable embedded logic used to hide it. Two heuristics for the novel bespoke fabric flow are introduced and evaluated against an exhaustive approach.

Result: Results show that using ARIANNA to customize redaction fabrics yields up to 3.3x lower overheads and 4x higher eFPGA fabric utilization compared to a one-fits-all fabric proposed in prior works.

Conclusion: ARIANNA provides a significant improvement in terms of overhead reduction and eFPGA fabric utilization, making it a valuable tool for designers looking to protect IP in the IC supply chain.

Abstract: In the modern global Integrated Circuit (IC) supply chain, protecting
intellectual property (IP) is a complex challenge, and balancing IP loss risk
and added cost for theft countermeasures is hard to achieve. Using embedded
configurable logic allows designers to completely hide the functionality of
selected design portions from parties that do not have access to the
configuration string (bitstream). However, the design space of redacted
solutions is huge, with trade-offs between the portions selected for redaction
and the configuration of the configurable embedded logic. We propose ARIANNA, a
complete flow that aids the designer in all the stages, from selecting the
logic to be hidden to tailoring the bespoke fabrics for the configurable logic
used to hide it. We present a security evaluation of the considered fabrics and
introduce two heuristics for the novel bespoke fabric flow. We evaluate the
heuristics against an exhaustive approach. We also evaluate the complete flow
using a selection of benchmarks. Results show that using ARIANNA to customize
the redaction fabrics yields up to 3.3x lower overheads and 4x higher eFPGA
fabric utilization than a one-fits-all fabric as proposed in prior works.

</details>


### [360] [Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack](https://arxiv.org/abs/2506.01011)
*Siqi Hui,Yiren Song,Sanping Zhou,Ye Deng,Wenli Huang,Jinjun Wang*

Main category: cs.CR

TL;DR: The paper introduces Lexical Bias Watermarking (LBW), a new watermarking method for autoregressive image generation models. It embeds watermarks into token maps by biasing token selection, resists regeneration attacks, and uses a pool of green lists for increased security.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking techniques are mainly designed for diffusion models and face challenges adapting to AR models that generate images sequentially through token prediction. Additionally, these techniques are vulnerable to diffusion-based regeneration attacks which can erase watermarks.

Method: Propose LBW, embedding watermarks directly into token maps by biasing token selection toward a predefined green list during generation. The green list is randomly sampled from a pool of green lists per image for enhanced security. Detection is achieved via quantization and statistical analysis of the token distribution.

Result: Extensive experiments show that LBW achieves superior watermark robustness, particularly effective in resisting regeneration attacks.

Conclusion: LBW provides a novel framework for watermarking AR models, ensuring robustness against regeneration attacks and offering seamless integration with existing AR models.

Abstract: Autoregressive (AR) image generation models have gained increasing attention
for their breakthroughs in synthesis quality, highlighting the need for robust
watermarking to prevent misuse. However, existing in-generation watermarking
techniques are primarily designed for diffusion models, where watermarks are
embedded within diffusion latent states. This design poses significant
challenges for direct adaptation to AR models, which generate images
sequentially through token prediction. Moreover, diffusion-based regeneration
attacks can effectively erase such watermarks by perturbing diffusion latent
states. To address these challenges, we propose Lexical Bias Watermarking
(LBW), a novel framework designed for AR models that resists regeneration
attacks. LBW embeds watermarks directly into token maps by biasing token
selection toward a predefined green list during generation. This approach
ensures seamless integration with existing AR models and extends naturally to
post-hoc watermarking. To increase the security against white-box attacks,
instead of using a single green list, the green list for each image is randomly
sampled from a pool of green lists. Watermark detection is performed via
quantization and statistical analysis of the token distribution. Extensive
experiments demonstrate that LBW achieves superior watermark robustness,
particularly in resisting regeneration attacks.

</details>


### [361] [Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution](https://arxiv.org/abs/2506.01055)
*Meysam Alizadeh,Zeynab Samei,Daria Stetsenko,Fabrizio Gilardi*

Main category: cs.CR

TL;DR: 该研究通过构建虚构的银行代理和改进的AgentDojo基准测试，揭示了大型语言模型在遭受提示注入攻击时可能导致个人数据泄露的问题。虽然部分防御措施有效，但涉及数据提取或授权流程的任务仍显示出较高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的关于大型语言模型提示注入的研究主要集中在通用任务和攻击上，对复杂威胁（如数据外泄）的关注有限。因此，本文旨在深入探讨提示注入如何导致工具调用代理泄露个人数据。

Method: 研究使用虚构的银行代理开发基于数据流的攻击，并将其整合到AgentDojo基准测试中。同时创建了一个更丰富的合成数据集，包含人类与AI的银行对话。通过在16个用户任务中评估攻击效果并分析48个扩展任务的表现，探讨不同任务类型、代理性能和防御措施的有效性。

Result: 在16个任务中，大型语言模型在攻击下的效用下降了15-50个百分点，平均攻击成功率为20%左右；一些防御措施将攻击成功率降至零。尽管大多数模型不会泄露密码等高度敏感数据，但仍可能泄露其他个人信息。在扩展的48个任务中，平均攻击成功率为15%，且没有内置防御完全阻止泄露。涉及数据提取或授权流程的任务攻击成功率最高。

Conclusion: 提示注入攻击对工具调用代理构成显著威胁，尤其是涉及数据提取或授权的工作流程。尽管部分防御措施有效，但模型仍存在泄露非高度敏感个人信息的风险，强调了进一步改进安全性和防御机制的重要性。

Abstract: Previous benchmarks on prompt injection in large language models (LLMs) have
primarily focused on generic tasks and attacks, offering limited insights into
more complex threats like data exfiltration. This paper examines how prompt
injection can cause tool-calling agents to leak personal data observed during
task execution. Using a fictitious banking agent, we develop data flow-based
attacks and integrate them into AgentDojo, a recent benchmark for agentic
security. To enhance its scope, we also create a richer synthetic dataset of
human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a
15-50 percentage point drop in utility under attack, with average attack
success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most
LLMs, even when successfully tricked by the attack, avoid leaking highly
sensitive data like passwords, likely due to safety alignments, but they remain
vulnerable to disclosing other personal data. The likelihood of password
leakage increases when a password is requested along with one or two additional
personal details. In an extended evaluation across 48 tasks, the average ASR is
around 15 percent, with no built-in AgentDojo defense fully preventing leakage.
Tasks involving data extraction or authorization workflows, which closely
resemble the structure of exfiltration attacks, exhibit the highest ASRs,
highlighting the interaction between task type, agent performance, and defense
efficacy.

</details>


### [362] [IDCloak: A Practical Secure Multi-party Dataset Join Framework for Vertical Privacy-preserving Machine Learning](https://arxiv.org/abs/2506.01072)
*Shuyu Chen,Guopeng Lin,Haoyu Niu,Lushan Song,Chengxun Hong,Weili Han*

Main category: cs.CR

TL;DR: The paper presents IDCloak, a novel framework for secure multi-party dataset join in vertical privacy-preserving machine learning (vPPML). It consists of two protocols: cmPSI and a secure feature alignment protocol. IDCloak maintains the privacy of IDs without relying on an auxiliary server and is efficient even as the number of parties increases.


<details>
  <summary>Details</summary>
Motivation: Existing methods for secure dataset join in vPPML are impractical due to security issues when exposing intersection IDs, reliance on a non-colluding auxiliary server, or being limited to two-party settings.

Method: IDCloak uses two protocols: 1) cmPSI - a circuit-based multi-party private set intersection protocol that obtains secret-shared flags indicating intersection IDs through an optimized communication structure combining OKVS and OPRF; 2) A secure multi-party feature alignment protocol that gets the secret-shared and joint dataset using secret-shared flags via an efficient secure shuffle protocol.

Result: Experiments show IDCloak is more efficient than iPrivjoin in two-party settings and comparable as the number of parties grows. The proposed cmPSI protocol offers stronger security guarantees while improving efficiency up to 7.78x in time and 8.73x in communication sizes compared to existing cmPSI protocols. The secure shuffle protocol outperforms state-of-the-art by up to 138.34x in time and 132.13x in communication sizes.

Conclusion: IDCloak is the first practical secure multi-party dataset join framework for vPPML that keeps IDs private without a non-colluding auxiliary server, demonstrating high efficiency and strong security guarantees.

Abstract: Vertical privacy-preserving machine learning (vPPML) enables multiple parties
to train models on their vertically distributed datasets while keeping datasets
private. In vPPML, it is critical to perform the secure dataset join, which
aligns features corresponding to intersection IDs across datasets and forms a
secret-shared and joint training dataset. However, existing methods for this
step could be impractical due to: (1) they are insecure when they expose
intersection IDs; or (2) they rely on a strong trust assumption requiring a
non-colluding auxiliary server; or (3) they are limited to the two-party
setting.
  This paper proposes IDCloak, the first practical secure multi-party dataset
join framework for vPPML that keeps IDs private without a non-colluding
auxiliary server. IDCloak consists of two protocols: (1) a circuit-based
multi-party private set intersection protocol (cmPSI), which obtains
secret-shared flags indicating intersection IDs via an optimized communication
structure combining OKVS and OPRF; (2) a secure multi-party feature alignment
protocol, which obtains the secret-shared and joint dataset using secret-shared
flags, via our proposed efficient secure shuffle protocol. Experiments show
that: (1) compared to the state-of-the-art secure two-party dataset join
framework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party
setting and comparable performance when the party number increases; (2)
compared to the state-of-the-art cmPSI protocol under honest majority, our
proposed cmPSI protocol provides a stronger security guarantee (dishonest
majority) while improving efficiency by up to $7.78\times$ in time and
$8.73\times$ in communication sizes; (3) our proposed secure shuffle protocol
outperforms the state-of-the-art shuffle protocol by up to $138.34\times$ in
time and $132.13\times$ in communication sizes.

</details>


### [363] [Vulnerability Management Chaining: An Integrated Framework for Efficient Cybersecurity Risk Prioritization](https://arxiv.org/abs/2506.01220)
*Naoyuki Shimizu,Masaki Hashimoto*

Main category: cs.CR

TL;DR: Cybersecurity teams struggle with vulnerability management due to the high volume of new CVEs annually. This paper proposes Vulnerability Management Chaining, a decision tree framework integrating KEV, EPSS, and CVSS for better prioritization. Experiments show significant efficiency improvements and threat coverage, reducing workload while identifying more exploited vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in traditional CVSS-based prioritization which requires addressing a large percentage of all vulnerabilities but often fails to correctly identify those that are actually exploited.

Method: Proposes Vulnerability Management Chaining, an integrated decision tree framework combining historical exploitation evidence (KEV), predictive threat modeling (EPSS), and technical impact assessment (CVSS).

Result: Demonstrates 14-18 fold efficiency improvements while maintaining 85%+ coverage of actual threats. Reduces urgent remediation workload by 95%. Identifies 57 additional exploited vulnerabilities not captured by KEV or EPSS alone.

Conclusion: Establishes the first empirically validated methodology for systematic vulnerability management integration using exclusively open-source data, making advanced vulnerability management accessible regardless of budget or expertise.

Abstract: Cybersecurity teams face an overwhelming vulnerability crisis: with 25,000+
new CVEs disclosed annually, traditional CVSS-based prioritization requires
addressing 60% of all vulnerabilities while correctly identifying only 20% of
those actually exploited. We propose Vulnerability Management Chaining, an
integrated decision tree framework combining historical exploitation evidence
(KEV), predictive threat modeling (EPSS), and technical impact assessment
(CVSS) to transform vulnerability management from reactive patching to
strategic threat-driven prioritization. Experimental validation using 28,377
real-world vulnerabilities demonstrates 14-18 fold efficiency improvements
while maintaining 85%+ coverage of actual threats. Organizations can reduce
urgent remediation workload by 95% (from ~16,000 to ~850 vulnerabilities). The
integration identifies 57 additional exploited vulnerabilities that neither KEV
nor EPSS captures individually. Our framework uses exclusively open-source
data, democratizing advanced vulnerability management regardless of budget or
expertise. This research establishes the first empirically validated
methodology for systematic vulnerability management integration, with immediate
applicability across diverse organizational contexts.

</details>


### [364] [SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs](https://arxiv.org/abs/2506.01227)
*Rakesh Podder,Turgay Caglar,Shadaab Kawnain Bashir,Sarath Sreedharan,Indrajit Ray,Indrakshi Ray*

Main category: cs.CR

TL;DR: SPEAR is a formal framework that uses AI planning causal formalism to model vulnerabilities and configurations in networked systems, providing diverse security hardening strategies for administrators.


<details>
  <summary>Details</summary>
Motivation: Graph-based frameworks used in network hardening lack the ability to incorporate network connectivity parameters into the attack graph, reason about the graph without complete information, provide administrator suggestions in an understandable format, and allow what-if analysis on various scenarios and attacker motives.

Method: SPEAR models vulnerabilities and configurations in a networked system using the causal formalism of AI planning. It converts network configurations and vulnerability descriptions into planning models expressed in PDDL, identifying diverse security hardening strategies.

Result: SPEAR presents security hardening strategies that are understandable to domain experts, enabling administrators to explore the network hardening solution space systematically and evaluate/compare different solutions.

Conclusion: SPEAR provides a tool-supported framework for security posture evaluation and analysis, filling gaps in current graph-based network hardening approaches by keeping the human-in-the-loop.

Abstract: Graph-based frameworks are often used in network hardening to help a cyber
defender understand how a network can be attacked and how the best defenses can
be deployed. However, incorporating network connectivity parameters in the
attack graph, reasoning about the attack graph when we do not have access to
complete information, providing system administrator suggestions in an
understandable format, and allowing them to do what-if analysis on various
scenarios and attacker motives is still missing. We fill this gap by presenting
SPEAR, a formal framework with tool support for security posture evaluation and
analysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI
planning to model vulnerabilities and configurations in a networked system. It
automatically converts network configurations and vulnerability descriptions
into planning models expressed in the Planning Domain Definition Language
(PDDL). SPEAR identifies a set of diverse security hardening strategies that
can be presented in a manner understandable to the domain expert. These allow
the administrator to explore the network hardening solution space in a
systematic fashion and help evaluate the impact and compare the different
solutions.

</details>


### [365] [Comprehensive Vulnerability Analysis is Necessary for Trustworthy LLM-MAS](https://arxiv.org/abs/2506.01245)
*Pengfei He,Yue Xing,Shen Dong,Juanhui Li,Zhenwei Dai,Xianfeng Tang,Hui Liu,Han Xu,Zhen Xiang,Charu C. Aggarwal,Hui Liu*

Main category: cs.CR

TL;DR: 这篇论文强调了对基于大型语言模型的多智能体系统（LLM-MAS）进行全面漏洞分析的重要性，并提出一个系统性的框架来统合不同研究，定义威胁模型并揭示组合效应带来的额外风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM-MAS在高风险应用中的部署增加，其复杂结构带来了新的安全威胁。然而，目前对于这些系统的独特攻击面（如智能体间通信、信任关系和工具集成）的研究仍十分有限。

Method: 论文提出了一种系统性框架，用于分析LLM-MAS的漏洞。该框架为每种类型的漏洞定义了基于实际攻击者能力的形式化威胁模型，并通过现实世界的LLM-MAS应用进行说明。此外，还探讨了组合效应如何使单个组件的漏洞在智能体通信中级联放大。

Result: 研究表明，LLM-MAS由于组合效应面临更高的风险，且当前缺乏针对这些系统的专门评估基准和信任管理系统。

Conclusion: 作者指出了未来研究的关键方向：开发适合LLM-MAS漏洞评估的基准、考虑特定于多智能体架构的新攻击方式，以及实施能够保障LLM-MAS安全的信任管理系统。

Abstract: This paper argues that a comprehensive vulnerability analysis is essential
for building trustworthy Large Language Model-based Multi-Agent Systems
(LLM-MAS). These systems, which consist of multiple LLM-powered agents working
collaboratively, are increasingly deployed in high-stakes applications but face
novel security threats due to their complex structures. While single-agent
vulnerabilities are well-studied, LLM-MAS introduces unique attack surfaces
through inter-agent communication, trust relationships, and tool integration
that remain significantly underexplored. We present a systematic framework for
vulnerability analysis of LLM-MAS that unifies diverse research. For each type
of vulnerability, we define formal threat models grounded in practical attacker
capabilities and illustrate them using real-world LLM-MAS applications. This
formulation enables rigorous quantification of vulnerability across different
architectures and provides a foundation for designing meaningful evaluation
benchmarks. Our analysis reveals that LLM-MAS faces elevated risk due to
compositional effects -- vulnerabilities in individual components can cascade
through agent communication, creating threat models not present in single-agent
systems. We conclude by identifying critical open challenges: (1) developing
benchmarks specifically tailored to LLM-MAS vulnerability assessment, (2)
considering new potential attacks specific to multi-agent architectures, and
(3) implementing trust management systems that can enforce security in LLM-MAS.
This research provides essential groundwork for future efforts to enhance
LLM-MAS trustworthiness as these systems continue their expansion into critical
applications.

</details>


### [366] [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models](https://arxiv.org/abs/2506.01307)
*Youze Wang,Wenbo Hu,Yinpeng Dong,Jing Liu,Hanwang Zhang,Richang Hong*

Main category: cs.CR

TL;DR: Multimodal Large Language Models (MLLMs) integrate various data types, yet suffer from jailbreak attacks. This paper proposes a multimodal universal jailbreak attack framework that generates high-quality undesirable outputs, revealing significant safety issues in MLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the unique security risks posed by integrating different modalities in MLLMs and to highlight vulnerabilities exposed by text-based jailbreak attacks.

Method: A unified multimodal universal jailbreak attack framework leveraging iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image.

Result: Validates that multimodal universal jailbreak attacks can lead to higher-quality undesirable generations across different MLLMs, revealing significant multimodal safety alignment issues.

Conclusion: There is an urgent need for robust safety measures in MLLMs to mitigate risks associated with their multimodal capabilities.

Abstract: Large Language Models (LLMs) have evolved into Multimodal Large Language
Models (MLLMs), significantly enhancing their capabilities by integrating
visual information and other types, thus aligning more closely with the nature
of human intelligence, which processes a variety of data forms beyond just
text. Despite advancements, the undesirable generation of these models remains
a critical concern, particularly due to vulnerabilities exposed by text-based
jailbreak attacks, which have represented a significant threat by challenging
existing safety protocols. Motivated by the unique security risks posed by the
integration of new and old modalities for MLLMs, we propose a unified
multimodal universal jailbreak attack framework that leverages iterative
image-text interactions and transfer-based strategy to generate a universal
adversarial suffix and image. Our work not only highlights the interaction of
image-text modalities can be used as a critical vulnerability but also
validates that multimodal universal jailbreak attacks can bring higher-quality
undesirable generations across different MLLMs. We evaluate the undesirable
context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and
InstructBLIP, and reveal significant multimodal safety alignment issues,
highlighting the inadequacy of current safety mechanisms against sophisticated
multimodal attacks. This study underscores the urgent need for robust safety
measures in MLLMs, advocating for a comprehensive review and enhancement of
security protocols to mitigate potential risks associated with multimodal
capabilities.

</details>


### [367] [Understanding the Identity-Transformation Approach in OIDC-Compatible Privacy-Preserving SSO Services](https://arxiv.org/abs/2506.01325)
*Jingqiang Lin,Baitao Zhang,Wei Wang,Quanwei Cai,Jiwu Jing,Huiyang He*

Main category: cs.CR

TL;DR: This paper explores the relationship between identity transformations in OIDC-compatible SSO services and OPRFs, providing new insights and constructions for privacy-preserving SSO.


<details>
  <summary>Details</summary>
Motivation: Identity transformations in UppreSSO provide OIDC-compatible SSO services that prevent IdP-based login tracing and RP-based identity linkage. However, there are essential issues of this approach that need further investigation.

Method: The authors explain suggestions for integrating identity transformations in OIDC-compatible SSO, uncover the relationship with OPRFs, analyze existing OPRF protocols, and construct new identity transformations based on OPRFs.

Result: New identity transformations were constructed based on OPRFs that satisfy different variations of SSO security requirements.

Conclusion: This is the first time the relationship between identity transformations in OIDC-compatible privacy-preserving SSO services and OPRFs has been uncovered and the SSO-related properties of OPRF protocols have been proved.

Abstract: OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers
to log into multiple websites, called relying parties (RPs), by her username
and credential set up in another trusted web system, called the identity
provider (IdP). Identity transformations are proposed in UppreSSO to provide
OIDC-compatible SSO services, preventing both IdP-based login tracing and
RP-based identity linkage. While security and privacy of SSO services in
UppreSSO have been proved, several essential issues of this
identity-transformation approach are not well studied. In this paper, we
comprehensively investigate the approach as below. Firstly, several suggestions
for the efficient integration of identity transformations in OIDC-compatible
SSO are explained. Then, we uncover the relationship between
identity-transformations in SSO and oblivious pseudo-random functions (OPRFs),
and present two variations of the properties required for SSO security as well
as the privacy requirements, to analyze existing OPRF protocols. Finally, new
identity transformations different from those designed in UppreSSO, are
constructed based on OPRFs, satisfying different variations of SSO security
requirements. To the best of our knowledge, this is the first time to uncover
the relationship between identity transformations in OIDC-compatible
privacy-preserving SSO services and OPRFs, and prove the SSO-related properties
(i.e., key-identifier freeness, RP designation and user identification) of OPRF
protocols, in addition to the basic properties of correctness, obliviousness
and pseudo-randomness.

</details>


### [368] [ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control](https://arxiv.org/abs/2506.01333)
*Manish Bhatt,Vineeth Sai Narajala,Idan Habler*

Main category: cs.CR

TL;DR: This paper addresses security vulnerabilities in the Model Context Protocol (MCP) used by Large Language Models (LLMs). It introduces Enhanced Tool Definition Interface (ETDI), a security extension that includes cryptographic identity verification, immutable versioned tool definitions, and explicit permission management. Additionally, it proposes fine-grained, policy-based access control to further secure interactions between LLMs and external tools.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance the security of the Model Context Protocol (MCP) which is crucial for integrating Large Language Models (LLMs) with external tools and data sources. Current MCP specifications have significant security vulnerabilities such as Tool Poisoning and Rug Pull attacks.

Method: The method involves introducing ETDI, which incorporates cryptographic identity verification, immutable versioned tool definitions, and explicit permission management often leveraging OAuth 2.0. Also, proposing extending MCP with fine-grained, policy-based access control using a dedicated policy engine.

Result: The result is a layered approach that aims to establish a more secure, trustworthy, and controllable ecosystem for AI applications interacting with LLMs and external tools.

Conclusion: By implementing ETDI and enhancing MCP with policy-based access control, the paper concludes that it can significantly improve the security and trustworthiness of interactions between LLMs and external tools.

Abstract: The Model Context Protocol (MCP) plays a crucial role in extending the
capabilities of Large Language Models (LLMs) by enabling integration with
external tools and data sources. However, the standard MCP specification
presents significant security vulnerabilities, notably Tool Poisoning and Rug
Pull attacks. This paper introduces the Enhanced Tool Definition Interface
(ETDI), a security extension designed to fortify MCP. ETDI incorporates
cryptographic identity verification, immutable versioned tool definitions, and
explicit permission management, often leveraging OAuth 2.0. We further propose
extending MCP with fine-grained, policy-based access control, where tool
capabilities are dynamically evaluated against explicit policies using a
dedicated policy engine, considering runtime context beyond static OAuth
scopes. This layered approach aims to establish a more secure, trustworthy, and
controllable ecosystem for AI applications interacting with LLMs and external
tools.

</details>


### [369] [Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems](https://arxiv.org/abs/2506.01384)
*Craig Steven Wright*

Main category: cs.CR

TL;DR: This paper provides a formal analysis of SPV clients versus full nodes, defining security and modeling transaction acceptance. It proves SPV clients are sufficient under honest-majority assumptions and less vulnerable to attacks than passive full nodes. Home-based full nodes increase systemic entropy without contributing to consensus integrity. SPV is shown to be the rational equilibrium strategy for non-mining participants.


<details>
  <summary>Details</summary>
Motivation: To provide a mathematically rigorous formal analysis of the security and functionality of SPV clients compared to home-based full nodes in the Bitcoin network.

Method: Formal analysis using axioms on behavioral divergence and communication topology, along with lemmas, propositions, and Monte Carlo simulations to model transaction acceptance, enforcement capability, and divergence probability under adversarial conditions.

Result: SPV clients are proven to be cryptographically sufficient under honest-majority assumptions and less vulnerable to attack compared to structurally passive, non-enforcing full nodes. Home-based full nodes increase systemic entropy without contributing to consensus integrity.

Conclusion: SPV clients represent the rational equilibrium strategy for non-mining participants and challenge the prevailing narrative that home validators enhance network security.

Abstract: This paper presents a mathematically rigorous formal analysis of Simplified
Payment Verification (SPV) clients, as specified in Section 8 of the original
Bitcoin white paper, versus non-mining full nodes operated by home users. It
defines security as resistance to divergence from global consensus and models
transaction acceptance, enforcement capability, and divergence probability
under adversarial conditions. The results demonstrate that SPV clients, despite
omitting script verification, are cryptographically sufficient under
honest-majority assumptions and topologically less vulnerable to attack than
structurally passive, non-enforcing full nodes. The paper introduces new axioms
on behavioral divergence and communication topology, proving that home-based
full nodes increase systemic entropy without contributing to consensus
integrity. Using a series of formally defined lemmas, propositions, and Monte
Carlo simulation results, it is shown that SPV clients represent the rational
equilibrium strategy for non-mining participants. This challenges the
prevailing narrative that home validators enhance network security, providing
formal and operational justifications for the sufficiency of SPV models.

</details>


### [370] [System Calls for Malware Detection and Classification: Methodologies and Applications](https://arxiv.org/abs/2506.01412)
*Bishwajit Prasad Gond,Durga Prasad Mohapatra*

Main category: cs.CR

TL;DR: 随着恶意软件变得越来越复杂和难以检测，恶意软件分析需要不断发展以保持领先地位。本章深入探讨了系统调用在恶意软件检测和分类中的应用，并介绍了静态和动态分析、沙盒等技术。通过结合机器学习、统计分析和异常检测等高级技术，研究者可以区分正常和恶意行为。此外，还讨论了这些技术在不同系统中的应用以及高级恶意软件的逃避检测策略。


<details>
  <summary>Details</summary>
Motivation: 恶意软件变得越来越复杂和难以检测，因此需要更先进的方法来进行恶意软件分析。系统调用和API调用是用户应用程序与操作系统及其内核之间的核心通信，能够提供关于软件或程序行为的宝贵见解，有助于发现可疑或有害活动。

Method: 使用系统调用和API调用进行恶意软件检测和分类，涵盖静态分析、动态分析和沙盒技术。结合机器学习、统计分析和异常检测等高级技术来分析系统调用模式。

Result: 能够区分正常和恶意行为，提高恶意软件检测和分类的准确性。同时，讨论了这些技术在Windows、Linux和Android等不同系统中的应用，以及高级恶意软件的逃避检测策略。

Conclusion: 系统调用和API调用是恶意软件检测和分类的重要工具，结合多种技术和方法可以有效提高检测效果，但需注意高级恶意软件的逃避策略。

Abstract: As malware continues to become more complex and harder to detect, Malware
Analysis needs to continue to evolve to stay one step ahead. One promising key
area approach focuses on using system calls and API Calls, the core
communication between user applications and the operating system and their
kernels. These calls provide valuable insight into how software or programs
behaves, making them an useful tool for spotting suspicious or harmful activity
of programs and software. This chapter takes a deep down look at how system
calls are used in malware detection and classification, covering techniques
like static and dynamic analysis, as well as sandboxing. By combining these
methods with advanced techniques like machine learning, statistical analysis,
and anomaly detection, researchers can analyze system call patterns to tell the
difference between normal and malicious behavior. The chapter also explores how
these techniques are applied across different systems, including Windows,
Linux, and Android, while also looking at the ways sophisticated malware tries
to evade detection.

</details>


### [371] [CSVAR: Enhancing Visual Privacy in Federated Learning via Adaptive Shuffling Against Overfitting](https://arxiv.org/abs/2506.01425)
*Zhuo Chen,Zhenya Ma,Yan Zhang,Donghua Cai,Ye Zhang,Qiushi Li,Yongheng Deng,Ye Guo,Ju Ren,Xuemin,Shen*

Main category: cs.CR

TL;DR: CSVAR is a novel image shuffling framework for federated learning that enhances visual privacy protection by generating obfuscated images and mitigating overfitting-induced privacy leaks.


<details>
  <summary>Details</summary>
Motivation: Federated learning preserves training data within local privacy domains, but aggregated model parameters may still reveal private characteristics. There is a need to address both overfitting-induced privacy leaks and raw image transmission risks.

Method: CSVAR adopts region-variance as a metric to measure visual privacy sensitivity across image regions. It adaptively partitions each region into multiple blocks, applying fine-grained partitioning to privacy-sensitive regions and coarse-grained partitioning to privacy-insensitive regions. Then, it shuffles blocks in both spatial domains and chromatic channels to hide visual spatial features and disrupt color distribution.

Result: Experimental evaluations on diverse real-world datasets show that CSVAR generates visually obfuscated images with high perceptual ambiguity, mitigates adversarial data reconstruction attacks, and achieves a good trade-off between visual privacy protection and model utility.

Conclusion: CSVAR presents an effective approach to enhance visual privacy protection in federated learning by addressing overfitting-induced privacy leaks and raw image transmission risks.

Abstract: Although federated learning preserves training data within local privacy
domains, the aggregated model parameters may still reveal private
characteristics. This vulnerability stems from clients' limited training data,
which predisposes models to overfitting. Such overfitting enables models to
memorize distinctive patterns from training samples, thereby amplifying the
success probability of privacy attacks like membership inference. To enhance
visual privacy protection in FL, we present CSVAR(Channel-Wise Spatial Image
Shuffling with Variance-Guided Adaptive Region Partitioning), a novel image
shuffling framework to generate obfuscated images for secure data transmission
and each training epoch, addressing both overfitting-induced privacy leaks and
raw image transmission risks. CSVAR adopts region-variance as the metric to
measure visual privacy sensitivity across image regions. Guided by this, CSVAR
adaptively partitions each region into multiple blocks, applying fine-grained
partitioning to privacy-sensitive regions with high region-variances for
enhancing visual privacy protection and coarse-grained partitioning to
privacy-insensitive regions for balancing model utility. In each region, CSVAR
then shuffles between blocks in both the spatial domains and chromatic channels
to hide visual spatial features and disrupt color distribution. Experimental
evaluations conducted on diverse real-world datasets demonstrate that CSVAR is
capable of generating visually obfuscated images that exhibit high perceptual
ambiguity to human eyes, simultaneously mitigating the effectiveness of
adversarial data reconstruction attacks and achieving a good trade-off between
visual privacy protection and model utility.

</details>


### [372] [Policy as Code, Policy as Type](https://arxiv.org/abs/2506.01446)
*Matthew D. Fuchs*

Main category: cs.CR

TL;DR: The paper explores expressing complex ABAC policies as types in dependently typed languages like Agda and Lean, offering superior safety compared to Rego, while discussing access control models and policy communication.


<details>
  <summary>Details</summary>
Motivation: To address the potential financial and reputational harm caused by badly typed actions, by leveraging dependently typed languages for expressing ABAC policies.

Method: Expressing ABAC policies as types in dependently typed languages such as Agda and Lean, comparing their safety features with the Rego policy language, and integrating future attributes that are distributed and signed.

Result: Demonstrates that dependently typed languages provide a single framework for expressing, analyzing, and implementing policies with superior safety due to their powerful type system and built-in proof assistant.

Conclusion: Dependently typed languages like Agda can effectively express complex ABAC policies as types, providing enhanced safety and a unified framework for policy management.

Abstract: Policies are designed to distinguish between correct and incorrect actions;
they are types. But badly typed actions may cause not compile errors, but
financial and reputational harm We demonstrate how even the most complex ABAC
policies can be expressed as types in dependently typed languages such as Agda
and Lean, providing a single framework to express, analyze, and implement
policies. We then go head-to-head with Rego, the popular and powerful
open-source ABAC policy language. We show the superior safety that comes with a
powerful type system and built-in proof assistant. In passing, we discuss
various access control models, sketch how to integrate in a future when
attributes are distributed and signed (as discussed at the W3C), and show how
policies can be communicated using just the syntax of the language. Our
examples are in Agda.

</details>


### [373] [First-Spammed, First-Served: MEV Extraction on Fast-Finality Blockchains](https://arxiv.org/abs/2506.01462)
*Krzysztof Gogol,Manvir Schneider,Claudio Tessone*

Main category: cs.CR

TL;DR: This research analyzes spam-based arbitrage strategies on fast-finality blockchains, finding that splitting profitable MEV opportunities into small transactions is optimal for CEX-DEX arbitrageurs. Empirical validation on Ethereum rollups shows 80% of reverted transactions are swaps, with 50% targeting USDC-WETH pools. Post-Dencun upgrade patterns indicate an intense latency race and reveal the fragility of fee-based ordering.


<details>
  <summary>Details</summary>
Motivation: To understand the economics and structure of spam-based arbitrage strategies on fast-finality blockchains, particularly focusing on CEX-DEX arbitrageurs and their transaction behaviors.

Method: Theoretical demonstration of optimal arbitrage strategies followed by empirical validation using execution graphs constructed from transaction traces to identify DEX/router interactions and targeted liquidity pools.

Result: 80% of reverted transactions are swaps, with 50% targeting USDC-WETH pools on Uniswap v3/v4. Reverted transactions rarely engage with Priority Fee Auctions, preferring duplicate submissions instead. Intense latency races occur on fast rollups like Arbitrum and ZKsync.

Conclusion: Spam-based arbitrage strategies become economically viable post-Dencun upgrade due to reduced L2 gas costs. The findings reveal the fragility of fee-based ordering under sub-second block times.

Abstract: This research analyzes the economics of spam-based arbitrage strategies on
fast-finality blockchains. We begin by theoretically demonstrating that,
splitting a profitable MEV opportunity into multiple small transactions is the
optimal strategy for CEX-DEX arbitrageurs. We then empirically validate these
findings on major Ethereum rollups. To uncover the structure of reverted
transactions, we construct execution graphs from transaction traces and
systematically search them to identify DEX or router interactions and targeted
liquidity pools. This analysis reveals that 80\% of reverted transactions are
swaps with approximately 50\% targeting USDC-WETH pools on Uniswap v3/v4. These
patterns intensified following the March 2024 Dencun upgrade, which lowered L2
gas costs and made spam-based arbitrage economically viable.
Counterintuitively, we find that these reverted MEV transactions rarely engage
with Priority Fee Auctions (PFAs), preferring to submit duplicate transactions
rather than bid for inclusion. Moreover, reverted transactions cluster at the
very top of blocks on fast rollups like Arbitrum and ZKsync, indicating an
intense latency race and revealing the fragility of fee-based ordering under
sub-second block times.

</details>


### [374] [Combining Different Existing Methods for Describing Steganography Hiding Methods](https://arxiv.org/abs/2506.01700)
*Steffen Wendzel,Christian Krätzer,Jana Dittmann,Luca Caviglione,Aleksandra Mileva,Tobias Schmidbauer,Claus Vielhauer,Sebastian Zander*

Main category: cs.CR

TL;DR: The paper introduces and explains the concept of descriptive methods for steganography, aiming to unify the description of steganography methods and network covert channels through tutorial-like organization and real-world examples.


<details>
  <summary>Details</summary>
Motivation: To address the overlap in concepts and terminology in steganography literature and aid in the unified description of steganography methods and network covert channels from a cybersecurity perspective.

Method: Providing an introduction to descriptive methods for steganography techniques in a tutorial format, combining existing descriptions and taxonomy objects for detailed categorization and description of hiding methods.

Result: Achieves a comprehensive categorization and description of hiding methods with practical real-world examples, effectively helping the research community.

Conclusion: Descriptive methods for steganography are challenging yet crucial; unifying these methods can significantly benefit cybersecurity research and risk mitigation.

Abstract: The proliferation of digital carriers that can be exploited to conceal
arbitrary data has greatly increased the number of techniques for implementing
network steganography. As a result, the literature overlaps greatly in terms of
concepts and terminology. Moreover, from a cybersecurity viewpoint, the same
hiding mechanism may be perceived differently, making harder the development of
a unique defensive strategy or the definition of practices to mitigate risks
arising from the use of steganography. To mitigate these drawbacks, several
researchers introduced approaches that aid in the unified description of
steganography methods and network covert channels.
  Understanding and combining all descriptive methods for steganography
techniques is a challenging but important task. For instance, researchers might
want to explain how malware applies a certain steganography technique or
categorize a novel hiding approach. Consequently, this paper aims to provide an
introduction to the concept of descriptive methods for steganography. The paper
is organized in the form of a tutorial, with the main goal of explaining how
existing descriptions and taxonomy objects can be combined to achieve a
detailed categorization and description of hiding methods. To show how this can
effectively help the research community, the paper also contains various
real-world examples.

</details>


### [375] [Predictive-CSM: Lightweight Fragment Security for 6LoWPAN IoT Networks](https://arxiv.org/abs/2506.01767)
*Somayeh Sobati-M*

Main category: cs.CR

TL;DR: 在6LoWPAN物联网网络中，碎片化通信存在关键漏洞，因为片段通常在确认合法性之前就被存储和处理。本文提出了一种称为Predictive-CSM的防御策略，结合了两种轻量级机制：一是跟踪节点随时间的行为并奖励一致成功交互、惩罚可疑模式；二是使用链式哈希检查数据包片段完整性。通过攻击模拟测试，Predictive-CSM能够有效保护网络传输并保持能效，为实际物联网系统中的碎片化通信安全提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 6LoWPAN物联网网络中的碎片化过程由于在确认合法性前就存储和处理片段，容易被攻击者利用，因此需要一种更适应性、行为感知的防御策略来解决此问题。

Method: 提出了Predictive-CSM系统，包含两种轻量级机制：1) 跟踪节点行为随时间变化，奖励良好交互并快速惩罚可疑模式；2) 使用链式哈希验证数据包片段的完整性，在早期发现不完整或被篡改的序列。

Result: 通过多种攻击场景（如提前注入碎片、重放头部信息、虚假数据洪泛）的模拟测试，Predictive-CSM成功保持了网络交付能力并维持了能源效率，且无需依赖重量级加密或刚性过滤器。

Conclusion: Predictive-CSM提供了一种适应性强、实时调整的防御方法，使受限设备可以根据观察到的情况而非仅是预设规则进行防御，为6LoWPAN物联网网络中的碎片化通信安全提供了改进方案。

Abstract: Fragmentation is a routine part of communication in 6LoWPAN-based IoT
networks,
  designed to accommodate small frame sizes on constrained wireless links.
However, this process
  introduces a critical vulnerability fragments are typically stored and
processed before their
  legitimacy is confirmed, allowing attackers to exploit this gap with minimal
effort.
  In this work, we explore a defense strategy that takes a more adaptive,
behavior-aware approach to this problem. Our system, called Predictive-CSM,
introduces a combination of two
  lightweight mechanisms. The first tracks how each node behaves over time,
rewarding consistent
  and successful interactions while quickly penalizing suspicious or failing
patterns. The second
  checks the integrity of packet fragments using a chained hash, allowing
incomplete or manipulated sequences to be caught early, before they can occupy
memory or waste processing time.
  We put this system to the test using a set of targeted attack simulations,
including early fragment injection, replayed headers, and flooding with fake
data. Across all scenarios, Predictive CSM preserved network delivery and
maintained energy efficiency, even under pressure. Rather
  than relying on heavyweight cryptography or rigid filters, this approach
allows constrained de vices to adapt their defenses in real time based on what
they observe, not just what they're
  told. In that way, it offers a step forward for securing fragmented
communication in real world
  IoT systems

</details>


### [376] [ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](https://arxiv.org/abs/2506.01770)
*Zeming Wei,Chengcan Wu,Meng Sun*

Main category: cs.CR

TL;DR: ReGA is a model-based analysis framework that uses representation-guided abstraction to improve the safety of large language models (LLMs). It leverages safety-critical representations to address scalability issues and effectively distinguishes between safe and harmful inputs.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have shown great success but there are concerns about their safety and security, such as generating harmful content and vulnerability to jailbreaking attacks. Existing model-based analysis methods suffer from scalability issues when applied to LLMs due to their vast feature spaces.

Method: The proposed method, ReGA, employs representation-guided abstraction by leveraging safety-critical representations which are low-dimensional directions in hidden states indicating safety-related concepts. This approach helps overcome the scalability issue in constructing abstract models for safety modeling of LLMs.

Result: ReGA performs well in distinguishing between safe and harmful inputs with an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. It also shows robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability.

Conclusion: ReGA provides an efficient and scalable solution to enhance the safety of LLMs by combining representation engineering with model-based abstraction, offering new ways to use software insights for AI safety.

Abstract: Large Language Models (LLMs) have achieved significant success in various
tasks, yet concerns about their safety and security have emerged. In
particular, they pose risks in generating harmful content and vulnerability to
jailbreaking attacks. To analyze and monitor machine learning models,
model-based analysis has demonstrated notable potential in stateful deep neural
networks, yet suffers from scalability issues when extending to LLMs due to
their vast feature spaces. In this paper, we propose ReGA, a model-based
analysis framework with representation-guided abstraction, to safeguard LLMs
against harmful prompts and generations. By leveraging safety-critical
representations, which are low-dimensional directions emerging in hidden states
that indicate safety-related concepts, ReGA effectively addresses the
scalability issue when constructing the abstract model for safety modeling. Our
comprehensive evaluation shows that ReGA performs sufficiently well in
distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at
the prompt level and 0.985 at the conversation level. Additionally, ReGA
exhibits robustness to real-world attacks and generalization across different
safety perspectives, outperforming existing safeguard paradigms in terms of
interpretability and scalability. Overall, ReGA serves as an efficient and
scalable solution to enhance LLM safety by integrating representation
engineering with model-based abstraction, paving the way for new paradigms to
utilize software insights for AI safety. Our code is available at
https://github.com/weizeming/ReGA.

</details>


### [377] [Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study](https://arxiv.org/abs/2506.01825)
*Chenyu Wang,Zhou Yang,Yaniv Harel,David Lo*

Main category: cs.CR

TL;DR: Code LLMs are vulnerable to backdoor attacks via triggers in inputs. An empirical study on code summarization investigates factors affecting these attacks, disproving previous consensuses and uncovering new critical factors.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate the factors influencing backdoor attacks on Code LLMs, including aspects not previously explored like training batch size, epoch number, and trigger design space.

Method: An empirical study using code summarization as an example to explore three categories of factors affecting backdoor effectiveness: data, model, and inference.

Result: Backdoors can be implanted at extremely low poisoning rates (as low as 0.004%), common defense mechanisms fail to remove poisoned samples, and smaller batch sizes increase attack risk. Other factors like trigger types, length, and token rarity also significantly impact backdoor effectiveness.

Conclusion: There is an urgent need for new defense mechanisms against backdoor attacks on Code LLMs, especially under extremely low poisoning rate settings.

Abstract: Code LLMs are increasingly employed in software development. However, studies
have shown that they are vulnerable to backdoor attacks: when a trigger (a
specific input pattern) appears in the input, the backdoor will be activated
and cause the model to generate malicious outputs. Researchers have designed
various triggers and demonstrated the feasibility of implanting backdoors by
poisoning a fraction of the training data. Some basic conclusions have been
made, such as backdoors becoming easier to implant when more training data are
modified. However, existing research has not explored other factors influencing
backdoor attacks on Code LLMs, such as training batch size, epoch number, and
the broader design space for triggers, e.g., trigger length.
  To bridge this gap, we use code summarization as an example to perform an
empirical study that systematically investigates the factors affecting backdoor
effectiveness and understands the extent of the threat posed. Three categories
of factors are considered: data, model, and inference, revealing previously
overlooked findings. We find that the prevailing consensus -- that attacks are
ineffective at extremely low poisoning rates -- is incorrect. The absolute
number of poisoned samples matters as well. Specifically, poisoning just 20 out
of 454K samples (0.004\% poisoning rate -- far below the minimum setting of
0.1\% in prior studies) successfully implants backdoors! Moreover, the common
defense is incapable of removing even a single poisoned sample from it.
Additionally, small batch sizes increase the risk of backdoor attacks. We also
uncover other critical factors such as trigger types, trigger length, and the
rarity of tokens in the triggers, leading to valuable insights for assessing
Code LLMs' vulnerability to backdoor attacks. Our study highlights the urgent
need for defense mechanisms against extremely low poisoning rate settings.

</details>


### [378] [Identifying Key Expert Actors in Cybercrime Forums Based on their Technical Expertise](https://arxiv.org/abs/2506.01848)
*Estelle Ruellan,Francois Labreche,Masarah Paquet-Clouston*

Main category: cs.CR

TL;DR: The study uses CVEs and CAPEC classifications to build a bimodal network, community detection, k-means, and a criminological framework to identify key hacker communities and expert actors in cybercrime forums.


<details>
  <summary>Details</summary>
Motivation: Current research on cyber threat intelligence has focused on identifying key threat actors but failed to consider the technical expertise of these actors.

Method: Using CVEs and CAPEC classifications to build a bimodal network, applying community detection, k-means clustering, and a criminological framework.

Result: Communities interested in specific attack patterns were identified. Key actors account for 4% of the population, while half are amateurs. Key actors represent a promising target for resource allocation.

Conclusion: Key actors with significant technical expertise are scarce and should be prioritized in cyber threat intelligence efforts. Further research should explore how these actors develop and use their expertise.

Abstract: The advent of Big Data has made the collection and analysis of cyber threat
intelligence challenging due to its volume, leading research to focus on
identifying key threat actors; yet these studies have failed to consider the
technical expertise of these actors. Expertise, especially towards specific
attack patterns, is crucial for cybercrime intelligence, as it focuses on
targeting actors with the knowledge and skills to attack enterprises. Using
CVEs and CAPEC classifications to build a bimodal network, as well as community
detection, k-means and a criminological framework, this study addresses the key
hacker identification problem by identifying communities interested in specific
attack patterns across cybercrime forums and their related key expert actors.
The analyses reveal several key contributions. First, the community structure
of the CAPEC-actor bimodal network shows that there exists groups of actors
interested in similar attack patterns across cybercrime forums. Second, key
actors identified in this study account for about 4% of the study population.
Third, about half of the study population are amateurs who show little
technical expertise. Finally, key actors highlighted in this study represent a
promising scarcity for resources allocation in cyber threat intelligence
production. Further research should look into how they develop and use their
technical expertise in cybercrime forums.

</details>


### [379] [Black-Box Crypto is Useless for Pseudorandom Codes](https://arxiv.org/abs/2506.01854)
*Sanjam Garg,Sam Gunn,Mingyuan Wang*

Main category: cs.CR

TL;DR: This paper demonstrates that the pseudorandomness of any code tolerating a constant rate of random errors cannot be based on black-box reductions to almost all generic cryptographic primitives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the limitations of black-box reductions in constructing pseudorandom codes which can tolerate a constant rate of random errors.

Method: Authors use the hypercontractivity theorem for Boolean functions to prove the impossibility in the random oracle model and extend it to 'crypto oracles'.

Result: It is shown that pseudorandom codes tolerating a constant rate of random errors cannot be based on black-box reductions to most generic cryptographic primitives.

Conclusion: Black-box reductions are not feasible for constructing pseudorandom codes with certain error tolerance, except for sub-constant rate of random errors as noted by Ghentiyala and Guruswami (2024).

Abstract: A pseudorandom code is a keyed error-correction scheme with the property that
any polynomial number of encodings appear random to any computationally bounded
adversary. We show that the pseudorandomness of any code tolerating a constant
rate of random errors cannot be based on black-box reductions to almost any
generic cryptographic primitive: for instance, anything that can be built from
random oracles, generic multilinear groups, and virtual black-box obfuscation.
Our result is optimal, as Ghentiyala and Guruswami (2024) observed that
pseudorandom codes tolerating any sub-constant rate of random errors exist
using a black-box reduction from one-way functions.
  The key technical ingredient in our proof is the hypercontractivity theorem
for Boolean functions, which we use to prove our impossibility in the random
oracle model. It turns out that this easily extends to an impossibility in the
presence of ``crypto oracles,'' a notion recently introduced -- and shown to be
capable of implementing all the primitives mentioned above -- by Lin, Mook, and
Wichs (EUROCRYPT 2025).

</details>


### [380] [Synchronic Web Digital Identity: Speculations on the Art of the Possible](https://arxiv.org/abs/2506.01856)
*Thien-Nam Dinh,Justin Li,Mitch Negus,Ken Goss*

Main category: cs.CR

TL;DR: As online environments evolve, ensuring trust in information is crucial. This paper advocates for a broad infrastructure for digital identity, using Sandia's Synchronic Web technology to provide cryptographic provenance at internet scale.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the increasing adversarial threats in the digital landscape and the necessity to preserve trust within the public infosphere as search, social media, and artificial intelligence reshape collective knowledge.

Method: The method involves building an infrastructure from the perspective of a national laboratory based on the Synchronic Web, which asserts cryptographic provenance at Internet scale.

Result: The result includes abstractly describing infrastructural foundations and applied configurations expected to underpin future notions of digital identity, with a focus on extending existing work towards more public-facing domains.

Conclusion: The conclusion emphasizes the importance of adopting a non-standard but philosophically defensible notion of identity—digital identity as an unbroken sequence of states in a well-defined digital space.

Abstract: As search, social media, and artificial intelligence continue to reshape
collective knowledge, the preservation of trust on the public infosphere has
become a defining challenge of our time. Given the breadth and versatility of
adversarial threats, the best--and perhaps only--defense is an equally broad
and versatile infrastructure for digital identity.
  This document discusses the opportunities and implications of building such
an infrastructure from the perspective of a national laboratory. The technical
foundation for this discussion is the emergence of the Synchronic Web, a
Sandia-developed infrastructure for asserting cryptographic provenance at
Internet scale. As of the writing of this document, there is ongoing work to
develop the underlying technology and apply it to multiple mission-specific
domains within Sandia. The primary objective of this document to extend the
body of existing work toward the more public-facing domain of digital identity.
  Our approach depends on a non-standard, but philosophically defensible notion
of identity: digital identity is an unbroken sequence of states in a
well-defined digital space. From this foundation, we abstractly describe the
infrastructural foundations and applied configurations that we expect to
underpin future notions of digital identity.

</details>


### [381] [SoK: Concurrency in Blockchain -- A Systematic Literature Review and the Unveiling of a Misconception](https://arxiv.org/abs/2506.01885)
*Atefeh Zareh Chahoki,Maurice Herlihy,Marco Roveri*

Main category: cs.CR

TL;DR: This paper conducts the first survey of concurrency in smart contracts, providing a systematic literature review organized into key dimensions, including taxonomy of concurrency levels, vulnerabilities and countermeasures, revealing a flawed assumption in research and identifying gaps for future work.


<details>
  <summary>Details</summary>
Motivation: Concurrency and parallelism are critical to improving throughput in blockchain systems involving smart contracts, but they also introduce risks such as race conditions and non-determinism. There is a need for a comprehensive survey to understand these aspects better.

Method: The paper establishes a taxonomy of concurrency levels in blockchain systems, reviews vulnerabilities and attacks in concurrent operations, and discusses proposed solutions and countermeasures. It also critically examines existing research assumptions.

Result: The study reveals a flawed concurrency assumption in a major research category that has caused widespread misinterpretation. It identifies gaps in current research and outlines directions for future work to improve correctness and security in concurrent smart contract operations.

Conclusion: This work aims to correct the flawed assumption found in research and guide future research toward more accurate models to support the advancement of blockchain technology.

Abstract: Smart contracts, the cornerstone of blockchain technology, enable secure,
automated distributed execution. Given their role in handling large transaction
volumes across clients, miners, and validators, exploring concurrency is
critical. This includes concurrent transaction execution or validation within
blocks, block processing across shards, and miner competition to select and
persist transactions. Concurrency and parallelism are a double-edged sword:
while they improve throughput, they also introduce risks like race conditions,
non-determinism, and vulnerabilities such as deadlock and livelock.
  This paper presents the first survey of concurrency in smart contracts,
offering a systematic literature review organized into key dimensions. First,
it establishes a taxonomy of concurrency levels in blockchain systems and
discusses proposed solutions for future adoption. Second, it examines
vulnerabilities, attacks, and countermeasures in concurrent operations,
emphasizing the need for correctness and security. Crucially, we reveal a
flawed concurrency assumption in a major research category, which has led to
widespread misinterpretation. This work aims to correct that and guide future
research toward more accurate models. Finally, we identify gaps in each
category to outline future research directions and support blockchain's
advancement.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [382] [Quantum Neural Networks in Practice: A Comparative Study with Classical Models from Standard Data Sets to Industrial Images](https://arxiv.org/abs/2411.19276)
*Daniel Basilewitsch,João F. Bravo,Christian Tutschku,Frederick Struckmeier*

Main category: quant-ph

TL;DR: This paper compares the performance of classical, quantum, and hybrid neural networks for binary image classification.


<details>
  <summary>Details</summary>
Motivation: To understand how well quantum and hybrid models perform compared to classical ones in image classification tasks, and to identify factors influencing their performance.

Method: The study uses randomized NNs on dimensionality-reduced data and CNNs on full image data across three datasets (artificial hypercube, MNIST, industrial images). It evaluates various quantum model hyperparameters and compares them with literature models.

Result: Classical and quantum/hybrid models achieved similar accuracies. Quantum models showed lower variance in training parameters. The number of trainable parameters positively correlated with performance. Entangling gates were common in best-performing quantum NNs but not essential for hybrid CNNs.

Conclusion: Quantum machine learning shows potential for practical image classification tasks but has limitations such as transferability issues. Further research is needed in quantum circuit design, entanglement utilization, and cross-application model transferability.

Abstract: In this study, we compare the performance of randomized classical and quantum
neural networks (NNs) as well as classical and quantum-classical hybrid
convolutional neural networks (CNNs) for the task of binary image
classification. We use two distinct methodologies: using randomized NNs on
dimensionality-reduced data, and applying CNNs to full image data. We evaluate
these approaches on three data sets of increasing complexity: an artificial
hypercube dataset, MNIST handwritten digits and real-world industrial images.
We analyze correlations between classification accuracy and quantum model
hyperparameters, including the number of trainable parameters, feature encoding
methods, circuit layers, entangling gate type and structure, gate entangling
power, and measurement operators. For random quantum NNs, we compare their
performance against literature models. Classical and quantum/hybrid models
achieved statistically equivalent classification accuracies across most
datasets, with no approach demonstrating consistent superiority. We observe
that quantum models show lower variance with respect to initial training
parameters, suggesting better training stability. Among the hyperparameters
analyzed, only the number of trainable parameters showed a positive correlation
with the model performance. Around 94% of the best-performing quantum NNs had
entangling gates, although for hybrid CNNs, models without entanglement
performed equally well but took longer to converge. Cross-dataset performance
analysis revealed limited transferability of quantum models between different
classification tasks. Our study provides an industry perspective on quantum
machine learning for practical image classification tasks, highlighting both
current limitations and potential avenues for further research in quantum
circuit design, entanglement utilization, and model transferability across
varied applications.

</details>


### [383] [Synthesis of discrete-continuous quantum circuits with multimodal diffusion models](https://arxiv.org/abs/2506.01666)
*Florian Fürrutter,Zohim Chandani,Ikko Hamamura,Hans J. Briegel,Gorka Muñoz-Gil*

Main category: quant-ph

TL;DR: 研究人员提出了一种多模态去噪扩散模型，该模型可以同时生成电路结构及其连续参数以编译目标酉矩阵。通过两个独立的扩散过程，一个用于离散门选择，另一个用于参数预测。该模型在不同的实验中进行了基准测试，并利用其快速生成电路的能力，创建了大量特定操作的电路数据集，从而提取有价值的启发式方法，帮助发现量子电路合成的新见解。


<details>
  <summary>Details</summary>
Motivation: 目前最先进的量子操作编译方法虽然能够实现低编译误差，但运行时间长，且需要多次调用量子硬件或昂贵的经典模拟，限制了其扩展性。机器学习模型虽然已作为替代方案出现，但受限于离散门集。因此，需要一种新的方法来解决这些问题。

Method: 引入了一种多模态去噪扩散模型，该模型通过两个独立的扩散过程（一个用于离散门选择，另一个用于参数预测）同时生成电路结构和连续参数。

Result: 该模型在不同实验中表现出良好的准确性，能够在变化的量子比特数、电路深度和参数化门比例下工作，并能快速生成大量电路数据集，从中提取出有价值的启发式方法。

Conclusion: 这种多模态去噪扩散模型为量子电路合成提供了新的思路，能够快速生成电路并提取启发式方法，有助于发现量子电路合成的新见解。

Abstract: Efficiently compiling quantum operations remains a major bottleneck in
scaling quantum computing. Today's state-of-the-art methods achieve low
compilation error by combining search algorithms with gradient-based parameter
optimization, but they incur long runtimes and require multiple calls to
quantum hardware or expensive classical simulations, making their scaling
prohibitive. Recently, machine-learning models have emerged as an alternative,
though they are currently restricted to discrete gate sets. Here, we introduce
a multimodal denoising diffusion model that simultaneously generates a
circuit's structure and its continuous parameters for compiling a target
unitary. It leverages two independent diffusion processes, one for discrete
gate selection and one for parameter prediction. We benchmark the model over
different experiments, analyzing the method's accuracy across varying qubit
counts, circuit depths, and proportions of parameterized gates. Finally, by
exploiting its rapid circuit generation, we create large datasets of circuits
for particular operations and use these to extract valuable heuristics that can
help us discover new insights into quantum circuit synthesis.

</details>


### [384] [A Quantum Information Theoretic Approach to Tractable Probabilistic Models](https://arxiv.org/abs/2506.01824)
*Pedro Zuidberg Dos Martires*

Main category: quant-ph

TL;DR: This paper introduces positive unital circuits (PUnCs) which generalize probabilistic circuits and circuit classes like PSD circuits by using quantum information theory framework.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of probabilistic circuits as generative models in machine learning, especially their ability to marginalize random variables in polytime.

Method: By studying probabilistic circuits through the lens of quantum information theory, the authors introduce PUnCs that extend circuit evaluations from probabilities to positive semi-definite matrices.

Result: PUnCs are shown to strictly generalize probabilistic circuits and other circuit classes such as PSD circuits.

Conclusion: The introduction of PUnCs provides a new perspective on probabilistic circuits and opens up possibilities for more advanced generative models.

Abstract: By recursively nesting sums and products, probabilistic circuits have emerged
in recent years as an attractive class of generative models as they enjoy, for
instance, polytime marginalization of random variables. In this work we study
these machine learning models using the framework of quantum information
theory, leading to the introduction of positive unital circuits (PUnCs), which
generalize circuit evaluations over positive real-valued probabilities to
circuit evaluations over positive semi-definite matrices. As a consequence,
PUnCs strictly generalize probabilistic circuits as well as recently introduced
circuit classes such as PSD circuits.

</details>


### [385] [Learning thermodynamic master equations for open quantum systems](https://arxiv.org/abs/2506.01882)
*Peter Sentz,Stanley Nicholson,Yujin Cho,Sohail Reddy,Brendan Keith,Stefanie Günther*

Main category: quant-ph

TL;DR: This paper introduces a data-driven model for open quantum systems that incorporates learnable, thermodynamically consistent terms.


<details>
  <summary>Details</summary>
Motivation: The characterization of Hamiltonians and other components of open quantum dynamical systems is crucial in quantum computing and applications. While scientific machine learning techniques have been applied, the natural nonlinearities in learnable models have not been incorporated using physical principles.

Method: A data-driven model for open quantum systems is presented, which includes learnable, thermodynamically consistent terms. The trained model estimates the system Hamiltonian and linear components of coupling to the environment.

Result: The model was validated on synthetic two and three-level data, as well as experimental two-level data collected from a quantum device at Lawrence Livermore National Laboratory.

Conclusion: The trained model is interpretable, providing direct estimates of the system Hamiltonian and linear components of coupling to the environment.

Abstract: The characterization of Hamiltonians and other components of open quantum
dynamical systems plays a crucial role in quantum computing and other
applications. Scientific machine learning techniques have been applied to this
problem in a variety of ways, including by modeling with deep neural networks.
However, the majority of mathematical models describing open quantum systems
are linear, and the natural nonlinearities in learnable models have not been
incorporated using physical principles. We present a data-driven model for open
quantum systems that includes learnable, thermodynamically consistent terms.
The trained model is interpretable, as it directly estimates the system
Hamiltonian and linear components of coupling to the environment. We validate
the model on synthetic two and three-level data, as well as experimental
two-level data collected from a quantum device at Lawrence Livermore National
Laboratory.

</details>


### [386] [Probing Quantum Spin Systems with Kolmogorov-Arnold Neural Network Quantum States](https://arxiv.org/abs/2506.01891)
*Mahmud Ashraf Shamim,Eric Reinhardt,Talal Ahmed Chowdhury,Sergei Gleyzer,Paulo T Araujo*

Main category: quant-ph

TL;DR: Neural Quantum States (NQS) are wave functions parametrized by neural networks. This paper proposes SineKAN, a NQS ansatz based on Kolmogorov-Arnold Networks that can capture the ground state energies, fidelities and correlation functions of various quantum models. It outperforms several other neural quantum state ansatze in studying the J1-J2 model.


<details>
  <summary>Details</summary>
Motivation: To improve the representation of quantum mechanical wave functions using neural networks, specifically to find an ansatz that can more accurately capture the properties of complex quantum systems.

Method: The method involves proposing a new Neural Quantum State ansatz called SineKAN, which is based on Kolmogorov-Arnold Networks. This ansatz represents quantum wave functions as nested univariate functions with learnable sinusoidal activation functions.

Result: SineKAN was able to capture the ground state energies, fidelities and various correlation functions of different quantum models including the 1D Transverse-Field Ising model, Anisotropic Heisenberg model, and Antiferromagnetic J1-J2 model. In particular, it outperformed other ansatze such as RBMs, LSTMs, and FFCNs when studying the J1-J2 model with 100 sites.

Conclusion: SineKAN represents an improvement over previous ansatze for representing quantum wave functions, particularly for complex systems like the J1-J2 model.

Abstract: Neural Quantum States (NQS) are a class of variational wave functions
parametrized by neural networks (NNs) to study quantum many-body systems. In
this work, we propose SineKAN, the NQS ansatz based on Kolmogorov-Arnold
Networks (KANs), to represent quantum mechanical wave functions as nested
univariate functions. We show that \sk wavefunction with learnable sinusoidal
activation functions can capture the ground state energies, fidelities and
various correlation functions of the 1D Transverse-Field Ising model,
Anisotropic Heisenberg model, and Antiferromagnetic $J_{1}-J_{2}$ model with
different chain lengths. In our study of the $J_1-J_2$ model with $L=100$
sites, we find that the SineKAN model outperforms several previously explored
neural quantum state ans\"atze, including Restricted Boltzmann Machines (RBMs),
Long Short-Term Memory models (LSTMs), and Feed-Forward Neural Networks (FFCN),
when compared to the results obtained from the Density Matrix Renormalization
Group (DMRG) algorithm.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [387] [From Initial Data to Boundary Layers: Neural Networks for Nonlinear Hyperbolic Conservation Laws](https://arxiv.org/abs/2506.01453)
*Igor Ciril,Khalil Haddaoui,Yohann Tendero*

Main category: math.AP

TL;DR: The paper explores using neural networks to approximate entropy solutions for nonlinear strictly hyperbolic conservation laws, providing a framework for efficient learning algorithms and demonstrating its potential via 1D scalar test cases.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for designing efficient and reliable learning algorithms that can accurately approximate entropy solutions to initial-boundary value problems in nonlinear strictly hyperbolic conservation laws.

Method: Introduce a general framework combining neural networks with fast convergence during training and accurate prediction capabilities. This framework is applied to approximate entropy solutions of hyperbolic conservation laws.

Result: The methodology was successfully evaluated through one-dimensional scalar test cases, showing promising results and indicating potential applicability to more complex industrial scenarios.

Conclusion: Neural networks provide a viable and effective means for approximating entropy solutions in nonlinear strictly hyperbolic conservation laws, with the proposed framework offering both efficiency and reliability.

Abstract: We address the approximation of entropy solutions to initial-boundary value
problems for nonlinear strictly hyperbolic conservation laws using neural
networks. A general and systematic framework is introduced for the design of
efficient and reliable learning algorithms, combining fast convergence during
training with accurate predictions. The methodology is assessed through a
series of one-dimensional scalar test cases, highlighting its potential
applicability to more complex industrial scenarios.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [388] [Artificial Empathy: AI based Mental Health](https://arxiv.org/abs/2506.00081)
*Aditya Naik,Jovi Thomas,Teja Sree,Himavant Reddy*

Main category: q-bio.OT

TL;DR: AI chatbots are used as 'Five minute therapists' or companions, with concerns about privacy and security. Scenario-based testing revealed limitations such as inconsistent tone and lack of crisis sensitivity.


<details>
  <summary>Details</summary>
Motivation: To understand how AI chatbots are utilized by individuals for mental health support and to identify potential improvements in their design and functionality.

Method: Study on participants who have previously used chatbots and scenario-based testing of large language model (LLM) chatbots.

Result: Participants valued anonymity and non-judgmental interactions but had concerns about privacy. Limitations of LLM chatbots include inconsistent tone, inappropriate responses, and lack of crisis sensitivity.

Conclusion: The findings can guide both the technology and mental health care industries in better utilizing AI chatbots to support individuals during challenging emotional periods.

Abstract: Many people suffer from mental health problems but not everyone seeks
professional help or has access to mental health care. AI chatbots have
increasingly become a go-to for individuals who either have mental disorders or
simply want someone to talk to. This paper presents a study on participants who
have previously used chatbots and a scenario-based testing of large language
model (LLM) chatbots. Our findings indicate that AI chatbots were primarily
utilized as a "Five minute therapist" or as a non-judgmental companion.
Participants appreciated the anonymity and lack of judgment from chatbots.
However, there were concerns about privacy and the security of sensitive
information. The scenario-based testing of LLM chatbots highlighted additional
issues. Some chatbots were consistently reassuring, used emojis and names to
add a personal touch, and were quick to suggest seeking professional help.
However, there were limitations such as inconsistent tone, occasional
inappropriate responses (e.g., casual or romantic), and a lack of crisis
sensitivity, particularly in recognizing red flag language and escalating
responses appropriately. These findings can inform both the technology and
mental health care industries on how to better utilize AI chatbots to support
individuals during challenging emotional periods.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [389] [Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation](https://arxiv.org/abs/2506.01591)
*Yuan Gan,Jiaxu Miao,Yunze Wang,Yi Yang*

Main category: cs.GR

TL;DR: An abstract about a new method called Silencer which protects portrait privacy from advanced image-to-video animation based on Latent Diffusion Models (LDM). It proposes a nullifying loss to ignore audio control and anti-purification loss to optimize inverted latent feature.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the ethical concerns in AI security regarding the misuse of highly realistic, synchronized videos created by Latent Diffusion Models (LDM), such as scams, political manipulation, and misinformation.

Method: This paper proposes Silencer, a two-stage method for protecting portrait privacy. The first stage introduces a nullifying loss to ignore audio control in talking-head generation. The second stage applies anti-purification loss in LDM to optimize the inverted latent feature for generating robust perturbations.

Result: Extensive experiments demonstrate that Silencer is effective in proactively protecting portrait privacy from advanced image-to-video animation.

Conclusion: This work aims to raise awareness among the AI security community about critical ethical issues related to talking-head generation techniques.

Abstract: Advances in talking-head animation based on Latent Diffusion Models (LDM)
enable the creation of highly realistic, synchronized videos. These fabricated
videos are indistinguishable from real ones, increasing the risk of potential
misuse for scams, political manipulation, and misinformation. Hence, addressing
these ethical concerns has become a pressing issue in AI security. Recent
proactive defense studies focused on countering LDM-based models by adding
perturbations to portraits. However, these methods are ineffective at
protecting reference portraits from advanced image-to-video animation. The
limitations are twofold: 1) they fail to prevent images from being manipulated
by audio signals, and 2) diffusion-based purification techniques can
effectively eliminate protective perturbations. To address these challenges, we
propose Silencer, a two-stage method designed to proactively protect the
privacy of portraits. First, a nullifying loss is proposed to ignore audio
control in talking-head generation. Second, we apply anti-purification loss in
LDM to optimize the inverted latent feature to generate robust perturbations.
Extensive experiments demonstrate the effectiveness of Silencer in proactively
protecting portrait privacy. We hope this work will raise awareness among the
AI security community regarding critical ethical issues related to talking-head
generation techniques. Code: https://github.com/yuangan/Silencer.

</details>


### [390] [Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing](https://arxiv.org/abs/2506.00512)
*Yang Zheng,Mengqi Huang,Nan Chen,Zhendong Mao*

Main category: cs.GR

TL;DR: 本文提出了一种名为Pro3D-Editor的新框架，通过渐进视图范式实现精确的3D编辑，该方法在编辑准确性和空间一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本引导3D编辑方法采用视图无差别范式，忽略不同视图间的相互依赖关系，导致多视角编辑不一致的问题。

Method: 提出Pro3D-Editor框架，包含主视图采样器、关键视图渲染器和全视图优化器。主视图采样器动态采样并编辑最显著的视图；关键视图渲染器通过MoVE-LoRA技术传播编辑语义到其他关键视图；全视图优化器基于多视角编辑结果对3D对象进行最终编辑和优化。

Result: 大量实验表明，该方法在编辑准确性和空间一致性方面优于现有方法。

Conclusion: Pro3D-Editor框架通过渐进视图范式实现了更一致和精确的3D编辑，具有广泛的实际应用潜力。

Abstract: Text-guided 3D editing aims to precisely edit semantically relevant local 3D
regions, which has significant potential for various practical applications
ranging from 3D games to film production. Existing methods typically follow a
view-indiscriminate paradigm: editing 2D views indiscriminately and projecting
them back into 3D space. However, they overlook the different cross-view
interdependencies, resulting in inconsistent multi-view editing. In this study,
we argue that ideal consistent 3D editing can be achieved through a
\textit{progressive-views paradigm}, which propagates editing semantics from
the editing-salient view to other editing-sparse views. Specifically, we
propose \textit{Pro3D-Editor}, a novel framework, which mainly includes
Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view
Sampler dynamically samples and edits the most editing-salient view as the
primary view. Key-view Render accurately propagates editing semantics from the
primary view to other key views through its Mixture-of-View-Experts Low-Rank
Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based
on the edited multi-views. Extensive experiments demonstrate that our method
outperforms existing methods in editing accuracy and spatial consistency.

</details>


### [391] [Neural Path Guiding with Distribution Factorization](https://arxiv.org/abs/2506.00839)
*Pedro Figueiredo,Qihao He,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: This paper introduces a neural path guiding method for Monte Carlo integration in rendering, featuring a 2D distribution broken into two 1D PDFs modeled by neural networks, showing superiority in complex light transport scenes.


<details>
  <summary>Details</summary>
Motivation: Current neural methods for Monte Carlo integration in rendering either lack speed or expressiveness, necessitating a balanced approach.

Method: The method involves decomposing a 2D distribution into two 1D probability distribution functions (PDFs), each modeled by a neural network estimating the distribution at discrete coordinates. Sampling is done through interpolation and training maximizes the similarity of learned and target distributions while caching incoming radiance with an additional network.

Result: Through extensive experiments, the proposed approach outperforms existing methods, especially in challenging scenes involving complex light transport.

Conclusion: The neural path guiding method provides a sufficiently expressive and reasonably fast solution for Monte Carlo integration in rendering.

Abstract: In this paper, we present a neural path guiding method to aid with Monte
Carlo (MC) integration in rendering. Existing neural methods utilize
distribution representations that are either fast or expressive, but not both.
We propose a simple, but effective, representation that is sufficiently
expressive and reasonably fast. Specifically, we break down the 2D distribution
over the directional domain into two 1D probability distribution functions
(PDF). We propose to model each 1D PDF using a neural network that estimates
the distribution at a set of discrete coordinates. The PDF at an arbitrary
location can then be evaluated and sampled through interpolation. To train the
network, we maximize the similarity of the learned and target distributions. To
reduce the variance of the gradient during optimizations and estimate the
normalization factor, we propose to cache the incoming radiance using an
additional network. Through extensive experiments, we demonstrate that our
approach is better than the existing methods, particularly in challenging
scenes with complex light transport.

</details>


### [392] [Image Generation from Contextually-Contradictory Prompts](https://arxiv.org/abs/2506.01929)
*Saar Huberman,Or Patashnik,Omer Dahary,Ron Mokady,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 文本到图像扩散模型在生成高质量、多样化的图像方面表现出色，但在处理与已学习先验矛盾的概念组合时常常失败。本文提出了一种分阶段的提示分解框架，通过一系列代理提示指导去噪过程，解决了上下文矛盾问题。实验表明，该方法显著提高了生成图像与文本提示的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理包含与已学习先验矛盾的概念组合的提示时，无法生成语义准确的结果。

Method: 提出了一种分阶段的提示分解框架，利用大型语言模型（LLM）分析目标提示，识别矛盾并生成替代表达，以确保上下文连贯性。每个代理提示与特定去噪阶段预期出现的语义内容相匹配。

Result: 实验表明，该方法在各种具有挑战性的提示下，显著提高了生成图像与文本提示的一致性。

Conclusion: 所提出的方法通过将提示信息与去噪进程对齐，实现了细粒度的语义控制和在存在上下文矛盾情况下的准确图像生成。

Abstract: Text-to-image diffusion models excel at generating high-quality, diverse
images from natural language prompts. However, they often fail to produce
semantically accurate results when the prompt contains concept combinations
that contradict their learned priors. We define this failure mode as contextual
contradiction, where one concept implicitly negates another due to entangled
associations learned during training. To address this, we propose a stage-aware
prompt decomposition framework that guides the denoising process using a
sequence of proxy prompts. Each proxy prompt is constructed to match the
semantic content expected to emerge at a specific stage of denoising, while
ensuring contextual coherence. To construct these proxy prompts, we leverage a
large language model (LLM) to analyze the target prompt, identify
contradictions, and generate alternative expressions that preserve the original
intent while resolving contextual conflicts. By aligning prompt information
with the denoising progression, our method enables fine-grained semantic
control and accurate image generation in the presence of contextual
contradictions. Experiments across a variety of challenging prompts show
substantial improvements in alignment to the textual prompt.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [393] [CineMA: A Foundation Model for Cine Cardiac MRI](https://arxiv.org/abs/2506.00679)
*Yunguan Fu,Weixi Yi,Charlotte Manisty,Anish N Bhuva,Thomas A Treibel,James C Moon,Matthew J Clarkson,Rhodri Huw Davies,Yipeng Hu*

Main category: eess.IV

TL;DR: CineMA is a self-supervised autoencoder model trained on 74,916 cine CMR studies to automate tasks with limited labels, matching or outperforming CNNs in various tasks.


<details>
  <summary>Details</summary>
Motivation: Extracting clinically important measurements from CMR remains time-consuming and subjective, thus there is a need for automation.

Method: CineMA is a self-supervised autoencoder model trained on a large dataset of cine CMR studies. It is fine-tuned and evaluated across multiple datasets and tasks.

Result: CineMA matches or outperforms CNNs in various tasks, demonstrating greater label efficiency and reducing the burden of clinician labelling.

Conclusion: CineMA supports replacing task-specific training with fine-tuning foundation models in future cardiac imaging applications, promoting reproducibility and accelerating clinical translation.

Abstract: Cardiac magnetic resonance (CMR) is a key investigation in clinical
cardiovascular medicine and has been used extensively in population research.
However, extracting clinically important measurements such as ejection fraction
for diagnosing cardiovascular diseases remains time-consuming and subjective.
We developed CineMA, a foundation AI model automating these tasks with limited
labels. CineMA is a self-supervised autoencoder model trained on 74,916 cine
CMR studies to reconstruct images from masked inputs. After fine-tuning, it was
evaluated across eight datasets on 23 tasks from four categories: ventricle and
myocardium segmentation, left and right ventricle ejection fraction
calculation, disease detection and classification, and landmark localisation.
CineMA is the first foundation model for cine CMR to match or outperform
convolutional neural networks (CNNs). CineMA demonstrated greater label
efficiency than CNNs, achieving comparable or better performance with fewer
annotations. This reduces the burden of clinician labelling and supports
replacing task-specific training with fine-tuning foundation models in future
cardiac imaging applications. Models and code for pre-training and fine-tuning
are available at https://github.com/mathpluscode/CineMA, democratising access
to high-performance models that otherwise require substantial computational
resources, promoting reproducibility and accelerating clinical translation.

</details>


### [394] [Flexible Mixed Precision Quantization for Learned Image Compression](https://arxiv.org/abs/2506.01221)
*Md Adnan Faisal Hossain,Zhihao Duan,Fengqing Zhu*

Main category: eess.IV

TL;DR: This paper proposes a Flexible Mixed Precision Quantization (FMPQ) method and an adaptive search algorithm for Learned Image Compression (LIC) models, which improves BD-Rate performance while maintaining similar model size.


<details>
  <summary>Details</summary>
Motivation: Learned Image Compression (LIC) has high computational costs. Existing quantization methods use fixed-precision which is not optimal due to varying sensitivity of different layers in neural networks.

Method: The FMPQ method assigns different bit-widths to different layers based on the fractional change in rate-distortion loss. An adaptive search algorithm is introduced to reduce the time-complexity of finding the desired quantization bit-width distribution.

Result: The proposed method shows improved BD-Rate performance compared to other works on LIC model quantization under similar model size constraints.

Conclusion: FMPQ with the adaptive search algorithm provides an effective solution for reducing computational costs in LIC while maintaining or improving performance.

Abstract: Despite its improvements in coding performance compared to traditional
codecs, Learned Image Compression (LIC) suffers from large computational costs
for storage and deployment. Model quantization offers an effective solution to
reduce the computational complexity of LIC models. However, most existing works
perform fixed-precision quantization which suffers from sub-optimal utilization
of resources due to the varying sensitivity to quantization of different layers
of a neural network. In this paper, we propose a Flexible Mixed Precision
Quantization (FMPQ) method that assigns different bit-widths to different
layers of the quantized network using the fractional change in rate-distortion
loss as the bit-assignment criterion. We also introduce an adaptive search
algorithm which reduces the time-complexity of searching for the desired
distribution of quantization bit-widths given a fixed model size. Evaluation of
our method shows improved BD-Rate performance under similar model size
constraints compared to other works on quantization of LIC models. We have made
the source code available at gitlab.com/viper-purdue/fmpq.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [395] [Rethinking Hybrid Retrieval: When Small Embeddings and LLM Re-ranking Beat Bigger Models](https://arxiv.org/abs/2506.00049)
*Arjun Rao,Hanieh Alipour,Nick Pendar*

Main category: cs.IR

TL;DR: This paper compares embedding models in tri-modal hybrid retrieval for RAG systems, revealing that MiniLM-v6 outperforms BGE-Large when combined with LLM-based re-ranking. This suggests embedding model selection should focus on compatibility with multi-signal fusion and LLM alignment.


<details>
  <summary>Details</summary>
Motivation: To explore the performance of different embedding models within a tri-modal hybrid retrieval framework for RAG systems, challenging the assumption that larger models always perform better.

Method: Comparison of dense semantic, sparse lexical, and graph-based embeddings using MiniLM-v6 and BGE-Large architectures, integrated with LLM-based re-ranking in a tri-modal hybrid framework.

Result: MiniLM-v6 demonstrates superior performance compared to BGE-Large, particularly in agentic re-ranking scenarios, leading to significant improvements in retrieval quality on SciFact, FIQA, and NFCorpus datasets.

Conclusion: For RAG systems, embedding model selection should prioritize compatibility with multi-signal fusion and LLM alignment over model size, potentially reducing computational needs while enhancing retrieval accuracy and efficiency.

Abstract: This paper presents a comparison of embedding models in tri-modal hybrid
retrieval for Retrieval-Augmented Generation (RAG) systems. We investigate the
fusion of dense semantic, sparse lexical, and graph-based embeddings, focusing
on the performance of the MiniLM-v6 and BGE-Large architectures. Contrary to
conventional assumptions, our results show that the compact MiniLM-v6
outperforms the larger BGE-Large when integrated with LLM-based re-ranking
within our tri-modal hybrid framework. Experiments conducted on the SciFact,
FIQA, and NFCorpus datasets demonstrate significant improvements in retrieval
quality with the MiniLM-v6 configuration. The performance difference is
particularly pronounced in agentic re-ranking scenarios, indicating better
alignment between MiniLM-v6's embedding space and LLM reasoning. Our findings
suggest that embedding model selection for RAG systems should prioritize
compatibility with multi-signal fusion and LLM alignment, rather than relying
solely on larger models. This approach may reduce computational requirements
while improving retrieval accuracy and efficiency.

</details>


### [396] [Gated Multimodal Graph Learning for Personalized Recommendation](https://arxiv.org/abs/2506.00107)
*Sibei Liu,Yuanzhe Zhang,Xiang Li,Yunbo Liu,Chengwei Feng,Hao Yang*

Main category: cs.IR

TL;DR: RLMultimodalRec是一种轻量级模块化推荐框架，结合基于图的用户建模与自适应多模态项目编码。通过门控融合模块动态平衡视觉和文本模态贡献，并使用两层LightGCN编码器捕捉高阶协作信号。实验表明，该模型在Amazon产品数据集上显著优于多个基线方法。


<details>
  <summary>Details</summary>
Motivation: 多模态推荐通过整合丰富的内容信息（如产品图像和文本描述），有望缓解协同过滤中的冷启动和稀疏性问题。但如何有效将异构模态整合到统一推荐框架中仍是一个挑战，现有方法通常依赖固定的融合策略或复杂架构，可能无法适应模态质量差异或引入不必要的计算开销。

Method: 提出了一种名为RLMultimodalRec的轻量级模块化推荐框架，结合了基于图的用户建模和自适应多模态项目编码。具体来说，模型采用门控融合模块动态平衡视觉和文本模态的贡献，生成精细且基于内容的项目表示；同时使用两层LightGCN编码器，在不依赖非线性变换的情况下，通过用户-项目交互图传播嵌入以捕获高阶协作信号。

Result: 在Amazon产品领域的实际数据集上的实验结果表明，RLMultimodalRec在多个竞争基线上表现出色，包括协同过滤、视觉感知和多模态GNN方法。该方法在保持可扩展性和可解释性的同时，在top-K推荐指标上实现了显著改进。

Conclusion: RLMultimodalRec提供了一种有效的解决方案，用于解决多模态推荐中的挑战。它通过轻量级设计和自适应模态融合，在性能、可扩展性和可解释性之间取得了良好的平衡，适合实际部署。

Abstract: Multimodal recommendation has emerged as a promising solution to alleviate
the cold-start and sparsity problems in collaborative filtering by
incorporating rich content information, such as product images and textual
descriptions. However, effectively integrating heterogeneous modalities into a
unified recommendation framework remains a challenge. Existing approaches often
rely on fixed fusion strategies or complex architectures , which may fail to
adapt to modality quality variance or introduce unnecessary computational
overhead.
  In this work, we propose RLMultimodalRec, a lightweight and modular
recommendation framework that combines graph-based user modeling with adaptive
multimodal item encoding. The model employs a gated fusion module to
dynamically balance the contribution of visual and textual modalities, enabling
fine-grained and content-aware item representations. Meanwhile, a two-layer
LightGCN encoder captures high-order collaborative signals by propagating
embeddings over the user-item interaction graph without relying on nonlinear
transformations.
  We evaluate our model on a real-world dataset from the Amazon product domain.
Experimental results demonstrate that RLMultimodalRec consistently outperforms
several competitive baselines, including collaborative filtering, visual-aware,
and multimodal GNN-based methods. The proposed approach achieves significant
improvements in top-K recommendation metrics while maintaining scalability and
interpretability, making it suitable for practical deployment.

</details>


### [397] [Query Drift Compensation: Enabling Compatibility in Continual Learning of Retrieval Embedding Models](https://arxiv.org/abs/2506.00037)
*Dipam Goswami,Liying Wang,Bartłomiej Twardowski,Joost van de Weijer*

Main category: cs.IR

TL;DR: The paper proposes a method for continual learning in text embedding models to address the issue of forgetting old tasks when updating with new data, employing embedding distillation and query drift compensation to maintain compatibility without re-indexing.


<details>
  <summary>Details</summary>
Motivation: Text embedding models are typically studied under static training data conditions, limiting their application to dynamic scenarios where new data emerges over time. Updating these models leads to non-compatibility issues with previously indexed corpora, necessitating re-indexing which is computationally expensive.

Method: The authors establish a continual learning benchmark with large-scale datasets and train dense retrieval embedding models on new data while observing forgetting of old tasks due to embedding drift. They use embedding distillation on both queries and documents to maintain stability and propose a query drift compensation method to project new model embeddings into the old embedding space.

Result: The proposed method significantly improves performance without requiring any re-indexing of the corpus, maintaining compatibility with previously indexed corpus embeddings extracted using the old model.

Conclusion: The study demonstrates an effective approach to continually update text embedding models without the need for re-indexing by addressing the problem of embedding drift and forgetting, thus enabling more dynamic applications.

Abstract: Text embedding models enable semantic search, powering several NLP
applications like Retrieval Augmented Generation by efficient information
retrieval (IR). However, text embedding models are commonly studied in
scenarios where the training data is static, thus limiting its applications to
dynamic scenarios where new training data emerges over time. IR methods
generally encode a huge corpus of documents to low-dimensional embeddings and
store them in a database index. During retrieval, a semantic search over the
corpus is performed and the document whose embedding is most similar to the
query embedding is returned. When updating an embedding model with new training
data, using the already indexed corpus is suboptimal due to the
non-compatibility issue, since the model which was used to obtain the
embeddings of the corpus has changed. While re-indexing of old corpus documents
using the updated model enables compatibility, it requires much higher
computation and time. Thus, it is critical to study how the already indexed
corpus can still be effectively used without the need of re-indexing. In this
work, we establish a continual learning benchmark with large-scale datasets and
continually train dense retrieval embedding models on query-document pairs from
new datasets in each task and observe forgetting on old tasks due to
significant drift of embeddings. We employ embedding distillation on both query
and document embeddings to maintain stability and propose a novel query drift
compensation method during retrieval to project new model query embeddings to
the old embedding space. This enables compatibility with previously indexed
corpus embeddings extracted using the old model and thus reduces the
forgetting. We show that the proposed method significantly improves performance
without any re-indexing. Code is available at
https://github.com/dipamgoswami/QDC.

</details>


### [398] [Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and Discretizing Dense Retrieval](https://arxiv.org/abs/2506.00041)
*Seongwan Park,Taeklim Kim,Youngjoong Ko*

Main category: cs.IR

TL;DR: CL-SR is a new retrieval framework using latent concepts from Sparse Autoencoders to make Dense Passage Retrieval models interpretable, efficient and robust.


<details>
  <summary>Details</summary>
Motivation: Dense Passage Retrieval models have strong performance but lack interpretability.

Method: Using Sparse Autoencoders, decompose dense embeddings into interpretable latent concepts, generate natural language descriptions for these concepts and use them in the Concept-Level Sparse Retrieval framework.

Result: CL-SR has high index-space and computational efficiency while maintaining good performance despite vocabulary and semantic mismatches.

Conclusion: CL-SR combines the benefits of dense embeddings with the transparency and efficiency of sparse representations.

Abstract: Despite their strong performance, Dense Passage Retrieval (DPR) models suffer
from a lack of interpretability. In this work, we propose a novel
interpretability framework that leverages Sparse Autoencoders (SAEs) to
decompose previously uninterpretable dense embeddings from DPR models into
distinct, interpretable latent concepts. We generate natural language
descriptions for each latent concept, enabling human interpretations of both
the dense embeddings and the query-document similarity scores of DPR models. We
further introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework
that directly utilizes the extracted latent concepts as indexing units. CL-SR
effectively combines the semantic expressiveness of dense embeddings with the
transparency and efficiency of sparse representations. We show that CL-SR
achieves high index-space and computational efficiency while maintaining robust
performance across vocabulary and semantic mismatches.

</details>


### [399] [Graph Contrastive Learning for Optimizing Sparse Data in Recommender Systems with LightGCL](https://arxiv.org/abs/2506.00048)
*Aravinda Jatavallabha,Prabhanjan Bharadwaj,Ashish Chander*

Main category: cs.IR

TL;DR: Graph Neural Networks (GNNs) are effective for recommendation systems but face challenges with data sparsity and noise. LightGCL, a new model using Singular Value Decomposition (SVD) for graph augmentation, addresses these issues by preserving semantic integrity and capturing global collaborative signals, surpassing state-of-the-art models in benchmarks while improving fairness and resilience.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing GNN-based recommendation systems which suffer from data sparsity and noise.

Method: Implemented LightGCL, a graph contrastive learning model that employs Singular Value Decomposition (SVD) for robust graph augmentation, avoiding stochastic or heuristic perturbations.

Result: LightGCL shows significant improvements over state-of-the-art models in benchmark datasets, demonstrating better performance, fairness, and resilience to popularity bias.

Conclusion: LightGCL is an effective solution for enhancing recommendation systems, particularly addressing data sparsity and noise issues.

Abstract: Graph Neural Networks (GNNs) are powerful tools for recommendation systems,
but they often struggle under data sparsity and noise. To address these issues,
we implemented LightGCL, a graph contrastive learning model that uses Singular
Value Decomposition (SVD) for robust graph augmentation, preserving semantic
integrity without relying on stochastic or heuristic perturbations. LightGCL
enables structural refinement and captures global collaborative signals,
achieving significant gains over state-of-the-art models across benchmark
datasets. Our experiments also demonstrate improved fairness and resilience to
popularity bias, making it well-suited for real-world recommender systems.

</details>


### [400] [Bridging the Gap: From Ad-hoc to Proactive Search in Conversations](https://arxiv.org/abs/2506.00983)
*Chuan Meng,Francesco Tonolini,Fengran Mo,Nikolaos Aletras,Emine Yilmaz,Gabriella Kazai*

Main category: cs.IR

TL;DR: Conv2Query is a conversation-to-query framework that adapts ad-hoc retrievers to proactive search in conversations (PSC) by bridging the input gap between ad-hoc search and PSC, significantly improving retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Proactive search in conversations (PSC) attempts to reduce user effort in formulating explicit queries. However, previous methods either directly use conversational context as input to ad-hoc retrievers or fine-tune them on PSC data, which has limitations due to the input mismatch between ad-hoc search and PSC.

Method: The proposed Conv2Query framework maps conversational context into ad-hoc queries, addressing the input mismatch issue. These queries can be used with off-the-shelf ad-hoc retrievers or for further fine-tuning on PSC data.

Result: Extensive experiments on two PSC datasets demonstrate that Conv2Query significantly enhances the performance of ad-hoc retrievers, both when used directly and after fine-tuning on PSC data.

Conclusion: Conv2Query effectively bridges the input gap between ad-hoc search and PSC, leading to improved retrieval quality in proactive search within conversations.

Abstract: Proactive search in conversations (PSC) aims to reduce user effort in
formulating explicit queries by proactively retrieving useful relevant
information given conversational context. Previous work in PSC either directly
uses this context as input to off-the-shelf ad-hoc retrievers or further
fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on
short and concise queries, while the PSC input is longer and noisier. This
input mismatch between ad-hoc search and PSC limits retrieval quality. While
fine-tuning on PSC data helps, its benefits remain constrained by this input
gap. In this work, we propose Conv2Query, a novel conversation-to-query
framework that adapts ad-hoc retrievers to PSC by bridging the input gap
between ad-hoc search and PSC. Conv2Query maps conversational context into
ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc
retrievers or for further fine-tuning on PSC data. Extensive experiments on two
PSC datasets show that Conv2Query significantly improves ad-hoc retrievers'
performance, both when used directly and after fine-tuning on PSC.

</details>


### [401] [DV365: Extremely Long User History Modeling at Instagram](https://arxiv.org/abs/2506.00450)
*Wenhan Lyu,Devashish Tyagi,Yihang Yang,Ziwei Li,Ajay Somani,Karthikeyan Shanmugasundaram,Nikola Andrejevic,Ferdi Adeputra,Curtis Zeng,Arun K. Singh,Maxime Ransan,Sagar Jain*

Main category: cs.IR

TL;DR: The paper introduces DV365, a new user embedding that uses multi-slicing and summarization to capture long-term user interest with history lengths up to 70,000. It complements advanced sequence models in Instagram, is used across 15 models in Instagram and Threads, and has been successfully in production for over a year.


<details>
  <summary>Details</summary>
Motivation: Long user history is crucial for recommendation systems but incorporating it can be costly in terms of power consumption and GPU usage.

Method: Offline embedding is chosen over end-to-end sequence length optimization methods to model extremely long user sequences cost-effectively. A new strategy called multi-slicing and summarization is proposed to generate user representation capturing stable long-term interests.

Result: DV365 embedding, produced by a single upstream foundational model, has been launched across 15 different models in Instagram and Threads with significant impact and proven effective in production for over a year.

Conclusion: DV365 provides highly generalizable user representations of long-term stable interests and is an effective incremental addition to advanced attentive user sequence models.

Abstract: Long user history is highly valuable signal for recommendation systems, but
effectively incorporating it often comes with high cost in terms of data center
power consumption and GPU. In this work, we chose offline embedding over
end-to-end sequence length optimization methods to enable extremely long user
sequence modeling as a cost-effective solution, and propose a new user
embedding learning strategy, multi-slicing and summarization, that generates
highly generalizable user representation of user's long-term stable interest.
History length we encoded in this embedding is up to 70,000 and on average
40,000. This embedding, named as DV365, is proven highly incremental on top of
advanced attentive user sequence models deployed in Instagram. Produced by a
single upstream foundational model, it is launched in 15 different models
across Instagram and Threads with significant impact, and has been production
battle-proven for >1 year since our first launch.

</details>


### [402] [Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System](https://arxiv.org/abs/2506.00828)
*Chao Wang,Yue Zheng,Yujing Zhang,Yan Feng,Zhe Wang,Xiaowei Shi,An You,Yu Chen*

Main category: cs.IR

TL;DR: In a single-slot recommendation system, user feedback is limited to one item at a time. This paper introduces the Breaker model which integrates user representation clustering with a multi-tower structure for cluster-specific preference modeling, addressing shortcut biases related to user intrinsic tendencies. Experiments show it surpasses baselines and is deployed on Meituan serving tens of millions daily.


<details>
  <summary>Details</summary>
Motivation: To address the issue that in single-slot recommendation systems, only pointwise modeling solutions can be adopted which focus solely on modeling the likelihood of clicks or conversions without the ability to capture ranking information among different items directly. Also, to mitigate the shortcut bias caused by abundant user-side information compared to item-side information.

Method: The Breaker model integrates an auxiliary task of user representation clustering with a multi-tower structure for cluster-specific preference modeling. A delayed parameter update mechanism is also proposed to enhance training stability and convergence.

Result: Both offline and online experiments demonstrate that the Breaker model surpasses the baselines.

Conclusion: The Breaker model successfully addresses shortcut biases related to user intrinsic tendencies in single-slot recommendation systems and has been deployed on Meituan serving tens of millions of users daily.

Abstract: In a single-slot recommendation system, users are only exposed to one item at
a time, and the system cannot collect user feedback on multiple items
simultaneously. Therefore, only pointwise modeling solutions can be adopted,
focusing solely on modeling the likelihood of clicks or conversions for items
by users to learn user-item preferences, without the ability to capture the
ranking information among different items directly. However, since user-side
information is often much more abundant than item-side information, the model
can quickly learn the differences in user intrinsic tendencies, which are
independent of the items they are exposed to. This can cause these intrinsic
tendencies to become a shortcut bias for the model, leading to insufficient
mining of the most concerned user-item preferences. To solve this challenge, we
introduce the Breaker model. Breaker integrates an auxiliary task of user
representation clustering with a multi-tower structure for cluster-specific
preference modeling. By clustering user representations, we ensure that users
within each cluster exhibit similar characteristics, which increases the
complexity of the pointwise recommendation task on the user side. This forces
the multi-tower structure with cluster-driven parameter learning to better
model user-item preferences, ultimately eliminating shortcut biases related to
user intrinsic tendencies. In terms of training, we propose a delayed parameter
update mechanism to enhance training stability and convergence, enabling
end-to-end joint training of the auxiliary clustering and classification tasks.
Both offline and online experiments demonstrate that our method surpasses the
baselines. It has already been deployed and is actively serving tens of
millions of users daily on Meituan, one of the most popular e-commerce
platforms for services.

</details>


### [403] [GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion](https://arxiv.org/abs/2506.01673)
*Sunkyung Lee,Minjin Choi,Eunseong Choi,Hye-young Kim,Jongwuk Lee*

Main category: cs.IR

TL;DR: GRAM is a new generative recommender system that improves upon existing models by incorporating implicit item relationships and efficiently using detailed item information, showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current generative recommendation systems in utilizing implicit item relationships and rich, lengthy item information.

Method: Proposes GRAM with two innovations: semantic-to-lexical translation for encoding item relationships into LLM vocabulary space, and multi-granular late fusion for integrating rich semantics with minimal information loss.

Result: Experiments on four datasets show GRAM outperforms eight state-of-the-art models with 11.5-16.0% improvement in Recall@5 and 5.3-13.6% in NDCG@5.

Conclusion: GRAM effectively addresses the challenges in generative recommendation and demonstrates superior performance.

Abstract: Generative recommendation is an emerging paradigm that leverages the
extensive knowledge of large language models by formulating recommendations
into a text-to-text generation task. However, existing studies face two key
limitations in (i) incorporating implicit item relationships and (ii) utilizing
rich yet lengthy item information. To address these challenges, we propose a
Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM),
introducing two synergistic innovations. First, we design semantic-to-lexical
translation to encode implicit hierarchical and collaborative item
relationships into the vocabulary space of LLMs. Second, we present
multi-granular late fusion to integrate rich semantics efficiently with minimal
information loss. It employs separate encoders for multi-granular prompts,
delaying the fusion until the decoding stage. Experiments on four benchmark
datasets show that GRAM outperforms eight state-of-the-art generative
recommendation models, achieving significant improvements of 11.5-16.0% in
Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at
https://github.com/skleee/GRAM.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [404] [Pushing the Limits of Beam Search Decoding for Transducer-based ASR models](https://arxiv.org/abs/2506.00185)
*Lilit Grigoryan,Vladimir Bataev,Andrei Andrusenko,Hainan Xu,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: The paper presents a universal method to accelerate beam search for Transducers in ASR systems, introducing ALSD++ and AES++ algorithms. It uses batch operations, tree-based hypothesis structure, novel blank scoring, and CUDA graph execution to narrow the speed gap between beam and greedy modes while improving WER and shallow fusion.


<details>
  <summary>Details</summary>
Motivation: Transducer models are effective for ASR but beam search slows them down due to repeated evaluations of key network components, limiting practical applications.

Method: The proposed method involves batch operations, a tree-based hypothesis structure, novel blank scoring for enhanced shallow fusion, and CUDA graph execution for efficient GPU inference to accelerate beam search for Transducers.

Result: This method narrows the speed gap between beam and greedy modes to only 10-20%, achieves 14-30% relative improvement in WER compared to greedy decoding, and improves shallow fusion for low-resource up to 11%.

Conclusion: The universal method significantly accelerates beam search for Transducers, leading to optimized algorithms ALSD++ and AES++, with all algorithms being open sourced.

Abstract: Transducer models have emerged as a promising choice for end-to-end ASR
systems, offering a balanced trade-off between recognition accuracy, streaming
capabilities, and inference speed in greedy decoding. However, beam search
significantly slows down Transducers due to repeated evaluations of key network
components, limiting practical applications. This paper introduces a universal
method to accelerate beam search for Transducers, enabling the implementation
of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes
batch operations, a tree-based hypothesis structure, novel blank scoring for
enhanced shallow fusion, and CUDA graph execution for efficient GPU inference.
This narrows the speed gap between beam and greedy modes to only 10-20% for the
whole system, achieves 14-30% relative improvement in WER compared to greedy
decoding, and improves shallow fusion for low-resource up to 11% compared to
existing implementations. All the algorithms are open sourced.

</details>


### [405] [SoundSculpt: Direction and Semantics Driven Ambisonic Target Sound Extraction](https://arxiv.org/abs/2506.00273)
*Tuochao Chen,D Shin,Hakan Erdogan,Sinan Hersek*

Main category: eess.AS

TL;DR: The paper introduces SoundSculpt, a neural network that extracts target sound fields from ambisonic recordings using spatial and semantic information.


<details>
  <summary>Details</summary>
Motivation: To improve the extraction of target sound fields from ambisonic recordings by leveraging both spatial and semantic information for better performance in complex audio environments.

Method: SoundSculpt employs an ambisonic-in-ambisonic-out architecture conditioned on spatial information (e.g., target direction) and semantic embeddings (e.g., derived from image segmentation and captioning). It is trained and evaluated on synthetic and real ambisonic mixtures.

Result: SoundSculpt demonstrates superior performance compared to various signal processing baselines. Combining spatial and semantic information is beneficial when secondary sound sources are close to the target. Comparisons of two different semantic embeddings derived from text descriptions using text encoders were conducted.

Conclusion: SoundSculpt successfully extracts target sound fields from ambisonic recordings with improved performance due to the combination of spatial and semantic conditioning.

Abstract: This paper introduces SoundSculpt, a neural network designed to extract
target sound fields from ambisonic recordings. SoundSculpt employs an
ambisonic-in-ambisonic-out architecture and is conditioned on both spatial
information (e.g., target direction obtained by pointing at an immersive video)
and semantic embeddings (e.g., derived from image segmentation and captioning).
Trained and evaluated on synthetic and real ambisonic mixtures, SoundSculpt
demonstrates superior performance compared to various signal processing
baselines. Our results further reveal that while spatial conditioning alone can
be effective, the combination of spatial and semantic information is beneficial
in scenarios where there are secondary sound sources spatially close to the
target. Additionally, we compare two different semantic embeddings derived from
a text description of the target sound using text encoders.

</details>


### [406] [CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer](https://arxiv.org/abs/2506.00800)
*Daiki Takeuchi,Binh Thien Nguyen,Masahiro Yasuda,Yasunori Ohishi,Daisuke Niizumi,Noboru Harada*

Main category: eess.AS

TL;DR: CLAP-ART, an AAC method using 'semantic-rich and discrete' tokens as input through vector quantization of pre-trained audio representations, outperforms EnCLAP on two AAC benchmarks.


<details>
  <summary>Details</summary>
Motivation: EnCodec's discrete tokens used in EnCLAP are designed for waveform reconstruction rather than capturing the semantic contexts of general sounds which is the goal of AAC.

Method: Propose CLAP-ART that computes semantic-rich discrete tokens from pre-trained audio representations via vector quantization to be used as input for AAC.

Result: CLAP-ART outperforms baseline EnCLAP on two AAC benchmarks.

Conclusion: Semantic-rich discrete tokens derived from semantically rich audio representations are beneficial for Automated Audio Captioning.

Abstract: Automated Audio Captioning (AAC) aims to describe the semantic contexts of
general sounds, including acoustic events and scenes, by leveraging effective
acoustic features. To enhance performance, an AAC method, EnCLAP, employed
discrete tokens from EnCodec as an effective input for fine-tuning a language
model BART. However, EnCodec is designed to reconstruct waveforms rather than
capture the semantic contexts of general sounds, which AAC should describe. To
address this issue, we propose CLAP-ART, an AAC method that utilizes
``semantic-rich and discrete'' tokens as input. CLAP-ART computes semantic-rich
discrete tokens from pre-trained audio representations through vector
quantization. We experimentally confirmed that CLAP-ART outperforms baseline
EnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens
derived from semantically rich AR are beneficial for AAC.

</details>


### [407] [Confidence intervals for forced alignment boundaries using model ensembles](https://arxiv.org/abs/2506.01256)
*Matthew C. Kelley*

Main category: eess.AS

TL;DR: The paper presents a method using neural network ensemble technique to derive confidence intervals for boundaries in forced alignment, showing slight improvement and providing uncertainty measures.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of most forced alignment tools that provide only a single estimate of a boundary, thus lacking uncertainty measures.

Method: Using a neural network ensemble technique with ten different segment classifier neural networks previously trained, repeat the alignment process with each model, place the boundary at the median of the boundaries in the ensemble, and construct 97.85% confidence intervals using order statistics.

Result: On the Buckeye and TIMIT corpora, the ensemble boundaries show a slight improvement over using just a single model.

Conclusion: The confidence intervals are incorporated into Praat TextGrids and output as a table, allowing researchers to analyze uncertainty separately or incorporate it into their analyses.

Abstract: Forced alignment is a common tool to align audio with orthographic and
phonetic transcriptions. Most forced alignment tools provide only a single
estimate of a boundary. The present project introduces a method of deriving
confidence intervals for these boundaries using a neural network ensemble
technique. Ten different segment classifier neural networks were previously
trained, and the alignment process is repeated with each model. The alignment
ensemble is then used to place the boundary at the median of the boundaries in
the ensemble, and 97.85% confidence intervals are constructed using order
statistics. On the Buckeye and TIMIT corpora, the ensemble boundaries show a
slight improvement over using just a single model. The confidence intervals are
incorporated into Praat TextGrids using a point tier, and they are also output
as a table for researchers to analyze separately as diagnostics or to
incorporate uncertainty into their analyses.

</details>


### [408] [Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech](https://arxiv.org/abs/2506.01618)
*Karl El Hajal,Enno Hermann,Sevada Hovsepyan,Mathew Magimai. -Doss*

Main category: eess.AS

TL;DR: The paper proposes a syllable-based rhythm modeling method within the RnV framework to convert dysarthric speech for enhancing ASR performance. Experiments show significant improvements in ASR for severe dysarthria using LF-MMI models, while fine-tuning Whisper had minimal impact.


<details>
  <summary>Details</summary>
Motivation: ASR systems perform poorly with dysarthric speech due to high inter-speaker variability and slow speaking rates. This motivated the exploration of dysarthric-to-healthy speech conversion methods.

Method: The paper extends the Rhythm and Voice (RnV) conversion framework by incorporating a syllable-based rhythm modeling method designed for dysarthric speech. The impact on ASR is evaluated by training LF-MMI models and fine-tuning Whisper on converted speech.

Result: Experiments on the Torgo corpus indicate that LF-MMI models trained on converted speech achieve significant reductions in word error rates, particularly for more severe cases of dysarthria. Fine-tuning Whisper on converted data has little effect on its performance.

Conclusion: The findings suggest that unsupervised rhythm and voice conversion holds promise for improving ASR performance with dysarthric speech.

Abstract: Automatic speech recognition (ASR) systems struggle with dysarthric speech
due to high inter-speaker variability and slow speaking rates. To address this,
we explore dysarthric-to-healthy speech conversion for improved ASR
performance. Our approach extends the Rhythm and Voice (RnV) conversion
framework by introducing a syllable-based rhythm modeling method suited for
dysarthric speech. We assess its impact on ASR by training LF-MMI models and
fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal
that LF-MMI achieves significant word error rate reductions, especially for
more severe cases of dysarthria, while fine-tuning Whisper on converted data
has minimal effect on its performance. These results highlight the potential of
unsupervised rhythm and voice conversion for dysarthric ASR. Code available at:
https://github.com/idiap/RnV

</details>


### [409] [On-device Streaming Discrete Speech Units](https://arxiv.org/abs/2506.01845)
*Kwanghee Choi,Masao Someki,Emma Strubell,Shinji Watanabe*

Main category: eess.AS

TL;DR: The paper explores a method to make Discrete Speech Units (DSUs) more efficient by reducing the attention window and model size, leading to a 50% reduction in FLOPs with minimal impact on accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable practical use of DSUs for on-device streaming speech applications by overcoming limitations such as requiring full-length speech input and computationally expensive models.

Method: Reducing both the attention window and the model size while preserving the effectiveness of DSUs derived from self-supervised speech models.

Result: Achieved a 50% reduction in FLOPs with only a 6.5% relative increase in character error rate (CER) on the ML-SUPERB 1h dataset.

Conclusion: DSUs have significant potential for real-time speech processing in resource-constrained environments.

Abstract: Discrete speech units (DSUs) are derived from clustering the features of
self-supervised speech models (S3Ms). DSUs offer significant advantages for
on-device streaming speech applications due to their rich phonetic information,
high transmission efficiency, and seamless integration with large language
models. However, conventional DSU-based approaches are impractical as they
require full-length speech input and computationally expensive S3Ms. In this
work, we reduce both the attention window and the model size while preserving
the effectiveness of DSUs. Our results demonstrate that we can reduce
floating-point operations (FLOPs) by 50% with only a relative increase of 6.5%
in character error rate (CER) on the ML-SUPERB 1h dataset. These findings
highlight the potential of DSUs for real-time speech processing in
resource-constrained environments.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [410] [Neural Network-based Information-Theoretic Transceivers for High-Order Modulation Schemes](https://arxiv.org/abs/2506.00368)
*Ngoc Long Pham,Tri Nhu Do*

Main category: eess.SP

TL;DR: This paper proposes an NN-based bitwise receiver and a novel symbol-wise AE-based E2E system for improving computational efficiency and performance in communication systems.


<details>
  <summary>Details</summary>
Motivation: To develop AI-native E2E communication systems using neural networks, which can potentially enhance the performance and efficiency of traditional systems.

Method: The authors propose an NN-based bitwise receiver to improve computational efficiency while maintaining performance comparable to baseline demappers. They also introduce a symbol-wise AE-based E2E system that jointly optimizes the transmitter and receiver at the physical layer.

Result: The NN-based receiver achieves accurate numerical BER, and the AE-based system outperforms baseline architectures, especially for higher-order modulation schemes. The training SNR significantly impacts the system's performance during inference at different SNR levels.

Conclusion: NN-based receivers and AE-based E2E systems show promise in enhancing the efficiency and performance of communication systems, with particular advantages for higher-order modulation schemes.

Abstract: Neural network (NN)-based end-to-end (E2E) communication systems, in which
each system component may consist of a portion of a neural network, have been
investigated as potential tools for developing artificial intelligence
(Al)-native E2E systems. In this paper, we propose an NN-based bitwise receiver
that improves computational efficiency while maintaining performance comparable
to baseline demappers. Building on this foundation, we introduce a novel
symbol-wise autoencoder (AE)-based E2E system that jointly optimizes the
transmitter and receiver at the physical layer. We evaluate the proposed
NN-based receiver using bit-error rate (BER) analysis to confirm that the
numerical BER achieved by NN-based receivers or transceivers is accurate.
Results demonstrate that the AE-based system outperforms baseline
architectures, particularly for higher-order modulation schemes. We further
show that the training signal-to-noise ratio (SNR) significantly affects the
performance of the systems when inference is conducted at different SNR levels.

</details>


### [411] [Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention](https://arxiv.org/abs/2506.00452)
*TaeJun Ha,Chaehyun Jung,Hyeonuk Kim,Jeongwoo Park,Jeonghun Park*

Main category: eess.SP

TL;DR: 提出了一种新的基于模型的深度神经网络框架A-MMSE，利用注意力变换器学习最优MMSE滤波器，通过单一线性操作进行信道估计，降低计算复杂度。此外，还开发了两阶段注意力编码器和秩适应扩展，以提高学习效率和灵活性。实验表明，A-MMSE在各种信噪比条件下均优于基线方法，并重新定义了性能复杂性权衡的标准。


<details>
  <summary>Details</summary>
Motivation: 在OFDM中，准确的信道估计至关重要，但传统的信号处理方法（如MMSE估计）需要难以获得的二阶统计信息，而现有的基于深度神经网络的方法则通常计算复杂度较高。

Method: 提出了一种名为A-MMSE的新框架，结合了注意力机制和MMSE估计。该框架通过注意力变换器学习最优MMSE滤波器，并通过单一线性操作进行信道估计，避免了非线性激活，从而降低了计算复杂度。为了提高学习效率，设计了一个两阶段注意力编码器来捕捉信道相关结构。此外，还提出了A-MMSE的秩适应扩展，允许在复杂性和信道估计准确性之间进行灵活权衡。

Result: 广泛的仿真结果表明，与现有基线方法相比，A-MMSE在各种信噪比条件下的归一化MSE表现更优。其秩适应扩展进一步优化了性能复杂性权衡。

Conclusion: A-MMSE及其秩适应扩展在性能和复杂性之间建立了新的前沿，为实际信道估计方法设定了新标准。

Abstract: In orthogonal frequency division multiplexing (OFDM), accurate channel
estimation is crucial. Classical signal processing based approaches, such as
minimum mean-squared error (MMSE) estimation, often require second-order
statistics that are difficult to obtain in practice. Recent deep neural
networks based methods have been introduced to address this; yet they often
suffer from high complexity. This paper proposes an Attention-aided MMSE
(A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter
via the Attention Transformer. Once trained, the A-MMSE estimates the channel
through a single linear operation for channel estimation, eliminating nonlinear
activations during inference and thus reducing computational complexity. To
enhance the learning efficiency of the A-MMSE, we develop a two-stage Attention
encoder, designed to effectively capture the channel correlation structure.
Additionally, a rank-adaptive extension of the proposed A-MMSE allows flexible
trade-offs between complexity and channel estimation accuracy. Extensive
simulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE
consistently outperforms other baseline methods in terms of normalized MSE
across a wide range of SNR conditions. In particular, the A-MMSE and its
rank-adaptive extension establish a new frontier in the performance complexity
trade-off, redefining the standard for practical channel estimation methods.

</details>


### [412] [Power-of-Two (PoT) Weights in Large Language Models (LLMs)](https://arxiv.org/abs/2506.00315)
*Mahmoud Elgenedy*

Main category: eess.SP

TL;DR: The paper explores using power of two (PoT) quantization to reduce complexity in large language models (LLMs), showing promising results with minimal cross entropy loss degradation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of implementing complex neural networks, particularly LLMs, on edge devices with limited memory and processing power.

Method: Investigating the use of power of two (PoT) quantization for linear layers weights and transformer tables in LLMs, which reduces memory and computational requirements by converting multiplications to bit shifting.

Result: Preliminary results on Nano-GPT and a 124-M GPT-2 model show that PoT quantization is promising, with cross entropy loss degradation of approximately [1.3-0.88] for a number of bits range [4-6] to represent power levels.

Conclusion: PoT quantization provides significant memory and computational reduction in LLMs with minimal impact on performance.

Abstract: Complexity of Neural Networks is increasing rapidly due to the massive
increase in model parameters. Specifically, in Large Language Models (LLMs),
the number of model parameters has grown exponentially in the past few years,
for example, from 1.5 billion parameters in GPT2 to 175 billion in GPT3. This
raises a significant challenge for implementation, especially for Edge devices
where memory and processing power are very limited. In this work, we
investigate reducing LLM complexity with special type of quantization, power of
two (PoT), for linear layers weights and transformer tables. PoT not only
provides memory reduction but more importantly provides significant
computational reduction through converting multiplication to bit shifting. We
obtained preliminary results of PoT quantization on Nano-GPT implementation
using Shakespeare dataset. We then extended results to 124-M GPT-2 model. The
PoT quantization results are shown to be very promising with cross entropy loss
degradation $\approx$[1.3-0.88] with number of bits range [4-6] to represent
power levels.

</details>


### [413] [From Turbulence to Tranquility: AI-Driven Low-Altitude Network](https://arxiv.org/abs/2506.01378)
*Kürşat Tekbıyık,Amir Hossein Fahim Raouf,İsmail Güvenç,Mingzhe Chen,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: eess.SP

TL;DR: This paper addresses the challenges of Low Altitude Economy (LAE) networks and proposes intelligent LAE networks through machine learning-based spectrum sensing, AI-optimized resource allocation, and testbed-driven validation. It emphasizes the role of federated and reinforcement learning in adaptive decision-making and highlights real-world platforms like AERPAW for system refinement.


<details>
  <summary>Details</summary>
Motivation: Low Altitude Economy (LAE) networks hold great potential in urban mobility, emergency response, and aerial logistics, but they face challenges such as spectrum management, interference mitigation, and real-time coordination.

Method: The study explores three core elements: machine learning-based spectrum sensing and coexistence, AI-optimized resource allocation and trajectory planning, and testbed-driven validation and standardization. Federated and reinforcement learning techniques are used to support decentralized, adaptive decision-making under constraints.

Result: Federated and reinforcement learning techniques can effectively support decentralized, adaptive decision-making for LAE networks under mobility and energy constraints. Real-world platforms like AERPAW play a crucial role in bridging the gap between simulation and deployment.

Conclusion: This study provides a forward-looking roadmap for developing efficient and interoperable AI-driven LAE ecosystems.

Abstract: Low Altitude Economy (LAE) networks own transformative potential in urban
mobility, emergency response, and aerial logistics. However, these networks
face significant challenges in spectrum management, interference mitigation,
and real-time coordination across dynamic and resource-constrained
environments. After addressing these challenges, this study explores three core
elements for enabling intelligent LAE networks as follows machine
learning-based spectrum sensing and coexistence, artificial intelligence
(AI)-optimized resource allocation and trajectory planning, and testbed-driven
validation and standardization. We highlight how federated and reinforcement
learning techniques support decentralized, adaptive decision-making under
mobility and energy constraints. In addition, we discuss the role of real-world
platforms such as AERPAW in bridging the gap between simulation and deployment
and enabling iterative system refinement under realistic conditions. This study
aims to provide a forward-looking roadmap toward developing efficient and
interoperable AI-driven LAE ecosystems.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [414] [Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor](https://arxiv.org/abs/2506.01162)
*Maryam Aliakbarpour,Zhan Shi,Ria Stevens,Vincent X. Wang*

Main category: cs.DS

TL;DR: 本文提出了一种在中心模型下具有差分隐私的假设选择算法，该算法运行时间接近线性，达到最优近似因子，样本复杂度仅略有增加。


<details>
  <summary>Details</summary>
Motivation: 密度估计是统计学中的基本问题，而假设选择涉及从给定候选分布中确定最能描述数据分布的那个。之前的研究虽然有效解决了非隐私场景下的假设选择问题，但在差分隐私约束下的高效算法尚待探索。

Method: 作者设计了一种在中心模型下满足差分隐私的算法，该算法相对于假设数量n的运行时间接近线性，并达到了最优的近似因子α=3。此外，算法的样本复杂度保持polylogarithmic级别，相较于先前需要二次时间复杂度的上界有了显著改进。

Result: 所提出的算法不仅在时间复杂度上实现了突破，还保持了与最优非隐私算法相同的近似因子，并且样本复杂度仅略有增加。

Conclusion: 本文通过提出一种高效的、满足差分隐私的假设选择算法，解决了[Bun, Kamath, Steinke, Wu, NeurIPS 2019]中提出的开放问题，推动了差分隐私领域内假设选择问题的研究进展。

Abstract: Estimating the density of a distribution from its samples is a fundamental
problem in statistics. Hypothesis selection addresses the setting where, in
addition to a sample set, we are given $n$ candidate distributions -- referred
to as hypotheses -- and the goal is to determine which one best describes the
underlying data distribution. This problem is known to be solvable very
efficiently, requiring roughly $O(\log n)$ samples and running in
$\tilde{O}(n)$ time. The quality of the output is measured via the total
variation distance to the unknown distribution, and the approximation factor of
the algorithm determines how large this distance is compared to the optimal
distance achieved by the best candidate hypothesis. It is known that $\alpha =
3$ is the optimal approximation factor for this problem. We study hypothesis
selection under the constraint of differential privacy. We propose a
differentially private algorithm in the central model that runs in
nearly-linear time with respect to the number of hypotheses, achieves the
optimal approximation factor, and incurs only a modest increase in sample
complexity, which remains polylogarithmic in $n$. This resolves an open
question posed by [Bun, Kamath, Steinke, Wu, NeurIPS 2019]. Prior to our work,
existing upper bounds required quadratic time.

</details>


### [415] [Randomized Dimensionality Reduction for Euclidean Maximization and Diversity Measures](https://arxiv.org/abs/2506.00165)
*Jie Gao,Rajesh Jayaram,Benedikt Kolbe,Shay Sapir,Chris Schwiegelshohn,Sandeep Silwal,Erik Waingarten*

Main category: cs.DS

TL;DR: The paper explores dimension reduction for maximization problems like max-matching, max-spanning tree, and max TSP, revealing that the effectiveness is closely related to the doubling dimension of the dataset. A target dimension of $O(\lambda_X)$ is both sufficient and necessary for preserving near-optimal solutions.


<details>
  <summary>Details</summary>
Motivation: Dimensionality reduction has been a successful technique for speeding up large-scale Euclidean optimization problems. However, its application to maximization problems and measures of dataset diversity needs further investigation.

Method: The authors analyze how dimensionality reduction affects various maximization problems by relating it to the doubling dimension $\lambda_X$ of the dataset. They prove theoretical bounds on the required target dimension and validate their findings empirically.

Result: The results show that a target dimension of $O(\lambda_X)$ is sufficient to approximately preserve near-optimal solutions for the studied maximization problems, which also turns out to be necessary in some cases. Empirical evidence supports the quality of solutions found in the projected space and demonstrates speedups due to dimensionality reduction.

Conclusion: This study establishes the connection between dimension reduction and the doubling dimension for maximization problems, offering both theoretical insights and practical implications for efficient computation.

Abstract: Randomized dimensionality reduction is a widely-used algorithmic technique
for speeding up large-scale Euclidean optimization problems. In this paper, we
study dimension reduction for a variety of maximization problems, including
max-matching, max-spanning tree, max TSP, as well as various measures for
dataset diversity. For these problems, we show that the effect of dimension
reduction is intimately tied to the \emph{doubling dimension} $\lambda_X$ of
the underlying dataset $X$ -- a quantity measuring intrinsic dimensionality of
point sets. Specifically, we prove that a target dimension of $O(\lambda_X)$
suffices to approximately preserve the value of any near-optimal solution,which
we also show is necessary for some of these problems. This is in contrast to
classical dimension reduction results, whose dependence increases with the
dataset size $|X|$. We also provide empirical results validating the quality of
solutions found in the projected space, as well as speedups due to
dimensionality reduction.

</details>


### [416] [Learning DNF through Generalized Fourier Representations](https://arxiv.org/abs/2506.01075)
*Mohsen Heidari,Roni Khardon*

Main category: cs.DS

TL;DR: 本文提出了广义傅里叶展开，可用于任何分布，并通过修改算法工具来学习DNF表达式，同时分析了相关分布的性质和约束条件。


<details>
  <summary>Details</summary>
Motivation: 传统的傅里叶表示在算法和复杂性分析中应用广泛，但其主要适用于均匀分布或乘积分布。因此，需要一种更通用的方法来处理其他类型的分布。

Method: 引入广义傅里叶展开并通过贝叶斯网络表示分布；修改现有算法以使用该广义展开进行学习；分析特定类型分布下的$L_1$谱范数；证明差分有界树BN分布下DNF的可学习性；开发学习此类分布的算法。

Result: 成功将广义傅里叶展开应用于任意分布的学习问题；证明了差分有界树BN分布下DNF的可学习性；提出了学习此类分布的算法。

Conclusion: 广义傅里叶展开为处理任意分布提供了有力工具，进一步扩展了基于傅里叶表示的学习理论。

Abstract: The Fourier representation for the uniform distribution over the Boolean cube
has found numerous applications in algorithms and complexity analysis. Notably,
in learning theory, learnability of Disjunctive Normal Form (DNF) under uniform
as well as product distributions has been established through such
representations. This paper makes five main contributions. First, it introduces
a generalized Fourier expansion that can be used with any distribution $D$
through the representation of the distribution as a Bayesian network (BN).
Second, it shows that the main algorithmic tools for learning with the Fourier
representation, that use membership queries to approximate functions by
recovering their heavy Fourier coefficients, can be used with slight
modifications with the generalized expansion. These results hold for any
distribution. Third, it analyzes the $L_1$ spectral norm of conjunctions under
the new expansion, showing that it is bounded for a class of distributions
which can be represented by difference bounded tree BN, where a parent node in
the BN representation can change the conditional expectation of a child node by
at most $\alpha<0.5$. Lower bounds are presented to show that such constraints
are necessary. The fourth contribution uses these results to show the
learnability of DNF with membership queries under difference bounded tree BN.
The final contribution is to develop an algorithm for learning
difference-bounded tree BN distributions, thus extending the DNF learnability
result to cases where the distribution is not known in advance.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [417] [Shill Bidding Prevention in Decentralized Auctions Using Smart Contracts](https://arxiv.org/abs/2506.00282)
*M. A. Bouaicha,G. Destefanis,T. Montanaro,N. Lasla,L. Patrono*

Main category: cs.GT

TL;DR: The paper proposes a conceptual framework using blockchain smart contracts to deter auction fraud through dynamic, behavior-based penalties in online auctions.


<details>
  <summary>Details</summary>
Motivation: To address the significant risks posed by fraudulent behaviors such as shill bidding in online auctions.

Method: The framework applies dynamic, behavior-based penalties using blockchain smart contracts. It introduces an economic disincentive system where penalty severity scales with suspicious bidding patterns, evaluated by the Bid Shill Score (BSS) that assesses nine distinct bidding behaviors.

Result: Simulations confirm the effectiveness of the model: the dynamic penalty mechanism reduces the profitability of shill bidding while keeping penalties low for honest bidders. The system introduces moderate gas and latency overhead, making it practical for real-world use.

Conclusion: The approach provides a practical method for behavior-based fraud prevention in decentralized systems where trust cannot be assumed.

Abstract: In online auctions, fraudulent behaviors such as shill bidding pose
significant risks. This paper presents a conceptual framework that applies
dynamic, behavior-based penalties to deter auction fraud using blockchain smart
contracts. Unlike traditional post-auction detection methods, this approach
prevents manipulation in real-time by introducing an economic disincentive
system where penalty severity scales with suspicious bidding patterns. The
framework employs the proposed Bid Shill Score (BSS) to evaluate nine distinct
bidding behaviors, dynamically adjusting the penalty fees to make fraudulent
activity financially unaffordable while providing fair competition.
  The system is implemented within a decentralized English auction on the
Ethereum blockchain, demonstrating how smart contracts enforce transparent
auction rules without trusted intermediaries. Simulations confirm the
effectiveness of the proposed model: the dynamic penalty mechanism reduces the
profitability of shill bidding while keeping penalties low for honest bidders.
Performance evaluation shows that the system introduces only moderate gas and
latency overhead, keeping transaction costs and response times within practical
bounds for real-world use. The approach provides a practical method for
behaviour-based fraud prevention in decentralised systems where trust cannot be
assumed.

</details>


### [418] [The Disparate Effects of Partial Information in Bayesian Strategic Learning](https://arxiv.org/abs/2506.00627)
*Srikanth Avasarala,Serena Wang,Juba Ziani*

Main category: cs.GT

TL;DR: 研究了部分信息关于评分规则对战略性学习环境中公平性的影响。发现对于天真的代理人，效用差异可能随着噪声无限增长，并且低成本修改特征的群体可能在有限透明度下受到不成比例的伤害；而对于贝叶斯代理人，差异保持有界。差异通常在中等透明度水平下最小化。最后，扩展分析到群体不仅在成本上不同，也在先验信念上不同的情况，研究这种不对称如何影响公平性。


<details>
  <summary>Details</summary>
Motivation: 了解不同成本特征修改的群体之间的结果差异如何产生，以及这些差异如何随学习者规则的透明度变化。

Method: 考虑两种不同的代理模型：天真的代理人和贝叶斯代理人。天真的代理人直接接受嘈杂信号，而贝叶斯代理人根据信号更新先验信念。通过分析不同透明度水平下的差异，并研究其与噪声的关系。

Result: 对于天真的代理人，效用差异可以随着噪声无限增长，低成本群体可能在有限透明度下受到更多伤害；对于贝叶斯代理人，差异保持有界，并且通常在中等透明度水平下最小化。进一步扩展到群体在成本和先验信念都不同的情况下，研究不对称如何影响公平性。

Conclusion: 评分规则的部分信息显著影响战略性学习环境中的公平性，不同类型的代理人和透明度水平导致的结果差异需要特别关注，尤其是在设计公平的机器学习系统时。

Abstract: We study how partial information about scoring rules affects fairness in
strategic learning settings. In strategic learning, a learner deploys a scoring
rule, and agents respond strategically by modifying their features -- at some
cost -- to improve their outcomes. However, in our work, agents do not observe
the scoring rule directly; instead, they receive a noisy signal of said rule.
We consider two different agent models: (i) naive agents, who take the noisy
signal at face value, and (ii) Bayesian agents, who update a prior belief based
on the signal.
  Our goal is to understand how disparities in outcomes arise between groups
that differ in their costs of feature modification, and how these disparities
vary with the level of transparency of the learner's rule. For naive agents, we
show that utility disparities can grow unboundedly with noise, and that the
group with lower costs can, perhaps counter-intuitively, be disproportionately
harmed under limited transparency. In contrast, for Bayesian agents,
disparities remain bounded. We provide a full characterization of disparities
across groups as a function of the level of transparency and show that they can
vary non-monotonically with noise; in particular, disparities are often
minimized at intermediate levels of transparency. Finally, we extend our
analysis to settings where groups differ not only in cost, but also in prior
beliefs, and study how this asymmetry influences fairness.

</details>


### [419] [Empirical Validation of the Independent Chip Model](https://arxiv.org/abs/2506.00180)
*Juho Kim*

Main category: cs.GT

TL;DR: The paper introduces a new dataset of over ten thousand poker tournaments to validate the independent chip model (ICM). It finds that ICM outperforms a proposed baseline and provides evidence that ICM underestimates large-stacked players' performances while overestimating short-stacked ones.


<details>
  <summary>Details</summary>
Motivation: To scrutinize the real-world performance of the Independent Chip Model (ICM) in poker tournaments, which has not been sufficiently examined at a large scale.

Method: Introduction of a new dataset with results from over ten thousand poker tournaments. Conducting two experiments: comparing ICM's accuracy with a proposed baseline and assessing ICM's estimation of player performances based on stack sizes.

Result: ICM performs more accurately than the proposed baseline. Empirical evidence shows ICM underestimates performances of players with larger stacks and overestimates those who are short-stacked.

Conclusion: This study contributes to future research in developing algorithms for estimating player value in poker tournaments.

Abstract: The independent chip model (ICM) forms a cornerstone of all modern poker
tournament strategy. However, despite its prominence, the ICM's performance in
the real world has not been sufficiently scrutinized, especially at a large
scale. In this paper, we introduce our new dataset of poker tournaments,
consisting of results of over ten thousand events. Then, using this dataset, we
perform two experiments as part of a large-scale empirical validation of the
ICM. First, we verify that the ICM performs more accurately than a baseline we
propose. Second, we obtain empirical evidence of the ICM underestimating the
performances of players with larger stacks while overestimating those who are
short-stacked. Our contributions may be useful to future researchers developing
new algorithms for estimating a player's value in poker tournaments.

</details>


### [420] [General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess](https://arxiv.org/abs/2506.01242)
*Brian Hu Zhang,Tuomas Sandholm*

Main category: cs.GT

TL;DR: The paper introduces Obscuro, the first superhuman AI for Fog of War chess, which advances search in imperfect-information games and outperforms previous AIs and human players.


<details>
  <summary>Details</summary>
Motivation: To create an AI that achieves superhuman performance in Fog of War chess, a challenging imperfect-information game variant.

Method: Developed Obscuro, introducing advancements to search in imperfect-information games, enabling strong and scalable reasoning.

Result: Experiments demonstrate that Obscuro is significantly stronger than the prior state-of-the-art AI and human players, including the world's best.

Conclusion: FoW chess is the largest turn-based game with imperfect information where superhuman performance has been achieved, marking a significant milestone in AI research.

Abstract: Since the advent of AI, games have served as progress benchmarks. Meanwhile,
imperfect-information variants of chess have existed for over a century,
present extreme challenges, and have been the focus of significant AI research.
Beyond calculation needed in regular chess, they require reasoning about
information gathering, the opponent's knowledge, signaling, etc. The most
popular variant, Fog of War (FoW) chess (aka. dark chess) is a recognized
challenge problem in AI after superhuman performance was reached in no-limit
Texas hold'em poker. We present Obscuro, the first superhuman AI for FoW chess.
It introduces advances to search in imperfect-information games, enabling
strong, scalable reasoning. Experiments against the prior state-of-the-art AI
and human players -- including the world's best -- show that Obscuro is
significantly stronger. FoW chess is the largest (by amount of imperfect
information) turn-based game in which superhuman performance has been achieved
and the largest game in which imperfect-information search has been
successfully applied.

</details>


### [421] [Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts](https://arxiv.org/abs/2506.01685)
*Benjamin Schiffer,Mark Sellke*

Main category: cs.GT

TL;DR: 在高维情境下的激励相容探索模型中，通过温和的几何条件，可以实现维度和其他参数多项式级别的样本复杂度的激励相容算法，突破了初始探索阶段指数级样本复杂度的障碍。


<details>
  <summary>Details</summary>
Motivation: 在高维情境下，现有激励相容算法的初始探索阶段需要指数级样本复杂度，限制了高效学习的可能性。

Method: 研究线性bandit模型，其中动作位于欧几里得单位球内，设计了一个激励相容的探索算法。

Result: 该算法的样本复杂度随维度和其他参数呈多项式增长，而非指数增长，解决了高维情境下的探索障碍。

Conclusion: 在特定几何条件下，激励相容性不会妨碍遗憾最优性，从而为高维情境下的高效学习提供了可能。

Abstract: In the incentivized exploration model, a principal aims to explore and learn
over time by interacting with a sequence of self-interested agents. It has been
recently understood that the main challenge in designing incentive-compatible
algorithms for this problem is to gather a moderate amount of initial data,
after which one can obtain near-optimal regret via posterior sampling. With
high-dimensional contexts, however, this \emph{initial exploration} phase
requires exponential sample complexity in some cases, which prevents efficient
learning unless initial data can be acquired exogenously. We show that these
barriers to exploration disappear under mild geometric conditions on the set of
available actions, in which case incentive-compatibility does not preclude
regret-optimality. Namely, we consider the linear bandit model with actions in
the Euclidean unit ball, and give an incentive-compatible exploration algorithm
with sample complexity that scales polynomially with the dimension and other
parameters.

</details>


### [422] [Should Decision-Makers Reveal Classifiers in Online Strategic Classification?](https://arxiv.org/abs/2506.01936)
*Han Shao,Shuo Xie,Kunhe Yang*

Main category: cs.GT

TL;DR: 在在线战略分类中，限制代理对当前分类器的访问可能会适得其反，导致决策者犯更多的错误。


<details>
  <summary>Details</summary>
Motivation: 研究了在在线战略分类问题中，限制代理对当前分类器的访问是否能防止因操作而导致的误分类错误。

Method: 考虑了一个扩展的在线战略分类设置，其中代理人缺乏关于当前分类器的直接知识，而是基于历史分类器的加权平均值进行操作。

Result: 结果表明，在这种设置下，与完全知识设置相比，决策者的错误增加了$(1-\gamma)^{-1}$或$k_{\text{in}}$倍，其中$k_{\text{in}}$是操纵图的最大入度，$\gamma$是表示代理人对过去分类器的记忆的折扣因子。

Conclusion: 研究表明，限制对分类器的访问可能会适得其反，降低决策者在在线战略分类中的表现。

Abstract: Strategic classification addresses a learning problem where a decision-maker
implements a classifier over agents who may manipulate their features in order
to receive favorable predictions. In the standard model of online strategic
classification, in each round, the decision-maker implements and publicly
reveals a classifier, after which agents perfectly best respond based on this
knowledge. However, in practice, whether to disclose the classifier is often
debated -- some decision-makers believe that hiding the classifier can prevent
misclassification errors caused by manipulation.
  In this paper, we formally examine how limiting the agents' access to the
current classifier affects the decision-maker's performance. Specifically, we
consider an extended online strategic classification setting where agents lack
direct knowledge about the current classifier and instead manipulate based on a
weighted average of historically implemented classifiers. Our main result shows
that in this setting, the decision-maker incurs $(1-\gamma)^{-1}$ or
$k_{\text{in}}$ times more mistakes compared to the full-knowledge setting,
where $k_{\text{in}}$ is the maximum in-degree of the manipulation graph
(representing how many distinct feature vectors can be manipulated to appear as
a single one), and $\gamma$ is the discount factor indicating agents' memory of
past classifiers. Our results demonstrate how withholding access to the
classifier can backfire and degrade the decision-maker's performance in online
strategic classification.

</details>


### [423] [Online Competitive Information Gathering for Partially Observable Trajectory Games](https://arxiv.org/abs/2506.01927)
*Mel Krusniak,Hang Xu,Parker Palermo,Forrest Laine*

Main category: cs.GT

TL;DR: The paper presents an online method for computing rational trajectory plans in partially observable stochastic games (POSGs), which enables game-theoretic agents to actively gather information about their opponents.


<details>
  <summary>Details</summary>
Motivation: Game-theoretic agents need to optimally gather information about their opponents, but planning in fully continuous POSGs is intractable without heavy offline computation or assumptions on the order of belief maintained by each player.

Method: The authors formulate a finite history/horizon refinement of POSGs and present an online method that computes rational trajectory plans. This method leverages particle-based estimations of the joint state space and performs stochastic gradient play, with necessary adjustments for deployment on individual agents.

Result: The method was tested in continuous pursuit-evasion and warehouse-pickup scenarios, as well as extensions to more than two players and complex environments with obstacles. It demonstrated active information gathering and outperformed passive competitors.

Conclusion: This approach allows for competitive information gathering behavior in trajectory space and provides a tractable solution for online planning in POSGs.

Abstract: Game-theoretic agents must make plans that optimally gather information about
their opponents. These problems are modeled by partially observable stochastic
games (POSGs), but planning in fully continuous POSGs is intractable without
heavy offline computation or assumptions on the order of belief maintained by
each player. We formulate a finite history/horizon refinement of POSGs which
admits competitive information gathering behavior in trajectory space, and
through a series of approximations, we present an online method for computing
rational trajectory plans in these games which leverages particle-based
estimations of the joint state space and performs stochastic gradient play. We
also provide the necessary adjustments required to deploy this method on
individual agents. The method is tested in continuous pursuit-evasion and
warehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more
complex environments with visual and physical obstacles), demonstrating
evidence of active information gathering and outperforming passive competitors.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [424] [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Syed Zawad,Holger Boche,Walid Saad*

Main category: cs.CY

TL;DR: The paper investigates safety degradation in telecom-tuned LLMs and proposes three safety realignment defenses (SafeInstruct, SafeLoRA, and SafeMERGE) that restore safety without compromising performance.


<details>
  <summary>Details</summary>
Motivation: To understand how fine-tuning large language models for telecom tasks can compromise model safety and to address the lack of safety alignment in publicly available Telecom LLMs.

Method: Investigate safety degradation in telecom-tuned LLMs using three representative datasets from the GenAINet initiative. Analyze publicly available Telecom LLMs trained via continual pre-training. Evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and SafeMERGE) using established red-teaming benchmarks.

Result: Safety degradation was found even in structured and seemingly harmless datasets. Safety realignment defenses effectively restored safety without compromising downstream task performance, leading to Safe teleCOMMunication (SafeCOMM) models.

Conclusion: The work serves as a diagnostic study and practical guide for safety realignment in telecom-tuned LLMs, emphasizing the importance of safety-aware instruction and fine-tuning for real-world deployments.

Abstract: Fine-tuning large language models (LLMs) for telecom tasks and datasets is a
common practice to adapt general-purpose models to the telecom domain. However,
little attention has been paid to how this process may compromise model safety.
Recent research has shown that even benign fine-tuning can degrade the safety
alignment of LLMs, causing them to respond to harmful or unethical user
queries. In this paper, we investigate this issue for telecom-tuned LLMs using
three representative datasets featured by the GenAINet initiative. We show that
safety degradation persists even for structured and seemingly harmless datasets
such as 3GPP standards and tabular records, indicating that telecom-specific
data is not immune to safety erosion during fine-tuning. We further extend our
analysis to publicly available Telecom LLMs trained via continual pre-training,
revealing that safety alignment is often severely lacking, primarily due to the
omission of safety-focused instruction tuning. To address these issues in both
fine-tuned and pre-trained models, we conduct extensive experiments and
evaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and
SafeMERGE) using established red-teaming benchmarks. The results show that,
across all settings, the proposed defenses can effectively restore safety after
harmful degradation without compromising downstream task performance, leading
to Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as
a diagnostic study and practical guide for safety realignment in telecom-tuned
LLMs, and emphasizes the importance of safety-aware instruction and fine-tuning
for real-world deployments of Telecom LLMs.

</details>


### [425] [Children's Voice Privacy: First Steps And Emerging Challenges](https://arxiv.org/abs/2506.00100)
*Ajinkya Kulkarni,Francisco Teixeira,Enno Hermann,Thomas Rolland,Isabel Trancoso,Mathew Magimai Doss*

Main category: cs.CY

TL;DR: 儿童在语音技术中是最少被代表的群体之一，也是在隐私方面最脆弱的群体之一。尽管如此，针对这一人群的匿名化技术却很少受到关注。本文研究了为成人语音设计的语音匿名化技术应用于儿童语音时的效果，并建立了基线。研究使用了三个儿童数据集、六种匿名化方法以及客观和主观效用指标进行评估。结果表明，现有的成人系统仍然能够保护儿童的语音隐私，但效用下降更为严重。此外，主观研究表明，儿童语音的质量自动评估方法存在挑战，需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 儿童在语音技术中的代表性不足，且在隐私方面较为脆弱，但针对儿童的语音匿名化技术研究较少。因此，有必要评估为成人设计的语音匿名化技术在儿童语音中的适用性及效果。

Method: 使用三个儿童数据集、六种匿名化方法以及客观和主观效用指标对成人语音匿名化技术进行评估。

Result: 现有的成人语音匿名化系统能够保护儿童的语音隐私，但其效用下降程度较高。同时，自动评估方法在儿童语音质量评价上面临挑战。

Conclusion: 为成人设计的语音匿名化技术可以用于保护儿童语音隐私，但需要改进以减少效用损失。此外，儿童语音的质量评估需要进一步研究。

Abstract: Children are one of the most under-represented groups in speech technologies,
as well as one of the most vulnerable in terms of privacy. Despite this,
anonymization techniques targeting this population have received little
attention. In this study, we seek to bridge this gap, and establish a baseline
for the use of voice anonymization techniques designed for adult speech when
applied to children's voices. Such an evaluation is essential, as children's
speech presents a distinct set of challenges when compared to that of adults.
This study comprises three children's datasets, six anonymization methods, and
objective and subjective utility metrics for evaluation. Our results show that
existing systems for adults are still able to protect children's voice privacy,
but suffer from much higher utility degradation. In addition, our subjective
study displays the challenges of automatic evaluation methods for speech
quality in children's speech, highlighting the need for further research.

</details>


### [426] [The Folly of AI for Age Verification](https://arxiv.org/abs/2506.00038)
*Reid McIlroy-Young*

Main category: cs.CY

TL;DR: In the near future, deploying an AI system for age verification is folly.


<details>
  <summary>Details</summary>
Motivation: A governmental body will be asked to allow companies to use AI for age verification.

Method: Showing that other very similar systems (facial recognition and remote proctoring software) have similar issues despite years of efforts to mitigate their biases.

Result: The resulting system will both be easily circumvented and disproportionately misclassify minorities and low socioeconomic status users.

Conclusion: Deploying an AI system for age verification in the near future is not a wise choice due to technical limitations.

Abstract: In the near future a governmental body will be asked to allow companies to
use AI for age verification. If they allow it the resulting system will both be
easily circumvented and disproportionately misclassify minorities and low
socioeconomic status users. This is predictable by showing that other very
similar systems (facial recognition and remote proctoring software) have
similar issues despite years of efforts to mitigate their biases. These biases
are due to technical limitations both of the AI models themselves and the
physical hardware they are running on that will be difficult to overcome below
the cost of government ID-based age verification. Thus in, the near future,
deploying an AI system for age verification is folly.

</details>


### [427] [Risks of AI-driven product development and strategies for their mitigation](https://arxiv.org/abs/2506.00047)
*Jan Göpfert,Jann M. Weinand,Patrick Kuckertz,Noah Pflugradt,Jochen Linßen*

Main category: cs.CY

TL;DR: The paper discusses risks in AI-driven product development and proposes principles for safer practices.


<details>
  <summary>Details</summary>
Motivation: To address the risks associated with increasing reliance on non-human agents in automated product development.

Method: Outlines a set of principles emphasizing human oversight, accountability, and explainable design for safer AI-driven product development.

Result: Covers technical risks affecting product quality and safety, and sociotechnical risks impacting society.

Conclusion: This discussion aims to balance opportunities and risks in AI-driven product development without hindering progress.

Abstract: Humanity is progressing towards automated product development, a trend that
promises faster creation of better products and thus the acceleration of
technological progress. However, increasing reliance on non-human agents for
this process introduces many risks. This perspective aims to initiate a
discussion on these risks and appropriate mitigation strategies. To this end,
we outline a set of principles for safer AI-driven product development which
emphasize human oversight, accountability, and explainable design, among
others. The risk assessment covers both technical risks which affect product
quality and safety, and sociotechnical risks which affect society. While
AI-driven product development is still in its early stages, this discussion
will help balance its opportunities and risks without delaying essential
progress in understanding, norm-setting, and regulation.

</details>


### [428] [Prompt Engineer: Analyzing Skill Requirements in the AI Job Market](https://arxiv.org/abs/2506.00058)
*An Vu,Jonas Oppenlaender*

Main category: cs.CY

TL;DR: The study analyzed LinkedIn job postings and found that prompt engineering is a rare but distinct role requiring AI knowledge, prompt design, communication, and problem-solving skills.


<details>
  <summary>Details</summary>
Motivation: To understand the skills required for the new job role of prompt engineer and how prevalent these jobs are as large language models (LLMs) become more prominent.

Method: Analyzed 20,662 job postings on LinkedIn, including 72 prompt engineer positions, to identify the unique skill profile of prompt engineers.

Result: Prompt engineering jobs are rare (less than 0.5% of sampled job postings) but require specific skills such as AI knowledge (22.8%), prompt design (18.7%), communication (21.9%), and creative problem-solving (15.8%). These significantly differ from established roles like data scientists and machine learning engineers.

Conclusion: Prompt engineering is emerging as its own profession with a distinct skill set, aiding job seekers, employers, and educational institutions in understanding this new field.

Abstract: The rise of large language models (LLMs) has created a new job role: the
Prompt Engineer. Despite growing interest in this position, we still do not
fully understand what skills this new job role requires or how common these
jobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt
engineer positions, to learn more about this emerging role. We found that
prompt engineering is still rare (less than 0.5% of sampled job postings) but
has a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt
design skills (18.7%), good communication (21.9%), and creative problem-solving
(15.8%) skills. These requirements significantly differ from those of
established roles, such as data scientists and machine learning engineers,
showing that prompt engineering is becoming its own profession. Our findings
help job seekers, employers, and educational institutions in better
understanding the emerging field of prompt engineering.

</details>


### [429] [Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports](https://arxiv.org/abs/2506.00060)
*Sina Amirrajab,Volker Vehof,Michael Bietenbeck,Ali Yilmaz*

Main category: cs.CY

TL;DR: The study explores the application of privacy-preserving, locally-deployed open-source LLMs in extracting diagnostic information from CMR reports. Nine models were evaluated and most performed exceptionally well with Gemma2 leading at an F1 score of 0.98. Top four models outperformed a board-certified cardiologist.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of open-source LLMs in accurately analyzing CMR reports while preserving patient privacy.

Method: Evaluated nine open-source LLMs on their ability to classify CMR reports into diagnostic categories using metrics like accuracy, precision, recall, and F1 score.

Result: Most models performed excellently with Gemma2 achieving the highest F1 score of 0.98 followed by Qwen2.5:32B and DeepseekR1-32B with scores of 0.96 and 0.95 respectively. The top four models surpassed a board-certified cardiologist in all evaluation metrics.

Conclusion: Open-source LLMs can be effectively implemented in clinical settings for automated analysis of imaging reports, providing accurate, fast, and resource-efficient diagnostic categorization.

Abstract: Purpose: We investigated the utilization of privacy-preserving,
locally-deployed, open-source Large Language Models (LLMs) to extract
diagnostic information from free-text cardiovascular magnetic resonance (CMR)
reports. Materials and Methods: We evaluated nine open-source LLMs on their
ability to identify diagnoses and classify patients into various cardiac
diagnostic categories based on descriptive findings in 109 clinical CMR
reports. Performance was quantified using standard classification metrics
including accuracy, precision, recall, and F1 score. We also employed confusion
matrices to examine patterns of misclassification across models. Results: Most
open-source LLMs demonstrated exceptional performance in classifying reports
into different diagnostic categories. Google's Gemma2 model achieved the
highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B
with F1 scores of 0.96 and 0.95, respectively. All other evaluated models
attained average scores above 0.93, with Mistral and DeepseekR1-7B being the
only exceptions. The top four LLMs outperformed our board-certified
cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR
reports. Conclusion: Our findings demonstrate the feasibility of implementing
open-source, privacy-preserving LLMs in clinical settings for automated
analysis of imaging reports, enabling accurate, fast and resource-efficient
diagnostic categorization.

</details>


### [430] [Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs](https://arxiv.org/abs/2506.00072)
*Nariman Naderi,Zahra Atf,Peter R Lewis,Aref Mahjoub far,Seyed Amir Ahmad Safavi-Naini,Ali Soroush*

Main category: cs.CY

TL;DR: This paper explores how different prompt engineering techniques affect the accuracy and confidence of large language models in medical contexts, using a dataset of Persian board exam questions. It finds that while some techniques like Chain-of-Thought improve accuracy, they can also lead to overconfidence, indicating the need for better calibration especially in high-stakes medical applications.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of various prompt engineering methods on both the accuracy and confidence levels of large language models when applied to medical tasks, particularly using Persian board exam questions as a testbed.

Method: Evaluated five LLMs across 156 configurations with varying temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100) using AUC-ROC, Brier Score, and Expected Calibration Error metrics.

Result: Chain-of-Thought prompts enhanced accuracy but caused overconfidence; Emotional prompting further increased confidence levels without corresponding accuracy improvements. Smaller models underperformed while proprietary models were more accurate but still lacked calibrated confidence.

Conclusion: Prompt engineering in medical applications should focus not only on improving accuracy but also on managing uncertainty and ensuring proper calibration to avoid potential harm from overconfident predictions.

Abstract: This paper investigates how prompt engineering techniques impact both
accuracy and confidence elicitation in Large Language Models (LLMs) applied to
medical contexts. Using a stratified dataset of Persian board exam questions
across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,
Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These
configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles
(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales
(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error
(ECE) to evaluate alignment between confidence and actual performance.
Chain-of-Thought prompts improved accuracy but also led to overconfidence,
highlighting the need for calibration. Emotional prompting further inflated
confidence, risking poor decisions. Smaller models like Llama-3.1-8b
underperformed across all metrics, while proprietary models showed higher
accuracy but still lacked calibrated confidence. These results suggest prompt
engineering must address both accuracy and uncertainty to be effective in
high-stakes medical tasks.

</details>


### [431] [Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations](https://arxiv.org/abs/2506.00074)
*Daniele Barolo,Chiara Valentin,Fariba Karimi,Luis Galárraga,Gonzalo G. Méndez,Lisette Espín-Noboa*

Main category: cs.CY

TL;DR: 本文评估了六个开放权重LLM在物理专家推荐任务中的表现，发现模型存在不一致性和偏差问题，mixtral-8x7b表现最稳定，而llama3.1-70b变化最大。


<details>
  <summary>Details</summary>
Motivation: 研究六种LLM在推荐物理领域专家时的表现，以了解其一致性、事实性及潜在偏见。

Method: 使用来自美国物理学会和OpenAlex的真实数据作为基准，对六个LLM在五个任务（按领域、学科、时代、资历和学者对照的顶级专家）上的输出进行比较分析。

Result: 所有模型都存在不一致性和偏见；mixtral-8x7b输出最稳定，llama3.1-70b变化最大；许多模型有重复问题，部分模型格式错误较多；LLM通常能推荐真实科学家，但在特定查询中准确性下降，且倾向于推荐资深学者；存在性别、种族等代表性偏差。

Conclusion: 需要改进LLM以提供更可靠和平等的学术推荐。

Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b,
llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending
experts in physics across five tasks: top-k experts by field, influential
scientists by discipline, epoch, seniority, and scholar counterparts. The
evaluation examines consistency, factuality, and biases related to gender,
ethnicity, academic popularity, and scholar similarity. Using ground-truth data
from the American Physical Society and OpenAlex, we establish scholarly
benchmarks by comparing model outputs to real-world academic records. Our
analysis reveals inconsistencies and biases across all models. mixtral-8x7b
produces the most stable outputs, while llama3.1-70b shows the highest
variability. Many models exhibit duplication, and some, particularly gemma2-9b
and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real
scientists, but accuracy drops in field-, epoch-, and seniority-specific
queries, consistently favoring senior scholars. Representation biases persist,
replicating gender imbalances (reflecting male predominance),
under-representing Asian scientists, and over-representing White scholars.
Despite some diversity in institutional and collaboration networks, models
favor highly cited and productive scholars, reinforcing the rich-getricher
effect while offering limited geographical representation. These findings
highlight the need to improve LLMs for more reliable and equitable scholarly
recommendations.

</details>


### [432] [Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry](https://arxiv.org/abs/2506.00076)
*Andrew Cornfeld,Ashley Miller,Mercedes Mora-Figueroa,Kurt Samuels,Anthony Palomba*

Main category: cs.CY

TL;DR: This paper develops a machine learning framework combining NLP features from TV episode dialogues with traditional viewership data to improve viewership forecasting accuracy and provide interpretable metrics for industry professionals.


<details>
  <summary>Details</summary>
Motivation: To address the high financial risk in television programming decisions by enhancing the accuracy of episodic viewership forecasting beyond traditional methods.

Method: The study integrates NLP features, such as emotional tone, cognitive complexity, and narrative structure extracted from episode dialogues, with historical viewership data using SARIMAX, rolling XGBoost, and feature selection models. It also introduces a similarity scoring method based on Euclidean distance between aggregate dialogue vectors.

Result: NLP features contribute meaningful improvements in forecasting performance for some series when combined with prior viewership data. The framework reveals genre-specific performance and provides interpretable metrics for audience behavior.

Conclusion: The machine learning framework incorporating NLP features offers valuable insights into audience behavior and enhances viewership forecasting accuracy for certain TV series.

Abstract: Television networks face high financial risk when making programming
decisions, often relying on limited historical data to forecast episodic
viewership. This study introduces a machine learning framework that integrates
natural language processing (NLP) features from over 25000 television episodes
with traditional viewership data to enhance predictive accuracy. By extracting
emotional tone, cognitive complexity, and narrative structure from episode
dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,
and feature selection models. While prior viewership remains a strong baseline
predictor, NLP features contribute meaningful improvements for some series. We
also introduce a similarity scoring method based on Euclidean distance between
aggregate dialogue vectors to compare shows by content. Tested across diverse
genres, including Better Call Saul and Abbott Elementary, our framework reveals
genre-specific performance and offers interpretable metrics for writers,
executives, and marketers seeking data-driven insight into audience behavior.

</details>


### [433] [Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values](https://arxiv.org/abs/2506.00079)
*John P. Dickerson,Hadi Hosseini,Samarth Khanna,Leona Pierce*

Main category: cs.CY

TL;DR: 大型语言模型（LLMs）在高风险决策中的应用，如分配稀缺资源，引发了关于其与人类道德价值观一致性的关键问题。研究发现，LLMs在肾脏分配场景中表现出与人类价值观显著偏离，并倾向于做出确定性决策而非犹豫不决。然而，少量样本的低秩监督微调可以有效提高决策一致性和校准犹豫建模。这些结果表明，在道德/伦理领域中对LLMs进行显式对齐策略的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在高风险决策领域的快速集成，例如分配稀缺资源如捐赠器官等，迫切需要评估它们是否与人类的道德价值观相一致。

Method: 系统地评估多个知名LLMs在肾脏分配情景下的行为，与人类偏好进行对比，分析LLMs在属性优先级和决策方式上的差异，并探索通过少量样本的低秩监督微调来改善LLMs的决策一致性及犹豫建模的方法。

Result: LLMs在优先考虑各种属性时明显偏离人类价值观，且更倾向于做出确定性决策，而非像人类一样表现出犹豫。但通过低秩监督微调，使用少量样本即可有效提升决策一致性和校准犹豫建模。

Conclusion: 该研究强调了在道德和伦理领域中，对LLMs实施明确对齐策略的重要性。

Abstract: The rapid integration of Large Language Models (LLMs) in high-stakes
decision-making -- such as allocating scarce resources like donor organs --
raises critical questions about their alignment with human moral values. We
systematically evaluate the behavior of several prominent LLMs against human
preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark
deviations from human values in prioritizing various attributes, and ii) in
contrast to humans, LLMs rarely express indecision, opting for deterministic
decisions even when alternative indecision mechanisms (e.g., coin flipping) are
provided. Nonetheless, we show that low-rank supervised fine-tuning with few
samples is often effective in improving both decision consistency and
calibrating indecision modeling. These findings illustrate the necessity of
explicit alignment strategies for LLMs in moral/ethical domains.

</details>


### [434] [Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products](https://arxiv.org/abs/2506.00080)
*Stefan Pasch*

Main category: cs.CY

TL;DR: 通过分析G2.com上超过10万条AI产品的用户评论，研究发现了一系列与AI治理相关的主题，这些主题既包括技术领域也涵盖非技术领域。虽然与现有的AI治理和伦理框架有重叠，但也揭示了一些被忽视的方面，如项目管理、战略发展和客户互动。这表明需要更加以用户为中心、基于实证的AI治理方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI治理的重要性日益增加，许多高层次的框架和原则已经被提出，但它们可能无法完全涵盖在组织和操作环境中与AI系统交互的实际关注点。

Method: 采用自下而上的方法，利用BERTopic从G2.com上超过10万条AI产品用户评论中提取潜在主题，并识别出与AI治理最相关的主题。

Result: 揭示了一组多样化的治理相关主题，覆盖了技术与非技术领域，包括组织流程的不同阶段和AI价值链的各个环节。同时，发现了与现有框架重叠以及一些被忽视的领域。

Conclusion: 本研究强调了需要更多基于实证、以用户为中心的AI治理方法，以补充规范模型并更好地反映治理在实际应用中的展开方式。

Abstract: With the growing importance of AI governance, numerous high-level frameworks
and principles have been articulated by policymakers, institutions, and expert
communities to guide the development and application of AI. While such
frameworks offer valuable normative orientation, they may not fully capture the
practical concerns of those who interact with AI systems in organizational and
operational contexts. To address this gap, this study adopts a bottom-up
approach to explore how governance-relevant themes are expressed in user
discourse. Drawing on over 100,000 user reviews of AI products from G2.com, we
apply BERTopic to extract latent themes and identify those most semantically
related to AI governance. The analysis reveals a diverse set of
governance-relevant topics spanning both technical and non-technical domains.
These include concerns across organizational processes-such as planning,
coordination, and communication-as well as stages of the AI value chain,
including deployment infrastructure, data handling, and analytics. The findings
show considerable overlap with institutional AI governance and ethics
frameworks on issues like privacy and transparency, but also surface overlooked
areas such as project management, strategy development, and customer
interaction. This highlights the need for more empirically grounded,
user-centered approaches to AI governance-approaches that complement normative
models by capturing how governance unfolds in applied settings. By
foregrounding how governance is enacted in practice, this study contributes to
more inclusive and operationally grounded approaches to AI governance and
digital policy.

</details>


### [435] [TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents](https://arxiv.org/abs/2506.00089)
*Hyundong Jin,Sicheol Sung,Shinwoo Park,SeungYeop Baik,Yo-Sub Han*

Main category: cs.CY

TL;DR: The paper addresses the issue of over-reliance on LLMs by proposing TRAPDOC, a framework that injects imperceptible phantom tokens into documents to produce incorrect but plausible outputs from LLMs. This method aims to discourage misuse and promote responsible engagement with LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work stems from the growing societal concern of over-reliance on LLMs, where users delegate tasks without meaningful engagement, leading to potential misuse.

Method: The method involves injecting imperceptible phantom tokens into documents, causing LLMs to generate incorrect but plausible outputs. This technique is implemented in the TRAPDOC framework.

Result: Empirical evaluations demonstrate the effectiveness of TRAPDOC on proprietary LLMs compared to several baselines.

Conclusion: TRAPDOC provides a strong foundation for encouraging more responsible and thoughtful interaction with language models.

Abstract: The reasoning, writing, text-editing, and retrieval capabilities of
proprietary large language models (LLMs) have advanced rapidly, providing users
with an ever-expanding set of functionalities. However, this growing utility
has also led to a serious societal concern: the over-reliance on LLMs. In
particular, users increasingly delegate tasks such as homework, assignments, or
the processing of sensitive documents to LLMs without meaningful engagement.
This form of over-reliance and misuse is emerging as a significant social
issue. In order to mitigate these issues, we propose a method injecting
imperceptible phantom tokens into documents, which causes LLMs to generate
outputs that appear plausible to users but are in fact incorrect. Based on this
technique, we introduce TRAPDOC, a framework designed to deceive over-reliant
LLM users. Through empirical evaluation, we demonstrate the effectiveness of
our framework on proprietary LLMs, comparing its impact against several
baselines. TRAPDOC serves as a strong foundation for promoting more responsible
and thoughtful engagement with language models. Our code is available at
https://github.com/jindong22/TrapDoc.

</details>


### [436] [Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use](https://arxiv.org/abs/2506.00094)
*Konstantin Aal,Tanja Aal,Vasil Navumau,David Unbehaun,Claudia Müller,Volker Wulf,Sarah Rüller*

Main category: cs.CY

TL;DR: 这篇论文探讨了在个人和职业工作流程中整合人工智能（AI）的情感、伦理和实践层面，重点关注作为被AI增强的人类（'c(ai)borg'）感到内疚的概念。通过自传民族志方法，作者反思了使用AI工具一年的经验，从最初的内疚和抗拒转变为通过技能建设和透明度获得的赋权。关键发现强调了基础学术技能、高级AI素养以及诚实地处理AI结果的重要性。论文提倡将AI作为协作伙伴公开接纳的未来愿景，促进创新与公平，同时解决获取与能动性问题。通过将内疚重新定义为成长，文章呼吁对AI整合采取深思熟虑且包容的方法。


<details>
  <summary>Details</summary>
Motivation: 受到唐娜·哈拉维（Donna Haraway）的《赛博格宣言》启发，探索AI如何挑战传统意义上关于创造力、原创性和知识劳动的概念，并研究人类与AI结合后的情感和伦理影响。

Method: 采用自传民族志方法，作者回顾并分析了他们在一年期间使用AI工具的经历和感受。

Result: 揭示了从初始阶段的内疚和抗拒到通过技能发展和透明度实现赋权的转变过程，强调了基础学术技能、AI素养及诚实对待AI结果的重要性。

Conclusion: 倡导将AI视为协作伙伴，通过重新定义内疚为成长，呼吁以深思熟虑和包容的方式整合AI，推动创新与公平，同时关注获取与能动性问题。

Abstract: This paper explores the emotional, ethical and practical dimensions of
integrating Artificial Intelligence (AI) into personal and professional
workflows, focusing on the concept of feeling guilty as a 'c(ai)borg' - a human
augmented by AI. Inspired by Donna Haraway's Cyborg Manifesto, the study
explores how AI challenges traditional notions of creativity, originality and
intellectual labour. Using an autoethnographic approach, the authors reflect on
their year-long experiences with AI tools, revealing a transition from initial
guilt and reluctance to empowerment through skill-building and transparency.
Key findings highlight the importance of basic academic skills, advanced AI
literacy and honest engagement with AI results. The c(ai)borg vision advocates
for a future where AI is openly embraced as a collaborative partner, fostering
innovation and equity while addressing issues of access and agency. By
reframing guilt as growth, the paper calls for a thoughtful and inclusive
approach to AI integration.

</details>


### [437] [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095)
*Yuchong Li,Xiaojun Zeng,Chihua Fang,Jian Yang,Lei Zhang*

Main category: cs.CY

TL;DR: Hepato-pancreato-biliary (HPB) disorders are a significant global health challenge. Although large language models (LLMs) perform well in general medical question-answering tasks, they lack HPB coverage and clinical case handling. This study establishes an HPB disease evaluation benchmark called ClinBench-HBP, revealing the limitations of current LLMs in HPB diseases and emphasizing the need for future medical LLMs to handle complex clinical diagnostics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of HPB coverage and clinical cases in current evaluation benchmarks for large language models in medical question-answering tasks.

Method: Systematically establish an HPB disease evaluation benchmark named ClinBench-HBP, comprising 3,535 closed-ended multiple-choice questions and 337 open-ended real diagnosis cases covering all main categories and subcategories of HPB diseases according to ICD-10. The questions are curated from public datasets and synthesized data, while clinical cases are collected from medical journals, case-sharing platforms, and hospitals.

Result: Evaluation shows that commercial LLMs perform well on medical exam questions but show substantial performance degradation on HPB diagnosis tasks, particularly on complex, inpatient clinical cases. Medical LLMs also exhibit limited generalizability to HPB diseases.

Conclusion: Current LLMs have critical limitations in handling HPB diseases, highlighting the need for future medical LLMs to focus on real, complex clinical diagnostics rather than simple medical exam questions.

Abstract: Hepato-pancreato-biliary (HPB) disorders represent a global public health
challenge due to their high morbidity and mortality. Although large language
models (LLMs) have shown promising performance in general medical
question-answering tasks, the current evaluation benchmarks are mostly derived
from standardized examinations or manually designed questions, lacking HPB
coverage and clinical cases. To address these issues, we systematically
eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended
multiple-choice questions and 337 open-ended real diagnosis cases, which
encompasses all the 33 main categories and 465 subcategories of HPB diseases
defined in the International Statistical Classification of Diseases, 10th
Revision (ICD-10). The multiple-choice questions are curated from public
datasets and synthesized data, and the clinical cases are collected from
prestigious medical journals, case-sharing platforms, and collaborating
hospitals. By evalauting commercial and open-source general and medical LLMs on
our established benchmark, namely ClinBench-HBP, we find that while commercial
LLMs perform competently on medical exam questions, they exhibit substantial
performance degradation on HPB diagnosis tasks, especially on complex,
inpatient clinical cases. Those medical LLMs also show limited generalizability
to HPB diseases. Our results reveal the critical limitations of current LLMs in
the domain of HPB diseases, underscoring the imperative need for future medical
LLMs to handle real, complex clinical diagnostics rather than simple medical
exam questions. The benchmark will be released at the homepage.

</details>


### [438] [The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features](https://arxiv.org/abs/2506.00203)
*Omid Reza Abbasi,Franz Welscher,Georg Weinberger,Johannes Scholz*

Main category: cs.CY

TL;DR: 尽管大语言模型（LLMs）在地理任务中表现出一定能力，但其准确性和可靠性仍存在不一致性。本研究比较了GPT-4o和Gemini 2.0 Flash在三个关键地理空间任务中的表现，发现两者在不同任务中的表现各有优劣，但均未达到高精度的奥地利联邦州重建。因此，需要通过地理信息微调来提升LLMs在GIScience和Geoinformatics中的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，对其在提供事实信息方面的可信度的关注日益增加，特别是在准确表示地理世界方面。因此，有必要评估LLMs在地理空间任务中的表现及可信度。

Method: 本研究选取了GPT-4o和Gemini 2.0 Flash两个模型，并对它们进行了三个关键地理空间任务的评估：地理编码（geocoding）、高度估算（elevation estimation）和反向地理编码（reverse geocoding）。

Result: 1. 在地理编码任务中，GPT-4o误差较大，而Gemini 2.0 Flash虽然更精确但存在显著系统偏差。
2. 高度估算任务中，两模型均低估了奥地利的高度，但捕捉到了整体地形趋势，Gemini 2.0 Flash在东部地区表现更好。
3. 反向地理编码任务中，Gemini 2.0 Flash在总体准确率和F1分数上优于GPT-4o，但在联邦州重建任务中仍然存在持续误分类现象。

Conclusion: LLMs可以在一定程度上近似地理信息，但其准确性和可靠性并不一致。为提高LLMs在GIScience和Geoinformatics中的实用性，需要使用地理信息对其进行微调。

Abstract: As large language models (LLMs) continue to evolve, questions about their
trustworthiness in delivering factual information have become increasingly
important. This concern also applies to their ability to accurately represent
the geographic world. With recent advancements in this field, it is relevant to
consider whether and to what extent LLMs' representations of the geographical
world can be trusted. This study evaluates the performance of GPT-4o and Gemini
2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and
reverse geocoding. In the geocoding task, both models exhibited systematic and
random errors in estimating the coordinates of St. Anne's Column in Innsbruck,
Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash
demonstrating more precision but a significant systematic offset. For elevation
estimation, both models tended to underestimate elevations across Austria,
though they captured overall topographical trends, and Gemini 2.0 Flash
performed better in eastern regions. The reverse geocoding task, which involved
identifying Austrian federal states from coordinates, revealed that Gemini 2.0
Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating
better consistency across regions. Despite these findings, neither model
achieved an accurate reconstruction of Austria's federal states, highlighting
persistent misclassifications. The study concludes that while LLMs can
approximate geographic information, their accuracy and reliability are
inconsistent, underscoring the need for fine-tuning with geographical
information to enhance their utility in GIScience and Geoinformatics.

</details>


### [439] [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)
*Hayoung Jung,Shravika Mittal,Ananya Aatreya,Navreet Kaur,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CY

TL;DR: The paper presents the first large-scale study of OUD-related myths on YouTube, introduces MythTriage for efficient myth labeling, and analyzes search results and recommendations to uncover how myths persist.


<details>
  <summary>Details</summary>
Motivation: To understand the prevalence of misinformation in health topics online, particularly opioid-use disorder (OUD), which is a leading cause of death in the U.S. and an understudied topic.

Method: Validated 8 pervasive myths with clinical experts, released an expert-labeled video dataset, introduced MythTriage - an efficient triage pipeline using a lightweight model for routine cases and deferring harder ones to a large language model (LLM).

Result: MythTriage achieves up to 0.86 macro F1-score while reducing annotation time and financial cost by over 76% compared to experts and full LLM labeling. Analyzed 2.9K search results and 343K recommendations, uncovering insights on how myths persist on YouTube.

Conclusion: This study provides actionable insights for public health and platform moderation regarding the persistence of OUD-related myths on YouTube.

Abstract: Understanding the prevalence of misinformation in health topics online can
inform public health policies and interventions. However, measuring such
misinformation at scale remains a challenge, particularly for high-stakes but
understudied topics like opioid-use disorder (OUD)--a leading cause of death in
the U.S. We present the first large-scale study of OUD-related myths on
YouTube, a widely-used platform for health information. With clinical experts,
we validate 8 pervasive myths and release an expert-labeled video dataset. To
scale labeling, we introduce MythTriage, an efficient triage pipeline that uses
a lightweight model for routine cases and defers harder ones to a
high-performing, but costlier, large language model (LLM). MythTriage achieves
up to 0.86 macro F1-score while estimated to reduce annotation time and
financial cost by over 76% compared to experts and full LLM labeling. We
analyze 2.9K search results and 343K recommendations, uncovering how myths
persist on YouTube and offering actionable insights for public health and
platform moderation.

</details>


### [440] [Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety](https://arxiv.org/abs/2506.00415)
*Matthew Brophy*

Main category: cs.CY

TL;DR: This paper argues that the Method of Wide Reflective Equilibrium (MWRE) is a suitable framework for aligning large language models (LLMs) with human values, offering enhancements in revisability, legitimacy, and ethical grounding.


<details>
  <summary>Details</summary>
Motivation: As LLMs become increasingly powerful, it is crucial to ensure they are beneficial, safe, and aligned with human values. Current alignment techniques have limitations which MWRE can address.

Method: The paper uses MWRE as a framework to critically analyze current LLM alignment efforts, highlighting its potential to improve dynamic revisability, procedural legitimacy, and overall ethical grounding.

Result: MWRE offers a more robust path to justification than prevailing methods and better represents the intricate reality of LLM alignment.

Conclusion: MWRE serves as a valuable heuristic for guiding the future development of more ethically sound and justifiably aligned AI systems.

Abstract: As large language models (LLMs) become more powerful and pervasive across
society, ensuring these systems are beneficial, safe, and aligned with human
values is crucial. Current alignment techniques, like Constitutional AI (CAI),
involve complex iterative processes. This paper argues that the Method of Wide
Reflective Equilibrium (MWRE) -- a well-established coherentist moral
methodology -- offers a uniquely apt framework for understanding current LLM
alignment efforts. Moreover, this methodology can substantively augment these
processes by providing concrete pathways for improving their dynamic
revisability, procedural legitimacy, and overall ethical grounding. Together,
these enhancements can help produce more robust and ethically defensible
outcomes. MWRE, emphasizing the achievement of coherence between our considered
moral judgments, guiding moral principles, and relevant background theories,
arguably better represents the intricate reality of LLM alignment and offers a
more robust path to justification than prevailing foundationalist models or
simplistic input-output evaluations. While current methods like CAI bear a
structural resemblance to MWRE, they often lack its crucial emphasis on
dynamic, bi-directional revision of principles and the procedural legitimacy
derived from such a process. While acknowledging various disanalogies (e.g.,
consciousness, genuine understanding in LLMs), the paper demonstrates that MWRE
serves as a valuable heuristic for critically analyzing current alignment
efforts and for guiding the future development of more ethically sound and
justifiably aligned AI systems.

</details>


### [441] [Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education](https://arxiv.org/abs/2506.00057)
*Yiwei Sun*

Main category: cs.CY

TL;DR: This paper uses hierarchical Bayesian modeling to analyze student response data in an engineering course, identifying difficult topics and distinct learner subgroups, providing educators with interpretable metrics for personalized teaching.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying difficult topics and supporting diverse student needs in entry-level engineering courses.

Method: Hierarchical Bayesian modeling was applied to a large-scale dataset from an undergraduate Statics course to quantify skill difficulty and individual student abilities.

Result: Clear patterns of skill mastery were identified, along with distinct student subgroups based on learning trajectories. Some concepts consistently challenge students while others are easily mastered.

Conclusion: The hierarchical Bayesian method provides reliable, interpretable metrics that enable personalized teaching strategies to improve student engagement and success.

Abstract: Educators teaching entry-level university engineering modules face the
challenge of identifying which topics students find most difficult and how to
support diverse student needs effectively. This study demonstrates a rigorous
yet interpretable statistical approach -- hierarchical Bayesian modeling --
that leverages detailed student response data to quantify both skill difficulty
and individual student abilities. Using a large-scale dataset from an
undergraduate Statics course, we identified clear patterns of skill mastery and
uncovered distinct student subgroups based on their learning trajectories. Our
analysis reveals that certain concepts consistently present challenges,
requiring targeted instructional support, while others are readily mastered and
may benefit from enrichment activities. Importantly, the hierarchical Bayesian
method provides educators with intuitive, reliable metrics without sacrificing
predictive accuracy. This approach allows for data-informed decisions, enabling
personalized teaching strategies to improve student engagement and success. By
combining robust statistical methods with clear interpretability, this study
equips educators with actionable insights to better support diverse learner
populations.

</details>


### [442] [Explainable AI Systems Must Be Contestable: Here's How to Make It Happen](https://arxiv.org/abs/2506.01662)
*Catarina Moreira,Anna Palatkina,Dacia Braca,Dylan M. Walsh,Peter J. Leihn,Fang Chen,Nina C. Hubig*

Main category: cs.CY

TL;DR: The paper provides the first formal definition of contestability in XAI, a modular framework with by-design and post-hoc mechanisms, and the Contestability Assessment Scale to guide practitioners in meeting regulatory requirements.


<details>
  <summary>Details</summary>
Motivation: Contestability has become a necessary safeguard in AI regulations focused on system safety, but it lacks a clear definition or algorithmic guarantee in XAI. Practitioners need concrete guidance to meet these requirements.

Method: The authors conducted a systematic literature review to define contestability in XAI aligned with stakeholder needs and regulations. They introduced a modular framework encompassing human-centered interfaces, technical architectures, legal processes, and organizational workflows, along with the Contestability Assessment Scale as a composite metric.

Result: Case studies across various domains showed where current systems are lacking and demonstrated how the proposed framework can lead to targeted improvements.

Conclusion: This work transforms the concept of contestability from regulatory theory into a practical toolset for practitioners to implement genuine recourse and accountability in AI systems.

Abstract: As AI regulations around the world intensify their focus on system safety,
contestability has become a mandatory, yet ill-defined, safeguard. In XAI,
"contestability" remains an empty promise: no formal definition exists, no
algorithm guarantees it, and practitioners lack concrete guidance to satisfy
regulatory requirements. Grounded in a systematic literature review, this paper
presents the first rigorous formal definition of contestability in explainable
AI, directly aligned with stakeholder requirements and regulatory mandates. We
introduce a modular framework of by-design and post-hoc mechanisms spanning
human-centered interfaces, technical architectures, legal processes, and
organizational workflows. To operationalize our framework, we propose the
Contestability Assessment Scale, a composite metric built on more than twenty
quantitative criteria. Through multiple case studies across diverse application
domains, we reveal where state-of-the-art systems fall short and show how our
framework drives targeted improvements. By converting contestability from
regulatory theory into a practical framework, our work equips practitioners
with the tools to embed genuine recourse and accountability into AI systems.

</details>


### [443] [Systematic Hazard Analysis for Frontier AI using STPA](https://arxiv.org/abs/2506.01782)
*Simon Mylius*

Main category: cs.CY

TL;DR: STPA can enhance the safety assurance of frontier AI systems by identifying missed causal factors, improving robustness and supporting scalability.


<details>
  <summary>Details</summary>
Motivation: Frontier AI companies have published safety frameworks but lack detailed structured approaches to hazard identification and analysis. STPA offers a systematic methodology for this purpose.

Method: Apply STPA to the threat model and scenario from 'A Sketch of an AI Control Safety Case', deriving Unsafe Control Actions and exploring Loss Scenarios.

Result: STPA identifies causal factors that unstructured methodologies may miss, improving the robustness of safety assurance for AI systems.

Conclusion: STPA could complement existing AI governance techniques, increase safety assurance, and support scalability in analysis.

Abstract: All of the frontier AI companies have published safety frameworks where they
define capability thresholds and risk mitigations that determine how they will
safely develop and deploy their models. Adoption of systematic approaches to
risk modelling, based on established practices used in safety-critical
industries, has been recommended, however frontier AI companies currently do
not describe in detail any structured approach to identifying and analysing
hazards. STPA (Systems-Theoretic Process Analysis) is a systematic methodology
for identifying how complex systems can become unsafe, leading to hazards. It
achieves this by mapping out controllers and controlled processes then
analysing their interactions and feedback loops to understand how harmful
outcomes could occur (Leveson & Thomas, 2018). We evaluate STPA's ability to
broaden the scope, improve traceability and strengthen the robustness of safety
assurance for frontier AI systems. Applying STPA to the threat model and
scenario described in 'A Sketch of an AI Control Safety Case' (Korbak et al.,
2025), we derive a list of Unsafe Control Actions. From these we select a
subset and explore the Loss Scenarios that lead to them if left unmitigated. We
find that STPA is able to identify causal factors that may be missed by
unstructured hazard analysis methodologies thereby improving robustness. We
suggest STPA could increase the safety assurance of frontier AI when used to
complement or check coverage of existing AI governance techniques including
capability thresholds, model evaluations and emergency procedures. The
application of a systematic methodology supports scalability by increasing the
proportion of the analysis that could be conducted by LLMs, reducing the burden
on human domain experts.

</details>


### [444] [Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act](https://arxiv.org/abs/2506.01931)
*Rui-Jie Yew,Bill Marino,Suresh Venkatasubramanian*

Main category: cs.CY

TL;DR: The paper explores avoision strategies that firms may use to minimize the regulatory burden of the EU AI Act, presenting a framework and taxonomy for understanding these tactics.


<details>
  <summary>Details</summary>
Motivation: To provide an adversarial framework for analyzing potential 'avoision' strategies that firms might adopt in response to the EU AI Act.

Method: Developing a taxonomy and organizing avoision strategies into three tiers based on AIA exposure, specifying organizational and technological forms of avoision within each tier.

Result: A framework and taxonomy for understanding avoision strategies across different levels of AIA exposure is presented.

Conclusion: This adversarial framework can be used for 'red teaming' the AIA and upcoming AI regulations.

Abstract: The shape of AI regulation is beginning to emerge, most prominently through
the EU AI Act (the "AIA"). By 2027, the AIA will be in full effect, and firms
are starting to adjust their behavior in light of this new law. In this paper,
we present a framework and taxonomy for reasoning about "avoision" -- conduct
that walks the line between legal avoidance and evasion -- that firms might
engage in so as to minimize the regulatory burden the AIA poses. We organize
these avoision strategies around three "tiers" of increasing AIA exposure that
regulated entities face depending on: whether their activities are (1) within
scope of the AIA, (2) exempted from provisions of the AIA, or are (3) placed in
a category with higher regulatory scrutiny. In each of these tiers and for each
strategy, we specify the organizational and technological forms through which
avoision may manifest. Our goal is to provide an adversarial framework for "red
teaming" the AIA and AI regulation on the horizon.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [445] [PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using Multicenter Lung Cancer Histopathology Image Dataset](https://arxiv.org/abs/2506.00096)
*Liangrui Pan,Qingchun Liang,Shen Zhao,Songqing Fan,Shaoliang Peng*

Main category: q-bio.GN

TL;DR: 本研究构建了一个名为PathGene的数据集，结合病理图像与基因测序报告，用于预测肺癌患者的基因突变、亚型、外显子位置及肿瘤突变负荷（TMB）。通过基准测试11种多实例学习方法，该研究为早期肺癌遗传筛查和个性化治疗提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 由于医疗资源的区域差异和基因组检测成本高昂，利用人工智能从常规组织病理学图像中推断基因突变和外显子变异可以极大地促进精准治疗。然而，现有深度学习模型在预测关键基因突变方面的表现仍不理想，且主要局限于早期筛查任务。

Method: 研究人员构建了PathGene数据集，包含来自1576名患者（第二湘雅医院）和448名TCGA-LUAD患者的病理图像及其对应的下一代测序报告。数据集连接了全切片图像与驱动基因突变状态、突变亚型、外显子和肿瘤突变负荷（TMB）状态。随后，使用11种多实例学习方法对突变、亚型、外显子和TMB预测任务进行基准测试。

Result: 实验结果表明，这些方法为肺癌患者的早期遗传筛查提供了有价值的替代方案，并有助于临床医生快速制定个性化的精准靶向治疗计划。

Conclusion: PathGene数据集及其相关方法为基于病理图像的生物标志物预测模型开发提供了重要支持，推动了精准肿瘤学的发展。

Abstract: Accurately predicting gene mutations, mutation subtypes and their exons in
lung cancer is critical for personalized treatment planning and prognostic
assessment. Faced with regional disparities in medical resources and the high
cost of genomic assays, using artificial intelligence to infer these mutations
and exon variants from routine histopathology images could greatly facilitate
precision therapy. Although some prior studies have shown that deep learning
can accelerate the prediction of key gene mutations from lung cancer pathology
slides, their performance remains suboptimal and has so far been limited mainly
to early screening tasks. To address these limitations, we have assembled
PathGene, which comprises histopathology images paired with next-generation
sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central
South University, and 448 TCGA-LUAD patients. This multi-center dataset links
whole-slide images to driver gene mutation status, mutation subtypes, exon, and
tumor mutational burden (TMB) status, with the goal of leveraging pathology
images to predict mutations, subtypes, exon locations, and TMB for early
genetic screening and to advance precision oncology. Unlike existing datasets,
we provide molecular-level information related to histopathology images in
PathGene to facilitate the development of biomarker prediction models. We
benchmarked 11 multiple-instance learning methods on PathGene for mutation,
subtype, exon, and TMB prediction tasks. These experimental methods provide
valuable alternatives for early genetic screening of lung cancer patients and
assisting clinicians to quickly develop personalized precision targeted
treatment plans for patients. Code and data are available at
https://github.com/panliangrui/NIPS2025/.

</details>


### [446] [Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A Transformer-Based Ensemble Approach with Monte Carlo Dropout](https://arxiv.org/abs/2506.00662)
*Taeho Jo,Eun Hye Lee,Alzheimer's Disease Sequencing Project*

Main category: q-bio.GN

TL;DR: In this paper, researchers developed a transformer-based ensemble model (TrUE-Net) with Monte Carlo Dropout for Alzheimer's disease classification from whole-genome sequencing data. By excluding uncertain cases identified through uncertainty estimation, the model improved accuracy and F1 score significantly.


<details>
  <summary>Details</summary>
Motivation: Alzheimer's disease is genetically complex, making robust classification from genomic data challenging.

Method: The team developed TrUE-Net, which combines a transformer preserving SNP sequence structure and a concurrent random forest using flattened genotypes. They applied Monte Carlo Dropout for uncertainty estimation to separate samples into certain and uncertain groups.

Result: From 1050 individuals analyzed, overall accuracy was 0.6514 and AUC was 0.6636. Excluding the uncertain group improved accuracy by 10.24% and F1 score by 23.62%.

Conclusion: Monte Carlo Dropout-driven uncertainty helps identify ambiguous cases that may require further clinical evaluation, thus enhancing reliability in AD genomic classification.

Abstract: INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating
robust classification from genomic data. METHODS: We developed a
transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for
uncertainty estimation in AD classification from whole-genome sequencing (WGS).
We combined a transformer that preserves single-nucleotide polymorphism (SNP)
sequence structure with a concurrent random forest using flattened genotypes.
An uncertainty threshold separated samples into an uncertain (high-variance)
group and a more certain (low-variance) group. RESULTS: We analyzed 1050
individuals, holding out half for testing. Overall accuracy and area under the
receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636,
respectively. Excluding the uncertain group improved accuracy from 0.6263 to
0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase).
DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous
cases that may require further clinical evaluation, thus improving reliability
in AD genomic classification.

</details>


### [447] [GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes](https://arxiv.org/abs/2506.01456)
*Lina Qin,Cheng Zhu,Chuqi Zhou,Yukun Huang,Jiayi Zhu,Ping Liang,Jinju Wang,Yixing Huang,Cheng Luo,Dezhong Yao,Ying Tan*

Main category: q-bio.GN

TL;DR: 为了解决当前深度学习方法在阿尔茨海默病（AD）多模态数据融合中的不足，本文提出了动态多模态角色交换网络（GenDMR）。该网络通过编码单核苷酸多态性（SNPs）的空间组织、多实例注意力模块、主导模态选择模块和对比自蒸馏模块，增强了遗传特征的表示，并实现了基于主导和辅助模态的双向协同更新机制。在ADNI公开数据集上，GenDMR达到了最先进的性能，并确认了12个与AD相关的潜在高风险基因。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在选择和编码遗传信息方面讨论不足，并且由于影像学特征相比遗传特征具有显著优越的分类价值，许多研究在多模态融合中过于强调影像学特征，削弱了遗传特征的独特价值。

Method: 提出了一种新的方法来编码SNPs的空间组织，以增强其基因组上下文的表示；引入了多实例注意力模块，用于自适应量化SNPs和脑区的疾病风险并提高模型可解释性；设计了主导模态选择模块和对比自蒸馏模块，结合两者实现动态教师-学生角色交换机制，促进不同模态数据的双向协同更新。

Result: GenDMR在ADNI公开数据集上达到了最先进的性能，并可视化了对不同SNPs的注意力，确认了12个与AD相关的潜在高风险基因，包括经典的APOE和最近备受关注的重要风险基因。

Conclusion: GenDMR展示了其在探索AD遗传特征方面的可解释分析能力，为多模态数据融合技术的发展提供了新见解和视角。

Abstract: Recent studies have shown that integrating multimodal data fusion techniques
for imaging and genetic features is beneficial for the etiological analysis and
predictive diagnosis of Alzheimer's disease (AD). However, there are several
critical flaws in current deep learning methods. Firstly, there has been
insufficient discussion and exploration regarding the selection and encoding of
genetic information. Secondly, due to the significantly superior classification
value of AD imaging features compared to genetic features, many studies in
multimodal fusion emphasize the strengths of imaging features, actively
mitigating the influence of weaker features, thereby diminishing the learning
of the unique value of genetic features. To address this issue, this study
proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we
develop a novel approach to encode the spatial organization of single
nucleotide polymorphisms (SNPs), enhancing the representation of their genomic
context. Additionally, to adaptively quantify the disease risk of SNPs and
brain region, we propose a multi-instance attention module to enhance model
interpretability. Furthermore, we introduce a dominant modality selection
module and a contrastive self-distillation module, combining them to achieve a
dynamic teacher-student role exchange mechanism based on dominant and auxiliary
modalities for bidirectional co-updating of different modal data. Finally,
GenDMR achieves state-of-the-art performance on the ADNI public dataset and
visualizes attention to different SNPs, focusing on confirming 12 potential
high-risk genes related to AD, including the most classic APOE and recently
highlighted significant risk genes. This demonstrates GenDMR's interpretable
analytical capability in exploring AD genetic features, providing new insights
and perspectives for the development of multimodal data fusion techniques.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [448] [Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization](https://arxiv.org/abs/2506.00002)
*Hao Mark Chen,Zehuan Zhang,Wanru Zhao,Nicholas Lane,Hongxiang Fan*

Main category: cs.AR

TL;DR: 近年来，随着AI技术在电子设计自动化中的应用增加，大型语言模型（LLMs）在辅助硬件设计生成方面引起了广泛关注。然而，LLM生成的硬件设计质量仍不足以满足实际部署需求。本文提出了一种两阶段框架，通过探索分散式训练和个人化推理来应对数据可用性、质量和推理效率的挑战。实验结果表明，该框架可显著提高模型的硬件设计生成能力，实现语义准确率提升33%~50%，速度提升2.3倍。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在硬件设计生成领域取得进展，但其生成的硬件设计质量仍不足以满足实际部署需求，主要受限于数据可用性、数据质量和推理时效率不足等问题。

Method: 本文提出了一种两阶段框架：
1. 第一阶段采用分层分散式训练机制，利用私人领域设计源，解决数据共享限制，并通过用户定义的指标优化低质量数据对模型聚合的影响。
2. 第二阶段专注于客户端个性化，引入新度量Trueput以优化推理时间和自定义采样策略，从而提升生成速度和质量。

Result: 实验结果表明，该框架在经典和量子基准测试中均表现出色，能够显著提高模型的硬件设计生成能力。具体来说，根据生成任务难度不同，语义准确率提升了33%~50%，速度提升了2.3倍。

Conclusion: 本文提出的两阶段框架通过分散式训练和个人化推理有效解决了LLM辅助硬件设计生成中的关键挑战，显著提高了生成质量和效率，为现有方法提供了正交增强。

Abstract: Recent years have witnessed a significant increase in the adoption of AI
techniques to enhance electronic design automation. In particular, the
emergence of Large Language Models (LLMs) has sparked significant interest in
LLM-assisted hardware design generation, spanning applications from classical
digital circuits to quantum computing. Despite substantial progress in this
direction, the quality of LLM-generated hardware design still cannot meet the
requirements for practical deployment. In this work, we identify three critical
challenges hindering the development of LLM-assisted hardware design
generation: 1) limited data availability, 2) varied data quality, 3) inadequate
inference-time efficiency. To address these fundamental challenges, this paper
introduces a two-stage framework for AI-assisted hardware design by exploring
decentralized training and personalized inference. In the first stage, we
propose to harness private domain design sources through a hierarchical
decentralized training mechanism that addresses data-sharing constraints. To
mitigate the impact of low-quality data, we identify optimization opportunities
in hardware generation tasks, using user-defined metrics for model aggregation.
The second stage focuses on client personalization to enhance both speed and
quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware
generation efficiency. To optimize Trueput, we implement personalized
inference-time acceleration and customized sampling strategies. Evaluating both
classical and quantum benchmarks, our experimental results demonstrate that the
proposed two-stage framework can significantly improve the model capability for
hardware design generation. As orthogonal enhancements to existing methods, our
framework can achieve $33\% \sim 50\%$ semantic accuracy improvement and $2.3$
times speedup, depending on the difficulty of the generation tasks.

</details>


### [449] [Rapid yet accurate Tile-circuit and device modeling for Analog In-Memory Computing](https://arxiv.org/abs/2506.00004)
*J. Luquin,C. Mackin,S. Ambrogio,A. Chen,F. Baldi,G. Miralles,M. J. Rasch,J. Büchel,M. Lalwani,W. Ponghiran,P. Solomon,H. Tsai,G. W. Burr,P. Narayanan*

Main category: cs.AR

TL;DR: Analog In-Memory Compute (AIMC) can significantly enhance the energy efficiency of deep learning, but analog-domain non-idealities within the 'Tiles' performing Matrix-Vector Multiply (MVM) operations can degrade neural-network task accuracy. This paper quantifies these impacts and develops a mathematical model for MAC operations mapped to analog tiles, which can predict MVM tile-outputs accurately and rapidly. Additionally, it introduces a statistical model of PCM read noise and integrates these effects into a PyTorch-based framework to evaluate their impact on Transformer networks like BERT and ALBERT.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of how analog-domain device and circuit non-idealities affect the accuracy of neural networks when using AIMC for deep learning tasks, aiming to improve the resilience of large neural networks deployed on AIMC hardware.

Method: The method involves developing a mathematical model for MAC operations in analog tiles, incorporating instantaneous-current IR-drop and ADC quantization effects. A statistical model of PCM read noise is also derived from experimental measurements. These models are integrated into a PyTorch-based framework to assess the accuracy impact on Transformer networks.

Result: Hardware-aware fine-tuning with simple Gaussian noise improves resilience against ADC quantization and PCM read noise, but is less effective against IR-drop due to its non-linear and dynamic nature.

Conclusion: More complex training approaches incorporating advanced models like the Tile-circuit model will be essential for effectively deploying large neural networks onto AIMC hardware.

Abstract: Analog In-Memory Compute (AIMC) can improve the energy efficiency of Deep
Learning by orders of magnitude. Yet analog-domain device and circuit
non-idealities -- within the analog ``Tiles'' performing Matrix-Vector Multiply
(MVM) operations -- can degrade neural-network task accuracy. We quantify the
impact of low-level distortions and noise, and develop a mathematical model for
Multiply-ACcumulate (MAC) operations mapped to analog tiles.
Instantaneous-current IR-drop (the most significant circuit non-ideality), and
ADC quantization effects are fully captured by this model, which can predict
MVM tile-outputs both rapidly and accurately, as compared to much slower
rigorous circuit simulations. A statistical model of PCM read noise at
nanosecond timescales is derived from -- and matched against -- experimental
measurements. We integrate these (statistical) device and (deterministic)
circuit effects into a PyTorch-based framework to assess the accuracy impact on
the BERT and ALBERT Transformer networks. We show that hardware-aware
fine-tuning using simple Gaussian noise provides resilience against ADC
quantization and PCM read noise effects, but is less effective against IR-drop.
This is because IR-drop -- although deterministic -- is non-linear, is changing
significantly during the time-integration window, and is ultimately dependent
on all the excitations being introduced in parallel into the analog tile. The
apparent inability of simple Gaussian noise applied during training to properly
prepare a DNN network for IR-drop during inference implies that more complex
training approaches -- incorporating advances such as the Tile-circuit model
introduced here -- will be critical for resilient deployment of large neural
networks onto AIMC hardware.

</details>


### [450] [Emerging ML-AI Techniques for Analog and RF EDA](https://arxiv.org/abs/2506.00007)
*Zhengfeng Wu,Ziyi Chen,Nnaemeka Achebe,Vaibhav V. Rao,Pratik Shrestha,Ioannis Savidis*

Main category: cs.AR

TL;DR: This paper surveys the integration of machine learning into EDA workflows for analog and RF circuits, emphasizing how ML can enhance automation, improve design quality, and reduce time-to-market while addressing unique analog design challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complex constraints, nonlinear design spaces, and high computational costs in analog circuit design by leveraging machine learning techniques.

Method: Review state-of-the-art learning and optimization techniques applied to various circuit tasks such as constraint formulation, topology generation, device modeling, sizing, placement, and routing.

Result: Showcases the ability of machine learning to improve automation, design quality, and efficiency in analog and RF circuit design workflows.

Conclusion: Machine learning offers significant potential to overcome challenges in analog and RF circuit design, with emerging trends indicating further advancements and considerations needed.

Abstract: This survey explores the integration of machine learning (ML) into EDA
workflows for analog and RF circuits, addressing challenges unique to analog
design, which include complex constraints, nonlinear design spaces, and high
computational costs. State-of-the-art learning and optimization techniques are
reviewed for circuit tasks such as constraint formulation, topology generation,
device modeling, sizing, placement, and routing. The survey highlights the
capability of ML to enhance automation, improve design quality, and reduce
time-to-market while meeting the target specifications of an analog or RF
circuit. Emerging trends and cross-cutting challenges, including robustness to
variations and considerations of interconnect parasitics, are also discussed.

</details>


### [451] [AI Accelerators for Large Language Model In-ference: Architecture Analysis and Scaling Strategies](https://arxiv.org/abs/2506.00008)
*Amit Sharma*

Main category: cs.AR

TL;DR: The paper conducts a workload-centric, cross-architectural performance study of commercial AI accelerators for large-language models (LLMs). It compares different hardware aspects and scaling techniques, providing guidance for matching workloads to accelerators.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the performance variations across different commercial AI accelerators used in LLMs inference and to identify architectural gaps for future designs.

Method: The study involves comparing memory hierarchies, compute fabrics, and on-chip interconnects of various AI accelerators including GPU-based chips, hybrid packages, and wafer-scale engines. It also examines four scaling techniques for trillion-parameter models.

Result: There is up to 3.7x performance variation across architectures with changes in batch size and sequence length. Expert parallelism offers an 8.4x parameter-to-compute advantage but has 2.1x higher latency variance than tensor parallelism.

Conclusion: The findings offer quantitative guidance for aligning workloads with suitable accelerators and highlight architectural deficiencies that need to be addressed in next-generation designs.

Abstract: The rapid growth of large-language models (LLMs) is driving a new wave of
specialized hardware for inference. This paper presents the first
workload-centric, cross-architectural performance study of commercial AI
accelerators, spanning GPU-based chips, hybrid packages, and wafer-scale
engines. We compare memory hierarchies, compute fabrics, and on-chip
interconnects, and observe up to 3.7x performance variation across
architectures as batch size and sequence length change. Four scaling techniques
for trillion-parameter models are examined; expert parallelism offers an 8.4x
parameter-to-compute advantage but incurs 2.1x higher latency variance than
tensor parallelism. These findings provide quantitative guidance for matching
workloads to accelerators and reveal architectural gaps that next-generation
designs must address.

</details>


### [452] [VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration](https://arxiv.org/abs/2506.01166)
*Shereef Helal,Alberto Garcia-Ortiz,Lennart Bamberg*

Main category: cs.AR

TL;DR: VUSA is a systolic-array architecture that can grow virtually based on sparsity to perform larger matrix multiplications with the same number of MAC units, saving 37% in area and 68% in power efficiency compared to a baseline.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of DNN accelerators for Edge-AI applications by leveraging unstructured sparsity.

Method: Introduction of VUSA, an architecture that virtually grows according to the level of sparsity to execute larger matrix multiplications without increasing the number of physical MAC units.

Result: Achieved 37% savings in area and 68% in power efficiency compared to a baseline systolic array, while maintaining peak-performance and supporting any DNN regardless of sparsity.

Conclusion: VUSA provides a more efficient architecture for DNN accelerators, being application-independent and suitable for general-purpose AI acceleration.

Abstract: Leveraging high degrees of unstructured sparsity is a promising approach to
enhance the efficiency of deep neural network DNN accelerators - particularly
important for emerging Edge-AI applications. We introduce VUSA, a
systolic-array architecture that virtually grows based on the present sparsity
to perform larger matrix multiplications with the same number of physical
multiply-accumulate MAC units. The proposed architecture achieves saving by 37%
and 68% in area and power efficiency, respectively, at the same
peak-performance, compared to a baseline systolic array architecture in a
commercial 16-nm technology. Still, the proposed architecture supports
acceleration for any DNN with any sparsity - even no sparsity at all. Thus, the
proposed architecture is application-independent, making it viable for
general-purpose AI acceleration.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [453] [A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things](https://arxiv.org/abs/2506.00133)
*Mohammadhossein Homaei,Mehran Tarif,Agustin Di Bartolo,Oscar Mogollon Gutierrez,Mar Avila*

Main category: cs.NI

TL;DR: The paper proposes RL-RPL-UA, a reinforcement learning-based routing protocol for underwater networks which improves packet delivery, reduces energy use, and extends network lifetime compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional routing protocols like RPL are not suitable for underwater conditions due to challenges such as low bandwidth, high latency, mobility, and limited energy resources.

Method: RL-RPL-UA is introduced where each node includes a lightweight RL agent that selects the best parent node based on local information including packet delivery ratio, buffer level, link quality, and remaining energy. It maintains compatibility with standard RPL messages and adds a dynamic objective function for real-time decision-making.

Result: Simulations show that RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per packet by 14.8%, and extends network lifetime by 80 seconds compared to traditional methods.

Conclusion: RL-RPL-UA is an effective and energy-efficient routing solution for underwater networks.

Abstract: The Internet of Underwater Things (IoUT) faces major challenges such as low
bandwidth, high latency, mobility, and limited energy resources. Traditional
routing protocols like RPL, which were designed for land-based networks, do not
perform well in these underwater conditions. This paper introduces RL-RPL-UA, a
new routing protocol that uses reinforcement learning to improve performance in
underwater environments. Each node includes a lightweight RL agent that selects
the best parent node based on local information such as packet delivery ratio,
buffer level, link quality, and remaining energy. RL-RPL-UA keeps full
compatibility with standard RPL messages and adds a dynamic objective function
to support real-time decision-making. Simulations using Aqua-Sim show that
RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per
packet by 14.8%, and extends network lifetime by 80 seconds compared to
traditional methods. These results suggest that RL-RPL-UA is a promising and
energy-efficient routing solution for underwater networks.

</details>


### [454] [Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison](https://arxiv.org/abs/2506.00924)
*Parsa Hassani Shariat Panahi,Amir Hossein Jalilvand,M. Hasan Najafi*

Main category: cs.NI

TL;DR: The paper proposes a dual-layer framework for network operator-side quality of experience (QoE) assessment integrating objective network modeling and subjective user perception. It uses a machine learning model for objective QoE prediction based on network parameters, and a semantic filtering and scoring pipeline to process live-stream comments for subjective feedback.


<details>
  <summary>Details</summary>
Motivation: To create a scalable and interpretable method for assessing Quality of Experience (QoE) that integrates both objective network metrics and subjective user perception extracted from live-streaming platforms.

Method: Develops a machine learning model trained on mean opinion scores (MOS) for predicting user-perceived video quality using network parameters. Implements a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback using a large language model.

Result: Constructed a labeled dataset of 47,894 live-stream comments with about 34,000 identified as QoE-relevant. Proposed a delta MOS metric for measuring deviations in service quality. Confirmed the framework's effectiveness in identifying service disruptions through a controlled outage simulation.

Conclusion: The dual-layer framework allows for real-time interpretation of performance deviations and comparison between subjective user feedback and objective network-based QoE estimates.

Abstract: This paper introduces a dual-layer framework for network operator-side
quality of experience (QoE) assessment that integrates both objective network
modeling and subjective user perception extracted from live-streaming
platforms. On the objective side, we develop a machine learning model trained
on mean opinion scores (MOS) computed via the ITU-T P.1203 reference
implementation, allowing accurate prediction of user-perceived video quality
using only network parameters such as packet loss, delay, jitter, and
throughput without reliance on video content or client-side instrumentation. On
the subjective side, we present a semantic filtering and scoring pipeline that
processes user comments from live streams to extract performance-related
feedback. A large language model is used to assign scalar MOS scores to
filtered comments in a deterministic and reproducible manner. To support
scalable and interpretable analysis, we construct a labeled dataset of 47,894
live-stream comments, of which about 34,000 are identified as QoE-relevant
through multi-layer semantic filtering. Each comment is enriched with simulated
Internet Service Provider attribution and temporally aligned using synthetic
timestamps in 5-min intervals. The resulting dataset enables operator-level
aggregation and time-series analysis of user-perceived quality. A delta MOS
metric is proposed to measure each Internet service provider's deviation from
platform-wide sentiment, allowing detection of localized degradations even in
the absence of direct network telemetry. A controlled outage simulation
confirms the framework's effectiveness in identifying service disruptions
through comment-based trends alone. The system provides each operator with its
own subjective MOS and the global platform average per interval, enabling
real-time interpretation of performance deviations and comparison with
objective network-based QoE estimates.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [455] [A Topological Semantics of Dialogue: Nerve Structures and Logical Extraction](https://arxiv.org/abs/2506.00615)
*Andreu Ballus Santacana*

Main category: cs.LO

TL;DR: The paper introduces a concise, topologically-motivated semantics for finite dialogues by mapping utterances to open sets in a semantic space and extracting combinatorial invariants.


<details>
  <summary>Details</summary>
Motivation: To provide a straightforward criterion for merging separate transcripts without contradiction and enable effective enumeration of all logical consequences of the entire dialogue.

Method: Mapping each utterance to an open set in a fixed semantic space, building the corresponding nerve complex of joint satisfiability, and extracting fundamental combinatorial invariants such as negative nerve and global interpretation subspace.

Result: Practical demonstration in the Wolfram Language showing algorithms for constructing nerves, detecting inconsistencies, and computing the global interpretation.

Conclusion: This framework is grounded in classical duality and topological semantics while drawing on recent advances in topological data analysis and dialogue-based semantics.

Abstract: We introduce a concise, topologically-motivated semantics for finite
dialogues by mapping each utterance to an open set in a fixed semantic space,
building the corresponding nerve complex of joint satisfiability, and
extracting fundamental combinatorial invariants:
  1. The negative nerve, which enumerates all finite collections of utterances
whose
  opens have empty intersection, providing a straightforward criterion for
merging
  separate transcripts without contradiction.
  2. The global interpretation subspace, the unique minimal open in which all
asserted
  utterances hold simultaneously, enabling effective enumeration of all logical
  consequences of the entire dialogue.
  3. A practical demonstration in the Wolfram Language, with algorithms for
constructing
  nerves, detecting inconsistencies, and computing the global interpretation,
thereby
  illustrating computational feasibility.
  Our framework is grounded in classical duality and topological semantics
(Stone duality, Priestley duality, Tarski's semantics, coherence-space methods,
Scott domains, topos semantics, and homotopy type theory) while drawing on
recent advances in topological data analysis and dialogue-based semantics.

</details>


### [456] [Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization](https://arxiv.org/abs/2506.00674)
*Zhiwei Zhang,Samy Wu Fung,Anastasios Kyrillidis,Stanley Osher,Moshe Y. Vardi*

Main category: cs.LO

TL;DR: The paper proposes unconstrained continuous optimization formulations for hybrid SAT solving using penalty terms, showing that unconstrained optimizers can improve SAT solving on hybrid benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current SAT solvers are highly efficient in handling CNF formulas but struggle with non-CNF (hybrid) constraints. Previous work uses polynomial representations with box constraints that may hinder the use of powerful unconstrained optimizers.

Method: The authors develop unconstrained continuous optimization formulations for hybrid SAT solving by incorporating penalty terms. They also provide theoretical analysis on when these penalty terms are necessary.

Result: Empirical results indicate that unconstrained optimizers, such as Adam, can significantly enhance SAT solving on hybrid benchmarks.

Conclusion: The study reveals the potential benefits of integrating continuous optimization and machine-learning-based methods to effectively solve hybrid SAT problems.

Abstract: The Boolean satisfiability (SAT) problem lies at the core of many
applications in combinatorial optimization, software verification,
cryptography, and machine learning. While state-of-the-art solvers have
demonstrated high efficiency in handling conjunctive normal form (CNF)
formulas, numerous applications require non-CNF (hybrid) constraints, such as
XOR, cardinality, and Not-All-Equal constraints. Recent work leverages
polynomial representations to represent such hybrid constraints, but it relies
on box constraints that can limit the use of powerful unconstrained optimizers.
In this paper, we propose unconstrained continuous optimization formulations
for hybrid SAT solving by penalty terms. We provide theoretical insights into
when these penalty terms are necessary and demonstrate empirically that
unconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid
benchmarks. Our results highlight the potential of combining continuous
optimization and machine-learning-based methods for effective hybrid SAT
solving.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [457] [Diff-SPORT: Diffusion-based Sensor Placement Optimization and Reconstruction of Turbulent flows in urban environments](https://arxiv.org/abs/2506.00214)
*Abhijeet Vishwasrao,Sai Bharath Chandra Gutha,Andres Cremades,Klas Wijk,Aakash Patil,Catherine Gorle,Beverley J McKeon,Hossein Azizpour,Ricardo Vinuesa*

Main category: physics.flu-dyn

TL;DR: Diff-SPORT is a diffusion-based framework for high-fidelity flow reconstruction and optimal sensor placement in urban environments, which combines generative diffusion model with MAP inference scheme and Shapley-value attribution framework to provide scalable and interpretable solution.


<details>
  <summary>Details</summary>
Motivation: Rapid urbanization requires accurate and efficient monitoring of turbulent wind patterns for air quality, climate resilience and infrastructure design. Traditional methods face accuracy degradations under practical constraints.

Method: Diff-SPORT combines a generative diffusion model with a maximum a posteriori (MAP) inference scheme and a Shapley-value attribution framework. It offers a modular, zero-shot alternative to retraining-intensive strategies.

Result: Diff-SPORT achieves significant speedups while maintaining both statistical and instantaneous flow fidelity. It supports fast and reliable urban flow monitoring under extreme sparsity.

Conclusion: Diff-SPORT paves the way for integrating generative modeling and explainability in sustainable urban intelligence.

Abstract: Rapid urbanization demands accurate and efficient monitoring of turbulent
wind patterns to support air quality, climate resilience and infrastructure
design. Traditional sparse reconstruction and sensor placement strategies face
major accuracy degradations under practical constraints. Here, we introduce
Diff-SPORT, a diffusion-based framework for high-fidelity flow reconstruction
and optimal sensor placement in urban environments. Diff-SPORT combines a
generative diffusion model with a maximum a posteriori (MAP) inference scheme
and a Shapley-value attribution framework to propose a scalable and
interpretable solution. Compared to traditional numerical methods, Diff-SPORT
achieves significant speedups while maintaining both statistical and
instantaneous flow fidelity. Our approach offers a modular, zero-shot
alternative to retraining-intensive strategies, supporting fast and reliable
urban flow monitoring under extreme sparsity. Diff-SPORT paves the way for
integrating generative modeling and explainability in sustainable urban
intelligence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [458] [Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings](https://arxiv.org/abs/2506.00348)
*Shivam Shorewala,Zihao Yang*

Main category: stat.ML

TL;DR: The paper introduces MOVDA, a new framework that improves traditional rating systems by incorporating margin of victory data. It outperforms standard ELO and Bayesian baselines in NBA data experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional rating systems like ELO only focus on binary outcomes and lack effective methods to incorporate margin of victory (MOV) data. This leads to information loss and less accurate skill assessments.

Method: MOVDA uses the deviation between the true MOV and a modeled expectation to enhance ratings. It learns a domain-specific, non-linear function (scaled hyperbolic tangent) to predict expected MOV based on rating differentials. The difference between true and expected MOV provides a weighted signal for rating updates.

Result: In experiments with professional NBA data (2013-2023, 13,619 games), MOVDA significantly outperforms standard ELO and Bayesian baselines. It reduces Brier score prediction error by 1.54%, increases outcome accuracy by 0.58%, and accelerates rating convergence by 13.5% compared to TrueSkill.

Conclusion: MOVDA offers a theoretically motivated, empirically superior, and computationally efficient approach to integrating performance magnitude into skill rating for competitive environments.

Abstract: Knowledge of accurate relative skills in any competitive system is essential,
but foundational approaches such as ELO discard extremely relevant performance
data by concentrating exclusively on binary outcomes. While margin of victory
(MOV) extensions exist, they often lack a definitive method for incorporating
this information. We introduce Margin of Victory Differential Analysis (MOVDA),
a framework that enhances traditional rating systems by using the deviation
between the true MOV and a $\textit{modeled expectation}$. MOVDA learns a
domain-specific, non-linear function (a scaled hyperbolic tangent that captures
saturation effects and home advantage) to predict expected MOV based on rating
differentials. Crucially, the $\textit{difference}$ between the true and
expected MOV provides a subtle and weighted signal for rating updates,
highlighting informative deviations in all levels of contests. Extensive
experiments on professional NBA basketball data (from 2013 to 2023, with 13,619
games) show that MOVDA significantly outperforms standard ELO and Bayesian
baselines. MOVDA reduces Brier score prediction error by $1.54\%$ compared to
TrueSkill, increases outcome accuracy by $0.58\%$, and most importantly
accelerates rating convergence by $13.5\%$, while maintaining the computational
efficiency of the original ELO updates. MOVDA offers a theoretically motivated,
empirically superior, and computationally lean approach to integrating
performance magnitude into skill rating for competitive environments like the
NBA.

</details>


### [459] [Minimax Rates for the Estimation of Eigenpairs of Weighted Laplace-Beltrami Operators on Manifolds](https://arxiv.org/abs/2506.00171)
*Nicolás García Trillos,Chenghui Li,Raghavendra Venkatraman*

Main category: stat.ML

TL;DR: The paper investigates the problem of estimating eigenpairs of elliptic differential operators from samples on a manifold, analyzing minimax risk and approximation rates using graph Laplacians.


<details>
  <summary>Details</summary>
Motivation: To understand how well eigenpairs of elliptic differential operators can be estimated from data samples distributed on a manifold, particularly for unsupervised learning applications involving graph Laplacians.

Method: Analyzing the minimax risk for eigenpair estimation, studying approximation rates of graph Laplacians, and proving bounds under assumptions about the distribution and manifold geometry.

Result: The statistical minimax rate for approximating eigenvalues and eigenvectors is determined to be $n^{-2/(d+4)}$, matching rates in related density estimation problems. Eigenpairs of graph Laplacians provide estimators with errors matching these lower bounds under stronger regularity assumptions.

Conclusion: The analysis expands existing literature by considering stronger error norms and providing uniform convergence rates over smooth distributions, showing that these rates are essentially sharp under high graph connectivity.

Abstract: We study the problem of estimating eigenpairs of elliptic differential
operators from samples of a distribution $\rho$ supported on a manifold $M$.
The operators discussed in the paper are relevant in unsupervised learning and
in particular are obtained by taking suitable scaling limits of widely used
graph Laplacians over data clouds. We study the minimax risk for this eigenpair
estimation problem and explore the rates of approximation that can be achieved
by commonly used graph Laplacians built from random data. More concretely,
assuming that $\rho$ belongs to a certain family of distributions with
controlled second derivatives, and assuming that the $d$-dimensional manifold
$M$ where $\rho$ is supported has bounded geometry, we prove that the
statistical minimax rate for approximating eigenvalues and eigenvectors in the
$H^1(M)$-sense is $n^{-2/(d+4)}$, a rate that matches the minimax rate for a
closely related density estimation problem. We then revisit the literature
studying Laplacians over proximity graphs in the large data limit and prove
that, under slightly stronger regularity assumptions on the data generating
model, eigenpairs of graph Laplacians induce manifold agnostic estimators with
an error of approximation that, up to logarithmic corrections, matches our
lower bounds. Our analysis allows us to expand the existing literature on
graph-based learning in at least two significant ways: 1) we consider stronger
norms to measure the error of approximation than the ones that had been
analyzed in the past; 2) our rates of convergence are uniform over a family of
smooth distributions and do not just apply to densities with special
symmetries, and, as a consequence of our lower bounds, are essentially sharp
when the connectivity of the graph is sufficiently high.

</details>


### [460] [Overfitting has a limitation: a model-independent generalization error bound based on Rényi entropy](https://arxiv.org/abs/2506.00182)
*Atsushi Suzuki*

Main category: stat.ML

TL;DR: 通过引入与模型无关的泛化误差上界，本研究解释了在数据量充足的情况下，即使模型规模无限增大，也能保持较小的泛化误差。此外，还说明了为什么向数据中注入随机噪声会显著降低泛化性能，并将无免费午餐定理扩展为依赖于数据分布的形式。


<details>
  <summary>Details</summary>
Motivation: 当前对大规模机器学习模型的泛化误差理解不足，特别是传统分析方法无法完全解释超大规模模型的成功原因。

Method: 提出一种与模型无关的泛化误差上界，适用于仅由数据直方图决定输出的算法（如经验风险最小化或基于梯度的方法）。该上界仅依赖于数据生成分布的Rényi熵，并证明数据量相对于此熵充足时，可以维持较小的泛化误差。同时，将无免费午餐定理改编为依赖于数据分布的形式。

Result: 表明泛化误差上界与Rényi熵紧密相关，且数据量充足时，即使模型规模无限增大，也能保持较小的泛化误差。此外，成功解释了注入随机噪声导致泛化性能下降的现象，并验证了所提泛化误差上界的紧致性。

Conclusion: 本研究提供了一种新的视角来理解大规模机器学习模型的泛化行为，强调了数据量和数据分布特性（如Rényi熵）在泛化性能中的关键作用。

Abstract: Will further scaling up of machine learning models continue to bring success?
A significant challenge in answering this question lies in understanding
generalization error, which is the impact of overfitting. Understanding
generalization error behavior of increasingly large-scale machine learning
models remains a significant area of investigation, as conventional analyses
often link error bounds to model complexity, failing to fully explain the
success of extremely large architectures. This research introduces a novel
perspective by establishing a model-independent upper bound for generalization
error applicable to algorithms whose outputs are determined solely by the
data's histogram, such as empirical risk minimization or gradient-based
methods. Crucially, this bound is shown to depend only on the R\'enyi entropy
of the data-generating distribution, suggesting that a small generalization
error can be maintained even with arbitrarily large models, provided the data
quantity is sufficient relative to this entropy. This framework offers a direct
explanation for the phenomenon where generalization performance degrades
significantly upon injecting random noise into data, where the performance
degrade is attributed to the consequent increase in the data distribution's
R\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be
data-distribution-dependent, demonstrating that an amount of data corresponding
to the R\'enyi entropy is indeed essential for successful learning, thereby
highlighting the tightness of our proposed generalization bound.

</details>


### [461] [Riemannian Principal Component Analysis](https://arxiv.org/abs/2506.00226)
*Oldemar Rodríguez*

Main category: stat.ML

TL;DR: This paper proposes Riemannian Principal Component Analysis (R-PCA) to extend traditional PCA for data on Riemannian manifolds, overcoming limitations of Principal Geodesic Analysis (PGA) when dealing with general datasets lacking an implicit local distance notion.


<details>
  <summary>Details</summary>
Motivation: The motivation is the limitation of Principal Geodesic Analysis (PGA) when handling general datasets without an implicit local distance notion. The authors aim to develop a method that can perform dimensionality reduction and statistical analysis directly on Riemannian manifolds for such datasets.

Method: The proposed method, Riemannian Principal Component Analysis (R-PCA), adapts PCA methodology to Riemannian manifolds by equipping data tables with local metrics, allowing incorporation of manifold geometry.

Result: The framework provides a unified approach for dimensionality reduction and statistical analysis on manifolds, applicable to datasets with region-specific or part-specific distance notions.

Conclusion: R-PCA extends the capabilities of traditional PCA to Riemannian manifolds, enabling respect for intrinsic geometric properties of data and opening new possibilities for manifold-based analysis.

Abstract: This paper proposes an innovative extension of Principal Component Analysis
(PCA) that transcends the traditional assumption of data lying in Euclidean
space, enabling its application to data on Riemannian manifolds. The primary
challenge addressed is the lack of vector space operations on such manifolds.
Fletcher et al., in their work {\em Principal Geodesic Analysis for the Study
of Nonlinear Statistics of Shape}, proposed Principal Geodesic Analysis (PGA)
as a geometric approach to analyze data on Riemannian manifolds, particularly
effective for structured datasets like medical images, where the manifold's
intrinsic structure is apparent. However, PGA's applicability is limited when
dealing with general datasets that lack an implicit local distance notion. In
this work, we introduce a generalized framework, termed {\em Riemannian
Principal Component Analysis (R-PCA)}, to extend PGA for any data endowed with
a local distance structure. Specifically, we adapt the PCA methodology to
Riemannian manifolds by equipping data tables with local metrics, enabling the
incorporation of manifold geometry. This framework provides a unified approach
for dimensionality reduction and statistical analysis directly on manifolds,
opening new possibilities for datasets with region-specific or part-specific
distance notions, ensuring respect for their intrinsic geometric properties.

</details>


### [462] [Bayesian Data Sketching for Varying Coefficient Regression Models](https://arxiv.org/abs/2506.00270)
*Rajarshi Guhaniyogi,Laura Baracaldo,Sudipto Banerjee*

Main category: stat.ML

TL;DR: This paper presents a method for Bayesian data sketching in varying coefficient models to handle large datasets by compressing the functional response vector and predictor matrix via random linear transformation, allowing for efficient posterior computations without new models or specialized hardware.


<details>
  <summary>Details</summary>
Motivation: Bayesian variants of varying coefficient models have not been widely used in large data applications due to slow posterior computations with MCMC algorithms. The motivation is to develop an approach that can overcome computational challenges posed by large sample sizes.

Method: The method involves compressing the functional response vector and predictor matrix using a random linear transformation to achieve dimension reduction. This compressed data is then used for inference, enabling standard methods and algorithms for varying coefficient regression models to be applied without needing new models or specialized hardware.

Result: The approach allows for fully model-based Bayesian inference on large datasets without requiring new models or algorithms, or specialized computational hardware.

Conclusion: Bayesian data sketching for varying coefficient models provides an effective solution for analyzing large functional data by utilizing dimension reduction through compression, thus overcoming computational limitations.

Abstract: Varying coefficient models are popular for estimating nonlinear regression
functions in functional data models. Their Bayesian variants have received
limited attention in large data applications, primarily due to prohibitively
slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms.
We introduce Bayesian data sketching for varying coefficient models to obviate
computational challenges presented by large sample sizes. To address the
challenges of analyzing large data, we compress the functional response vector
and predictor matrix by a random linear transformation to achieve dimension
reduction and conduct inference on the compressed data. Our approach
distinguishes itself from several existing methods for analyzing large
functional data in that it requires neither the development of new models or
algorithms, nor any specialized computational hardware while delivering fully
model-based Bayesian inference. Well-established methods and algorithms for
varying coefficient regression models can be applied to the compressed data.

</details>


### [463] [Label-shift robust federated feature screening for high-dimensional classification](https://arxiv.org/abs/2506.00379)
*Qi Qin,Erbo Li,Xingxiang Li,Yifan Sun,Wu Wang,Chen Xu*

Main category: stat.ML

TL;DR: This paper introduces a framework unifying feature screening methods and proposes LR-FFS, a label-shift robust federated feature screening method with federated estimation procedure. LR-FFS addresses label shift challenges in distributed learning without extra computational burdens, ensuring privacy protection and maintaining screening effectiveness.


<details>
  <summary>Details</summary>
Motivation: Feature screening is crucial for high-dimensional classification tasks to reduce computational costs and handle large datasets. However, data heterogeneity such as label shifting across different clients poses significant challenges in distributed and federated learning environments.

Method: The paper proposes LR-FFS (Label-Shift Robust Federated Feature Screening) which leverages conditional distribution functions and expectations to address label shift issues. It integrates a federated estimation procedure that ensures computational efficiency and privacy protection while preserving screening effectiveness comparable to centralized processing.

Result: LR-FFS demonstrates superior performance across diverse client environments including varying class distributions, sample sizes, and missing categorical data. Theoretical analyses and experimental results confirm its robustness against model misspecification and outliers, as well as its ability to control false discovery rates effectively.

Conclusion: LR-FFS provides an effective solution to the label shift problem in federated learning, offering both theoretical guarantees and practical advantages in various real-world scenarios.

Abstract: Distributed and federated learning are important tools for high-dimensional
classification of large datasets. To reduce computational costs and overcome
the curse of dimensionality, feature screening plays a pivotal role in
eliminating irrelevant features during data preprocessing. However, data
heterogeneity, particularly label shifting across different clients, presents
significant challenges for feature screening. This paper introduces a general
framework that unifies existing screening methods and proposes a novel utility,
label-shift robust federated feature screening (LR-FFS), along with its
federated estimation procedure. The framework facilitates a uniform analysis of
methods and systematically characterizes their behaviors under label shift
conditions. Building upon this framework, LR-FFS leverages conditional
distribution functions and expectations to address label shift without adding
computational burdens and remains robust against model misspecification and
outliers. Additionally, the federated procedure ensures computational
efficiency and privacy protection while maintaining screening effectiveness
comparable to centralized processing. We also provide a false discovery rate
(FDR) control method for federated feature screening. Experimental results and
theoretical analyses demonstrate LR-FFS's superior performance across diverse
client environments, including those with varying class distributions, sample
sizes, and missing categorical data.

</details>


### [464] [Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling](https://arxiv.org/abs/2506.00446)
*Tatsuki Takahashi,Chihiro Maru,Hiroko Shoji*

Main category: stat.ML

TL;DR: In off-policy evaluation for large ranking action spaces, this paper proposes GMIPS estimator with lower variance and MRIPS variant that balances bias and variance effectively.


<details>
  <summary>Details</summary>
Motivation: Existing estimators in off-policy evaluation face high variance issues when dealing with large ranking action spaces.

Method: Two new assumptions are introduced: no direct effect on rankings and user behavior model on ranking embedding spaces. Then, the GMIPS estimator is proposed, including its variant MRIPS which incorporates a doubly marginalized importance weight based on a cascade behavior assumption.

Result: GMIPS achieves the lowest MSE among existing estimators. MRIPS effectively balances the trade-off between bias and variance even as the ranking action spaces increase.

Conclusion: GMIPS and its variant MRIPS provide statistically desirable properties for off-policy evaluation in large ranking action spaces.

Abstract: Off-policy evaluation (OPE) in ranking settings with large ranking action
spaces, which stems from an increase in both the number of unique actions and
length of the ranking, is essential for assessing new recommender policies
using only logged bandit data from previous versions. To address the high
variance issues associated with existing estimators, we introduce two new
assumptions: no direct effect on rankings and user behavior model on ranking
embedding spaces. We then propose the generalized marginalized inverse
propensity score (GMIPS) estimator with statistically desirable properties
compared to existing ones. Finally, we demonstrate that the GMIPS achieves the
lowest MSE. Notably, among GMIPS variants, the marginalized reward interaction
IPS (MRIPS) incorporates a doubly marginalized importance weight based on a
cascade behavior assumption on ranking embeddings. MRIPS effectively balances
the trade-off between bias and variance, even as the ranking action spaces
increase and the above assumptions may not hold, as evidenced by our
experiments.

</details>


### [465] [Score Matching With Missing Data](https://arxiv.org/abs/2506.00557)
*Josh Givens,Song Liu,Henry W J Reeve*

Main category: stat.ML

TL;DR: 本文研究了在数据不完整情况下的得分匹配方法，并提出了两种变体，一种是重要性加权（IW）方法，适用于小样本低维情形；另一种是变分方法，适用于复杂高维情形。


<details>
  <summary>Details</summary>
Motivation: 得分匹配是一个重要的工具，用于学习数据分布，但在数据不完整的情况下，其应用较少。因此，本文试图解决数据部分缺失时的得分匹配问题。

Method: 作者提出两种得分匹配变体：重要性加权（IW）方法和变分方法。其中，IW方法在有限域设置中提供了有限样本边界，而变分方法则适用于更复杂的高维设置。

Result: 实验结果表明，在小样本低维情况下，IW方法表现出色；而在复杂高维情况下，变分方法表现更佳。此外，这些方法在真实和模拟数据的图模型估计任务上也得到了验证。

Conclusion: 本文提出的得分匹配变体能够有效处理数据缺失问题，为实际应用提供了新的解决方案。

Abstract: Score matching is a vital tool for learning the distribution of data with
applications across many areas including diffusion processes, energy based
modelling, and graphical model estimation. Despite all these applications,
little work explores its use when data is incomplete. We address this by
adapting score matching (and its major extensions) to work with missing data in
a flexible setting where data can be partially missing over any subset of the
coordinates. We provide two separate score matching variations for general use,
an importance weighting (IW) approach, and a variational approach. We provide
finite sample bounds for our IW approach in finite domain settings and show it
to have especially strong performance in small sample lower dimensional cases.
Complementing this, we show our variational approach to be strongest in more
complex high-dimensional settings which we demonstrate on graphical model
estimation tasks on both real and simulated data.

</details>


### [466] [Generalized Linear Markov Decision Process](https://arxiv.org/abs/2506.00818)
*Sinian Zhang,Kaicheng Zhang,Ziping Xu,Tianxi Cai,Doudou Zhou*

Main category: stat.ML

TL;DR: An extension of the linear MDP framework, GLMDP models rewards with generalized linear models while keeping linear transition dynamics. Two offline RL algorithms are developed with theoretical guarantees and improved sample efficiency.


<details>
  <summary>Details</summary>
Motivation: The restrictive assumption of linear MDPs limits their applicability in real-world domains where rewards often exhibit nonlinear or discrete structures. Applications such as healthcare and e-commerce need a more flexible framework due to scarce data and binary or count-valued reward signals.

Method: Proposed the Generalized Linear MDP (GLMDP) framework which uses generalized linear models for reward modeling while maintaining linear transition dynamics. Established Bellman completeness regarding a new function class accommodating nonlinear rewards. Developed two offline RL algorithms GPEVI and SS-GPEVI that utilize both labeled and unlabeled trajectories.

Result: Achieved theoretical guarantees on policy suboptimality. Demonstrated improved sample efficiency in scenarios where reward labels are expensive or limited.

Conclusion: GLMDP provides a more flexible framework for real-world applications with nonlinear or discrete reward structures, improving sample efficiency in data-scarce environments.

Abstract: The linear Markov Decision Process (MDP) framework offers a principled
foundation for reinforcement learning (RL) with strong theoretical guarantees
and sample efficiency. However, its restrictive assumption-that both transition
dynamics and reward functions are linear in the same feature space-limits its
applicability in real-world domains, where rewards often exhibit nonlinear or
discrete structures. Motivated by applications such as healthcare and
e-commerce, where data is scarce and reward signals can be binary or
count-valued, we propose the Generalized Linear MDP (GLMDP) framework-an
extension of the linear MDP framework-that models rewards using generalized
linear models (GLMs) while maintaining linear transition dynamics. We establish
the Bellman completeness of GLMDPs with respect to a new function class that
accommodates nonlinear rewards and develop two offline RL algorithms:
Generalized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant
(SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our
algorithms achieve theoretical guarantees on policy suboptimality and
demonstrate improved sample efficiency in settings where reward labels are
expensive or limited.

</details>


### [467] [Projection Pursuit Density Ratio Estimation](https://arxiv.org/abs/2506.00866)
*Meilin Wang,Wei Huang,Mingming Gong,Zheng Zhang*

Main category: stat.ML

TL;DR: Density ratio estimation (DRE) is crucial in machine learning but faces challenges with parametric and non-parametric methods. This paper proposes a new DRE approach using projection pursuit (PP) approximation, which handles high dimensionality and maintains model flexibility. The method is consistent, has a defined convergence rate, and performs better than existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Current DRE methods either lead to biased results if models are misspecified (parametric methods) or suffer from the curse of dimensionality with large data dimensions (non-parametric methods).

Method: The paper proposes a novel DRE approach based on projection pursuit (PP) approximation, leveraging PP to address high dimensionality while preserving model flexibility for accurate DRE.

Result: The proposed estimator is proven to be consistent with a defined convergence rate. Experimental results indicate superior performance compared to existing alternatives across various applications.

Conclusion: The newly proposed DRE method using PP approximation effectively mitigates issues related to high dimensionality and model specification, offering improved accuracy and outperforming other methods.

Abstract: Density ratio estimation (DRE) is a paramount task in machine learning, for
its broad applications across multiple domains, such as covariate shift
adaptation, causal inference, independence tests and beyond. Parametric methods
for estimating the density ratio possibly lead to biased results if models are
misspecified, while conventional non-parametric methods suffer from the curse
of dimensionality when the dimension of data is large. To address these
challenges, in this paper, we propose a novel approach for DRE based on the
projection pursuit (PP) approximation. The proposed method leverages PP to
mitigate the impact of high dimensionality while retaining the model
flexibility needed for the accuracy of DRE. We establish the consistency and
the convergence rate for the proposed estimator. Experimental results
demonstrate that our proposed method outperforms existing alternatives in
various applications.

</details>


### [468] [Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise](https://arxiv.org/abs/2506.00933)
*Zhihao Xu,Saisai Ding,Zhikun Zhang,Xiangjun Wang*

Main category: stat.ML

TL;DR: The paper proposes an improved deep neural networks framework for parameter identification and prediction in stochastic Volterra integral equations driven by Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: Parameter identification for differential equations has been extensively studied, but the focus on integral equations, particularly stochastic Volterra integral equations, remains limited. This research aims to address this gap.

Method: An improved deep neural networks framework is proposed. The network represents primary variables and their integrals, incorporating inter-output relationships into the loss function. This approach enhances parameter estimation accuracy and extends to predict system behavior outside the integration interval.

Result: Numerical experiments demonstrate the effectiveness of the framework in both parameter identification and prediction tasks. It shows robust performance under varying noise levels and provides accurate solutions for modeling stochastic systems.

Conclusion: The proposed deep neural networks framework successfully addresses the parameter identification problem in stochastic Volterra integral equations and accurately predicts system behavior.

Abstract: Integral equations are widely used in fields such as applied modeling,
medical imaging, and system identification, providing a powerful framework for
solving deterministic problems. While parameter identification for differential
equations has been extensively studied, the focus on integral equations,
particularly stochastic Volterra integral equations, remains limited. This
research addresses the parameter identification problem, also known as the
equation reconstruction problem, in Volterra integral equations driven by
Gaussian noise. We propose an improved deep neural networks framework for
estimating unknown parameters in the drift term of these equations. The network
represents the primary variables and their integrals, enhancing parameter
estimation accuracy by incorporating inter-output relationships into the loss
function. Additionally, the framework extends beyond parameter identification
to predict the system's behavior outside the integration interval. Prediction
accuracy is validated by comparing predicted and true trajectories using a 95%
confidence interval. Numerical experiments demonstrate the effectiveness of the
proposed deep neural networks framework in both parameter identification and
prediction tasks, showing robust performance under varying noise levels and
providing accurate solutions for modeling stochastic systems.

</details>


### [469] [Generative diffusion posterior sampling for informative likelihoods](https://arxiv.org/abs/2506.01083)
*Zheng Zhao*

Main category: stat.ML

TL;DR: SMC methods have been successful for conditional sampling of generative diffusion models. This paper proposes a new diffusion posterior SMC sampler with improved statistical efficiencies.


<details>
  <summary>Details</summary>
Motivation: To improve the statistical efficiencies of Sequential Monte Carlo (SMC) methods, particularly under outlier conditions or highly informative likelihoods.

Method: The method involves constructing an observation path that correlates with the diffusion model and designing the sampler to leverage this correlation for more efficient sampling.

Result: Empirical results demonstrate the efficiency of the proposed sampler.

Conclusion: A new diffusion posterior SMC sampler has been proposed and shown to achieve improved statistical efficiencies.

Abstract: Sequential Monte Carlo (SMC) methods have recently shown successful results
for conditional sampling of generative diffusion models. In this paper we
propose a new diffusion posterior SMC sampler achieving improved statistical
efficiencies, particularly under outlier conditions or highly informative
likelihoods. The key idea is to construct an observation path that correlates
with the diffusion model and to design the sampler to leverage this correlation
for more efficient sampling. Empirical results conclude the efficiency.

</details>


### [470] [Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\ell^1$-regularization](https://arxiv.org/abs/2506.01143)
*Hannes Matt,Dominik Stöger*

Main category: stat.ML

TL;DR: 在过参数化的线性回归问题中，研究了深度D≥2的对角线性神经网络中的隐式正则化。通过分析梯度流轨迹的极限点与ℓ¹最小化问题解之间的逼近误差，精确描述了初始化规模α对逼近误差的影响，并展示了深度对误差的不同影响。数值实验表明，更深的网络可能具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 了解梯度下降在过参数化模型中的隐式偏差，特别是在对角线性神经网络中的表现。已有的研究表明，在小权重初始化下，梯度下降倾向于选择ℓ¹范数最小的解，但需要更深入地研究不同深度网络下的隐式正则化行为。

Method: 研究深度D≥2的对角线性神经网络在过参数化线性回归问题中的隐式正则化。通过推导逼近误差的上下界，分析初始化规模α对误差的影响，并比较不同深度（D=2和D≥3）情况下的误差变化规律。

Result: 发现对于D≥3，误差随α线性减小；而对于D=2，误差以α^(1-ϱ)的速度减小，其中参数ϱ∈[0,1)可以明确刻画，并且与稀疏恢复文献中的空空间性质常数密切相关。此外，数值实验证明了理论结果的正确性，并表明更深的网络可能有更好的泛化性能。

Conclusion: 隐式正则化的行为在不同深度的对角线性神经网络中表现出显著差异，特别是深度对逼近误差的影响。更深的网络（D≥3）可能带来更好的泛化性能，尤其是在实际的初始化规模下。

Abstract: Modern machine learning models are often trained in a setting where the
number of parameters exceeds the number of training samples. To understand the
implicit bias of gradient descent in such overparameterized models, prior work
has studied diagonal linear neural networks in the regression setting. These
studies have shown that, when initialized with small weights, gradient descent
tends to favor solutions with minimal $\ell^1$-norm - an effect known as
implicit regularization. In this paper, we investigate implicit regularization
in diagonal linear neural networks of depth $D\ge 2$ for overparameterized
linear regression problems. We focus on analyzing the approximation error
between the limit point of gradient flow trajectories and the solution to the
$\ell^1$-minimization problem. By deriving tight upper and lower bounds on the
approximation error, we precisely characterize how the approximation error
depends on the scale of initialization $\alpha$. Our results reveal a
qualitative difference between depths: for $D \ge 3$, the error decreases
linearly with $\alpha$, whereas for $D=2$, it decreases at rate
$\alpha^{1-\varrho}$, where the parameter $\varrho \in [0,1)$ can be explicitly
characterized. Interestingly, this parameter is closely linked to so-called
null space property constants studied in the sparse recovery literature. We
demonstrate the asymptotic tightness of our bounds through explicit examples.
Numerical experiments corroborate our theoretical findings and suggest that
deeper networks, i.e., $D \ge 3$, may lead to better generalization,
particularly for realistic initialization scales.

</details>


### [471] [Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation](https://arxiv.org/abs/2506.01267)
*Jingfu Peng,Yuhong Yang*

Main category: stat.ML

TL;DR: 本文研究了非参数回归中对抗性攻击下的统计最优性和最小最大收敛率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型容易受到未来输入数据中的细微扰动影响，即对抗性攻击。虽然已有许多对抗性学习方法，但其在鲁棒损失方面的统计最优性仍缺乏深入研究，特别是针对未来X-攻击的最小最大收敛率和最优估计量构造尚未解决。

Method: 作者在非参数回归的背景下，假设回归函数的平滑性和输入扰动集合的几何结构适合的情况下，首先建立了对抗性Lq-风险下的最小最大收敛率，并提出了一种分段局部多项式估计器以实现最小最大最优性。此外，还构建了一个数据驱动的自适应估计器，该估计器在广泛的非参数和对抗性类别中，以对数因子接近最优率。

Result: 所提出的估计器达到了理论上的最小最大最优性，阐明了平滑度水平和扰动幅度如何影响未来X-攻击下对抗性学习的基本极限。同时，数据驱动的自适应估计器也显示出良好的性能。

Conclusion: 本文解决了非参数回归中对抗性学习的最小最大收敛率问题，提出了达到最优性的估计方法，并揭示了未来X-攻击下对抗性学习的基本限制。

Abstract: Despite tremendous advancements of machine learning models and algorithms in
various application domains, they are known to be vulnerable to subtle, natural
or intentionally crafted perturbations in future input data, known as
adversarial attacks. While numerous adversarial learning methods have been
proposed, fundamental questions about their statistical optimality in robust
loss remain largely unanswered. In particular, the minimax rate of convergence
and the construction of rate-optimal estimators under future $X$-attacks are
yet to be worked out.
  In this paper, we address this issue in the context of nonparametric
regression, under suitable assumptions on the smoothness of the regression
function and the geometric structure of the input perturbation set. We first
establish the minimax rate of convergence under adversarial $L_q$-risks with $1
\leq q \leq \infty$ and propose a piecewise local polynomial estimator that
achieves the minimax optimality. The established minimax rate elucidates how
the smoothness level and perturbation magnitude affect the fundamental limit of
adversarial learning under future $X$-attacks. Furthermore, we construct a
data-driven adaptive estimator that is shown to achieve, within a logarithmic
factor, the optimal rate across a broad scale of nonparametric and adversarial
classes.

</details>


### [472] [Near-Optimal Clustering in Mixture of Markov Chains](https://arxiv.org/abs/2506.01324)
*Junghyun Lee,Yassir Jedra,Alexandre Proutière,Se-Young Yun*

Main category: stat.ML

TL;DR: The paper addresses the problem of clustering trajectories generated by unknown ergodic Markov chains, providing a novel two-stage algorithm that achieves near-optimal clustering error without requiring prior knowledge of model-specific quantities.


<details>
  <summary>Details</summary>
Motivation: Clustering trajectories based on their underlying generative models is crucial for understanding complex dynamical systems. Existing approaches either have suboptimal guarantees or require detailed prior knowledge of the models, which limits their applicability.

Method: The authors derive an instance-dependent lower bound on the clustering error rate governed by weighted KL divergence between transition kernels. They propose a two-stage clustering algorithm: Stage I uses spectral clustering with a new injective Euclidean embedding for ergodic Markov chains; Stage II refines the clusters through likelihood-based reassignment.

Result: The proposed method achieves a near-optimal clustering error with high probability under specific conditions on trajectory length $H$ and total trajectory time $TH$. It improves upon state-of-the-art guarantees and does not require prior knowledge of model-specific quantities.

Conclusion: The paper concludes with a discussion on the gap between the derived upper and lower bounds, offering insights into the unique structure of the clustering problem involving ergodic Markov chains.

Abstract: We study the problem of clustering $T$ trajectories of length $H$, each
generated by one of $K$ unknown ergodic Markov chains over a finite state space
of size $S$. The goal is to accurately group trajectories according to their
underlying generative model. We begin by deriving an instance-dependent,
high-probability lower bound on the clustering error rate, governed by the
weighted KL divergence between the transition kernels of the chains. We then
present a novel two-stage clustering algorithm. In Stage~I, we apply spectral
clustering using a new injective Euclidean embedding for ergodic Markov chains
-- a contribution of independent interest that enables sharp concentration
results. Stage~II refines the initial clusters via a single step of
likelihood-based reassignment. Our method achieves a near-optimal clustering
error with high probability, under the conditions $H =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} (S^2 \vee \pi_{\min}^{-1}))$ and $TH =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} S^2 )$, where $\pi_{\min}$ is the
minimum stationary probability of a state across the $K$ chains and
$\gamma_{\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements
provide significant improvements, if not at least comparable, to the
state-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm
offers a key practical advantage: unlike existing approach, it requires no
prior knowledge of model-specific quantities (e.g., separation between kernels
or visitation probabilities). We conclude by discussing the inherent gap
between our upper and lower bounds, providing insights into the unique
structure of this clustering problem.

</details>


### [473] [Signature Maximum Mean Discrepancy Two-Sample Statistical Tests](https://arxiv.org/abs/2506.01718)
*Andrew Alden,Blanka Horvath,Zacharia Issa*

Main category: stat.ML

TL;DR: Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning research. The resulting signature MMD (sig-MMD) can be used to define a metric between distributions on path space. This work is dedicated to understanding the possibilities and challenges associated with applying the sig-MMD as a statistical tool in practice.


<details>
  <summary>Details</summary>
Motivation: To explore the possibilities and challenges of using sig-MMD as a statistical tool in practice for comparing distributions on path space.

Method: Introduce and explain the sig-MMD, provide practical examples for its use, present examples that can lead to Type 2 errors, and then present techniques to mitigate these errors.

Result: Provided easily accessible and verifiable examples for the practical use of sig-MMD, identified scenarios where Type 2 errors may occur, and presented techniques to mitigate these errors.

Conclusion: The sig-MMD can be effectively used as a statistical tool to compare distributions on path space, but care must be taken to avoid Type 2 errors, especially in limited data settings.

Abstract: Maximum Mean Discrepancy (MMD) is a widely used concept in machine learning
research which has gained popularity in recent years as a highly effective tool
for comparing (finite-dimensional) distributions. Since it is designed as a
kernel-based method, the MMD can be extended to path space valued distributions
using the signature kernel. The resulting signature MMD (sig-MMD) can be used
to define a metric between distributions on path space. Similarly to the
original use case of the MMD as a test statistic within a two-sample testing
framework, the sig-MMD can be applied to determine if two sets of paths are
drawn from the same stochastic process. This work is dedicated to understanding
the possibilities and challenges associated with applying the sig-MMD as a
statistical tool in practice. We introduce and explain the sig-MMD, and provide
easily accessible and verifiable examples for its practical use. We present
examples that can lead to Type 2 errors in the hypothesis test, falsely
indicating that samples have been drawn from the same underlying process (which
generally occurs in a limited data setting). We then present techniques to
mitigate the occurrence of this type of error.

</details>


### [474] [Machine-Learned Sampling of Conditioned Path Measures](https://arxiv.org/abs/2506.01904)
*Qijia Jiang,Reuben Cohn-Gordon*

Main category: stat.ML

TL;DR: 提出了一种基于受控平衡动力学和Wasserstein度量空间优化的算法，用于从后验路径测度中采样，并可与神经网络结合以学习目标轨迹集合。


<details>
  <summary>Details</summary>
Motivation: 在没有数据访问的情况下，需要一种方法来从一般的先验过程中采样后验路径测度，并且这种方法可以与神经网络结合以学习目标轨迹集合。

Method: 利用受控平衡动力学逐步传输两个路径测度之间的信息，以及使用Wasserstein度量的空间优化技术，演化出符合指定似然性的密度曲线。

Result: 所提出的算法具有理论依据，可以在无需数据访问的情况下，与神经网络无缝集成以学习目标轨迹集合。

Conclusion: 该算法为从后验路径测度中采样提供了一种新方法，并展示了其在与神经网络结合方面的潜力。

Abstract: We propose algorithms for sampling from posterior path measures $P(C([0, T],
\mathbb{R}^d))$ under a general prior process. This leverages ideas from (1)
controlled equilibrium dynamics, which gradually transport between two path
measures, and (2) optimization in $\infty$-dimensional probability space
endowed with a Wasserstein metric, which can be used to evolve a density curve
under the specified likelihood. The resulting algorithms are theoretically
grounded and can be integrated seamlessly with neural networks for learning the
target trajectory ensembles, without access to data.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [475] [Unfolding Boxes with Local Constraints](https://arxiv.org/abs/2506.01079)
*Long Qian,Eric Wang,Bernardo Subercaseaux,Marijn J. H. Heule*

Main category: cs.CG

TL;DR: 作者提出了一种新的SAT-based方法，通过用简单局部约束替代全局约束，显著提高了计算和枚举共同盒子展开图的可扩展性。该方法能够找到更大面积的共同展开图，并反驳了Xu等人（2017）关于三个盒子最小共同展开面积的猜想。


<details>
  <summary>Details</summary>
Motivation: 现有的SAT编码在处理全局约束（如图连通性或无环性）时存在困难，导致其在大规模问题上的表现不佳。为了解决这一问题，需要一种更高效的编码方式来提高可扩展性。

Method: 提出了一种新的SAT-based方法，用简单的局部约束替代全局约束，从而改善传播属性并提高求解器性能。此方法用于计算和枚举可以折叠成多个非同构盒子的多边形。

Result: 新方法成功将共同盒子展开图的计算规模从面积88扩展到超过150，并将枚举规模从面积30扩展到60。此外，还排除了46、54和58作为三个盒子最小共同展开面积的可能性，反驳了先前的猜想。

Conclusion: 通过改进SAT编码，使用局部约束替代全局约束，显著提升了计算和枚举共同盒子展开图的能力，同时修正了关于最小共同展开面积的理论认知。

Abstract: We consider the problem of finding and enumerating polyominos that can be
folded into multiple non-isomorphic boxes. While several computational
approaches have been proposed, including SAT, randomized algorithms, and
decision diagrams, none has been able to perform at scale. We argue that
existing SAT encodings are hindered by the presence of global constraints
(e.g., graph connectivity or acyclicity), which are generally hard to encode
effectively and hard for solvers to reason about. In this work, we propose a
new SAT-based approach that replaces these global constraints with simple local
constraints that have substantially better propagation properties. Our approach
dramatically improves the scalability of both computing and enumerating common
box unfoldings: (i) while previous approaches could only find common unfoldings
of two boxes up to area 88, ours easily scales beyond 150, and (ii) while
previous approaches were only able to enumerate common unfoldings up to area
30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the
smallest areas allowing a common unfolding of three boxes, thereby refuting a
conjecture of Xu et al. (2017).

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [476] [An Accurate and Efficient Vulnerability Propagation Analysis Framework](https://arxiv.org/abs/2506.01342)
*Bonan Ruan,Zhiwei Lin,Jiahao Liu,Chuqi Zhang,Kaihang Ji,Zhenkai Liang*

Main category: cs.SE

TL;DR: This paper proposes a novel approach to accurately assess the impact of software supply chain vulnerabilities, including a hierarchical worklist-based algorithm and VPSS metric. It is implemented in the Java Maven ecosystem.


<details>
  <summary>Details</summary>
Motivation: Existing studies either have coarse granularity leading to many false positives or fail to accomplish whole-ecosystem vulnerability propagation analysis. Additionally, no metric exists to specifically quantify the dynamic impact of vulnerability propagation across software supply chains.

Method: A hierarchical worklist-based algorithm for whole-ecosystem and call-graph-level vulnerability propagation analysis and Vulnerability Propagation Scoring System (VPSS) as a dynamic metric to quantify the scope and evolution of vulnerability impacts in software supply chains.

Result: Experimental results show that the approach enables effective ecosystem-wide vulnerability propagation analysis and provides a practical, quantitative measure of vulnerability impact through VPSS.

Conclusion: The proposed approach addresses the limitations of existing methods by providing accurate and comprehensive vulnerability impact assessment.

Abstract: Identifying the impact scope and scale is critical for software supply chain
vulnerability assessment. However, existing studies face substantial
limitations. First, prior studies either work at coarse package-level
granularity, producing many false positives, or fail to accomplish
whole-ecosystem vulnerability propagation analysis. Second, although
vulnerability assessment indicators like CVSS characterize individual
vulnerabilities, no metric exists to specifically quantify the dynamic impact
of vulnerability propagation across software supply chains. To address these
limitations and enable accurate and comprehensive vulnerability impact
assessment, we propose a novel approach: (i) a hierarchical worklist-based
algorithm for whole-ecosystem and call-graph-level vulnerability propagation
analysis and (ii) the Vulnerability Propagation Scoring System (VPSS), a
dynamic metric to quantify the scope and evolution of vulnerability impacts in
software supply chains. We implement a prototype of our approach in the Java
Maven ecosystem and evaluate it on 100 real-world vulnerabilities. Experimental
results demonstrate that our approach enables effective ecosystem-wide
vulnerability propagation analysis, and provides a practical, quantitative
measure of vulnerability impact through VPSS.

</details>


### [477] [Supporting architecture evaluation for ATAM scenarios with LLMs](https://arxiv.org/abs/2506.00150)
*Rafael Capilla,J. Andrés Díaz-Pace,Yamid Ramírez,Jennifer Pérez,Vanessa Rodríguez-Horcajo*

Main category: cs.SE

TL;DR: The paper explores using LLMs, specifically MS Copilot, to automate parts of architecture evaluation for software design, finding that LLMs can provide better and more accurate results in analyzing quality scenarios.


<details>
  <summary>Details</summary>
Motivation: To reduce manual effort in architecture evaluation and make the assessment and selection of scenarios more efficient.

Method: Using MS Copilot as an LLM tool to analyze quality scenarios suggested by students in a software architecture course and comparing the students' results with the assessment provided by the LLM.

Result: In most cases, the LLM produces better and more accurate results regarding risks, sensitivity points, and tradeoff analysis of the quality scenarios.

Conclusion: Generative AI has the potential to partially automate and support architecture evaluation tasks, improving human decision-making.

Abstract: Architecture evaluation methods have long been used to evaluate software
designs. Several evaluation methods have been proposed and used to analyze
tradeoffs between different quality attributes. Having competing qualities
leads to conflicts for selecting which quality-attribute scenarios are the most
suitable ones that an architecture should tackle and for prioritizing the
scenarios required by the stakeholders. In this context, architecture
evaluation is carried out manually, often involving long brainstorming sessions
to decide which are the most adequate quality scenarios. To reduce this effort
and make the assessment and selection of scenarios more efficient, we suggest
the usage of LLMs to partially automate evaluation activities. As a first step
to validate this hypothesis, this work studies MS Copilot as an LLM tool to
analyze quality scenarios suggested by students in a software architecture
course and compares the students' results with the assessment provided by the
LLM. Our initial study reveals that the LLM produces in most cases better and
more accurate results regarding the risks, sensitivity points and tradeoff
analysis of the quality scenarios. Overall, the use of generative AI has the
potential to partially automate and support the architecture evaluation tasks,
improving the human decision-making process.

</details>


### [478] [An LLM Agent for Functional Bug Detection in Network Protocols](https://arxiv.org/abs/2506.00714)
*Mingwei Zheng,Chengpeng Wang,Xuwei Liu,Jinyao Guo,Shiwei Feng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: This paper presents RFCScan, an autonomous agent using large language models to detect functional bugs in network protocol implementations by checking conformance with RFC specifications. Evaluated on six real-world implementations, it identifies 47 bugs with high precision.


<details>
  <summary>Details</summary>
Motivation: Functional correctness of network protocol implementations is crucial for reliability and security. Traditional tools struggle to detect functional bugs due to the need for deep semantic analysis across specification documents and source code.

Method: RFCScan consists of two components: an indexing agent that hierarchically summarizes protocol code semantics to generate semantic indexes, and a detection agent that uses demand-driven retrieval to identify potential inconsistencies with RFC specifications.

Result: Evaluated on six real-world network protocol implementations, RFCScan identified 47 functional bugs with 81.9% precision, with 20 bugs confirmed or fixed by developers.

Conclusion: RFCScan demonstrates effectiveness in detecting functional bugs in network protocol implementations through its innovative use of large language models and demand-driven retrieval.

Abstract: Functional correctness is critical for ensuring the reliability and security
of network protocol implementations. Functional bugs, instances where
implementations diverge from behaviors specified in RFC documents, can lead to
severe consequences, including faulty routing, authentication bypasses, and
service disruptions. Detecting these bugs requires deep semantic analysis
across specification documents and source code, a task beyond the capabilities
of traditional static analysis tools. This paper introduces RFCScan, an
autonomous agent that leverages large language models (LLMs) to detect
functional bugs by checking conformance between network protocol
implementations and their RFC specifications. Inspired by the human auditing
procedure, RFCScan comprises two key components: an indexing agent and a
detection agent. The former hierarchically summarizes protocol code semantics,
generating semantic indexes that enable the detection agent to narrow down the
scanning scope. The latter employs demand-driven retrieval to iteratively
collect additional relevant data structures and functions, eventually
identifying potential inconsistencies with the RFC specifications effectively.
We evaluate RFCScan across six real-world network protocol implementations.
RFCScan identifies 47 functional bugs with 81.9% precision, of which 20 bugs
have been confirmed or fixed by developers.

</details>


### [479] [CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning](https://arxiv.org/abs/2506.00750)
*Monoshi Kumar Roy,Simin Chen,Benjamin Steenhoek,Jinjun Peng,Gail Kaiser,Baishakhi Ray,Wei Le*

Main category: cs.SE

TL;DR: CodeSense is a new benchmark for fine-grained code reasoning tasks in real-world software engineering, revealing performance gaps in LLMs and providing tools for future research.


<details>
  <summary>Details</summary>
Motivation: Existing code reasoning benchmarks mostly rely on synthetic datasets or educational coding problems with coarse-grained reasoning tasks, which limits their effectiveness in practical software engineering contexts.

Method: Proposed CodeSense benchmark collects Python, C and Java projects from real-world repositories, executes tests to collect execution traces and constructs a ground truth dataset for fine-grained semantic reasoning tasks. Comprehensive evaluations are performed on state-of-the-art LLMs.

Result: LLMs show a clear performance gap in handling fine-grained reasoning tasks. Prompting techniques like chain-of-thought and in-context learning help but the lack of code semantics fundamentally limits models' capabilities.

Conclusion: CodeSense offers a strong basis for future benchmark construction and model post-training by providing an execution tracing framework and tool set.

Abstract: Understanding and reasoning about code semantics is essential for enhancing
code LLMs' abilities to solve real-world software engineering (SE) tasks.
Although several code reasoning benchmarks exist, most rely on synthetic
datasets or educational coding problems and focus on coarse-grained reasoning
tasks such as input/output prediction, limiting their effectiveness in
evaluating LLMs in practical SE contexts. To bridge this gap, we propose
CodeSense, the first benchmark that makes available a spectrum of fine-grained
code reasoning tasks concerned with the software engineering of real-world
code. We collected Python, C and Java software projects from real-world
repositories. We executed tests from these repositories, collected their
execution traces, and constructed a ground truth dataset for fine-grained
semantic reasoning tasks. We then performed comprehensive evaluations on
state-of-the-art LLMs. Our results show a clear performance gap for the models
to handle fine-grained reasoning tasks. Although prompting techniques such as
chain-of-thought and in-context learning helped, the lack of code semantics in
LLMs fundamentally limit models' capabilities of code reasoning. Besides
dataset, benchmark and evaluation, our work produced an execution tracing
framework and tool set that make it easy to collect ground truth for
fine-grained SE reasoning tasks, offering a strong basis for future benchmark
construction and model post training. Our code and data are located at
https://codesense-bench.github.io/.

</details>


### [480] [Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation](https://arxiv.org/abs/2506.00788)
*Djaber Rouabhia,Ismail Hadjadj*

Main category: cs.SE

TL;DR: 九种大型语言模型（LLMs）被评估用于通过自然语言用例为无方法的UML图增加行为方法。尽管存在一些不一致性，但所有模型都生成了符合UML规范的有效PlantUML图，展示了LLMs在自动化行为建模中的潜力，但仍需人类监督以确保准确性、适当性和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 自动化地从自然语言用例中丰富UML类图的行为方法是一项重大挑战，需要寻找有效的解决方案。

Method: 研究评估了九个大型语言模型，使用21个结构化的废物管理用例，为包含21个类和17个关系的无方法UML图添加行为方法。通过六个指标对产生的90个图表进行评估：方法数量、签名丰富度、注解完整性、结构保真度、语法正确性以及命名收敛性。

Result: 所有LLM都生成了符合UML规范的有效PlantUML图。一些模型在方法覆盖率和注解准确性方面表现出色，而另一些模型则显示了更丰富的参数化但较弱的可追溯性。总的来说，LLMs能够生成结构良好的方法并具有连贯的命名。

Conclusion: 尽管LLMs在自动化行为建模方面有潜力，但在标注和签名上仍存在不一致。因此，改进提示工程和模型选择是必要的，同时人类监督对于确保准确性、适当性和语义对齐至关重要。这使得LLMs成为软件设计中的协作伙伴。

Abstract: Automating the enrichment of UML class diagrams with behavioral methods from
natural language use cases is a significant challenge. This study evaluates
nine large language models (LLMs) in augmenting a methodless UML diagram (21
classes, 17 relationships) using 21 structured waste-management use cases. A
total of 90 diagrams (3,373 methods) were assessed across six metrics: method
quantity, signature richness (visibility, names, parameters, return types),
annotation completeness (linking to use cases/actions), structural fidelity,
syntactic correctness (PlantUML compilation), and naming convergence (across
models). All LLMs produced valid PlantUML diagrams adhering to UML conventions.
Some models excelled in method coverage and annotation accuracy, while others
showed richer parameterization but weaker traceability. These results
demonstrate that LLMs can generate well-structured methods with consistent
naming, advancing automated behavioral modeling. However, inconsistencies in
annotations and signatures highlight the need for improved prompt engineering
and model selection. The rapid generation of these methods supports Agile
practices by enabling faster design iterations. Despite their capabilities,
human oversight is essential to ensure accuracy, appropriateness, and semantic
alignment. This positions LLMs as collaborative partners in software design.
All experimental artifacts (\texttt{.puml}, \texttt{.png}, \texttt{.csv}) are
publicly available for reproducibility.

</details>


### [481] [Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models](https://arxiv.org/abs/2506.00128)
*Gabriel Aracena,Kyle Luster,Fabio Santos,Igor Steinmacher,Marco A. Gerosa*

Main category: cs.SE

TL;DR: 有效利用LLM进行问题分类，减少对大规模数据集的依赖。研究中GPT-4o在多个数据集上表现出色，优于DeepSeek R1模型。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的问题报告优先级排序有助于优化资源分配和信息恢复，但手动分类费力且缺乏可扩展性。传统机器学习方法需要大量数据训练，因此探索使用大型语言模型（LLMs）作为替代方案。

Method: 采用两种主要的大型语言模型，开发基于LLM的问题分类方法，并在多个数据集上比较其性能。

Result: GPT-4o在NLBSE 2024竞赛的问题分类中表现最佳，且在NLBSE 2023竞赛数据集上，其F1得分比DeepSeek R1高出20%。微调后的GPT-4o模型平均F1得分为80.7%，而DeepSeek R1为59.33%。增加数据集大小并未提高F1得分。

Conclusion: 基于LLM的方法可以有效分类问题报告，减少对大规模数据集的需求，同时保持分类可靠性。

Abstract: Effective prioritization of issue reports in software engineering helps to
optimize resource allocation and information recovery. However, manual issue
classification is laborious and lacks scalability. As an alternative, many open
source software (OSS) projects employ automated processes for this task, yet
this method often relies on large datasets for adequate training.
Traditionally, machine learning techniques have been used for issue
classification. More recently, large language models (LLMs) have emerged as
powerful tools for addressing a range of software engineering challenges,
including code and test generation, mapping new requirements to legacy software
endpoints, and conducting code reviews. The following research investigates an
automated approach to issue classification based on LLMs. By leveraging the
capabilities of such models, we aim to develop a robust system for prioritizing
issue reports, mitigating the necessity for extensive training data while also
maintaining reliability in classification. In our research, we developed an
LLM-based approach for accurately labeling issues by selecting two of the most
prominent large language models. We then compared their performance across
multiple datasets. Our findings show that GPT-4o achieved the best results in
classifying issues from the NLBSE 2024 competition. Moreover, GPT-4o
outperformed DeepSeek R1, achieving an F1 score 20% higher when both models
were trained on the same dataset from the NLBSE 2023 competition, which was ten
times larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained
an average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved
59.33%. Increasing the dataset size did not improve the F1 score, reducing the
dependence on massive datasets for building an efficient solution to issue
classification.

</details>


### [482] [CODEMENV: Benchmarking Large Language Models on Code Migration](https://arxiv.org/abs/2506.00894)
*Keyuan Cheng,Xudong Shen,Yihao Yang,Tengyue Wang,Yang Cao,Muhammad Asif Ali,Hanbin Wang,Lijie Hu,Di Wang*

Main category: cs.SE

TL;DR: Large language models (LLMs) have not been thoroughly studied in code migration. This paper introduces CODEMENV, a benchmark to evaluate LLMs' abilities in this area. It consists of 922 examples and covers three core tasks. Experimental results with seven LLMs show an average pass@1 rate of 26.50%, with GPT-4O scoring highest at 43.84%. Key findings indicate LLMs are more proficient with newer function versions but sometimes exhibit logical inconsistencies.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the insufficient study of LLMs' effectiveness in code migration, which involves adapting code to run in different environments.

Method: The method involves introducing CODEMENV, a new benchmark specifically designed for assessing LLMs' abilities in code migration scenarios. It includes 922 examples across 19 Python and Java packages and covers three core tasks: identifying incompatible functions, detecting changes in function definitions, and adapting code to target environments.

Result: The experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include LLMs being more proficient with newer function versions and exhibiting logical inconsistencies in some cases.

Conclusion: The conclusion is that while LLMs show potential in code migration tasks, there are areas for improvement, particularly in handling legacy code and avoiding logical inconsistencies.

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various software engineering tasks; however, their effectiveness in code
migration, adapting code to run in different environments, remains
insufficiently studied. In this work, we introduce CODEMENV: Code Migration
Across Environment, a new benchmark specifically designed to assess LLMs'
abilities in code migration scenarios. CODEMENV consists of 922 examples
spanning 19 Python and Java packages, and covers three core tasks: (1)
identifying functions incompatible with specific versions, (2) detecting
changes in function definitions, and (3) adapting code to target environments.
Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1
rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings
include: (i) LLMs tend to be more proficient with newer function versions,
which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical
inconsistencies by identifying function changes irrelevant to the intended
migration environment. The datasets are available at
https://github.com/xdshen-ai/Benchmark-of-Code-Migration.

</details>


### [483] [Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models](https://arxiv.org/abs/2506.00943)
*Chanuka Wijayakoon,Hai Dong,H. M. N. Dilum Bandara,Zahir Tari,Anurag Soin*

Main category: cs.SE

TL;DR: 通过使用大型语言模型（LLMs）从自然语言法律合同生成合法合规的智能合约，研究发现尽管所有LLMs都能生成语法正确的代码，但它们的法律合规性差异显著，较大的模型通常表现出更高的合规水平。此外，提出的度量标准为自动化和自我改进的开发工作流提供了基础。


<details>
  <summary>Details</summary>
Motivation: 确保智能合约的法律合规性具有挑战性，现有的方法需要法律和软件开发领域的专业知识以及大量的手动努力。

Method: 提出了一套新的度量标准来量化法律合规性，选择四个LLMs生成20个智能合约，并基于五个法律合同分析其法律合规性。

Result: 所有LLMs生成的代码语法正确，但法律合规性存在显著差异，较大模型表现出更高水平的合规性，且所提度量标准能提供细致区分和跨域应用。

Conclusion: LLMs可以在严格审查下生成符合法律规定的智能合约初始代码，所提度量标准为自动化和自我改进的开发流程提供了基础。

Abstract: Smart contracts can implement and automate parts of legal contracts, but
ensuring their legal compliance remains challenging. Existing approaches such
as formal specification, verification, and model-based development require
expertise in both legal and software development domains, as well as extensive
manual effort. Given the recent advances of Large Language Models (LLMs) in
code generation, we investigate their ability to generate legally compliant
smart contracts directly from natural language legal contracts, addressing
these challenges. We propose a novel suite of metrics to quantify legal
compliance based on modeling both legal and smart contracts as processes and
comparing their behaviors. We select four LLMs, generate 20 smart contracts
based on five legal contracts, and analyze their legal compliance. We find that
while all LLMs generate syntactically correct code, there is significant
variance in their legal compliance with larger models generally showing higher
levels of compliance. We also evaluate the proposed metrics against properties
of software metrics, showing they provide fine-grained distinctions, enable
nuanced comparisons, and are applicable across domains for code from any
source, LLM or developer. Our results suggest that LLMs can assist in
generating starter code for legally compliant smart contracts with strict
reviews, and the proposed metrics provide a foundation for automated and
self-improving development workflows.

</details>


### [484] [Greening AI-enabled Systems with Software Engineering: A Research Agenda for Environmentally Sustainable AI Practices](https://arxiv.org/abs/2506.01774)
*Luís Cruz,João Paulo Fernandes,Maja H. Kirkeby,Silverio Martínez-Fernández,June Sallou,Hina Anwar,Enrique Barba Roque,Justus Bogner,Joel Castaño,Fernando Castor,Aadil Chasmawala,Simão Cunha,Daniel Feitosa,Alexandra González,Andreas Jedlitschka,Patricia Lago,Ana Oprescu,Pooja Rani,João Saraiva,Federica Sarro,Raghavendra Selvan,Karthik Vaidhyanathan,Roberto Verdecchia,Ivan P. Yamshchikov,Henry Muccini*

Main category: cs.SE

TL;DR: The abstract discusses a workshop on 'Greening AI with Software Engineering' which identified key challenges and proposed research agenda for developing environmentally sustainable AI systems.


<details>
  <summary>Details</summary>
Motivation: To address the rapidly increasing environmental impact of AI-enabled systems and emphasize the role of software engineering in creating sustainable solutions.

Method: Through keynotes, flash talks, and collaborative discussions among 29 interdisciplinary participants at a workshop held in Lausanne, Switzerland.

Result: Identification of key challenges such as energy assessment, benchmarking practices, sustainability-aware architectures, runtime adaptation, empirical methodologies, and education. A research agenda was presented with open research directions and practical recommendations.

Conclusion: A clear path forward has been outlined to guide the development of environmentally sustainable AI systems using software engineering principles.

Abstract: The environmental impact of Artificial Intelligence (AI)-enabled systems is
increasing rapidly, and software engineering plays a critical role in
developing sustainable solutions. The "Greening AI with Software Engineering"
CECAM-Lorentz workshop (no. 1358, 2025) funded by the Centre Europ\'een de
Calcul Atomique et Mol\'eculaire and the Lorentz Center, provided an
interdisciplinary forum for 29 participants, from practitioners to academics,
to share knowledge, ideas, practices, and current results dedicated to
advancing green software and AI research. The workshop was held February 3-7,
2025, in Lausanne, Switzerland. Through keynotes, flash talks, and
collaborative discussions, participants identified and prioritized key
challenges for the field. These included energy assessment and standardization,
benchmarking practices, sustainability-aware architectures, runtime adaptation,
empirical methodologies, and education. This report presents a research agenda
emerging from the workshop, outlining open research directions and practical
recommendations to guide the development of environmentally sustainable
AI-enabled systems rooted in software engineering principles.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [485] [Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese](https://arxiv.org/abs/2506.00019)
*William Alberto Cruz-Castañeda,Marcellus Amadeus*

Main category: cs.CL

TL;DR: This report shares the experience of developing Amadeus Verbo, a family of large language models for Brazilian Portuguese with various sizes and types. The main objective is to demonstrate the ease of fine-tuning foundation models to promote open-source development.


<details>
  <summary>Details</summary>
Motivation: To democratize the open-source development of Brazilian Portuguese LLMs when data and resources are available.

Method: Developing a family of large language models named Amadeus Verbo including base-tuned, merged, and instruction-tuned models in different sizes.

Result: A set of models with parameters ranging from 0.5B to 72B have been successfully developed and are available at HuggingFace.

Conclusion: It is easy to fine-tune foundation models to promote the open-source development of Brazilian Portuguese LLMs.

Abstract: This report introduces the experience of developing Amadeus Verbo, a family
of large language models for Brazilian Portuguese. To handle diverse use cases,
Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in
sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main
objective is to show how easy it is to fine-tune foundation models to
democratize the open-source development of Brazilian Portuguese LLMs when data
and resources are available. Amadeus-Verbo family models are all available at
HuggingFace at
https://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.

</details>


### [486] [Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs](https://arxiv.org/abs/2506.00061)
*Wiktoria Mieleszczenko-Kowszewicz,Beata Bajcar,Aleksander Szczęsny,Maciej Markiewicz,Jolanta Babiak,Berenika Dyczek,Przemysław Kazienko*

Main category: cs.CL

TL;DR: This paper presents the Social Influence Technique Taxonomy (SITT), a framework of 58 techniques in nine categories for detecting social influence in text. It evaluates LLMs' ability to identify these using a 746-dialogue dataset, showing moderate success but highlighting limitations and the need for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To systematically categorize and detect subtle forms of social influence in textual content, providing a comprehensive framework and evaluating current LLMs' capabilities in this domain.

Method: Developed SITT with 58 techniques in nine categories. Constructed a 746-dialogue corpus annotated by experts, translated from Polish to English. Used hierarchical multi-label classification to benchmark five LLMs on identifying social influence techniques.

Result: Some models like Claude 3.5 achieved moderate success (F1 score = 0.45 for categories), but overall model performance is limited, especially for context-sensitive techniques.

Conclusion: Current LLMs have key limitations in detecting nuanced linguistic cues of social influence. Domain-specific fine-tuning is important. The work provides a novel resource and evaluation approach for studying LLMs' detection and replication of social influence strategies.

Abstract: In this work we present the Social Influence Technique Taxonomy (SITT), a
comprehensive framework of 58 empirically grounded techniques organized into
nine categories, designed to detect subtle forms of social influence in textual
content. We also investigate the LLMs ability to identify various forms of
social influence. Building on interdisciplinary foundations, we construct the
SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and
translated into English -- to evaluate the ability of LLMs to identify these
techniques. Using a hierarchical multi-label classification setup, we benchmark
five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our
results show that while some models, notably Claude 3.5, achieved moderate
success (F1 score = 0.45 for categories), overall performance of models remains
limited, particularly for context-sensitive techniques. The findings
demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic
cues and underscore the importance of domain-specific fine-tuning. This work
contributes a novel resource and evaluation example for understanding how LLMs
detect, classify, and potentially replicate strategies of social influence in
natural dialogues.

</details>


### [487] [Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling](https://arxiv.org/abs/2506.00064)
*Jiayi Zeng,Yizhe Feng,Mengliang He,Wenhui Lei,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Main category: cs.CL

TL;DR: Large language models (LLMs) need to improve in proactive error handling without explicit instructions. A new benchmark called Mis-prompt with four evaluation tasks, an error category taxonomy, and a new evaluation dataset is introduced. Experiments show LLMs perform poorly in this area but can improve with SFT on error handling instances.


<details>
  <summary>Details</summary>
Motivation: Current error-handling methods require explicit instructions which are often unavailable in real-world scenarios. There is a need for LLMs to handle errors proactively without these explicit instructions.

Method: Introduced a new benchmark named Mis-prompt that includes four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Analyzed the performance of current LLMs on this benchmark.

Result: Current LLMs demonstrate poor performance in proactive error handling. However, applying Supervised Fine-Tuning (SFT) on error handling instances improves their proactive error handling capabilities.

Conclusion: The research highlights the challenge of proactive error handling in LLMs and shows that while current models struggle, they can be enhanced with specific fine-tuning.

Abstract: Large language models (LLMs) have demonstrated significant advancements in
error handling. Current error-handling works are performed in a passive manner,
with explicit error-handling instructions. However, in real-world scenarios,
explicit error-handling instructions are usually unavailable. In this paper,
our work identifies this challenge as how to conduct proactive error handling
without explicit error handling instructions. To promote further research, this
work introduces a new benchmark, termed Mis-prompt, consisting of four
evaluation tasks, an error category taxonomy, and a new evaluation dataset.
Furthermore, this work analyzes current LLMs' performance on the benchmark, and
the experimental results reveal that current LLMs show poor performance on
proactive error handling, and SFT on error handling instances improves LLMs'
proactive error handling capabilities. The dataset will be publicly available.

</details>


### [488] [You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models](https://arxiv.org/abs/2506.00065)
*Dota Tianai Dong,Yifan Luo,Po-Ya Angela Wang,Asli Ozyurek,Paula Rubio-Fernandez*

Main category: cs.CL

TL;DR: Multimodal language models struggle with reference words, especially possessive and demonstrative pronouns, due to limitations in perspective-taking and spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how well multimodal language models (MLMs) use reference words compared to humans, focusing on word classes with increasing cognitive demands.

Method: Compared human and MLM use of vocabulary words, possessive pronouns, and demonstrative pronouns. Evaluated seven state-of-the-art MLMs against human participants.

Result: MLMs perform near human-level on vocabulary tasks but show significant deficits in using possessive and demonstrative pronouns, revealing difficulties in perspective-taking and spatial reasoning.

Conclusion: Current NLP systems face challenges in producing grammatical forms that require pragmatics and social cognition.

Abstract: Multimodal language models (MLMs) increasingly communicate in human-like
ways, yet their ability to use reference words remains largely overlooked
despite their ubiquity in everyday communication. Our study addresses this gap
by comparing human and MLM use of three word classes with increasing cognitive
demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and
demonstrative pronouns (`this one' vs `that one'). Evaluating seven
state-of-the-art MLMs against human participants, we observe a clear difficulty
hierarchy: while MLMs approach human-level performance on the vocabulary task,
they show substantial deficits with possessives and demonstratives. Our
analysis reveals these difficulties stem from limitations in perspective-taking
and spatial reasoning. Although prompt engineering improved model performance
on possessive use, demonstrative use remained well below human-level
competence. These findings provide theoretical and empirical evidence that
producing grammatical forms requiring pragmatics and social cognition remains a
clear challenge in current NLP systems.

</details>


### [489] [Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages](https://arxiv.org/abs/2506.00068)
*Afrozah Nadeem,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: This paper analyzes political bias in 13 LLMs across 5 low-resource Pakistani languages, revealing a predominance of liberal-left values with shifts towards authoritarian framing in regional languages, and calls for culturally grounded multilingual bias auditing frameworks.


<details>
  <summary>Details</summary>
Motivation: To systematically examine politico-economic biases in Large Language Models within non-Western, low-resource multilingual contexts, specifically focusing on languages spoken in Pakistan.

Method: A novel framework integrating an adapted Political Compass Test with multi-level framing analysis is proposed. This method quantitatively assesses political orientation on economic and social axes while qualitatively analyzing framing through content, style, and emphasis. Prompts are aligned with socio-political themes relevant to Pakistani society.

Result: LLMs mainly align with liberal-left values, reflecting Western training data influences, but show shifts towards authoritarian framing in regional languages. Model-specific bias signatures and language-conditioned variations in ideological expression were identified.

Conclusion: There is an urgent need for culturally grounded, multilingual bias auditing frameworks to address the observed biases in LLMs.

Abstract: Large Language Models (LLMs) are increasingly shaping public discourse, yet
their politico-economic biases remain underexamined in non-Western and
low-resource multilingual contexts. This paper presents a systematic analysis
of political bias in 13 state-of-the-art LLMs across five low-resource
languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We
propose a novel framework that integrates an adapted Political Compass Test
(PCT) with a multi-level framing analysis. Our method combines quantitative
assessment of political orientation across economic (left-right) and social
(libertarian-authoritarian) axes with qualitative analysis of framing through
content, style, and emphasis. We further contextualize this analysis by
aligning prompts with 11 key socio-political themes relevant to Pakistani
society. Our results reveal that LLMs predominantly align with liberal-left
values, echoing Western training data influences, but exhibit notable shifts
toward authoritarian framing in regional languages, suggesting strong cultural
modulation effects. We also identify consistent model-specific bias signatures
and language-conditioned variations in ideological expression. These findings
show the urgent need for culturally grounded, multilingual bias auditing
frameworks.

</details>


### [490] [Evaluating the Sensitivity of LLMs to Prior Context](https://arxiv.org/abs/2506.00069)
*Robert Hankache,Kingsley Nketia Acheampong,Liang Song,Marek Brynda,Raad Khraishi,Greig A. Cowan*

Main category: cs.CL

TL;DR: 尽管大语言模型（LLMs）在多轮对话等持续交互场景中被越来越多地部署，但目前的基准测试主要集中在单轮问答任务上，未能反映多轮交流的影响。为此，我们引入了一套新的基准测试，系统地变化先前上下文的数量和性质，并评估了包括GPT、Claude和Gemini在内的多个传统LLM对上下文变化的敏感性。结果表明，在多轮交互中，LLM在多项选择题上的表现可能会显著下降，某些模型的表现下降幅度高达73%，即使是像GPT-4o这样的高性能模型准确率也下降了32%。此外，较大模型与较小模型之间的相对表现并不总是可预测的。更重要的是，将任务描述战略性地放置在上下文中可以大大减轻性能下降，提高多达3.5倍的准确性。这些发现强调了需要强大的策略来设计、评估和缓解LLM中的上下文相关敏感性。


<details>
  <summary>Details</summary>
Motivation: 当前流行的基准测试主要关注单轮问答任务，无法充分捕捉多轮交互对话中上下文对大语言模型（LLMs）性能的影响。因此，研究者们希望填补这一空白，理解扩展上下文如何影响LLMs的性能。

Method: 研究者们引入了一组新的基准测试，系统地改变先前上下文的数量和性质。然后，使用这些基准测试评估多个传统的LLM（如GPT、Claude和Gemini），以测量它们对上下文变化的敏感性。

Result: 研究表明，LLM在多轮交互中的多项选择题上的表现可能会显著下降，某些模型的表现下降幅度高达73%，即使是非常先进的模型如GPT-4o也出现了32%的准确率下降。此外，通过战略性地放置任务描述，可以显著减轻性能下降，最多可提高3.5倍的准确性。

Conclusion: 为了更好地设计、评估和缓解LLM中的上下文相关敏感性，需要采取稳健的策略。这强调了在多轮交互场景中理解和优化LLM性能的重要性。

Abstract: As large language models (LLMs) are increasingly deployed in multi-turn
dialogue and other sustained interactive scenarios, it is essential to
understand how extended context affects their performance. Popular benchmarks,
focusing primarily on single-turn question answering (QA) tasks, fail to
capture the effects of multi-turn exchanges. To address this gap, we introduce
a novel set of benchmarks that systematically vary the volume and nature of
prior context. We evaluate multiple conventional LLMs, including GPT, Claude,
and Gemini, across these benchmarks to measure their sensitivity to contextual
variations. Our findings reveal that LLM performance on multiple-choice
questions can degrade dramatically in multi-turn interactions, with performance
drops as large as 73% for certain models. Even highly capable models such as
GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative
performance of larger versus smaller models is not always predictable.
Moreover, the strategic placement of the task description within the context
can substantially mitigate performance drops, improving the accuracy by as much
as a factor of 3.5. These findings underscore the need for robust strategies to
design, evaluate, and mitigate context-related sensitivity in LLMs.

</details>


### [491] [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085)
*Vincent Siu,Nicholas Crispino,Zihao Yu,Sam Pan,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.CL

TL;DR: COSMIC is an automated framework that identifies viable steering directions and target layers in LLMs using cosine similarity, without relying on predefined refusal templates or manual analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying refusal behaviors in LLMs and to develop a method that does not rely on predefined refusal templates or require manual analysis.

Method: The COSMIC framework uses cosine similarity metrics for inversion of concepts to identify viable steering directions and target layers in LLMs. It is independent of model outputs and does not require assumptions about the model's refusal behavior.

Result: COSMIC achieves steering performance comparable to prior methods but without needing specific refusal tokens. It reliably identifies refusal directions in adversarial settings and weakly aligned models, and can steer these models towards safer behavior with minimal increase in false refusals.

Conclusion: COSMIC demonstrates robustness across a wide range of alignment conditions and provides an effective way to steer LLMs towards safer behavior.

Abstract: Large Language Models (LLMs) encode behaviors such as refusal within their
activation space, yet identifying these behaviors remains a significant
challenge. Existing methods often rely on predefined refusal templates
detectable in output tokens or require manual analysis. We introduce
\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an
automated framework for direction selection that identifies viable steering
directions and target layers using cosine similarity - entirely independent of
model outputs. COSMIC achieves steering performance comparable to prior methods
without requiring assumptions about a model's refusal behavior, such as the
presence of specific refusal tokens. It reliably identifies refusal directions
in adversarial settings and weakly aligned models, and is capable of steering
such models toward safer behavior with minimal increase in false refusals,
demonstrating robustness across a wide range of alignment conditions.

</details>


### [492] [SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset](https://arxiv.org/abs/2506.00087)
*Peng Xie,Xingyuan Liu,Tsz Wai Chan,Yequan Bie,Yangqiu Song,Yang Wang,Hao Chen,Kani Chen*

Main category: cs.CL

TL;DR: The paper presents LinguaMaster, a framework for synthesizing multilingual data, and SwitchLingua, a large-scale code-switching dataset across 12 languages. It also introduces Semantic-Aware Error Rate (SAER), a new metric for evaluating ASR systems in code-switching contexts.


<details>
  <summary>Details</summary>
Motivation: Code-switching poses challenges for ASR systems due to the lack of large-scale, diverse datasets that capture multilingual and multicultural dynamics.

Method: Developed LinguaMaster, a multi-agent collaboration framework for efficient multilingual data synthesis. Curated SwitchLingua, a large-scale code-switching dataset with 420K textual samples across 12 languages and over 80 hours of audio from speakers of diverse backgrounds. Proposed SAER, a novel evaluation metric sensitive to code-switching scenarios.

Result: SwitchLingua offers rich linguistic and cultural diversity, serving as a foundational resource for advancing research in multilingual and multicultural domains. SAER provides a more accurate assessment of system performance in code-switching contexts.

Conclusion: The introduction of SwitchLingua and SAER addresses critical gaps in code-switching research, promoting the development of more effective multilingual applications.

Abstract: Code-switching (CS) is the alternating use of two or more languages within a
conversation or utterance, often influenced by social context and speaker
identity. This linguistic phenomenon poses challenges for Automatic Speech
Recognition (ASR) systems, which are typically designed for a single language
and struggle to handle multilingual inputs. The growing global demand for
multilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech
(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the
inadequacy of existing monolingual datasets.
  Although some code-switching datasets exist, most are limited to bilingual
mixing within homogeneous ethnic groups, leaving a critical need for a
large-scale, diverse benchmark akin to ImageNet in computer vision.
  To bridge this gap, we introduce \textbf{LinguaMaster}, a multi-agent
collaboration framework specifically designed for efficient and scalable
multilingual data synthesis. Leveraging this framework, we curate
\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic
code-switching dataset, including: (1) 420K CS textual samples across 12
languages, and (2) over 80 hours of audio recordings from 174 speakers
representing 18 countries/regions and 63 racial/ethnic backgrounds, based on
the textual data. This dataset captures rich linguistic and cultural diversity,
offering a foundational resource for advancing multilingual and multicultural
research. Furthermore, to address the issue that existing ASR evaluation
metrics lack sensitivity to code-switching scenarios, we propose the
\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that
incorporates semantic information, providing a more accurate and context-aware
assessment of system performance.

</details>


### [493] [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088)
*Qing Li,Jiahui Geng,Zongxiong Chen,Derui Zhu,Yuxia Wang,Congbo Ma,Chenyang Lyu,Fakhri Karray*

Main category: cs.CL

TL;DR: In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination remains a significant challenge. Current classification-based methods are efficient but unreliable when non-factual information arises in the early or mid-sequence of outputs. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space.


<details>
  <summary>Details</summary>
Motivation: Hallucination is a significant challenge for real-world deployment of large language models. Current classification-based methods struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability.

Method: The proposed method, Hallucination Detection-Neural Differential Equations (HD-NDEs), applies neural differential equations to model the dynamic system in the latent space of LLMs. The sequence in the latent space is then mapped to the classification space for truth assessment.

Result: Extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs. It achieves over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.

Conclusion: HD-NDEs is a novel and effective method for detecting hallucinations in LLMs, improving upon current state-of-the-art techniques.

Abstract: In recent years, large language models (LLMs) have made remarkable
advancements, yet hallucination, where models produce inaccurate or non-factual
statements, remains a significant challenge for real-world deployment. Although
current classification-based methods, such as SAPLMA, are highly efficient in
mitigating hallucinations, they struggle when non-factual information arises in
the early or mid-sequence of outputs, reducing their reliability. To address
these issues, we propose Hallucination Detection-Neural Differential Equations
(HD-NDEs), a novel method that systematically assesses the truthfulness of
statements by capturing the full dynamics of LLMs within their latent space.
Our approaches apply neural differential equations (Neural DEs) to model the
dynamic system in the latent space of LLMs. Then, the sequence in the latent
space is mapped to the classification space for truth assessment. The extensive
experiments across five datasets and six widely used LLMs demonstrate the
effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC
on the True-False dataset compared to state-of-the-art techniques.

</details>


### [494] [Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models](https://arxiv.org/abs/2506.00134)
*Fardin Ahsan Sakib,Ziwei Zhu,Karen Trister Grace,Meliha Yetisgen,Ozlem Uzuner*

Main category: cs.CL

TL;DR: The paper explores the issue of Social Determinants of Health (SDOH) extraction from clinical text using large language models (LLMs), focusing on drug status extraction. It finds that LLMs may rely on superficial cues like alcohol or smoking mentions to falsely predict drug use and uncovers gender disparities in model performance. The study evaluates mitigation strategies such as prompt engineering and chain-of-thought reasoning to reduce false positives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical need for accurate SDOH extraction from clinical text, highlighting the potential problems with LLMs' reliance on superficial cues which can lead to spurious predictions in healthcare analytics.

Method: Using the MIMIC portion of the SHAC dataset, the research focuses on drug status extraction as a case study. They analyze how mentions of alcohol or smoking affect LLMs' predictions of drug use and investigate gender disparities in model performance. Mitigation strategies including prompt engineering and chain-of-thought reasoning are evaluated.

Result: The results show that alcohol or smoking mentions can falsely induce models to predict drug use where none exists, and gender disparities exist in model performance. The mitigation strategies help reduce these false positives and enhance LLM reliability.

Conclusion: Accurate SDOH extraction is essential for healthcare analytics, but LLMs require careful handling due to their reliance on superficial cues. The study provides insights into improving LLM reliability through specific mitigation strategies.

Abstract: Social determinants of health (SDOH) extraction from clinical text is
critical for downstream healthcare analytics. Although large language models
(LLMs) have shown promise, they may rely on superficial cues leading to
spurious predictions. Using the MIMIC portion of the SHAC (Social History
Annotation Corpus) dataset and focusing on drug status extraction as a case
study, we demonstrate that mentions of alcohol or smoking can falsely induce
models to predict current/past drug use where none is present, while also
uncovering concerning gender disparities in model performance. We further
evaluate mitigation strategies - such as prompt engineering and
chain-of-thought reasoning - to reduce these false positives, providing
insights into enhancing LLM reliability in health domains.

</details>


### [495] [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)
*Mingqian Zheng,Wenjia Hu,Patrick Zhao,Motahhare Eslami,Jena D. Hwang,Faeze Brahman,Carolyn Rose,Maarten Sap*

Main category: cs.CL

TL;DR: Current LLMs face a tradeoff between safety and user experience due to their tendency to refuse potentially harmful queries without considering user intent. A study with 480 participants evaluating 3,840 query-response pairs shows that response strategy significantly impacts user experience more than actual user motivation. Partial compliance, which provides general information without actionable details, is the optimal refusal strategy, reducing negative perceptions by over 50% compared to outright refusals. However, state-of-the-art LLMs rarely use partial compliance naturally, and reward models undervalue it. The work suggests focusing on thoughtful refusals rather than intent detection for effective guardrails.


<details>
  <summary>Details</summary>
Motivation: To address the tradeoff between safety and user experience in current LLMs that refuse potentially harmful queries regardless of user intent.

Method: Conducting a study with 480 participants evaluating 3,840 query-response pairs to examine the impact of different refusal strategies on user perceptions across varying motivations. Analyzing response patterns of 9 state-of-the-art LLMs and evaluating how 6 reward models score different refusal strategies.

Result: Partial compliance emerges as the optimal refusal strategy, reducing negative user perceptions significantly compared to outright refusals. State-of-the-art LLMs rarely deploy partial compliance naturally, and reward models currently undervalue it.

Conclusion: Effective guardrails for AI safety require focusing on crafting thoughtful refusals rather than detecting intent, ensuring both safety and sustained user engagement.

Abstract: Current LLMs are trained to refuse potentially harmful input queries
regardless of whether users actually had harmful intents, causing a tradeoff
between safety and user experience. Through a study of 480 participants
evaluating 3,840 query-response pairs, we examine how different refusal
strategies affect user perceptions across varying motivations. Our findings
reveal that response strategy largely shapes user experience, while actual user
motivation has negligible impact. Partial compliance -- providing general
information without actionable details -- emerges as the optimal strategy,
reducing negative user perceptions by over 50% to flat-out refusals.
Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and
evaluate how 6 reward models score different refusal strategies, demonstrating
that models rarely deploy partial compliance naturally and reward models
currently undervalue it. This work demonstrates that effective guardrails
require focusing on crafting thoughtful refusals rather than detecting intent,
offering a path toward AI safety mechanisms that ensure both safety and
sustained user engagement.

</details>


### [496] [Structure-Aware Fill-in-the-Middle Pretraining for Code](https://arxiv.org/abs/2506.00204)
*Linyuan Gong,Alvin Cheung,Mostafa Elhoushi,Sida Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为AST-FIM的预训练策略，利用抽象语法树（ASTs）来屏蔽完整的语法结构，以生成更连贯的训练示例。并通过Real-FIM-Eval基准评估了实际的FIM编程任务，结果表明AST-FIM在真实代码编辑任务中优于传统的随机字符屏蔽方法。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在处理代码时将其视为纯文本，并随机屏蔽字符跨度，这与代码的实际语法结构不匹配。因此，需要一种新的预训练方法来更好地捕捉代码的语法结构和常见的代码编辑模式。

Method: 提出了一种基于抽象语法树（ASTs）的预训练策略AST-FIM，该策略屏蔽完整的语法结构，而不是随机字符。同时，引入了一个名为Real-FIM-Eval的基准，用于评估真实的FIM编程任务。

Result: 在具有1B和8B参数的模型上的实验表明，AST-FIM在真实世界的代码编辑任务中表现优异，比标准的随机字符屏蔽方法高出5个百分点。

Conclusion: AST-FIM是一种有效的预训练策略，能够生成更符合代码结构的训练示例，从而提升模型在真实代码编辑任务中的性能。

Abstract: Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where
models complete code segments given surrounding context. However, existing LLMs
treat code as plain text and mask random character spans. We propose and
evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees
(ASTs) to mask complete syntactic structures at scale, ensuring coherent
training examples better aligned with universal code structures and common code
editing patterns such as blocks, expressions, or functions. To evaluate
real-world fill-in-the-middle (FIM) programming tasks, we introduce
Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12
languages. On infilling tasks, experiments on 1B and 8B parameter models show
that AST-FIM is particularly beneficial for real-world code editing as it
outperforms standard random-character FIM by up to 5 pts on standard FIM
benchmarks. Our code is publicly available at
https://github.com/gonglinyuan/ast_fim.

</details>


### [497] [REIC: RAG-Enhanced Intent Classification at Scale](https://arxiv.org/abs/2506.00210)
*Ziji Zhang,Michael Yang,Zhiyu Chen,Yingying Zhuang,Shu-Ting Pi,Qun Liu,Rajashekar Maragoud,Vy Nguyen,Anurag Beniwal*

Main category: cs.CL

TL;DR: An accurate intent classification method named REIC is proposed to solve the scalability challenges in customer service. It uses retrieval-augmented generation to incorporate knowledge dynamically and has been proven effective through experiments.


<details>
  <summary>Details</summary>
Motivation: As companies expand their product lines, the increasing number of intents and variations in taxonomy across different verticals pose scalability challenges for intent classification in customer service.

Method: REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge without frequent retraining.

Result: REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings, both in-domain and out-of-domain scenarios.

Conclusion: REIC demonstrates potential for real-world deployment in adaptive and large-scale intent classification systems.

Abstract: Accurate intent classification is critical for efficient routing in customer
service, ensuring customers are connected with the most suitable agents while
reducing handling times and operational costs. However, as companies expand
their product lines, intent classification faces scalability challenges due to
the increasing number of intents and variations in taxonomy across different
verticals. In this paper, we introduce REIC, a Retrieval-augmented generation
Enhanced Intent Classification approach, which addresses these challenges
effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically
incorporate relevant knowledge, enabling precise classification without the
need for frequent retraining. Through extensive experiments on real-world
datasets, we demonstrate that REIC outperforms traditional fine-tuning,
zero-shot, and few-shot methods in large-scale customer service settings. Our
results highlight its effectiveness in both in-domain and out-of-domain
scenarios, demonstrating its potential for real-world deployment in adaptive
and large-scale intent classification systems.

</details>


### [498] [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
*Lihao Sun,Chengzhi Mao,Valentin Hofmann,Xuechunzi Bai*

Main category: cs.CL

TL;DR: 尽管价值对齐的语言模型（LMs）在显式偏见评估中看似无偏，但它们在隐式的词汇关联任务中常表现出刻板印象。我们发现，对齐过程意外地放大了模型输出中的隐性偏见。具体来说，与未对齐的模型不同，对齐后的语言模型在上下文模糊时会忽略种族概念的早期内部表示，这可能导致安全防护机制失效，产生非预期的偏见。受此启发，我们提出了一种新的偏见缓解策略，通过激励模型早期层对种族概念的表征来减少隐性偏见。与传统的机器遗忘方法不同，引导模型更多地关注种族概念可以有效减轻隐性偏见。


<details>
  <summary>Details</summary>
Motivation: 研究者注意到尽管现有的价值对齐语言模型在显式偏见测试中表现良好，但在隐式词联想任务中却展现出刻板印象，因此他们希望探索这种差异背后的机制，并寻找更有效的偏见缓解策略。

Method: 研究者首先分析了对齐和未对齐语言模型在处理模糊上下文时对种族概念的表征差异，发现对齐模型在早期内部表示中忽略了种族概念，从而未能激活安全防护机制。基于这一发现，他们提出了一个新的偏见缓解策略，即通过激励模型在早期层中更多地表征种族概念来减轻隐性偏见。

Result: 实验结果表明，引导模型增加对种族概念的关注能够有效地减少隐性偏见，这种方法比传统的机器遗忘技术更为有效。

Conclusion: 本研究表明，语言模型中的种族忽视现象可能会无意间延续微妙的偏见。为了更好地缓解隐性偏见，应该采用新的策略，让模型更加意识到种族概念，而不是简单地忽略它们。

Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias
evaluations, they often exhibit stereotypes in implicit word association tasks,
raising concerns about their fair usage. We investigate the mechanisms behind
this discrepancy and find that alignment surprisingly amplifies implicit bias
in model outputs. Specifically, we show that aligned LMs, unlike their
unaligned counterparts, overlook racial concepts in early internal
representations when the context is ambiguous. Not representing race likely
fails to activate safety guardrails, leading to unintended biases. Inspired by
this insight, we propose a new bias mitigation strategy that works by
incentivizing the representation of racial concepts in the early model layers.
In contrast to conventional mitigation methods of machine unlearning, our
interventions find that steering the model to be more aware of racial concepts
effectively mitigates implicit bias. Similar to race blindness in humans,
ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.

</details>


### [499] [Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings](https://arxiv.org/abs/2506.00277)
*Hans W. A. Hanley,Zakir Durumeric*

Main category: cs.CL

TL;DR: 研究人员提出了一种新颖的多语言嵌入方法Matryoshka，能够根据嵌入维度的不同子集确定故事相似性，并开发了一种高效的分层聚类算法来识别独特的新闻故事、叙述和主题。该模型在SemEval 2022任务8测试数据集上取得了最先进的性能（皮尔逊ρ=0.816）。


<details>
  <summary>Details</summary>
Motivation: 当前的文本聚类方法扩展性差、依赖不透明的相似性度量，并且在多语言环境中表现不佳。

Method: 首先训练多语言Matryoshka嵌入，通过检查嵌入维度的不同子集来确定故事相似性；然后开发一种有效的分层聚类算法，利用Matryoshka嵌入的层次结构来识别独特的新闻故事、叙述和主题。

Result: 该方法在SemEval 2022任务8测试数据集上达到了最先进的性能，皮尔逊相关系数为0.816。并且能够成功地识别和聚类真实世界新闻数据集中的故事、叙述和总体主题。

Conclusion: 所提出的方法是一种新颖、可扩展、可解释、分层和多语言的新闻文章和社会媒体数据聚类方法，能够有效地识别和聚类新闻故事、叙述和主题。

Abstract: Contextual large language model embeddings are increasingly utilized for
topic modeling and clustering. However, current methods often scale poorly,
rely on opaque similarity metrics, and struggle in multilingual settings. In
this work, we present a novel, scalable, interpretable, hierarchical, and
multilingual approach to clustering news articles and social media data. To do
this, we first train multilingual Matryoshka embeddings that can determine
story similarity at varying levels of granularity based on which subset of the
dimensions of the embeddings is examined. This embedding model achieves
state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson
$\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering
algorithm that leverages the hierarchical nature of Matryoshka embeddings to
identify unique news stories, narratives, and themes. We conclude by
illustrating how our approach can identify and cluster stories, narratives, and
overarching themes within real-world news datasets.

</details>


### [500] [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)
*Ahmed Elhady,Eneko Agirre,Mikel Artetxe*

Main category: cs.CL

TL;DR: 在持续预训练（CPT）以适应新语言时，包含英语数据对验证困惑度无影响，但对目标语言的下游能力至关重要。不包含英语会导致灾难性遗忘，损害模型在下游任务中的表现。研究引入课程学习和权重指数移动平均（EMA）作为有效替代方案，减少对英语的需求，为未来更有效的方法提供了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管在使用持续预训练（CPT）方法来使现有大语言模型（LLMs）适应新语言时通常会包含一部分英语数据，但其作用尚未被深入研究。

Method: 通过引入一种与语言无关的上下文学习（ICL）基准，研究了在CPT过程中包含英语数据的影响，并分析了灾难性遗忘现象及其对模型参数的显著影响。同时提出课程学习和权重指数移动平均（EMA）作为减少对英语需求的有效方法。

Result: 研究表明，包含英语数据对验证困惑度无直接影响，但在目标语言中出现下游能力方面起着关键作用。如果不包含英语，将导致早期灾难性遗忘，从而损害模型在目标语言中的泛化能力。而提出的课程学习和EMA方法可以有效缓解这一问题。

Conclusion: 本研究揭示了在进行语言适应的CPT过程中，新兴能力出现的动力学机制，为设计更有效的CPT方法提供了理论基础和实践指导。

Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large
language models (LLMs) to new languages. When doing so, it is common practice
to include a portion of English data in the mixture, but its role has not been
carefully studied to date. In this work, we show that including English does
not impact validation perplexity, yet it is critical for the emergence of
downstream capabilities in the target language. We introduce a
language-agnostic benchmark for in-context learning (ICL), which reveals
catastrophic forgetting early on CPT when English is not included. This in turn
damages the ability of the model to generalize to downstream prompts in the
target language as measured by perplexity, even if it does not manifest in
terms of accuracy until later in training, and can be tied to a big shift in
the model parameters. Based on these insights, we introduce curriculum learning
and exponential moving average (EMA) of weights as effective alternatives to
mitigate the need for English. All in all, our work sheds light into the
dynamics by which emergent abilities arise when doing CPT for language
adaptation, and can serve as a foundation to design more effective methods in
the future.

</details>


### [501] [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)
*John Harvill,Ziwei Fan,Hao Wang,Yizhou Sun,Hao Ding,Luke Huan,Anoop Deoras*

Main category: cs.CL

TL;DR: This paper presents a task-agnostic lossless compression technique for LLM prompts, achieving significant reductions in input token sequence length and encoding computation without losing semantic information.


<details>
  <summary>Details</summary>
Motivation: Existing work on prompt compression focuses on lossy methods which may not be suitable for tasks requiring strict preservation of semantics/syntax. Thus, there is a need for a lossless compression method.

Method: The authors introduce a task-agnostic lossless compression technique similar to LZ77, which reduces the input token sequence length by 27% and 18% for two evaluation tasks respectively.

Result: This lossless compression technique significantly reduces encoding computation (47% and 33% less) due to the quadratic nature of attention. It performs only slightly worse than using uncompressed inputs, with the gap expected to close with larger models and more computing resources.

Conclusion: A task-agnostic lossless compression technique for LLM prompts has been developed, offering substantial reductions in input length and computation without loss of semantic information.

Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses
on lossy methods that try to maximize the retention of semantic information
that is relevant to downstream tasks while significantly reducing the sequence
length. In this paper, we introduce a task-agnostic lossless compression
technique similar to LZ77 that makes it possible to reduce the input token
sequence length on average by 27\% and 18\% for the two evaluation tasks
explored here. Given that we use transformer-based LLMs, this equates to 47\%
and 33\% less encoding computation, respectively, due to the quadratic nature
of attention. The token sequence transformation is trivial to reverse and
highlights that no semantic information is lost in the process. We evaluate our
proposed approach on two tasks that require strict preservation of
semantics/syntax and demonstrate that existing lossy compression methods
perform poorly in this setting. We find that our lossless compression technique
produces only a small gap in performance compared to using the uncompressed
input and posit that larger models and an expanded computing budget would
likely erase the gap entirely.

</details>


### [502] [An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3](https://arxiv.org/abs/2506.00312)
*Brendan Sands,Yining Wang,Chenhao Xu,Yuxuan Zhou,Lai Wei,Rohitash Chandra*

Main category: cs.CL

TL;DR: This paper proposes a framework that uses three LLMs to generate movie reviews and evaluates their performance by comparing with IMDb user reviews.


<details>
  <summary>Details</summary>
Motivation: To explore the capability of LLMs in generating high-quality movie reviews similar to human-generated ones.

Method: The study employs three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0) using movie subtitles and screenplays as input to generate movie reviews. The generated reviews are evaluated against IMDb user reviews based on vocabulary, sentiment polarity, similarity, and thematic consistency.

Result: LLMs can generate syntactically fluent and structurally complete movie reviews. However, there is a noticeable gap in emotional richness and stylistic coherence compared to IMDb reviews. DeepSeek-V3 produced the most balanced reviews, GPT-4o overemphasised positive emotions, and Gemini-2.0 better captured negative emotions but with excessive intensity.

Conclusion: While LLMs show potential in movie review generation, improvements are needed for emotional richness and stylistic coherence. The generated reviews are difficult to distinguish from human-generated ones.

Abstract: Large language models (LLMs) have been prominent in various tasks, including
text generation and summarisation. The applicability of LLMs to the generation
of product reviews is gaining momentum, paving the way for the generation of
movie reviews. In this study, we propose a framework that generates movie
reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate
their performance by comparing the generated outputs with IMDb user reviews. We
use movie subtitles and screenplays as input to the LLMs and investigate how
they affect the quality of reviews generated. We review the LLM-based movie
reviews in terms of vocabulary, sentiment polarity, similarity, and thematic
consistency in comparison to IMDB user reviews. The results demonstrate that
LLMs are capable of generating syntactically fluent and structurally complete
movie reviews. Nevertheless, there is still a noticeable gap in emotional
richness and stylistic coherence between LLM-generated and IMDb reviews,
suggesting that further refinement is needed to improve the overall quality of
movie review generation. We provided a survey-based analysis where participants
were told to distinguish between LLM and IMDb user reviews. The results show
that LLM-generated reviews are difficult to distinguish from IMDB user reviews.
We found that DeepSeek-V3 produced the most balanced reviews, closely matching
IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0
captured negative emotions better but showed excessive emotional intensity.

</details>


### [503] [Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs](https://arxiv.org/abs/2506.00344)
*Sungjae Lee,Hoyoung Kim,Jeongyeon Hwang,Eunhyeok Park,Jungseul Ok*

Main category: cs.CL

TL;DR: The paper introduces Latent Semantic Clustering (LSC), a method using internal hidden states of LLMs for semantic clustering to improve computational efficiency in test-time scaling without performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing semantic clustering methods that rely on external models, leading to high computational costs and insufficient context-aware semantics.

Method: Propose Latent Semantic Clustering (LSC), which utilizes the internal hidden states of generator LLMs for clustering outputs, avoiding the use of external models.

Result: Experiments across various LLMs and datasets demonstrate that LSC enhances computational efficiency in test-time scaling while maintaining or surpassing current methods' performance.

Conclusion: LSC is a lightweight, context-sensitive approach that effectively improves test-time scaling efficiency for LLMs.

Abstract: Scaling test-time computation--generating and analyzing multiple or
sequential outputs for a single input--has become a promising strategy for
improving the reliability and quality of large language models (LLMs), as
evidenced by advances in uncertainty quantification and multi-step reasoning. A
key shared component is semantic clustering, which groups outputs that differ
in form but convey the same meaning. Semantic clustering enables estimation of
the distribution over the semantics of outputs and helps avoid redundant
exploration of reasoning paths. However, existing approaches typically rely on
external models, which introduce substantial computational overhead and often
fail to capture context-aware semantics. We propose Latent Semantic Clustering
(LSC), a lightweight and context-sensitive method that leverages the generator
LLM's internal hidden states for clustering, eliminating the need for external
models. Our extensive experiment across various LLMs and datasets shows that
LSC significantly improves the computational efficiency of test-time scaling
while maintaining or exceeding the performance of existing methods.

</details>


### [504] [Scaling Textual Gradients via Sampling-Based Momentum](https://arxiv.org/abs/2506.00400)
*Zixin Ding,Junyuan Hong,Jiachen T. Wang,Zinan Lin,Zhangyang Wang,Yuxin Chen*

Main category: cs.CL

TL;DR: An empirical study shows that increasing the number of training examples first improves but then degrades TGD's performance across NLP tasks. To address this, a new method called TSGD-M is proposed which reweights prompt sampling based on past batch distributions.


<details>
  <summary>Details</summary>
Motivation: Scaling the number of training examples initially improves but later degrades the performance of Textual Gradient Descent (TGD) in multiple downstream NLP tasks.

Method: Propose Textual Stochastic Gradient Descent with Momentum (TSGD-M), which facilitates scalable in-context learning by reweighting prompt sampling based on past batch distributions.

Result: TSGD-M significantly outperforms TGD baselines without reweighted sampling across nine NLP tasks spanning three domains, while also reducing variance in most tasks.

Conclusion: TSGD-M is an effective method for optimizing textual prompts and can improve performance while reducing computational cost.

Abstract: As prompts play an increasingly critical role in large language models
(LLMs), optimizing textual prompts has become a crucial challenge. The Textual
Gradient Descent (TGD) framework has emerged as a promising data-driven
approach that iteratively refines textual prompts using LLM - suggested updates
(or textual gradients) over minibatches of training samples. In this paper, we
empirically demonstrate that scaling the number of training examples initially
improves but later degrades TGD's performance across multiple downstream NLP
tasks. However, while data scaling improves results for most tasks, it also
significantly increases the computational cost when leveraging LLMs. To address
this, we draw inspiration from numerical gradient descent and propose Textual
Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates
scalable in-context learning by reweighting prompt sampling based on past batch
distributions. Across nine NLP tasks spanning three domains - including
BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks
- TSGD-M significantly outperforms TGD baselines that do not incorporate
reweighted sampling, while also reducing variance in most tasks.

</details>


### [505] [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413)
*Daniel Israel,Guy Van den Broeck,Aditya Grover*

Main category: cs.CL

TL;DR: The paper proposes adaptive parallel decoding (APD) to enhance the generation speed of diffusion large language models (dLLMs) by dynamically adjusting the number of tokens sampled in parallel, achieving higher throughput with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of autoregressive decoding's sequential nature which bottlenecks LLMs' generation speed and improve dLLMs' parallel token generation without significant quality sacrifice.

Method: Introduce APD that dynamically adjusts the number of parallel sampled tokens through a multiplicative mixture of dLLM marginal probabilities and joint sequence probabilities under an auxiliary autoregressive model. Further optimize APD with KV caching and limiting masked input size.

Result: APD provides significantly higher throughput with only minimal quality degradations on downstream benchmarks.

Conclusion: APD is a successful method for improving dLLMs' generation speed with three tunable parameters for flexible tradeoff between throughput and quality.

Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding,
where tokens are predicted sequentially one by one. Alternatively, diffusion
large language models (dLLMs) theoretically allow for parallel token
generation, but in practice struggle to achieve the speed of autoregressive
models without significantly sacrificing quality. We therefore introduce
adaptive parallel decoding (APD), a novel method that dynamically adjusts the
number of tokens sampled in parallel. We achieve this by defining a
multiplicative mixture between the dLLM marginal probabilities and the joint
probability of sequences under a small auxiliary autoregressive model. This
inverts the standard setup of speculative decoding, where the goal is to sample
from a large autoregressive verifier by drafting from a smaller model. We
further optimize APD by enabling KV caching and limiting the size of the masked
input. Altogether, our method puts forward three tunable parameters to flexibly
tradeoff throughput and quality. We show that APD provides markedly higher
throughput with minimal quality degradations on downstream benchmarks.

</details>


### [506] [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)
*Siqi Liang,Sumyeong Ahn,Paramveer S. Dhillon,Jiayu Zhou*

Main category: cs.CL

TL;DR: In context learning (ICL) needs good demonstrations, but when there are too many noisy ones, current methods don't work well. This paper finds two problems with the way we measure perplexity and proposes a new method to fix it, which works better at finding clean samples and performs as well as if all samples were clean, even with lots of noise.


<details>
  <summary>Details</summary>
Motivation: The motivation is that current methods for detecting noisy annotations in ICL break down when there's a high level of noise in the data, leading to inaccurate perplexity measurements.

Method: The method involves introducing a dual debiasing framework that corrects perplexity estimates using synthesized neighbors, resulting in a robust Sample Cleanliness Score.

Result: This method shows superior noise detection capabilities, performs comparably to fully clean datasets, and remains robust even with extremely high noise ratios.

Conclusion: The conclusion is that this new approach improves the ability to detect and handle noisy annotations in ICL, enhancing overall performance.

Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn
from large annotated corpora. Existing approaches detect noisy annotations by
ranking local perplexities, presuming that noisy samples yield higher
perplexities than their clean counterparts. However, this assumption breaks
down when the noise ratio is high and many demonstrations are flawed. We
reexamine the perplexity based paradigm for text generation under noisy
annotations, highlighting two sources of bias in perplexity: the annotation
itself and the domain specific knowledge inherent in large language models
(LLMs). To overcome these biases, we introduce a dual debiasing framework that
uses synthesized neighbors to explicitly correct perplexity estimates, yielding
a robust Sample Cleanliness Score. This metric uncovers absolute sample
cleanliness regardless of the overall corpus noise level. Extensive experiments
demonstrate our method's superior noise detection capabilities and show that
its final ICL performance is comparable to that of a fully clean demonstration
corpus. Moreover, our approach remains robust even when noise ratios are
extremely high.

</details>


### [507] [Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions](https://arxiv.org/abs/2506.00421)
*Jihyoung Jang,Minwook Bae,Minji Kim,Dilek Hakkani-Tur,Hyounghun Kim*

Main category: cs.CL

TL;DR: 研究人员发现，当前的多模态聊天机器人主要集中在以图像为中心的任务上，而忽视了听觉方面，并且互动通常是静态的，无法自然地将模态融入对话中。为了解决这些问题，他们引入了一个新的多模态对话数据集$M^3C$和一个具有多模态记忆检索功能的新模型。该模型在处理视觉和听觉输入、理解并做出适当回应以及在复杂的真实环境中与多个说话者进行长期对话方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态聊天机器人主要关注视觉信息，而忽略了听觉信息。此外，现有的研究通常集中在静态交互上，无法自然地将模态融入对话中。这限制了聊天机器人在真实世界中的应用。因此，需要一种能够同时处理视觉和听觉信息并将其自然地融入对话中的方法。

Method: 研究人员提出了一个新的多模态对话数据集$M^3C$（Multimodal Multi-Session Multi-Party Conversation），并开发了一个具有多模态记忆检索功能的新模型。该模型经过训练后，可以无缝地与多个说话者进行长期对话，并能有效地处理视觉和听觉输入。

Result: 实验结果表明，该模型在处理复杂的多模态对话任务时表现优异。人类评估也显示，该模型在保持连贯和动态的互动方面具有很强的能力。

Conclusion: 通过引入$M^3C$数据集和具有多模态记忆检索功能的新模型，本研究成功地提升了聊天机器人在真实世界中的交互能力。这为未来开发更先进的多模态对话系统奠定了基础。

Abstract: As chatbots continue to evolve toward human-like, real-world, interactions,
multimodality remains an active area of research and exploration. So far,
efforts to integrate multimodality into chatbots have primarily focused on
image-centric tasks, such as visual dialogue and image-based instructions,
placing emphasis on the "eyes" of human perception while neglecting the "ears",
namely auditory aspects. Moreover, these studies often center around static
interactions that focus on discussing the modality rather than naturally
incorporating it into the conversation, which limits the richness of
simultaneous, dynamic engagement. Furthermore, while multimodality has been
explored in multi-party and multi-session conversations, task-specific
constraints have hindered its seamless integration into dynamic, natural
conversations. To address these challenges, this study aims to equip chatbots
with "eyes and ears" capable of more immersive interactions with humans. As
part of this effort, we introduce a new multimodal conversation dataset,
Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel
multimodal conversation model featuring multimodal memory retrieval. Our model,
trained on the $M^3C$, demonstrates the ability to seamlessly engage in
long-term conversations with multiple speakers in complex, real-world-like
settings, effectively processing visual and auditory inputs to understand and
respond appropriately. Human evaluations highlight the model's strong
performance in maintaining coherent and dynamic interactions, demonstrating its
potential for advanced multimodal conversational agents.

</details>


### [508] [PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings](https://arxiv.org/abs/2506.00481)
*Junseo Kim,Jongwook Han,Dongmin Choi,Jongwook Yoon,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: The paper introduces the Personalized Visual Persuasion (PVP) dataset, which includes persuasive images and evaluators' personal information. They develop a generator and evaluator using this dataset, finding psychological characteristics improve image persuasiveness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets connecting image persuasiveness with personal information of evaluators in the field of personalized visual persuasion.

Method: Releasing the PVP dataset containing 28,454 persuasive images across 596 messages and 9 strategies, with persuasiveness scores from 2,521 annotators and their demographic/psychological traits. Developing a persuasive image generator and automated evaluator to demonstrate utility.

Result: Incorporating psychological characteristics improves generation and evaluation of persuasive images, providing insights for personalized visual persuasion.

Conclusion: The PVP dataset facilitates advancements in personalized visual persuasion technology by offering comprehensive data linking image persuasiveness to evaluator's personal information.

Abstract: Visual persuasion, which uses visual elements to influence cognition and
behaviors, is crucial in fields such as advertising and political
communication. With recent advancements in artificial intelligence, there is
growing potential to develop persuasive systems that automatically generate
persuasive images tailored to individuals. However, a significant bottleneck in
this area is the lack of comprehensive datasets that connect the persuasiveness
of images with the personal information about those who evaluated the images.
To address this gap and facilitate technological advancements in personalized
visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,
comprising 28,454 persuasive images across 596 messages and 9 persuasion
strategies. Importantly, the PVP dataset provides persuasiveness scores of
images evaluated by 2,521 human annotators, along with their demographic and
psychological characteristics (personality traits and values). We demonstrate
the utility of our dataset by developing a persuasive image generator and an
automated evaluator, and establish benchmark baselines. Our experiments reveal
that incorporating psychological characteristics enhances the generation and
evaluation of persuasive images, providing valuable insights for personalized
visual persuasion.

</details>


### [509] [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/abs/2506.00519)
*Yuxi Sun,Aoqi Zuo,Wei Gao,Jing Ma*

Main category: cs.CL

TL;DR: The paper proposes CausalAbstain, a method for Large Language Models (LLMs) to make better abstention decisions in multilingual settings by selecting helpful feedback among multiple generated ones. It improves interpretability and outperforms baselines on QA tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs often face knowledge disparities across languages and current abstention strategies may be affected by inaccuracies and biases in the generated feedback.

Method: CausalAbstain helps LLMs determine whether to use multiple generated feedback responses and how to identify the most useful ones from a causal perspective.

Result: CausalAbstain effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language and multilingual settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks.

Conclusion: CausalAbstain is an effective method for improving abstention decisions of LLMs in multilingual settings.

Abstract: Large Language Models (LLMs) often exhibit knowledge disparities across
languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps
is a promising strategy to reduce hallucinations in multilingual settings.
Current abstention strategies for multilingual scenarios primarily rely on
generating feedback in various languages using LLMs and performing
self-reflection. However, these methods can be adversely impacted by
inaccuracies and biases in the generated feedback. To address this, from a
causal perspective, we introduce \textit{CausalAbstain}, a method that helps
LLMs determine whether to utilize multiple generated feedback responses and how
to identify the most useful ones. Extensive experiments demonstrate that
\textit{CausalAbstain} effectively selects helpful feedback and enhances
abstention decisions with interpretability in both native language
(\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings,
outperforming strong baselines on two benchmark datasets covering encyclopedic
and commonsense knowledge QA tasks. Our code and data are open-sourced at
https://github.com/peachch/CausalAbstain.

</details>


### [510] [Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning](https://arxiv.org/abs/2506.00527)
*Runtao Ren,Jian Ma,Jianxi Luo*

Main category: cs.CL

TL;DR: 为了解决IP领域RAG系统处理多样用户查询的难题，本文提出了MQG-RFM框架，通过结合大型语言模型和检索微调技术提升检索与生成质量。实验表明，该方法在专利咨询和新技术报告数据集上显著提高了检索准确率和生成质量，并已被实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统难以应对包括口语表达、拼写错误和术语模糊等多样化的用户查询，导致检索不准确和响应效果不佳。

Method: 提出名为MQG-RFM的新框架，利用大型语言模型模拟多样化用户询问并微调检索模型以对齐语义相同但语言形式不同的问题；采用轻量级的数据调优范式，结合提示工程生成查询和困难负样本挖掘，从而增强检索鲁棒性而无需昂贵的基础架构更改。

Result: 实验结果表明，在台湾专利Q&A数据集上，检索准确率分别提升了185.62%和262.26%，生成质量分别提升了14.22%和53.58%。

Conclusion: MQG-RFM提供了一个实用且可扩展的方法，用于快速且成本效益高的部署，特别适合中小型机构寻求可靠的专利智能解决方案，并已在中国最大的专业研究社交网络平台ScholarMate上得到应用。

Abstract: Retrieval-Augmented Generation (RAG) systems in the Intellectual Property
(IP) field often struggle with diverse user queries, including colloquial
expressions, spelling errors, and ambiguous terminology, leading to inaccurate
retrieval and suboptimal responses. To address this challenge, we propose
Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a
novel framework that leverages large language models (LLMs) to simulate varied
user inquiries and fine-tunes retrieval models to align semantically equivalent
but linguistically diverse questions. Unlike complex architectural
modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining
prompt-engineered query generation with hard negative mining to enhance
retrieval robustness without costly infrastructure changes. Experimental
results on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval
accuracy on the Patent Consultation dataset and 262.26% improvement on the
Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in
generation quality over the baselines, respectively. By bridging the gap
between user intent and system comprehension through semantic-aware retrieval
optimization, MQG-RFM offers a practical, scalable approach for rapid,
cost-effective deployment among small and medium-sized agencies seeking
reliable patent intelligence solutions. Additionally, our proposed method has
already been adopted by ScholarMate, the largest professional research social
networking platform in China, to support real-world development and deployment.
A demo version of the instantiated is available at
https://github.com/renruntao/patent_rag.

</details>


### [511] [Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://arxiv.org/abs/2506.00536)
*Changyue Wang,Weihang Su,Qingyao Ai,Yujia Zhou,Yiqun Liu*

Main category: cs.CL

TL;DR: DecKER is a novel in-context editing (ICE) framework for Large Language Models that decouples reasoning from knowledge editing, showing significant improvements on multi-hop QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing ICE approaches fail to separate newly injected knowledge from the model's original reasoning process, leading to conflicts between external updates and internal parametric knowledge.

Method: Propose DecKER, which generates a masked reasoning path and resolves knowledge edits via hybrid retrieval and model-based validation, thereby decoupling reasoning from knowledge editing.

Result: Experiments on multi-hop QA benchmarks demonstrate that DecKER significantly outperforms existing ICE methods in mitigating knowledge conflicts and preserving reasoning consistency.

Conclusion: DecKER provides an effective solution for knowledge editing in Large Language Models by addressing the issue of entangled reasoning and knowledge.

Abstract: Knowledge editing aims to efficiently update Large Language Models (LLMs) by
modifying specific knowledge without retraining the entire model. Among
knowledge editing approaches, in-context editing (ICE) offers a lightweight
solution by injecting new knowledge directly into the input context, leaving
model parameters unchanged. However, existing ICE approaches do not explicitly
separate the newly injected knowledge from the model's original reasoning
process. This entanglement often results in conflicts between external updates
and internal parametric knowledge, undermining the consistency and accuracy of
the reasoning path.In this work, we conduct preliminary experiments to examine
how parametric knowledge influences reasoning path planning. We find that the
model's reasoning is tightly coupled with its internal knowledge, and that
naively injecting new information without adapting the reasoning path often
leads to performance degradation, particularly in multi-hop tasks. To this end,
we propose DecKER, a novel ICE framework that decouples reasoning from
knowledge editing by generating a masked reasoning path and then resolving
knowledge edits via hybrid retrieval and model-based validation. Experiments on
multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE
methods by mitigating knowledge conflicts and preserving reasoning consistency.
Our code is available at: https://github.com/bebr2/DecKER .

</details>


### [512] [Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages](https://arxiv.org/abs/2506.00549)
*Hyangsuk Min,Yuho Lee,Minjeong Ban,Jiaqi Deng,Nicole Hee-Yeon Kim,Taewon Yun,Hang Su,Jason Cai,Hwanjun Song*

Main category: cs.CL

TL;DR: The paper presents MSumBench, a new evaluation framework for text summarization in English and Chinese that incorporates domain-specific criteria and a multi-agent debate system to improve annotation quality. It evaluates eight models, finding varied performance patterns, and investigates large language models' abilities as summary evaluators.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for text summarization lack domain-specific assessment criteria, are mostly focused on English, and face challenges with human annotation due to complex reasoning requirements.

Method: Introduced MSumBench which provides multi-dimensional, multi-domain evaluations for summarization in both English and Chinese. Specialized assessment criteria for each domain were included along with a multi-agent debate system to enhance the quality of annotations.

Result: Evaluated eight modern summarization models revealing different performance patterns across domains and languages. Also examined large language models as summary evaluators discovering correlations between their evaluation and summarization capabilities and identifying bias in assessing self-generated summaries.

Conclusion: MSumBench offers an advanced approach to evaluate summarization models across multiple dimensions and domains in two languages while highlighting systematic biases in some models.

Abstract: Evaluation frameworks for text summarization have evolved in terms of both
domain coverage and metrics. However, existing benchmarks still lack
domain-specific assessment criteria, remain predominantly English-centric, and
face challenges with human annotation due to the complexity of reasoning. To
address these, we introduce MSumBench, which provides a multi-dimensional,
multi-domain evaluation of summarization in English and Chinese. It also
incorporates specialized assessment criteria for each domain and leverages a
multi-agent debate system to enhance annotation quality. By evaluating eight
modern summarization models, we discover distinct performance patterns across
domains and languages. We further examine large language models as summary
evaluators, analyzing the correlation between their evaluation and
summarization capabilities, and uncovering systematic bias in their assessment
of self-generated summaries. Our benchmark dataset is publicly available at
https://github.com/DISL-Lab/MSumBench.

</details>


### [513] [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
*Ming Wang,Peidong Wang,Lin Wu,Xiaocui Yang,Daling Wang,Shi Feng,Yuxin Chen,Bixuan Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: Researchers have developed AnnaAgent, a system that simulates mental health seekers more realistically by incorporating an emotion modulator, complaint elicitor, and tertiary memory mechanism. It addresses the challenges of dynamic evolution and multi-session memory in psychological counseling.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of dynamic evolution and multi-session memory in simulating real seekers' fluctuating mental states during multiple counseling sessions.

Method: AnnaAgent is equipped with an emotion modulator and a complaint elicitor trained on real counseling dialogues for dynamic control of configurations. It also has a tertiary memory mechanism integrating short-term and long-term memory across sessions.

Result: Evaluation results indicate that AnnaAgent provides more realistic seeker simulation in psychological counseling compared to existing methods.

Conclusion: AnnaAgent successfully addresses the key challenges in seeker simulation and advances AI-driven mental health solutions.

Abstract: Constrained by the cost and ethical concerns of involving real seekers in
AI-driven mental health, researchers develop LLM-based conversational agents
(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,
to simulate seekers. While these efforts advance AI in mental health, achieving
more realistic seeker simulation remains hindered by two key challenges:
dynamic evolution and multi-session memory. Seekers' mental states often
fluctuate during counseling, which typically spans multiple sessions. To
address this, we propose AnnaAgent, an emotional and cognitive dynamic agent
system equipped with tertiary memory. AnnaAgent incorporates an emotion
modulator and a complaint elicitor trained on real counseling dialogues,
enabling dynamic control of the simulator's configurations. Additionally, its
tertiary memory mechanism effectively integrates short-term and long-term
memory across sessions. Evaluation results, both automated and manual,
demonstrate that AnnaAgent achieves more realistic seeker simulation in
psychological counseling compared to existing baselines. The ethically reviewed
and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.

</details>


### [514] [Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples](https://arxiv.org/abs/2506.00622)
*Haesung Pyun,Yoonah Park,Yohan Jo*

Main category: cs.CL

TL;DR: In dialogue state tracking (DST), in-context learning includes a retriever and a DST model. Existing training data construction methods for retrievers have limitations, so the paper proposes CombiSearch to score effective in-context examples based on their combinatorial impact on DST performance. Experiments show that retrievers trained with CombiSearch surpass state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for constructing training data for retrievers in dialogue state tracking suffer from three key limitations: not considering the synergistic effect of examples, not factoring in the linguistic characteristics of the query sufficiently, and not directly optimizing scoring for DST performance.

Method: The paper presents CombiSearch, which scores effective in-context examples based on their combinatorial impact on DST performance.

Result: Evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Additionally, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed.

Conclusion: CombiSearch significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training.

Abstract: In dialogue state tracking (DST), in-context learning comprises a retriever
that selects labeled dialogues as in-context examples and a DST model that uses
these examples to infer the dialogue state of the query dialogue. Existing
methods for constructing training data for retrievers suffer from three key
limitations: (1) the synergistic effect of examples is not considered, (2) the
linguistic characteristics of the query are not sufficiently factored in, and
(3) scoring is not directly optimized for DST performance. Consequently, the
retriever can fail to retrieve examples that would substantially improve DST
performance. To address these issues, we present CombiSearch, a method that
scores effective in-context examples based on their combinatorial impact on DST
performance. Our evaluation on MultiWOZ shows that retrievers trained with
CombiSearch surpass state-of-the-art models, achieving a 20x gain in data
efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch
attains a 12% absolute improvement in the upper bound DST performance over
traditional approaches when no retrieval errors are assumed. This significantly
increases the headroom for practical DST performance while demonstrating that
existing methods rely on suboptimal data for retriever training.

</details>


### [515] [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
*Lorenzo Jaime Yu Flores,Ori Ernst,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 提出了一种新的、与任务无关的置信度度量方法，改进了BART和Flan-T5在文本生成任务中的校准效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本生成模型的置信度度量不够准确，尤其是在存在多个有效答案的情况下，这限制了模型的实际应用价值。

Method: 设计了仅依赖模型输出概率的任务无关置信度度量，无需额外微调或启发式规则。

Result: 成功提升了BART和Flan-T5在摘要、翻译和问答任务上的置信度校准性能。

Conclusion: 新提出的置信度度量方法可以更好地适应文本生成任务的特点，提高模型实用性。

Abstract: Well-calibrated model confidence scores can improve the usefulness of text
generation models. For example, users can be prompted to review predictions
with low confidence scores, to prevent models from returning bad or potentially
dangerous predictions. However, confidence metrics are not always well
calibrated in text generation. One reason is that in generation, there can be
many valid answers, which previous methods do not always account for. Hence, a
confident model could distribute its output probability among multiple
sequences because they are all valid. We propose task-agnostic confidence
metrics suited to generation, which rely solely on the probabilities associated
with the model outputs without the need for further fine-tuning or heuristics.
Using these, we are able to improve the calibration of BART and Flan-T5 on
summarization, translation, and QA datasets.

</details>


### [516] [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
*Weijie Xu,Shixian Cui,Xi Fang,Chi Xue,Stephanie Eckman,Chandan Reddy*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在多选题上的表现被评估，但许多现实问题需要从选项集中识别所有正确答案。本文介绍SATA-BENCH，首个针对跨领域Select All That Apply (SATA)问题的LLM基准测试。评估27个开源和专有模型发现，即使最强模型也仅达到41.8%的完全匹配率。研究指出两个核心问题：选择偏差和计数偏差，并提出Choice Funnel解码策略以改善这些问题，该策略使完全匹配率提高29%，同时将推理成本降低64%以上。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs的评估主要集中于单答案多项选择任务，然而许多实际问题需要确定所有正确的答案。这种能力尚未得到充分研究。

Method: 开发了SATA-BENCH基准测试，用于评估LLMs在不同领域的SATA问题上的表现。通过分析发现模型存在选择偏差和计数偏差的问题，并提出了Choice Funnel解码策略，结合标记去偏差和自适应阈值来指导模型进行完整和准确的选择。

Result: Choice Funnel相比竞争基线提高了29%的完全匹配率，同时降低了超过64%的推理成本。揭示了当前LLMs在多答案推理中的基本局限性。

Conclusion: 发现了LLMs在多答案推理中的显著缺陷，并引入了一个新的框架来诊断和改进这一能力。发布了SATA-BENCH和Choice Funnel以促进LLMs在真实多答案应用中稳健决策的发展。

Abstract: Large language models (LLMs) are increasingly evaluated on single-answer
multiple-choice tasks, yet many real-world problems require identifying all
correct answers from a set of options. This capability remains underexplored.
We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on
Select All That Apply (SATA) questions across diverse domains, including
reading comprehension, law, and biomedicine. Our evaluation of 27 open-source
and proprietary models reveals a significant gap: even the strongest model
achieves only 41.8% exact match, exposing LLMs' inability to reliably identify
all correct answers. We find that this weakness stems from two core challenges:
selection bias - models favor certain choices regardless of content, and count
bias - models fail to predict the correct number of answers. To address these
issues, we propose Choice Funnel, a decoding strategy that combines token
debiasing with adaptive thresholding to guide models toward complete and
accurate selections. Choice Funnel achieves up to 29% higher exact match than
competitive baselines while reducing inference cost by over 64%. Our findings
expose fundamental limitations in current LLMs and introduce a new framework
for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and
Choice Funnel to promote LLM development for robust decision-making in
realistic, multi-answer applications.

</details>


### [517] [Clinical Annotations for Automatic Stuttering Severity Assessment](https://arxiv.org/abs/2506.00644)
*Ana Rita Valente,Rufael Marew,Hawau Olamide Toyin,Hamdan Al-Ali,Anelise Bohnen,Inma Becerra,Elsa Marta Soares,Goncalo Leal,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: This paper aims to enhance the FluencyBank dataset with a new stuttering annotation scheme based on clinical standards, incorporating multi-modal features and expert consensus for training and evaluating stuttering assessment models.


<details>
  <summary>Details</summary>
Motivation: Stuttering is a complex disorder requiring specialized expertise for effective assessment and treatment. There is a need for a high-quality annotated dataset that mirrors real-world clinical expertise for training and evaluating stuttering assessment models.

Method: The authors hired expert clinicians to label the data according to established clinical standards, creating a multi-modal annotation scheme incorporating audiovisual features for detecting and classifying stuttering moments, secondary behaviors, and tension scores. They also provided a test set with highly reliable annotations based on expert consensus.

Result: The experiments and analysis demonstrated the complexity of the task, emphasizing the necessity of extensive clinical expertise for valid training and evaluation of stuttering assessment models.

Conclusion: High-quality annotations based on clinical expertise are crucial for developing effective stuttering assessment models.

Abstract: Stuttering is a complex disorder that requires specialized expertise for
effective assessment and treatment. This paper presents an effort to enhance
the FluencyBank dataset with a new stuttering annotation scheme based on
established clinical standards. To achieve high-quality annotations, we hired
expert clinicians to label the data, ensuring that the resulting annotations
mirror real-world clinical expertise. The annotations are multi-modal,
incorporating audiovisual features for the detection and classification of
stuttering moments, secondary behaviors, and tension scores. In addition to
individual annotations, we additionally provide a test set with highly reliable
annotations based on expert consensus for assessing individual annotators and
machine learning models. Our experiments and analysis illustrate the complexity
of this task that necessitates extensive clinical expertise for valid training
and evaluation of stuttering assessment models.

</details>


### [518] [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
*Lang Xiong,Raina Gao,Alyssa Jeong,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: The paper introduces Sarc7, a benchmark for classifying 7 types of sarcasm. It proposes an emotion-based method for sarcasm generation and classification that outperforms other methods.


<details>
  <summary>Details</summary>
Motivation: Sarcasm is challenging for computational models due to its nuanced nature, yet it's important for interpreting human communication.

Method: The authors developed Sarc7, which classifies 7 types of sarcasm. They evaluated classification using zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based prompting technique. For generation, they proposed an emotion-based method identifying incongruity, shock value, and context dependency.

Result: Gemini 2.5 with emotion-based prompting achieved the highest F1 score of 0.3664 in classification experiments. Human evaluators found the emotion-based method produced 38.46% more successful generations compared to zero-shot prompting.

Conclusion: The emotion-based prompting technique improves both sarcasm classification and generation over existing methods.

Abstract: Sarcasm is a form of humor where expressions convey meanings opposite to
their literal interpretations. Classifying and generating sarcasm using large
language models is vital for interpreting human communication. Sarcasm poses
challenges for computational models, due to its nuanced nature. We introduce
Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,
brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries
of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,
chain-of-thought (CoT), and a novel emotion-based prompting technique. We
propose an emotion-based generation method developed by identifying key
components of sarcasm-incongruity, shock value, and context dependency. Our
classification experiments show that Gemini 2.5, using emotion-based prompting,
outperforms other setups with an F1 score of 0.3664. Human evaluators preferred
our emotion-based prompting, with 38.46% more successful generations than
zero-shot prompting.

</details>


### [519] [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/abs/2506.00694)
*Li Zhang,Morgan Gray,Jaromir Savelka,Kevin D. Ashley*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) have potential in complex legal tasks, but their reliability is still a concern. This paper introduces an automated pipeline to evaluate LLM performance on generating legal arguments focusing on faithfulness, factor utilization, and appropriate abstention.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reliability in Large Language Models when performing complex legal tasks such as argument generation, this study aims to create an automated evaluation pipeline that assesses key aspects like faithfulness, factor utilization, and appropriate abstention.

Method: An external LLM extracts factors from generated arguments and compares them with ground-truth factors from input case triples. The method evaluates LLMs on three tests: generating a standard 3-ply argument, generating an argument with swapped precedent roles, and recognizing the impossibility of argument generation due to lack of shared factors and abstaining.

Result: Current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests but often fail to utilize all relevant factors. Most models also failed the abstention test by generating spurious arguments despite the lack of common factors.

Conclusion: This automated pipeline provides a scalable method for assessing crucial LLM behaviors in legal argument generation, emphasizing the need for improvements in factor utilization and robust abstention capabilities.

Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks
like argument generation, yet their reliability remains a concern. Building
upon pilot work assessing LLM generation of 3-ply legal arguments using human
evaluation, this paper introduces an automated pipeline to evaluate LLM
performance on this task, specifically focusing on faithfulness (absence of
hallucination), factor utilization, and appropriate abstention. We define
hallucination as the generation of factors not present in the input case
materials and abstention as the model's ability to refrain from generating
arguments when instructed and no factual basis exists. Our automated method
employs an external LLM to extract factors from generated arguments and
compares them against the ground-truth factors provided in the input case
triples (current case and two precedent cases). We evaluated eight distinct
LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply
argument, 2) generating an argument with swapped precedent roles, and 3)
recognizing the impossibility of argument generation due to lack of shared
factors and abstaining. Our findings indicate that while current LLMs achieve
high accuracy (over 90%) in avoiding hallucination on viable argument
generation tests (Tests 1 & 2), they often fail to utilize the full set of
relevant factors present in the cases. Critically, on the abstention test (Test
3), most models failed to follow instructions to stop, instead generating
spurious arguments despite the lack of common factors. This automated pipeline
provides a scalable method for assessing these crucial LLM behaviors,
highlighting the need for improvements in factor utilization and robust
abstention capabilities before reliable deployment in legal settings. Project
page:
https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.

</details>


### [520] [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
*Debarati Bhattacharjee,Ashish Anand*

Main category: cs.CL

TL;DR: This paper presents a framework for converting argumentative texts into argument knowledge graphs (AKGs) to make argument structures more understandable and to prepare for future reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To create a more understandable graphical view of argumentative structures and to enable the identification of indirect relations in arguments, many of which are implicit.

Method: The method involves annotating argumentative components and relations, constructing a knowledge base graph with metadata attributes, using premises and inference rules from the KB to form arguments via modus ponens, creating an AKG, and identifying missing inference rules by markers.

Result: The framework successfully creates AKGs that capture important argumentative features and identify previously undetectable undercut attacks. It also prepares for future reasoning tasks by helping models learn implicit indirect relations.

Conclusion: The proposed AKG format, with annotated inference rules and modus ponens, is expected to assist reasoning models in learning implicit indirect relations within arguments.

Abstract: This paper presents a framework to convert argumentative texts into argument
knowledge graphs (AKG). Starting with basic annotations of argumentative
components (ACs) and argumentative relations (ARs), we enrich the information
by constructing a knowledge base (KB) graph with metadata attributes for nodes.
Next, we use premises and inference rules from the KB to form arguments by
applying modus ponens. From these arguments, we create an AKG. The nodes and
edges of the AKG have attributes that capture important argumentative features.
We also find missing inference rules by identifying markers. This makes it
possible to identify undercut attacks that were previously undetectable in
existing datasets. The AKG gives a graphical view of the argumentative
structure that is easier to understand than theoretical formats. It also
prepares the ground for future reasoning tasks, including checking the
coherence of arguments and identifying opportunities for revision. For this, it
is important to find indirect relations, many of which are implicit. Our
proposed AKG format, with annotated inference rules and modus ponens, will help
reasoning models learn the implicit indirect relations that require inference
over arguments and the relations between them.

</details>


### [521] [Length Aware Speech Translation for Video Dubbing](https://arxiv.org/abs/2506.00740)
*Harveen Singh Chadha,Aswin Shanmugam Subramanian,Vikas Joshi,Shubham Bansal,Jian Xue,Rupeshkumar Mehta,Jinyu Li*

Main category: cs.CL

TL;DR: The paper presents a phoneme-based end-to-end length-sensitive speech translation (LSST) model with length-aware beam search (LABS) for video dubbing, improving synchronization quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of aligning translated audio with source audio in real-time, on-device video dubbing.

Method: Developed a phoneme-based end-to-end length-sensitive speech translation (LSST) model and introduced length-aware beam search (LABS).

Result: Maintained comparable BLEU scores to baseline while enhancing synchronization quality with MOS gains of 0.34 for Spanish and 0.65 for Korean.

Conclusion: The LSST model with LABS improves synchronization in video dubbing without compromising translation quality.

Abstract: In video dubbing, aligning translated audio with the source audio is a
significant challenge. Our focus is on achieving this efficiently, tailored for
real-time, on-device video dubbing scenarios. We developed a phoneme-based
end-to-end length-sensitive speech translation (LSST) model, which generates
translations of varying lengths short, normal, and long using predefined tags.
Additionally, we introduced length-aware beam search (LABS), an efficient
approach to generate translations of different lengths in a single decoding
pass. This approach maintained comparable BLEU scores compared to a baseline
without length awareness while significantly enhancing synchronization quality
between source and target audio, achieving a mean opinion score (MOS) gain of
0.34 for Spanish and 0.65 for Korean, respectively.

</details>


### [522] [Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](https://arxiv.org/abs/2506.00743)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.CL

TL;DR: This paper proposes an efficient PEFT method for MHA-based language models in Federated Learning, using head pruning, weighted aggregation, and client selection to reduce complexity while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying PEFT in privacy-preserving distributed learning frameworks like Federated Learning, especially with resource-constrained devices and diverse data distributions.

Method: The method involves head pruning guided by importance scores based on attention head confidence, a novel head-specific weighted aggregation mechanism, and a client selection strategy.

Result: Achieved sparsity levels up to 90%, communication advantage up to 1.8x, reduction in training OPs of 3.9x, with accuracy drop under 2% on MultiNLI, 20 Newsgroups, XL-Sum, and E2E NLG datasets.

Conclusion: The proposed PEFT method within the FL framework effectively reduces training complexity and communication costs while preserving model performance.

Abstract: Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in
adapting Large Language Models (LLMs) for downstream tasks in Natural Language
Processing. However, its adoption in privacy-preserving distributed learning
frameworks, such as Federated Learning (FL), remains relatively limited. This
is mainly due to challenges specific to FL, such as resource-constrained
devices and diverse data distributions among clients. In this paper, we propose
an efficient method to perform PEFT within the FL framework for Multi-Head
Attention (MHA) based language models. We address the challenges through head
pruning, a novel head-specific weighted aggregation mechanism, and a client
selection strategy. Head pruning minimizes training complexity within the
clients, guided by the importance score computed based on the confidence of the
attention head. Weighted aggregation of heads ensures the global model captures
crucial updates from diverse clients complementing our client selection
strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,
XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model
with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting
in a communication advantage of up to 1.8x and a reduction in training OPs of
3.9x while maintaining the accuracy drop under 2%.

</details>


### [523] [KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision](https://arxiv.org/abs/2506.00783)
*Rong Wu,Pinlong Cai,Jianbiao Mei,Licheng Wen,Tao Hu,Xuemeng Yang,Daocheng Fu,Botian Shi*

Main category: cs.CL

TL;DR: 大型语言模型在复杂推理任务中存在可解释性和可信度不足的问题，本文提出KG-TRACES框架来增强其推理能力。通过监督符号关系路径预测、三元组级别推理路径预测以及基于推理路径的归因意识推理过程生成，该框架使模型能够在推理时适应知识图谱可用或不可用的情况，并以可解释和有源可追溯的方式进行推理。实验表明，该框架显著优于现有方法，并且在医学等领域具有良好的迁移性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理任务上取得了显著进展，但在复杂推理问题上的表现受到缺乏可解释性和可信度的限制，这限制了它们在实际复杂推理场景中的应用。

Method: 提出了一个名为KG-TRACES的新框架，该框架通过明确监督推理路径和过程来增强大型语言模型的推理能力。具体来说，KG-TRACES联合监督模型进行以下操作：(1) 预测符号关系路径；(2) 预测完整的三元组级别的推理路径；(3) 生成基于推理路径的归因意识推理过程。在推理阶段，模型能够适应知识图谱可用和不可用的情况，当可能时从知识图谱检索推理路径，或者仅使用内在知识预测合理的推理路径。

Result: 通过广泛的实验验证了KG-TRACES在复杂推理任务上的优越性能。在WebQSP数据集上，相比现有最先进方法，提高了Hits@1 1.6%和F1 4.7%；在CWQ数据集上，分别提高了Hits@1 4.8%和F1 2.1%。此外，还展示了该框架在医学等专业领域的迁移能力，并通过可视化推理中间步骤证明了显式监督导致更稳定和目标导向的推理过程，与正确答案高度一致。

Conclusion: KG-TRACES框架成功增强了大型语言模型在复杂推理任务中的表现，同时提供了更高的可解释性和可信度，为解决复杂推理问题提供了一种有效的方法。

Abstract: Large language models (LLMs) have made remarkable strides in various natural
language processing tasks, but their performance on complex reasoning problems
remains hindered by a lack of explainability and trustworthiness. This issue,
often manifesting as hallucinations or unattributable reasoning processes,
limits their applicability in complex reasoning scenarios. To address this, we
propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain
Explanation Supervision (KG-TRACES), a novel framework that enhances the
reasoning ability of LLMs through explicit supervision over reasoning paths and
processes. KG-TRACES jointly supervises the model to: (1) predict symbolic
relation paths, (2) predict full triple-level reasoning paths, and (3) generate
attribution-aware reasoning processes grounded in the reasoning paths. At
inference phase, the model adapts to both KG-available and KG-unavailable
scenarios, retrieving reasoning paths from a KG when possible or predicting
plausible reasoning paths with only intrinsic knowledge when not. This design
enables the model to reason in an explainable and source-attributable pattern.
Through extensive experiments on complex reasoning tasks, we demonstrate that
KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%
and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%
in F1 on CWQ. Moreover, we show its transferability to specialized domains such
as medicine. By visualizing the intermediate steps of reasoning processes, we
further show that the explicit supervision introduced by KG-TRACES leads to
more stable and goal-directed reasoning processes, aligning closely with
correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.

</details>


### [524] [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)
*Shenghe Zheng,Qianjia Cheng,Junchi Yao,Mengsong Wu,haonan he,Ning Ding,Yu Cheng,Shuyue Hu,Lei Bai,Dongzhan Zhou,Ganqu Cui,Peng Ye*

Main category: cs.CL

TL;DR: 本研究创建了一个名为PHYSICS的数据集，包含16,568个高质量物理问题，涵盖多个主题和难度级别，以促进大型语言模型在物理推理方面的能力发展。同时提出了一种Rule+Model评估框架来解决现有框架的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在数学和编程竞赛等高级推理任务上取得了显著进展，但物理学作为既需要推理又对现实世界理解至关重要的学科，却未受到足够的学术和工业关注。

Method: 构建了包含16,568个高质量物理问题的数据集PHYSICS，这些问题来自超过100本教科书，并涵盖了力学、电磁学、热力学、光学和现代物理五个主要领域。数据集分为训练集和测试集，并提供了由强大推理模型生成的训练数据推理路径。此外，还提出了一个针对物理问题的Rule+Model评估框架。

Result: 当前最先进的开源和专有模型在处理与物理相关的任务时表现出局限性。通过新的数据集和评估方法，可以更有效地推进LLMs在物理领域的应用和发展。

Conclusion: 希望所提出的数据集和评估方法能够共同推动LLMs在物理领域的发展，克服现有模型在处理物理相关任务中的限制。

Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced
reasoning tasks such as mathematics and coding competitions. Meanwhile,
physics, despite being both reasoning-intensive and essential to real-world
understanding, received limited academic and industrial attention. This paper
introduces PHYSICS, a dataset containing 16,568 high-quality physics problems
spanning subjects and difficulty levels, to facilitate this issue.
Specifically, PHYSICS is curated with exercises from over 100 textbooks through
a carefully designed pipeline for quality control. It covers five major physics
domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern
Physics. It also spans a wide range of difficulty levels, from high school to
graduate-level physics courses. To utilize the data for improving and
evaluating the model's physical reasoning capabilities, we split the dataset
into training and test sets, and provide reasoning paths generated by powerful
reasoning models for the training data to facilitate model training. In
addition, for the evaluation part, we find that existing evaluation frameworks
exhibit biases in aspects such as units, simplification, and precision in
physics domain. To balance efficiency and accuracy, we introduce a Rule+Model
evaluation framework tailored to physics problems. Our evaluations on current
state-of-the-art open-source and proprietary models highlight the limitations
of current models in handling physics-related tasks. We hope that our dataset
and evaluation methodology will jointly advance the development of LLMs in the
field of physics.

</details>


### [525] [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)
*Yongkang Xiao,Rui Zhang*

Main category: cs.CL

TL;DR: HERGC is a new framework for multimodal knowledge graph completion which uses heterogeneous experts representation and generative LLMs to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing MMKGC methods are limited by the closed-world assumption and discriminative training objectives. Generative approaches using LLMs have shown promise in unimodal KG completion, but their potential in MMKGC is underexplored.

Method: HERGC consists of a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves candidates for incomplete triples, and a Generative LLM Predictor fine-tuned on minimal instruction data to select the correct answer.

Result: HERGC achieves state-of-the-art performance and demonstrates effectiveness and robustness on three standard MMKG benchmarks.

Conclusion: HERGC bridges the gap between traditional MMKGC methods and generative LLM approaches, offering a promising direction for future research in this area.

Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)
by incorporating diverse modalities such as images and text. Multi-modal
knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals
to infer missing facts, thereby mitigating the intrinsic incompleteness of
MMKGs. Existing MMKGC methods typically leverage only the information contained
in the MMKGs under the closed-world assumption and adopt discriminative
training objectives, which limits their reasoning capacity during completion.
Recent generative completion approaches powered by advanced large language
models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph
completion, but their potential in MMKGC remains largely unexplored. To bridge
this gap, we propose HERGC, a Heterogeneous Experts Representation and
Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous
Experts Representation Retriever that enriches and fuses multimodal information
and retrieves a compact candidate set for each incomplete triple. It then uses
a Generative LLM Predictor fine-tuned on minimal instruction data to accurately
identify the correct answer from these candidates. Extensive experiments on
three standard MMKG benchmarks demonstrate HERGC's effectiveness and
robustness, achieving state-of-the-art performance.

</details>


### [526] [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/abs/2506.00829)
*Keyuan Cheng,Zijian Kan,Zhixian He,Zhuoran Zhang,Muhammad Asif Ali,Ke Xu,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: The paper introduces COMPKE, a new benchmark for evaluating knowledge editing methods in large language models through complex real-life questions. It assesses four methods, revealing significant variations in their effectiveness across different models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for assessing knowledge editing in large language models inadequately evaluate the application of updated knowledge in real-life scenarios requiring complex reasoning.

Method: Introduced COMPKE - Complex Question Answering under Knowledge Editing benchmark consisting of 11,924 complex questions reflecting real-life situations to extensively evaluate four knowledge editing methods.

Result: Evaluation shows notable variation in method effectiveness across different models (e.g., MeLLo accuracy drops from 39.47 on GPT-4O-MINI to 3.83 on QWEN2.5-3B). Investigation into disparities is conducted from methodological and model-specific perspectives.

Conclusion: COMPKE fills a gap in current benchmarks by better assessing how well updated models apply knowledge in real-life complex reasoning scenarios.

Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language
models, has gathered great attention. Current benchmarks primarily use
multi-hop question answering to assess and analyze newly injected or updated
knowledge. However, we argue that these benchmarks fail to effectively evaluate
how well the updated models apply this knowledge in real-life scenarios,
particularly when questions require complex reasoning, involving one-to-many
relationships or multi-step logical intersections. To fill in this gap, we
introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge
Editing, which includes 11,924 complex questions that reflect real-life
situations. We conduct an extensive evaluation of four knowledge editing
methods on COMPKE, revealing that their effectiveness varies notably across
different models. For instance, MeLLo attains an accuracy of 39.47 on
GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further
investigate the underlying causes of these disparities from both methodological
and model-specific perspectives. The datasets are available at
https://github.com/kzjkzj666/CompKE.

</details>


### [527] [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)
*Jiawei Gu,Ziting Xian,Yuanzhen Xie,Ye Liu,Enjie Liu,Ruichao Zhong,Mochi Gao,Yunzhi Tan,Bo Hu,Zang Li*

Main category: cs.CL

TL;DR: Large language models (LLMs) perform well on plain text tasks but not on structured data. The paper introduces Contrastive Retrieval-Augmented Generation on Experience (CoRE), which builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL). Experiments show CoRE improves performance on Text-to-SQL and TableQA tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs achieve strong performance on plain text tasks but underperform on structured data like tables and databases due to underexposure during pre-training and rigid text-to-structure transfer mechanisms.

Method: Introduced CoRE, a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Also used Monte Carlo Tree Search (MCTS)-generated Experience Memory to expand training data 8-9x.

Result: Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks.

Conclusion: This training-free and continual method propels LLMs toward structured knowledge expertise.

Abstract: Large language models (LLMs) achieve strong performance on plain text tasks
but underperform on structured data like tables and databases. Potential
challenges arise from their underexposure during pre-training and rigid
text-to-structure transfer mechanisms. Unlike humans who seamlessly apply
learned patterns across data modalities, LLMs struggle to infer implicit
relationships embedded in tabular formats, especially in the absence of
explicit structural guidance. To bridge this cognitive gap, we introduce
Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework
that builds experience memory representations and enhances generalization
through contrastive In-Context Learning (ICL) to simulate human-like knowledge
transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly
improves performance, achieving average gains of 3.44% and 4.24%, with up to
17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated
Experience Memory expands training data 8-9x, enhancing diversity and domain
coverage. This training-free and continual method propels LLMs toward
structured knowledge expertise.

</details>


### [528] [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/abs/2506.00077)
*Edward Wang,Tianyu Wang,Avanti Athreya,Vince Lyzinski,Carey E. Priebe*

Main category: cs.CL

TL;DR: The paper proposes interacting Gaussian mixture models (GMMs) as an alternative to large language models (LLMs) in studying human behavior due to LLMs' computational complexity. It compares GMMs with LLMs and finds that GMMs can capture key dynamics of LLMs, offering a simpler approach.


<details>
  <summary>Details</summary>
Motivation: To address the computational complexity and expense associated with using large language models (LLMs) for studying human behavior in social sciences, especially when large-scale experiments are impractical.

Method: Introducing and evaluating interacting Gaussian mixture models (GMMs) as an alternative framework to LLMs. The method involves comparing a simplified model of GMMs with experimental simulations of LLMs, focusing on their updating and response mechanisms based on feedback.

Result: Interacting GMMs effectively capture important features of the dynamics present in interacting LLMs, showing key similarities and differences between the two models.

Conclusion: Gaussian mixture models provide benefits as a simpler alternative to LLMs for modeling interactions. The paper discusses potential modifications to GMMs and suggests future research directions.

Abstract: Large language models (LLMs) are a powerful tool with the ability to match
human capabilities and behavior in many settings. Retrieval-augmented
generation (RAG) further allows LLMs to generate diverse output depending on
the contents of their RAG database. This motivates their use in the social
sciences to study human behavior between individuals when large-scale
experiments are infeasible. However, LLMs depend on complex, computationally
expensive algorithms. In this paper, we introduce interacting Gaussian mixture
models (GMMs) as an alternative to similar frameworks using LLMs. We compare a
simplified model of GMMs to select experimental simulations of LLMs whose
updating and response depend on feedback from other LLMs. We find that
interacting GMMs capture important features of the dynamics in interacting
LLMs, and we investigate key similarities and differences between interacting
LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture
models, potential modifications, and future research directions.

</details>


### [529] [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
*Jacky Tai-Yu Lu,Jung Chiang,Chi-Sheng Chen,Anna Nai-Yun Tung,Hsiang Wei Hu,Yuan Chiao Cheng*

Main category: cs.CL

TL;DR: 研究人员提出了EEG2TEXT-CN，这是一个早期的开放词汇 EEG-to-text 生成框架，专为中文设计。该架构通过掩码预训练和对比学习，将多通道脑信号与自然语言表示对齐。使用 ChineseEEG 数据集的一个子集，将 EEG 分段为每个字符的嵌入，并在零样本设置中预测完整句子。评估结果显示有希望的词汇对齐，最佳 BLEU-1 得分为6.38%。尽管语法流畅性仍然是一个挑战，但研究结果展示了从 EEG 进行非语音、跨模态语言解码的可行性，为多语言脑到文本的研究开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专为中文设计的开放词汇 EEG-to-text 生成框架，因此需要一种能够实现非语音、跨模态语言解码的方法，以推动多语言脑到文本研究的发展。

Method: EEG2TEXT-CN 架构基于生物学基础的 EEG 编码器（NICE-EEG）和紧凑的预训练语言模型（MiniLM）。通过掩码预训练和对比学习，将多通道脑信号与自然语言表示对齐。使用 ChineseEEG 数据集的一个子集，将 EEG 分段为每个字符的嵌入，并在零样本设置中预测完整句子。解码器通过教师强制和填充掩码进行训练，以适应可变长度序列。

Result: 在超过1,500个训练-验证句子和300个保留测试样本上的评估显示有希望的词汇对齐，最佳 BLEU-1 得分为6.38%。尽管语法流畅性仍然是一个挑战，但结果展示了从 EEG 进行非语音、跨模态语言解码的可行性。

Conclusion: EEG2TEXT-CN 的提出展示了从 EEG 进行非语音、跨模态语言解码的可行性，为多语言脑到文本的研究开辟了新方向，并为未来的中文认知-语言接口奠定了基础。

Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one
of the earliest open-vocabulary EEG-to-text generation frameworks tailored for
Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact
pretrained language model (MiniLM), our architecture aligns multichannel brain
signals with natural language representations via masked pretraining and
contrastive learning. Using a subset of the ChineseEEG dataset, where each
sentence contains approximately ten Chinese characters aligned with 128-channel
EEG recorded at 256 Hz, we segment EEG into per-character embeddings and
predict full sentences in a zero-shot setting. The decoder is trained with
teacher forcing and padding masks to accommodate variable-length sequences.
Evaluation on over 1,500 training-validation sentences and 300 held-out test
samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%.
While syntactic fluency remains a challenge, our findings demonstrate the
feasibility of non-phonetic, cross-modal language decoding from EEG. This work
opens a new direction in multilingual brain-to-text research and lays the
foundation for future cognitive-language interfaces in Chinese.

</details>


### [530] [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)
*Junying Wang,Wenzhe Li,Yalun Wu,Yingji Liang,Yijin Guo,Chunyi Li,Haodong Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper introduces A4Bench, a benchmark for assessing the affordance perception abilities of MLLMs. It evaluates 17 MLLMs and finds significant gaps in their environmental understanding compared to human performance.


<details>
  <summary>Details</summary>
Motivation: To explore and evaluate the affordance perception capabilities of Multimodal Large Language Models (MLLMs), which is crucial for intuitive and safe interactions but remains underexplored.

Method: Introduced A4Bench, a benchmark with two dimensions: Constitutive Affordance and Transformative Affordance, evaluated 17 MLLMs (nine proprietary and eight open-source) against human performance.

Result: Proprietary models generally outperform open-source counterparts, but all exhibit limited capabilities, particularly in transformative affordance perception. Top-performing models significantly lag behind human performance.

Conclusion: Critical gaps in environmental understanding of MLLMs were highlighted, providing a foundation for advancing AI systems toward more robust, context-aware interactions.

Abstract: Affordance theory posits that environments inherently offer action
possibilities that shape perception and behavior. While Multimodal Large
Language Models (MLLMs) excel in vision-language tasks, their ability to
perceive affordance, which is crucial for intuitive and safe interactions,
remains underexplored. To address this, we introduce A4Bench, a novel benchmark
designed to evaluate the affordance perception abilities of MLLMs across two
dimensions: 1) Constitutive Affordance}, assessing understanding of inherent
object properties through 1,282 question-answer pairs spanning nine
sub-disciplines, and 2) Transformative Affordance, probing dynamic and
contextual nuances (e.g., misleading, time-dependent, cultural, or
individual-specific affordance) with 718 challenging question-answer pairs.
Evaluating 17 MLLMs (nine proprietary and eight open-source) against human
performance, we find that proprietary models generally outperform open-source
counterparts, but all exhibit limited capabilities, particularly in
transformative affordance perception. Furthermore, even top-performing models,
such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag
behind human performance (best: 85.34%, worst: 81.25%). These findings
highlight critical gaps in environmental understanding of MLLMs and provide a
foundation for advancing AI systems toward more robust, context-aware
interactions. The dataset is available in
https://github.com/JunyingWang959/A4Bench/.

</details>


### [531] [LaMP-QA: A Benchmark for Personalized Long-form Question Answering](https://arxiv.org/abs/2506.00137)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: The paper introduces LaMP-QA, a benchmark for evaluating personalized long-form answer generation in question answering systems. It covers three major categories and over 45 subcategories. Comprehensive evaluations show incorporating personalized context leads to up to 39% performance improvement. The benchmark is publicly released.


<details>
  <summary>Details</summary>
Motivation: There is a lack of resources for training and evaluating personalized question answering systems, which are essential for user-centric applications.

Method: Introduced LaMP-QA benchmark covering questions from Arts & Entertainment, Lifestyle & Personal Development, and Society & Culture with over 45 subcategories. Conducted human and automatic evaluations comparing multiple strategies and benchmarking non-personalized and personalized approaches based on large language models.

Result: Incorporating personalized context provided up to 39% performance improvements in personalized question answering systems.

Conclusion: LaMP-QA benchmark is publicly released to support future research in personalized question answering.

Abstract: Personalization is essential for question answering systems that are
user-centric. Despite its importance, personalization in answer generation has
been relatively underexplored. This is mainly due to lack of resources for
training and evaluating personalized question answering systems. We address
this gap by introducing LaMP-QA -- a benchmark designed for evaluating
personalized long-form answer generation. The benchmark covers questions from
three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal
Development, and (3) Society & Culture, encompassing over 45 subcategories in
total. To assess the quality and potential impact of the LaMP-QA benchmark for
personalized question answering, we conduct comprehensive human and automatic
evaluations, to compare multiple evaluation strategies for evaluating generated
personalized responses and measure their alignment with human preferences.
Furthermore, we benchmark a number of non-personalized and personalized
approaches based on open-source and proprietary large language models (LLMs).
Our results show that incorporating the personalized context provided leads to
performance improvements of up to 39%. The benchmark is publicly released to
support future research in this area.

</details>


### [532] [Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages](https://arxiv.org/abs/2506.00912)
*Yongdong chi,Hanqing Wang,Zonghan Yang,Jian Yang,Xiao Yan,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: This paper introduces Pi-SQL, a method that uses Python as a bridge to convert natural language queries into SQL programs. It improves execution accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current prompt-based methods for Text-to-SQL conversion, which suffer from a large semantic gap between natural language and SQL due to SQL's low-resource nature.

Method: Pi-SQL generates Python programs from natural language queries, using them as an intermediary step before generating SQL programs. The Python programs provide detailed step-by-step guidelines, helping to produce more accurate SQL queries.

Result: Pi-SQL achieves superior execution speed with a reward-based valid efficiency score 4.55 higher than the best-performing baseline and improves execution accuracy by up to 3.20.

Conclusion: Pi-SQL effectively bridges the gap between natural language and SQL through Python, demonstrating significant improvements in both execution accuracy and efficiency.

Abstract: Text-to-SQL transforms the user queries from natural language to executable
SQL programs, enabling non-experts to interact with complex databases. Existing
prompt-based methods craft meticulous text guidelines and examples to
facilitate SQL generation, but their accuracy is hindered by the large semantic
gap between the texts and the low-resource SQL programs. In this work, we
propose Pi-SQL, which incorporates the high-resource Python program as a pivot
to bridge between the natural language query and SQL program. In particular,
Pi-SQL first generates Python programs that provide fine-grained step-by-step
guidelines in their code blocks or comments, and then produces an SQL program
following the guidance of each Python program.The final SQL program matches the
reference Python program's query results and, through selection from candidates
generated by different strategies, achieves superior execution speed, with a
reward-based valid efficiency score up to 4.55 higher than the best-performing
baseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which
improves the execution accuracy of the best-performing baseline by up to 3.20.

</details>


### [533] [How do Transformer Embeddings Represent Compositions? A Functional Analysis](https://arxiv.org/abs/2506.00914)
*Aishik Nagar,Ishaan Singh Rawal,Mansi Dhanania,Cheston Tan*

Main category: cs.CL

TL;DR: This paper investigates compositionality in transformer-based models like Mistral, OpenAI Large, and Google embedding models comparing them with BERT, using six diverse models of compositionality. Ridge regression best explains compositionality and most embedding models are found to be highly compositional except BERT.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer-based models represent compound words and if these representations are compositional, which is crucial for reasoning and generalization in human intelligence.

Method: Evaluate compositionality by examining six diverse models (addition, multiplication, dilation, regression, etc.) and compare the performance of Mistral, OpenAI Large, Google embedding models with BERT. Verify findings with a synthetic dataset of adjective-noun compositions.

Result: Ridge regression best accounts for compositionality and vector addition model performs almost as well. Most embedding models are highly compositional while BERT shows much poorer compositionality.

Conclusion: The study presents a thorough investigation of compositionality in various transformer-based models revealing significant differences in their compositional capabilities.

Abstract: Compositionality is a key aspect of human intelligence, essential for
reasoning and generalization. While transformer-based models have become the de
facto standard for many language modeling tasks, little is known about how they
represent compound words, and whether these representations are compositional.
In this study, we test compositionality in Mistral, OpenAI Large, and Google
embedding models, and compare them with BERT. First, we evaluate
compositionality in the representations by examining six diverse models of
compositionality (addition, multiplication, dilation, regression, etc.). We
find that ridge regression, albeit linear, best accounts for compositionality.
Surprisingly, we find that the classic vector addition model performs almost as
well as any other model. Next, we verify that most embedding models are highly
compositional, while BERT shows much poorer compositionality. We verify and
visualize our findings with a synthetic dataset consisting of fully transparent
adjective-noun compositions. Overall, we present a thorough investigation of
compositionality.

</details>


### [534] [Structuring Radiology Reports: Challenging LLMs with Lightweight Models](https://arxiv.org/abs/2506.00200)
*Johannes Moll,Louisa Fay,Asfandyar Azhar,Sophie Ostmeier,Tim Lueth,Sergios Gatidis,Curtis Langlotz,Jean-Benoit Delbrouck*

Main category: cs.CL

TL;DR: The paper explores lightweight encoder-decoder models for structuring radiology reports, comparing them with large language models. The best lightweight model outperforms prompt-based LLMs and offers a more sustainable solution despite modest improvements from LoRA-finetuned LLMs which require significantly greater resources.


<details>
  <summary>Details</summary>
Motivation: Radiology reports are crucial but lack standardization, impacting both human understanding and machine learning applications. Large language models, while capable in reformatting clinical text, have limitations due to high computational demands, lack of transparency, and data privacy concerns.

Method: The study investigates T5 and BERT2BERT (<300M parameters) lightweight encoder-decoder models for structuring radiology reports from MIMIC-CXR and CheXpert Plus datasets. These models are benchmarked against eight open-source LLMs (1B-70B) adapted via prefix prompting, in-context learning, and LoRA finetuning.

Result: The top-performing lightweight model surpasses all LLMs adapted using prompt-based techniques on a human-annotated test set. Some LoRA-finetuned LLMs show minor improvements over the lightweight model in specific sections but at a much higher computational cost.

Conclusion: Lightweight, task-specific models present a viable, sustainable, and privacy-preserving alternative for structuring clinical text, particularly suitable for healthcare settings with limited resources.

Abstract: Radiology reports are critical for clinical decision-making but often lack a
standardized format, limiting both human interpretability and machine learning
(ML) applications. While large language models (LLMs) have shown strong
capabilities in reformatting clinical text, their high computational
requirements, lack of transparency, and data privacy concerns hinder practical
deployment. To address these challenges, we explore lightweight encoder-decoder
models (<300M parameters)-specifically T5 and BERT2BERT-for structuring
radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark
these models against eight open-source LLMs (1B-70B), adapted using prefix
prompting, in-context learning (ICL), and low-rank adaptation (LoRA)
finetuning. Our best-performing lightweight model outperforms all LLMs adapted
using prompt-based techniques on a human-annotated test set. While some
LoRA-finetuned LLMs achieve modest gains over the lightweight model on the
Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,
GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of
substantially greater computational resources. For example, LLaMA-3-70B
incurred more than 400 times the inference time, cost, and carbon emissions
compared to the lightweight model. These results underscore the potential of
lightweight, task-specific models as sustainable and privacy-preserving
solutions for structuring clinical text in resource-constrained healthcare
settings.

</details>


### [535] [anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding](https://arxiv.org/abs/2506.00942)
*Haitao Li,Ziyu Li,Yiheng Mao,Ziyi Liu,Zhoujian Sun,Zhengxing Huang*

Main category: cs.CL

TL;DR: The paper presents anyECG-chat, a multimodal large language model designed for versatile ECG analysis tasks using the anyECG dataset.


<details>
  <summary>Details</summary>
Motivation: Existing ECG-focused MLLMs are primarily limited to report generation tasks with single 12-lead, short-duration ECG inputs. Also, existing ECG-QA datasets are monotonous and do not fully utilize the potential of MLLMs in ECG analysis.

Method: The authors constructed the anyECG dataset covering diverse tasks such as report generation, abnormal waveform localization, and open-ended question answering. They introduced long-duration reduced-lead ECGs and multiple ECG comparison scenarios. The anyECG-chat model was proposed to support dynamic-length and multiple ECG inputs, trained using a three-stage curriculum training recipe with the anyECG dataset.

Result: Evaluation showed that anyECG-chat supports various practical application scenarios including common report generation, abnormal waveform localization for home environment ECGs, and comprehensive comparative analysis of multiple ECGs.

Conclusion: anyECG-chat is capable of supporting a broad range of tasks and more flexible ECG inputs, thus better utilizing the potential of MLLMs in ECG analysis.

Abstract: The advent of multimodal large language models (MLLMs) has sparked interest
in their application to electrocardiogram (ECG) analysis. However, existing
ECG-focused MLLMs primarily focus on report generation tasks, often limited to
single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the
potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that
supports a broader range of tasks and more flexible ECG inputs. However,
existing ECG-QA datasets are often monotonous. To address this gap, we first
constructed the anyECG dataset, which encompasses a wide variety of tasks,
including report generation, abnormal waveform localization, and open-ended
question answering. In addition to standard hospital ECGs, we introduced
long-duration reduced-lead ECGs for home environments and multiple ECG
comparison scenarios commonly encountered in clinical practice. Furthermore, we
propose the anyECG-chat model, which supports dynamic-length ECG inputs and
multiple ECG inputs. We trained the model using a three-stage curriculum
training recipe with the anyECG dataset. A comprehensive evaluation was
conducted, demonstrating that anyECG-chat is capable of supporting various
practical application scenarios, including not only common report generation
tasks but also abnormal waveform localization for long-duration reduced-lead
ECGs in home environments and comprehensive comparative analysis of multiple
ECGs.

</details>


### [536] [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
*Qichao Wang,Ziqiao Meng,Wenqian Cui,Yifei Zhang,Pengcheng Wu,Bingzhe Wu,Irwin King,Liang Chen,Peilin Zhao*

Main category: cs.CL

TL;DR: 受GPT-4o的启发，研究者们对使语音语言模型（SLMs）能够与人类进行自然、流畅的口语互动越来越感兴趣。本文系统地探讨了在现代大型语言模型中使用双声道语音数据，并引入了一种新的生成建模范式——Next-Token-Pair Prediction (NTPP)，首次使用仅解码器架构实现与说话人无关的双声道口语对话学习。实验结果表明，NTPP显著提高了SLMs的对话能力，并且相比现有方法，其推理延迟大大降低，适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 尽管最近在使语音语言模型（SLMs）能够与人类进行自然、流畅的口语互动方面取得了进展，但目前的方法尚未充分利用双声道语音数据，而这种数据本质上捕捉了人类对话的结构和动态。

Method: 提出了一种新的生成建模范式——Next-Token-Pair Prediction (NTPP)，该方法能够在仅解码器架构上实现与说话人无关的双声道口语对话学习。

Result: 实证结果显示，所提出的方法NTPP显著提高了SLMs在轮次预测、响应连贯性和自然性方面的对话能力。此外，与现有方法相比，NTPP实现了显著更低的推理延迟，强调了其实时应用的实际效率。

Conclusion: NTPP方法通过利用双声道语音数据，显著提高了SLMs的对话能力，并且具有较低的推理延迟，适用于实时应用。这为未来的研究提供了一个有前途的方向，特别是在需要高效对话系统的实际场景中。

Abstract: Inspired by the impressive capabilities of GPT-4o, there is growing interest
in enabling speech language models (SLMs) to engage in natural, fluid spoken
interactions with humans. Recent advancements have led to the development of
several SLMs that demonstrate promising results in this area. However, current
approaches have yet to fully exploit dual-channel speech data, which inherently
captures the structure and dynamics of human conversation. In this work, we
systematically explore the use of dual-channel speech data in the context of
modern large language models, and introduce a novel generative modeling
paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent
dual-channel spoken dialogue learning using decoder-only architectures for the
first time. We evaluate our approach on standard benchmarks, and empirical
results show that our proposed method, NTPP, significantly improves the
conversational abilities of SLMs in terms of turn-taking prediction, response
coherence, and naturalness. Moreover, compared to existing methods, NTPP
achieves substantially lower inference latency, highlighting its practical
efficiency for real-time applications.

</details>


### [537] [What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training](https://arxiv.org/abs/2506.00981)
*Marianne de Heer Kloots,Hosein Mohebbi,Charlotte Pouw,Gaofei Shen,Willem Zuidema,Martijn Bentum*

Main category: cs.CL

TL;DR: Self-supervised Wav2Vec2 models pre-trained exclusively on Dutch better represent Dutch linguistic features compared to pre-training on English or multilingual data, which aligns with downstream ASR performance.


<details>
  <summary>Details</summary>
Motivation: To investigate how language-specific speech representations are learned by self-supervised models and whether pre-training on specific languages improves the encoding of language-specific linguistic information.

Method: Tested the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models pre-trained on Dutch, English, and multilingual data. Used trained clustering or classification probes and zero-shot metrics for analysis.

Result: Pre-training exclusively on Dutch improved the representation of Dutch linguistic features more than pre-training on similar amounts of English or larger amounts of multilingual data. This advantage was detected by trained probes and partially observable using zero-shot metrics.

Conclusion: Language-specific pre-training enhances the encoding of linguistic features, which correlates with better performance on downstream tasks like Automatic Speech Recognition.

Abstract: How language-specific are speech representations learned by self-supervised
models? Existing work has shown that a range of linguistic features can be
successfully decoded from end-to-end models trained only on speech recordings.
However, it's less clear to what extent pre-training on specific languages
improves language-specific linguistic information. Here we test the encoding of
Dutch phonetic and lexical information in internal representations of
self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the
representation of Dutch linguistic features as compared to pre-training on
similar amounts of English or larger amounts of multilingual data. This
language-specific advantage is well-detected by trained clustering or
classification probes, and partially observable using zero-shot metrics.
Furthermore, the language-specific benefit on linguistic feature encoding
aligns with downstream performance on Automatic Speech Recognition.

</details>


### [538] [DLM-One: Diffusion Language Models for One-Step Sequence Generation](https://arxiv.org/abs/2506.00290)
*Tianqi Chen,Shujian Zhang,Mingyuan Zhou*

Main category: cs.CL

TL;DR: This paper introduces DLM-One, a score-distillation-based framework that achieves up to ~500x speedup in inference time while maintaining competitive performance on benchmark text generation tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate whether a one-step sequence generation framework can achieve substantial gains in sampling efficiency for language modeling with continuous diffusion language models.

Method: DLM-One aligns the scores of a student model's outputs in the continuous token embedding space with the score function of a pretrained teacher DLM, eliminating the need for iterative refinement.

Result: DLM-One achieves up to ~500x speedup in inference time while maintaining competitive performance on benchmark text generation tasks.

Conclusion: One-step diffusion is a promising direction for efficient, high-quality language generation and broader adoption of continuous diffusion models in natural language processing.

Abstract: This paper introduces DLM-One, a score-distillation-based framework for
one-step sequence generation with continuous diffusion language models (DLMs).
DLM-One eliminates the need for iterative refinement by aligning the scores of
a student model's outputs in the continuous token embedding space with the
score function of a pretrained teacher DLM. We investigate whether DLM-One can
achieve substantial gains in sampling efficiency for language modeling. Through
comprehensive experiments on DiffuSeq -- a representative continuous DLM -- we
show that DLM-One achieves up to ~500x speedup in inference time while
maintaining competitive performance on benchmark text generation tasks used to
evaluate the teacher models. We further analyze the method's empirical behavior
across multiple datasets, providing initial insights into its generality and
practical applicability. Our findings position one-step diffusion as a
promising direction for efficient, high-quality language generation and broader
adoption of continuous diffusion models operating in embedding space for
natural language processing.

</details>


### [539] [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://arxiv.org/abs/2506.01034)
*Benjamin Matthias Ruppik,Julius von Rohrscheidt,Carel van Niekerk,Michael Heck,Renato Vukovic,Shutong Feng,Hsien-chin Lin,Nurul Lubis,Bastian Rieck,Marcus Zibrowius,Milica Gašić*

Main category: cs.CL

TL;DR: 研究通过测量语言模型潜在空间的局部维度，分析训练和微调的影响，发现局部维度均值可预测模型训练能力极限、过拟合及grokking现象，并提出局部维度减少预示性能提升的实用启发式方法。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型内部机制颇具挑战，尤其是微调如何影响模型行为仍需大量实证评估，本文旨在从几何属性视角探索训练与微调对模型的影响。

Method: 通过测量上下文语言模型潜在空间的局部维度并分析其在训练和微调过程中的变化，探讨局部维度与模型训练动态及泛化能力之间的关系。

Result: 局部维度均值可预测模型训练能力耗尽（如对话状态跟踪任务）、过拟合（如情感识别任务）和grokking现象（算术任务）。实验表明，局部维度均值下降通常伴随并预测性能提升。

Conclusion: 本研究为从业者提供了关于微调对嵌入空间影响的深入理解，有助于在特定应用中优化模型配置，同时推动了关于LLM解释性、适应性和泛化性的讨论。

Abstract: Understanding the internal mechanisms of large language models (LLMs) remains
a challenging and complex endeavor. Even fundamental questions, such as how
fine-tuning affects model behavior, often require extensive empirical
evaluation. In this paper, we introduce a novel perspective based on the
geometric properties of contextual latent embeddings to study the effects of
training and fine-tuning. To that end, we measure the local dimensions of a
contextual language model's latent space and analyze their shifts during
training and fine-tuning. We show that the local dimensions provide insights
into the model's training dynamics and generalization ability. Specifically,
the mean of the local dimensions predicts when the model's training
capabilities are exhausted, as exemplified in a dialogue state tracking task,
overfitting, as demonstrated in an emotion recognition task, and grokking, as
illustrated with an arithmetic task. Furthermore, our experiments suggest a
practical heuristic: reductions in the mean local dimension tend to accompany
and predict subsequent performance gains. Through this exploration, we aim to
provide practitioners with a deeper understanding of the implications of
fine-tuning on embedding spaces, facilitating informed decisions when
configuring models for specific applications. The results of this work
contribute to the ongoing discourse on the interpretability, adaptability, and
generalizability of LLMs by bridging the gap between intrinsic model mechanisms
and geometric properties in the respective embeddings.

</details>


### [540] [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)
*Yu Zheng,Yuan Yuan,Yong Li,Paolo Santi*

Main category: cs.CL

TL;DR: Graph probing is a new method to uncover the functional connectivity topology of LLM neurons and its relation to language generation performance, revealing universal predictability and consistent neural topological structures across different LLMs.


<details>
  <summary>Details</summary>
Motivation: To gain deeper understanding of how neurons functionally co-activate in LLMs to give rise to emergent capabilities and improve safer development of LLMs.

Method: Introduce graph probing, analyze internal neural graphs across diverse LLM families and scales, perform graph matching analysis.

Result: Discovered universal predictability of next-token prediction performance using only neural topology, robust even with sparse connections or early pretraining steps. Found intricate and consistent neural topological structures across different LLMs.

Conclusion: Graph probing reveals significant insights into the functional connectivity topology of LLM neurons, contributing to a better understanding of their language generation abilities.

Abstract: Probing large language models (LLMs) has yielded valuable insights into their
internal mechanisms by linking neural representations to interpretable
semantics. However, how neurons functionally co-activate with each other to
give rise to emergent capabilities remains largely unknown, hindering a deeper
understanding and safer development of LLMs. In this work, we introduce graph
probing, a method for uncovering the functional connectivity topology of LLM
neurons and relating it to language generation performance. By analyzing
internal neural graphs across diverse LLM families and scales, we discover a
universal predictability of next-token prediction performance using only neural
topology. This predictability is robust even when retaining just 1% of neuron
connections or probing models after only 8 pretraining steps, highlighting the
sparsity and early emergence of topological patterns. Further graph matching
analysis suggests that, despite significant distinctions in architectures,
parameters, and training data, different LLMs develop intricate and consistent
neural topological structures that may form the foundation for their language
generation abilities. Codes and data for the graph probing toolbox are released
at https://github.com/DavyMorgan/llm-graph-probing.

</details>


### [541] [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
*Thinh Pham,Nguyen Nguyen,Pratibha Zunjare,Weiyuan Chen,Yu-Min Tseng,Tu Vu*

Main category: cs.CL

TL;DR: The paper introduces SealQA, a benchmark for evaluating language models on fact-seeking questions with conflicting or unhelpful search results. It highlights the limitations of current models in reasoning and accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark that evaluates how well search-augmented language models handle challenging fact-seeking questions where web search provides conflicting, noisy, or unhelpful results.

Method: Introduced SealQA in three versions: Seal-0 (main), Seal-Hard, and LongSeal to test different aspects like factual accuracy, reasoning capabilities, and long-context multi-document reasoning.

Result: Current frontier LLMs perform poorly across all flavors of SealQA. Models struggle with noisy search results and increasing compute doesn't guarantee performance improvement. Recent models still fail to reliably identify relevant documents in complex settings.

Conclusion: SealQA reveals critical limitations in current language models' reasoning and accuracy capabilities when dealing with challenging fact-seeking questions.

Abstract: We introduce SealQA, a new challenge benchmark for evaluating
SEarch-Augmented Language models on fact-seeking questions where web search
yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:
(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and
reasoning capabilities, with Seal-0 focusing on the most challenging questions
where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)
LongSeal, which extends SealQA to test long-context, multi-document reasoning
in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations
in current models: Even frontier LLMs perform poorly across all SealQA flavors.
On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini
achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning
efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and
o3-mini are highly vulnerable to noisy search results. Notably, increasing
test-time compute does not yield reliable gains across o3-mini, o4-mini, and
o3, with performance often plateauing or even declining early. Additionally,
while recent models are less affected by the "lost-in-the-middle" issue, they
still fail to reliably identify relevant documents in LongSeal when faced with
numerous distractors. To facilitate future work, we release SealQA at
huggingface.co/datasets/vtllms/sealqa.

</details>


### [542] [Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements](https://arxiv.org/abs/2506.01089)
*Metehan Oguz,Yavuz Bakman,Duygu Nur Yaldiz*

Main category: cs.CL

TL;DR: This paper evaluates the performance of large language models (LLMs) on coreference resolution with indexicals, presenting a new dataset and revealing varying performances based on the type of indexical and syntactic cues.


<details>
  <summary>Details</summary>
Motivation: Previous studies have mainly focused on LLMs' coreference resolution abilities with nouns and third person pronouns. This study aims to fill the gap by examining LLMs' performance with indexicals such as 'I', 'you', 'here', and 'tomorrow'.

Method: The authors created the English Indexical Dataset containing 1600 multiple-choice questions and used it to evaluate several pioneering LLMs including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3.

Result: LLMs show impressive performance with some indexicals ('I'), but struggle with others ('you', 'here', 'tomorrow'). Additionally, syntactic cues can either enhance or diminish LLMs' performance depending on the indexical.

Conclusion: The study highlights the varying capabilities of LLMs in interpreting different types of indexicals and the impact of syntactic cues on their performance.

Abstract: Large Language Models (LLMs) have demonstrated impressive performances in
tasks related to coreference resolution. However, previous studies mostly
assessed LLM performance on coreference resolution with nouns and third person
pronouns. This study evaluates LLM performance on coreference resolution with
indexical like I, you, here and tomorrow, which come with unique challenges due
to their linguistic properties. We present the first study examining how LLMs
interpret indexicals in English, releasing the English Indexical Dataset with
1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,
Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that
LLMs exhibit an impressive performance with some indexicals (I), while
struggling with others (you, here, tomorrow), and that syntactic cues (e.g.
quotation) contribute to LLM performance with some indexicals, while they
reduce performance with others. Code and data are available at:
https://github.com/metehanoguzz/LLMs-Indexicals-English.

</details>


### [543] [EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models](https://arxiv.org/abs/2506.00479)
*Zekun Wang,Minghua Ma,Zexin Wang,Rongchuan Mu,Liping Shan,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: Large Vision-Language Models (LVLMs) are successful but computationally expensive. This paper evaluates acceleration techniques, introduces EffiVLM-Bench framework, and provides insights into optimal strategies for accelerating LVLMs.


<details>
  <summary>Details</summary>
Motivation: LVLMs have remarkable success but significant computational demands hinder practical deployment. Existing efficiency improvement methods lack comprehensive evaluation across diverse backbones, benchmarks, and metrics.

Method: Systematically evaluate mainstream acceleration techniques for LVLMs categorized into token and parameter compression. Introduce EffiVLM-Bench, a unified framework for assessing performance, generalization, loyalty, and exploring Pareto-optimal trade-offs.

Result: Extensive experiments and in-depth analyses offer insights into optimal strategies for accelerating LVLMs.

Conclusion: The paper provides a systematic evaluation of LVLM acceleration techniques, introduces EffiVLM-Bench, and open-sources code and recipes to foster future research.

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success, yet
their significant computational demands hinder practical deployment. While
efforts to improve LVLM efficiency are growing, existing methods lack
comprehensive evaluation across diverse backbones, benchmarks, and metrics. In
this work, we systematically evaluate mainstream acceleration techniques for
LVLMs, categorized into token and parameter compression. We introduce
EffiVLM-Bench, a unified framework for assessing not only absolute performance
but also generalization and loyalty, while exploring Pareto-optimal trade-offs.
Our extensive experiments and in-depth analyses offer insights into optimal
strategies for accelerating LVLMs. We open-source code and recipes for
EffiVLM-Bench to foster future research.

</details>


### [544] [From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models](https://arxiv.org/abs/2506.01133)
*Asım Ersoy,Basel Mousi,Shammur Chowdhury,Firoj Alam,Fahim Dalvi,Nadir Durrani*

Main category: cs.CL

TL;DR: Large language models (LLMs) show properties related to general intelligence, prompting questions about whether similar concepts emerge in speech-trained models or multimodal models. This paper uses Latent Concept Analysis to explore the conceptual structures learned by speech and textual models individually and jointly.


<details>
  <summary>Details</summary>
Motivation: To investigate if abstract semantic concepts emerge in models trained on modalities other than text, such as speech, and if joint training on multiple modalities leads to a richer semantic understanding.

Method: The study applies Latent Concept Analysis, an unsupervised method, to uncover and interpret latent representations in neural networks of speech and textual models, both individually and when trained jointly.

Result: The analysis reveals insights into how semantic abstractions form across different modalities, contributing to understanding the conceptual structures learned by these models.

Conclusion: Models trained on speech and other modalities can develop complex conceptual structures. Joint training may enhance semantic understanding, though further exploration is needed.

Abstract: The emergence of large language models (LLMs) has demonstrated that systems
trained solely on text can acquire extensive world knowledge, develop reasoning
capabilities, and internalize abstract semantic concepts--showcasing properties
that can be associated with general intelligence. This raises an intriguing
question: Do such concepts emerge in models trained on other modalities, such
as speech? Furthermore, when models are trained jointly on multiple modalities:
Do they develop a richer, more structured semantic understanding? To explore
this, we analyze the conceptual structures learned by speech and textual models
both individually and jointly. We employ Latent Concept Analysis, an
unsupervised method for uncovering and interpreting latent representations in
neural networks, to examine how semantic abstractions form across modalities.
For reproducibility we made scripts and other resources available to the
community.

</details>


### [545] [Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models](https://arxiv.org/abs/2506.00483)
*Aviv Jan,Dean Tahory,Omer Talmi,Omar Abo Mokh*

Main category: cs.CL

TL;DR: Auto-Patch is a novel method that dynamically patches hidden states during inference to enhance multi-hop reasoning in large language models (LLMs). Evaluated on the MuSiQue dataset, it improves the solve rate from 18.45% (baseline) to 23.63±0.7%.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with multi-hop questions which require linking information across multiple reasoning steps.

Method: Auto-Patch dynamically patches hidden states during inference using a learned classifier to selectively modify internal representations of LLMs.

Result: On the MuSiQue dataset, Auto-Patch increased the solve rate from 18.45% (baseline) to 23.63±0.7%, narrowing the gap to Chain-of-Thought prompting (27.44%).

Conclusion: The study demonstrates the potential of dynamic hidden state interventions for improving complex reasoning in LLMs.

Abstract: Multi-hop questions still stump large language models (LLMs), which struggle
to link information across multiple reasoning steps. We introduce Auto-Patch, a
novel method that dynamically patches hidden states during inference to enhance
multi-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch
selectively modifies internal representations using a learned classifier.
Evaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from
18.45\% (baseline) to 23.63~$\pm$~0.7\% (3 runs), narrowing the gap to
Chain-of-Thought prompting (27.44\%). Our results highlight the potential of
dynamic hidden state interventions for advancing complex reasoning in LLMs.

</details>


### [546] [Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures](https://arxiv.org/abs/2506.01197)
*Mark Muchane,Sean Richardson,Kiho Park,Victor Veitch*

Main category: cs.CL

TL;DR: A modified SAE architecture is introduced to model a semantic hierarchy of concepts, which improves reconstruction, interpretability and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Sparse dictionary learning and sparse autoencoders have a limitation in that they do not exploit nor represent the semantic relationships between learned concepts.

Method: Introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts.

Result: This architecture can learn semantic hierarchy, and it improves reconstruction, interpretability and computational efficiency.

Conclusion: The modified SAE architecture successfully models the semantic hierarchy of concepts, leading to improvements in multiple aspects including reconstruction, interpretability and computational efficiency.

Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts
to learn a set of human-understandable concepts that can explain variation on
an abstract space. A basic limitation of this approach is that it neither
exploits nor represents the semantic relationships between the learned
concepts. In this paper, we introduce a modified SAE architecture that
explicitly models a semantic hierarchy of concepts. Application of this
architecture to the internal representations of large language models shows
both that semantic hierarchy can be learned, and that doing so improves both
reconstruction and interpretability. Additionally, the architecture leads to
significant improvements in computational efficiency.

</details>


### [547] [Mamba Drafters for Speculative Decoding](https://arxiv.org/abs/2506.01206)
*Daewon Choi,Seunghyuk Oh,Saket Dingliwal,Jihoon Tack,Kyuyoung Kim,Woomin Song,Seojin Kim,Insu Han,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Main category: cs.CL

TL;DR: The paper proposes Mamba-based drafters that combine the advantages of external and self-speculation methods for speculative decoding in LLMs, offering faster drafting, lower memory usage, and high-quality outputs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing speculative decoding approaches which either suffer from slower drafting or require re-training.

Method: Introduces novel drafters based on Mamba (a state-of-the-art SSM), leveraging the linear structure of SSMs to avoid quadratic complexity, and uses a test-time tree search algorithm for generating high-quality draft candidates.

Result: Mamba-based drafters outperform existing external drafting methods, are comparable to state-of-the-art self-speculation approaches, use less memory, and maintain cross-model adaptability.

Conclusion: Mamba-based drafters present a promising solution for speculative decoding in LLMs by combining speed, efficiency, and flexibility.

Abstract: Speculative decoding has emerged as a promising approach to accelerating
large language model (LLM) generation using a fast drafter while maintaining
alignment with the target model's distribution. However, existing approaches
face a trade-off: external drafters offer flexibility but can suffer from
slower drafting, while self-speculation methods use drafters tailored to the
target model but require re-training. In this paper, we introduce novel
drafters based on Mamba, a state-of-the-art state space model (SSM), as a
solution that combines the best aspects of both approaches. By leveraging the
linear structure of SSMs, our approach avoids the quadratic complexity inherent
in traditional Transformer-based methods, enabling faster drafting and lower
memory usage while maintaining the flexibility to work across different target
models. We further enhance efficiency with a novel test-time tree search
algorithm for generating high-quality draft candidates. Our empirical
evaluation demonstrates that Mamba-based drafters not only outperform existing
external drafting methods but are also comparable to state-of-the-art
self-speculation approaches while using less memory and maintaining their
cross-model adaptability.

</details>


### [548] [Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean](https://arxiv.org/abs/2506.01237)
*SungHo Kim,Nayeon Kim,Taehee Jeon,SangKeun Lee*

Main category: cs.CL

TL;DR: The paper introduces KoGEM, a benchmark for evaluating linguistic competence of LLMs and humans in Korean. It reveals LLMs' limitations and suggests ways to enhance their linguistic competence.


<details>
  <summary>Details</summary>
Motivation: To assess the linguistic competence of LLMs and humans in Korean and uncover hidden facets of LLMs' language understanding.

Method: Introduced KoGEM, consisting of 1.5k multiple-choice QA pairs across five main categories and 16 subcategories, and conducted zero-shot evaluation on 27 LLMs.

Result: LLMs perform well on straightforward tasks requiring definitional knowledge but struggle with tasks demanding real-world experiential knowledge.

Conclusion: KoGEM highlights the limitations of current LLMs in linguistic competence and paves the way for enhancing comprehensive language understanding.

Abstract: We introduce the $\underline{Ko}rean \underline{G}rammar
\underline{E}valuation Bench\underline{M}ark (KoGEM)$, designed to assess the
linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k
multiple-choice QA pairs covering five main categories and 16 subcategories.
The zero-shot evaluation of 27 LLMs of various sizes and types reveals that
while LLMs perform remarkably well on straightforward tasks requiring primarily
definitional knowledge, they struggle with tasks that demand the integration of
real-world experiential knowledge, such as phonological rules and
pronunciation. Furthermore, our in-depth analysis suggests that incorporating
such experiential knowledge could enhance the linguistic competence of LLMs.
With KoGEM, we not only highlight the limitations of current LLMs in linguistic
competence but also uncover hidden facets of LLMs in linguistic competence,
paving the way for enhancing comprehensive language understanding. Our code and
dataset are available at: https://github.com/SungHo3268/KoGEM.

</details>


### [549] [MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine](https://arxiv.org/abs/2506.01252)
*Shufeng Kong,Xingru Yang,Yuanyuan Wei,Zijie Wang,Hao Tang,Jiuqi Qin,Shuting Lan,Yingheng Wang,Junwen Bai,Zhuangbin Chen,Zibin Zheng,Caihua Liu,Hao Liang*

Main category: cs.CL

TL;DR: 为了评估大型语言模型在中医领域的知识、推理和安全性，研究者开发了一个多任务基准MTCMB。该基准包括12个子数据集，涵盖了知识问答、语言理解、诊断推理、处方生成和安全评估五个主要类别。初步结果显示，当前的大型语言模型在基础知识点上表现良好，但在临床推理、处方规划和安全合规方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在处理自然语言方面表现出色，但其在中医领域的系统性评估尚未得到充分发展。现有的基准要么过于狭隘地关注事实问答，要么缺乏领域特定的任务和临床现实主义。

Method: 与认证的中医专家合作开发了MTCMB，该基准包含12个子数据集，涵盖五个主要类别：知识问答、语言理解、诊断推理、处方生成和安全评估。这些数据集整合了真实世界的病例记录、国家执照考试和经典文本。

Result: 初步结果表明，目前的大型语言模型在基础知识点上的表现不错，但在临床推理、处方规划和安全合规方面存在明显不足。

Conclusion: 这项研究强调了对领域对齐基准（如MTCMB）的迫切需求，以指导更胜任和值得信赖的医疗AI系统的开发。所有数据集、代码和评估工具都可以公开获取。

Abstract: Traditional Chinese Medicine (TCM) is a holistic medical system with
millennia of accumulated clinical experience, playing a vital role in global
healthcare-particularly across East Asia. However, the implicit reasoning,
diverse textual forms, and lack of standardization in TCM pose major challenges
for computational modeling and evaluation. Large Language Models (LLMs) have
demonstrated remarkable potential in processing natural language across diverse
domains, including general medicine. Yet, their systematic evaluation in the
TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on
factual question answering or lack domain-specific tasks and clinical realism.
To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs
on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with
certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major
categories: knowledge QA, language understanding, diagnostic reasoning,
prescription generation, and safety evaluation. The benchmark integrates
real-world case records, national licensing exams, and classical texts,
providing an authentic and comprehensive testbed for TCM-capable models.
Preliminary results indicate that current LLMs perform well on foundational
knowledge but fall short in clinical reasoning, prescription planning, and
safety compliance. These findings highlight the urgent need for domain-aligned
benchmarks like MTCMB to guide the development of more competent and
trustworthy medical AI systems. All datasets, code, and evaluation tools are
publicly available at: https://github.com/Wayyuanyuan/MTCMB.

</details>


### [550] [DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://arxiv.org/abs/2506.01257)
*Jiancheng Ye,Sophie Bronstein,Jiarui Hai,Malak Abu Hashish*

Main category: cs.CL

TL;DR: DeepSeek-R1 is an open-source LLM that integrates MoE, CoT reasoning, and reinforcement learning, excelling in structured problem-solving but with limitations in bias and safety.


<details>
  <summary>Details</summary>
Motivation: To create a transparent and cost-effective alternative to proprietary large language models while maintaining advanced reasoning capabilities.

Method: Development of a hybrid architecture model combining mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning.

Result: Competitive performance on benchmarks like USMLE and AIME, efficient inference suitable for resource-constrained settings, but increased vulnerability to bias, misinformation, and adversarial manipulation.

Conclusion: DeepSeek-R1 advances open, scalable AI but requires collaborative governance for responsible deployment, with future research focusing on bias mitigation and natural language comprehension.

Abstract: DeepSeek-R1 is a cutting-edge open-source large language model (LLM)
developed by DeepSeek, showcasing advanced reasoning capabilities through a
hybrid architecture that integrates mixture of experts (MoE), chain of thought
(CoT) reasoning, and reinforcement learning. Released under the permissive MIT
license, DeepSeek-R1 offers a transparent and cost-effective alternative to
proprietary models like GPT-4o and Claude-3 Opus; it excels in structured
problem-solving domains such as mathematics, healthcare diagnostics, code
generation, and pharmaceutical research. The model demonstrates competitive
performance on benchmarks like the United States Medical Licensing Examination
(USMLE) and American Invitational Mathematics Examination (AIME), with strong
results in pediatric and ophthalmologic clinical decision support tasks. Its
architecture enables efficient inference while preserving reasoning depth,
making it suitable for deployment in resource-constrained settings. However,
DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,
adversarial manipulation, and safety failures - especially in multilingual and
ethically sensitive contexts. This survey highlights the model's strengths,
including interpretability, scalability, and adaptability, alongside its
limitations in general language fluency and safety alignment. Future research
priorities include improving bias mitigation, natural language comprehension,
domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1
represents a major advance in open, scalable AI, underscoring the need for
collaborative governance to ensure responsible and equitable deployment.

</details>


### [551] [Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model](https://arxiv.org/abs/2506.01266)
*Yuanhe Tian,Mingjie Deng,Guoqing Jin,Yan Song*

Main category: cs.CL

TL;DR: A new method for LLM detoxification using a compact, pre-trained calibration model is proposed, which reduces toxicity without compromising fluency or contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Existing LLM detoxification methods are computationally expensive, lack robustness, and often compromise LLMs' fluency and contextual understanding.

Method: Propose an approach that leverages a compact, pre-trained calibration model to guide the detoxification process of a target LLM via a lightweight intervention in its generation pipeline. The calibration model learns a detoxified embedding space from non-toxic data to steer the LLM away from generating harmful content.

Result: Experiment results on the benchmark dataset demonstrate that the approach reduces toxicity while maintaining reasonable content expression.

Conclusion: The proposed approach provides a simple yet effective solution for LLM detoxification.

Abstract: Existing approaches for Large language model (LLM) detoxification generally
rely on training on large-scale non-toxic or human-annotated preference data,
designing prompts to instruct the LLM to generate safe content, or modifying
the model parameters to remove toxic information, which are computationally
expensive, lack robustness, and often compromise LLMs' fluency and contextual
understanding. In this paper, we propose a simple yet effective approach for
LLM detoxification, which leverages a compact, pre-trained calibration model
that guides the detoxification process of a target LLM via a lightweight
intervention in its generation pipeline. By learning a detoxified embedding
space from non-toxic data, the calibration model effectively steers the LLM
away from generating harmful content. This approach only requires a one-time
training of the calibration model that is able to be seamlessly applied to
multiple LLMs without compromising fluency or contextual understanding.
Experiment results on the benchmark dataset demonstrate that our approach
reduces toxicity while maintaining reasonable content expression.

</details>


### [552] [L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models](https://arxiv.org/abs/2506.00863)
*Nidhi Kowtal,Raviraj Joshi*

Main category: cs.CL

TL;DR: The paper presents L3Cube-MahaEmotions, a new Marathi emotion recognition dataset with 11 fine-grained labels. Training data is annotated synthetically using GPT-4, while validation/test sets are manually labeled. Experiments show GPT-4 outperforms fine-tuned BERT models for low-resource emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Emotion recognition in low-resource languages like Marathi faces challenges due to limited annotated data. Existing datasets lack fine-grained emotion labels and reliable benchmarks.

Method: Developed a high-quality Marathi emotion recognition dataset with 11 fine-grained labels. Used Chain-of-Translation prompting technique where Marathi sentences were translated into English and emotion labeled via single prompt. Evaluated GPT-4 and Llama3-405B for synthetic annotation, selecting GPT-4 for superior label quality. Compared model performance using standard metrics and explored label aggregation strategies.

Result: GPT-4 predictions outperformed fine-tuned BERT models. BERT-based models trained on synthetic labels failed to surpass GPT-4. Generic LLMs like GPT-4 and Llama3-405B generalized better than fine-tuned BERT for complex low-resource emotion recognition tasks.

Conclusion: Synthetic annotations using advanced LLMs can effectively address data scarcity in low-resource emotion recognition tasks. High-quality human-labeled data remains crucial for benchmarking. The findings emphasize the complexity of emotion recognition and highlight the advantages of generic LLMs over specialized models.

Abstract: Emotion recognition in low-resource languages like Marathi remains
challenging due to limited annotated data. We present L3Cube-MahaEmotions, a
high-quality Marathi emotion recognition dataset with 11 fine-grained emotion
labels. The training data is synthetically annotated using large language
models (LLMs), while the validation and test sets are manually labeled to serve
as a reliable gold-standard benchmark. Building on the MahaSent dataset, we
apply the Chain-of-Translation (CoTR) prompting technique, where Marathi
sentences are translated into English and emotion labeled via a single prompt.
GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data
annotation due to superior label quality. We evaluate model performance using
standard metrics and explore label aggregation strategies (e.g., Union,
Intersection). While GPT-4 predictions outperform fine-tuned BERT models,
BERT-based models trained on synthetic labels fail to surpass GPT-4. This
highlights both the importance of high-quality human-labeled data and the
inherent complexity of emotion recognition. An important finding of this work
is that generic LLMs like GPT-4 and Llama3-405B generalize better than
fine-tuned BERT for complex low-resource emotion recognition tasks. The dataset
and model are shared publicly at https://github.com/l3cube-pune/MarathiNLP

</details>


### [553] [Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines](https://arxiv.org/abs/2506.01329)
*Guifeng Deng,Shuyin Rao,Tianyu Lin,Anlu Dai,Pan Wang,Junyi Xie,Haidong Song,Ke Zhao,Dongwu Xu,Zhengdong Cheng,Tao Li,Haiteng Jiang*

Main category: cs.CL

TL;DR: 心理支持热线对于危机干预至关重要，但由于需求增加面临重大挑战。大型语言模型（LLMs）可以支持危机评估，但在情感敏感环境中的能力尚不清楚。本文介绍了PsyCrisisBench，这是一个来自杭州心理援助热线的540个标注转录本基准，评估四个任务：情绪状态识别、自杀意念检测、自杀计划识别和风险评估。我们使用零样本、少量样本和微调范式评估了15个家族的64个LLMs（例如GPT、Claude、Gemini、Llama、Qwen、DeepSeek）。性能通过F1分数衡量，并通过Welch的t检验进行统计比较。LLMs在自杀意念检测（F1=0.880）、自杀计划识别（F1=0.779）和风险评估（F1=0.907）方面表现出色，通过少量样本和微调有所改善。情绪状态识别更具挑战性（最大F1=0.709），可能是由于失去了语音线索和模糊性。经过微调的1.5B参数模型（Qwen2.5-1.5B）在情绪和自杀意念方面超过了更大的模型。开源模型如QwQ-32B在大多数任务上表现与闭源模型相当（p>0.3），尽管闭源模型在情绪检测方面仍具有优势（p=0.007）。性能随着规模增长到一定程度；量化（AWQ）将GPU内存减少了70%，F1下降最小。LLMs在结构化的心理危机评估中显示出巨大的潜力，特别是在微调后。情绪识别由于上下文复杂性仍然有限。开放和闭源模型之间的差距缩小，加上有效的量化，表明可行的整合。PsyCrisisBench提供了一个强大的评估框架，以指导模型开发和心理健康领域的道德部署。


<details>
  <summary>Details</summary>
Motivation: 心理支持热线在危机干预中扮演重要角色，但需求激增带来了巨大挑战。研究旨在探讨大语言模型（LLMs）在心理危机评估中的应用潜力，特别是在情感敏感场景下的表现。

Method: 构建了一个名为PsyCrisisBench的基准数据集，包含540个来自杭州心理援助热线的标注转录本，用于评估四个任务：情绪状态识别、自杀意念检测、自杀计划识别和风险评估。利用零样本、少量样本和微调方法对15个家族的64个LLMs进行了评估，包括GPT、Claude、Gemini、Llama、Qwen和DeepSeek等模型。采用F1分数作为性能指标，并通过Welch的t检验进行统计比较。

Result: LLMs在自杀意念检测（F1=0.880）、自杀计划识别（F1=0.779）和风险评估（F1=0.907）方面表现出色，且通过少量样本学习和微调进一步提升。然而，情绪状态识别更具挑战性（最大F1=0.709），可能由于缺乏语音线索和模糊性。一个微调后的1.5B参数模型（Qwen2.5-1.5B）在情绪和自杀意念检测上超越了更大规模的模型。开源模型如QwQ-32B在多数任务上的表现与闭源模型相当（p>0.3），尽管闭源模型在情绪检测方面略占优势（p=0.007）。此外，量化技术（AWQ）显著减少了GPU内存占用（减少70%），同时对F1分数影响极小。

Conclusion: 大语言模型在结构化心理危机评估中展现出巨大潜力，特别是通过微调可以显著提高性能。情绪识别仍然是一个难点，主要是因为上下文的复杂性和非言语线索的缺失。开放源码和闭源模型之间的性能差距正在缩小，结合高效的量化技术，为实际应用提供了可行性。PsyCrisisBench提供了一个稳健的评估框架，有助于指导模型开发及在心理健康领域中的伦理部署。

Abstract: Psychological support hotlines are critical for crisis intervention but face
significant challenges due to rising demand. Large language models (LLMs) could
support crisis assessments, yet their capabilities in emotionally sensitive
contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540
annotated transcripts from the Hangzhou Psychological Assistance Hotline,
assessing four tasks: mood status recognition, suicidal ideation detection,
suicide plan identification, and risk assessment. We evaluated 64 LLMs across
15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,
few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with
statistical comparisons via Welch's t-tests. LLMs performed strongly on
suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),
and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood
status recognition was more challenging (max F1=0.709), likely due to lost
vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)
surpassed larger models on mood and suicidal ideation. Open-source models like
QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though
closed models retained an edge in mood detection (p=0.007). Performance scaled
with size up to a point; quantization (AWQ) reduced GPU memory by 70% with
minimal F1 degradation. LLMs show substantial promise in structured
psychological crisis assessments, especially with fine-tuning. Mood recognition
remains limited due to contextual complexity. The narrowing gap between open-
and closed-source models, combined with efficient quantization, suggests
feasible integration. PsyCrisisBench offers a robust evaluation framework to
guide model development and ethical deployment in mental health.

</details>


### [554] [KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors](https://arxiv.org/abs/2506.01357)
*Zhiyang Qi,Takumasa Kaneko,Keiko Takamizo,Mariko Ukiyo,Michimasa Inaba*

Main category: cs.CL

TL;DR: 研究人员通过让经过训练的咨询师模拟咨询者-客户互动，创建了一个名为KokoroChat的日语心理辅导对话数据集。该数据集包含6589个长篇对话和详细的客户反馈。实验表明，使用KokoroChat对开源大语言模型进行微调，可以提高生成的咨询响应的质量和咨询对话的自动评估。


<details>
  <summary>Details</summary>
Motivation: 生成心理辅导反应的语言模型需要高质量的数据集。然而，众包数据收集方法需要严格的工人培训，而来自真实辅导环境的数据可能会引发隐私和道德问题。此外，现有的大型语言模型生成的数据在多样性和真实性方面存在局限性。

Method: 采用角色扮演的方法，由受过训练的咨询师模拟咨询者-客户互动，以确保高质量的对话同时减少隐私风险。利用这种方法构建了一个名为KokoroChat的日语心理辅导对话数据集，其中包含6589个长篇对话和详细的客户反馈。

Result: 实验结果表明，使用KokoroChat对开源大语言模型进行微调，可以提高生成的心理辅导反应的质量和心理辅导对话的自动评估。

Conclusion: KokoroChat数据集提供了一种新的方法来生成高质量的心理辅导对话数据，同时减少了隐私和伦理问题的风险。

Abstract: Generating psychological counseling responses with language models relies
heavily on high-quality datasets. Crowdsourced data collection methods require
strict worker training, and data from real-world counseling environments may
raise privacy and ethical concerns. While recent studies have explored using
large language models (LLMs) to augment psychological counseling dialogue
datasets, the resulting data often suffers from limited diversity and
authenticity. To address these limitations, this study adopts a role-playing
approach where trained counselors simulate counselor-client interactions,
ensuring high-quality dialogues while mitigating privacy risks. Using this
method, we construct KokoroChat, a Japanese psychological counseling dialogue
dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive
client feedback. Experimental results demonstrate that fine-tuning open-source
LLMs with KokoroChat improves both the quality of generated counseling
responses and the automatic evaluation of counseling dialogues. The KokoroChat
dataset is available at https://github.com/UEC-InabaLab/KokoroChat.

</details>


### [555] [zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression](https://arxiv.org/abs/2506.01084)
*Saibo Geng,Nathan Ranchin,Yunzhen yao,Maxime Peyrard,Chris Wendler,Michael Gastpar,Robert West*

Main category: cs.CL

TL;DR: Tokenization efficiency is crucial for large language models (LLMs). Most models use static tokenizers which may not adapt well to specific inputs, causing longer token sequences and higher costs. This paper introduces zip2zip, a framework allowing LLMs to dynamically adjust token vocabulary at inference time via three key components: LZW-based tokenizer, an embedding layer, and a causal language modeling variant. Through parameter-efficient finetuning in 10 GPU-hours, the resulting LLMs reduce input/output sequence length by 20-60%, improving inference latency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static tokenizers that fail to adapt to domain- or language-specific inputs, leading to inefficiencies in tokenization and increased computational costs.

Method: The method involves developing zip2zip, a framework with three components: an LZW-based tokenizer that creates reusable 'hypertokens', an embedding layer for computing embeddings of new hypertokens, and a causal language modeling variant for training on compressed sequences. The model is finetuned in a parameter-efficient manner.

Result: Existing LLMs can be adapted to the zip2zip framework within 10 GPU-hours. The adapted models effectively utilize hypertokens during inference, reducing sequence lengths by 20-60% and significantly improving inference latency.

Conclusion: The zip2zip framework demonstrates the potential for dynamic token adjustment in LLMs, offering more efficient tokenization and faster inference, particularly beneficial for domain- or language-specific tasks.

Abstract: Tokenization efficiency plays a critical role in the performance and cost of
large language models (LLMs), yet most models rely on static tokenizers
optimized for general-purpose corpora. These tokenizers' fixed vocabularies
often fail to adapt to domain- or language-specific inputs, leading to longer
token sequences and higher computational costs. We introduce zip2zip, a
framework that enables LLMs to dynamically adjust token vocabulary at inference
time, allowing for fewer generated tokens and thus faster inference. zip2zip
consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch
(LZW) compression that incrementally compresses tokens into reusable
"hypertokens" on the fly; (2) an embedding layer that computes embeddings for
newly formed hypertokens at runtime; and (3) a causal language modeling variant
that trains the model to operate on hypertokenized, compressed sequences. We
show that an existing LLM can be zip2zip-fied in 10 GPU-hours via
parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to
use hypertokens at inference time, reducing input and output sequence length by
20-60\%, with significant improvements in inference latency.

</details>


### [556] [A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition](https://arxiv.org/abs/2506.01147)
*Prerak Srivastava,Giulio Corallo,Sergey Rybalko*

Main category: cs.CL

TL;DR: A new character-level log parser with a novel neural architecture for better log template extraction is proposed, matching LLM-based parsers in accuracy and beating semantic parsers in efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing log parsers fail to capture fine-grained details, resulting in lower accuracy and utility in downstream tasks.

Method: Propose a character-level log parser using a new neural architecture that aggregates character embeddings and estimates binary-coded decimals for detailed log template extraction.

Result: The parser performs as accurately as LLM-based parsers and more efficiently than semantic parsers on revised Loghub-2k and a manual industrial dataset.

Conclusion: This low-resource parser offers an efficient and accurate solution for log parsing tasks.

Abstract: System-generated logs are typically converted into categorical log templates
through parsing. These templates are crucial for generating actionable insights
in various downstream tasks. However, existing parsers often fail to capture
fine-grained template details, leading to suboptimal accuracy and reduced
utility in downstream tasks requiring precise pattern identification. We
propose a character-level log parser utilizing a novel neural architecture that
aggregates character embeddings. Our approach estimates a sequence of
binary-coded decimals to achieve highly granular log templates extraction. Our
low-resource character-level parser, tested on revised Loghub-2k and a manually
annotated industrial dataset, matches LLM-based parsers in accuracy while
outperforming semantic parsers in efficiency.

</details>


### [557] [Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers](https://arxiv.org/abs/2506.01215)
*Woomin Song,Sai Muralidhar Jayanthi,Srikanth Ronanki,Kanthashree Mysore Sathyendra,Jinwoo Shin,Aram Galstyan,Shubham Katiyar,Sravan Babu Bodapati*

Main category: cs.CL

TL;DR: The paper presents REFORM, a novel two-phase inference framework for handling long contexts in large language models. It achieves significant performance gains and reduces inference time and memory usage compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods for processing long contexts in large language models either struggle with information preservation or require substantial memory resources.

Method: REFORM uses a two-phase approach: 1) incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy; 2) identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache.

Result: REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length, outperforms baselines on Infinite-Bench and MM-NIAH, reduces inference time by 30% and peak memory usage by 5%.

Conclusion: REFORM efficiently handles long contexts, achieving both superior performance and efficiency.

Abstract: As large language models increasingly gain popularity in real-world
applications, processing extremely long contexts, often exceeding the model's
pre-trained context limits, has emerged as a critical challenge. While existing
approaches to efficient long-context processing show promise, recurrent
compression-based methods struggle with information preservation, whereas
random access approaches require substantial memory resources. We introduce
REFORM, a novel inference framework that efficiently handles long contexts
through a two-phase approach. First, it incrementally processes input chunks
while maintaining a compressed KV cache, constructs cross-layer context
embeddings, and utilizes early exit strategy for improved efficiency. Second,
it identifies and gathers essential tokens via similarity matching and
selectively recomputes the KV cache. Compared to baselines, REFORM achieves
over 50% and 27% performance gains on RULER and BABILong respectively at 1M
context length. It also outperforms baselines on Infinite-Bench and MM-NIAH,
demonstrating flexibility across diverse tasks and domains. Additionally,
REFORM reduces inference time by 30% and peak memory usage by 5%, achieving
both efficiency and superior performance.

</details>


### [558] [Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes](https://arxiv.org/abs/2506.01512)
*Meng Li,Michael Vrazitulis,David Schlangen*

Main category: cs.CL

TL;DR: Rational speakers can match expressions to evidence strength, while large language models (LLMs) struggle with generating reliable expressions of uncertainty. This paper evaluates LLMs' knowledge of epistemic modality using controlled stories and finds their performance limited and not robust, suggesting a need to enrich LLMs' semantic knowledge of epistemic modality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge faced by current large language models in generating corresponding utterances based on the assessment of facts and confidence in uncertain real-world environments, and to examine the linguistic knowledge of uncertainty encoded in the latent space of LLMs.

Method: Drawing on typological frameworks of epistemic expressions, the paper evaluates LLMs' knowledge of epistemic modality using controlled stories.

Result: The experiments reveal that LLMs' performance in generating epistemic expressions is limited and not robust, leading to unreliable expressions of uncertainty.

Conclusion: It is necessary to enrich the semantic knowledge of epistemic modality in LLMs to build uncertainty-aware models.

Abstract: Rational speakers are supposed to know what they know and what they do not
know, and to generate expressions matching the strength of evidence. In
contrast, it is still a challenge for current large language models to generate
corresponding utterances based on the assessment of facts and confidence in an
uncertain real-world environment. While it has recently become popular to
estimate and calibrate confidence of LLMs with verbalized uncertainty, what is
lacking is a careful examination of the linguistic knowledge of uncertainty
encoded in the latent space of LLMs. In this paper, we draw on typological
frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic
modality, using controlled stories. Our experiments show that the performance
of LLMs in generating epistemic expressions is limited and not robust, and
hence the expressions of uncertainty generated by LLMs are not always reliable.
To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge
of epistemic modality in LLMs.

</details>


### [559] [V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat](https://arxiv.org/abs/2506.01524)
*Qi Lin,Weikai Xu,Lisi Chen,Bin Dai*

Main category: cs.CL

TL;DR: 提出V-VAE框架和HumanChatData数据集，以实现更符合人类特征的对话系统。


<details>
  <summary>Details</summary>
Motivation: 现有的角色扮演和基于人格的聊天方法依赖于静态角色描述、粗粒度信号空间和低质量合成数据，无法捕捉人类对话中动态细粒度细节。

Method: 提出Verbal Variational Auto-Encoding (V-VAE)框架，包含变分自编码模块和细粒度控制空间，通过可解释的潜在变量动态调整对话行为。同时构建高质量数据集HumanChatData和基准测试HumanChatBench。

Result: 实验表明，基于V-VAE的LLMs在HumanChatBench和DialogBench上持续优于标准基线，证明了V-VAE和HumanChatData的有效性。

Conclusion: V-VAE框架和HumanChatData数据集可以显著提高对话系统的性能，使其更加符合人类特征。

Abstract: With the continued proliferation of Large Language Model (LLM) based
chatbots, there is a growing demand for generating responses that are not only
linguistically fluent but also consistently aligned with persona-specific
traits in conversations. However, existing role-play and persona-based chat
approaches rely heavily on static role descriptions, coarse-grained signal
space, and low-quality synthetic data, which fail to capture dynamic
fine-grained details in human-like chat. Human-like chat requires modeling
subtle latent traits, such as emotional tone, situational awareness, and
evolving personality, which are difficult to predefine and cannot be easily
learned from synthetic or distillation-based data. To address these
limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework,
containing a variational auto-encoding module and fine-grained control space
which dynamically adapts dialogue behaviour based on fine-grained,
interpretable latent variables across talking style, interaction patterns, and
personal attributes. We also construct a high-quality dataset, HumanChatData,
and benchmark HumanChatBench to address the scarcity of high-quality data in
the human-like domain. Experiments show that LLMs based on V-VAE consistently
outperform standard baselines on HumanChatBench and DialogBench, which further
demonstrates the effectiveness of V-VAE and HumanChatData.

</details>


### [560] [Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries](https://arxiv.org/abs/2506.01535)
*Haruki Sakajo,Yusuke Ide,Justin Vasselli,Yusuke Sakai,Yingtao Tian,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: The paper presents a new method for cross-lingual vocabulary transfer using bilingual dictionaries which works effectively for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Current methods face challenges when applied to languages with limited resources.

Method: The proposed method uses bilingual dictionaries and leverages a property of BPE tokenizers where removing a subword from the vocabulary causes a fallback to shorter subwords. Embeddings of target subwords are estimated iteratively.

Result: The experimental results show that this approach outperforms existing methods for low-resource languages.

Conclusion: A dictionary-based approach for cross-lingual vocabulary transfer is effective.

Abstract: Cross-lingual vocabulary transfer plays a promising role in adapting
pre-trained language models to new languages, including low-resource languages.
Existing approaches that utilize monolingual or parallel corpora face
challenges when applied to languages with limited resources. In this work, we
propose a simple yet effective vocabulary transfer method that utilizes
bilingual dictionaries, which are available for many languages, thanks to
descriptive linguists. Our proposed method leverages a property of BPE
tokenizers where removing a subword from the vocabulary causes a fallback to
shorter subwords. The embeddings of target subwords are estimated iteratively
by progressively removing them from the tokenizer. The experimental results
show that our approach outperforms existing methods for low-resource languages,
demonstrating the effectiveness of a dictionary-based approach for
cross-lingual vocabulary transfer.

</details>


### [561] [A Platform for Investigating Public Health Content with Efficient Concern Classification](https://arxiv.org/abs/2506.01308)
*Christopher Li,Rickard Stureborg,Bhuwan Dhingra,Jun Yang*

Main category: cs.CL

TL;DR: The paper introduces ConcernScope, a platform using a teacher-student framework for knowledge transfer to identify health concerns in text data. Applications include data exploration, trend identification, and analyzing topic frequency changes around significant events.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding online content related to public health concerns and effectively responding to it, which has contributed to stalled uptake of preventive measures globally.

Method: ConcernScope uses a teacher-student framework for knowledge transfer between large language models and light-weight classifiers. The platform supports massive file uploads, URL scraping, and direct text editing based on a taxonomy of public health concerns.

Result: Demonstrates guided data exploration, identification of trends through time series analysis of 186,000 samples, and finding trends in topic frequency before and after significant events.

Conclusion: ConcernScope is intended for public health officials to help understand and respond to public health concerns expressed in online content.

Abstract: A recent rise in online content expressing concerns with public health
initiatives has contributed to already stalled uptake of preemptive measures
globally. Future public health efforts must attempt to understand such content,
what concerns it may raise among readers, and how to effectively respond to it.
To this end, we present ConcernScope, a platform that uses a teacher-student
framework for knowledge transfer between large language models and light-weight
classifiers to quickly and effectively identify the health concerns raised in a
text corpus. The platform allows uploading massive files directly,
automatically scraping specific URLs, and direct text editing. ConcernScope is
built on top of a taxonomy of public health concerns. Intended for public
health officials, we demonstrate several applications of this platform: guided
data exploration to find useful examples of common concerns found in online
community datasets, identification of trends in concerns through an example
time series analysis of 186,000 samples, and finding trends in topic frequency
before and after significant events.

</details>


### [562] [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://arxiv.org/abs/2506.01347)
*Xinyu Zhu,Mengzhou Xia,Zhepei Wei,Wei-Lin Chen,Danqi Chen,Yu Meng*

Main category: cs.CL

TL;DR: Reinforcement learning with verifiable rewards (RLVR) is effective for training language models on reasoning tasks. Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients.


<details>
  <summary>Details</summary>
Motivation: To better understand the mechanism of RLVR, the authors decompose the learning signal into reinforcing correct responses (PSR) and penalizing incorrect ones (NSR).

Method: The authors train Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset using only negative samples (NSR) without reinforcing correct responses.

Result: Training with only negative samples consistently improves performance over the base model across the entire Pass@$k$ spectrum ($k$ up to $256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@$1$ but degrades performance at higher $k$ due to reduced diversity.

Conclusion: Penalizing incorrect responses may contribute more to performance than previously recognized. The authors propose a variant of the RL objective that upweights NSR, improving overall Pass@$k$ performance on various datasets.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training language models (LMs) on reasoning tasks that elicit emergent long
chains of thought (CoTs). Unlike supervised learning, it updates the model
using both correct and incorrect samples via policy gradients. To better
understand its mechanism, we decompose the learning signal into reinforcing
correct responses and penalizing incorrect ones, referred to as Positive and
Negative Sample Reinforcement (PSR and NSR), respectively. We train
Qwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a
surprising result: training with only negative samples -- without reinforcing
correct responses -- can be highly effective: it consistently improves
performance over the base model across the entire Pass@$k$ spectrum ($k$ up to
$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing
only correct responses improves Pass@$1$ but degrades performance at higher
$k$, due to reduced diversity. These inference-scaling trends highlight that
solely penalizing incorrect responses may contribute more to performance than
previously recognized. Through gradient analysis, we show that NSR works by
suppressing incorrect generations and redistributing probability mass toward
other plausible candidates, guided by the model's prior beliefs. It refines the
model's existing knowledge rather than introducing entirely new behaviors.
Building on this insight, we propose a simple variant of the RL objective that
upweights NSR, and show that it consistently improves overall Pass@$k$
performance on MATH, AIME 2025, and AMC23. Our code is available at
https://github.com/TianHongZXY/RLVR-Decomposed.

</details>


### [563] [Self-Refining Language Model Anonymizers via Adversarial Distillation](https://arxiv.org/abs/2506.01420)
*Kyuyoung Kim,Hyunjun Jeon,Jinwoo Shin*

Main category: cs.CL

TL;DR: SEAL是一种新的框架，用于训练小型语言模型进行匿名化处理，无需依赖外部昂贵模型。通过对抗性蒸馏框架，SLMs在隐私保护和效用之间取得了与GPT-4相当的平衡，并且在自我优化后甚至超过了GPT-4的隐私保护性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在敏感领域的应用带来了隐私风险。现有的基于LLM的匿名化方法虽然有所帮助，但通常依赖专有模型，这引发了成本和数据安全的担忧。因此，需要一种不依赖外部昂贵模型的匿名化解决方案。

Method: 提出了一种名为SEAL的对抗性蒸馏框架，利用LLM匿名化器和推理模型之间的对抗交互，收集匿名文本和推断属性的轨迹，通过监督微调和偏好学习将这些能力蒸馏到小型语言模型中。生成的模型能够匿名化文本并批评其输出，从而通过自我优化迭代提高匿名化质量。

Result: 在SynthPAI数据集上的实验表明，使用SEAL训练的SLMs显著提高了匿名化能力。特别是8B模型达到了与GPT-4匿名化器相当的隐私-效用权衡，并且在自我优化后在隐私保护方面超过了GPT-4。

Conclusion: SEAL框架有效地训练了SLMs作为高效的匿名化器，能够在不需要外部昂贵模型的情况下实现良好的匿名化效果。研究者们还发布了完整的实验数据集以促进进一步的研究。

Abstract: Large language models (LLMs) are increasingly used in sensitive domains,
where their ability to infer personal data from seemingly benign text poses
emerging privacy risks. While recent LLM-based anonymization methods help
mitigate such risks, they often rely on proprietary models (e.g., GPT-4),
raising concerns about cost and the potential exposure of sensitive data to
untrusted external systems. To address this, we introduce SElf-refining
Anonymization with Language model (SEAL), a novel distillation framework for
training small language models (SLMs) to perform effective anonymization
without relying on external costly models at inference time. We leverage
adversarial interactions between an LLM anonymizer and an inference model to
collect trajectories of anonymized texts and inferred attributes, which are
used to distill anonymization, adversarial inference, and utility evaluation
capabilities into SLMs via supervised fine-tuning and preference learning. The
resulting models learn to both anonymize text and critique their outputs,
enabling iterative improvement of anonymization quality via self-refinement.
Experiments on SynthPAI, a dataset of synthetic personal profiles and text
comments, demonstrate that SLMs trained with SEAL achieve substantial
improvements in anonymization capabilities. Notably, 8B models attain a
privacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with
self-refinement, even surpass it in terms of privacy. These results show the
effectiveness of our adversarial distillation framework in training SLMs as
efficient anonymizers. To facilitate further research, we release the full
dataset used in our experiments.

</details>


### [564] [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)
*Chaoyue He,Xin Zhou,Yi Wu,Xinjia Yu,Yan Zhang,Lei Zhang,Di Wang,Shengfei Lyu,Hong Xu,Xiaoqiao Wang,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: The paper introduces ESGenius, a benchmark for assessing LLMs in ESG and sustainability question answering. It includes ESGenius-QA with 1136 questions and ESGenius-Corpus with 231 documents. Experiments show moderate zero-shot performance but significant improvement with RAG.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve LLMs' proficiency in ESG and sustainability-related question answering.

Method: ESGenius consists of ESGenius-QA (multiple-choice questions) and ESGenius-Corpus (curated documents). A two-stage evaluation protocol (zero-shot and RAG) is used to assess LLMs.

Result: State-of-the-art models achieve moderate zero-shot accuracy (55-70%), but RAG significantly improves performance, especially for smaller models.

Conclusion: ESGenius highlights the need for grounding responses in authoritative sources for better ESG understanding and is the first benchmark focusing on this domain for LLMs.

Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing
the proficiency of Large Language Models (LLMs) in Environmental, Social and
Governance (ESG) and sustainability-focused question answering. ESGenius
comprises two key components: (i) ESGenius-QA, a collection of 1 136
multiple-choice questions generated by LLMs and rigorously validated by domain
experts, covering a broad range of ESG pillars and sustainability topics. Each
question is systematically linked to its corresponding source text, enabling
transparent evaluation and supporting retrieval-augmented generation (RAG)
methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231
foundational frameworks, standards, reports and recommendation documents from
seven authoritative sources. Moreover, to fully assess the capabilities and
adaptation potential of the model, we implement a rigorous two-stage evaluation
protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging
from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models
achieve only moderate performance in zero-shot settings, with accuracies
typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs
in interdisciplinary contexts. However, models employing RAG show significant
performance improvements, particularly for smaller models. For example,
"DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\%
with RAG. These results underscore the necessity of grounding responses in
authoritative sources for enhanced ESG understanding. To the best of our
knowledge, ESGenius is the first benchmark curated for LLMs and the relevant
enhancement technologies that focuses on ESG and sustainability topics.

</details>


### [565] [MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
*Kensuke Mitsuzawa*

Main category: cs.CL

TL;DR: This paper introduces MMD-Sense-Analysis, a new method using Maximum Mean Discrepancy (MMD) for detecting and explaining shifts in word meanings over time, showing its effectiveness through empirical results.


<details>
  <summary>Details</summary>
Motivation: To identify and interpret shifts in word meanings over time, which is essential for understanding linguistic and social backgrounds.

Method: Proposes MMD-Sense-Analysis that uses Maximum Mean Discrepancy (MMD) to select semantically meaningful variables and quantify changes across time periods.

Result: Empirical assessment results demonstrate the effectiveness of MMD-Sense-Analysis in detecting word sense changes.

Conclusion: MMD-Sense-Analysis is an effective approach for word sense change detection, marking the first application of MMD in this field.

Abstract: Word sense analysis is an essential analysis work for interpreting the
linguistic and social backgrounds. The word sense change detection is a task of
identifying and interpreting shifts in word meanings over time. This paper
proposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean
Discrepancy (MMD) to select semantically meaningful variables and quantify
changes across time periods. This method enables both the identification of
words undergoing sense shifts and the explanation of their evolution over
multiple historical periods. To my knowledge, this is the first application of
MMD to word sense change detection. Empirical assessment results demonstrate
the effectiveness of the proposed approach.

</details>


### [566] [When LLMs Team Up: The Emergence of Collaborative Affective Computing](https://arxiv.org/abs/2506.01698)
*Wenna Lai,Haoran Xie,Guandong Xu,Qing Li,S. Joe Qin*

Main category: cs.CL

TL;DR: The paper explores Large Language Model (LLM)-based collaboration systems in Affective Computing (AC) to overcome limitations in affective reasoning, reviewing methods, comparing strategies, analyzing potential, and discussing challenges for future advancements.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional pipeline architectures in AC tasks and the cognitive limitations of LLMs in affective reasoning, such as misinterpreting cultural nuances or contextual emotions.

Method: Systematic review of LLM-based collaboration systems in AC including collaboration strategies, mechanisms, key functions, applications, experimental comparisons across representative tasks, and analysis of system potential.

Result: Demonstrates the potential of LLM-based collaboration systems to enhance robustness and adaptability in complex affective reasoning.

Conclusion: This work is the first systematic exploration of collaborative intelligence with LLMs in AC, paving the way for more powerful applications approaching human-like social intelligence.

Abstract: Affective Computing (AC) is essential in bridging the gap between human
emotional experiences and machine understanding. Traditionally, AC tasks in
natural language processing (NLP) have been approached through pipeline
architectures, which often suffer from structure rigidity that leads to
inefficiencies and limited adaptability. The advent of Large Language Models
(LLMs) has revolutionized this field by offering a unified approach to
affective understanding and generation tasks, enhancing the potential for
dynamic, real-time interactions. However, LLMs face cognitive limitations in
affective reasoning, such as misinterpreting cultural nuances or contextual
emotions, and hallucination problems in decision-making. To address these
challenges, recent research advocates for LLM-based collaboration systems that
emphasize interactions among specialized models and LLMs, mimicking human-like
affective intelligence through the synergy of emotional and rational thinking
that aligns with Dual Process Theory in psychology. This survey aims to provide
a comprehensive overview of LLM-based collaboration systems in AC, exploring
from structured collaborations to autonomous collaborations. Specifically, it
includes: (1) A systematic review of existing methods, focusing on
collaboration strategies, mechanisms, key functions, and applications; (2)
Experimental comparisons of collaboration strategies across representative
tasks in affective understanding and generation; (3) An analysis highlighting
the potential of these systems to enhance robustness and adaptability in
complex affective reasoning; (4) A discussion of key challenges and future
research directions to further advance the field. This work is the first to
systematically explore collaborative intelligence with LLMs in AC, paving the
way for more powerful applications that approach human-like social
intelligence.

</details>


### [567] [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
*Soyoung Oh,Xinting Huang,Mathis Pink,Michael Hahn,Vera Demberg*

Main category: cs.CL

TL;DR: This paper explores how a pretrained transformer processes idioms by identifying specific attention heads that enhance figurative meanings while suppressing literal ones, revealing the model's dual processing pathway for idiomatic expressions.


<details>
  <summary>Details</summary>
Motivation: To understand how large pretrained transformers handle the ambiguity of idioms, specifically their ability to interpret idioms figuratively or literally.

Method: Using mechanistic interpretability tools, researchers localized three steps in idiom processing within LLama3.2-1B-base, focusing on early attention and MLP sublayers.

Result: Specific attention heads were found to boost figurative meanings and suppress literal interpretations, with the model maintaining both readings via an intermediate path and a parallel bypass route.

Conclusion: The study provides mechanistic evidence for idiom comprehension in autoregressive transformers.

Abstract: Idioms present a unique challenge for language models due to their
non-compositional figurative meanings, which often strongly diverge from the
idiom's literal interpretation. This duality requires a model to learn
representing and deciding between the two meanings to interpret an idiom in a
figurative sense, or literally. In this paper, we employ tools from mechanistic
interpretability to trace how a large pretrained causal transformer
(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom
processing: First, the idiom's figurative meaning is retrieved in early
attention and MLP sublayers. We identify specific attention heads which boost
the figurative meaning of the idiom while suppressing the idiom's literal
interpretation. The model subsequently represents the figurative representation
through an intermediate path. Meanwhile, a parallel bypass route forwards
literal interpretation, ensuring that a both reading remain available. Overall,
our findings provide a mechanistic evidence for idiom comprehension in an
autoregressive transformer.

</details>


### [568] [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/abs/2506.01776)
*Yile Liu,Ziwei Ma,Xiu Jiang,Jinglu Hu,Jing Chang,Liang Li*

Main category: cs.CL

TL;DR: 开发了MaXIFE，一个包含23种语言和1667个可验证指令任务的全面评估基准，用于评估多语言环境下的指令遵循能力，并通过规则和模型两种评估方法，为多个LLM建立了基线结果。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法通常关注单一语言场景，忽略了多语言和跨语言环境中的挑战与差异。为了填补这一空白，需要一个能够评估跨语言指令遵循能力的工具。

Method: 创建了一个名为MaXIFE的综合评估基准，涵盖23种语言和1667个可验证指令任务，结合了基于规则的评估和基于模型的评估，以确保效率和准确性。

Result: 使用MaXIFE对多个领先的商业和开源大语言模型进行了评估，并为未来的比较提供了基线结果。

Conclusion: MaXIFE作为一个标准化的多语言指令遵循评估工具，有助于推动自然语言处理领域的研究和发展。

Abstract: With the rapid adoption of large language models (LLMs) in natural language
processing, the ability to follow instructions has emerged as a key metric for
evaluating their practical utility. However, existing evaluation methods often
focus on single-language scenarios, overlooking the challenges and differences
present in multilingual and cross-lingual contexts. To address this gap, we
introduce MaXIFE: a comprehensive evaluation benchmark designed to assess
instruction-following capabilities across 23 languages with 1,667 verifiable
instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based
Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to
evaluate several leading commercial and open-source LLMs, establishing baseline
results for future comparisons. By providing a standardized tool for
multilingual instruction-following evaluation, MaXIFE aims to advance research
and development in natural language processing.

</details>


### [569] [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
*Shuai Wang,Yinan Yu*

Main category: cs.CL

TL;DR: iQUEST is a KBQA framework that breaks down complex queries into simpler sub-questions and uses a GNN to incorporate 2-hop neighbor information, improving multi-hop reasoning with external knowledge resources.


<details>
  <summary>Details</summary>
Motivation: LLMs often face factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources like KGs can provide a more reliable reasoning foundation.

Method: The iQUEST framework decomposes complex queries into simpler sub-questions and integrates a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step.

Result: Experiments show consistent improvement by iQUEST across four benchmark datasets and four LLMs.

Conclusion: iQUEST addresses the challenges of coherent reasoning paths and avoiding premature discarding of critical connections in multi-hop reasoning.

Abstract: While Large Language Models (LLMs) excel at many natural language processing
tasks, they often suffer from factual inaccuracies in knowledge-intensive
scenarios. Integrating external knowledge resources, particularly knowledge
graphs (KGs), provides a transparent and updatable foundation for more reliable
reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons
over KGs, is central to this effort, especially for complex, multi-hop queries.
However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent
reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop
connections. To address these issues, we introduce iQUEST, a question-guided
KBQA framework that iteratively decomposes complex queries into simpler
sub-questions, ensuring a structured and focused reasoning trajectory.
Additionally, we integrate a Graph Neural Network (GNN) to look ahead and
incorporate 2-hop neighbor information at each reasoning step. This dual
approach strengthens the reasoning process, enabling the model to explore
viable paths more effectively. Detailed experiments demonstrate the consistent
improvement delivered by iQUEST across four benchmark datasets and four LLMs.

</details>


### [570] [CiteEval: Principle-Driven Citation Evaluation for Source Attribution](https://arxiv.org/abs/2506.01829)
*Yumo Xu,Peng Qi,Jifan Chen,Kunlun Liu,Rujun Han,Lan Liu,Bonan Min,Vittorio Castelli,Arshit Gupta,Zhiguo Wang*

Main category: cs.CL

TL;DR: Citation quality is crucial in information-seeking systems, but current evaluation frameworks are suboptimal. This work introduces CiteEval, a new framework for fine-grained citation assessment, along with CiteBench and CiteEval-Auto for efficient evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve the evaluation of citation quality in information-seeking systems, as existing methods relying on Natural Language Inference (NLI) are deemed insufficient for assessing the true supportiveness of citations.

Method: The method involves introducing CiteEval, a framework for fine-grained citation assessment considering broader contexts such as cited sources, user queries, and generated text. Additionally, CiteBench, a multi-domain benchmark with human annotations, is constructed. CiteEval-Auto, model-based metrics showing strong correlation with human judgments, is also developed.

Result: Experiments show that CiteEval-Auto outperforms existing metrics in capturing the multifaceted nature of citations across diverse systems.

Conclusion: CiteEval provides a principled and scalable approach to evaluate and enhance model-generated citations, marking an advancement in citation quality evaluation.

Abstract: Citation quality is crucial in information-seeking systems, directly
influencing trust and the effectiveness of information access. Current
evaluation frameworks, both human and automatic, mainly rely on Natural
Language Inference (NLI) to assess binary or ternary supportiveness from cited
sources, which we argue is a suboptimal proxy for citation evaluation. In this
work we introduce CiteEval, a citation evaluation framework driven by
principles focusing on fine-grained citation assessment within a broad context,
encompassing not only the cited sources but the full retrieval context, user
query, and generated text. Guided by the proposed framework, we construct
CiteBench, a multi-domain benchmark with high-quality human annotations on
citation quality. To enable efficient evaluation, we further develop
CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation
with human judgments. Experiments across diverse systems demonstrate
CiteEval-Auto's superior ability to capture the multifaceted nature of
citations compared to existing metrics, offering a principled and scalable
approach to evaluate and improve model-generated citations.

</details>


### [571] [Esoteric Language Models](https://arxiv.org/abs/2506.01928)
*Subham Sekhar Sahoo,Zhihan Yang,Yash Akhauri,Johnna Liu,Deepansha Singh,Zhoujun Cheng,Zhengzhong Liu,Eric Xing,John Thickstun,Arash Vahdat*

Main category: cs.CL

TL;DR: Eso-LMs融合了自回归(AR)和掩码扩散模型(MDM)范式，首次在保留并行生成能力的同时引入MDM的键值缓存(KV caching)，优化采样调度使推理速度比标准MDM快65倍，比之前的半自回归方法快4倍，并且在语言建模基准上达到新水平。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的语言模型（特别是MDM）表现出色，但在困惑度方面仍逊于自回归模型，并且缺乏关键的推理效率特性如KV缓存。为了解决这些限制，同时保持并行生成的优势，作者试图将AR和MDM范式结合起来。

Method: 提出了Eso-LMs模型家族，该模型结合了AR和MDM范式，实现了困惑度的平滑插值，同时克服了各自的局限性。首次为MDM引入了KV缓存，保留了并行生成的能力，并采用优化的采样调度以提高推理效率。

Result: Eso-LMs在标准语言建模基准上达到了新的最佳性能；与标准MDM相比，推理速度快65倍，比之前的半自回归方法快4倍。

Conclusion: Eso-LMs通过融合AR和MDM范式，在提升性能的同时显著提高了推理效率，是首个在保留并行生成能力的情况下为MDM引入KV缓存的方法。

Abstract: Diffusion-based language models offer a compelling alternative to
autoregressive (AR) models by enabling parallel and controllable generation.
Among this family of models, Masked Diffusion Models (MDMs) achieve the
strongest performance but still underperform AR models in perplexity and lack
key inference-time efficiency features--most notably, KV caching. In this work,
we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,
enabling smooth interpolation between their perplexities while overcoming their
respective limitations. Eso-LMs set a new state of the art on standard language
modeling benchmarks. Crucially, we are the **first to introduce KV caching for
MDMs** while preserving parallel generation, significantly improving inference
efficiency. Combined with an optimized sampling schedule, our method achieves
up to **65x** faster inference than standard MDMs and **4x** faster inference
than prior semi-autoregressive approaches. We provide the code and model
checkpoints on the project page:
[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)

</details>


### [572] [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
*Shenzhi Wang,Le Yu,Chang Gao,Chujie Zheng,Shixuan Liu,Rui Lu,Kai Dang,Xionghui Chen,Jianxin Yang,Zhenru Zhang,Yuqiong Liu,An Yang,Andrew Zhao,Yang Yue,Shiji Song,Bowen Yu,Gao Huang,Junyang Lin*

Main category: cs.CL

TL;DR: RLVR is improved by focusing on high-entropy tokens, leading to better performance with fewer updates.


<details>
  <summary>Details</summary>
Motivation: To explore the mechanisms of RLVR in enhancing LLM reasoning capabilities and understand its effects through the lens of token entropy patterns.

Method: Analyze token entropy patterns in Chain-of-Thought reasoning during RLVR training, focusing on high-entropy tokens as critical forks for reasoning pathways.

Result: Restricting policy gradient updates to high-entropy tokens improves RLVR performance significantly, surpassing full-gradient updates on larger base models.

Conclusion: The effectiveness of RLVR mainly comes from optimizing high-entropy tokens that determine reasoning directions; understanding RLVR via token-entropy perspective can further improve LLM reasoning.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful approach to enhancing the reasoning capabilities of Large Language
Models (LLMs), while its mechanisms are not yet well understood. In this work,
we undertake a pioneering exploration of RLVR through the novel perspective of
token entropy patterns, comprehensively analyzing how different tokens
influence reasoning performance. By examining token entropy patterns in
Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of
tokens exhibit high entropy, and these tokens act as critical forks that steer
the model toward diverse reasoning pathways. Furthermore, studying how entropy
patterns evolve during RLVR training reveals that RLVR largely adheres to the
base model's entropy patterns, primarily adjusting the entropy of high-entropy
tokens. These findings highlight the significance of high-entropy tokens (i.e.,
forking tokens) to RLVR. We ultimately improve RLVR by restricting policy
gradient updates to forking tokens and uncover a finding even beyond the 80/20
rule: utilizing only 20% of the tokens while maintaining performance comparable
to full-gradient updates on the Qwen3-8B base model and significantly
surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71
on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,
highlighting a strong scaling trend. In contrast, training exclusively on the
80% lowest-entropy tokens leads to a marked decline in performance. These
findings indicate that the efficacy of RLVR primarily arises from optimizing
the high-entropy tokens that decide reasoning directions. Collectively, our
results highlight the potential to understand RLVR through a token-entropy
perspective and optimize RLVR by leveraging high-entropy minority tokens to
further improve LLM reasoning.

</details>


### [573] [Self-ensemble: Mitigating Confidence Distortion for Large Language Models](https://arxiv.org/abs/2506.01951)
*Zicheng Xu,Guanchu Wang,Guangyao Zheng,Yu-Neng Chuang,Alexander Szalay,Xia Hu,Vladimir Braverman*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) have a confidence distortion problem in multi-choice question-answering, which gets worse with more answer choices. The proposed solution is called Self-ensemble, which divides answer choices into groups and combines LLM predictions to make a final decision.


<details>
  <summary>Details</summary>
Motivation: To solve the confidence distortion problem that Large Language Models (LLMs) experience in multi-choice question-answering tasks, particularly as the number of answer choices increases.

Method: Self-ensemble method splits the choices into several groups and ensembles LLM predictions across these groups to reach a final decision.

Result: Experimental results on three LLMs and datasets show that Self-ensemble effectively addresses the confidence distortion problem of LLMs, outperforming standard inference and baseline methods.

Conclusion: Self-ensemble is a plug-and-play solution that can be integrated into existing LLM architecture without needing labeled datasets for parameter tuning.

Abstract: Although Large Language Models (LLMs) perform well in general fields, they
exhibit a confidence distortion problem on multi-choice question-answering
(MCQA), particularly as the number of answer choices increases. Specifically,
on MCQA with many choices, LLMs suffer from under-confidence in correct
predictions and over-confidence in incorrect ones, leading to a substantially
degraded performance. To solve this problem, we propose Self-ensemble in this
work. Our method splits the choices into several groups and ensembles LLM
predictions across these groups to reach a final decision. The advantage of
Self-ensemble is its plug-and-play nature, where it can be integrated into
existing LLM architecture based on a designed attention mask and positional
encoding, without requiring labeled datasets for parameter tuning. Experimental
results on three LLMs and datasets demonstrate that Self-ensemble
comprehensively addresses the confidence distortion problem of LLMs,
outperforming standard inference as well as baseline methods.

</details>


### [574] [WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks](https://arxiv.org/abs/2506.01952)
*Atsuyuki Miyai,Zaiying Zhao,Kazuki Egashira,Atsuki Sato,Tatsumi Sunada,Shota Onohara,Hiromasa Yamanishi,Mashiro Toyooka,Kunato Nishina,Ryoma Maeda,Kiyoharu Aizawa,Toshihiko Yamasaki*

Main category: cs.CL

TL;DR: This paper presents WebChoreArena, a benchmark expanding WebArena to include tedious and complex tasks. It integrates massive memory, calculation, and long-term memory challenges. Experiments show evolving LLMs like GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro improve performance on WebChoreArena, yet significant room for improvement remains.


<details>
  <summary>Details</summary>
Motivation: To assess if web browsing agents powered by LLMs can handle tedious and complex tasks beyond general browsing.

Method: Introduced WebChoreArena, a benchmark with 532 curated tasks incorporating massive memory, calculation, and long-term memory challenges, built on WebArena simulation environments.

Result: LLMs like GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro showed significant performance improvements on WebChoreArena, but still have substantial room for improvement compared to WebArena.

Conclusion: WebChoreArena effectively measures the advancement of state-of-the-art LLMs and highlights the increased challenges in handling complex and tedious tasks.

Abstract: Powered by a large language model (LLM), a web browsing agent operates web
browsers in a human-like manner and offers a highly transparent path toward
automating a wide range of everyday tasks. As web agents become increasingly
capable and demonstrate proficiency in general browsing tasks, a critical
question emerges: Can they go beyond general browsing to robustly handle tasks
that are tedious and complex, or chores that humans often avoid doing
themselves? In this paper, we introduce WebChoreArena, a new fully reproducible
benchmark comprising 532 carefully curated tasks designed to extend the scope
of WebArena beyond general browsing to more labor-intensive and tedious tasks.
WebChoreArena systematically integrates three key challenges: (i) Massive
Memory tasks requiring accurate retrieval of large amounts of information in
the observations, (ii) Calculation tasks demanding precise mathematical
reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory
across multiple webpages. Built on top of the fully reproducible and widely
adopted four WebArena simulation environments, WebChoreArena ensures strict
reproducibility and enables fair, direct comparisons with the established
WebArena benchmark, offering key insights into agent progress. Our experimental
results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7
Sonnet, and Gemini 2.5 Pro, significant improvements in performance are
observed on WebChoreArena. These findings suggest that WebChoreArena is
well-suited to measure the advancement of state-of-the-art LLMs with greater
clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,
there remains substantial room for improvement compared to WebArena,
highlighting the increased challenges posed by WebChoreArena.

</details>


### [575] [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954)
*Jennifer Chen,Aidar Myrzakhan,Yaxin Luo,Hassaan Muhammad Khan,Sondos Mahmoud Bsharat,Zhiqiang Shen*

Main category: cs.CL

TL;DR: DRAG是一个新的框架，用于将大规模语言模型（LLMs）的知识提炼到小型语言模型（SLMs）中。通过证据和知识图谱蒸馏，DRAG显著减少了模型大小和计算成本，同时提高了事实准确性并减少了幻觉生成。实验表明，DRAG比之前的RAG方法（如MiniRAG）表现更好，效率更高，且有助于缓解用户隐私风险。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的检索增强生成（RAG）方法在确保事实一致性及知识检索方面表现出色，但它们需要大量的计算资源，并容易产生幻觉内容。因此，研究者希望开发一种更高效、更轻量的解决方案来解决这些问题。

Method: DRAG采用了一种新颖的方法，即利用证据和知识图谱进行蒸馏，从而将大型语言模型的知识有效地转移到小型语言模型上。具体来说，该方法通过对齐小型模型预测与结构化知识图谱和排序证据来减少幻觉并提高事实准确性。

Result: 实验结果表明，在多个基准测试中，DRAG的表现优于先前的竞争性RAG方法（如MiniRAG），提升幅度高达27.7%，同时保持了高水平的效率和可靠性。

Conclusion: DRAG为部署增强型检索和生成能力提供了一个实用且资源高效的方案，特别适合小型语言模型。这为未来在资源受限环境下的应用开辟了新途径。

Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for
tasks requiring factual consistency and robust knowledge retrieval. However,
large-scale RAG systems consume significant computational resources and are
prone to generating hallucinated content from Humans. In this work, we
introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from
large-scale Language Models (LLMs) into small LMs (SLMs). Our approach
leverages evidence- and knowledge graph-based distillation, ensuring that the
distilled model retains critical factual knowledge while significantly reducing
model size and computational cost. By aligning the smaller model's predictions
with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$
effectively mitigates hallucinations and improves factual accuracy. We further
present a case demonstrating how our framework mitigates user privacy risks and
introduce a corresponding benchmark. Experimental evaluations on multiple
benchmarks demonstrate that our method outperforms the prior competitive RAG
methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving
high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a
practical and resource-efficient roadmap to deploying enhanced retrieval and
generation capabilities in small-sized LLMs.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [576] [MolTextNet: A Two-Million Molecule-Text Dataset for Multimodal Molecular Learning](https://arxiv.org/abs/2506.00009)
*Yihan Zhu,Gang Liu,Eric Inae,Meng Jiang*

Main category: q-bio.BM

TL;DR: The paper introduces MolTextNet, a large dataset of molecule-text pairs that overcomes previous limitations in scale and informativeness. It is constructed using a synthetic text generation pipeline integrating various molecular features and data, creating structured descriptions for 2.5 million molecules from ChEMBL35. Pretraining multimodal models on MolTextNet improves performance in downstream tasks, showcasing its potential to advance multimodal modeling in molecular science.


<details>
  <summary>Details</summary>
Motivation: Existing molecule-text datasets are limited in scale and informativeness, restricting the training of generalizable multimodal models.

Method: A synthetic text generation pipeline was proposed to construct MolTextNet, which integrates structural features, computed properties, bioactivity data, and synthetic complexity. GPT-4o-mini was used to create structured descriptions for 2.5 million molecules from ChEMBL35.

Result: MolTextNet supports diverse downstream tasks such as property prediction and structure retrieval. Pretraining CLIP-style models with Graph Neural Networks and ModernBERT on MolTextNet yields improved performance.

Conclusion: MolTextNet has the potential to significantly advance foundational multimodal modeling in molecular science.

Abstract: Small molecules are essential to drug discovery, and graph-language models
hold promise for learning molecular properties and functions from text.
However, existing molecule-text datasets are limited in scale and
informativeness, restricting the training of generalizable multimodal models.
We present MolTextNet, a dataset of 2.5 million high-quality molecule-text
pairs designed to overcome these limitations. To construct it, we propose a
synthetic text generation pipeline that integrates structural features,
computed properties, bioactivity data, and synthetic complexity. Using
GPT-4o-mini, we create structured descriptions for 2.5 million molecules from
ChEMBL35, with text over 10 times longer than prior datasets. MolTextNet
supports diverse downstream tasks, including property prediction and structure
retrieval. Pretraining CLIP-style models with Graph Neural Networks and
ModernBERT on MolTextNet yields improved performance, highlighting its
potential for advancing foundational multimodal modeling in molecular science.
Our dataset is available at
https://huggingface.co/datasets/liuganghuggingface/moltextnet.

</details>


### [577] [ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search](https://arxiv.org/abs/2506.00925)
*Mengdi Liu,Xiaoxue Cheng,Zhangyang Gao,Hong Chang,Cheng Tan,Shiguang Shan,Xilin Chen*

Main category: q-bio.BM

TL;DR: 设计能够折叠成目标3D结构的蛋白质序列（即蛋白质逆向折叠）是蛋白质工程中的基本挑战。现有的深度学习方法虽然通过恢复天然序列取得了令人印象深刻的性能，但往往忽略了该问题的一对多性质：多个不同的序列可以折叠成相同的结构。为了解决这个权衡，我们提出了ProtInvTree，这是第一个用于蛋白质逆向折叠的奖励引导树搜索框架。ProtInvTree在多个基准测试中优于最先进的基线模型，生成结构一致且多样化的序列，包括那些远离天然真实值的序列。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法虽然通过恢复天然序列取得了令人印象深刻的性能，但往往忽略了该问题的一对多性质：多个不同的序列可以折叠成相同的结构。

Method: 提出了ProtInvTree，这是一种将序列生成重新定义为深思熟虑、逐步决策制定过程的方法，允许探索多种设计路径，并通过自我评估、前瞻和回溯利用有希望的候选对象。提出了一种两阶段的聚焦和接地动作机制，将位置选择和残基生成解耦。还引入了一种跳跃去噪策略，以有效评估中间状态并避免完整展开。基于预训练的蛋白质语言模型，ProtInvTree支持灵活的测试时间扩展，无需重新训练即可扩展搜索深度和广度。

Result: 在多个基准测试中，ProtInvTree优于最先进的基线模型，生成了结构一致且多样化的序列，包括那些远离天然真实值的序列。

Conclusion: ProtInvTree是一种新颖的奖励引导树搜索框架，适用于蛋白质逆向折叠问题，能够在保持结构一致性的同时设计多样化序列。

Abstract: Designing protein sequences that fold into a target 3D structure, known as
protein inverse folding, is a fundamental challenge in protein engineering.
While recent deep learning methods have achieved impressive performance by
recovering native sequences, they often overlook the one-to-many nature of the
problem: multiple diverse sequences can fold into the same structure. This
motivates the need for a generative model capable of designing diverse
sequences while preserving structural consistency. To address this trade-off,
we introduce ProtInvTree, the first reward-guided tree-search framework for
protein inverse folding. ProtInvTree reformulates sequence generation as a
deliberate, step-wise decision-making process, enabling the exploration of
multiple design paths and exploitation of promising candidates through
self-evaluation, lookahead, and backtracking. We propose a two-stage
focus-and-grounding action mechanism that decouples position selection and
residue generation. To efficiently evaluate intermediate states, we introduce a
jumpy denoising strategy that avoids full rollouts. Built upon pretrained
protein language models, ProtInvTree supports flexible test-time scaling by
expanding the search depth and breadth without retraining. Empirically,
ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,
generating structurally consistent yet diverse sequences, including those far
from the native ground truth.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [578] [How hard is learning to cut? Trade-offs and sample complexity](https://arxiv.org/abs/2506.00252)
*Sammy Khalife,Andrea Lodi*

Main category: math.OC

TL;DR: 本研究提供了用于评估切割平面选择的两个评分函数（分支定界树规模和闭合间隙）的新样本复杂度下界，并通过图神经网络在集合覆盖和设施选址整数规划模型上的实验证明了闭合间隙评分作为减少分支定界树规模的有效代理。这是首次对这两个评分同时进行理论与计算分析的工作。


<details>
  <summary>Details</summary>
Motivation: 近年来，数据驱动方法被广泛应用于优化分支定界算法的不同阶段决策，如分支选择或切割平面选择。对于切割平面选择，已有两种评分函数：分支定界树规模和闭合间隙。然而，关于这些评分函数的学习复杂度及它们之间的关系尚缺乏深入研究。

Method: 1. 提出适用于两类评分函数的新样本复杂度下界，表明学习过程需要的样本数量与从同一类函数中学习通用目标函数类似。
2. 将结果扩展到仅从Simplex表格中选择切割平面的受限情况。
3. 使用图神经网络在集合覆盖和设施选址整数规划模型上进行实验验证，比较闭合间隙评分与分支定界树规模的关系。

Result: 1. 理论上证明了新样本复杂度下界接近已知的神经网络上界。
2. 实验结果表明，闭合间隙评分是减少分支定界树规模的有效代理。
3. 首次对两种评分函数进行了全面的理论与计算分析。

Conclusion: 本文提出了新的样本复杂度下界，揭示了学习切割平面选择问题的难度，并通过实验验证了闭合间隙评分在优化分支定界树规模中的有效性。

Abstract: In the recent years, branch-and-cut algorithms have been the target of
data-driven approaches designed to enhance the decision making in different
phases of the algorithm such as branching, or the choice of cutting planes
(cuts). In particular, for cutting plane selection two score functions have
been proposed in the literature to evaluate the quality of a cut:
branch-and-cut tree size and gap closed. In this paper, we present new sample
complexity lower bounds, valid for both scores. We show that for a wide family
of classes $\mathcal{F}$ that maps an instance to a cut, learning over an
unknown distribution of the instances to minimize those scores requires at
least (up to multiplicative constants) as many samples as learning from the
same class function $\mathcal{F}$ any generic target function (using square
loss). Our results also extend to the case of learning from a restricted set of
cuts, namely those from the Simplex tableau. To the best of our knowledge,
these constitute the first lower bounds for the learning-to-cut framework. We
compare our bounds to known upper bounds in the case of neural networks and
show they are nearly tight. We illustrate our results with a graph neural
network selection evaluated on set covering and facility location integer
programming models and we empirically show that the gap closed score is an
effective proxy to minimize the branch-and-cut tree size. Although the gap
closed score has been extensively used in the integer programming literature,
this is the first principled analysis discussing both scores at the same time
both theoretically and computationally.

</details>


### [579] [An adaptive data sampling strategy for stabilizing dynamical systems via controller inference](https://arxiv.org/abs/2506.01816)
*Steffen W. R. Werner,Benjamin Peherstorfer*

Main category: math.OC

TL;DR: 提出了一种自适应采样方案，可以同时生成数据和稳定系统，从而在数据收集过程中避免不稳定。该方法在边缘情况和极限状态下特别有效。


<details>
  <summary>Details</summary>
Motivation: 学习从数据中控制稳定器是工程应用中的重要任务，但收集信息数据具有挑战性，因为不稳定系统通常会导致迅速增长或不规则的轨迹。

Method: 提出了一种自适应采样方案，在收集数据的同时稳定系统以避免不稳定。在温和假设下，该方法可以生成用于稳定的最小规模的信息数据集。

Result: 数值实验表明，与无指导的数据生成相比，使用新提出的自适应采样方法进行控制器推理可以减少多达一个数量级的数据样本。

Conclusion: 该研究表明，所提出的方法为在容易发生不稳定且数据收集困难的边缘情况和极限状态下稳定系统开辟了道路。

Abstract: Learning stabilizing controllers from data is an important task in
engineering applications; however, collecting informative data is challenging
because unstable systems often lead to rapidly growing or erratic trajectories.
In this work, we propose an adaptive sampling scheme that generates data while
simultaneously stabilizing the system to avoid instabilities during the data
collection. Under mild assumptions, the approach provably generates data sets
that are informative for stabilization and have minimal size. The numerical
experiments demonstrate that controller inference with the novel adaptive
sampling approach learns controllers with up to one order of magnitude fewer
data samples than unguided data generation. The results show that the proposed
approach opens the door to stabilizing systems in edge cases and limit states
where instabilities often occur and data collection is inherently difficult.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [580] [$\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time](https://arxiv.org/abs/2506.00358)
*Sarthak Kumar Maharana,Saksham Singh Kushwaha,Baoming Zhang,Adrian Rodriguez,Songtao Wei,Yapeng Tian,Yunhui Guo*

Main category: cs.SD

TL;DR: Recent audio-visual models perform well but lack robustness under distributional shifts. Existing benchmarks are insufficient for assessing this robustness. This paper introduces AVROBUSTBENCH, a benchmark with 4 datasets and 75 corruptions to evaluate audio-visual model robustness. Evaluations show declining robustness with increased corruption severity. Online TTA methods offer minimal improvements. The proposed AV2C approach improves performance on VGGSOUND-2C.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of understanding of the robustness of recent audio-visual models under distributional shifts and the insufficiency of existing robustness benchmarks that mainly focus on single modalities. Real-world scenarios often involve simultaneous shifts in both audio and visual modalities.

Method: Introduced AVROBUSTBENCH, a comprehensive benchmark consisting of four datasets (AUDIOSET-2C, VGGSOUND-2C, KINETICS-2C, EPICKITCHENS-2C) each with 75 co-occurring and correlated bimodal audio-visual corruptions. Proposed AV2C, a simple TTA approach enabling on-the-fly cross-modal fusion by penalizing high-entropy samples.

Result: State-of-the-art supervised and self-supervised audio-visual models show declining robustness as corruption severity increases. Online TTA methods provide minimal improvements under bimodal corruptions. AV2C achieves improvements on VGGSOUND-2C.

Conclusion: AVROBUSTBENCH can steer the development of more effective and robust audio-visual TTA approaches.

Abstract: While recent audio-visual models have demonstrated impressive performance,
their robustness to distributional shifts at test-time remains not fully
understood. Existing robustness benchmarks mainly focus on single modalities,
making them insufficient for thoroughly assessing the robustness of
audio-visual models. Motivated by real-world scenarios where shifts can occur
$\textit{simultaneously}$ in both audio and visual modalities, we introduce
$\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the
test-time robustness of audio-visual recognition models.
$\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets,
$\texttt{AUDIOSET-2C}$, $\texttt{VGGSOUND-2C}$, $\texttt{KINETICS-2C}$, and
$\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual
corruptions that are $\textit{co-occurring}$ and $\textit{correlated}$. Through
extensive evaluations, we observe that state-of-the-art supervised and
self-supervised audio-visual models exhibit declining robustness as corruption
severity increases. Furthermore, online test-time adaptation (TTA) methods, on
$\texttt{VGGSOUND-2C}$ and $\texttt{KINETICS-2C}$, offer minimal improvements
in performance under bimodal corruptions. We further propose $\texttt{AV2C}$, a
simple TTA approach enabling on-the-fly cross-modal fusion by penalizing
high-entropy samples, which achieves improvements on $\texttt{VGGSOUND-2C}$. We
hope that $\texttt{AVROBUSTBENCH}$ will steer the development of more effective
and robust audio-visual TTA approaches. Our code is available
$\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.

</details>


### [581] [MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation](https://arxiv.org/abs/2506.00385)
*Yakun Song,Jiawei Chen,Xiaobin Zhuang,Chenpeng Du,Ziyang Ma,Jian Wu,Jian Cong,Dongya Jia,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Main category: cs.SD

TL;DR: MagiCodec is a novel single-layer, streaming Transformer-based audio codec that surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the bottleneck of existing codecs that are optimized primarily for reconstruction quality at the expense of the downstream modelability of the encoded tokens.

Method: MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity.

Result: Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages.

Conclusion: MagiCodec improves compatibility with language-model-based generative architectures and its code and pre-trained models are available.

Abstract: Neural audio codecs have made significant strides in efficiently mapping raw
audio waveforms into discrete token representations, which are foundational for
contemporary audio generative models. However, most existing codecs are
optimized primarily for reconstruction quality, often at the expense of the
downstream modelability of the encoded tokens. Motivated by the need to
overcome this bottleneck, we introduce $\textbf{MagiCodec}$, a novel
single-layer, streaming Transformer-based audio codec. MagiCodec is designed
with a multistage training pipeline that incorporates Gaussian noise injection
and latent regularization, explicitly targeting the enhancement of semantic
expressiveness in the generated codes while preserving high reconstruction
fidelity. We analytically derive the effect of noise injection in the frequency
domain, demonstrating its efficacy in attenuating high-frequency components and
fostering robust tokenization. Extensive experimental evaluations show that
MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and
downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like
distributions, as observed in natural languages, thereby improving
compatibility with language-model-based generative architectures. The code and
pre-trained models are available at https://github.com/Ereboas/MagiCodec.

</details>


### [582] [XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](https://arxiv.org/abs/2506.00462)
*Ioan-Paul Ciobanu,Andrei-Iulian Hiji,Nicolae-Catalin Ristea,Paul Irofti,Cristian Rusu,Radu Tudor Ionescu*

Main category: cs.SD

TL;DR: 尽管许多音频deepfake检测器在域内测试中表现接近完美，但新提出的XMAD-Bench基准显示，在跨域多语言环境下，这些模型的表现可能仅略好于随机猜测。这表明需要开发更稳健的deepfake检测器。


<details>
  <summary>Details</summary>
Motivation: 随着deepfake技术的发展，虚假音频对公众构成威胁，如金融诈骗、身份盗窃和错误信息传播。尽管已有研究报道了高准确率的deepfake检测器，但它们通常在域内设置下进行测试，即训练集和测试集中的deepfake样本由相同的生成模型产生。因此，研究人员希望探索检测器在更具挑战性的跨域环境下的表现。

Method: 提出XMAD-Bench，一个大规模跨域多语言音频deepfake基准，包含668.8小时的真实和deepfake语音数据。该数据集中，训练和测试分割中的说话人、生成方法和真实音频来源各不相同，从而形成一个挑战性的跨域评估环境。通过域内和跨域实验比较检测器性能。

Result: 实验结果表明，deepfake检测器在域内测试中的表现通常接近100%，但在跨域测试中，其性能有时仅略好于随机猜测。

Conclusion: XMAD-Bench揭示了当前音频deepfake检测器在跨不同语言、说话人、生成方法和数据源时缺乏泛化能力的问题，强调了开发更稳健检测器的必要性。

Abstract: Recent advances in audio generation led to an increasing number of deepfakes,
making the general public more vulnerable to financial scams, identity theft,
and misinformation. Audio deepfake detectors promise to alleviate this issue,
with many recent studies reporting accuracy rates close to 99%. However, these
methods are typically tested in an in-domain setup, where the deepfake samples
from the training and test sets are produced by the same generative models. To
this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual
audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In
our novel dataset, the speakers, the generative methods, and the real audio
sources are distinct across training and test splits. This leads to a
challenging cross-domain evaluation setup, where audio deepfake detectors can
be tested ``in the wild''. Our in-domain and cross-domain experiments indicate
a clear disparity between the in-domain performance of deepfake detectors,
which is usually as high as 100%, and the cross-domain performance of the same
models, which is sometimes similar to random chance. Our benchmark highlights
the need for the development of robust audio deepfake detectors, which maintain
their generalization capacity across different languages, speakers, generative
methods, and data sources. Our benchmark is publicly released at
https://github.com/ristea/xmad-bench/.

</details>


### [583] [Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models](https://arxiv.org/abs/2506.00832)
*Kyowoon Lee,Artyom Stitsyuk,Gunu Jho,Inchul Hwang,Jaesik Choi*

Main category: cs.SD

TL;DR: An abstract about a new method called Counterfactual Activation Editing in TTS models.


<details>
  <summary>Details</summary>
Motivation: Recent advances in Text-to-Speech (TTS) have significantly improved speech naturalness, increasing the demand for precise prosody control and mispronunciation correction.

Method: Counterfactual Activation Editing, a model-agnostic method that manipulates internal representations in a pre-trained TTS model to achieve post-hoc control of prosody and pronunciation.

Result: Experimental results show that our method effectively adjusts prosodic features and corrects mispronunciations while preserving synthesis quality.

Conclusion: This opens the door to inference-time refinement of TTS outputs without retraining, bridging the gap between pre-trained TTS models and editable speech synthesis.

Abstract: Recent advances in Text-to-Speech (TTS) have significantly improved speech
naturalness, increasing the demand for precise prosody control and
mispronunciation correction. Existing approaches for prosody manipulation often
depend on specialized modules or additional training, limiting their capacity
for post-hoc adjustments. Similarly, traditional mispronunciation correction
relies on grapheme-to-phoneme dictionaries, making it less practical in
low-resource settings. We introduce Counterfactual Activation Editing, a
model-agnostic method that manipulates internal representations in a
pre-trained TTS model to achieve post-hoc control of prosody and pronunciation.
Experimental results show that our method effectively adjusts prosodic features
and corrects mispronunciations while preserving synthesis quality. This opens
the door to inference-time refinement of TTS outputs without retraining,
bridging the gap between pre-trained TTS models and editable speech synthesis.

</details>


### [584] [CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching](https://arxiv.org/abs/2506.00885)
*Leying Zhang,Yao Qian,Xiaofei Wang,Manthan Thakker,Dongmei Wang,Jianwei Yu,Haibin Wu,Yuxuan Hu,Jinyu Li,Yanmin Qian,Sheng Zhao*

Main category: cs.SD

TL;DR: CoVoMix2 is a new framework for multi-speaker dialogue generation which predicts mel-spectrograms from transcriptions, using strategies like speaker disentanglement and random masking. It surpasses existing models in speech quality, consistency, and inference speed while supporting controllable dialogue features.


<details>
  <summary>Details</summary>
Motivation: Current systems for multi-speaker dialogue generation face challenges such as maintaining speaker consistency, modeling overlapping speech, and efficiently synthesizing coherent conversations.

Method: CoVoMix2 uses a fully non-autoregressive framework to directly predict mel-spectrograms from multi-stream transcriptions via a flow-matching-based generative model. Strategies include transcription-level speaker disentanglement, sentence-level alignment, and prompt-level random masking.

Result: CoVoMix2 outperforms strong baselines (e.g., MoonCast, Sesame) in speech quality, speaker consistency, and inference speed. It supports controllable dialogue generation with overlapping speech and precise timing control.

Conclusion: CoVoMix2 demonstrates state-of-the-art performance in multi-talker dialogue generation without requiring transcriptions for the prompt, showing strong generalizability for real-world speech scenarios.

Abstract: Generating natural-sounding, multi-speaker dialogue is crucial for
applications such as podcast creation, virtual agents, and multimedia content
generation. However, existing systems struggle to maintain speaker consistency,
model overlapping speech, and synthesize coherent conversations efficiently. In
this paper, we introduce CoVoMix2, a fully non-autoregressive framework for
zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts
mel-spectrograms from multi-stream transcriptions using a flow-matching-based
generative model, eliminating the reliance on intermediate token
representations. To better capture realistic conversational dynamics, we
propose transcription-level speaker disentanglement, sentence-level alignment,
and prompt-level random masking strategies. Our approach achieves
state-of-the-art performance, outperforming strong baselines like MoonCast and
Sesame in speech quality, speaker consistency, and inference speed. Notably,
CoVoMix2 operates without requiring transcriptions for the prompt and supports
controllable dialogue generation, including overlapping speech and precise
timing control, demonstrating strong generalizability to real-world speech
generation scenarios.

</details>


### [585] [In-the-wild Audio Spatialization with Flexible Text-guided Localization](https://arxiv.org/abs/2506.00927)
*Tianrui Pan,Jie Liu,Zewen Huang,Jie Tang,Gangshan Wu*

Main category: cs.SD

TL;DR: The paper proposes a Text-guided Audio Spatialization (TAS) framework to enhance immersive experiences in AR, VR, and embodied AI applications by offering flexible and interactive control over binaural audio signals.


<details>
  <summary>Details</summary>
Motivation: Existing audio spatialization methods lack the flexibility and interactive control required for complex multi-object user-interactive environments.

Method: The TAS framework uses text prompts for flexible control and is trained on the SpatialTAS dataset with 376,000 simulated binaural audio samples. The model learns binaural differences guided by 3D spatial location and relative position prompts, augmented by flipped-channel audio.

Result: The model outperforms existing methods on both simulated and real-recorded datasets, showing superior generalization and accuracy. An assessment model based on Llama-3.1-8B demonstrates that text prompts provide flexible and interactive control for generating high-quality binaural audio with semantic consistency.

Conclusion: Text prompts enable flexible and interactive control for generating binaural audio with excellent quality and semantic consistency in spatial locations.

Abstract: To enhance immersive experiences, binaural audio offers spatial awareness of
sounding objects in AR, VR, and embodied AI applications. While existing audio
spatialization methods can generally map any available monaural audio to
binaural audio signals, they often lack the flexible and interactive control
needed in complex multi-object user-interactive environments. To address this,
we propose a Text-guided Audio Spatialization (TAS) framework that utilizes
flexible text prompts and evaluates our model from unified generation and
comprehension perspectives. Due to the limited availability of premium and
large-scale stereo data, we construct the SpatialTAS dataset, which encompasses
376,000 simulated binaural audio samples to facilitate the training of our
model. Our model learns binaural differences guided by 3D spatial location and
relative position prompts, augmented by flipped-channel audio. It outperforms
existing methods on both simulated and real-recorded datasets, demonstrating
superior generalization and accuracy. Besides, we develop an assessment model
based on Llama-3.1-8B, which evaluates the spatial semantic coherence between
our generated binaural audio and text prompts through a spatial reasoning task.
Results demonstrate that text prompts provide flexible and interactive control
to generate binaural audio with excellent quality and semantic consistency in
spatial locations. Dataset is available at
\href{https://github.com/Alice01010101/TASU}

</details>


### [586] [General-purpose audio representation learning for real-world sound scenes](https://arxiv.org/abs/2506.00934)
*Goksenin Yuksel,Marcel van Gerven,Kiki van der Heijden*

Main category: cs.SD

TL;DR: This paper proposes GRAMs, a novel self-supervised training approach for robust spatial audio representation learning in naturalistic, noisy sound scenes. It can be applied to any masking-based deep learning model and shows state-of-the-art performance on naturalistic sound scenes and spatial audio representation learning.


<details>
  <summary>Details</summary>
Motivation: Current audio foundation models are trained and tested on dry, non-spatial, single-source audio clips, which limits their success in real-world situations and results in spatially unaware audio embeddings.

Method: The authors propose a new self-supervised training approach called GRAMs (General-Purpose, Real-world Audio Models). This method enables robust spatial audio representation learning for naturalistic, noisy sound scenes and can be applied to any masking-based deep learning model. They train two state-of-the-art models with different backbones using the GRAM approach.

Result: GRAMs minimize the performance gap between dry, non-spatial, single-source sound scenes and naturalistic sound scenes for crucial tasks such as auditory scene analysis. They outperform existing state-of-the-art audio foundation models at a fraction of the training steps and show state-of-the-art performance on sound localization tasks, exceeding even supervised sound localization models.

Conclusion: The proposed GRAM approach represents a significant advancement towards robust audio foundation models for real-world applications with state-of-the-art performance on naturalistic sound scenes as well as spatial audio representation learning.

Abstract: While audio foundation models perform well on myriad of tasks from sound
classification to speech analysis, these models are trained and tested on dry,
non-spatial, single-source audio clips. This limits their success in real-world
situations and results in spatially unaware audio embeddings. To address these
limitations, we propose a novel self-supervised training approach for
General-Purpose, Real-world Audio Models (GRAMs). The GRAM training approach
enables robust spatial audio representation learning for naturalistic, noisy
sound scenes and can be applied to any masking-based deep learning model. We
demonstrate the success of our approach by training two state-of-the-art
models, one with a transformer and one with a mamba backbone. We assess the
quality of the extracted audio representations from GRAMs using the original
version of the HEAR benchmark, a newly synthesized, naturalistic version of the
HEAR benchmark, and novel sound localization tasks based on HEAR benchmark
datasets. The results show that our approach minimizes the performance gap
between dry, non-spatial, single-source sound scenes and naturalistic sound
scenes for crucial tasks such as auditory scene analysis, outperforming
existing state-of-the-art audio foundation models at a fraction of the training
steps. Moreover, GRAMs show state-of-the-art performance on sound localization
tasks, exceeding even supervised sound localization models. In sum, the
proposed approach represents a significant advancement towards robust audio
foundation models for real-world applications with state-of-the-art performance
on naturalistic sound scenes as well as spatial audio representation learning.

</details>


### [587] [A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement](https://arxiv.org/abs/2506.01023)
*Shenghui Lu,Hukai Huang,Jinanglong Yao,Kaidi Wang,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: This paper proposes HDF-Net that combines sub-band processing and deep filtering for single-channel speech enhancement, demonstrating superior performance with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To fully exploit information from the target time-frequency (TF) bin and its surrounding TF bins for improving single-channel speech enhancement.

Method: The model integrates a sub-band module capturing surrounding frequency bin information at the input and a deep filtering module applying filtering at the output. Deep filtering is decoupled into temporal and frequency components within a two-stage framework, and the TAConv module is introduced to enhance convolutional feature extraction.

Result: Experimental results show that the proposed HDF-Net effectively uses surrounding TF bin information, outperforming other advanced systems while utilizing fewer resources.

Conclusion: HDF-Net successfully enhances single-channel speech by leveraging surrounding TF bin information and reducing resource usage.

Abstract: This paper proposes a model that integrates sub-band processing and deep
filtering to fully exploit information from the target time-frequency (TF) bin
and its surrounding TF bins for single-channel speech enhancement. The sub-band
module captures surrounding frequency bin information at the input, while the
deep filtering module applies filtering at the output to both the target TF bin
and its surrounding TF bins. To further improve the model performance, we
decouple deep filtering into temporal and frequency components and introduce a
two-stage framework, reducing the complexity of filter coefficient prediction
at each stage. Additionally, we propose the TAConv module to strengthen
convolutional feature extraction. Experimental results demonstrate that the
proposed hierarchical deep filtering network (HDF-Net) effectively utilizes
surrounding TF bin information and outperforms other advanced systems while
using fewer resources.

</details>


### [588] [The iNaturalist Sounds Dataset](https://arxiv.org/abs/2506.00343)
*Mustafa Chasmai,Alexander Shepard,Subhransu Maji,Grant Van Horn*

Main category: cs.SD

TL;DR: The paper introduces iNatSounds, a large dataset of audio files from various species, explores different model architectures for classification tasks, and highlights its potential applications in public engagement and ecological research.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive and accessible dataset of species sounds to facilitate research and development of models that can process large audio collections, aiding biologists, ecologists, and land use managers.

Method: Compilation of 230,000 audio files from over 5,500 species contributed by 27,000 recordists; benchmarking multiple backbone architectures using multiclass and multilabel classification objectives.

Result: iNatSounds proves to be a valuable pretraining resource even with weak labeling, showing effectiveness on downstream evaluation datasets with strong labels.

Conclusion: iNatSounds is released as a freely accessible archive to promote research in species sound analysis, with potential to enhance public engagement applications and assist professionals in understanding species compositions.

Abstract: We present the iNaturalist Sounds Dataset (iNatSounds), a collection of
230,000 audio files capturing sounds from over 5,500 species, contributed by
more than 27,000 recordists worldwide. The dataset encompasses sounds from
birds, mammals, insects, reptiles, and amphibians, with audio and species
labels derived from observations submitted to iNaturalist, a global citizen
science platform. Each recording in the dataset varies in length and includes a
single species annotation. We benchmark multiple backbone architectures,
comparing multiclass classification objectives with multilabel objectives.
Despite weak labeling, we demonstrate that iNatSounds serves as a useful
pretraining resource by benchmarking it on strongly labeled downstream
evaluation datasets. The dataset is available as a single, freely accessible
archive, promoting accessibility and research in this important domain. We
envision models trained on this data powering next-generation public engagement
applications, and assisting biologists, ecologists, and land use managers in
processing large audio collections, thereby contributing to the understanding
of species compositions in diverse soundscapes.

</details>


### [589] [FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion](https://arxiv.org/abs/2506.01111)
*Shunian Chen,Xinyuan Xie,Zheshu Chen,Liyan Zhao,Owen Lee,Zhan Su,Qilin Sun,Benyou Wang*

Main category: cs.SD

TL;DR: 受到人类听觉感知的启发，本文提出了一种新的两阶段自动化管道，用于生成高质量、大规模的音频字幕。该方法首先使用专门的预训练模型提取多模态线索，然后利用大型语言模型生成详细且上下文感知的音频字幕。本文的主要贡献包括：1）细粒度音频字幕生成的可扩展方法；2）FusionAudio数据集，包含120万条详细字幕和600万问答对；3）使用FusionAudio增强的音频模型，特别是基于CLAP的音频编码器，具有优越的音频-文本对齐和指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化音频字幕生成方法往往缺乏细粒度细节和上下文准确性，主要原因是依赖于有限的单模态或浅层多模态信息。为了解决这一问题，本文从人类听觉感知中汲取灵感，设计了一种新的方法。

Method: 本文提出的方法分为两个阶段：第一阶段使用专门的预训练模型提取多样化的上下文线索（如语音、音乐、一般声音以及相关视频中的视觉信息）；第二阶段利用大型语言模型综合这些丰富的多模态输入，生成详细的、上下文感知的音频字幕。

Result: 通过使用FusionAudio数据集，本文开发出了增强的音频模型，特别是基于CLAP的音频编码器，其在音频-文本对齐和指令跟随方面表现出色。此外，本文还展示了所提出方法在生成细粒度音频字幕方面的有效性。

Conclusion: 本文提出了一种新颖的两阶段自动化管道，能够生成高质量、大规模的音频字幕，并为此构建了FusionAudio数据集，推动了复杂音频环境的自动化理解的发展。

Abstract: High-quality, large-scale audio captioning is crucial for advancing audio
understanding, yet current automated methods often generate captions that lack
fine-grained detail and contextual accuracy, primarily due to their reliance on
limited unimodal or superficial multimodal information. Drawing inspiration
from human auditory perception, which adeptly integrates cross-modal cues and
performs sophisticated auditory scene analysis, we introduce a novel two-stage
automated pipeline. This pipeline first employs specialized pretrained models
to extract diverse contextual cues (e.g., speech, music, general sounds, and
visual information from associated video). A large language model (LLM) then
synthesizes these rich, multimodal inputs to generate detailed and
context-aware audio captions. Key contributions of this work include: (1) the
proposed scalable method for fine-grained audio caption generation; (2)
FusionAudio, a new large-scale dataset comprising 1.2 million such detailed
captions, combined with 6 million QA pairs; and (3) enhanced audio models
developed using FusionAudio, specifically a CLAP-based audio encoder with
superior audio-text alignment and instruction following. This paper paves the
way for more nuanced and accurate automated understanding of complex audio
environments. Code and data can be found in
https://github.com/satsuki2486441738/FusionAudio.

</details>


### [590] [Learning to Upsample and Upmix Audio in the Latent Domain](https://arxiv.org/abs/2506.00681)
*Dimitrios Bralios,Paris Smaragdis,Jonah Casebeer*

Main category: cs.SD

TL;DR: 提出了一种在自动编码器潜在空间内进行音频处理操作的框架，大幅简化训练流程并提升效率。


<details>
  <summary>Details</summary>
Motivation: 尽管神经音频自动编码器已被广泛使用，但大多数音频处理操作仍然低效地作用于原始波形或频谱表示，而非压缩表示。

Method: 通过完全在自动编码器的潜在空间中执行音频处理操作，仅使用潜在L1重建项和单一潜在对抗判别器来简化训练，避免了解码到原始音频格式的需要。

Result: 在带宽扩展和单声道至立体声上混实验中，实现了高达100倍的计算效率提升，同时保持与对原始音频后处理相当的质量。

Conclusion: 该框架为已包含自动编码器的音频处理管道提供了更高效的范式，显著加速了各种音频任务的工作流程。

Abstract: Neural audio autoencoders create compact latent representations that preserve
perceptually important information, serving as the foundation for both modern
audio compression systems and generation approaches like next-token prediction
and latent diffusion. Despite their prevalence, most audio processing
operations, such as spatial and spectral up-sampling, still inefficiently
operate on raw waveforms or spectral representations rather than directly on
these compressed representations. We propose a framework that performs audio
processing operations entirely within an autoencoder's latent space,
eliminating the need to decode to raw audio formats. Our approach dramatically
simplifies training by operating solely in the latent domain, with a latent L1
reconstruction term, augmented by a single latent adversarial discriminator.
This contrasts sharply with raw-audio methods that typically require complex
combinations of multi-scale losses and discriminators. Through experiments in
bandwidth extension and mono-to-stereo up-mixing, we demonstrate computational
efficiency gains of up to 100x while maintaining quality comparable to
post-processing on raw audio. This work establishes a more efficient paradigm
for audio processing pipelines that already incorporate autoencoders, enabling
significantly faster and more resource-efficient workflows across various audio
tasks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [591] [FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing](https://arxiv.org/abs/2506.01566)
*Mika Markus Müller,Konstantin Lübeck,Alexander Louis-Ferdinand Jung,Jannik Steinmetz,Oliver Bringmann*

Main category: cs.PF

TL;DR: The paper introduces FlexiSAGA, a configurable AI hardware accelerator for sparse and dense processing of DNNs. It supports multiple dataflows and includes a tailored DNN pruning method. Results show significant speedup over existing platforms.


<details>
  <summary>Details</summary>
Motivation: To address the computational complexity challenge of DNN inference on resource-constrained edge devices by exploiting sparsity in DNN operator weights.

Method: Development of FlexiSAGA, an architecturally configurable and dataflow-flexible AI hardware accelerator supporting seven different sparse and dense dataflows. Also, proposing a DNN pruning method specifically designed for the FlexiSAGA architecture.

Result: Achieved whole DNN sparse-over-dense inference speedup ranging from 1.41 to 4.28, outperforming commercial and literature-reported accelerator platforms.

Conclusion: FlexiSAGA demonstrates efficient processing of resource intensive DNN operators and facilitates a DNN/HW co-design flow.

Abstract: Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs),
have become an important tool for a wide range of applications, from computer
vision to natural language processing. However, the computational complexity of
DNN inference poses a significant challenge, particularly for processing on
resource-constrained edge devices. One promising approach to address this
challenge is the exploitation of sparsity in DNN operator weights.
  In this work, we present FlexiSAGA, an architecturally configurable and
dataflow-flexible AI hardware accelerator for the sparse and dense processing
of general matrix multiplications (GEMMs). FlexiSAGA supports seven different
sparse and dense dataflows, enabling efficient processing of resource intensive
DNN operators. Additionally, we propose a DNN pruning method specifically
tailored towards the FlexiSAGA architecture, allowing for near-optimal
processing of dense and sparse convolution and fully-connected operators,
facilitating a DNN/HW co-design flow. Our results show a whole DNN
sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming
commercial and literature-reported accelerator platforms.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [592] [Trilevel Memetic Algorithm for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2506.01065)
*Ivan Milinović,Leon Stjepan Uroić,Marko Đurasević*

Main category: cs.NE

TL;DR: The paper presents a Trilevel Memetic Algorithm (TMA) for solving the Electric Vehicle Routing Problem (EVRP). It combines genetic algorithms with dynamic programming, showing competitive performance in benchmark tests but faces scalability issues.


<details>
  <summary>Details</summary>
Motivation: To address the optimization challenges posed by EVRP, which includes battery constraints and charging stations, requiring advanced algorithms for efficient solutions.

Method: A Trilevel Memetic Algorithm (TMA) that hierarchically optimizes customer sequences, route assignments, and charging station insertions using a combination of genetic algorithms and dynamic programming.

Result: Competitive performance in benchmark tests on WCCI2020 instances, matching best-known results for small-scale cases.

Conclusion: TMA demonstrates strong potential for sustainable logistics planning despite computational demands limiting scalability.

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the capacitated vehicle
routing problem by incorporating battery constraints and charging stations,
posing significant optimization challenges. This paper introduces a Trilevel
Memetic Algorithm (TMA) that hierarchically optimizes customer sequences, route
assignments, and charging station insertions. The method combines genetic
algorithms with dynamic programming, ensuring efficient and high-quality
solutions. Benchmark tests on WCCI2020 instances show competitive performance,
matching best-known results for small-scale cases. While computational demands
limit scalability, TMA demonstrates strong potential for sustainable logistics
planning.

</details>


### [593] [Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator](https://arxiv.org/abs/2506.01107)
*Abderrahim Bendahi,Benjamin Doerr,Adrien Fradin,Johannes F. Lutzeyer*

Main category: cs.NE

TL;DR: The paper proposes two modifications to the move-acceptance hyper-heuristic algorithm, which significantly improve its performance on various benchmark functions.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of leaving local optima in optimization problems by improving the move-acceptance hyper-heuristic algorithm.

Method: (i) Use a simple two-state Markov chain instead of random choice for selecting between acceptance operators. (ii) Replace all-moves acceptance operator with an only-worsening operator.

Result: The first modification reduces runtime on Jump$_m$ functions from $\Omega(n^{2m-1})$ to $O(n^{m+1})$. The second modification further reduces runtime on Jump functions to $O(n^3 \log n)$ regardless of gap size. Overall, the modified algorithm achieves a runtime of $O(n^{k+1} \log n)$ on SEQOPT$_k$ functions.

Conclusion: The proposed modifications to the move-acceptance hyper-heuristic greatly improve its performance on a wide range of benchmark functions.

Abstract: The move-acceptance hyper-heuristic was recently shown to be able to leave
local optima with astonishing efficiency (Lissovoi et al., Artificial
Intelligence (2023)). In this work, we propose two modifications to this
algorithm that demonstrate impressive performances on a large class of
benchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i)
Instead of randomly choosing between the only-improving and any-move acceptance
operator, we take this choice via a simple two-state Markov chain. This
modification alone reduces the runtime on Jump$_m$ functions with gap parameter
$m$ from $\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves
acceptance operator with the operator that only accepts worsenings. Such a,
counter-intuitive, operator has not been used before in the literature.
However, our proofs show that our only-worsening operator can greatly help in
leaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3
\log n)$ independent of the gap size. In general, we prove a remarkably good
runtime of $O(n^{k+1} \log n)$ for our Markov move-acceptance hyper-heuristic
on all members of a new benchmark class SEQOPT$_k$, which contains a large
number of functions having $k$ successive local optima, and which contains the
commonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.

</details>


### [594] [SpiceMixer -- Netlist-Level Circuit Evolution](https://arxiv.org/abs/2506.01497)
*Stefan Uhlich,Andrea Bonetti,Arun Venkitaraman,Chia-Yu Hsieh,Mustafa Emre Gürsoy,Ryoga Matsuo,Lorenzo Servadei*

Main category: cs.NE

TL;DR: The paper presents SpiceMixer, a genetic algorithm for synthesizing novel analog circuits by evolving SPICE netlists. It operates directly on netlist lines, uses normalized format to enhance genetic operators and outperforms existing methods in various tasks including designing an analog classifier circuit with 89% accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a more flexible and effective method for analog circuit synthesis that can operate on any component or subcircuit type and support general-purpose genetic operations.

Method: SpiceMixer is a genetic algorithm that evolves SPICE netlists to synthesize novel analog circuits. It operates directly on netlist lines using a normalized format to improve the effectiveness of crossover, mutation, and pruning.

Result: SpiceMixer successfully synthesizes standard cells (inverter, two-input NAND, latch) and designs an analog classifier circuit for the Iris dataset with 89% accuracy on the test set, consistently outperforming existing synthesis methods across all evaluated tasks.

Conclusion: SpiceMixer represents a significant advancement in analog circuit synthesis, offering superior performance and versatility through its unique approach of evolving SPICE netlists.

Abstract: This paper introduces SpiceMixer, a genetic algorithm developed to synthesize
novel analog circuits by evolving SPICE netlists. Unlike conventional methods,
SpiceMixer operates directly on netlist lines, enabling compatibility with any
component or subcircuit type and supporting general-purpose genetic operations.
By using a normalized netlist format, the algorithm enhances the effectiveness
of its genetic operators: crossover, mutation, and pruning. We show that
SpiceMixer achieves superior performance in synthesizing standard cells
(inverter, two-input NAND, and latch) and in designing an analog classifier
circuit for the Iris dataset, reaching an accuracy of 89% on the test set.
Across all evaluated tasks, SpiceMixer consistently outperforms existing
synthesis methods.

</details>


### [595] [Engram Memory Encoding and Retrieval: A Neurocomputational Perspective](https://arxiv.org/abs/2506.01659)
*Daniel Szelogowski*

Main category: cs.NE

TL;DR: 尽管对记忆的生物学基础进行了大量研究，但大脑如何编码、存储和检索记忆的确切机制仍未完全理解。本文通过整合细胞神经科学和计算建模的见解，提出了一个全面的理论基础，解释了记忆效率、容量和稳定性如何从可塑性和稀疏性约束的相互作用中产生，并为未来的记忆研究提供了路线图。


<details>
  <summary>Details</summary>
Motivation: 现有的研究虽然支持记忆印迹理论，但缺乏一个全面的计算框架将生物发现与机制模型整合起来。

Method: 综合细胞神经科学和计算建模的见解，探讨记忆印迹神经元的识别和操作方法，突触可塑性机制对稳定记忆痕迹的贡献，以及稀疏性如何促进高效、抗干扰的记忆表示。同时考察相关计算方法，如稀疏正则化、记忆印迹门控和受生物启发的架构（如稀疏分布式内存和脉冲神经网络）。

Result: 研究表明，记忆效率、容量和稳定性来源于可塑性和稀疏性约束的交互作用。

Conclusion: 通过整合神经生物学和计算视角，本文为记忆印迹研究提供了全面的理论基础，并为未来记忆机制的研究提出了路线图，对记忆相关疾病的诊断和治疗具有重要意义。

Abstract: Despite substantial research into the biological basis of memory, the precise
mechanisms by which experiences are encoded, stored, and retrieved in the brain
remain incompletely understood. A growing body of evidence supports the engram
theory, which posits that sparse populations of neurons undergo lasting
physical and biochemical changes to support long-term memory. Yet, a
comprehensive computational framework that integrates biological findings with
mechanistic models remains elusive. This work synthesizes insights from
cellular neuroscience and computational modeling to address key challenges in
engram research: how engram neurons are identified and manipulated; how
synaptic plasticity mechanisms contribute to stable memory traces; and how
sparsity promotes efficient, interference-resistant representations. Relevant
computational approaches -- such as sparse regularization, engram gating, and
biologically inspired architectures like Sparse Distributed Memory and spiking
neural networks -- are also examined. Together, these findings suggest that
memory efficiency, capacity, and stability emerge from the interaction of
plasticity and sparsity constraints. By integrating neurobiological and
computational perspectives, this paper provides a comprehensive theoretical
foundation for engram research and proposes a roadmap for future inquiry into
the mechanisms underlying memory, with implications for the diagnosis and
treatment of memory-related disorders.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [596] [Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics](https://arxiv.org/abs/2506.00070)
*Dongyoung Kim,Sumin Park,Huiwon Jang,Jinwoo Shin,Jaehyung Kim,Younggyo Seo*

Main category: cs.RO

TL;DR: Large Vision-Language Models (LVLMs) have shown potential in advancing robotics through embodied reasoning and robot control. However, Supervised Fine-Tuning (SFT) often used for training on these tasks has limitations such as heuristic datasets and issues like catastrophic forgetting. To solve these problems, the paper introduces Robot-R1, a framework using reinforcement learning to enhance embodied reasoning specifically for robot control.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to overcome the limitations of SFT methods in training LVLMs for robot control, particularly addressing issues with dataset construction and performance degradation due to catastrophic forgetting and reduced generalization.

Method: Robot-R1 leverages reinforcement learning to improve embodied reasoning for robot control. It learns to predict the next keypoint state needed for task completion based on the current scene image and environment metadata from expert demonstrations. The model samples reasoning-based responses and reinforces those leading to more accurate predictions.

Result: Experiments demonstrate that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Notably, despite having only 7B parameters, Robot-R1 surpasses GPT-4o in reasoning tasks related to low-level action control.

Conclusion: Robot-R1 represents a significant advancement in using reinforcement learning to enhance embodied reasoning for robot control, offering improved performance over traditional SFT methods.

Abstract: Large Vision-Language Models (LVLMs) have recently shown great promise in
advancing robotics by combining embodied reasoning with robot control. A common
approach involves training on embodied reasoning tasks related to robot control
using Supervised Fine-Tuning (SFT). However, SFT datasets are often
heuristically constructed and not explicitly optimized for improving robot
control. Furthermore, SFT often leads to issues such as catastrophic forgetting
and reduced generalization performance. To address these limitations, we
introduce Robot-R1, a novel framework that leverages reinforcement learning to
enhance embodied reasoning specifically for robot control. Robot-R1 learns to
predict the next keypoint state required for task completion, conditioned on
the current scene image and environment metadata derived from expert
demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples
reasoning-based responses and reinforces those that lead to more accurate
predictions. Our experiments show that models trained with Robot-R1 outperform
SFT methods on embodied reasoning tasks. Despite having only 7B parameters,
Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action
control, such as spatial and primitive movement reasoning.

</details>


### [597] [Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation](https://arxiv.org/abs/2506.00075)
*Diego Pollini,Bruna V. Guterres,Rodrigo S. Guerra,Ricardo B. Grando*

Main category: cs.RO

TL;DR: The paper integrates ChatGPT with ROS 2 in a Gazebo simulation to reduce interaction latency and enhance human-robot collaboration without middleware, showing a 7.01% decrease in communication latency.


<details>
  <summary>Details</summary>
Motivation: To address the latency issues caused by computational complexity of LLMs in human-robot interactions within industrial robotics.

Method: Integration of ChatGPT natural language model with ROS 2 in a simulated Gazebo environment without using a middleware transport platform, allowing the robot to respond to text and voice commands.

Result: Experiments show a 7.01% average decrease in communication latency, improving execution speed, usability, and accessibility of human-robot interaction.

Conclusion: This approach successfully mitigates interaction latency and enhances robotic system control, promoting smoother real-time operations for industrial automation.

Abstract: The integration of Large Language Models (LLMs), such as GPT, in industrial
robotics enhances operational efficiency and human-robot collaboration.
However, the computational complexity and size of these models often provide
latency problems in request and response times. This study explores the
integration of the ChatGPT natural language model with the Robot Operating
System 2 (ROS 2) to mitigate interaction latency and improve robotic system
control within a simulated Gazebo environment. We present an architecture that
integrates these technologies without requiring a middleware transport
platform, detailing how a simulated mobile robot responds to text and voice
commands. Experimental results demonstrate that this integration improves
execution speed, usability, and accessibility of the human-robot interaction by
decreasing the communication latency by 7.01\% on average. Such improvements
facilitate smoother, real-time robot operations, which are crucial for
industrial automation and precision tasks.

</details>


### [598] [Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments](https://arxiv.org/abs/2506.00083)
*Jiawei Hou,Xiangyang Xue,Taiping Zeng*

Main category: cs.RO

TL;DR: An autonomous service robot operation solution named Hi-Dyna Graph is proposed, which combines global topological graphs with dynamic subgraphs to achieve context-aware decision-making in changing environments. Powered by LLMs, it completes complex tasks without further training.


<details>
  <summary>Details</summary>
Motivation: Current methods either fail to model transient object relationships or incur prohibitive computational costs. There is a need for an efficient and effective way to represent scenes for robotic autonomy in human-centric environments.

Method: Hi-Dyna Graph integrates persistent global layouts with localized dynamic semantics. It constructs a global topological graph from RGB-D inputs, while environmental and egocentric cameras populate dynamic subgraphs. These subgraphs are anchored to the global topology using semantic and spatial constraints, enabling seamless updates. An LLM-powered agent interprets the unified graph to infer task triggers and generate instructions.

Result: Complex experiments demonstrate the superior scene representation effectiveness of Hi-Dyna Graph. Real-world deployments with a mobile manipulator validate its practicality, as robots autonomously complete complex tasks in dynamic scenes like cafeteria assistance.

Conclusion: Hi-Dyna Graph provides an effective hierarchical dynamic scene graph architecture for embodied robotic autonomy, enabling context-aware decision-making in changing environments.

Abstract: Autonomous operation of service robotics in human-centric scenes remains
challenging due to the need for understanding of changing environments and
context-aware decision-making. While existing approaches like topological maps
offer efficient spatial priors, they fail to model transient object
relationships, whereas dense neural representations (e.g., NeRF) incur
prohibitive computational costs. Inspired by the hierarchical scene
representation and video scene graph generation works, we propose Hi-Dyna
Graph, a hierarchical dynamic scene graph architecture that integrates
persistent global layouts with localized dynamic semantics for embodied robotic
autonomy. Our framework constructs a global topological graph from posed RGB-D
inputs, encoding room-scale connectivity and large static objects (e.g.,
furniture), while environmental and egocentric cameras populate dynamic
subgraphs with object position relations and human-object interaction patterns.
A hybrid architecture is conducted by anchoring these subgraphs to the global
topology using semantic and spatial constraints, enabling seamless updates as
the environment evolves. An agent powered by large language models (LLMs) is
employed to interpret the unified graph, infer latent task triggers, and
generate executable instructions grounded in robotic affordances. We conduct
complex experiments to demonstrate Hi-Dyna Grap's superior scene representation
effectiveness. Real-world deployments validate the system's practicality with a
mobile manipulator: robotics autonomously complete complex tasks with no
further training or complex rewarding in a dynamic scene as cafeteria
assistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video
demonstration and more details.

</details>


### [599] [LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks](https://arxiv.org/abs/2506.00411)
*Yi Yang,Jiaxuan Sun,Siqi Kou,Yihan Wang,Zhijie Deng*

Main category: cs.RO

TL;DR: The paper introduces LoHoVLA, a unified VLA framework for long-horizon tasks that combines high-level task planning and low-level motion control. It uses a pretrained VLM as its backbone and a hierarchical closed-loop control mechanism, achieving superior performance on embodied tasks in the Ravens simulator.


<details>
  <summary>Details</summary>
Motivation: Real-world embodied agents face challenges with long-horizon tasks that require both high-level task planning and low-level motion control. Existing VLA models struggle with planning, while hierarchical architectures suffer from coordination issues.

Method: LoHoVLA leverages a large pretrained vision language model (VLM) to generate language tokens for sub-task generation and action tokens for robot action prediction. It also employs a hierarchical closed-loop control mechanism to address errors in both high-level planning and low-level control.

Result: LoHoVLA significantly outperforms both hierarchical and standard VLA approaches on long-horizon embodied tasks within the Ravens simulator.

Conclusion: The findings highlight the potential of unified architectures like LoHoVLA in advancing generalizable embodied intelligence.

Abstract: Real-world embodied agents face long-horizon tasks, characterized by
high-level goals demanding multi-step solutions beyond single actions.
Successfully navigating these requires both high-level task planning (i.e.,
decomposing goals into sub-tasks) and low-level motion control (i.e.,
generating precise robot actions). While existing vision language action (VLA)
models and hierarchical architectures offer potential in embodied tasks, the
former often falter in planning, and the latter can suffer from coordination
issues, both hampering performance. We introduce a new unified VLA framework
for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA
leverages a large pretrained vision language model (VLM) as the backbone to
jointly generate language and action tokens for sub-task generation and robot
action prediction, respectively. This shared representation promotes better
generalization across tasks. Additionally, LoHoVLA embraces a hierarchical
closed-loop control mechanism to mitigate errors originating from both
high-level planning and low-level control. To train LoHoVLA, we introduce
LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon
tasks, each with 1,000 expert demonstrations composed of visual observations,
linguistic goals, sub-tasks, and robot actions. Experimental results show that
LoHoVLA significantly surpasses both hierarchical and standard VLA approaches
on long-horizon embodied tasks in the Ravens simulator. These findings
underscore the promise of unified architectures for advancing generalizable
embodied intelligence.

</details>


### [600] [Diffusion Models for Increasing Accuracy in Olfaction Sensors and Datasets](https://arxiv.org/abs/2506.00455)
*Kordel K. France,Ovidiu Daescu*

Main category: cs.RO

TL;DR: A new machine learning method using diffusion-based molecular generation is introduced to improve robotic odour source localization accuracy, overcoming limitations in olfactory datasets and sensor resolutions by integrating visual analysis, language processing, and molecular generation.


<details>
  <summary>Details</summary>
Motivation: Current OSL methods often suffer from ambiguities due to limitations in olfactory datasets and sensor resolutions, leading to misattribution of odours to incorrect objects.

Method: The novel method uses diffusion-based molecular generation to enhance odour localization accuracy. It can be used alone or with automated olfactory dataset construction pipelines with vision-language models. The generative process expands the chemical space beyond current limitations, identifying potential odourant molecules not previously documented.

Result: The generated molecules can be more accurately validated using advanced olfactory sensors, improving the ability of olfaction-vision models on robots to associate odours with their correct sources, thus enhancing navigation and decision-making in environments where olfactory cues are essential.

Conclusion: This methodology represents a foundational advancement in robotic olfaction, providing a scalable solution to challenges posed by limited olfactory data and sensor ambiguities.

Abstract: Robotic odour source localization (OSL) is a critical capability for
autonomous systems operating in complex environments. However, current OSL
methods often suffer from ambiguities, particularly when robots misattribute
odours to incorrect objects due to limitations in olfactory datasets and sensor
resolutions. To address this challenge, we introduce a novel machine learning
method using diffusion-based molecular generation to enhance odour localization
accuracy that can be used by itself or with automated olfactory dataset
construction pipelines with vision-language models (VLMs) This generative
process of our diffusion model expands the chemical space beyond the
limitations of both current olfactory datasets and the training data of VLMs,
enabling the identification of potential odourant molecules not previously
documented. The generated molecules can then be more accurately validated using
advanced olfactory sensors which emulate human olfactory recognition through
electronic sensor arrays. By integrating visual analysis, language processing,
and molecular generation, our framework enhances the ability of
olfaction-vision models on robots to accurately associate odours with their
correct sources, thereby improving navigation and decision-making in
environments where olfactory cues are essential. Our methodology represents a
foundational advancement in the field of robotic olfaction, offering a scalable
solution to the challenges posed by limited olfactory data and sensor
ambiguities.

</details>


### [601] [Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance](https://arxiv.org/abs/2506.00494)
*Ali Ghanizadeh,Ali Ahmadi,Arash Bahrami*

Main category: cs.RO

TL;DR: This paper explores the challenge of modeling non-linear grasp force and deformation in Soft Fin-Ray grippers, using finite element method (FEM) to estimate deflections and contact forces, and constructing a multilayer perception (MLP) for predicting contact force and tip displacement. The study employs multi-objective optimization techniques, specifically non-dominated sorting genetic algorithm (NSGA-II), to balance delicate manipulation and high-force applications in soft robotic gripper design.


<details>
  <summary>Details</summary>
Motivation: Soft Fin-Ray grippers are valued for their ability to perform delicate and careful manipulation across various fields, but there is a challenge in modeling their non-linear grasp force and deformation behaviors for design purposes. Additionally, there is a trade-off between exerting higher forces and maintaining delicate handling.

Method: The researchers use finite element method (FEM) to estimate deflections and contact forces when the Fin-Ray finger grasps cylindrical objects. This data is used to construct a multilayer perception (MLP) model that predicts contact force and tip displacement. Multi-objective optimization techniques, particularly the non-dominated sorting genetic algorithm (NSGA-II), are employed to find an optimized set of solutions balancing force application and delicate manipulation.

Result: The methodologies presented can improve the design and gripping performance of soft robotic grippers. They allow for selecting a design suitable for both delicate grasping and high-force applications.

Conclusion: The study successfully demonstrates the effectiveness of using FEM, MLP, and NSGA-II in optimizing the design of soft Fin-Ray grippers, providing a framework for achieving better performance in both delicate and forceful manipulation tasks.

Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which
has caused notable attention in different fields. These grippers can handle
objects of various forms and sizes safely. The internal structure of the
Fin-Ray finger plays a significant role in its adaptability and grasping
performance. However, modeling the non-linear grasp force and deformation
behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger
becomes more rigid and capable of exerting higher forces, it becomes less
delicate in handling objects. The contrast between these two objectives gives
rise to a multi-objective optimization problem. In this study, we employ finite
element method (FEM) to estimate the deflections and contact forces of the
Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a
multilayer perception (MLP) for prediction of the contact force and the tip
displacement. The FEM dataset consists of three input and four target features.
The three input features of the MLP and optimization design variables are the
thickness of the front and supporting beams, the thickness of the cross beams,
and the equal spacing between the cross beams. In addition, the target features
are the maximum contact forces and maximum tip displacements in x- and
y-directions. The magnitude of maximum contact force and magnitude of maximum
tip displacement are the two objectives, showing the trade-off between force
and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized
set of solutions are found using multi-objective optimal techniques. We use
non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our
findings demonstrate that our methodologies can be used to improve the design
and gripping performance of soft robotic grippers, helping us to choose a
design not only for delicate grasping but also for high-force applications.

</details>


### [602] [Evaluating Robot Policies in a World Model](https://arxiv.org/abs/2506.00613)
*Julian Quevedo,Percy Liang,Sherry Yang*

Main category: cs.RO

TL;DR: 本文提出了一种基于世界模型的策略评估方法（WPE），通过训练动作条件视频生成模型来模拟真实环境，并使用视觉语言模型作为奖励函数进行蒙特卡洛模拟。尽管在物体交互方面存在挑战，但WPE在模仿机器人手臂运动方面具有高保真度，并能保持不同策略的相对排名。


<details>
  <summary>Details</summary>
Motivation: 评估机器人控制策略具有挑战性，因为现实世界的测试成本高昂，而手工制作的模拟往往无法准确反映真实条件，导致模拟评估与实际结果之间的相关性较差。

Method: 1. 训练一个动作条件视频生成模型作为真实环境的代理。
2. 提出一种推理方案：块自回归扩散变换器，具有可调上下文和解码范围长度，以减少世界模型中的误差积累。
3. 提出基于地面真实视频和生成视频之间一致性的度量标准，以确保世界模型遵循动作输入。
4. 使用世界模型进行策略评估，执行蒙特卡洛模拟，并使用视觉语言模型作为奖励函数。

Result: - WPE倾向于低估分布内动作的策略值，高估分布外动作的策略值。
- WPE保持了不同策略的相对排名。
- 在模拟真实机器人执行中，WPE在模仿机器人手臂运动方面具有高保真度。
- 模拟高度逼真的物体交互仍具挑战性。

Conclusion: 尽管存在局限性，世界模型可以作为评估机器人策略的一个起点，在实际部署前提供有价值的参考。

Abstract: Robotics has broad applications from automating house chores to taking care
of patients. However, evaluating robot control policies is challenging, as
real-world testing is expensive, while handcrafted simulations often fail to
accurately reflect real-world conditions, resulting in poor correlation between
simulated evaluation and real-world outcomes. In this work, we investigate
World-model-based Policy Evaluation (WPE). We first train an action-conditioned
video generation model as a proxy to real-world environments. To enable
efficient rollouts of hundreds of interactive steps while mitigating error
accumulation in the world model, we propose an inference scheme which we call
Blockwise-Autoregressive Diffusion Transformer with adjustable context and
decoding horizon lengths. To ensure that the world model indeed follows action
input, we propose metrics based on the agreement between the ground truth video
and generated video conditioned on the same sequence of actions to evaluate the
world model. We then use the world model for policy evaluation by performing
Monte Carlo rollouts in the world model while employing a vision-language model
(VLM) as a reward function. Interestingly, we found that WPE tends to
underestimate the policy values for in-distribution actions and overestimate
policy values for out-of-distribution actions. Nevertheless, WPE preserves the
relative rankings of different policies. In emulating real robot executions,
WPE achieves high fidelity in mimicing robot arm movements as in real videos,
while emulating highly realistic object interaction remains challenging.
Despite this limitation, we show that a world model can serve as a starting
point for evaluating robot policies before real-world deployment.

</details>


### [603] [DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2506.00819)
*Dawood Wasif,Terrence J Moore,Chandan K Reddy,Jin-Hee Cho*

Main category: cs.RO

TL;DR: DriveMind, a unified semantic reward framework for autonomous driving systems that integrates contrastive Vision-Language Model (VLM) encoder, novelty-triggered VLM encoder-decoder, hierarchical safety module and compact predictive world model. It outperforms baselines in CARLA Town 2 and demonstrates robust cross-domain alignment.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving systems lack interpretability and formal safety guarantees while recent vision-language-guided reinforcement learning methods are limited in adaptability to dynamic driving scenes.

Method: DriveMind integrates four main components: a contrastive Vision-Language Model (VLM) encoder for stepwise semantic anchoring; a novelty-triggered VLM encoder-decoder, fine-tuned via chain-of-thought (CoT) distillation, for dynamic prompt generation upon semantic drift; a hierarchical safety module enforcing kinematic constraints; and a compact predictive world model to reward alignment with anticipated ideal states.

Result: DriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route completion, and near-zero collisions in CARLA Town 2, outperforming baselines by over 4% in success rate. Its semantic reward generalizes zero-shot to real dash-cam data with minimal distributional shift.

Conclusion: DriveMind presents a promising approach to enhance the performance and safety of autonomous driving systems, demonstrating strong potential for real-world deployment.

Abstract: End-to-end autonomous driving systems map sensor data directly to control
commands, but remain opaque, lack interpretability, and offer no formal safety
guarantees. While recent vision-language-guided reinforcement learning (RL)
methods introduce semantic feedback, they often rely on static prompts and
fixed objectives, limiting adaptability to dynamic driving scenes. We present
DriveMind, a unified semantic reward framework that integrates: (i) a
contrastive Vision-Language Model (VLM) encoder for stepwise semantic
anchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via
chain-of-thought (CoT) distillation, for dynamic prompt generation upon
semantic drift; (iii) a hierarchical safety module enforcing kinematic
constraints (e.g., speed, lane centering, stability); and (iv) a compact
predictive world model to reward alignment with anticipated ideal states.
DriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route
completion, and near-zero collisions in CARLA Town 2, outperforming baselines
by over 4% in success rate. Its semantic reward generalizes zero-shot to real
dash-cam data with minimal distributional shift, demonstrating robust
cross-domain alignment and potential for real-world deployment.

</details>


### [604] [Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey](https://arxiv.org/abs/2506.00098)
*Edgar Welte,Rania Rayyes*

Main category: cs.RO

TL;DR: Dexterous manipulation in humanoid robotics requires efficient learning methods. This survey reviews existing approaches and highlights interactive imitation learning as a promising direction.


<details>
  <summary>Details</summary>
Motivation: Dexterous manipulation is crucial for humanoid robots to operate in human-centric environments, but traditional learning methods face challenges such as high-dimensional control, limited training data, and covariate shift.

Method: The paper provides a comprehensive review of learning-based methods for dexterous manipulation, including imitation learning, reinforcement learning, and hybrid approaches, with a focus on interactive imitation learning.

Result: Interactive imitation learning shows potential for enhancing dexterous manipulation, though its application remains limited. The survey identifies gaps and outlines future research directions.

Conclusion: Interactive imitation learning is a promising yet underexplored area for improving dexterous manipulation in humanoid robotics.

Abstract: Dexterous manipulation is a crucial yet highly complex challenge in humanoid
robotics, demanding precise, adaptable, and sample-efficient learning methods.
As humanoid robots are usually designed to operate in human-centric
environments and interact with everyday objects, mastering dexterous
manipulation is critical for real-world deployment. Traditional approaches,
such as reinforcement learning and imitation learning, have made significant
strides, but they often struggle due to the unique challenges of real-world
dexterous manipulation, including high-dimensional control, limited training
data, and covariate shift. This survey provides a comprehensive overview of
these challenges and reviews existing learning-based methods for dexterous
manipulation, spanning imitation learning, reinforcement learning, and hybrid
approaches. A promising yet underexplored direction is interactive imitation
learning, where human feedback actively refines a robot's behavior during
training. While interactive imitation learning has shown success in various
robotic tasks, its application to dexterous manipulation remains limited. To
address this gap, we examine current interactive imitation learning techniques
applied to other robotic tasks and discuss how these methods can be adapted to
enhance dexterous manipulation. By synthesizing state-of-the-art research, this
paper highlights key challenges, identifies gaps in current methodologies, and
outlines potential directions for leveraging interactive imitation learning to
improve dexterous robotic skills.

</details>


### [605] [Learning Aerodynamics for the Control of Flying Humanoid Robots](https://arxiv.org/abs/2506.00305)
*Antonello Paolino,Gabriele Nava,Fabio Di Natale,Fabio Bergonti,Punith Reddy Vanteddu,Donato Grassi,Luca Riccobene,Alex Zanotti,Renato Tognaccini,Gianluca Iaccarino,Daniele Pucci*

Main category: cs.RO

TL;DR: This paper explores the challenges and solutions in modeling and controlling aerodynamic forces for flying humanoid robots, presenting both technological and scientific contributions.


<details>
  <summary>Details</summary>
Motivation: To enhance the versatility of humanoid robots in diverse environments by providing them with aerial capabilities.

Method: 1. Developed iRonCub-Mk1, a jet-powered humanoid robot optimized for jet engine integration. 2. Conducted hardware modifications for wind tunnel experiments to measure aerodynamic forces and surface pressures. 3. Utilized Computational Fluid Dynamics (CFD) simulations to calculate aerodynamic forces, validated through wind tunnel tests. 4. Created an automated CFD framework to expand the aerodynamic dataset for machine learning models. 5. Integrated Deep Neural Network and linear regression models into a simulator for designing aerodynamic-aware controllers.

Result: Successful validation of aerodynamic models through flight simulations and balancing experiments on the physical prototype of iRonCub-Mk1.

Conclusion: The technological and scientific advancements presented provide a comprehensive approach to model and control aerodynamic forces for flying humanoid robots.

Abstract: Robots with multi-modal locomotion are an active research field due to their
versatility in diverse environments. In this context, additional actuation can
provide humanoid robots with aerial capabilities. Flying humanoid robots face
challenges in modeling and control, particularly with aerodynamic forces. This
paper addresses these challenges from a technological and scientific
standpoint. The technological contribution includes the mechanical design of
iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine
integration, and hardware modifications for wind tunnel experiments on humanoid
robots for precise aerodynamic forces and surface pressure measurements. The
scientific contribution offers a comprehensive approach to model and control
aerodynamic forces using classical and learning techniques. Computational Fluid
Dynamics (CFD) simulations calculate aerodynamic forces, validated through wind
tunnel experiments on iRonCub-Mk1. An automated CFD framework expands the
aerodynamic dataset, enabling the training of a Deep Neural Network and a
linear regression model. These models are integrated into a simulator for
designing aerodynamic-aware controllers, validated through flight simulations
and balancing experiments on the iRonCub-Mk1 physical prototype.

</details>


### [606] [Constrained Stein Variational Gradient Descent for Robot Perception, Planning, and Identification](https://arxiv.org/abs/2506.00589)
*Griffin Tabor,Tucker Hermans*

Main category: cs.RO

TL;DR: The paper introduces two novel frameworks applying constrained optimization principles to Stein variational gradient descent, enabling learning of approximate distributions without violating constraints for various robotic problems.


<details>
  <summary>Details</summary>
Motivation: Many core robotics problems can be framed as constrained optimization problems, with a need to handle uncertainty and identify multiple high-quality feasible solutions.

Method: Two novel frameworks are presented that apply principles of constrained optimization to the Stein variational gradient descent algorithm. These frameworks support multiple types of constrained optimizers and can handle arbitrary constraints.

Result: The frameworks successfully learn to approximate distributions without violating constraints for robot motion plans avoiding collisions, robot arm joint angles on the SE(3) manifold with table placement constraints, and object poses from point clouds with table placement constraints.

Conclusion: The proposed frameworks demonstrate effectiveness in handling various robotic problems while respecting all specified constraints.

Abstract: Many core problems in robotics can be framed as constrained optimization
problems. Often on these problems, the robotic system has uncertainty, or it
would be advantageous to identify multiple high quality feasible solutions. To
enable this, we present two novel frameworks for applying principles of
constrained optimization to the new variational inference algorithm Stein
variational gradient descent. Our general framework supports multiple types of
constrained optimizers and can handle arbitrary constraints. We demonstrate on
a variety of problems that we are able to learn to approximate distributions
without violating constraints. Specifically, we show that we can build
distributions of: robot motion plans that exactly avoid collisions, robot arm
joint angles on the SE(3) manifold with exact table placement constraints, and
object poses from point clouds with table placement constraints.

</details>


### [607] [Humanoid World Models: Open World Foundation Models for Humanoid Robotics](https://arxiv.org/abs/2506.01182)
*Muhammad Qasim Ali,Aditya Sridhar,Shahbuland Matiana,Alex Wong,Mohammad Al-Sharman*

Main category: cs.RO

TL;DR: This paper presents Humanoid World Models (HWM), which are lightweight, open-source video-based models that predict future egocentric observations given actions. Trained on 100 hours of humanoid demonstrations using Masked Transformers and FlowMatching, these models incorporate architectural variants with attention mechanisms and parameter sharing strategies that reduce model size by 33-53% with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: To enable humanoid robots to perform complex tasks in human-centered environments, robust predictive models are needed to reason about the outcomes of their actions.

Method: The authors introduce HWM, train generative models (Masked Transformers and FlowMatching) on humanoid demonstrations, explore different attention mechanisms and parameter sharing strategies to reduce model size while maintaining performance.

Result: Parameter sharing techniques reduce model size by 33-53% with minimal impact on performance or visual fidelity. The models can be trained and deployed using only 1-2 GPUs in academic or small lab settings.

Conclusion: HWM offers a practical solution for training predictive models for humanoid robots in resource-constrained environments without significant loss in performance.

Abstract: Humanoid robots have the potential to perform complex tasks in human centered
environments but require robust predictive models to reason about the outcomes
of their actions. We introduce Humanoid World Models (HWM) a family of
lightweight open source video based models that forecast future egocentric
observations conditioned on actions. We train two types of generative models
Masked Transformers and FlowMatching on 100 hours of humanoid demonstrations.
Additionally we explore architectural variants with different attention
mechanisms and parameter sharing strategies. Our parameter sharing techniques
reduce model size by 33 to 53 with minimal impact on performance or visual
fidelity. HWM is designed to be trained and deployed in practical academic and
small lab settings such as 1 to 2 GPUs.

</details>


### [608] [OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation](https://arxiv.org/abs/2506.01196)
*Ishika Singh,Ankit Goyal,Stan Birchfield,Dieter Fox,Animesh Garg,Valts Blukis*

Main category: cs.RO

TL;DR: The paper introduces OG-VLA, an architecture that merges VLA models with 3D-aware policies for robot actions guided by language instructions and multi-view observations. It demonstrates strong generalization and adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of 3D-aware robot policies in generalizing to unseen instructions, scenes, and objects, and the sensitivity of VLAs to camera and robot pose variations.

Method: OG-VLA projects diverse view observations into a point cloud, renders them from canonical views, and uses vision backbone, LLM, and image diffusion model to generate end-effector position and orientation images.

Result: Shows state-of-the-art generalization to unseen environments with over 40% relative improvements and maintains robust performance in seen settings. Also demonstrates real-world adaptation with few demonstrations.

Conclusion: OG-VLA successfully combines the strengths of VLAs and 3D-aware policies, improving generalization and real-world adaptability in robot manipulation tasks.

Abstract: We introduce OG-VLA, a novel architecture and learning framework that
combines the generalization strengths of Vision Language Action models (VLAs)
with the robustness of 3D-aware policies. We address the challenge of mapping
natural language instructions and multi-view RGBD observations to quasi-static
robot actions. 3D-aware robot policies achieve state-of-the-art performance on
precise robot manipulation tasks, but struggle with generalization to unseen
instructions, scenes, and objects. On the other hand, VLAs excel at
generalizing across instructions and scenes, but can be sensitive to camera and
robot pose variations. We leverage prior knowledge embedded in language and
vision foundation models to improve generalization of 3D-aware keyframe
policies. OG-VLA projects input observations from diverse views into a point
cloud which is then rendered from canonical orthographic views, ensuring input
view invariance and consistency between input and output spaces. These
canonical views are processed with a vision backbone, a Large Language Model
(LLM), and an image diffusion model to generate images that encode the next
position and orientation of the end-effector on the input scene. Evaluations on
the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization
to unseen environments, with over 40% relative improvements while maintaining
robust performance in seen settings. We also show real-world adaption in 3 to 5
demonstrations along with strong generalization. Videos and resources at
https://og-vla.github.io/

</details>


### [609] [Sparse Imagination for Efficient Visual World Model Planning](https://arxiv.org/abs/2506.01392)
*Junha Chun,Youngjoon Jeong,Taesup Kim*

Main category: cs.RO

TL;DR: Sparse Imagination for Efficient Visual World Model Planning proposes a method to enhance computational efficiency in world model based planning by reducing token processing during forward prediction. This significantly accelerates planning while maintaining control fidelity, making it suitable for real-time applications.


<details>
  <summary>Details</summary>
Motivation: World model based planning has shown great potential but is computationally expensive, especially for robotics with constrained resources.

Method: The paper introduces Sparse Imagination, which uses a sparsely trained vision-based world model with randomized grouped attention strategy to adaptively adjust the number of tokens processed according to available computational resources.

Result: Experimental results indicate that sparse imagination maintains task performance and greatly improves inference efficiency, allowing for real-time decision-making.

Conclusion: Sparse Imagination offers a promising solution for deploying world models in real-time scenarios by balancing efficiency and performance.

Abstract: World model based planning has significantly improved decision-making in
complex environments by enabling agents to simulate future states and make
informed choices. However, ensuring the prediction accuracy of world models
often demands substantial computational resources, posing a major challenge for
real-time applications. This computational burden is particularly restrictive
in robotics, where resources are severely constrained. To address this
limitation, we propose a Sparse Imagination for Efficient Visual World Model
Planning, which enhances computational efficiency by reducing the number of
tokens processed during forward prediction. Our method leverages a sparsely
trained vision-based world model based on transformers with randomized grouped
attention strategy, allowing the model to adaptively adjust the number of
tokens processed based on the computational resource. By enabling sparse
imagination (rollout), our approach significantly accelerates planning while
maintaining high control fidelity. Experimental results demonstrate that sparse
imagination preserves task performance while dramatically improving inference
efficiency, paving the way for the deployment of world models in real-time
decision-making scenarios.

</details>


### [610] [LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation](https://arxiv.org/abs/2506.01538)
*Guobin Zhu,Rui Zhou,Wenkang Ji,Shiyu Zhao*

Main category: cs.RO

TL;DR: This paper presents LAMARL, a novel approach combining Multi-Agent Reinforcement Learning (MARL) with Large Language Models (LLMs) to improve sample efficiency and automate reward function generation for multi-robot systems. Experiments on shape assembly show significant improvements in sample efficiency and task completion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of low sample efficiency and manual reward tuning in MARL for complex multi-robot tasks, by leveraging the capabilities of LLMs that have shown promise in single-robot settings.

Method: LAMARL consists of two modules: the first uses LLMs to automatically generate prior policy and reward functions through structured prompts based on Chain-of-Thought (CoT) and basic APIs; the second module applies MARL using these generated functions to guide robot policy training effectively.

Result: Experiments demonstrate that LAMARL improves sample efficiency by an average of 185.9% and enhances task completion in both simulation and real-world shape assembly tasks. Structured prompts improve LLM output success rates by 28.5%-67.5%.

Conclusion: LAMARL successfully integrates LLMs with MARL, significantly improving sample efficiency and task performance for multi-robot systems without requiring manual design.

Abstract: Although Multi-Agent Reinforcement Learning (MARL) is effective for complex
multi-robot tasks, it suffers from low sample efficiency and requires iterative
manual reward tuning. Large Language Models (LLMs) have shown promise in
single-robot settings, but their application in multi-robot systems remains
largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL)
approach, which integrates MARL with LLMs, significantly enhancing sample
efficiency without requiring manual design. LAMARL consists of two modules: the
first module leverages LLMs to fully automate the generation of prior policy
and reward functions. The second module is MARL, which uses the generated
functions to guide robot policy training effectively. On a shape assembly
benchmark, both simulation and real-world experiments demonstrate the unique
advantages of LAMARL. Ablation studies show that the prior policy improves
sample efficiency by an average of 185.9% and enhances task completion, while
structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM
output success rates by 28.5%-67.5%. Videos and code are available at
https://guobin-zhu.github.io/LLM-MARL

</details>


### [611] [FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens](https://arxiv.org/abs/2506.01583)
*Yiming Zhong,Yumeng Liu,Chuyang Xiao,Zemin Yang,Youzhuo Wang,Yufei Zhu,Ye Shi,Yujing Sun,Xinge Zhu,Yuexin Ma*

Main category: cs.RO

TL;DR: The paper proposes a new method for learning visuomotor policies by modeling hierarchical frequency components and introducing continuous latent representations, which improves accuracy and efficiency in robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for learning visuomotor policies have limitations in action representation and network architectures, leading to challenges in generating precise actions efficiently. The authors observe that representing actions in the frequency domain can better capture the structured nature of motion.

Method: The method involves progressively modeling hierarchical frequency components of actions and introducing continuous latent representations to maintain smoothness and continuity in the action space.

Result: Extensive experiments on 2D and 3D robotic manipulation benchmarks show that the proposed approach outperforms existing methods in terms of accuracy and efficiency.

Conclusion: The frequency-domain autoregressive framework with continuous tokens has great potential for generalized robotic manipulation.

Abstract: Learning effective visuomotor policies for robotic manipulation is
challenging, as it requires generating precise actions while maintaining
computational efficiency. Existing methods remain unsatisfactory due to
inherent limitations in the essential action representation and the basic
network architectures. We observe that representing actions in the frequency
domain captures the structured nature of motion more effectively: low-frequency
components reflect global movement patterns, while high-frequency components
encode fine local details. Additionally, robotic manipulation tasks of varying
complexity demand different levels of modeling precision across these frequency
bands. Motivated by this, we propose a novel paradigm for visuomotor policy
learning that progressively models hierarchical frequency components. To
further enhance precision, we introduce continuous latent representations that
maintain smoothness and continuity in the action space. Extensive experiments
across diverse 2D and 3D robotic manipulation benchmarks demonstrate that our
approach outperforms existing methods in both accuracy and efficiency,
showcasing the potential of a frequency-domain autoregressive framework with
continuous tokens for generalized robotic manipulation.

</details>


### [612] [WoMAP: World Models For Embodied Open-Vocabulary Object Localization](https://arxiv.org/abs/2506.01600)
*Tenny Yin,Zhiting Mei,Tao Sun,Lihan Zha,Emily Zhou,Jeremy Bao,Miyu Yamane,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.RO

TL;DR: WoMAP is introduced to enhance language-instructed active object localization by employing a real-to-sim-to-real pipeline, distilling dense rewards from object detectors, and leveraging a latent world model for action grounding.


<details>
  <summary>Details</summary>
Motivation: Current methods either lack generalization beyond demonstration datasets or fail to generate physically grounded actions.

Method: Uses Gaussian Splatting-based real-to-sim-to-real pipeline for data generation, distills dense rewards signals from open-vocabulary object detectors, and leverages a latent world model for dynamics and rewards prediction.

Result: Achieves superior performance in zero-shot object localization tasks with significantly higher success rates compared to VLM and diffusion policy baselines. Demonstrates strong generalization and sim-to-real transfer on a TidyBot.

Conclusion: WoMAP provides an effective solution for efficient exploration of partially observable environments in language-instructed active object localization.

Abstract: Language-instructed active object localization is a critical challenge for
robots, requiring efficient exploration of partially observable environments.
However, state-of-the-art approaches either struggle to generalize beyond
demonstration datasets (e.g., imitation learning methods) or fail to generate
physically grounded actions (e.g., VLMs). To address these limitations, we
introduce WoMAP (World Models for Active Perception): a recipe for training
open-vocabulary object localization policies that: (i) uses a Gaussian
Splatting-based real-to-sim-to-real pipeline for scalable data generation
without the need for expert demonstrations, (ii) distills dense rewards signals
from open-vocabulary object detectors, and (iii) leverages a latent world model
for dynamics and rewards prediction to ground high-level action proposals at
inference time. Rigorous simulation and hardware experiments demonstrate
WoMAP's superior performance in a broad range of zero-shot object localization
tasks, with more than 9x and 2x higher success rates compared to VLM and
diffusion policy baselines, respectively. Further, we show that WoMAP achieves
strong generalization and sim-to-real transfer on a TidyBot.

</details>


### [613] [Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces](https://arxiv.org/abs/2506.01635)
*Julian Richter,Christopher Erdös,Christian Scheurer,Jochen J. Steil,Niels Dehio*

Main category: cs.RO

TL;DR: The paper introduces Riemannian Time Warping (RTW) to align multiple signals by leveraging the geometric structure of Riemannian manifolds, showing superior performance in averaging and classification tasks compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Temporal alignment of signals through time warping is essential in various fields but has been primarily focused on Euclidean space data. There's a need for extending this concept to Riemannian manifolds for broader applications, especially in robotics.

Method: The authors propose Riemannian Time Warping (RTW), which considers the geometric structure of the Riemannian manifold where the data resides to efficiently align multiple signals.

Result: Experiments on both synthetic and real-world data indicate that RTW outperforms state-of-the-art baselines in averaging and classification tasks, including tests with an LBR iiwa robot.

Conclusion: Riemannian Time Warping provides an effective solution for temporal signal alignment on Riemannian manifolds, opening new possibilities for applications in robotics and other areas.

Abstract: Temporal alignment of multiple signals through time warping is crucial in
many fields, such as classification within speech recognition or robot motion
learning. Almost all related works are limited to data in Euclidean space.
Although an attempt was made in 2011 to adapt this concept to unit quaternions,
a general extension to Riemannian manifolds remains absent. Given its
importance for numerous applications in robotics and beyond, we introduce
Riemannian Time Warping~(RTW). This novel approach efficiently aligns multiple
signals by considering the geometric structure of the Riemannian manifold in
which the data is embedded. Extensive experiments on synthetic and real-world
data, including tests with an LBR iiwa robot, demonstrate that RTW consistently
outperforms state-of-the-art baselines in both averaging and classification
tasks.

</details>


### [614] [Feel the Force: Contact-Driven Learning from Humans](https://arxiv.org/abs/2506.01944)
*Ademi Adeniji,Zhuoran Chen,Vincent Liu,Venkatesh Pattabiraman,Raunaq Bhirangi,Siddhant Haldar,Pieter Abbeel,Lerrel Pinto*

Main category: cs.RO

TL;DR: This paper presents FeelTheForce (FTF), a robot learning system that models human tactile behavior to learn force-sensitive manipulation, achieving a 77% success rate across tasks.


<details>
  <summary>Details</summary>
Motivation: Controlling fine-grained forces during manipulation is a core challenge in robotics. Policies learned from robot-collected data or simulation struggle to generalize across real-world interactions. Learning directly from humans offers a scalable solution but visual demonstrations alone lack the information needed to infer precise contact forces.

Method: Using a tactile glove to measure contact forces and a vision-based model to estimate hand pose, they train a closed-loop policy that continuously predicts the forces needed for manipulation. This policy is re-targeted to a Franka Panda robot with tactile gripper sensors using shared visual and action representations. A PD controller modulates gripper closure to track predicted forces.

Result: Their approach grounds robust low-level force control in scalable human supervision, achieving a 77% success rate across 5 force-sensitive manipulation tasks.

Conclusion: FeelTheForce (FTF) successfully models human tactile behavior for robots to perform force-sensitive manipulation tasks with high success rates.

Abstract: Controlling fine-grained forces during manipulation remains a core challenge
in robotics. While robot policies learned from robot-collected data or
simulation show promise, they struggle to generalize across the diverse range
of real-world interactions. Learning directly from humans offers a scalable
solution, enabling demonstrators to perform skills in their natural embodiment
and in everyday environments. However, visual demonstrations alone lack the
information needed to infer precise contact forces. We present FeelTheForce
(FTF): a robot learning system that models human tactile behavior to learn
force-sensitive manipulation. Using a tactile glove to measure contact forces
and a vision-based model to estimate hand pose, we train a closed-loop policy
that continuously predicts the forces needed for manipulation. This policy is
re-targeted to a Franka Panda robot with tactile gripper sensors using shared
visual and action representations. At execution, a PD controller modulates
gripper closure to track predicted forces-enabling precise, force-aware
control. Our approach grounds robust low-level force control in scalable human
supervision, achieving a 77% success rate across 5 force-sensitive manipulation
tasks. Code and videos are available at https://feel-the-force-ftf.github.io.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [615] [Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks](https://arxiv.org/abs/2506.00856)
*Qiang Chen,Tianyang Han,Jin Li,Ye Luo,Yuxiao Wu,Xiaowei Zhang,Tuo Zhou*

Main category: econ.EM

TL;DR: The paper explores the ability of an agentic AI to perform complex econometric analysis. An 'Econometrics AI Agent' built on MetaGPT shows superior performance in various tasks, outperforming benchmark LLMs and general-purpose agents. It establishes a testbed for AI's impact on social science research, enhances reproducibility, and offers teaching applications.


<details>
  <summary>Details</summary>
Motivation: To evaluate if AI can effectively replace human expertise in performing complex econometric analyses.

Method: Developed an 'Econometrics AI Agent' based on the MetaGPT framework with capabilities in strategic task planning, code generation and execution, error-based reflection, and iterative refinement through conversations. Performance was evaluated using two datasets constructed from academic coursework and published research papers.

Result: The domain-specialized agent significantly outperforms both benchmark large language models and general-purpose AI agents in econometric analysis tasks.

Conclusion: This work creates a platform for exploring AI's role in social science research, enabling cost-effective integration of domain expertise, and making advanced econometric methods accessible while enhancing research reproducibility and offering educational applications.

Abstract: Can AI effectively perform complex econometric analysis traditionally
requiring human expertise? This paper evaluates an agentic AI's capability to
master econometrics, focusing on empirical analysis performance. We develop an
``Econometrics AI Agent'' built on the open-source MetaGPT framework. This
agent exhibits outstanding performance in: (1) planning econometric tasks
strategically, (2) generating and executing code, (3) employing error-based
reflection for improved robustness, and (4) allowing iterative refinement
through multi-round conversations. We construct two datasets from academic
coursework materials and published research papers to evaluate performance
against real-world challenges. Comparative testing shows our domain-specialized
agent significantly outperforms both benchmark large language models (LLMs) and
general-purpose AI agents. This work establishes a testbed for exploring AI's
impact on social science research and enables cost-effective integration of
domain expertise, making advanced econometric methods accessible to users with
minimal coding expertise. Furthermore, our agent enhances research
reproducibility and offers promising pedagogical applications for econometrics
teaching.

</details>


### [616] [Stock Market Telepathy: Graph Neural Networks Predicting the Secret Conversations between MINT and G7 Countries](https://arxiv.org/abs/2506.01945)
*Nurbanu Bursa*

Main category: econ.EM

TL;DR: The paper explores the influence of G7 and MINT countries on stock market indices using MTGNN, revealing key influencers and superior forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand the relationships between financial markets of emerging (MINT) and developed (G7) economies for better prediction of stock price movements.

Method: Used MTGNN algorithm to examine main stock market indices of G7 and MINT countries from 2012 to 2024, considering complex spatio-temporal connections in multivariate time series.

Result: MTGNN showed that US and Canada are the most influential G7 countries, Indonesia and T"urkiye are the most influential MINT countries, and outperformed traditional methods in forecasting.

Conclusion: The study provides insights into economic blocks' markets and presents an effective approach to analyzing global stock market dynamics using MTGNN.

Abstract: Emerging economies, particularly the MINT countries (Mexico, Indonesia,
Nigeria, and T\"urkiye), are gaining influence in global stock markets,
although they remain susceptible to the economic conditions of developed
countries like the G7 (Canada, France, Germany, Italy, Japan, the United
Kingdom, and the United States). This interconnectedness and sensitivity of
financial markets make understanding these relationships crucial for investors
and policymakers to predict stock price movements accurately. To this end, we
examined the main stock market indices of G7 and MINT countries from 2012 to
2024, using a recent graph neural network (GNN) algorithm called multivariate
time series forecasting with graph neural network (MTGNN). This method allows
for considering complex spatio-temporal connections in multivariate time
series. In the implementations, MTGNN revealed that the US and Canada are the
most influential G7 countries regarding stock indices in the forecasting
process, and Indonesia and T\"urkiye are the most influential MINT countries.
Additionally, our results showed that MTGNN outperformed traditional methods in
forecasting the prices of stock market indices for MINT and G7 countries.
Consequently, the study offers valuable insights into economic blocks' markets
and presents a compelling empirical approach to analyzing global stock market
dynamics using MTGNN.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [617] [Advanced Nanostructured Topical Therapeutics for Psoriasis: Strategic Synthesis, Multimodal Characterization, and Preliminary Pharmacodynamic Profiling](https://arxiv.org/abs/2506.01572)
*Iqra Yousaf,Aqsa Yousaf*

Main category: physics.med-ph

TL;DR: This paper explores a new topical treatment for psoriasis using a combination of metal oxide nanoparticles and natural plant extracts. Testing on an animal model showed faster wound healing and reduced inflammation compared to control groups, suggesting promise for future treatments.


<details>
  <summary>Details</summary>
Motivation: Psoriasis is a chronic inflammatory skin disease that remains difficult to treat, prompting the search for more effective therapies.

Method: The study developed a gel containing cerium oxide (CeO2), zinc oxide (ZnO), and silver (Ag) nanoparticles combined with plant extracts from bitter melon, ginger, and neem in a fish collagen and agar base. The nanoparticles were characterized using UV-Vis spectroscopy, DLS, FTIR, and SEM. The formulation was then tested on an animal model of psoriasis.

Result: The treated group experienced faster wound healing and reduced inflammation compared to both placebo and untreated groups, with statistically significant results observed from Day 3, becoming more pronounced by Day 14.

Conclusion: The combination of nanoparticles with plant-based components in a topical gel shows potential as a new approach to treating psoriasis, warranting further studies to assess long-term safety and effectiveness.

Abstract: Psoriasis is a long-term inflammatory skin disease that remains difficult to
treat. In this study, we developed a new topical treatment by combining metal
oxide nanoparticles: cerium oxide (CeO2), zinc oxide (ZnO), and silver (Ag),
with natural plant extracts in a gel made from fish collagen and agar. The
nanoparticles were characterized using UV-Vis spectroscopy, dynamic light
scattering (DLS), Fourier-transform infrared spectroscopy (FTIR), and scanning
electron microscopy (SEM), showing good stability and a uniform particle size
distribution (ZnO averaged 66 nm).
  To enhance therapeutic potential, the gel was enriched with plant-derived
antioxidants from bitter melon, ginger, and neem. This formulation was tested
on an animal model of psoriasis. The treated group exhibited faster wound
healing and reduced inflammation compared to both placebo and untreated groups,
with statistically significant results (p < 0.01 to p < 0.001) observed from
Day 3, becoming more pronounced by Day 14.
  These results indicate that the combination of nanoparticles with plant-based
components in a topical gel may provide a promising new approach to psoriasis
treatment. Further studies are recommended to evaluate long-term safety and
therapeutic effectiveness.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [618] [Recover Experimental Data with Selection Bias using Counterfactual Logic](https://arxiv.org/abs/2506.00335)
*Jingyang He,Shuai Wang,Ang Li*

Main category: stat.ME

TL;DR: The paper addresses the challenge of selection bias in causal inference by exploring the recoverability of unbiased distributions from biased data, proposing methods that leverage partially unbiased observational data and counterfactual worlds to mitigate selection bias.


<details>
  <summary>Details</summary>
Motivation: Selection bias significantly impacts the validity of causal inference. Existing methods for recovering unbiased distributions face limitations due to complexity and reliance on observational data.

Method: The authors use Structural Causal Models (SCMs) to construct counterfactual worlds and derive graphical and theoretical criteria to determine if experimental distributions are unaffected by selection bias. They also propose methods to leverage partially unbiased observational data to recover unbiased distributions.

Result: Simulation studies show the practical utility of the proposed approach in mitigating selection bias in various realistic research scenarios.

Conclusion: The findings provide concrete guidance for addressing selection bias in applied causal inference, enhancing the robustness of causal conclusions.

Abstract: Selection bias, arising from the systematic inclusion or exclusion of certain
samples, poses a significant challenge to the validity of causal inference.
While Bareinboim et al. introduced methods for recovering unbiased
observational and interventional distributions from biased data using partial
external information, the complexity of the backdoor adjustment and the
method's strong reliance on observational data limit its applicability in many
practical settings. In this paper, we formally discover the recoverability of
$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly
constructing counterfactual worlds via Structural Causal Models (SCMs), we
analyze how selection mechanisms in the observational world propagate to the
counterfactual domain. We derive a complete set of graphical and theoretical
criteria to determine that the experimental distribution remain unaffected by
selection bias. Furthermore, we propose principled methods for leveraging
partially unbiased observational data to recover $P(Y^*_{x^*})$ from biased
experimental datasets. Simulation studies replicating realistic research
scenarios demonstrate the practical utility of our approach, offering concrete
guidance for mitigating selection bias in applied causal inference.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [619] [Video Signature: In-generation Watermarking for Latent Video Diffusion Models](https://arxiv.org/abs/2506.00652)
*Yu Huang,Junhao Chen,Qi Zheng,Hanqian Li,Shuliang Liu,Xuming Hu*

Main category: cs.CV

TL;DR: VIDSIG is an in-generation watermarking method for latent video diffusion models which enables implicit and adaptive watermark integration during generation, achieving the best overall performance in watermark extraction, visual quality, and generation efficiency.


<details>
  <summary>Details</summary>
Motivation: The rapid development of AIGC has led to significant progress in video generation but also raises serious concerns about intellectual property protection and reliable content tracing.

Method: Partially fine-tune the latent decoder with Perturbation-Aware Suppression (PAS) pre-identifying and freezing perceptually sensitive layers to preserve visual quality. Introduce a lightweight Temporal Alignment module that guides the decoder to generate coherent frame sequences during fine-tuning.

Result: Experimental results show that VIDSIG achieves the best overall performance in watermark extraction, visual quality, and generation efficiency. It also demonstrates strong robustness against both spatial and temporal tampering.

Conclusion: VIDSIG addresses the issues of existing methods for video generation watermarking by enabling implicit and adaptive watermark integration during generation.

Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) has
led to significant progress in video generation but also raises serious
concerns about intellectual property protection and reliable content tracing.
Watermarking is a widely adopted solution to this issue, but existing methods
for video generation mainly follow a post-generation paradigm, which introduces
additional computational overhead and often fails to effectively balance the
trade-off between video quality and watermark extraction. To address these
issues, we propose Video Signature (VIDSIG), an in-generation watermarking
method for latent video diffusion models, which enables implicit and adaptive
watermark integration during generation. Specifically, we achieve this by
partially fine-tuning the latent decoder, where Perturbation-Aware Suppression
(PAS) pre-identifies and freezes perceptually sensitive layers to preserve
visual quality. Beyond spatial fidelity, we further enhance temporal
consistency by introducing a lightweight Temporal Alignment module that guides
the decoder to generate coherent frame sequences during fine-tuning.
Experimental results show that VIDSIG achieves the best overall performance in
watermark extraction, visual quality, and generation efficiency. It also
demonstrates strong robustness against both spatial and temporal tampering,
highlighting its practicality in real-world scenarios.

</details>


### [620] [CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack](https://arxiv.org/abs/2506.00978)
*Zhan Li,Mingyu Zhao,Xin Dong,Haibin Ling,Bingyao Huang*

Main category: cs.CV

TL;DR: CAPAA introduces a novel method for projector-based adversarial attacks that works effectively across multiple classifiers and varying camera poses, achieving higher attack success rates and greater stealthiness.


<details>
  <summary>Details</summary>
Motivation: Existing projector-based adversarial attack methods are limited by their focus on individual classifiers and fixed camera poses, which reduces their effectiveness when new classifiers or camera poses are introduced.

Method: The paper develops Classifier-Agnostic Projector-Based Adversarial Attack (CAPAA) with a new classifier-agnostic adversarial loss and optimization framework. It aggregates loss gradients from multiple classifiers and uses an attention-based gradient weighting mechanism to concentrate perturbations on regions of high classification activation.

Result: Experimental evaluations show that CAPAA achieves both a higher attack success rate and greater stealthiness compared to existing methods.

Conclusion: CAPAA provides an effective solution for projector-based adversarial attacks in complex scenarios involving multiple classifiers and varying camera poses.

Abstract: Projector-based adversarial attack aims to project carefully designed light
patterns (i.e., adversarial projections) onto scenes to deceive deep image
classifiers. It has potential applications in privacy protection and the
development of more robust classifiers. However, existing approaches primarily
focus on individual classifiers and fixed camera poses, often neglecting the
complexities of multi-classifier systems and scenarios with varying camera
poses. This limitation reduces their effectiveness when introducing new
classifiers or camera poses. In this paper, we introduce Classifier-Agnostic
Projector-Based Adversarial Attack (CAPAA) to address these issues. First, we
develop a novel classifier-agnostic adversarial loss and optimization framework
that aggregates adversarial and stealthiness loss gradients from multiple
classifiers. Then, we propose an attention-based gradient weighting mechanism
that concentrates perturbations on regions of high classification activation,
thereby improving the robustness of adversarial projections when applied to
scenes with varying camera poses. Our extensive experimental evaluations
demonstrate that CAPAA achieves both a higher attack success rate and greater
stealthiness compared to existing baselines. Codes are available at:
https://github.com/ZhanLiQxQ/CAPAA.

</details>


### [621] [Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches](https://arxiv.org/abs/2506.00154)
*Agustín Roca,Gastón Castro,Gabriel Torre,Leonardo J. Colombo,Ignacio Mas,Javier Pereira,Juan I. Giribet*

Main category: cs.CV

TL;DR: This study compares state-of-the-art neural networks including YOLOv11 and RT-DETR for detecting marsh deer in UAV imagery, where specimens occupy a very small portion of the image and are occluded by vegetation.


<details>
  <summary>Details</summary>
Motivation: To improve UAV-based wildlife monitoring and conservation strategies through scalable and accurate AI-driven detection systems.

Method: Comparing the performance of state-of-the-art neural networks (YOLOv11 and RT-DETR) for detecting marsh deer in UAV imagery. Adding precise segmentation masks for datasets enabling fine-grained training of a YOLO model with a segmentation head included.

Result: Incorporating the segmentation head achieves superior detection performance.

Conclusion: This work contributes valuable insights for improving UAV-based wildlife monitoring and conservation strategies through scalable and accurate AI-driven detection systems.

Abstract: This study compares the performance of state-of-the-art neural networks
including variants of the YOLOv11 and RT-DETR models for detecting marsh deer
in UAV imagery, in scenarios where specimens occupy a very small portion of the
image and are occluded by vegetation. We extend previous analysis adding
precise segmentation masks for our datasets enabling a fine-grained training of
a YOLO model with a segmentation head included. Experimental results show the
effectiveness of incorporating the segmentation head achieving superior
detection performance. This work contributes valuable insights for improving
UAV-based wildlife monitoring and conservation strategies through scalable and
accurate AI-driven detection systems.

</details>


### [622] [Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes](https://arxiv.org/abs/2506.00227)
*Anthony Gosselin,Ge Ya Luo,Luis Lara,Florian Golemo,Derek Nowrouzezahrai,Liam Paull,Alexia Jolicoeur-Martineau,Christopher Pal*

Main category: cs.CV

TL;DR: Ctrl-Crash is a new model that can generate realistic car crash videos with controllable factors like bounding boxes, crash types, and initial image frame. It uses classifier-free guidance for fine-grained control and performs better than previous methods in both quantitative and qualitative evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion techniques have difficulty generating realistic car crash images due to the lack of accident data in driving datasets, which hinders improvements in traffic safety.

Method: The proposed method, Ctrl-Crash, is a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. It allows counterfactual scenario generation and leverages classifier-free guidance with independently tunable scales for each conditioning signal.

Result: Ctrl-Crash achieves state-of-the-art performance in quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on human evaluation of physical realism and video quality.

Conclusion: Ctrl-Crash provides a significant advancement in generating realistic and controllable car crash simulations, contributing to improvements in traffic safety.

Abstract: Video diffusion techniques have advanced significantly in recent years;
however, they struggle to generate realistic imagery of car crashes due to the
scarcity of accident events in most driving datasets. Improving traffic safety
requires realistic and controllable accident simulations. To tackle the
problem, we propose Ctrl-Crash, a controllable car crash video generation model
that conditions on signals such as bounding boxes, crash types, and an initial
image frame. Our approach enables counterfactual scenario generation where
minor variations in input can lead to dramatically different crash outcomes. To
support fine-grained control at inference time, we leverage classifier-free
guidance with independently tunable scales for each conditioning signal.
Ctrl-Crash achieves state-of-the-art performance across quantitative video
quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a
human-evaluation of physical realism and video quality compared to prior
diffusion-based methods.

</details>


### [623] [Latent Guidance in Diffusion Models for Perceptual Evaluations](https://arxiv.org/abs/2506.00327)
*Shreshth Saini,Ru-Ling Liao,Yan Ye,Alan C. Bovik*

Main category: cs.CV

TL;DR: 尽管最近在生成高维图像数据和执行各种下游任务的潜在扩散模型方面取得了进展，但在无参考图像质量评估（NR-IQA）任务中，对这些模型内的感知一致性进行探索的研究很少。本文假设潜在扩散模型在数据流形内隐式地表现出感知上一致的局部区域，并基于此提出了一种名为感知流形引导（PMG）的算法。该方法利用预训练的潜在扩散模型和感知质量特征，从去噪U-Net中获得与感知一致的多尺度和多时间步特征图。实验表明，这些超特征在IQA任务中与人类感知具有高度相关性。我们的方法可以应用于任何现有的预训练潜在扩散模型，并且易于集成。据我们所知，这是第一篇关于使用感知特征引导扩散模型用于NR-IQA的工作。广泛的实验证明，我们的方法LGDM在IQA数据集上达到了最先进的性能，突显了扩散模型在NR-IQA任务中的优越泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在无参考图像质量评估（NR-IQA）任务中，潜在扩散模型内的感知一致性尚未得到充分探索。

Method: 提出了一种名为感知流形引导（PMG）的算法，利用预训练的潜在扩散模型和感知质量特征，从去噪U-Net中获得与感知一致的多尺度和多时间步特征图。

Result: 实验结果表明，这些超特征与人类感知具有高度相关性，LGDM方法在IQA数据集上达到了最先进的性能。

Conclusion: 本研究首次将感知特征引入扩散模型用于NR-IQA任务，证明了其优越的泛化能力和应用潜力。

Abstract: Despite recent advancements in latent diffusion models that generate
high-dimensional image data and perform various downstream tasks, there has
been little exploration into perceptual consistency within these models on the
task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we
hypothesize that latent diffusion models implicitly exhibit perceptually
consistent local regions within the data manifold. We leverage this insight to
guide on-manifold sampling using perceptual features and input measurements.
Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that
utilizes pretrained latent diffusion models and perceptual quality features to
obtain perceptually consistent multi-scale and multi-timestep feature maps from
the denoising U-Net. We empirically demonstrate that these hyperfeatures
exhibit high correlation with human perception in IQA tasks. Our method can be
applied to any existing pretrained latent diffusion model and is
straightforward to integrate. To the best of our knowledge, this paper is the
first work on guiding diffusion model with perceptual features for NR-IQA.
Extensive experiments on IQA datasets show that our method, LGDM, achieves
state-of-the-art performance, underscoring the superior generalization
capabilities of diffusion models for NR-IQA tasks.

</details>


### [624] [Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models](https://arxiv.org/abs/2506.00607)
*JungWoo Chae,Jiyoon Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: The paper proposes a parallel rescaling technique for personalized diffusion models which improves prompt alignment and visual fidelity without needing additional training data or annotations.


<details>
  <summary>Details</summary>
Motivation: Personalizing diffusion models to specific users or concepts is challenging, especially with few reference images, due to overfitting and misalignment issues in existing methods.

Method: The method involves decomposing the consistency guidance signal into parallel and orthogonal components relative to classifier free guidance (CFG) and rescaling the parallel component to minimize interference with CFG while preserving subject identity.

Result: Extensive experiments demonstrate improved prompt alignment and visual fidelity compared to baseline methods, even on challenging stylized prompts.

Conclusion: Parallel rescaled guidance shows potential for more stable and accurate personalization for diverse user inputs.

Abstract: Personalizing diffusion models to specific users or concepts remains
challenging, particularly when only a few reference images are available.
Existing methods such as DreamBooth and Textual Inversion often overfit to
limited data, causing misalignment between generated images and text prompts
when attempting to balance identity fidelity with prompt adherence. While
Direct Consistency Optimization (DCO) with its consistency-guided sampling
partially alleviates this issue, it still struggles with complex or stylized
prompts. In this paper, we propose a parallel rescaling technique for
personalized diffusion models. Our approach explicitly decomposes the
consistency guidance signal into parallel and orthogonal components relative to
classifier free guidance (CFG). By rescaling the parallel component, we
minimize disruptive interference with CFG while preserving the subject's
identity. Unlike prior personalization methods, our technique does not require
additional training data or expensive annotations. Extensive experiments show
improved prompt alignment and visual fidelity compared to baseline methods,
even on challenging stylized prompts. These findings highlight the potential of
parallel rescaled guidance to yield more stable and accurate personalization
for diverse user inputs.

</details>


### [625] [Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining](https://arxiv.org/abs/2506.00633)
*Daniele Molino,Camillo Maria Caruso,Filippo Ruffini,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 本研究提出了一种结合潜在扩散模型与3D对比视觉-语言预训练方案的新型Text-to-CT生成架构，通过在CT-RATE数据集上的评估，证明了其在图像保真度、临床相关性和语义对齐方面的优越性能，并展示了合成CT扫描在增强真实数据以提升下游诊断性能方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管文本条件生成模型在2D医学影像（如胸部X光片）的合成上取得了显著进展，但将文本到图像的生成扩展到高维度、解剖结构复杂且缺乏强大框架的3D CT影像中仍是一个重大挑战。

Method: 研究引入了一种新的Text-to-CT生成架构，该架构融合了一个潜在扩散模型和一个3D对比视觉-语言预训练方案。采用双编码器CLIP风格模型，在配对的CT体积和放射学报告上进行训练，以建立共享嵌入空间作为生成的条件输入。通过预先训练好的体积VAE将CT体积压缩到低维潜在空间，从而实现高效的3D去噪扩散，而无需外部超分辨率阶段。

Result: 在CT-RATE数据集上的评估显示，该方法在图像保真度、临床相关性和语义对齐方面具有竞争力的表现，显著优于先前的文本到CT生成基线。此外，研究表明，通过该框架合成的CT扫描能够有效增强真实数据，提高下游诊断性能。

Conclusion: 结果表明，模态特定的视觉-语言对齐是高质量3D医学图像生成的关键组成部分。通过整合对比预训练和体积扩散，该方法为从文本合成具有临床意义的CT体积提供了一个可扩展且可控的解决方案，为数据增强、医学教育和自动临床模拟等新应用铺平了道路。

Abstract: Objective: While recent advances in text-conditioned generative models have
enabled the synthesis of realistic medical images, progress has been largely
confined to 2D modalities such as chest X-rays. Extending text-to-image
generation to volumetric Computed Tomography (CT) remains a significant
challenge, due to its high dimensionality, anatomical complexity, and the
absence of robust frameworks that align vision-language data in 3D medical
imaging. Methods: We introduce a novel architecture for Text-to-CT generation
that combines a latent diffusion model with a 3D contrastive vision-language
pretraining scheme. Our approach leverages a dual-encoder CLIP-style model
trained on paired CT volumes and radiology reports to establish a shared
embedding space, which serves as the conditioning input for generation. CT
volumes are compressed into a low-dimensional latent space via a pretrained
volumetric VAE, enabling efficient 3D denoising diffusion without requiring
external super-resolution stages. Results: We evaluate our method on the
CT-RATE dataset and conduct a comprehensive assessment of image fidelity,
clinical relevance, and semantic alignment. Our model achieves competitive
performance across all tasks, significantly outperforming prior baselines for
text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by
our framework can effectively augment real data, improving downstream
diagnostic performance. Conclusion: Our results show that modality-specific
vision-language alignment is a key component for high-quality 3D medical image
generation. By integrating contrastive pretraining and volumetric diffusion,
our method offers a scalable and controllable solution for synthesizing
clinically meaningful CT volumes from text, paving the way for new applications
in data augmentation, medical education, and automated clinical simulation.

</details>


### [626] [From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models](https://arxiv.org/abs/2506.00718)
*Tianqin Li,Ziqi Wen,Leiran Song,Jun Liu,Zhi Jing,Tai Sing Lee*

Main category: cs.CV

TL;DR: 现代视觉模型在特定训练条件下可以展现与格式塔法则一致的行为，例如使用掩码自动编码（MAE）训练的视觉变压器（ViTs）。这些模型在扭曲空间关系测试平台（DiSRT）上表现出对全局空间结构的高度敏感性，有时甚至超过人类表现。然而，分类微调会削弱这种能力，而Top-K激活稀疏机制能够恢复全局敏感性。


<details>
  <summary>Details</summary>
Motivation: 研究人类视觉如何利用局部线索组织成全局形式，并探索现代视觉模型是否能展现出类似的行为，以及这些行为在何种训练条件下出现。

Method: 通过引入Distorted Spatial Relationship Testbench (DiSRT)，评估模型对全局空间扰动的敏感性。使用Vision Transformers (ViTs) 和 ConvNeXt 模型，比较自监督和监督训练条件下的性能差异。进一步探讨分类微调和Top-K激活稀疏机制对模型性能的影响。

Result: 自监督训练（如MAE、CLIP）的模型比监督基线模型表现更好，在某些情况下甚至超过人类水平。ConvNeXt模型也展现了与格式塔法则一致的表示方式，表明这种敏感性不依赖于注意力架构。然而，分类微调降低了模型的全局敏感性，而Top-K激活稀疏机制有助于恢复这种能力。

Conclusion: 特定训练条件可以促进或抑制视觉模型的格式塔样感知，DiSRT可作为诊断不同模型对全局结构敏感性的工具。

Abstract: Human vision organizes local cues into coherent global forms using Gestalt
principles like closure, proximity, and figure-ground assignment -- functions
reliant on global spatial structure. We investigate whether modern vision
models show similar behaviors, and under what training conditions these emerge.
We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE)
exhibit activation patterns consistent with Gestalt laws, including illusory
contour completion, convexity preference, and dynamic figure-ground
segregation. To probe the computational basis, we hypothesize that modeling
global dependencies is necessary for Gestalt-like organization. We introduce
the Distorted Spatial Relationship Testbench (DiSRT), which evaluates
sensitivity to global spatial perturbations while preserving local textures.
Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform
supervised baselines and sometimes even exceed human performance. ConvNeXt
models trained with MAE also exhibit Gestalt-compatible representations,
suggesting such sensitivity can arise without attention architectures. However,
classification finetuning degrades this ability. Inspired by biological vision,
we show that a Top-K activation sparsity mechanism can restore global
sensitivity. Our findings identify training conditions that promote or suppress
Gestalt-like perception and establish DiSRT as a diagnostic for global
structure sensitivity across models.

</details>


### [627] [ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary](https://arxiv.org/abs/2506.00742)
*Zeqi Gu,Yin Cui,Zhaoshuo Li,Fangyin Wei,Yunhao Ge,Jinwei Gu,Ming-Yu Liu,Abe Davis,Yifan Ding*

Main category: cs.CV

TL;DR: ArtiScene is a training-free automated pipeline for scene design that uses 2D images as intermediaries to guide 3D synthesis, outperforming benchmarks in layout and aesthetic quality.


<details>
  <summary>Details</summary>
Motivation: Designing 3D scenes traditionally requires artistic expertise and proficiency with complex software. Although recent text-to-3D methods have simplified this process, their performance is limited by the lack of high-quality 3D data.

Method: The method generates 2D images from a scene description, extracts object shape and appearance to create 3D models, and assembles them into the final scene using geometry, position, and pose information from the intermediary image.

Result: ArtiScene outperforms state-of-the-art benchmarks in layout and aesthetic quality by quantitative metrics, with a 74.89% winning rate in user studies and 95.07% in GPT-4o evaluation.

Conclusion: ArtiScene provides a generalizable solution for a wide range of scenes and styles, offering significant improvements in both layout and aesthetic quality.

Abstract: Designing 3D scenes is traditionally a challenging task that demands both
artistic expertise and proficiency with complex software. Recent advances in
text-to-3D generation have greatly simplified this process by letting users
create scenes based on simple text descriptions. However, as these methods
generally require extra training or in-context learning, their performance is
often hindered by the limited availability of high-quality 3D data. In
contrast, modern text-to-image models learned from web-scale images can
generate scenes with diverse, reliable spatial layouts and consistent, visually
appealing styles. Our key insight is that instead of learning directly from 3D
scenes, we can leverage generated 2D images as an intermediary to guide 3D
synthesis. In light of this, we introduce ArtiScene, a training-free automated
pipeline for scene design that integrates the flexibility of free-form
text-to-image generation with the diversity and reliability of 2D intermediary
layouts.
  First, we generate 2D images from a scene description, then extract the shape
and appearance of objects to create 3D models. These models are assembled into
the final scene using geometry, position, and pose information derived from the
same intermediary image. Being generalizable to a wide range of scenes and
styles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in
layout and aesthetic quality by quantitative metrics. It also averages a 74.89%
winning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project
page: https://artiscene-cvpr.github.io/

</details>


### [628] [L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning](https://arxiv.org/abs/2506.00816)
*Xiang Zhang,Run He,Jiao Chen,Di Fang,Ming Li,Ziqian Zeng,Cen Chen,Huiping Zhuang*

Main category: cs.CV

TL;DR: In multi-label class-incremental learning, the model faces issues like label absence and class imbalance. This paper presents Label-Augmented Analytic Adaptation (L3A), which includes pseudo-label and weighted analytic classifier modules to solve these problems without storing past samples. Experiments show L3A performs better than existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in multi-label class-incremental learning such as label absence and class imbalance.

Method: Propose Label-Augmented Analytic Adaptation (L3A) with two key modules - pseudo-label (PL) module for label augmentation and weighted analytic classifier (WAC) for adaptively balancing class contribution.

Result: Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms existing methods in MLCIL tasks.

Conclusion: L3A is an effective approach for multi-label class-incremental learning without storing past samples.

Abstract: Class-incremental learning (CIL) enables models to learn new classes
continually without forgetting previously acquired knowledge. Multi-label CIL
(MLCIL) extends CIL to a real-world scenario where each sample may belong to
multiple classes, introducing several challenges: label absence, which leads to
incomplete historical information due to missing labels, and class imbalance,
which results in the model bias toward majority classes. To address these
challenges, we propose Label-Augmented Analytic Adaptation (L3A), an
exemplar-free approach without storing past samples. L3A integrates two key
modules. The pseudo-label (PL) module implements label augmentation by
generating pseudo-labels for current phase samples, addressing the label
absence problem. The weighted analytic classifier (WAC) derives a closed-form
solution for neural networks. It introduces sample-specific weights to
adaptively balance the class contribution and mitigate class imbalance.
Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms
existing methods in MLCIL tasks. Our code is available at
https://github.com/scut-zx/L3A.

</details>


### [629] [Towards Predicting Any Human Trajectory In Context](https://arxiv.org/abs/2506.00871)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: TrajICL is an In-Context Learning framework for pedestrian trajectory prediction that enables rapid adaptation without fine-tuning on scenario-specific data.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting pedestrian trajectories often require fine-tuning on scenario-specific data, which can be impractical on edge devices due to constrained computational resources.

Method: The paper proposes TrajICL, a framework that includes spatio-temporal similarity-based example selection (STES) and prediction-guided example selection (PG-ES) methods. The model is trained on a large-scale synthetic dataset instead of small real-world datasets.

Result: Extensive experiments show that TrajICL achieves significant adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches on multiple public benchmarks.

Conclusion: TrajICL provides a solution for pedestrian trajectory prediction that does not require fine-tuning, making it suitable for edge devices with limited computational resources.

Abstract: Predicting accurate future trajectories of pedestrians is essential for
autonomous systems but remains a challenging task due to the need for
adaptability in different environments and domains. A common approach involves
collecting scenario-specific data and performing fine-tuning via
backpropagation. However, this process is often impractical on edge devices due
to constrained computational resources. To address this challenge, we introduce
TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory
prediction that enables rapid adaptation without fine-tuning on the
scenario-specific data. We propose a spatio-temporal similarity-based example
selection (STES) method that selects relevant examples from previously observed
trajectories within the same scene by identifying similar motion patterns at
corresponding locations. To further refine this selection, we introduce
prediction-guided example selection (PG-ES), which selects examples based on
both the past trajectory and the predicted future trajectory, rather than
relying solely on the past trajectory. This approach allows the model to
account for long-term dynamics when selecting examples. Finally, instead of
relying on small real-world datasets with limited scenario diversity, we train
our model on a large-scale synthetic dataset to enhance its prediction ability
by leveraging in-context examples. Extensive experiments demonstrate that
TrajICL achieves remarkable adaptation across both in-domain and cross-domain
scenarios, outperforming even fine-tuned approaches across multiple public
benchmarks. The code will be released at
https://fujiry0.github.io/TrajICL-project-page.

</details>


### [630] [Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation](https://arxiv.org/abs/2506.00129)
*Edward Fish,Richard Bowden*

Main category: cs.CV

TL;DR: The paper proposes Geo-Sign, a method that uses hyperbolic geometry to enhance skeletal representations for Sign Language Translation.


<details>
  <summary>Details</summary>
Motivation: Recent progress in Sign Language Translation has primarily focused on improving the representational capacity of large language models to include Sign Language features. This work aims to explore an alternative direction by enhancing the geometric properties of skeletal representations themselves.

Method: The method involves leveraging the properties of hyperbolic geometry to model the hierarchical structure inherent in sign language kinematics. It projects skeletal features derived from Spatio-Temporal Graph Convolutional Networks (ST-GCNs) into the Poincar\'e ball model to create more discriminative embeddings. The approach includes a hyperbolic projection layer, a weighted Fr\'echet mean aggregation scheme, and a geometric contrastive loss operating directly in hyperbolic space.

Result: This work shows the potential of hyperbolic geometry to improve skeletal representations for Sign Language Translation, surpassing state-of-the-art RGB methods while maintaining privacy and boosting computational efficiency.

Conclusion: Geo-Sign demonstrates the effectiveness of hyperbolic geometry in enhancing skeletal representations for Sign Language Translation.

Abstract: Recent progress in Sign Language Translation (SLT) has focussed primarily on
improving the representational capacity of large language models to incorporate
Sign Language features. This work explores an alternative direction: enhancing
the geometric properties of skeletal representations themselves. We propose
Geo-Sign, a method that leverages the properties of hyperbolic geometry to
model the hierarchical structure inherent in sign language kinematics. By
projecting skeletal features derived from Spatio-Temporal Graph Convolutional
Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more
discriminative embeddings, particularly for fine-grained motions like finger
articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet
mean aggregation scheme, and a geometric contrastive loss operating directly in
hyperbolic space. These components are integrated into an end-to-end
translation framework as a regularisation function, to enhance the
representations within the language model. This work demonstrates the potential
of hyperbolic geometry to improve skeletal representations for Sign Language
Translation, improving on SOTA RGB methods while preserving privacy and
improving computational efficiency. Code available here:
https://github.com/ed-fish/geo-sign.

</details>


### [631] [Uneven Event Modeling for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.00891)
*Sa Zhu,Huashan Chen,Wanqian Zhang,Jinchao Zhang,Zexian Yang,Xiaoshuai Hao,Bo Li*

Main category: cs.CV

TL;DR: The paper introduces Uneven Event Modeling (UEM) framework for partially relevant video retrieval (PRVR), which includes Progressive-Grouped Video Segmentation (PGVS) and Context-Aware Event Refinement (CAER) modules. This method outperforms previous ones on two PRVR benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for PRVR segment videos into fixed, equal-length clips leading to unclear event boundaries and misalignment in event representations due to mean pooling.

Method: The authors propose the UEM framework with two key components: PGVS that segments videos considering temporal dependencies and semantic similarity, and CAER that refines event representation using text's cross-attention.

Result: The proposed method achieves state-of-the-art performance on two PRVR benchmarks through extensive experiments.

Conclusion: UEM framework effectively addresses ambiguities in event boundaries and misalignment issues in PRVR.

Abstract: Given a text query, partially relevant video retrieval (PRVR) aims to
retrieve untrimmed videos containing relevant moments, wherein event modeling
is crucial for partitioning the video into smaller temporal events that
partially correspond to the text. Previous methods typically segment videos
into a fixed number of equal-length clips, resulting in ambiguous event
boundaries. Additionally, they rely on mean pooling to compute event
representations, inevitably introducing undesired misalignment. To address
these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first
introduce the Progressive-Grouped Video Segmentation (PGVS) module, to
iteratively formulate events in light of both temporal dependencies and
semantic similarity between consecutive frames, enabling clear event
boundaries. Furthermore, we also propose the Context-Aware Event Refinement
(CAER) module to refine the event representation conditioned the text's
cross-attention. This enables event representations to focus on the most
relevant frames for a given text, facilitating more precise text-video
alignment. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on two PRVR benchmarks.

</details>


### [632] [ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment](https://arxiv.org/abs/2506.00238)
*Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: This paper proposes a VLM-based zero-shot VQA (ZeShot-VQA) method that can be applied on new datasets without fine-tuning and is able to generate unseen answers during training, demonstrating flexibility.


<details>
  <summary>Details</summary>
Motivation: Current VQA models lack the ability to answer open-ended questions and require finetuning/retraining when new possible answers emerge. Large-scale Vision-Language Models have shown strong performance on vision/language tasks without fine-tuning.

Method: The proposed method is a VLM-based zero-shot VQA (ZeShot-VQA) which investigates performance on post-disaster FloodNet dataset. It leverages zero-shot learning so it can be applied on new datasets without fine-tuning.

Result: ZeShot-VQA can process and generate answers not seen during training, showing its flexibility.

Conclusion: The ZeShot-VQA method takes advantage of zero-shot learning, making it applicable to new datasets without fine-tuning and capable of generating novel answers.

Abstract: Natural disasters usually affect vast areas and devastate infrastructures.
Performing a timely and efficient response is crucial to minimize the impact on
affected communities, and data-driven approaches are the best choice. Visual
question answering (VQA) models help management teams to achieve in-depth
understanding of damages. However, recently published models do not possess the
ability to answer open-ended questions and only select the best answer among a
predefined list of answers. If we want to ask questions with new additional
possible answers that do not exist in the predefined list, the model needs to
be fin-tuned/retrained on a new collected and annotated dataset, which is a
time-consuming procedure. In recent years, large-scale Vision-Language Models
(VLMs) have earned significant attention. These models are trained on extensive
datasets and demonstrate strong performance on both unimodal and multimodal
vision/language downstream tasks, often without the need for fine-tuning. In
this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and
investigate the performance of on post-disaster FloodNet dataset. Since the
proposed method takes advantage of zero-shot learning, it can be applied on new
datasets without fine-tuning. In addition, ZeShot-VQA is able to process and
generate answers that has been not seen during the training procedure, which
demonstrates its flexibility.

</details>


### [633] [IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection](https://arxiv.org/abs/2506.00979)
*Wayne Zhang,Changjiang Jiang,Zhonghao Zhang,Chenyang Si,Fengchang Yu,Wei Peng*

Main category: cs.CV

TL;DR: The paper presents IVY-FAKE, a large-scale dataset for explainable multimodal AIGC detection, and IVY-XDETECTOR, a unified architecture for detecting and explaining AIGC in both images and videos.


<details>
  <summary>Details</summary>
Motivation: Current AIGC detection methods lack interpretability and cannot detect both images and videos in a unified framework, compromising model transparency and trustworthiness.

Method: Introduced IVY-FAKE, a novel dataset with rich annotations for multimodal AIGC detection. Proposed IVY-XDETECTOR, a unified architecture that performs explainable detection for both image and video content using a vision-language model.

Result: Achieved state-of-the-art performance in multiple image and video detection benchmarks, demonstrating the advancements enabled by the new dataset and modeling framework.

Conclusion: IVY-FAKE and IVY-XDETECTOR provide significant improvements in AIGC detection with explainability and unified multimodal support.

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) in
visual domains has resulted in highly realistic synthetic images and videos,
driven by sophisticated generative frameworks such as diffusion-based
architectures. While these breakthroughs open substantial opportunities, they
simultaneously raise critical concerns about content authenticity and
integrity. Many current AIGC detection methods operate as black-box binary
classifiers, which offer limited interpretability, and no approach supports
detecting both images and videos in a unified framework. This dual limitation
compromises model transparency, reduces trustworthiness, and hinders practical
deployment. To address these challenges, we introduce IVY-FAKE , a novel,
unified, and large-scale dataset specifically designed for explainable
multimodal AIGC detection. Unlike prior benchmarks, which suffer from
fragmented modality coverage and sparse annotations, IVY-FAKE contains over
150,000 richly annotated training samples (images and videos) and 18,700
evaluation examples, each accompanied by detailed natural-language reasoning
beyond simple binary labels. Building on this, we propose Ivy Explainable
Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture
that jointly performs explainable detection for both image and video content.
Our unified vision-language model achieves state-of-the-art performance across
multiple image and video detection benchmarks, highlighting the significant
advancements enabled by our dataset and modeling framework. Our data is
publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.

</details>


### [634] [Quotient Network -- A Network Similar to ResNet but Learning Quotients](https://arxiv.org/abs/2506.00992)
*Peng Hui,Jiamuyang Zhao,Changxin Li,Qingzhen Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的网络结构，称为商网络（quotient network），解决了ResNet中的两个问题，并在CIFAR10、CIFAR100和SVHN数据集上证明了其性能优于ResNet。


<details>
  <summary>Details</summary>
Motivation: ResNet的核心思想是学习目标特征与现有特征之间的差异，但这种差异没有独立清晰的意义，并且基于绝对差而非相对差，对现有特征的大小敏感。为了解决这些问题，同时保留ResNet的优点，提出了一个新的网络。

Method: 新网络选择学习目标特征与现有特征的商，因此称为商网络（quotient network）。为了使该网络能够成功学习并实现更高性能，提出了一些设计规则，以确保其能够高效训练并优于ResNet。

Result: 实验结果表明，仅需对原始ResNet网络进行微小调整，无需增加新参数，该网络就能在CIFAR10、CIFAR100和SVHN数据集上稳定地显著优于ResNet。

Conclusion: 商网络解决了ResNet中特征差异无独立意义及对特征大小敏感的问题，在不增加参数的情况下，通过简单调整即可显著提升性能。

Abstract: The emergence of ResNet provides a powerful tool for training extremely deep
networks. The core idea behind it is to change the learning goals of the
network. It no longer learns new features from scratch but learns the
difference between the target and existing features. However, the difference
between the two kinds of features does not have an independent and clear
meaning, and the amount of learning is based on the absolute rather than the
relative difference, which is sensitive to the size of existing features. We
propose a new network that perfectly solves these two problems while still
having the advantages of ResNet. Specifically, it chooses to learn the quotient
of the target features with the existing features, so we call it the quotient
network. In order to enable this network to learn successfully and achieve
higher performance, we propose some design rules for this network so that it
can be trained efficiently and achieve better performance than ResNet.
Experiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network
can stably achieve considerable improvements over ResNet by simply making tiny
corresponding changes to the original ResNet network without adding new
parameters.

</details>


### [635] [Motion-Aware Concept Alignment for Consistent Video Editing](https://arxiv.org/abs/2506.01004)
*Tong Zhang,Juan C Leon Alcazar,Bernard Ghanem*

Main category: cs.CV

TL;DR: This paper presents MoCA-Video, a training-free framework that transfers semantic features from a reference image to a specific object in a video while preserving motion and context. It uses techniques like diagonal denoising, class-agnostic segmentation, momentum-based corrections, and gamma residual noise stabilization to ensure spatial consistency and temporal coherence. Evaluated with SSIM, LPIPS, and a new metric CASS, MoCA-Video surpasses baselines without any training or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between image-domain semantic mixing and video by introducing a training-free framework that can transfer semantic features from an image to a video object while maintaining motion and visual context.

Method: MoCA-Video leverages a diagonal denoising schedule and class-agnostic segmentation for object detection and tracking in the latent space, enabling precise control over the spatial location of blended objects. Momentum-based semantic corrections and gamma residual noise stabilization are used to ensure smooth frame transitions and temporal coherence.

Result: MoCA-Video outperforms current baselines on self-constructed datasets, achieving superior spatial consistency, coherent motion, and significantly higher scores on the newly introduced CASS metric, despite not undergoing any training or fine-tuning.

Conclusion: The study concludes that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis without the need for training.

Abstract: We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a
training-free framework bridging the gap between image-domain semantic mixing
and video. Given a generated video and a user-provided reference image,
MoCA-Video injects the semantic features of the reference image into a specific
object within the video, while preserving the original motion and visual
context. Our approach leverages a diagonal denoising schedule and
class-agnostic segmentation to detect and track objects in the latent space and
precisely control the spatial location of the blended objects. To ensure
temporal coherence, we incorporate momentum-based semantic corrections and
gamma residual noise stabilization for smooth frame transitions. We evaluate
MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,
and introduce a novel metric CASS (Conceptual Alignment Shift Score) to
evaluate the consistency and effectiveness of the visual shifts between the
source prompt and the modified video frames. Using self-constructed dataset,
MoCA-Video outperforms current baselines, achieving superior spatial
consistency, coherent motion, and a significantly higher CASS score, despite
having no training or fine-tuning. MoCA-Video demonstrates that structured
manipulation in the diffusion noise trajectory allows for controllable,
high-quality video synthesis.

</details>


### [636] [Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs](https://arxiv.org/abs/2506.01064)
*Yudong Zhang,Ruobing Xie,Yiqing Huang,Jiansheng Chen,Xingwu Sun,Zhanhui Kang,Di Wang,Yu Wang*

Main category: cs.CV

TL;DR: 近期大型视觉-语言模型(LVLMs)在多模态任务中表现出色，但对视觉对抗攻击较为脆弱。本文提出F3框架，采用‘以毒攻毒’策略，通过故意引入简单扰动来减轻对抗样本的危害，提高模型输出的可靠性和清洁度。F3具有无需训练、易于实施和显著计算效率等优势，适合大规模工业应用。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型在多模态任务中表现出色，但它们对视觉对抗攻击非常敏感，而目前针对此类对抗样本的有效净化方法研究较少。

Method: F3框架采用‘以毒攻毒’策略，通过向对抗样本有意引入简单扰动（如噪声），利用跨模态注意力机制从随机扰动的对抗样本中提取参考目标，从而优化模型的注意力分布，最终获得更干净和可靠的输出。

Result: 该方法在对抗样本净化方面取得了令人印象深刻的成果，同时展现出无需训练、易于实现和显著计算效率提升等优势。

Conclusion: F3框架提供了一种有效且高效的对抗样本净化方法，特别适用于需要强大性能和高效操作的大规模工业应用场景。代码将公开发布。

Abstract: Recent advances in large vision-language models (LVLMs) have showcased their
remarkable capabilities across a wide range of multimodal vision-language
tasks. However, these models remain vulnerable to visual adversarial attacks,
which can substantially compromise their performance. Despite their potential
impact, the development of effective methods for purifying such adversarial
examples has received relatively limited attention. In this paper, we introduce
F3, a novel adversarial purification framework that employs a counterintuitive
"fighting fire with fire" strategy: intentionally introducing simple
perturbations to adversarial examples to mitigate their harmful effects.
Specifically, F3 leverages cross-modal attentions derived from randomly
perturbed adversary examples as reference targets. By injecting noise into
these adversarial examples, F3 effectively refines their attention, resulting
in cleaner and more reliable model outputs. Remarkably, this seemingly
paradoxical approach of employing noise to counteract adversarial attacks
yields impressive purification results. Furthermore, F3 offers several distinct
advantages: it is training-free and straightforward to implement, and exhibits
significant computational efficiency improvements compared to existing
purification methods. These attributes render F3 particularly suitable for
large-scale industrial applications where both robust performance and
operational efficiency are critical priorities. The code will be made publicly
available.

</details>


### [637] [Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety](https://arxiv.org/abs/2506.01069)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Main category: cs.CV

TL;DR: The study explores the relationship between fingerprint patterns and ABO blood groups as a biometric identification tool, finding no significant correlation but suggesting potential in multi-modal biometric systems.


<details>
  <summary>Details</summary>
Motivation: To investigate if there is a significant relationship between fingerprint patterns and ABO blood groups that could be used to improve personal identification methods.

Method: 200 subjects' fingerprint types (loops, whorls, arches) and blood groups were compared using statistical tests such as chi-square and Pearson correlation.

Result: Loops were the most common fingerprint pattern and O+ the most prevalent blood group. No statistically significant difference found in fingerprint patterns across different blood groups.

Conclusion: Blood group data do not significantly improve personal identification when combined with fingerprinting; future research should consider larger samples, machine learning, and additional biometrics.

Abstract: Identification of a person is central in forensic science, security, and
healthcare. Methods such as iris scanning and genomic profiling are more
accurate but expensive, time-consuming, and more difficult to implement. This
study focuses on the relationship between the fingerprint patterns and the ABO
blood group as a biometric identification tool. A total of 200 subjects were
included in the study, and fingerprint types (loops, whorls, and arches) and
blood groups were compared. Associations were evaluated with statistical tests,
including chi-square and Pearson correlation. The study found that the loops
were the most common fingerprint pattern and the O+ blood group was the most
prevalent. Even though there was some associative pattern, there was no
statistically significant difference in the fingerprint patterns of different
blood groups. Overall, the results indicate that blood group data do not
significantly improve personal identification when used in conjunction with
fingerprinting. Although the study shows weak correlation, it may emphasize the
efforts of multi-modal based biometric systems in enhancing the current
biometric systems. Future studies may focus on larger and more diverse samples,
and possibly machine learning and additional biometrics to improve
identification methods. This study addresses an element of the ever-changing
nature of the fields of forensic science and biometric identification,
highlighting the importance of resilient analytical methods for personal
identification.

</details>


### [638] [GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking](https://arxiv.org/abs/2506.01078)
*Yufei Zhan,Ziheng Wu,Yousong Zhu,Rongkun Xue,Ruipu Luo,Zhenghao Chen,Can Zhang,Yifan Li,Zhentao He,Zheming Yang,Ming Tang,Minghui Qiu,Jinqiao Wang*

Main category: cs.CV

TL;DR: GThinker is a new MLLM that improves multimodal reasoning by incorporating Cue-Rethinking and a two-stage training pipeline, achieving better performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs underperform in vision-centric multimodal reasoning tasks due to ineffective integration of visual information and reliance on logic- and knowledge-based strategies.

Method: Introduced GThinker with Cue-Rethinking for grounding visual cues and iteratively reinterpreting them. Utilized a two-stage training pipeline including pattern-guided cold start and incentive reinforcement learning. Constructed GThinker-11K dataset for training.

Result: GThinker achieved 81.5% on M$^3$CoT benchmark, surpassing O4-mini model. Showed 2.1% average improvement on general scenario multimodal reasoning benchmarks while maintaining mathematical reasoning performance.

Conclusion: GThinker demonstrates significant advancements in multimodal reasoning across various domains, supported by extensive experiments and soon-to-be-released resources.

Abstract: Despite notable advancements in multimodal reasoning, leading Multimodal
Large Language Models (MLLMs) still underperform on vision-centric multimodal
reasoning tasks in general scenarios. This shortfall stems from their
predominant reliance on logic- and knowledge-based slow thinking strategies,
while effective for domains like math and science, fail to integrate visual
information effectively during reasoning. Consequently, these models often fail
to adequately ground visual cues, resulting in suboptimal performance in tasks
that require multiple plausible visual interpretations and inferences. To
address this, we present GThinker (General Thinker), a novel reasoning MLLM
excelling in multimodal reasoning across general scenarios, mathematics, and
science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that
grounds inferences in visual cues and iteratively reinterprets these cues to
resolve inconsistencies. Building on this pattern, we further propose a
two-stage training pipeline, including pattern-guided cold start and incentive
reinforcement learning, designed to enable multimodal reasoning capabilities
across domains. Furthermore, to support the training, we construct
GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths
and 4K curated reinforcement learning samples, filling the data gap toward
general multimodal reasoning. Extensive experiments demonstrate that GThinker
achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark
M$^3$CoT, surpassing the latest O4-mini model. It also shows an average
improvement of 2.1% on general scenario multimodal reasoning benchmarks, while
maintaining on-par performance in mathematical reasoning compared to
counterpart advanced reasoning models. The code, model, and data will be
released soon at https://github.com/jefferyZhan/GThinker.

</details>


### [639] [Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection](https://arxiv.org/abs/2506.01085)
*Shivam Chandhok,Qian Yang,Oscar Manas,Kanishk Jain,Leonid Sigal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 提出了一种名为PROGRESS的新框架，它是一种数据和计算高效的方法，允许视觉-语言模型动态选择接下来要学习的内容。该方法在多个指令调优数据集上表现出色，使用更少的数据和监督超越了现有最先进基线，并且具有良好的跨架构泛化和可转移性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型的指令调优虽然取得了成功，但成本高昂，需要大规模数据集、高质量注释和巨大的计算资源。因此，研究者希望开发一种更加数据和计算高效的方法来实现模型的学习。

Method: PROGRESS通过跟踪模型在不同技能上的学习进展，选择最具信息量的样本进行学习。具体来说，在每个训练阶段，模型会优先选择那些尚未掌握但又不是过于困难的样本。此外，PROGRESS不需要预先的答案注释，仅在必要时查询答案，并避免依赖辅助VLMs或计算密集型梯度运算来进行数据选择。

Result: 实验表明，PROGRESS在多个不同规模的指令调优数据集上一致地超越最先进的基线方法，同时使用的数据和监督更少。并且展示了强大的跨架构泛化能力和向更大模型的可转移性。

Conclusion: PROGRESS作为一种可扩展的解决方案，能够有效地控制技能获取顺序，减少对大量数据和监督的需求，为高效的视觉-语言模型学习提供了新途径。

Abstract: Instruction tuning has been central to the success of recent vision-language
models (VLMs), but it remains expensive-requiring large-scale datasets,
high-quality annotations, and large compute budgets. We propose PRioritized
cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data-
and compute-efficient framework that enables VLMs to dynamically select what to
learn next based on their evolving needs during training. At each stage, the
model tracks its learning progress across skills and selects the most
informative samples-those it has not already mastered and that are not too
difficult to learn at the current stage of training. This strategy effectively
controls skill acquisition and the order in which skills are learned.
Specifically, we sample from skills showing the highest learning progress,
prioritizing those with the most rapid improvement. Unlike prior methods,
PROGRESS requires no upfront answer annotations, queries answers only on a need
basis, avoids reliance on additional supervision from auxiliary VLMs, and does
not require compute-heavy gradient computations for data selection. Experiments
across multiple instruction-tuning datasets of varying scales demonstrate that
PROGRESS consistently outperforms state-of-the-art baselines with much less
data and supervision. Additionally, we show strong cross-architecture
generalization and transferability to larger models, validating PROGRESS as a
scalable solution for efficient learning.

</details>


### [640] [Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free](https://arxiv.org/abs/2506.00433)
*Luigi Sigillo,Shengfeng He,Danilo Comminiello*

Main category: cs.CV

TL;DR: LWD is a lightweight framework enabling ultra-high-resolution image generation without architectural modifications or extra computation, improving perceptual quality and reducing FID.


<details>
  <summary>Details</summary>
Motivation: High-resolution image synthesis poses challenges in balancing computational efficiency with fine-grained visual detail preservation.

Method: LWD introduces scale-consistent VAE objective for spectral fidelity, wavelet energy maps for detail-rich spatial regions identification, and time-dependent masking strategy focusing denoising on high-frequency components.

Result: LWD consistently improves perceptual quality and reduces FID in ultra-high-resolution image synthesis, outperforming strong baselines.

Conclusion: Frequency-aware, signal-driven supervision is an effective and efficient approach for high-resolution generative modeling.

Abstract: High-resolution image synthesis remains a core challenge in generative
modeling, particularly in balancing computational efficiency with the
preservation of fine-grained visual detail. We present Latent Wavelet Diffusion
(LWD), a lightweight framework that enables any latent diffusion model to scale
to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces
three key components: (1) a scale-consistent variational autoencoder objective
that enhances the spectral fidelity of latent representations; (2) wavelet
energy maps that identify and localize detail-rich spatial regions within the
latent space; and (3) a time-dependent masking strategy that focuses denoising
supervision on high-frequency components during training. LWD requires no
architectural modifications and incurs no additional computational overhead.
Despite its simplicity, it consistently improves perceptual quality and reduces
FID in ultra-high-resolution image synthesis, outperforming strong baseline
models. These results highlight the effectiveness of frequency-aware,
signal-driven supervision as a principled and efficient approach for
high-resolution generative modeling.

</details>


### [641] [CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting](https://arxiv.org/abs/2506.01109)
*Fengze Li,Yangle Liu,Jieming Ma,Hai-Ning Liang,Yaochun Shen,Huangxiang Li,Zhijing Wu*

Main category: cs.CV

TL;DR: 本论文提出了一种名为FruitLangGS的实时3D水果计数框架，解决了现有方法在推理速度、泛化能力和开放集语义控制支持方面的不足。通过空间重建、语义嵌入和语言引导实例估计，该框架实现了更高的渲染速度、语义灵活性和计数准确性。


<details>
  <summary>Details</summary>
Motivation: 在现实世界农业环境中，由于视觉遮挡、语义模糊和3D重建的高计算需求，精确的水果计数一直是一个长期存在的挑战。现有的基于神经辐射场的方法存在推理速度慢、泛化能力有限以及缺乏对开放集语义控制的支持等问题。

Method: FruitLangGS首先使用具有半径感知修剪和基于瓦片的光栅化的自适应高斯点阵流水线来重建果园规模的场景，以实现高效渲染。为了实现语义控制，每个高斯编码了一个压缩的CLIP对齐语言嵌入，形成紧凑且可查询的3D表示。在推理时，直接在3D空间中应用基于提示的语义过滤，无需依赖图像空间分割或视图级融合。然后通过分布感知采样将选定的高斯转换为密集点云，并进行聚类以估计水果数量。

Result: 实验结果表明，与先前的方法相比，FruitLangGS在真实果园数据上实现了更高的渲染速度、语义灵活性和计数准确性。

Conclusion: FruitLangGS提供了一种新的视角，用于在开放世界场景中进行语言驱动的实时神经渲染。

Abstract: Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.

</details>


### [642] [A Review on Coarse to Fine-Grained Animal Action Recognition](https://arxiv.org/abs/2506.01214)
*Ali Zia,Renuka Sharma,Abdelwahed Khamis,Xuesong Li,Muhammad Husnain,Numan Shafi,Saeed Anwar,Sabine Schmoelzl,Eric Stone,Lars Petersson,Vivien Rolland*

Main category: cs.CV

TL;DR: This review explores animal action recognition, comparing coarse-grained and fine-grained techniques, highlighting challenges unique to animals such as non-rigid body structures, frequent occlusions, and lack of large datasets. It evaluates spatio-temporal deep learning frameworks and suggests future directions for improving accuracy and generalisability.


<details>
  <summary>Details</summary>
Motivation: To examine the current state of research in animal behaviour recognition and elucidate the unique challenges associated with recognising subtle animal actions in outdoor environments, which differ significantly from human action recognition.

Method: Discussing the evolution of human action recognition and its relevance to animal action recognition, evaluating techniques like spatio-temporal deep learning frameworks (e.g., SlowFast) and assessing the strengths and weaknesses of current methodologies.

Result: Identification of challenges specific to animal action recognition, evaluation of existing techniques' effectiveness and limitations, and introduction of a recently-published dataset.

Conclusion: Future directions for advancing fine-grained action recognition are outlined to improve accuracy and generalisability in behaviour analysis across species.

Abstract: This review provides an in-depth exploration of the field of animal action
recognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques.
The primary aim is to examine the current state of research in animal behaviour
recognition and to elucidate the unique challenges associated with recognising
subtle animal actions in outdoor environments. These challenges differ
significantly from those encountered in human action recognition due to factors
such as non-rigid body structures, frequent occlusions, and the lack of
large-scale, annotated datasets. The review begins by discussing the evolution
of human action recognition, a more established field, highlighting how it
progressed from broad, coarse actions in controlled settings to the demand for
fine-grained recognition in dynamic environments. This shift is particularly
relevant for animal action recognition, where behavioural variability and
environmental complexity present unique challenges that human-centric models
cannot fully address. The review then underscores the critical differences
between human and animal action recognition, with an emphasis on high
intra-species variability, unstructured datasets, and the natural complexity of
animal habitats. Techniques like spatio-temporal deep learning frameworks
(e.g., SlowFast) are evaluated for their effectiveness in animal behaviour
analysis, along with the limitations of existing datasets. By assessing the
strengths and weaknesses of current methodologies and introducing a
recently-published dataset, the review outlines future directions for advancing
fine-grained action recognition, aiming to improve accuracy and
generalisability in behaviour analysis across species.

</details>


### [643] [Concept-Centric Token Interpretation for Vector-Quantized Generative Models](https://arxiv.org/abs/2506.00698)
*Tianze Yang,Yucheng Shi,Mengnan Du,Xuansheng Wu,Qiaoyu Tan,Jin Sun,Ninghao Liu*

Main category: cs.CV

TL;DR: This paper presents CORTEX, a method to interpret VQGMs by identifying concept-specific token combinations, enhancing model transparency and aiding in applications like targeted image editing.


<details>
  <summary>Details</summary>
Motivation: To better understand the critical tokens in Vector-Quantized Generative Models (VQGMs) for generating images of certain concepts.

Method: CORTEX framework uses two methods: 1) sample-level explanation analyzing token importance scores in individual images, and 2) codebook-level explanation exploring the entire codebook for globally relevant tokens.

Result: CORTEX effectively provides clear explanations of token usage in the generative process, outperforming baselines across multiple pretrained VQGMs.

Conclusion: CORTEX enhances VQGMs transparency and is useful for targeted image editing and shortcut feature detection.

Abstract: Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for
image generation. However, the key component of VQGMs -- the codebook of
discrete tokens -- is still not well understood, e.g., which tokens are
critical to generate an image of a certain concept? This paper introduces
Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting
VQGMs by identifying concept-specific token combinations. Our framework employs
two methods: (1) a sample-level explanation method that analyzes token
importance scores in individual images, and (2) a codebook-level explanation
method that explores the entire codebook to find globally relevant tokens.
Experimental results demonstrate CORTEX's efficacy in providing clear
explanations of token usage in the generative process, outperforming baselines
across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX
is useful in applications such as targeted image editing and shortcut feature
detection. Our code is available at https://github.com/YangTianze009/CORTEX.

</details>


### [644] [Common Inpainted Objects In-N-Out of Context](https://arxiv.org/abs/2506.00721)
*Tianze Yang,Tyson Jordan,Ninghao Liu,Jin Sun*

Main category: cs.CV

TL;DR: The paper introduces COinCO, a new dataset with 97,722 images created by replacing objects in COCO images via diffusion-based inpainting. It enables context learning and reveals patterns influencing inpainting success. The dataset supports tasks like context classification, object prediction from context, and fake detection.


<details>
  <summary>Details</summary>
Motivation: To address the lack of out-of-context examples in existing vision datasets, which limits the ability to study contextual understanding and object coherence in scenes.

Method: The authors create COinCO by systematically replacing objects in COCO images using diffusion-based inpainting. Inpainted objects are verified and categorized as in- or out-of-context using a multimodal large language model. This results in a dataset of 97,722 unique images featuring both coherent and inconsistent scenes.

Result: COinCO enables three key tasks: training context classifiers, predicting objects that naturally belong in given scenes, and enhancing fake detection without fine-tuning. The dataset uncovers significant patterns in semantic priors affecting inpainting success across object categories.

Conclusion: COinCO provides a controlled environment for exploring contextual variations and advancing context-aware visual understanding in computer vision and image forensics.

Abstract: We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel
dataset addressing the scarcity of out-of-context examples in existing vision
datasets. By systematically replacing objects in COCO images through
diffusion-based inpainting, we create 97,722 unique images featuring both
contextually coherent and inconsistent scenes, enabling effective context
learning. Each inpainted object is meticulously verified and categorized as in-
or out-of-context through a multimodal large language model assessment. Our
analysis reveals significant patterns in semantic priors that influence
inpainting success across object categories. We demonstrate three key tasks
enabled by COinCO: (1) training context classifiers that effectively determine
whether existing objects belong in their context; (2) a novel
Objects-from-Context prediction task that determines which new objects
naturally belong in given scenes at both instance and clique levels, and (3)
context-enhanced fake detection on state-of-the-art methods without
fine-tuning. COinCO provides a controlled testbed with contextual variations,
establishing a foundation for advancing context-aware visual understanding in
computer vision and image forensics. Our code and data are at:
https://github.com/YangTianze009/COinCO.

</details>


### [645] [Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression](https://arxiv.org/abs/2506.01234)
*Woojin Cho,Steve Andreas Immanuel,Junhyuk Heo,Darongsae Kwon*

Main category: cs.CV

TL;DR: Multispectral satellite images are crucial in various fields but present challenges due to high dimensionality and data volume. This paper introduces ImpliSat, a framework that uses Implicit Neural Representations (INR) for efficient compression and reconstruction of these images. A Fourier modulation algorithm is also introduced to optimize compression while preserving image details.


<details>
  <summary>Details</summary>
Motivation: Multispectral satellite images have significant applications in agriculture, fisheries, and environmental monitoring, but their high dimensionality, large data volumes, and diverse spatial resolutions make compression and analysis difficult.

Method: ImpliSat utilizes Implicit Neural Representations (INR) to model satellite images as continuous functions over coordinate space. It also incorporates a Fourier modulation algorithm that dynamically adapts to the spectral and spatial characteristics of each band.

Result: This approach captures fine spatial details across varying spatial resolutions and ensures optimal compression while preserving critical image details.

Conclusion: ImpliSat provides an effective solution for the compression and reconstruction of multispectral satellite data.

Abstract: Multispectral satellite images play a vital role in agriculture, fisheries,
and environmental monitoring. However, their high dimensionality, large data
volumes, and diverse spatial resolutions across multiple channels pose
significant challenges for data compression and analysis. This paper presents
ImpliSat, a unified framework specifically designed to address these challenges
through efficient compression and reconstruction of multispectral satellite
data. ImpliSat leverages Implicit Neural Representations (INR) to model
satellite images as continuous functions over coordinate space, capturing fine
spatial details across varying spatial resolutions. Furthermore, we introduce a
Fourier modulation algorithm that dynamically adjusts to the spectral and
spatial characteristics of each band, ensuring optimal compression while
preserving critical image details.

</details>


### [646] [Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors](https://arxiv.org/abs/2506.01247)
*Gerasimos Chatzoudis,Zhuowei Li,Gemma E. Moran,Hao Wang,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Visual Sparse Steering（VS2）的轻量级方法，可以在推理阶段引导视觉模型，无需重新训练或使用大型标注数据集。此外，还提出了VS2++和Prototype-Aligned Sparse Steering（PASS）两种改进方法，进一步提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在动态或资源受限的环境中，在推理阶段引导视觉基础模型而不需重新训练或访问大型标注数据集是一个具有挑战性的目标。

Method: 1. 提出了Visual Sparse Steering (VS2)，一种基于稀疏特征的轻量级测试时间方法，利用由top-k稀疏自编码器学习到的引导向量来指导视觉模型。
2. 进一步提出了检索增强变体VS2++，通过在推理时选择性放大伪标记邻居的相关稀疏特征来提升性能。
3. 提出了Prototype-Aligned Sparse Steering (PASS)，通过在SAE训练期间加入原型对齐损失，使稀疏特征与下游任务相关特征更好地对齐。

Result: 1. VS2在CIFAR-100、CUB-200和Tiny-ImageNet上分别超越了零样本CLIP 4.12%、1.08%和1.84%。
2. VS2++在有先验正负集的情况下，分别在CIFAR-100、CUB-200和Tiny-ImageNet上比CLIP零样本绝对提升了21.44%、7.08%和20.47%。
3. VS2和VS2++分别将每类准确率提高了高达25%和38%。
4. PASS在CIFAR-100上使用ViT-B/32时比VS2高出6.12%。

Conclusion: 本文提出的方法VS2、VS2++和PASS能够在不需要重新训练或访问大规模标注数据集的情况下有效提高视觉模型的性能，并且特别适合于动态或资源受限的环境。

Abstract: Steering vision foundation models at inference time without retraining or
access to large labeled datasets is a desirable yet challenging objective,
particularly in dynamic or resource-constrained settings. In this paper, we
introduce Visual Sparse Steering (VS2), a lightweight, test-time method that
guides vision models using steering vectors derived from sparse features
learned by top-$k$ Sparse Autoencoders without requiring contrastive data.
Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on
CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a
retrieval-augmented variant that selectively amplifies relevant sparse features
using pseudo-labeled neighbors at inference time. With oracle positive/negative
sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44%
on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2
and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing
that sparse steering benefits specific classes by disambiguating visually or
taxonomically proximate categories rather than providing a uniform boost.
Finally, to better align the sparse features learned through the SAE
reconstruction task with those relevant for downstream performance, we propose
Prototype-Aligned Sparse Steering (PASS). By incorporating a
prototype-alignment loss during SAE training, using labels only during training
while remaining fully test-time unsupervised, PASS consistently, though
modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100
with ViT-B/32.

</details>


### [647] [TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning](https://arxiv.org/abs/2506.00813)
*Jiaqi Luo,Yuan Yuan,Shixin Xu*

Main category: cs.CV

TL;DR: TIME is a new multimodal framework that combines TabPFN for tabular data and pretrained vision models for imaging data, showing superior performance in various tasks, especially medical applications.


<details>
  <summary>Details</summary>
Motivation: Current challenges in tabular-image multimodal learning include the lack of standardized pretrained representations for tabular data and difficulties handling missing values. This motivates the development of TIME.

Method: TIME uses TabPFN as a frozen tabular encoder to create embeddings resilient to missing data and combines these with image features from pretrained vision backbones. Various fusion strategies are explored.

Result: Extensive experiments on natural and medical datasets show that TIME consistently outperforms competitive baselines, whether the tabular inputs are complete or incomplete.

Conclusion: TIME addresses key challenges in tabular-image multimodal learning and demonstrates practical value in real-world scenarios.

Abstract: Tabular-image multimodal learning, which integrates structured tabular data
with imaging data, holds great promise for a variety of tasks, especially in
medical applications. Yet, two key challenges remain: (1) the lack of a
standardized, pretrained representation for tabular data, as is commonly
available in vision and language domains; and (2) the difficulty of handling
missing values in the tabular modality, which are common in real-world medical
datasets. To address these issues, we propose the TabPFN-Integrated Multimodal
Engine (TIME), a novel multimodal framework that builds on the recently
introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen
tabular encoder to generate robust, strong embeddings that are naturally
resilient to missing data, and combines them with image features from
pretrained vision backbones. We explore a range of fusion strategies and
tabular encoders, and evaluate our approach on both natural and medical
datasets. Extensive experiments demonstrate that TIME consistently outperforms
competitive baselines across both complete and incomplete tabular inputs,
underscoring its practical value in real-world multimodal learning scenarios.

</details>


### [648] [ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding](https://arxiv.org/abs/2506.01274)
*Hosu Lee,Junho Kim,Hyunjun Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, but video content understanding is still limited by inadequate frame selection strategies. This paper presents ReFoCUS, a new reinforcement learning-based method for optimizing frame selection policies to improve video content understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations in existing video content understanding approaches that rely on static heuristics or external retrieval modules for frame selection, which may not provide query-relevant information.

Method: ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals from a reference LMM to select frames that best support temporally grounded responses. It uses an autoregressive, conditional selection architecture for efficient exploration of the large combinatorial frame space, ensuring temporal coherence while reducing complexity.

Result: ReFoCUS does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks.

Conclusion: This work concludes that aligning frame selection with model-internal utility can significantly enhance video content understanding and reasoning performance.

Abstract: Recent progress in Large Multi-modal Models (LMMs) has enabled effective
vision-language reasoning, yet the ability to understand video content remains
constrained by suboptimal frame selection strategies. Existing approaches often
rely on static heuristics or external retrieval modules to feed frame
information into video-LLMs, which may fail to provide the query-relevant
information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame
Optimization for Contextual UnderStanding), a novel frame-level policy
optimization framework that shifts the optimization target from textual
responses to visual input selection. ReFoCUS learns a frame selection policy
via reinforcement learning, using reward signals derived from a reference LMM
to reflect the model's intrinsic preferences for frames that best support
temporally grounded responses. To efficiently explore the large combinatorial
frame space, we employ an autoregressive, conditional selection architecture
that ensures temporal coherence while reducing complexity. Our approach does
not require explicit supervision at the frame-level and consistently improves
reasoning performance across multiple video QA benchmarks, highlighting the
benefits of aligning frame selection with model-internal utility.

</details>


### [649] [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Yajing Xu,Min Zhang,Wen Zhang,Huajun Chen*

Main category: cs.CV

TL;DR: Multi-modal large language models (MLLMs) struggle with comprehending world knowledge through structured abstractions in visual form. The paper proposes M3STR, a new benchmark for evaluating MLLMs' capacity to understand multi-modal entities and their relationships, revealing deficiencies in processing abstractive visual information.


<details>
  <summary>Details</summary>
Motivation: Current evaluation benchmarks for MLLMs overlook the critical capacity to comprehend world knowledge with structured abstractions that appear in visual form.

Method: The paper devises M3STR, an innovative benchmark grounded in the Multi-Modal Map for STRuctured understanding, which leverages multi-modal knowledge graphs to synthesize images encapsulating subgraph architectures enriched with multi-modal entities.

Result: An extensive empirical analysis of 26 state-of-the-art MLLMs reveals persistent deficiencies in processing abstractive visual information with structured knowledge.

Conclusion: M3STR charts a pivotal trajectory for advancing MLLMs' holistic reasoning capacities.

Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous
modalities into LLMs, enabling a comprehensive understanding of diverse
scenarios and objects. Despite the proliferation of evaluation benchmarks and
leaderboards for MLLMs, they predominantly overlook the critical capacity of
MLLMs to comprehend world knowledge with structured abstractions that appear in
visual form. To address this gap, we propose a novel evaluation paradigm and
devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for
STRuctured understanding. This benchmark leverages multi-modal knowledge graphs
to synthesize images encapsulating subgraph architectures enriched with
multi-modal entities. M3STR necessitates that MLLMs not only recognize the
multi-modal entities within the visual inputs but also decipher intricate
relational topologies among them. We delineate the benchmark's statistical
profiles and automated construction pipeline, accompanied by an extensive
empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent
deficiencies in processing abstractive visual information with structured
knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic
reasoning capacities. Our code and data are released at
https://github.com/zjukg/M3STR

</details>


### [650] [Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras](https://arxiv.org/abs/2506.00904)
*Xander Küpers,Jeroen Klein Brinke,Rob Bemthuis,Ozlem Durmaz Incel*

Main category: cs.CV

TL;DR: 为了应对建筑行业中设备利用率低的问题，本文提出了Edge-IMI框架以检测闲置的工程机械，并通过与监控摄像头系统集成，实现了在资源受限的边缘计算设备上的执行。实验结果表明其对象检测器F1得分为71.75%，能够有效区分活跃和空闲状态的机械，同时减少了对高带宽云服务和昂贵硬件加速器的依赖。


<details>
  <summary>Details</summary>
Motivation: 建筑行业面临优化设备利用率的重大挑战，未充分利用的机器会增加运营成本并导致项目延误。因此，及时准确地监控设备活动对于识别空闲期和提高整体效率至关重要。

Method: Edge-IMI框架包含三个组件：对象检测、跟踪和空闲状态识别。该框架专为在基于CPU的资源受限边缘计算设备上执行而设计。使用来自ACID和MOCS基准的组合数据集评估性能。此外，还在Raspberry Pi 5和Intel NUC平台上评估了对象检测模型的性能，探讨实时处理的可行性及模型优化技术的影响。

Result: 实验结果表明，对象检测器的F1分数达到71.75%，显示了其在真实世界中的稳健检测能力。基于逻辑回归的空闲识别模块能够可靠地区分活跃和空闲状态的机械设备，且误报率极低。

Conclusion: Edge-IMI框架能够在资源受限的边缘计算设备上实现高效的现场推理，减少对高带宽云服务和昂贵硬件加速器的依赖，从而为建筑行业提供了一种优化设备利用率的有效解决方案。

Abstract: The construction industry faces significant challenges in optimizing
equipment utilization, as underused machinery leads to increased operational
costs and project delays. Accurate and timely monitoring of equipment activity
is therefore key to identifying idle periods and improving overall efficiency.
This paper presents the Edge-IMI framework for detecting idle construction
machinery, specifically designed for integration with surveillance camera
systems. The proposed solution consists of three components: object detection,
tracking, and idle state identification, which are tailored for execution on
resource-constrained, CPU-based edge computing devices. The performance of
Edge-IMI is evaluated using a combined dataset derived from the ACID and MOCS
benchmarks. Experimental results confirm that the object detector achieves an
F1 score of 71.75%, indicating robust real-world detection capabilities. The
logistic regression-based idle identification module reliably distinguishes
between active and idle machinery with minimal false positives. Integrating all
three modules, Edge-IMI enables efficient on-site inference, reducing reliance
on high-bandwidth cloud services and costly hardware accelerators. We also
evaluate the performance of object detection models on Raspberry Pi 5 and an
Intel NUC platforms, as example edge computing platforms. We assess the
feasibility of real-time processing and the impact of model optimization
techniques.

</details>


### [651] [Playing with Transformer at 30+ FPS via Next-Frame Diffusion](https://arxiv.org/abs/2506.01380)
*Xinle Cheng,Tianyu He,Jiayi Xu,Junliang Guo,Di He,Jiang Bian*

Main category: cs.CV

TL;DR: Next-Frame Diffusion (NFD)是一种自回归扩散变压器，结合了块状因果注意机制，能够通过并行标记生成实现迭代采样和高效推理。为了实现实时视频生成，文章引入了两种创新：扩展一致性蒸馏到视频领域以减少采样步骤，以及提出投机采样以充分利用并行计算。实验表明NFD在视觉质量和采样效率上优于自回归基线模型，并首次实现了在A100 GPU上超过30 FPS的自回归视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前自回归视频模型在创建交互式视频内容和流媒体应用方面具有优势，但实现实时视频生成仍面临高计算成本和硬件效率低下的挑战。

Method: 提出了Next-Frame Diffusion（NFD）模型，采用块状因果注意机制进行迭代采样和高效推理。同时，引入了两种创新：1）将一致性蒸馏扩展到视频领域，用于减少采样步骤；2）基于相邻帧通常共享相同动作输入的观察，提出投机采样方法。

Result: 在大规模动作条件视频生成基准上的实验表明，NFD在视觉质量和采样效率上均优于自回归基线模型，并且首次实现了在A100 GPU上超过30 FPS的自回归视频生成。

Conclusion: NFD模型通过技术创新成功解决了自回归视频生成中的实时性问题，在保证高质量的同时提高了生成效率。

Abstract: Autoregressive video models offer distinct advantages over bidirectional
diffusion models in creating interactive video content and supporting streaming
applications with arbitrary duration. In this work, we present Next-Frame
Diffusion (NFD), an autoregressive diffusion transformer that incorporates
block-wise causal attention, enabling iterative sampling and efficient
inference via parallel token generation within each frame. Nonetheless,
achieving real-time video generation remains a significant challenge for such
models, primarily due to the high computational cost associated with diffusion
sampling and the hardware inefficiencies inherent to autoregressive generation.
To address this, we introduce two innovations: (1) We extend consistency
distillation to the video domain and adapt it specifically for video models,
enabling efficient inference with few sampling steps; (2) To fully leverage
parallel computation, motivated by the observation that adjacent frames often
share the identical action input, we propose speculative sampling. In this
approach, the model generates next few frames using current action input, and
discard speculatively generated frames if the input action differs. Experiments
on a large-scale action-conditioned video generation benchmark demonstrate that
NFD beats autoregressive baselines in terms of both visual quality and sampling
efficiency. We, for the first time, achieves autoregressive video generation at
over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.

</details>


### [652] [VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding](https://arxiv.org/abs/2506.01388)
*Yihao Ding,Soyeon Caren Han,Yan Li,Josiah Poon*

Main category: cs.CV

TL;DR: Visually Rich Document Understanding (VRDU) is crucial for extracting information from complex documents. The VRD-IU Competition addressed challenges in form-like documents through two tracks, showcasing state-of-the-art methodologies and setting new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Form-like documents present unique challenges due to their complex layouts, multi-stakeholder involvement, and high structural variability. There is a need for improved methods in Visually Rich Document Understanding (VRDU) to extract key information effectively.

Method: The VRD-IU Competition featured two tracks: Track A focused on entity-based key information retrieval, while Track B targeted end-to-end key information localization from raw document images. State-of-the-art methodologies such as hierarchical decomposition, transformer-based retrieval, multimodal feature fusion, and advanced object detection techniques were employed.

Result: With over 20 participating teams, the competition showcased various successful approaches and top-performing models that set new benchmarks in VRDU.

Conclusion: The insights from the VRD-IU Competition provide valuable advancements in document intelligence, highlighting effective methodologies for extracting and localizing key information from visually rich and complex documents.

Abstract: Visually Rich Document Understanding (VRDU) has emerged as a critical field
in document intelligence, enabling automated extraction of key information from
complex documents across domains such as medical, financial, and educational
applications. However, form-like documents pose unique challenges due to their
complex layouts, multi-stakeholder involvement, and high structural
variability. Addressing these issues, the VRD-IU Competition was introduced,
focusing on extracting and localizing key information from multi-format forms
within the Form-NLU dataset, which includes digital, printed, and handwritten
documents. This paper presents insights from the competition, which featured
two tracks: Track A, emphasizing entity-based key information retrieval, and
Track B, targeting end-to-end key information localization from raw document
images. With over 20 participating teams, the competition showcased various
state-of-the-art methodologies, including hierarchical decomposition,
transformer-based retrieval, multimodal feature fusion, and advanced object
detection techniques. The top-performing models set new benchmarks in VRDU,
providing valuable insights into document intelligence.

</details>


### [653] [ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition](https://arxiv.org/abs/2506.01411)
*Minjeong Park,Hongbeen Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: The paper proposes ViTA-PAR, a model that uses multimodal prompting and vision-language alignment to improve pedestrian attribute recognition.


<details>
  <summary>Details</summary>
Motivation: PAR models need to capture both global and local features but struggle when attributes appear in unexpected body locations. Existing methods are often restricted to fixed regions.

Method: Propose ViTA-PAR with visual attribute prompts for global-to-local semantics, a learnable prompt template for textual embeddings, and alignment of visual and textual attribute features.

Result: Achieves competitive performance on four PAR benchmarks with efficient inference.

Conclusion: ViTA-PAR enhances attribute recognition through specialized multimodal prompting and vision-language alignment.

Abstract: The Pedestrian Attribute Recognition (PAR) task aims to identify various
detailed attributes of an individual, such as clothing, accessories, and
gender. To enhance PAR performance, a model must capture features ranging from
coarse-grained global attributes (e.g., for identifying gender) to fine-grained
local details (e.g., for recognizing accessories) that may appear in diverse
regions. Recent research suggests that body part representation can enhance the
model's robustness and accuracy, but these methods are often restricted to
attribute classes within fixed horizontal regions, leading to degraded
performance when attributes appear in varying or unexpected body locations. In
this paper, we propose Visual and Textual Attribute Alignment with Attribute
Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance
attribute recognition through specialized multimodal prompting and
vision-language alignment. We introduce visual attribute prompts that capture
global-to-local semantics, enabling diverse attribute representations. To
enrich textual embeddings, we design a learnable prompt template, termed person
and attribute context prompting, to learn person and attributes context.
Finally, we align visual and textual attribute features for effective fusion.
ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance
with efficient inference. We release our code and model at
https://github.com/mlnjeongpark/ViTA-PAR.

</details>


### [654] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
*Yulei Qin,Gang Li,Zongyi Li,Zihan Xu,Yuchen Shi,Zhekai Lin,Xiao Cui,Ke Li,Xing Sun*

Main category: cs.CV

TL;DR: Existing LLMs struggle with complex instructions, especially those with multiple constraints. The paper proposes a method to improve reasoning capabilities by decomposing instructions and using reinforcement learning with rule-centric rewards, leading to significant performance gains in a 1.5B LLM.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current chain-of-thought (CoT) methods which fail to effectively reason about complex instructions due to their superficial nature.

Method: The method involves decomposing complex instructions, acquiring reproducible data, and using reinforcement learning with verifiable rule-centric reward signals to enhance reasoning. It also uses sample-wise contrast for better CoT enforcement and behavior cloning of experts to shift the distribution from fast-thinking LLMs to skillful reasoners.

Result: Extensive evaluations on seven benchmarks confirm the validity of the proposed method, showing a 11.74% performance gain in a 1.5B LLM, making it comparable to an 8B LLM.

Conclusion: The systematic approach significantly boosts LLMs' ability to handle complex instructions, with promising results demonstrated through comprehensive evaluations.

Abstract: Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
are available at https://github.com/yuleiqin/RAIF.

</details>


### [655] [SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data](https://arxiv.org/abs/2506.01189)
*Emmanuel Hartman,Nicolas Charon*

Main category: cs.CV

TL;DR: The paper proposes SVarM which uses varifold representations for shape analysis, developing classification and regression models with strong performance and fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Performing statistical analysis on geometric data is challenging due to the non-Euclidean nature of shape spaces. There's a need for machine learning frameworks that incorporate invariances, such as shape parametrization, to ensure model generalizability.

Method: SVarM exploits varifold representations of shapes as measures and their duality with test functions h. It introduces a neural network-based representation of the trainable test function h for classification and regression models on shape datasets.

Result: This approach shows strong performance and robustness across various shape graph and surface datasets, achieving results comparable to state-of-the-art methods while significantly reducing the number of trainable parameters.

Conclusion: SVarM provides an effective method for shape analysis using varifold representations, demonstrating strong performance with fewer parameters.

Abstract: Despite progress in the rapidly developing field of geometric deep learning,
performing statistical analysis on geometric data--where each observation is a
shape such as a curve, graph, or surface--remains challenging due to the
non-Euclidean nature of shape spaces, which are defined as equivalence classes
under invariance groups. Building machine learning frameworks that incorporate
such invariances, notably to shape parametrization, is often crucial to ensure
generalizability of the trained models to new observations. This work proposes
SVarM to exploit varifold representations of shapes as measures and their
duality with test functions $h:\mathbb{R}^n \times S^{n-1} \to \mathbb{R}$.
This method provides a general framework akin to linear support vector machines
but operating instead over the infinite-dimensional space of varifolds. We
develop classification and regression models on shape datasets by introducing a
neural network-based representation of the trainable test function $h$. This
approach demonstrates strong performance and robustness across various shape
graph and surface datasets, achieving results comparable to state-of-the-art
methods while significantly reducing the number of trainable parameters.

</details>


### [656] [G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models](https://arxiv.org/abs/2506.01539)
*Tianjiao Zhang,Fei Zhang,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: The paper explores using a large-scale text-to-image diffusion model for Inexact Segmentation, emphasizing generative priors in Stable Diffusion to refine segmentation through pattern discrepancies.


<details>
  <summary>Details</summary>
Motivation: To address the Inexact Segmentation task by utilizing generative models instead of traditional discriminative-model-based paradigms or dense visual representations.

Method: Exploiting pattern discrepancies between original images and mask-conditional generated images to establish semantic correspondence alignment and update foreground probability for coarse-to-fine segmentation refinement.

Result: Comprehensive experiments validate the effectiveness and superiority of the proposed plug-and-play design.

Conclusion: The approach highlights the potential of leveraging generation discrepancies to model dense representations and encourages further exploration of generative methods for discriminative tasks.

Abstract: This paper considers the problem of utilizing a large-scale text-to-image
diffusion model to tackle the challenging Inexact Segmentation (IS) task.
Unlike traditional approaches that rely heavily on discriminative-model-based
paradigms or dense visual representations derived from internal attention
mechanisms, our method focuses on the intrinsic generative priors in Stable
Diffusion~(SD). Specifically, we exploit the pattern discrepancies between
original images and mask-conditional generated images to facilitate a
coarse-to-fine segmentation refinement by establishing a semantic
correspondence alignment and updating the foreground probability. Comprehensive
quantitative and qualitative experiments validate the effectiveness and
superiority of our plug-and-play design, underscoring the potential of
leveraging generation discrepancies to model dense representations and
encouraging further exploration of generative approaches for solving
discriminative tasks.

</details>


### [657] [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
*Bingqian Lin,Yunshuang Nie,Khun Loun Zai,Ziming Wei,Mingfei Han,Rongtao Xu,Minzhe Niu,Jianhua Han,Liang Lin,Cewu Lu,Xiaodan Liang*

Main category: cs.CV

TL;DR: The paper proposes EvolveNav, a self-improving embodied reasoning framework for enhancing LLM-based vision-language navigation with two stages: Formalized CoT Supervised Fine-Tuning and Self-Reflective Post-Training.


<details>
  <summary>Details</summary>
Motivation: To address the difficulties in mapping learning and unexplainable navigational decisions in current VLN agents using LLMs, as well as the challenges of obtaining perfect CoT labels and avoiding overfitting during pure CoT supervised fine-tuning.

Method: EvolveNav consists of two stages: 1) Formalized CoT Supervised Fine-Tuning where the model is trained with formalized CoT labels to activate navigational reasoning capabilities and increase reasoning speed; 2) Self-Reflective Post-Training where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels. A self-reflective auxiliary task contrasts correct and wrong reasoning patterns.

Result: Experimental results on popular VLN benchmarks show that EvolveNav outperforms previous LLM-based VLN approaches.

Conclusion: EvolveNav improves both navigational decision accuracy and interpretability by incorporating Chain-of-Thought training while addressing the issues of obtaining perfect CoT labels and overfitting through its self-reflective post-training approach.

Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following
natural language instructions is a long-standing goal in human-robot
interaction applications. Recent studies have revealed the potential of
training open-source Large Language Models (LLMs) to unleash LLMs' reasoning
ability for improving navigation, and simultaneously mitigate the domain gap
between LLMs' training corpus and the VLN task. However, these approaches
primarily adopt direct input-output mapping paradigms, causing the mapping
learning difficult and the navigational decisions unexplainable.
Chain-of-Thought (CoT) training is a promising way to improve both navigational
decision accuracy and interpretability, while the complexity of the navigation
task makes the perfect CoT labels unavailable and may lead to overfitting
through pure CoT supervised fine-tuning. In this paper, we propose a novel
sElf-improving embodied reasoning framework for boosting LLM-based
vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two
stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model
with formalized CoT labels to both activate the model's navigational reasoning
capabilities and increase the reasoning speed; (2) Self-Reflective
Post-Training, where the model is iteratively trained with its own reasoning
outputs as self-enriched CoT labels to enhance the supervision diversity. A
self-reflective auxiliary task is also introduced to encourage learning correct
reasoning patterns by contrasting with wrong ones. Experimental results on the
popular VLN benchmarks demonstrate the superiority of EvolveNav over previous
LLM-based VLN approaches. Code is available at
https://github.com/expectorlin/EvolveNav.

</details>


### [658] [EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models](https://arxiv.org/abs/2506.01608)
*Andy Bonnetto,Haozhe Qi,Franklin Leong,Matea Tashkovska,Mahdi Rad,Solaiman Shokur,Friedhelm Hummel,Silvestro Micera,Marc Pollefeys,Alexander Mathis*

Main category: cs.CV

TL;DR: This paper introduces the EPFL-Smart-Kitchen-30 dataset collected in a kitchen environment using nine static RGB-D cameras, IMUs, and one head-mounted HoloLens 2 headset to capture 3D hand, body, and eye movements. The dataset spans 29.7 hours of 16 subjects cooking four different recipes with densely annotated action segments. Four benchmarks are proposed to advance behavior understanding and modeling.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive dataset that captures humans carrying out complex tasks in a natural environment for advancing understanding of human motor and cognitive function.

Method: Collection of data using noninvasive motion capture platform inside a kitchen environment with multiple sensors including static RGB-D cameras, IMUs, and HoloLens 2 headset. Annotations of action sequences.

Result: EPFL-Smart-Kitchen-30 dataset with multi-view action data spanning 29.7 hours of 16 subjects cooking four different recipes. Densely annotated with 33.78 action segments per minute.

Conclusion: The EPFL-Smart-Kitchen-30 dataset is expected to pave the way for better methods and insights to understand ecologically-valid human behavior.

Abstract: Understanding behavior requires datasets that capture humans while carrying
out complex tasks. The kitchen is an excellent environment for assessing human
motor and cognitive function, as many complex actions are naturally exhibited
in kitchens from chopping to cleaning. Here, we introduce the
EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture
platform inside a kitchen environment. Nine static RGB-D cameras, inertial
measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to
capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is
a multi-view action dataset with synchronized exocentric, egocentric, depth,
IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects
cooking four different recipes. Action sequences were densely annotated with
33.78 action segments per minute. Leveraging this multi-modal dataset, we
propose four benchmarks to advance behavior understanding and modeling through
1) a vision-language benchmark, 2) a semantic text-to-motion generation
benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based
action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to
pave the way for better methods as well as insights to understand the nature of
ecologically-valid human behavior. Code and data are available at
https://github.com/amathislab/EPFL-Smart-Kitchen

</details>


### [659] [Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity](https://arxiv.org/abs/2506.01493)
*Yuya Kobayashi,Yuhta Takida,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: The paper proposes SCAD, a text-to-image GAN using specialized discriminators with Slicing Adversarial Networks (SANs) to enhance diversity and fidelity at significantly reduced training cost.


<details>
  <summary>Details</summary>
Motivation: To reduce the high training cost associated with large-scale GANs for text-to-image tasks while maintaining generation diversity and fidelity.

Method: Propose SCAD model with two specialized discriminators using Slicing Adversarial Networks (SANs) adapted for text-to-image tasks and introduce Per-Prompt Diversity (PPD) metric for quantitative evaluation.

Result: SCAD improves diversity for given prompts with better sample fidelity, achieves zero-shot FID competitive with latest large-scale GANs at much lower training cost.

Conclusion: SCAD efficiently enhances text-to-image generation diversity and fidelity with significantly reduced training cost.

Abstract: Recently, Generative Adversarial Networks (GANs) have been successfully
scaled to billion-scale large text-to-image datasets. However, training such
models entails a high training cost, limiting some applications and research
usage. To reduce the cost, one promising direction is the incorporation of
pre-trained models. The existing method of utilizing pre-trained models for a
generator significantly reduced the training cost compared with the other
large-scale GANs, but we found the model loses the diversity of generation for
a given prompt by a large margin. To build an efficient and high-fidelity
text-to-image GAN without compromise, we propose to use two specialized
discriminators with Slicing Adversarial Networks (SANs) adapted for
text-to-image tasks. Our proposed model, called SCAD, shows a notable
enhancement in diversity for a given prompt with better sample fidelity. We
also propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the
diversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID
competitive with the latest large-scale GANs at two orders of magnitude less
training cost.

</details>


### [660] [Multi-Modal Dataset Distillation in the Wild](https://arxiv.org/abs/2506.01586)
*Zhuohang Dang,Minnan Luo,Chengyou Jia,Hangwei Qian,Xiaojun Chang,Ivor W. Tsang*

Main category: cs.CV

TL;DR: 提出了一种名为MDW的框架，用于将嘈杂的多模态数据集提炼为紧凑干净的数据集，以实现有效和高效的模型训练。该框架通过引入可学习的细粒度对应关系和双重协作学习机制，增强了提炼数据的信息密度和有效性，并在各种压缩比下表现出显著的可扩展性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型的发展面临两个主要的数据挑战：1）需要大规模数据集进行训练，导致存储和计算成本高昂；2）网络爬取的数据不可避免地存在噪声（部分不匹配的对），严重影响模型性能。因此，需要一种方法来处理这些嘈杂的数据，生成高质量的紧凑数据集以支持有效和高效的模型训练。

Method: MDW框架包含以下关键步骤：1）引入可学习的细粒度对应关系，在提炼过程中优化提炼数据，强调区分性区域，提高信息密度和效果；2）采用双重协作学习机制，从真实数据中捕捉稳健的跨模态对应先验知识，避免数据噪声影响并减少信息损失。

Result: 实验验证了MDW在理论和实证上的有效性，其在各种压缩比下的表现超过现有方法15%以上，展现出卓越的可扩展性和实际应用潜力。

Conclusion: MDW是首个能够将嘈杂多模态数据集提炼为紧凑清洁数据集的框架，解决了大规模数据训练的成本和噪声问题，具有广泛的实用价值。

Abstract: Recent multi-modal models have shown remarkable versatility in real-world
applications. However, their rapid development encounters two critical data
challenges. First, the training process requires large-scale datasets, leading
to substantial storage and computational costs. Second, these data are
typically web-crawled with inevitable noise, i.e., partially mismatched pairs,
severely degrading model performance. To these ends, we propose Multi-modal
dataset Distillation in the Wild, i.e., MDW, the first framework to distill
noisy multi-modal datasets into compact clean ones for effective and efficient
model training. Specifically, MDW introduces learnable fine-grained
correspondences during distillation and adaptively optimizes distilled data to
emphasize correspondence-discriminative regions, thereby enhancing distilled
data's information density and efficacy. Moreover, to capture robust
cross-modal correspondence prior knowledge from real data, MDW proposes
dual-track collaborative learning to avoid the risky data noise, alleviating
information loss with certifiable noise tolerance. Extensive experiments
validate MDW's theoretical and empirical efficacy with remarkable scalability,
surpassing prior methods by over 15% across various compression ratios,
highlighting its appealing practicality for applications with diverse efficacy
and resource needs.

</details>


### [661] [Data Pruning by Information Maximization](https://arxiv.org/abs/2506.01701)
*Haoru Tan,Sitong Wu,Wei Huang,Shizhen Zhao,Xiaojuan Qi*

Main category: cs.CV

TL;DR: The paper introduces InfoMax, a data pruning method that enhances coreset informativeness by maximizing information content and minimizing redundancy. It uses importance scores for samples and pairwise similarities for redundancy, formalized as a discrete quadratic programming task with an efficient gradient-based solver.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and effectiveness of model training by selecting a highly informative subset (coreset) of data that maximizes information content while reducing redundancy.

Method: InfoMax measures individual sample information using importance scores and quantifies redundancy through pairwise sample similarities. The coreset selection problem is framed as a discrete quadratic programming task aiming to maximize total information content, solved efficiently via a gradient-based approach with sparsification techniques and dataset partitioning.

Result: InfoMax demonstrates superior performance in various data pruning tasks, such as image classification, vision-language pre-training, and instruction tuning for large language models, across datasets with millions of samples.

Conclusion: InfoMax is an effective and scalable data pruning method that enhances coreset informativeness, providing significant improvements in diverse machine learning tasks.

Abstract: In this paper, we present InfoMax, a novel data pruning method, also known as
coreset selection, designed to maximize the information content of selected
samples while minimizing redundancy. By doing so, InfoMax enhances the overall
informativeness of the coreset. The information of individual samples is
measured by importance scores, which capture their influence or difficulty in
model learning. To quantify redundancy, we use pairwise sample similarities,
based on the premise that similar samples contribute similarly to the learning
process. We formalize the coreset selection problem as a discrete quadratic
programming (DQP) task, with the objective of maximizing the total information
content, represented as the sum of individual sample contributions minus the
redundancies introduced by similar samples within the coreset. To ensure
practical scalability, we introduce an efficient gradient-based solver,
complemented by sparsification techniques applied to the similarity matrix and
dataset partitioning strategies. This enables InfoMax to seamlessly scale to
datasets with millions of samples. Extensive experiments demonstrate the
superior performance of InfoMax in various data pruning tasks, including image
classification, vision-language pre-training, and instruction tuning for large
language models.

</details>


### [662] [Efficient Egocentric Action Recognition with Multimodal Data](https://arxiv.org/abs/2506.01757)
*Marco Calzavara,Ard Kastrati,Matteo Macchini,Dushan Vasilevski,Roger Wattenhofer*

Main category: cs.CV

TL;DR: This paper explores the trade-offs between accuracy and computational efficiency in egocentric action recognition by varying the sampling frequency of RGB video and 3D hand pose inputs. Reducing the sampling rate of RGB frames while using higher-frequency 3D hand pose input can maintain high accuracy with significantly lower CPU usage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of deploying real-time egocentric action recognition algorithms on wearable XR devices, considering the trade-offs between portability, battery life, and computational resources.

Method: Systematically analyze the impact of sampling frequency across different input modalities (RGB video and 3D hand pose) on egocentric action recognition performance and CPU usage.

Result: Reducing the sampling rate of RGB frames, when complemented with higher-frequency 3D hand pose input, preserves high accuracy while significantly lowering CPU demands. A 3x reduction in CPU usage was observed with minimal to no loss in recognition performance.

Conclusion: Multimodal input strategies are a viable approach to achieving efficient, real-time egocentric action recognition on XR devices.

Abstract: The increasing availability of wearable XR devices opens new perspectives for
Egocentric Action Recognition (EAR) systems, which can provide deeper human
understanding and situation awareness. However, deploying real-time algorithms
on these devices can be challenging due to the inherent trade-offs between
portability, battery life, and computational resources. In this work, we
systematically analyze the impact of sampling frequency across different input
modalities - RGB video and 3D hand pose - on egocentric action recognition
performance and CPU usage. By exploring a range of configurations, we provide a
comprehensive characterization of the trade-offs between accuracy and
computational efficiency. Our findings reveal that reducing the sampling rate
of RGB frames, when complemented with higher-frequency 3D hand pose input, can
preserve high accuracy while significantly lowering CPU demands. Notably, we
observe up to a 3x reduction in CPU usage with minimal to no loss in
recognition performance. This highlights the potential of multimodal input
strategies as a viable approach to achieving efficient, real-time EAR on XR
devices.

</details>


### [663] [unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning](https://arxiv.org/abs/2506.01778)
*Yafei Yang,Zihui Zhang,Bo Yang*

Main category: cs.CV

TL;DR: This paper presents unMORE, a two-stage pipeline for unsupervised multi-object segmentation that significantly outperforms existing methods across various real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for unsupervised multi-object segmentation struggle with complex real-world objects and often only succeed in segmenting simple synthetic ones or discovering a limited number of real-world objects.

Method: unMORE is a two-stage pipeline. The first stage involves learning three levels of object-centric representations. The second stage uses a network-free multi-object reasoning module that leverages these learned priors to discover multiple objects without human labels.

Result: unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, particularly excelling in crowded images where other baselines fail.

Conclusion: unMORE achieves state-of-the-art results in unsupervised multi-object segmentation, demonstrating its effectiveness on complex real-world objects.

Abstract: We study the challenging problem of unsupervised multi-object segmentation on
single images. Existing methods, which rely on image reconstruction objectives
to learn objectness or leverage pretrained image features to group similar
pixels, often succeed only in segmenting simple synthetic objects or
discovering a limited number of real-world objects. In this paper, we introduce
unMORE, a novel two-stage pipeline designed to identify many complex objects in
real-world images. The key to our approach involves explicitly learning three
levels of carefully defined object-centric representations in the first stage.
Subsequently, our multi-object reasoning module utilizes these learned object
priors to discover multiple objects in the second stage. Notably, this
reasoning module is entirely network-free and does not require human labels.
Extensive experiments demonstrate that unMORE significantly outperforms all
existing unsupervised methods across 6 real-world benchmark datasets, including
the challenging COCO dataset, achieving state-of-the-art object segmentation
results. Remarkably, our method excels in crowded images where all baselines
collapse.

</details>


### [664] [Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition](https://arxiv.org/abs/2506.01806)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur*

Main category: cs.CV

TL;DR: The paper proposes a multi-stage transformer-based approach for contactless fingerprint matching that captures global spatial features and refines localized feature alignment, showing superior performance on public datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in contactless fingerprint recognition such as out-of-focus image acquisition, reduced contrast, finger positioning variations, and perspective distortion which hinder accuracy and reliability.

Method: A novel multi-stage transformer-based contactless fingerprint matching method that first captures global spatial features then refines localized feature alignment across samples using a hierarchical feature extraction and matching pipeline.

Result: Extensive evaluations on HKPolyU and RidgeBase datasets under various protocols show the proposed method outperforms existing approaches including COTS solutions.

Conclusion: The proposed multi-stage transformer-based approach enhances contactless fingerprint matching accuracy and reliability by effectively addressing current challenges.

Abstract: The increasing demand for hygienic and portable biometric systems has
underscored the critical need for advancements in contactless fingerprint
recognition. Despite its potential, this technology faces notable challenges,
including out-of-focus image acquisition, reduced contrast between fingerprint
ridges and valleys, variations in finger positioning, and perspective
distortion. These factors significantly hinder the accuracy and reliability of
contactless fingerprint matching. To address these issues, we propose a novel
multi-stage transformer-based contactless fingerprint matching approach that
first captures global spatial features and subsequently refines localized
feature alignment across fingerprint samples. By employing a hierarchical
feature extraction and matching pipeline, our method ensures fine-grained,
cross-sample alignment while maintaining the robustness of global feature
representation. We perform extensive evaluations on publicly available datasets
such as HKPolyU and RidgeBase under different evaluation protocols, such as
contactless-to-contact matching and contactless-to-contactless matching and
demonstrate that our proposed approach outperforms existing methods, including
COTS solutions.

</details>


### [665] [MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs](https://arxiv.org/abs/2506.01850)
*Wayner Barrios,Andrés Villa,Juan León Alcázar,SouYoung Jin,Bernard Ghanem*

Main category: cs.CV

TL;DR: MoDA is a lightweight module that enhances MLLMs' visual grounding by refining pre-aligned visual features through instruction-guided modulation.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle to ground fine-grained visual concepts in complex scenes.

Method: MoDA employs a Transformer-based cross-attention mechanism to generate a modulation mask over the aligned visual tokens, thereby emphasizing semantically relevant embedding dimensions based on the language instruction.

Result: MoDA improves visual grounding and generates more contextually appropriate responses.

Conclusion: MoDA demonstrates its effectiveness as a general-purpose enhancement for image-based MLLMs.

Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated
impressive performance on instruction-following tasks by integrating pretrained
visual encoders with large language models (LLMs). However, existing approaches
often struggle to ground fine-grained visual concepts in complex scenes. In
this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective
module designed to refine pre-aligned visual features through
instruction-guided modulation. Our approach follows the standard LLaVA training
protocol, consisting of a two-stage process: (1) aligning image features to the
LLMs input space via a frozen vision encoder and adapter layers, and (2)
refining those features using the MoDA adapter during the instructional tuning
stage. MoDA employs a Transformer-based cross-attention mechanism to generate a
modulation mask over the aligned visual tokens, thereby emphasizing
semantically relevant embedding dimensions based on the language instruction.
The modulated features are then passed to the LLM for autoregressive language
generation. Our experimental evaluation shows that MoDA improves visual
grounding and generates more contextually appropriate responses, demonstrating
its effectiveness as a general-purpose enhancement for image-based MLLMs.

</details>


### [666] [TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation](https://arxiv.org/abs/2506.01923)
*Amin Karimi Monsefi,Mridul Khurana,Rajiv Ramnath,Anuj Karpatne,Wei-Lun Chao,Cheng Zhang*

Main category: cs.CV

TL;DR: The paper proposes TaxaDiffusion, a framework that uses taxonomy to train diffusion models for generating fine-grained animal images with high accuracy. It progressively trains across different taxonomic levels from broad classifications down to species level, enabling accurate generation even with limited training samples.


<details>
  <summary>Details</summary>
Motivation: Current approaches for generating animal images often treat each species as an independent category, ignoring visual similarities between related species. This can lead to inefficiencies in learning and suboptimal performance when training data is limited.

Method: TaxaDiffusion incorporates domain knowledge by progressively training conditioned diffusion models across different taxonomic levels (Class, Order, Family, Genus, Species). This hierarchical learning strategy captures coarse-grained morphological traits shared by species with common ancestors before refining fine-grained differences at the species level.

Result: Extensive experiments on three fine-grained animal datasets show that TaxaDiffusion outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation, especially when training samples per species are limited.

Conclusion: TaxaDiffusion provides a novel approach for generating fine-grained animal images with high morphological and identity accuracy by leveraging taxonomic relationships. This method enables efficient learning and accurate generation even with limited data.

Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for
diffusion models to generate fine-grained animal images with high morphological
and identity accuracy. Unlike standard approaches that treat each species as an
independent category, TaxaDiffusion incorporates domain knowledge that many
species exhibit strong visual similarities, with distinctions often residing in
subtle variations of shape, pattern, and color. To exploit these relationships,
TaxaDiffusion progressively trains conditioned diffusion models across
different taxonomic levels -- starting from broad classifications such as Class
and Order, refining through Family and Genus, and ultimately distinguishing at
the Species level. This hierarchical learning strategy first captures
coarse-grained morphological traits shared by species with common ancestors,
facilitating knowledge transfer before refining fine-grained differences for
species-level distinction. As a result, TaxaDiffusion enables accurate
generation even with limited training samples per species. Extensive
experiments on three fine-grained animal datasets demonstrate that outperforms
existing approaches, achieving superior fidelity in fine-grained animal image
generation. Project page: https://amink8.github.io/TaxaDiffusion/

</details>


### [667] [MedEBench: Revisiting Text-instructed Image Editing](https://arxiv.org/abs/2506.01921)
*Minghao Liu,Zhitao He,Zhiyuan Fan,Qingyun Wang,Yi R. Fung*

Main category: cs.CV

TL;DR: The paper introduces MedEBench, a benchmark for evaluating text-guided medical image editing with clinically sourced data, an evaluation framework, model comparisons, and failure analysis protocol.


<details>
  <summary>Details</summary>
Motivation: Text-guided image editing has made progress in natural images but lacks adaptation and standardized evaluation in medical imaging. There is clinical potential for such editing in simulating surgical outcomes, creating teaching materials, and improving patient communication.

Method: MedEBench consists of 1,182 image-prompt triplets across 70 tasks and 13 anatomical regions. It includes an evaluation framework covering Editing Accuracy, Contextual Preservation, and Visual Quality, along with ROI masks. The paper also conducts a systematic comparison of seven state-of-the-art models and provides a failure analysis protocol based on attention grounding.

Result: MedEBench reveals common failure patterns among state-of-the-art models and offers insights into mislocalization through IoU between attention maps and ROIs.

Conclusion: MedEBench serves as a foundational benchmark for advancing reliable and clinically meaningful medical image editing systems.

Abstract: Text-guided image editing has seen rapid progress in natural image domains,
but its adaptation to medical imaging remains limited and lacks standardized
evaluation. Clinically, such editing holds promise for simulating surgical
outcomes, creating personalized teaching materials, and enhancing patient
communication. To bridge this gap, we introduce \textbf{MedEBench}, a
comprehensive benchmark for evaluating text-guided medical image editing. It
consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks
across 13 anatomical regions. MedEBench offers three key contributions: (1) a
clinically relevant evaluation framework covering Editing Accuracy, Contextual
Preservation, and Visual Quality, supported by detailed descriptions of
expected change and ROI (Region of Interest) masks; (2) a systematic comparison
of seven state-of-the-art models, revealing common failure patterns; and (3) a
failure analysis protocol based on attention grounding, using IoU between
attention maps and ROIs to identify mislocalization. MedEBench provides a solid
foundation for developing and evaluating reliable, clinically meaningful
medical image editing systems.

</details>


### [668] [Dual-Process Image Generation](https://arxiv.org/abs/2506.01955)
*Grace Luo,Jonathan Granskog,Aleksander Holynski,Trevor Darrell*

Main category: cs.CV

TL;DR: A dual-process distillation scheme is proposed, enabling feed-forward image generators to learn new tasks from deliberative VLMs by using a VLM to rate the generated images and backpropagating this gradient.


<details>
  <summary>Details</summary>
Motivation: Prior methods for controlling image generation are limited in their ability to be taught new tasks. Vision-language models (VLMs) can learn tasks in-context and produce correct outputs for a given input.

Method: The method involves a dual-process distillation scheme that allows feed-forward image generators to learn new tasks from deliberative VLMs. The scheme uses a VLM to rate the generated images and backpropagates this gradient to update the weights of the image generator.

Result: This technique enables multimodal controls for properties such as color palette, line weight, horizon position, and relative depth within a matter of minutes.

Conclusion: The general framework enables a wide variety of new control tasks through the same text-and-image based interface and showcases applications for different types of control signals.

Abstract: Prior methods for controlling image generation are limited in their ability
to be taught new tasks. In contrast, vision-language models, or VLMs, can learn
tasks in-context and produce the correct outputs for a given input. We propose
a dual-process distillation scheme that allows feed-forward image generators to
learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the
generated images and backpropagates this gradient to update the weights of the
image generator. Our general framework enables a wide variety of new control
tasks through the same text-and-image based interface. We showcase a handful of
applications of this technique for different types of control signals, such as
commonsense inferences and visual prompts. With our method, users can implement
multimodal controls for properties such as color palette, line weight, horizon
position, and relative depth within a matter of minutes. Project page:
https://dual-process.github.io.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [669] [Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department](https://arxiv.org/abs/2506.00241)
*Menglin Zhao,Zhuorui Yong,Ruijia Guan,Kai-Wei Chang,Adrian Haimovich,Kei Ouchi,Timothy Bickmore,Bingsheng Yao,Dakuo Wang,Smit Desai*

Main category: cs.HC

TL;DR: In emergency departments, serious illness conversations (SICs) are crucial for patient-centered care but face many challenges. After interviews and thematic analysis, researchers outlined a four-phase SIC workflow and identified needs such as AI support while preserving human connection. They provided design guidelines for integrating AI tools into clinical practices.


<details>
  <summary>Details</summary>
Motivation: To understand current practices in conducting serious illness conversations in Emergency Department settings and identify opportunities for AI support to overcome barriers faced by clinical care teams.

Method: Interviews were conducted with two domain experts and nine ED clinical care team members. Thematic analysis was applied to characterize the workflow of serious illness conversations and identify key needs and challenges.

Result: A four-phase serious illness conversation workflow was characterized: identification, preparation, conduction, and documentation. Key challenges include fragmented EHR data access, time constraints, emotional preparation demands, and documentation burdens. Participants expressed interest in AI tools for information synthesis, conversational support, and automated documentation while emphasizing the importance of maintaining human connection and clinical autonomy.

Conclusion: The study provides an empirical understanding of serious illness conversations in ED settings and offers design considerations for integrating AI tools that support these workflows within existing clinical practices.

Abstract: Serious illness conversations (SICs), discussions between clinical care teams
and patients with serious, life-limiting illnesses about their values, goals,
and care preferences, are critical for patient-centered care. Without these
conversations, patients often receive aggressive interventions that may not
align with their goals. Clinical care teams face significant barriers when
conducting serious illness conversations with older adult patients in Emergency
Department (ED) settings, where most older adult patients lack documented
treatment goals. To understand current practices and identify AI support
opportunities, we conducted interviews with two domain experts and nine ED
clinical care team members. Through thematic analysis, we characterized a
four-phase serious illness conversation workflow (identification, preparation,
conduction, documentation) and identified key needs and challenges at each
stage. Clinical care teams struggle with fragmented EHR data access, time
constraints, emotional preparation demands, and documentation burdens. While
participants expressed interest in AI tools for information synthesis,
conversational support, and automated documentation, they emphasized preserving
human connection and clinical autonomy. We present design guidelines for AI
tools supporting SIC workflows that fit within existing clinical practices.
This work contributes empirical understanding of ED-based serious illness
conversations and provides design considerations for AI in high-stakes clinical
environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [670] [Retrieval-Augmented Generation of Ontologies from Relational Databases](https://arxiv.org/abs/2506.01232)
*Mojtaba Nayyeri,Athish A Yogi,Nadeen Fathallah,Ratan Bahadur Thapa,Hans-Michael Tautenhahn,Anton Schnurpel,Steffen Staab*

Main category: cs.DB

TL;DR: The paper introduces RIGOR, a system that converts relational database schemas into rich OWL ontologies with minimal human intervention by iteratively generating and refining ontology fragments using LLMs and external knowledge sources.


<details>
  <summary>Details</summary>
Motivation: Transforming relational databases into knowledge graphs with enriched ontologies can enhance semantic interoperability and enable advanced graph-based learning and reasoning over data. However, current methods either require significant manual effort or produce only basic ontologies.

Method: RIGOR uses a Retrieval-augmented Generation (RAG) approach to combine the database schema and its documentation, a repository of domain ontologies, and a growing core ontology. It prompts a generative LLM to produce successive delta ontology fragments which are then refined by a judge-LLM before being merged into the core ontology. This process iterates table-by-table following foreign key constraints until all tables are covered.

Result: RIGOR produces ontologies that score highly on standard quality dimensions such as accuracy, completeness, conciseness, adaptability, clarity, and consistency. It significantly reduces the amount of manual effort required compared to previous approaches.

Conclusion: RIGOR provides an effective method for transforming relational schemas into rich OWL ontologies with minimal human effort, enhancing the potential for semantic interoperability and advanced graph-based learning and reasoning.

Abstract: Transforming relational databases into knowledge graphs with enriched
ontologies enhances semantic interoperability and unlocks advanced graph-based
learning and reasoning over data. However, previous approaches either demand
significant manual effort to derive an ontology from a database schema or
produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative
Generation of RDB Ontologies, an LLM-driven approach that turns relational
schemas into rich OWL ontologies with minimal human effort. RIGOR combines
three sources via RAG, the database schema and its documentation, a repository
of domain ontologies, and a growing core ontology, to prompt a generative LLM
for producing successive, provenance-tagged delta ontology fragments. Each
fragment is refined by a judge-LLM before being merged into the core ontology,
and the process iterates table-by-table following foreign key constraints until
coverage is complete. Applied to real-world databases, our approach outputs
ontologies that score highly on standard quality dimensions such as accuracy,
completeness, conciseness, adaptability, clarity, and consistency, while
substantially reducing manual effort.

</details>


### [671] [SIFBench: An Extensive Benchmark for Fatigue Analysis](https://arxiv.org/abs/2506.01173)
*Tushar Gautam,Robert M. Kirby,Jacob Hochhalter,Shandian Zhe*

Main category: cs.DB

TL;DR: The paper introduces SIFBench, an open-source database with over 5 million crack and component geometries for machine learning-based stress intensity factor (SIF) prediction. It provides a Python interface and baseline results using various ML models to lower the entry barrier for ML in damage tolerance design.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of stress intensity factors (SIFs) is crucial for assessing fatigue life and ensuring structural integrity, but progress has been hindered by the lack of high-quality datasets suitable for machine learning.

Method: The authors developed SIFBench, a large-scale benchmark database containing over 5 million geometries from finite element simulations across 37 scenarios. They provide a unified Python interface for data access and customization, along with baseline results from popular ML models and template code for evaluation.

Result: SIFBench significantly lowers the entry barrier for applying machine learning to SIF prediction and fosters advancements in damage tolerance design and predictive maintenance.

Conclusion: SIFBench represents a valuable resource for the development and application of machine learning methods in predicting stress intensity factors, contributing to improved structural integrity and safety.

Abstract: Fatigue-induced crack growth is a leading cause of structural failure across
critical industries such as aerospace, civil engineering, automotive, and
energy. Accurate prediction of stress intensity factors (SIFs) -- the key
parameters governing crack propagation in linear elastic fracture mechanics --
is essential for assessing fatigue life and ensuring structural integrity.
While machine learning (ML) has shown great promise in SIF prediction, its
advancement has been severely limited by the lack of rich, transparent,
well-organized, and high-quality datasets.
  To address this gap, we introduce SIFBench, an open-source, large-scale
benchmark database designed to support ML-based SIF prediction. SIFBench
contains over 5 million different crack and component geometries derived from
high-fidelity finite element simulations across 37 distinct scenarios, and
provides a unified Python interface for seamless data access and customization.
We report baseline results using a range of popular ML models -- including
random forests, support vector machines, feedforward neural networks, and
Fourier neural operators -- alongside comprehensive evaluation metrics and
template code for model training, validation, and assessment. By offering a
standardized and scalable resource, SIFBench substantially lowers the entry
barrier and fosters the development and application of ML methods in damage
tolerance design and predictive maintenance.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [672] [DiffPINN: Generative diffusion-initialized physics-informed neural networks for accelerating seismic wavefield representation](https://arxiv.org/abs/2506.00471)
*Shijun Cheng,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: To overcome the limitations of PINNs in seismic wavefield modeling, such as retraining and slow convergence, this paper proposes a latent diffusion-based strategy for rapid and effective PINN initialization. By employing an autoencoder and conditional diffusion model, the method accelerates training and maintains high accuracy for various velocity models.


<details>
  <summary>Details</summary>
Motivation: Physics-informed neural networks (PINNs) have shown promise in seismic wavefield modeling but face challenges like time-consuming retraining for different velocity models and slow convergence during training due to the complexity of wavefield solutions.

Method: The authors train multiple PINNs to represent frequency-domain scattered wavefields for various velocity models, flatten their parameters into vectors, and use an autoencoder to learn latent representations. A conditional diffusion model is then trained to store the distribution of these latent vectors, conditioned on velocity models. This model generates latent vectors for new velocity models, which are decoded by the autoencoder to produce complete PINN parameters.

Result: Experimental results demonstrate that the proposed method significantly speeds up training while maintaining high accuracy across both in-distribution and out-of-distribution velocity scenarios.

Conclusion: The latent diffusion-based strategy for PINN initialization effectively addresses the challenges of retraining and slow convergence in seismic wavefield modeling, offering a promising approach for future applications.

Abstract: Physics-informed neural networks (PINNs) offer a powerful framework for
seismic wavefield modeling, yet they typically require time-consuming
retraining when applied to different velocity models. Moreover, their training
can suffer from slow convergence due to the complexity of of the wavefield
solution. To address these challenges, we introduce a latent diffusion-based
strategy for rapid and effective PINN initialization. First, we train multiple
PINNs to represent frequency-domain scattered wavefields for various velocity
models, then flatten each trained network's parameters into a one-dimensional
vector, creating a comprehensive parameter dataset. Next, we employ an
autoencoder to learn latent representations of these parameter vectors,
capturing essential patterns across diverse PINN's parameters. We then train a
conditional diffusion model to store the distribution of these latent vectors,
with the corresponding velocity models serving as conditions. Once trained,
this diffusion model can generate latent vectors corresponding to new velocity
models, which are subsequently decoded by the autoencoder into complete PINN
parameters. Experimental results indicate that our method significantly
accelerates training and maintains high accuracy across in-distribution and
out-of-distribution velocity scenarios.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [673] [Using LLMs to Advance the Cognitive Science of Collectives](https://arxiv.org/abs/2506.00052)
*Ilia Sucholutsky,Katherine M. Collins,Nori Jacoby,Bill D. Thompson,Robert D. Hawkins*

Main category: q-bio.NC

TL;DR: LLMs can revolutionize the study of collective cognition, addressing complexities and highlighting risks.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored application of LLMs in studying collective cognition.

Method: Leveraging LLMs to tackle the complexities in studying collectives.

Result: Potential to overcome challenges and introduce new methods for studying collective cognition.

Conclusion: LLMs offer promising opportunities while raising important considerations for the study of collective cognition.

Abstract: LLMs are already transforming the study of individual cognition, but their
application to studying collective cognition has been underexplored. We lay out
how LLMs may be able to address the complexity that has hindered the study of
collectives and raise possible risks that warrant new methods.

</details>


### [674] [Human sensory-musculoskeletal modeling and control of whole-body movements](https://arxiv.org/abs/2506.00071)
*Chenhui Zuo,Guohao Lin,Chen Zhang,Shanning Zhuang,Yanan Sui*

Main category: q-bio.NC

TL;DR: This paper introduces SMS-Human, a sensory-musculoskeletal model that combines precise anatomical representations with multimodal sensory inputs. A stage-wise hierarchical deep reinforcement learning framework was developed for high-dimensional control in musculoskeletal systems. The simulation of movement tasks showed resemblance to natural human motor behaviours and revealed unmeasurable musculoskeletal dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand human movement control and investigate human behaviours by building dynamic models of the sensory-musculoskeletal system.

Method: Integrated precise anatomical representations with multimodal sensory inputs to develop SMS-Human. Applied a stage-wise hierarchical deep reinforcement learning framework to simulate movement tasks including bipedal locomotion, vision-guided object manipulation, and human-machine interaction during bicycling.

Result: The simulation closely resembled natural human motor behaviours and uncovered previously unmeasurable musculoskeletal dynamics.

Conclusion: This work provides deeper insights into sensorimotor dynamics, promotes quantitative understanding of human behaviours, and aids in designing systems with embodied intelligence.

Abstract: Coordinated human movement depends on the integration of multisensory inputs,
sensorimotor transformation, and motor execution, as well as sensory feedback
resulting from body-environment interaction. Building dynamic models of the
sensory-musculoskeletal system is essential for understanding movement control
and investigating human behaviours. Here, we report a human
sensory-musculoskeletal model, termed SMS-Human, that integrates precise
anatomical representations of bones, joints, and muscle-tendon units with
multimodal sensory inputs involving visual, vestibular, proprioceptive, and
tactile components. A stage-wise hierarchical deep reinforcement learning
framework was developed to address the inherent challenges of high-dimensional
control in musculoskeletal systems with integrated multisensory information.
Using this framework, we demonstrated the simulation of three representative
movement tasks, including bipedal locomotion, vision-guided object
manipulation, and human-machine interaction during bicycling. Our results
showed a close resemblance between natural and simulated human motor
behaviours. The simulation also revealed musculoskeletal dynamics that could
not be directly measured. This work sheds deeper insights into the sensorimotor
dynamics of human movements, facilitates quantitative understanding of human
behaviours in interactive contexts, and informs the design of systems with
embodied intelligence.

</details>


### [675] [Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation](https://arxiv.org/abs/2506.00138)
*Reece Keller,Alyn Tornell,Felix Pei,Xaq Pitkow,Leo Kozachkov,Aran Nayebi*

Main category: q-bio.NC

TL;DR: To bridge the gaps in reinforcement learning and systems neuroscience regarding autonomy, this paper introduces 3M-Progress, a novel model-based intrinsic drive that motivates naturalistic behavior by tracking divergence between the agent's current world model and an ethological prior. The method captures behavioral patterns and neural-glial dynamics of autonomously behaving larval zebrafish, providing a foundation for artificial agents with animal-like autonomy.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches fail to produce robust autonomous behaviors observed in animals, and systems neuroscience has largely overlooked the neural basis of autonomy.

Method: The method (3M-Progress) motivates naturalistic behavior by tracking divergence between the agent's current world model and an ethological prior.

Result: Artificial embodied agents trained with 3M-Progress capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously-behaving larval zebrafish, introducing the first goal-driven, population-level model of neural-glial computation.

Conclusion: The findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.

Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and
intelligent behavior in complex environments without relying on external reward
or task structure. Existing reinforcement learning approaches to exploration in
sparse reward and reward-free environments, including class of methods known as
intrinsic motivation, exhibit inconsistent exploration patterns and thus fail
to produce robust autonomous behaviors observed in animals. Moreover, systems
neuroscience has largely overlooked the neural basis of autonomy, focusing
instead on experimental paradigms where animals are motivated by external
reward rather than engaging in unconstrained, naturalistic and task-independent
behavior. To bridge these gaps, we introduce a novel model-based intrinsic
drive explicitly designed to capture robust autonomous exploration observed in
animals. Our method (3M-Progress) motivates naturalistic behavior by tracking
divergence between the agent's current world model and an ethological prior. We
demonstrate that artificial embodied agents trained with 3M-Progress capture
the explainable variance in behavioral patterns and whole-brain neural-glial
dynamics recorded from autonomously-behaving larval zebrafish, introducing the
first goal-driven, population-level model of neural-glial computation. Our
findings establish a computational framework connecting model-based intrinsic
motivation to naturalistic behavior, providing a foundation for building
artificial agents with animal-like autonomy.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [676] [A Foundation Model for Non-Destructive Defect Identification from Vibrational Spectra](https://arxiv.org/abs/2506.00725)
*Mouyang Cheng,Chu-Liang Fu,Bowen Yu,Eunbi Rha,Abhijatmedhi Chotrattanapituk,Douglas L Abernathy,Yongqiang Cheng,Mingda Li*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces DefectNet, a machine learning model that predicts the chemical identity and concentration of substitutional point defects in solids from vibrational spectra. It is trained on simulated spectra and validated on experimental data, demonstrating accuracy and transferability.


<details>
  <summary>Details</summary>
Motivation: Defects in solids significantly impact materials' properties, but non-destructively characterizing and quantifying these defects, especially when multiple types coexist, has been a long-standing challenge.

Method: DefectNet is a foundation machine learning model trained on over 16,000 simulated spectra from 2,000 semiconductors. It uses a tailored attention mechanism to predict up to six distinct defect elements with concentrations ranging from 0.2% to 25%, directly from phonon density-of-states (PDoS).

Result: DefectNet generalizes well to unseen crystals across 56 elements and can be fine-tuned on experimental data. Validation using inelastic scattering measurements shows its accuracy and transferability.

Conclusion: This work establishes vibrational spectroscopy as a viable, non-destructive method for point defect quantification in bulk materials, highlighting the potential of foundation models in data-driven defect engineering.

Abstract: Defects are ubiquitous in solids and strongly influence materials' mechanical
and functional properties. However, non-destructive characterization and
quantification of defects, especially when multiple types coexist, remain a
long-standing challenge. Here we introduce DefectNet, a foundation machine
learning model that predicts the chemical identity and concentration of
substitutional point defects with multiple coexisting elements directly from
vibrational spectra, specifically phonon density-of-states (PDoS). Trained on
over 16,000 simulated spectra from 2,000 semiconductors, DefectNet employs a
tailored attention mechanism to identify up to six distinct defect elements at
concentrations ranging from 0.2% to 25%. The model generalizes well to unseen
crystals across 56 elements and can be fine-tuned on experimental data.
Validation using inelastic scattering measurements of SiGe alloys and MgB$_2$
superconductor demonstrates its accuracy and transferability. Our work
establishes vibrational spectroscopy as a viable, non-destructive probe for
point defect quantification in bulk materials, and highlights the promise of
foundation models in data-driven defect engineering.

</details>


### [677] [Overcoming Data Scarcity in Scanning Tunnelling Microscopy Image Segmentation](https://arxiv.org/abs/2506.01678)
*Nikola L. Kolev,Max Trouton,Filippo Federici Canova,Geoff Thornton,David Z. Gao,Neil J. Curson,Taylor J. Z. Stock*

Main category: cond-mat.mtrl-sci

TL;DR: The paper proposes an automated STM image segmentation approach using few-shot and unsupervised learning, demonstrating strong generalisation capabilities on three distinct surfaces.


<details>
  <summary>Details</summary>
Motivation: To reduce the labour-intensive task of manual STM image analysis and to create a more flexible method that does not require large manually annotated datasets.

Method: An automated approach to the segmentation of STM images that uses both few-shot learning and unsupervised learning.

Result: The model exhibits strong generalisation capabilities and can be adapted to unseen surfaces with as few as one additional labelled data point. It was effective in recognising atomic features on Si(001), Ge(001), and TiO$_2$(110) surfaces.

Conclusion: This work represents a significant step towards efficient and material-agnostic automatic segmentation of STM images.

Abstract: Scanning tunnelling microscopy (STM) is a powerful technique for imaging
surfaces with atomic resolution, providing insight into physical and chemical
processes at the level of single atoms and molecules. A regular task of STM
image analysis is the identification and labelling of features of interest
against a uniform background. Performing this manually is a labour-intensive
task, requiring significant human effort. To reduce this burden, we propose an
automated approach to the segmentation of STM images that uses both few-shot
learning and unsupervised learning. Our technique offers greater flexibility
compared to previous supervised methods; it removes the requirement for large
manually annotated datasets and is thus easier to adapt to an unseen surface
while still maintaining a high accuracy. We demonstrate the effectiveness of
our approach by using it to recognise atomic features on three distinct
surfaces: Si(001), Ge(001), and TiO$_2$(110), including adsorbed AsH$_3$
molecules on the silicon and germanium surfaces. Our model exhibits strong
generalisation capabilities, and following initial training, can be adapted to
unseen surfaces with as few as one additional labelled data point. This work is
a significant step towards efficient and material-agnostic, automatic
segmentation of STM images.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [678] [Tensor Network for Anomaly Detection in the Latent Space of Proton Collision Events at the LHC](https://arxiv.org/abs/2506.00102)
*Ema Puljak,Maurizio Pierini,Artur Garcia-Saez*

Main category: hep-ph

TL;DR: The paper proposes a tensor network-based strategy for anomaly detection at the LHC, demonstrating superior performance compared to established quantum methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for constant innovation in algorithms and technologies to discover new phenomena at the LHC.

Method: A parametrized Matrix Product State with an isometric feature map, processing a latent representation of simulated LHC data generated by an autoencoder.

Result: Superior performance in identifying new phenomena compared to established quantum methods.

Conclusion: Tensor networks have the potential to enhance new-physics discovery at the LHC.

Abstract: The pursuit of discovering new phenomena at the Large Hadron Collider (LHC)
demands constant innovation in algorithms and technologies. Tensor networks are
mathematical models on the intersection of classical and quantum machine
learning, which present a promising and efficient alternative for tackling
these challenges. In this work, we propose a tensor network-based strategy for
anomaly detection at the LHC and demonstrate its superior performance in
identifying new phenomena compared to established quantum methods. Our model is
a parametrized Matrix Product State with an isometric feature map, processing a
latent representation of simulated LHC data generated by an autoencoder. Our
results highlight the potential of tensor networks to enhance new-physics
discovery.

</details>


### [679] [Generator Based Inference (GBI)](https://arxiv.org/abs/2506.00119)
*Chi Lung Cheng,Ranit Das,Runze Li,Radha Mastandrea,Vinicius Mikuni,Benjamin Nachman,David Shih,Gup Singh*

Main category: hep-ph

TL;DR: The paper proposes Generator Based Inference (GBI) framework that integrates machine learning with generators, including Simulation Based Inference (SBI), and focuses on resonant anomaly detection using data-driven generators to perform parameter estimation.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for integrating machine learning with generators in statistical inference for physics, expanding beyond traditional simulation-based methods to incorporate data-driven generator approaches.

Method: Propose the GBI framework, examine methods within it like SBI and focus on resonant anomaly detection using data-driven generators learned from sidebands for parameter estimation.

Result: Achieved new state-of-the-art for anomaly detection sensitivity on the LHCO community benchmark dataset, making statistical outputs directly interpretable.

Conclusion: The GBI framework shows potential in transforming statistical outputs of anomaly detection to be directly interpretable, advancing the field with data-derived generators.

Abstract: Statistical inference in physics is often based on samples from a generator
(sometimes referred to as a ``forward model") that emulate experimental data
and depend on parameters of the underlying theory. Modern machine learning has
supercharged this workflow to enable high-dimensional and unbinned analyses to
utilize much more information than ever before. We propose a general framework
for describing the integration of machine learning with generators called
Generator Based Inference (GBI). A well-studied special case of this setup is
Simulation Based Inference (SBI) where the generator is a physics-based
simulator. In this work, we examine other methods within the GBI toolkit that
use data-driven methods to build the generator. In particular, we focus on
resonant anomaly detection, where the generator describing the background is
learned from sidebands. We show how to perform machine learning-based parameter
estimation in this context with data-derived generators. This transforms the
statistical outputs of anomaly detection to be directly interpretable and the
performance on the LHCO community benchmark dataset establishes a new
state-of-the-art for anomaly detection sensitivity.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [680] [Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments](https://arxiv.org/abs/2506.00352)
*Chinkit Patel,Kee Siong Ng*

Main category: cs.DC

TL;DR: Many large enterprises lack an efficient way to support their Data and AI teams. This paper proposes a solution in the form of an on-demand self-service data-platform infrastructure.


<details>
  <summary>Details</summary>
Motivation: To provide Data and AI teams with a way to rapidly spin up and tear down self-service data and compute infrastructure, experiment with new tools, and deploy data products into operational use.

Method: Propose an efficient method leveraging immutable container operating systems and infrastructure-as-code methodologies to create vendor-neutral and short-lived Kubernetes clusters both on-premises and in any cloud environment.

Result: The proposed approach can serve as a repeatable, portable and cost-efficient alternative or complement to commercial Platform-as-a-Service (PaaS) offerings.

Conclusion: This approach is particularly important for supporting interoperability in complex data mesh environments with a mix of modern and legacy compute infrastructure.

Abstract: Many large enterprises that operate highly governed and complex ICT
environments have no efficient and effective way to support their Data and AI
teams in rapidly spinning up and tearing down self-service data and compute
infrastructure, to experiment with new data analytic tools, and deploy data
products into operational use. This paper proposes a key piece of the solution
to the overall problem, in the form of an on-demand self-service data-platform
infrastructure to empower de-centralised data teams to build data products on
top of centralised templates, policies and governance. The core innovation is
an efficient method to leverage immutable container operating systems and
infrastructure-as-code methodologies for creating, from scratch, vendor-neutral
and short-lived Kubernetes clusters on-premises and in any cloud environment.
Our proposed approach can serve as a repeatable, portable and cost-efficient
alternative or complement to commercial Platform-as-a-Service (PaaS) offerings,
and this is particularly important in supporting interoperability in complex
data mesh environments with a mix of modern and legacy compute infrastructure.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [681] [Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models](https://arxiv.org/abs/2506.00033)
*Valerie Tsao,Nathaniel W. Chaney,Manolis Veveakis*

Main category: stat.AP

TL;DR: 本研究提出了一种条件数据插补框架，通过结合扩散模型和预克里金掩模，可以从低至1%的观测覆盖率中重建完整的温度场，从而填补历史再分析和实时预测中的关键数据空白。


<details>
  <summary>Details</summary>
Motivation: 气候模型依赖于准确的初始条件假设，但由于系统的混沌性质和数据稀缺性，这一假设面临挑战。特别是在全球尺度和百年时间尺度上，数据缺失是常见现象。不确定性来源包括稀疏、噪声观测以及模型内部简化带来的变异性。因此，需要一种方法在极端稀疏条件下重建完整状态场。

Method: 研究提出了一种条件数据插补框架，利用扩散模型和由预克里金掩模引导的方法，从极低观测覆盖率（如1%）的数据中重建完整的温度场。该方法通过数据同化技术，在部分观测条件下推断出完整状态场。

Result: 该框架在不同观测密度下均表现出强大的重建准确性，验证区域为南大平原，时间范围为2018-2020年夏季下午时段的温度场。结果表明，该模型在历史再分析和实时预报中有潜在应用价值。

Conclusion: 所提出的条件数据插补框架能够在极端稀疏观测条件下有效重建温度场，为解决气候模型中的数据缺口问题提供了新途径，具有重要的实际应用前景。

Abstract: The large underlying assumption of climate models today relies on the basis
of a "confident" initial condition, a reasonably plausible snapshot of the
Earth for which all future predictions depend on. However, given the inherently
chaotic nature of our system, this assumption is complicated by sensitive
dependence, where small uncertainties in initial conditions can lead to
exponentially diverging outcomes over time. This challenge is particularly
salient at global spatial scales and over centennial timescales, where data
gaps are not just common but expected. The source of uncertainty is two-fold:
(1) sparse, noisy observations from satellites and ground stations, and (2)
internal variability stemming from the simplifying approximations within the
models themselves.
  In practice, data assimilation methods are used to reconcile this missing
information by conditioning model states on partial observations. Our work
builds on this idea but operates at the extreme end of sparsity. We propose a
conditional data imputation framework that reconstructs full temperature fields
from as little as 1% observational coverage. The method leverages a diffusion
model guided by a prekriged mask, effectively inferring the full-state fields
from minimal data points. We validate our framework over the Southern Great
Plains, focusing on afternoon (12:00-6:00 PM) temperature fields during the
summer months of 2018-2020. Across varying observational densities--from swath
data to isolated in-situ sensors--our model achieves strong reconstruction
accuracy, highlighting its potential to fill in critical data gaps in both
historical reanalysis and real-time forecasting pipelines.

</details>


### [682] [Probabilistic intraday electricity price forecasting using generative machine learning](https://arxiv.org/abs/2506.00044)
*Jieyu Chen,Sebastian Lerch,Melanie Schienle,Tomasz Serafin,Rafał Weron*

Main category: stat.AP

TL;DR: The paper presents a new generative neural network model for forecasting intraday electricity prices in Germany, which supports trading strategies and outperforms benchmark methods in terms of profit gains.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing significance of intraday electricity trading in Europe, necessitating better price forecasting and decision-support tools.

Method: A novel generative neural network model is proposed to create probabilistic path forecasts for intraday electricity prices, aiding in constructing trading strategies for Germany's intraday market.

Result: The method performs competitively against state-of-the-art benchmarks in statistical metrics and yields higher profit gains in a fixed-volume trading scenario.

Conclusion: Generative machine learning models show promise in electricity price forecasting and economic evaluation is crucial.

Abstract: The growing importance of intraday electricity trading in Europe calls for
improved price forecasting and tailored decision-support tools. In this paper,
we propose a novel generative neural network model to generate probabilistic
path forecasts for intraday electricity prices and use them to construct
effective trading strategies for Germany's continuous-time intraday market. Our
method demonstrates competitive performance in terms of statistical evaluation
metrics compared to two state-of-the-art statistical benchmark approaches. To
further assess its economic value, we consider a realistic fixed-volume trading
scenario and propose various strategies for placing market sell orders based on
the path forecasts. Among the different trading strategies, the price paths
generated by our generative model lead to higher profit gains than the
benchmark methods. Our findings highlight the potential of generative machine
learning tools in electricity price forecasting and underscore the importance
of economic evaluation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [683] [Improving statistical learning methods via features selection without replacement sampling and random projection](https://arxiv.org/abs/2506.00053)
*Sulaiman khan,Muhammad Ahmad,Fida Ullah,Carlos Aguilar Ibañez,José Eduardo Valdez Rodriguez*

Main category: q-bio.QM

TL;DR: The paper proposes a machine learning approach for cancer classification using microarray data, incorporating FSWOR, projection method, and ensemble classifiers with LDA and Naïve Bayes, achieving a test score of 96%.


<details>
  <summary>Details</summary>
Motivation: Cancer is a genetic disease characterized by genetic and epigenetic alterations leading to uncontrolled cell growth. High-dimensional microarray datasets present challenges for classification models due to the 'small n, large p' problem, resulting in overfitting.

Method: 1) A machine learning-based approach integrating the Feature Selection Without Replacement (FSWOR) technique and a projection method was proposed. 2) The Kendall statistical test was applied to identify significant genes from the brain cancer microarray dataset, reducing the feature space. 3) Machine learning models were used with k-fold cross validation techniques, incorporating ensemble classifiers with LDA projection and Naïve Bayes.

Result: The model achieved a test score of 96%, outperforming existing methods by 9.09%. It demonstrated effectiveness in high-dimensional gene expression analysis, improving classification accuracy while mitigating overfitting.

Conclusion: This study contributes to cancer biomarker discovery by offering a robust computational method for analyzing microarray data.

Abstract: Cancer is fundamentally a genetic disease characterized by genetic and
epigenetic alterations that disrupt normal gene expression, leading to
uncontrolled cell growth and metastasis. High-dimensional microarray datasets
pose challenges for classification models due to the "small n, large p"
problem, resulting in overfitting. This study makes three different key
contributions: 1) we propose a machine learning-based approach integrating the
Feature Selection Without Re-placement (FSWOR) technique and a projection
method to improve classification accuracy. 2) We apply the Kendall statistical
test to identify the most significant genes from the brain cancer mi-croarray
dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3)
we apply machine learning models using k-fold cross validation techniques in
which our model incorpo-rates ensemble classifiers with LDA projection and
Na\"ive Bayes, achieving a test score of 96%, outperforming existing methods by
9.09%. The results demonstrate the effectiveness of our ap-proach in
high-dimensional gene expression analysis, improving classification accuracy
while mitigating overfitting. This study contributes to cancer biomarker
discovery, offering a robust computational method for analyzing microarray
data.

</details>


### [684] [Enhancing Drug Discovery: Autoencoder-Based Latent Space Augmentation for Improved Molecular Solubility Prediction using LatMixSol](https://arxiv.org/abs/2506.00223)
*Mohammad Saleh Hasankhani*

Main category: q-bio.QM

TL;DR: 提出了一种新的潜在空间增强框架LatMixSol，结合了基于自动编码器的特征压缩和引导插值，以丰富训练数据并提高分子溶解度预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测分子溶解度是药物发现早期阶段的关键，但传统机器学习模型由于标记数据有限和分子描述符的高维特性而面临重大挑战。

Method: 首先使用两层自动编码器将分子描述符编码到低维潜在空间中。然后应用光谱聚类将化学相似的分子分组，从而在聚类内进行目标MixUp样插值。通过混合聚类成员的潜在向量并将其解码回原始特征空间来生成合成样本。

Result: 在Huuskonen溶解度基准上评估，LatMixSol在四个梯度提升回归器中的三个（CatBoost、LightGBM、HistGradientBoosting）上表现出一致的改进，RMSE降低3.2-7.6%，R平方增加0.5-1.5%。特别是HistGradientBoosting显示出最显著的增强，RMSE提高了7.6%。

Conclusion: 集群引导的潜在空间增强保留了化学有效性，同时扩大了数据集多样性，为资源受限的药物发现管道提供了一种计算高效的策略来增强预测模型。

Abstract: Accurate prediction of molecular solubility is a cornerstone of early-stage
drug discovery, yet conventional machine learning models face significant
challenges due to limited labeled data and the high-dimensional nature of
molecular descriptors. To address these issues, we propose LatMixSol, a novel
latent space augmentation framework that combines autoencoder-based feature
compression with guided interpolation to enrich training data. Our approach
first encodes molecular descriptors into a low-dimensional latent space using a
two-layer autoencoder. Spectral clustering is then applied to group chemically
similar molecules, enabling targeted MixUp-style interpolation within clusters.
Synthetic samples are generated by blending latent vectors of cluster members
and decoding them back to the original feature space. Evaluated on the
Huuskonen solubility benchmark, LatMixSol demonstrates consistent improvements
across three of four gradient-boosted regressors (CatBoost, LightGBM,
HistGradientBoosting), achieving RMSE reductions of 3.2-7.6% and R-squared
increases of 0.5-1.5%. Notably, HistGradientBoosting shows the most significant
enhancement with a 7.6% RMSE improvement. Our analysis confirms that
cluster-guided latent space augmentation preserves chemical validity while
expanding dataset diversity, offering a computationally efficient strategy to
enhance predictive models in resource-constrained drug discovery pipelines.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [685] [React to Surprises: Stable-by-Design Neural Feedback Control and the Youla-REN](https://arxiv.org/abs/2506.01226)
*Nicholas H. Barbara,Ruigang Wang,Alexandre Megretski,Ian R. Manchester*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study parameterizations of stabilizing nonlinear policies for
learning-based control. We propose a structure based on a nonlinear version of
the Youla-Ku\v{c}era parameterization combined with robust neural networks such
as the recurrent equilibrium network (REN). The resulting parameterizations are
unconstrained, and hence can be searched over with first-order optimization
methods, while always ensuring closed-loop stability by construction. We study
the combination of (a) nonlinear dynamics, (b) partial observation, and (c)
incremental closed-loop stability requirements (contraction and Lipschitzness).
We find that with any two of these three difficulties, a contracting and
Lipschitz Youla parameter always leads to contracting and Lipschitz closed
loops. However, if all three hold, then incremental stability can be lost with
exogenous disturbances. Instead, a weaker condition is maintained, which we
call d-tube contraction and Lipschitzness. We further obtain converse results
showing that the proposed parameterization covers all contracting and Lipschitz
closed loops for certain classes of nonlinear systems. Numerical experiments
illustrate the utility of our parameterization when learning controllers with
built-in stability certificates for: i) ``economic'' rewards without
stabilizing effects; ii) short training horizons; and iii) uncertain systems.

</details>


### [686] [Interpretable reinforcement learning for heat pump control through asymmetric differentiable decision trees](https://arxiv.org/abs/2506.01641)
*Toon Van Puyvelde,Mehran Zareh,Chris Develder*

Main category: eess.SY

TL;DR: A novel asymmetric soft DDT construction method is proposed to improve the efficient use of decision nodes in home energy management systems, enhancing both interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of deep reinforcement learning (DRL) limits its adoption by energy management companies due to lack of transparent decision-making feedback. Explainable reinforcement learning (XRL), especially soft differential decision tree (DDT) distillation, aims to address this issue but suffers from reduced interpretability when deep, full trees are needed for high performance.

Method: Propose an asymmetric soft DDT construction method that adaptively constructs trees by expanding nodes only when necessary, improving the efficient use of decision nodes and enhancing both interpretability and performance.

Result: Asymmetric DDTs have the potential to provide transparent, efficient, and high-performing decision-making in home energy management systems.

Conclusion: Asymmetric soft DDTs could be a promising approach in explainable reinforcement learning for home energy management systems.

Abstract: In recent years, deep reinforcement learning (DRL) algorithms have gained
traction in home energy management systems. However, their adoption by energy
management companies remains limited due to the black-box nature of DRL, which
fails to provide transparent decision-making feedback. To address this,
explainable reinforcement learning (XRL) techniques have emerged, aiming to
make DRL decisions more transparent. Among these, soft differential decision
tree (DDT) distillation provides a promising approach due to the clear decision
rules they are based on, which can be efficiently computed. However, achieving
high performance often requires deep, and completely full, trees, which reduces
interpretability. To overcome this, we propose a novel asymmetric soft DDT
construction method. Unlike traditional soft DDTs, our approach adaptively
constructs trees by expanding nodes only when necessary. This improves the
efficient use of decision nodes, which require a predetermined depth to
construct full symmetric trees, enhancing both interpretability and
performance. We demonstrate the potential of asymmetric DDTs to provide
transparent, efficient, and high-performing decision-making in home energy
management systems.

</details>


### [687] [Data-assimilated model-informed reinforcement learning](https://arxiv.org/abs/2506.01755)
*Defne E. Ozan,Andrea Nóvoa,Georgios Rigas,Luca Magri*

Main category: eess.SY

TL;DR: This paper presents DA-MIRL, a method that combines low-order models, data assimilation, and reinforcement learning to control chaotic systems with partial and noisy observations. It successfully suppresses chaos in the Kuramoto-Sivashinsky equation.


<details>
  <summary>Details</summary>
Motivation: Controlling spatio-temporally chaotic systems is challenging due to high dimensionality and unpredictability. Traditional model-free reinforcement learning requires full state observations, but real-world sensors often provide only partial and noisy measurements.

Method: The proposed method, DA-MIRL, integrates three components: (i) low-order models to approximate high-dimensional dynamics; (ii) sequential data assimilation to correct model predictions when observations are available; and (iii) an off-policy actor-critic RL algorithm to learn an optimal control strategy based on corrected state estimates.

Result: DA-MIRL successfully estimates and suppresses chaotic dynamics in real time from partial observations and approximate models when tested on the spatiotemporally chaotic solutions of the Kuramoto-Sivashinsky equation.

Conclusion: This work provides new opportunities for controlling partially observable chaotic systems.

Abstract: The control of spatio-temporally chaos is challenging because of high
dimensionality and unpredictability. Model-free reinforcement learning (RL)
discovers optimal control policies by interacting with the system, typically
requiring observations of the full physical state.In practice, sensors often
provide only partial and noisy measurements (observations) of the system. The
objective of this paper is to develop a framework that enables the control of
chaotic systems with partial and noisy observability. The proposed method,
data-assimilated model-informed reinforcement learning (DA-MIRL), integrates
(i) low-order models to approximate high-dimensional dynamics; (ii) sequential
data assimilation to correct the model prediction when observations become
available; and (iii) an off-policy actor-critic RL algorithm to adaptively
learn an optimal control strategy based on the corrected state estimates. We
test DA-MIRL on the spatiotemporally chaotic solutions of the
Kuramoto-Sivashinsky equation. We estimate the full state of the environment
with (i) a physics-based model, here, a coarse-grained model; and (ii) a
data-driven model, here, the control-aware echo state network, which is
proposed in this paper. We show that DA-MIRL successfully estimates and
suppresses the chaotic dynamics of the environment in real time from partial
observations and approximate models. This work opens opportunities for the
control of partially observable chaotic systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [688] [Literature Review Of Multi-Agent Debate For Problem-Solving](https://arxiv.org/abs/2506.00066)
*Arne Tillmann*

Main category: cs.MA

TL;DR: Multi-agent large language models (MA-LLMs) use multiple interacting agents to solve complex tasks better than single-agent models, but they have higher computational costs and unique challenges.


<details>
  <summary>Details</summary>
Motivation: To address the lack of direct comparisons in the field of MA-LLMs and understand how factors like scalability, communication structure, and decision-making processes influence their performance.

Method: Literature review synthesizing the latest research on agent profiles, communication structures, and decision-making processes from both traditional multi-agent systems and state-of-the-art MA-LLM studies.

Result: Multi-agent approaches can yield superior results in solving complex tasks, but they face elevated computational costs and under-explored challenges unique to MA-LLM.

Conclusion: The findings provide researchers and practitioners with a roadmap for developing robust and efficient multi-agent AI solutions.

Abstract: Multi-agent large language models (MA-LLMs) are a rapidly growing research
area that leverages multiple interacting language agents to tackle complex
tasks, outperforming single-agent large language models. This literature review
synthesizes the latest research on agent profiles, communication structures,
and decision-making processes, drawing insights from both traditional
multi-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims
to address the lack of direct comparisons in the field, illustrating how
factors like scalability, communication structure, and decision-making
processes influence MA-LLM performance. By examining frequent practices and
outlining current challenges, the review reveals that multi-agent approaches
can yield superior results but also face elevated computational costs and
under-explored challenges unique to MA-LLM. Overall, these findings provide
researchers and practitioners with a roadmap for developing robust and
efficient multi-agent AI solutions.

</details>


### [689] [Sorrel: A simple and flexible framework for multi-agent reinforcement learning](https://arxiv.org/abs/2506.00228)
*Rebekah A. Gelpí,Yibing Ju,Ethan C. Jackson,Yikai Tang,Shon Verch,Claas Voelcker,William A. Cunningham*

Main category: cs.MA

TL;DR: The paper introduces Sorrel, a Python interface for creating and testing multi-agent reinforcement learning environments, emphasizing simplicity and accessibility to help social scientists study group dynamics.


<details>
  <summary>Details</summary>
Motivation: To provide a tool that is simple and accessible for social scientists to investigate how learning and social interaction lead to the development and change of group dynamics.

Method: Designing and implementing Sorrel, a Python interface with a psychologically intuitive structure for the basic agent-environment loop in multi-agent reinforcement learning environments.

Result: Sorrel offers an easy-to-use platform for generating and testing new multi-agent reinforcement learning environments, making it suitable for social scientists.

Conclusion: Sorrel is presented as a useful tool with its design philosophy and features outlined, aiming to facilitate research into group dynamics through learning and social interaction.

Abstract: We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple
Python interface for generating and testing new multi-agent reinforcement
learning environments. This interface places a high degree of emphasis on
simplicity and accessibility, and uses a more psychologically intuitive
structure for the basic agent-environment loop, making it a useful tool for
social scientists to investigate how learning and social interaction leads to
the development and change of group dynamics. In this short paper, we outline
the basic design philosophy and features of Sorrel.

</details>


### [690] [Agentic AI and Multiagentic: Are We Reinventing the Wheel?](https://arxiv.org/abs/2506.01463)
*V. Botti*

Main category: cs.MA

TL;DR: This paper criticizes the misuse of 'Agentic AI' and 'Multiagentic AI', advocating for the use of established terminology in AI literature.


<details>
  <summary>Details</summary>
Motivation: To clarify the confusion between recent popular terms 'Agentic AI' and 'Multiagentic AI' with well-established concepts in AI literature such as intelligent agents and multi-agent systems.

Method: Reviewing theoretical origins of 'agentic' from social sciences and philosophy, summarizing foundational works on intelligent agents and multi-agent systems, examining classic agent architectures and discussing recent developments in LLMs and agent platforms based on LLMs.

Result: The term 'AI Agentic' is often misused for what are essentially AI agents, and 'AI Multiagentic' for multi-agent systems, ignoring decades of research in autonomous agents and multi-agent systems.

Conclusion: The article advocates for scientific and technological rigour, encouraging the incorporation of existing knowledge and standards into the new wave of LLM-based AI agents to avoid reinventing the wheel.

Abstract: The terms Agentic AI and Multiagentic AI have recently gained popularity in
discussions on generative artificial intelligence, often used to describe
autonomous software agents and systems composed of such agents. However, the
use of these terms confuses these buzzwords with well-established concepts in
AI literature: intelligent agents and multi-agent systems. This article offers
a critical analysis of this conceptual misuse. We review the theoretical
origins of "agentic" in the social sciences (Bandura, 1986) and philosophical
notions of intentionality (Dennett, 1971), and then summarise foundational
works on intelligent agents and multi-agent systems by Wooldridge, Jennings and
others. We examine classic agent architectures, from simple reactive agents to
Belief-Desire-Intention (BDI) models, and highlight key properties (autonomy,
reactivity, proactivity, social capability) that define agency in AI. We then
discuss recent developments in large language models (LLMs) and agent platforms
based on LLMs, including the emergence of LLM-powered AI agents and open-source
multi-agent orchestration frameworks. We argue that the term AI Agentic is
often used as a buzzword for what are essentially AI agents, and AI
Multiagentic for what are multi-agent systems. This confusion overlooks decades
of research in the field of autonomous agents and multi-agent systems. The
article advocates for scientific and technological rigour and the use of
established terminology from the state of the art in AI, incorporating the
wealth of existing knowledge, including standards for multi-agent system
platforms, communication languages and coordination and cooperation algorithms,
agreement technologies (automated negotiation, argumentation, virtual
organisations, trust, reputation, etc.), into the new and promising wave of
LLM-based AI agents, so as not to end up reinventing the wheel.

</details>
