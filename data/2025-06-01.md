<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 37]
- [cs.LG](#cs.LG) [Total: 163]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.DB](#cs.DB) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [econ.GN](#econ.GN) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.RO](#cs.RO) [Total: 7]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 39]
- [math.NA](#math.NA) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.NE](#cs.NE) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 7]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.DS](#cs.DS) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.GT](#cs.GT) [Total: 2]
- [eess.SP](#eess.SP) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Design and testing of an agent chatbot supporting decision making with public transport data](https://arxiv.org/abs/2505.22698)
*Luca Fantin,Marco Antonelli,Margherita Cesetti,Daniele Irto,Bruno Zamengo,Francesco Silvestri*

Main category: cs.AI

TL;DR: This paper presents a chatbot based on an agent architecture that interacts with datasets of public transportation services, executes SQL queries, visualizes data, and supports decision making. It also addresses the challenge of collecting data to measure the chatbot's performance by creating a dataset through extensive testing.


<details>
  <summary>Details</summary>
Motivation: To simplify the complex process of analyzing large quantities of data related to public transportation services for most users, offering a user-friendly tool for interacting with these datasets and supporting decision making.

Method: Developing a chatbot based on an agent architecture that extends the capabilities of a core Large Language Model (LLM). The chatbot can perform SQL queries, plot data, and create maps from trip coordinates. Additionally, implementing a workflow to extensively test the chatbot, generating a dataset to evaluate its performance in terms of answer consistency and query correctness.

Result: The chatbot has been extensively tested, resulting in a dataset that evaluates its performance, particularly focusing on the consistency of its answers and the correctness of the generated queries.

Conclusion: A chatbot was developed to facilitate interaction with public transportation service datasets and support decision making. The paper also successfully addressed the issue of collecting data to measure the chatbot's performance.

Abstract: Assessing the quality of public transportation services requires the analysis
of large quantities of data on the scheduled and actual trips and documents
listing the quality constraints each service needs to meet. Interrogating such
datasets with SQL queries, organizing and visualizing the data can be quite
complex for most users. This paper presents a chatbot offering a user-friendly
tool to interact with these datasets and support decision making. It is based
on an agent architecture, which expands the capabilities of the core Large
Language Model (LLM) by allowing it to interact with a series of tools that can
execute several tasks, like performing SQL queries, plotting data and creating
maps from the coordinates of a trip and its stops. This paper also tackles one
of the main open problems of such Generative AI projects: collecting data to
measure the system's performance. Our chatbot has been extensively tested with
a workflow that asks several questions and stores the generated query, the
retrieved data and the natural language response for each of them. Such
questions are drawn from a set of base examples which are then completed with
actual data from the database. This procedure yields a dataset for the
evaluation of the chatbot's performance, especially the consistency of its
answers and the correctness of the generated queries.

</details>


### [2] [Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields](https://arxiv.org/abs/2505.22753)
*Arseniy Pertzovsky,Roni Stern,Ariel Felner,Roie Zivan*

Main category: cs.AI

TL;DR: The paper explores the application of Artificial Potential Fields (APFs) in solving Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. While APF does not benefit MAPF, it significantly boosts LMAPF's system throughput by up to 7 times.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of Artificial Potential Fields (APFs) in enhancing the performance of algorithms designed for Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems.

Method: Integrate APFs into various MAPF algorithms such as Prioritized Planning, MAPF-LNS2, and Priority Inheritance with Backtracking (PIBT) to solve both MAPF and LMAPF problems.

Result: Experimental results indicate that incorporating APFs does not provide advantages for MAPF problems but can increase overall system throughput for LMAPF problems by up to a 7-fold factor.

Conclusion: Artificial Potential Fields are more advantageous for Lifelong MAPF problems compared to standard MAPF scenarios.

Abstract: We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent
Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of
agents must move to their goal locations without collisions, whereas in LMAPF,
new goals are generated upon arrival. We propose methods for incorporating APFs
in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and
Priority Inheritance with Backtracking (PIBT). Experimental results show that
using APF is not beneficial for MAPF but yields up to a 7-fold increase in
overall system throughput for LMAPF.

</details>


### [3] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: Mathematical reasoning tasks are key benchmarks for LLMs. While RL methods like GRPO boost performance, accuracy alone doesn't assess capabilities well. We propose decomposing problem-solving into Plan, Execute, and Verify skills. GRPO mainly enhances execution robustness via 'temperature distillation' but struggles with new problems due to insufficient planning ('coverage wall'). A synthetic task confirms these findings and identifies conditions under which RL could improve exploration and generalization.


<details>
  <summary>Details</summary>
Motivation: To better understand the capabilities of LLMs in mathematical reasoning tasks beyond mere accuracy metrics, by assessing which problem-solving skills have been internalized.

Method: Decompose problem solving into fundamental capabilities: Plan, Execute, and Verify. Use empirical analysis on GRPO-enhanced models to study skill improvements. Construct a minimal synthetic solution-tree navigation task to explore RL's impact on exploration and generalization.

Result: GRPO primarily boosts execution robustness (temperature distillation) but struggles with new problems (coverage wall). The synthetic task replicates these findings and identifies potential conditions for overcoming the coverage wall.

Conclusion: RL can enhance LLM reasoning, particularly execution robustness, but has limitations in planning and generalization. Insights suggest a path toward overcoming these barriers.

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [4] [Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds](https://arxiv.org/abs/2505.22779)
*Mohammad Helal Uddin,Sabur Baidya*

Main category: cs.AI

TL;DR: The paper proposes a hybrid approach using smartphone sensor data and social media analysis to evaluate depression levels, achieving high accuracy in activity recognition, sentiment analysis, and depression severity classification without compromising personal privacy.


<details>
  <summary>Details</summary>
Motivation: Mental disorders like depression and anxiety are significant global challenges, especially among individuals with social avoidance tendencies. There is a need for effective methods to monitor depression without breaching personal privacy.

Method: The study uses CNN-based deep learning models and Naive Bayes classification to identify physical activities and classify user sentiments from Twitter interactions. It recruits 33 participants for data acquisition, extracts nine features, and evaluates weekly depression scores using the Geriatric Depression Scale (GDS) questionnaire.

Result: The approach achieves an activity recognition accuracy of 95% and a sentiment analysis accuracy of 95.6%. Several physical activity features significantly correlate with depression symptom severity. The SVM-based algorithm for classifying depression severity demonstrates an accuracy of 94%, outperforming other models.

Conclusion: This hybrid method is simple yet highly effective for long-term depression monitoring without compromising personal privacy.

Abstract: Mental disorders including depression, anxiety, and other neurological
disorders pose a significant global challenge, particularly among individuals
exhibiting social avoidance tendencies. This study proposes a hybrid approach
by leveraging smartphone sensor data measuring daily physical activities and
analyzing their social media (Twitter) interactions for evaluating an
individual's depression level. Using CNN-based deep learning models and Naive
Bayes classification, we identify human physical activities accurately and also
classify the user sentiments. A total of 33 participants were recruited for
data acquisition, and nine relevant features were extracted from the physical
activities and analyzed with their weekly depression scores, evaluated using
the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six
are derived from physical activities, achieving an activity recognition
accuracy of 95%, while three features stem from sentiment analysis of Twitter
activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several
physical activity features exhibited significant correlations with the severity
of depression symptoms. For classifying the depression severity, a support
vector machine (SVM)-based algorithm is employed that demonstrated a very high
accuracy of 94%, outperforming alternative models, e.g., the multilayer
perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly
effective in the long run for monitoring depression without breaching personal
privacy.

</details>


### [5] [The WHY in Business Processes: Unification of Causal Process Models](https://arxiv.org/abs/2505.22871)
*Yuval David,Fabiana Fournier,Lior Limonad,Inna Skarbovsky*

Main category: cs.AI

TL;DR: The paper proposes a novel method to unify multiple causal process variants into a consistent model, addressing challenges in handling missing values and expressing alternating conditions.


<details>
  <summary>Details</summary>
Motivation: Causal reasoning is crucial for business process interventions but existing methods lack the ability to capture alternating causal conditions across multiple variants, leading to challenges in handling missing values and expressing these conditions among log splits.

Method: A novel method is proposed to unify multiple causal process variants into a consistent model. This method preserves the correctness of original causal models while explicitly representing their causal-flow alternations.

Result: The method has been formally defined, proved, and evaluated on three open and two proprietary datasets, demonstrating its effectiveness.

Conclusion: The proposed method successfully addresses the limitations of previous approaches by unifying causal process variants into a consistent model, with an open-source implementation released.

Abstract: Causal reasoning is essential for business process interventions and
improvement, requiring a clear understanding of causal relationships among
activity execution times in an event log. Recent work introduced a method for
discovering causal process models but lacked the ability to capture alternating
causal conditions across multiple variants. This raises the challenges of
handling missing values and expressing the alternating conditions among log
splits when blending traces with varying activities.
  We propose a novel method to unify multiple causal process variants into a
consistent model that preserves the correctness of the original causal models,
while explicitly representing their causal-flow alternations. The method is
formally defined, proved, evaluated on three open and two proprietary datasets,
and released as an open-source implementation.

</details>


### [6] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
*Massimiliano Pronesti,Michela Lorandi,Paul Flanagan,Oisin Redmon,Anya Belz,Yufang Hou*

Main category: cs.AI

TL;DR: This paper presents a numeric reasoning system for systematic reviews in medicine, which extracts structured numerical evidence and applies domain knowledge to derive outcome-specific conclusions. The system includes a numeric data extraction model trained via supervised fine-tuning and reinforcement learning. Evaluated on CochraneForest benchmark, it shows significant improvements over retrieval-based systems and large language models.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews in medicine are crucial for evidence-based decision-making but face bottlenecks in automating the process of extracting numeric evidence and determining study-level conclusions. Existing methods rely on textual inference, which may fail to capture numeric reasoning behind expert assessments.

Method: The authors conceptualise the problem as one of quantitative reasoning, developing a numeric reasoning system that consists of a numeric data extraction model and an effect estimate component. They train the numeric data extraction model using different strategies including supervised fine-tuning (SFT) and reinforcement learning (RL) with a new value reward model.

Result: On the CochraneForest benchmark, the best-performing approach -- using RL to train a small-scale number extraction model -- yields up to a 21% absolute improvement in F1 score over retrieval-based systems and outperforms general-purpose LLMs by up to 9%.

Conclusion: The results demonstrate the promise of reasoning-driven approaches for automating systematic evidence synthesis in medicine.

Abstract: Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [7] [Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages](https://arxiv.org/abs/2505.22948)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.AI

TL;DR: The paper introduces Foundation Molecular Grammar (FMG), which uses multi-modal foundation models to create an interpretable molecular language, improving on previous grammar learning methods in molecular generation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current grammar learning methods that rely on expert annotation or unreliable heuristics, by introducing a new approach that leverages multi-modal foundation models for better interpretability.

Method: FMG renders molecules as images, describes them as text, and aligns information across modalities using prompt learning, exploiting the chemical knowledge of an MMFM.

Result: FMG excels in synthesizability, diversity, and data efficiency while offering built-in chemical interpretability for automated molecular discovery workflows.

Conclusion: FMG can be used as a drop-in replacement for prior grammar learning approaches in molecular generation and property prediction.

Abstract: Recent data-efficient molecular generation approaches exploit graph grammars
to introduce interpretability into the generative models. However, grammar
learning therein relies on expert annotation or unreliable heuristics for
algorithmic inference. We propose Foundation Molecular Grammar (FMG), which
leverages multi-modal foundation models (MMFMs) to induce an interpretable
molecular language. By exploiting the chemical knowledge of an MMFM, FMG
renders molecules as images, describes them as text, and aligns information
across modalities using prompt learning. FMG can be used as a drop-in
replacement for the prior grammar learning approaches in molecular generation
and property prediction. We show that FMG not only excels in synthesizability,
diversity, and data efficiency but also offers built-in chemical
interpretability for automated molecular discovery workflows. Code is available
at https://github.com/shiningsunnyday/induction.

</details>


### [8] [Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/abs/2505.22954)
*Jenny Zhang,Shengran Hu,Cong Lu,Robert Lange,Jeff Clune*

Main category: cs.AI

TL;DR: The paper introduces Darwin Gödel Machine (DGM), a self-improving AI system that modifies its own code and empirically validates each change using coding benchmarks, inspired by Darwinian evolution. The DGM significantly enhances performance on coding tasks compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Today's AI systems have fixed architectures and cannot autonomously and continuously improve themselves. Automating the advance of AI could accelerate development and allow benefits sooner.

Method: DGM iteratively modifies its own code and improves its ability to modify itself. It maintains an archive of generated coding agents, grows the archive by sampling an agent and creating a new version, forming a tree of diverse high-quality agents for parallel exploration.

Result: Empirically, DGM automatically improves its coding capabilities, increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. It outperforms baselines without self-improvement or open-ended exploration.

Conclusion: DGM is a significant step toward self-improving AI, capable of gathering stepping stones along paths leading to endless innovation.

Abstract: Today's AI systems have human-designed, fixed architectures and cannot
autonomously and continuously improve themselves. The advance of AI could
itself be automated. If done safely, that would accelerate AI development and
allow us to reap its benefits much sooner. Meta-learning can automate the
discovery of novel algorithms, but is limited by first-order improvements and
the human design of a suitable search space. The G\"odel machine proposed a
theoretical alternative: a self-improving AI that repeatedly modifies itself in
a provably beneficial manner. Unfortunately, proving that most changes are net
beneficial is impossible in practice. We introduce the Darwin G\"odel Machine
(DGM), a self-improving system that iteratively modifies its own code (thereby
also improving its ability to modify its own codebase) and empirically
validates each change using coding benchmarks. Inspired by Darwinian evolution
and open-endedness research, the DGM maintains an archive of generated coding
agents. It grows the archive by sampling an agent from it and using a
foundation model to create a new, interesting, version of the sampled agent.
This open-ended exploration forms a growing tree of diverse, high-quality
agents and allows the parallel exploration of many different paths through the
search space. Empirically, the DGM automatically improves its coding
capabilities (e.g., better code editing tools, long-context window management,
peer-review mechanisms), increasing performance on SWE-bench from 20.0% to
50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly
outperforms baselines without self-improvement or open-ended exploration. All
experiments were done with safety precautions (e.g., sandboxing, human
oversight). The DGM is a significant step toward self-improving AI, capable of
gathering its own stepping stones along paths that unfold into endless
innovation.

</details>


### [9] [Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness](https://arxiv.org/abs/2505.22960)
*Yongjin Yang,Euiin Yi,Jongwoo Ko,Kimin Lee,Zhijing Jin,Se-Young Yun*

Main category: cs.AI

TL;DR: 论文通过系统化研究多智能体辩论（MAD）方法，揭示其在数学推理任务中相较于自我智能体扩展的优势有限，但在任务难度增加和模型能力下降时更有效；而在安全性任务中，MAD的协作改进可能增加漏洞，但多样化的智能体配置有助于逐步降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究利用多智能体辩论（MAD）方法，但对其在不同条件下的有效性与单智能体方法的系统性比较尚不明确。因此，本文旨在填补这一空白，将MAD视为一种测试时计算扩展技术，探索其协同改进和多样化探索的能力。

Method: 对MAD与强大的单智能体测试时扩展基线在数学推理和安全相关任务上进行全面实证对比。系统考察任务难度、模型规模和智能体多样性对MAD性能的影响。

Result: 1. 数学推理：MAD相较于单智能体扩展优势有限，但随着问题难度增加和模型能力下降表现更佳，智能体多样性影响较小。
2. 安全任务：MAD的协作改进可能增加漏洞，但通过引入多样化的智能体配置可逐步降低攻击成功率。

Conclusion: 本研究结果为未来更高效、更有策略地部署MAD系统提供了关键指导。

Abstract: The remarkable growth in large language model (LLM) capabilities has spurred
exploration into multi-agent systems, with debate frameworks emerging as a
promising avenue for enhanced problem-solving. These multi-agent debate (MAD)
approaches, where agents collaboratively present, critique, and refine
arguments, potentially offer improved reasoning, robustness, and diverse
perspectives over monolithic models. Despite prior studies leveraging MAD, a
systematic understanding of its effectiveness compared to self-agent methods,
particularly under varying conditions, remains elusive. This paper seeks to
fill this gap by conceptualizing MAD as a test-time computational scaling
technique, distinguished by collaborative refinement and diverse exploration
capabilities. We conduct a comprehensive empirical investigation comparing MAD
with strong self-agent test-time scaling baselines on mathematical reasoning
and safety-related tasks. Our study systematically examines the influence of
task difficulty, model scale, and agent diversity on MAD's performance. Key
findings reveal that, for mathematical reasoning, MAD offers limited advantages
over self-agent scaling but becomes more effective with increased problem
difficulty and decreased model capability, while agent diversity shows little
benefit. Conversely, for safety tasks, MAD's collaborative refinement can
increase vulnerability, but incorporating diverse agent configurations
facilitates a gradual reduction in attack success through the collaborative
refinement process. We believe our findings provide critical guidance for the
future development of more effective and strategically deployed MAD systems.

</details>


### [10] [Strategic Reflectivism In Intelligent Systems](https://arxiv.org/abs/2505.22987)
*Nick Byrd*

Main category: cs.AI

TL;DR: 通过综合20世纪中期的有界理性概念与21世纪对人类和机器实验的最新结果，本文提出了战略性反思主义（Strategic Reflectivism）。该理论强调智能系统（无论是人类还是人工）的关键在于实用地在直觉和反思推理之间进行切换，以最优地满足竞争目标。


<details>
  <summary>Details</summary>
Motivation: 由于20世纪末理性大战引发了关于直觉和反思思维的本质和规范的辩论，而这些辩论基于20世纪中期的有界理性等思想挑战了自19世纪以来的理想化理性观念。随着21世纪的认知科学家将这些双过程理论应用于人工智能，有必要回顾历史教训并结合新成果进行整合。

Method: 文章综合了20世纪中期关于有界理性的思想以及21世纪针对人类和机器的实验结果，提出了一种新的理论框架——战略性反思主义，强调在直觉和反思推理之间的实用切换。

Result: 提出战略性反思主义理论，表明智能系统的核心在于能够在直觉和反思推理之间灵活切换，从而最优实现竞争性目标。此外，该理论超越了简单指标（如模型规模或思考链条），随着对直觉和反思价值理解的加深，其实际应用性不断增强。

Conclusion: 战略性反思主义为理解和设计智能系统提供了一个新的视角，强调了在直觉和反思推理之间进行实用切换的重要性，并建立在美国实用主义的基础上。

Abstract: By late 20th century, the rationality wars had launched debates about the
nature and norms of intuitive and reflective thinking. Those debates drew from
mid-20th century ideas such as bounded rationality, which challenged more
idealized notions of rationality observed since the 19th century. Now that 21st
century cognitive scientists are applying the resulting dual process theories
to artificial intelligence, it is time to dust off some lessons from this
history. So this paper synthesizes old ideas with recent results from
experiments on humans and machines. The result is Strategic Reflectivism, which
takes the position that one key to intelligent systems (human or artificial) is
pragmatic switching between intuitive and reflective inference to optimally
fulfill competing goals. Strategic Reflectivism builds on American Pragmatism,
transcends superficial indicators of reflective thinking such as model size or
chains of thought, and becomes increasingly actionable as we learn more about
the value of intuition and reflection.

</details>


### [11] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/abs/2505.22990)
*Pin-Han Chen,Yu-Sheng Lin,Wei-Cheng Lee,Tin-Yu Leu,Po-Hsiang Hsu,Anjana Dissanayake,Sungjin Oh,Chinq-Shiun Chiu*

Main category: cs.AI

TL;DR: The paper presents MenTeR, a multiagent AI framework for automating and enhancing the analog circuit design process.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in manual analog circuit design such as intricacy, time-consumption, and reliance on expert intuition.

Method: MenTeR uses multiple specialized AI agents that collaborate to handle different aspects of the design process including specification understanding, circuit optimization, and test bench validation.

Result: MenTeR accelerates the design cycle time, facilitates broader exploration of the design space, and demonstrates robust capabilities in handling real-world analog systems.

Conclusion: MenTeR provides a foundation for future 'RF/Analog Copilots' that can work seamlessly with human designers.

Abstract: RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [12] [Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction](https://arxiv.org/abs/2505.23034)
*Guangyi Liu,Yongqi Zhang,Xunyuan Liu,Quanming Yao*

Main category: cs.AI

TL;DR: A new framework named CBR-DDI is proposed, which uses pharmacological principles from historical cases to improve LLM reasoning for DDI tasks. It constructs a knowledge repository through LLMs and GNNs, applies hybrid retrieval mechanism and dual-layer prompting, and introduces representative sampling strategy. Experiments show it achieves SOTA performance with 28.7% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of applying large language models (LLMs) effectively in drug-drug interaction (DDI) prediction for treatment safety.

Method: CBR-DDI constructs a knowledge repository by leveraging LLMs to extract pharmacological insights and graph neural networks (GNNs) to model drug associations. A hybrid retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to effectively retrieve and reuse relevant cases. Also, a representative sampling strategy for dynamic case refinement is introduced.

Result: CBR-DDI achieves state-of-the-art performance, with a significant 28.7% accuracy improvement over both popular LLMs and CBR baseline, while maintaining high interpretability and flexibility.

Conclusion: CBR-DDI, inspired by clinical practice, significantly improves LLM reasoning for DDI tasks, achieving superior performance, interpretability, and flexibility.

Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety.
While large language models (LLMs) show promise in pharmaceutical tasks, their
effectiveness in DDI prediction remains challenging. Inspired by the
well-established clinical practice where physicians routinely reference similar
historical cases to guide their decisions through case-based reasoning (CBR),
we propose CBR-DDI, a novel framework that distills pharmacological principles
from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI
constructs a knowledge repository by leveraging LLMs to extract pharmacological
insights and graph neural networks (GNNs) to model drug associations. A hybrid
retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to
effectively retrieve and reuse relevant cases. We further introduce a
representative sampling strategy for dynamic case refinement. Extensive
experiments demonstrate that CBR-DDI achieves state-of-the-art performance,
with a significant 28.7% accuracy improvement over both popular LLMs and CBR
baseline, while maintaining high interpretability and flexibility.

</details>


### [13] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
*Yutong Xie,Zhuoheng Li,Xiyuan Wang,Yijun Pan,Qijia Liu,Xingzhi Cui,Kuang-Yu Lo,Ruoyi Gao,Xingjian Zhang,Jin Huang,Walter Yuan,Matthew O. Jackson,Qiaozhu Mei*

Main category: cs.AI

TL;DR: Be.FM is an open foundation model designed for human behavior modeling, which can predict behaviors, infer characteristics, generate insights and apply behavioral science knowledge.


<details>
  <summary>Details</summary>
Motivation: Foundation models have been successful in many fields, but their potential in modeling and understanding human behavior has not been fully explored.

Method: Be.FM is built upon open-source large language models and fine-tuned on a diverse range of behavioral data. A comprehensive set of benchmark tasks is constructed to test the capabilities of behavioral foundation models.

Result: Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.

Conclusion: Be.FM represents one of the first open foundation models designed specifically for human behavior modeling.

Abstract: Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [14] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/abs/2505.23075)
*Amit Kumthekar,Zion Tilley,Henry Duong,Bhargav Patel,Michael Magnoli,Ahmed Omar,Ahmed Nasser,Chaitanya Gharpure,Yevgen Reztzov*

Main category: cs.AI

TL;DR: The paper introduces Consensus Mechanism, an ensemble framework mimicking clinical decision-making to improve and adapt LLMs in medical evaluations. It outperforms individual models on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the risks of obsolescence and rigid dependence on single model systems in current LLM applications for clinical tasks.

Method: Developed a novel framework called Consensus Mechanism which employs an ensemble of specialized medical expert agents to enhance clinical decision making and maintain robust adaptability.

Result: On various medical benchmarks (MedMCQA, MedQA, MedXpertQA), the Consensus Mechanism showed higher accuracy compared to other models like OpenAI's O3 and Google's Gemini 2.5 Pro. It also performed better in differential diagnosis generation with improved recall, precision, and top-1 accuracy.

Conclusion: The Consensus Mechanism provides a flexible and effective solution for improving LLM performance in clinical decision-making, showing significant improvements across different evaluation metrics.

Abstract: Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [15] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
*Zeyu Liu,Yuhang Liu,Guanghao Zhu,Congkai Xie,Zhen Li,Jianbo Yuan,Xinyao Wang,Qing Li,Shing-Chi Cheung,Shengyu Zhang,Fei Wu,Hongxia Yang*

Main category: cs.AI

TL;DR: The paper presents Infi-MMR, a framework to improve reasoning in Multimodal Small Language Models (MSLMs) through three phases, resulting in the model Infi-MMR-3B with advanced multimodal reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in extending reasoning capabilities from large language models to multimodal models, particularly for smaller models with weaker reasoning foundations.

Method: A novel framework named Infi-MMR is designed with three phases: Foundational Reasoning Activation using textual datasets, Cross-Modal Reasoning Adaptation with caption-augmented data, and Multimodal Reasoning Enhancement using curated caption-free data.

Result: Infi-MMR-3B achieves state-of-the-art performance in multimodal math reasoning (43.68% on MathVerse testmini, 27.04% on MathVision test, 21.33% on OlympiadBench) and general reasoning (67.2% on MathVista testmini).

Conclusion: The Infi-MMR framework successfully unlocks the reasoning potential of MSLMs, demonstrating significant advancements in both multimodal and general reasoning abilities.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [16] [A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy](https://arxiv.org/abs/2505.23397)
*Ahmad Mohsin,Helge Janicke,Ahmed Ibrahim,Iqbal H. Sarker,Seyit Camtepe*

Main category: cs.AI

TL;DR: This article proposes a structured framework for Human-AI collaboration in Security Operations Centers (SOCs) with five levels of AI autonomy, mapped to Human-in-the-Loop roles and task-specific trust thresholds. It includes a case study on cybersecurity AI-Avatar, demonstrating reduced alert fatigue and enhanced response coordination.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI.

Method: A novel autonomy tiered framework is proposed, grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop (HITL) roles and task-specific trust thresholds.

Result: The framework enables adaptive and explainable AI integration across core SOC functions, reducing alert fatigue, enhancing response coordination, and strategically calibrating trust.

Conclusion: This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.

Abstract: This article presents a structured framework for Human-AI collaboration in
Security Operations Centers (SOCs), integrating AI autonomy, trust calibration,
and Human-in-the-loop decision making. Existing frameworks in SOCs often focus
narrowly on automation, lacking systematic structures to manage human
oversight, trust calibration, and scalable autonomy with AI. Many assume static
or binary autonomy settings, failing to account for the varied complexity,
criticality, and risk across SOC tasks considering Humans and AI collaboration.
To address these limitations, we propose a novel autonomy tiered framework
grounded in five levels of AI autonomy from manual to fully autonomous, mapped
to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This
enables adaptive and explainable AI integration across core SOC functions,
including monitoring, protection, threat detection, alert triage, and incident
response. The proposed framework differentiates itself from previous research
by creating formal connections between autonomy, trust, and HITL across various
SOC levels, which allows for adaptive task distribution according to
operational complexity and associated risks. The framework is exemplified
through a simulated cyber range that features the cybersecurity AI-Avatar, a
fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates
human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing
response coordination, and strategically calibrating trust. This research
systematically presents both the theoretical and practical aspects and
feasibility of designing next-generation cognitive SOCs that leverage AI not to
replace but to enhance human decision-making.

</details>


### [17] [Conceptual Framework Toward Embodied Collective Adaptive Intelligence](https://arxiv.org/abs/2505.23153)
*Fan Wang,Shaoshan Liu*

Main category: cs.AI

TL;DR: Collective Adaptive Intelligence (CAI) is a transformative approach in AI that allows autonomous agents to collaborate, adapt, and self-organize. This article introduces a conceptual framework for designing and analyzing CAI.


<details>
  <summary>Details</summary>
Motivation: To provide a structured foundation for understanding and implementing Collective Adaptive Intelligence (CAI) to guide researchers in developing more resilient, scalable, and adaptable AI systems.

Method: Introduces a conceptual framework delineating key attributes including task generalization, resilience, scalability, and self-assembly to bridge theoretical foundations with practical methodologies.

Result: The framework aims to facilitate robust performance of AI systems in real-world scenarios by enabling them to reconfigure in response to unforeseen challenges.

Conclusion: This work seeks to guide researchers and practitioners in engineering adaptive, emergent intelligence across various domains through the proposed framework.

Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in
artificial intelligence, wherein numerous autonomous agents collaborate, adapt,
and self-organize to navigate complex, dynamic environments. This paradigm is
particularly impactful in embodied AI applications, where adaptability and
resilience are paramount. By enabling systems to reconfigure themselves in
response to unforeseen challenges, CAI facilitate robust performance in
real-world scenarios. This article introduces a conceptual framework for
designing and analyzing CAI. It delineates key attributes including task
generalization, resilience, scalability, and self-assembly, aiming to bridge
theoretical foundations with practical methodologies for engineering adaptive,
emergent intelligence. By providing a structured foundation for understanding
and implementing CAI, this work seeks to guide researchers and practitioners in
developing more resilient, scalable, and adaptable AI systems across various
domains.

</details>


### [18] [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/abs/2505.23281)
*Mislav Balunović,Jasper Dekoninck,Ivo Petrov,Nikola Jovanović,Martin Vechev*

Main category: cs.AI

TL;DR: 尽管数学基准测试有所进步，但在线可用性可能混淆了推理和记忆。MathArena是一个新基准，使用实时数学竞赛问题评估大型语言模型，发现AIME 2024存在污染迹象，而在SMT 2025等更难的比赛中，顶级模型表现出强大的推理能力。此外，MathArena首次对证明写作能力进行评估，结果显示顶级模型在USAMO 2025上的得分低于25％。


<details>
  <summary>Details</summary>
Motivation: 当前常用的数学评估数据集在线上广泛可得，难以区分真实推理与潜在的记忆效果，并且缺乏对证明写作能力的评价。

Method: 引入MathArena这一新基准，利用定期举办的数学竞赛题目对大型语言模型进行实时评估，以消除预先接触题目导致的污染风险，并涵盖证明写作能力的评估。

Result: 在AIME 2024中发现明显的污染迹象，而SMT 2025等较新的竞赛显示顶级模型具备出色的推理能力；但在USAMO 2025证明写作任务中，顶级模型表现较差，得分低于25%。

Conclusion: MathArena为评估大型语言模型的数学推理和证明写作能力提供了一个严谨、与时俱进的框架，将继续跟踪模型在新发布竞赛中的进展。

Abstract: The rapid advancement of reasoning capabilities in large language models
(LLMs) has led to notable improvements on mathematical benchmarks. However,
many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely
available online, making it difficult to disentangle genuine reasoning from
potential memorization. Furthermore, these benchmarks do not evaluate
proof-writing capabilities, which are crucial for many mathematical tasks. To
address this, we introduce MathArena, a new benchmark based on the following
key insight: recurring math competitions provide a stream of high-quality,
challenging problems that can be used for real-time evaluation of LLMs. By
evaluating models as soon as new problems are released, we effectively
eliminate the risk of contamination. Using this framework, we find strong signs
of contamination in AIME 2024. Nonetheless, evaluations on harder competitions,
such as SMT 2025 -- published well after model release dates -- demonstrate
impressive reasoning capabilities in top-performing models. MathArena is also
the first benchmark for proof-writing capabilities. On USAMO 2025, even top
models score below 25%, far behind their performance on final-answer tasks. So
far, we have evaluated 30 models across five competitions, totaling 149
problems. As an evolving benchmark, MathArena will continue to track the
progress of LLMs on newly released competitions, ensuring rigorous and
up-to-date evaluation of mathematical reasoning.

</details>


### [19] [AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning](https://arxiv.org/abs/2505.23381)
*Bowen Ping,Minnan Luo,Zhuohang Dang,Chenxi Wang,Chengyou Jia*

Main category: cs.AI

TL;DR: AutoGPS is a neuro-symbolic collaborative framework that combines neural cross-modal comprehension and symbolic reasoning to solve geometry problems with high reliability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Geometry problem solving requires exceptional multimodal comprehension and rigorous mathematical reasoning capabilities. Existing approaches have limitations in reliability and interpretability.

Method: AutoGPS employs a Multimodal Problem Formalizer (MPF) and a Deductive Symbolic Reasoner (DSR). The MPF translates geometry problems into structured formal language representations with feedback from DSR. The DSR formulates geometry problem solving as a hypergraph expansion task, executing mathematically rigorous and reliable derivation to produce minimal and human-readable stepwise solutions.

Result: Extensive experimental evaluations demonstrate that AutoGPS achieves state-of-the-art performance on benchmark datasets. Human stepwise-reasoning evaluation confirms AutoGPS's impressive reliability and interpretability, with 99% stepwise logical coherence.

Conclusion: AutoGPS addresses the challenges of geometry problem solving by providing concise, reliable, and human-interpretable reasoning processes.

Abstract: Geometry problem solving presents distinctive challenges in artificial
intelligence, requiring exceptional multimodal comprehension and rigorous
mathematical reasoning capabilities. Existing approaches typically fall into
two categories: neural-based and symbolic-based methods, both of which exhibit
limitations in reliability and interpretability. To address this challenge, we
propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry
problems with concise, reliable, and human-interpretable reasoning processes.
Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a
Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal
comprehension to translate geometry problems into structured formal language
representations, with feedback from DSR collaboratively. The DSR takes the
formalization as input and formulates geometry problem solving as a hypergraph
expansion task, executing mathematically rigorous and reliable derivation to
produce minimal and human-readable stepwise solutions. Extensive experimental
evaluations demonstrate that AutoGPS achieves state-of-the-art performance on
benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms
AutoGPS's impressive reliability and interpretability, with 99\% stepwise
logical coherence. The project homepage is at
https://jayce-ping.github.io/AutoGPS-homepage.

</details>


### [20] [GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning](https://arxiv.org/abs/2505.23399)
*Jusheng Zhang,Yijia Fan,Wenjun Lin,Ruiqi Chen,Haoyi Jiang,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: GAM-Agent is a game-theoretic multi-agent framework that enhances vision-language reasoning through structured communication and uncertainty-aware collaboration, significantly improving performance on multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness and interpretability of vision-language reasoning by formulating it as a non-zero-sum game between specialized agents.

Method: The framework consists of base agents specializing in visual perception subtasks and a critical agent verifying logic consistency and factual correctness. They communicate via structured claims, evidence, and uncertainty estimates. An uncertainty-aware controller dynamically adjusts agent collaboration, triggering multi-round debates when needed.

Result: Experiments on four benchmarks show significant improvements across various VLM backbones, with accuracy boosts of 5--6% for small-to-mid scale models and up to 2--3% for strong models like GPT-4o.

Conclusion: GAM-Agent offers a modular, scalable, and generalizable approach toward reliable and explainable multi-agent multimodal reasoning.

Abstract: We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing
vision-language reasoning. Unlike prior single-agent or monolithic models,
GAM-Agent formulates the reasoning process as a non-zero-sum game between base
agents--each specializing in visual perception subtasks--and a critical agent
that verifies logic consistency and factual correctness. Agents communicate via
structured claims, evidence, and uncertainty estimates. The framework
introduces an uncertainty-aware controller to dynamically adjust agent
collaboration, triggering multi-round debates when disagreement or ambiguity is
detected. This process yields more robust and interpretable predictions.
Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and
V*Bench--demonstrate that GAM-Agent significantly improves performance across
various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid
scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\%, and still enhances
strong models like GPT-4o by up to 2--3\%. Our approach is modular, scalable,
and generalizable, offering a path toward reliable and explainable multi-agent
multimodal reasoning.

</details>


### [21] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/abs/2505.23432)
*Elisa Celis,Lingxiao Huang,Nisheeth K. Vishnoi*

Main category: cs.AI

TL;DR: The paper presents a mathematical framework modeling jobs, workers, and worker-job fit, decomposing skills into decision-level and action-level subskills to show how GenAI complements human skills.


<details>
  <summary>Details</summary>
Motivation: To understand the role of Generative AI in relation to human workers and determine under what conditions GenAI complements rather than replaces human skills.

Method: Developed a mathematical model that categorizes skills into decision-level and action-level subskills. Analyzed changes in subskill abilities and their impact on job success probability, identifying sufficient conditions for improved performance through complementary subskills.

Result: Identified conditions where combining workers with complementary subskills outperforms single workers. Demonstrated alignment with real-world data from O*NET and Big-Bench Lite.

Conclusion: GenAI can significantly complement human skills, especially for lower-skilled workers, enhancing overall productivity without necessarily replacing human workers.

Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [22] [Emergent Risk Awareness in Rational Agents under Resource Constraints](https://arxiv.org/abs/2505.23436)
*Daniel Jarne Ornia,Nicholas Bishop,Joel Dyer,Wei-Chen Lee,Ani Calinescu,Doyne Farme,Michael Wooldridge*

Main category: cs.AI

TL;DR: The paper explores how AI agents behave under resource constraints using a survival bandit framework, identifying conditions leading to misalignment and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: AI agents need to make decisions under resource or failure constraints which can lead to implicit trade-offs and potential misalignment with human objectives.

Method: Formalise the problem through a survival bandit framework, provide theoretical and empirical analysis, identify conditions for misalignment, and propose mitigation mechanisms.

Result: Quantified the impact of survival-driven preference shifts, identified conditions for misalignment, and proposed ways to mitigate risk-seeking or risk-averse behaviours.

Conclusion: This work enhances understanding of AI agent behaviour under survival pressure and provides guidelines for safe deployment in critical environments.

Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed
to interact with humans and to solve sequential decision-making problems under
(approximate) utility functions and internal models. When such problems have
resource or failure constraints where action sequences may be forcibly
terminated once resources are exhausted, agents face implicit trade-offs that
reshape their utility-driven (rational) behaviour. Additionally, since these
agents are typically commissioned by a human principal to act on their behalf,
asymmetries in constraint exposure can give rise to previously unanticipated
misalignment between human objectives and agent incentives. We formalise this
setting through a survival bandit framework, provide theoretical and empirical
results that quantify the impact of survival-driven preference shifts, identify
conditions under which misalignment emerges and propose mechanisms to mitigate
the emergence of risk-seeking or risk-averse behaviours. As a result, this work
aims to increase understanding and interpretability of emergent behaviours of
AI agents operating under such survival pressure, and offer guidelines for
safely deploying such AI systems in critical resource-limited environments.

</details>


### [23] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473)
*Xiaorui Wu,Xiaofeng Mao,Fei Li,Xin Zhang,Xiaolu Zhang,Jun Zhou,Yuxiang Peng,Li Zheng,Chong Teng,Donghong Ji,Zhuang Li*

Main category: cs.AI

TL;DR: The paper introduces EVOREFUSE, a prompt optimization approach to generate diverse pseudo-malicious instructions that cause large language models (LLMs) to refuse responses unnecessarily. It creates two datasets, EVOREFUSE-TEST and EVOREFUSE-ALIGN, which help evaluate and mitigate over-refusals in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for collecting pseudo-malicious instructions either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts.

Method: EVOREFUSE employs an evolutionary algorithm exploring the instruction space via mutation strategies and recombination to iteratively evolve seed instructions and maximize evidence lower bound on LLM refusal probability.

Result: Using EVOREFUSE, two novel datasets were created: EVOREFUSE-TEST outperforms other benchmarks with higher average refusal triggering rates, lexical diversity, and improved LLM response confidence scores across 9 LLMs. EVOREFUSE-ALIGN provides responses for supervised and preference-based alignment training, reducing over-refusals by up to 14.31% without compromising safety.

Conclusion: Models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. EVOREFUSE helps evaluate and mitigate over-refusals in LLMs.

Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


### [24] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
*Xiang Li,Haiyang Yu,Xinghua Zhang,Ziyang Huang,Shizhu He,Kang Liu,Jun Zhao,Fei Huang,Yongbin Li*

Main category: cs.AI

TL;DR: Process Reward Models (PRMs) are important for complex reasoning tasks. However, current benchmarks do not systematically evaluate PRMs under various reasoning patterns. To address this, the authors introduce Socratic-PRMBench, a benchmark with 2995 flawed reasoning paths across six reasoning patterns. Experiments reveal deficiencies in existing PRMs, highlighting the need for improvement.


<details>
  <summary>Details</summary>
Motivation: PRMs are crucial for verifying intermediate reasoning steps in complex tasks. However, current benchmarks lack systematic evaluation of PRMs under diverse reasoning patterns.

Method: Introduced Socratic-PRMBench, a benchmark comprising 2995 flawed reasoning paths across six reasoning patterns (Transformation, Decomposition, Regather, Deduction, Verification, Integration). Evaluated both PRMs and LLMs prompted as critic models on this benchmark.

Result: Identified significant deficiencies in existing PRMs when evaluating reasoning steps under various patterns.

Conclusion: Socratic-PRMBench provides a comprehensive testbed for systematic PRM evaluation and highlights areas for future PRM development.

Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [25] [Autoformalization in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2505.23486)
*Ke Weng,Lun Du,Sirui Li,Wangyue Lu,Haozhe Sun,Hengyu Liu,Tiancheng Zhang*

Main category: cs.AI

TL;DR: Autoformalization is the process of transforming informal mathematical propositions into verifiable formal representations. This survey reviews recent advances in autoformalization, examining its applications across various mathematical domains and levels of difficulty, analyzing the end-to-end workflow from data preprocessing to model design and evaluation, exploring its role in enhancing the verifiability of LLM-generated outputs, summarizing key open-source models and datasets supporting current research, and discussing open challenges and promising future directions for the field.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of recent advances in autoformalization from both mathematical and LLM-centric perspectives.

Method: Examine how autoformalization is applied across various mathematical domains and levels of difficulty, analyze the end-to-end workflow from data preprocessing to model design and evaluation, explore the emerging role of autoformalization in enhancing the verifiability of LLM-generated outputs.

Result: Summarize key open-source models and datasets supporting current research, and discuss open challenges and promising future directions for the field.

Conclusion: Autoformalization has witnessed substantial growth driven by the rapid progress in artificial intelligence, particularly large language models (LLMs), bringing both new opportunities and unique challenges.

Abstract: Autoformalization, the process of transforming informal mathematical
propositions into verifiable formal representations, is a foundational task in
automated theorem proving, offering a new perspective on the use of mathematics
in both theoretical and applied domains. Driven by the rapid progress in
artificial intelligence, particularly large language models (LLMs), this field
has witnessed substantial growth, bringing both new opportunities and unique
challenges. In this survey, we provide a comprehensive overview of recent
advances in autoformalization from both mathematical and LLM-centric
perspectives. We examine how autoformalization is applied across various
mathematical domains and levels of difficulty, and analyze the end-to-end
workflow from data preprocessing to model design and evaluation. We further
explore the emerging role of autoformalization in enhancing the verifiability
of LLM-generated outputs, highlighting its potential to improve both the
trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key
open-source models and datasets supporting current research, and discuss open
challenges and promising future directions for the field.

</details>


### [26] [TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/abs/2505.23518)
*Hangoo Kang,Jehyeok Yeon,Gagandeep Singh*

Main category: cs.AI

TL;DR: This paper introduces TRAP, a generative adversarial framework that manipulates autonomous AI systems' decision-making through human-imperceptible cross-modal manipulations. It uses diffusion-based semantic injections and achieves 100% attack success rate on leading models without requiring access to model internals.


<details>
  <summary>Details</summary>
Motivation: To address the new attack surfaces for adversarial manipulation in autonomous agentic AI systems powered by vision-language models (VLMs), which existing adversarial attacks fail to exploit stealthily and effectively.

Method: TRAP combines negative prompt-based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking. It produces visually natural images that induce consistent selection biases in agentic AI systems without needing access to model internals.

Result: TRAP achieves a 100% attack success rate on leading VLMs including LLaVA-34B, Gemma3, and Mistral-3.1 across multi-candidate decision scenarios on the Microsoft COCO dataset, significantly outperforming baselines such as SPSA, Bandit, and standard diffusion approaches.

Conclusion: Autonomous agents are vulnerable to being consistently misled through human-imperceptible cross-modal manipulations, highlighting the need for defense strategies beyond pixel-level robustness to address semantic vulnerabilities in cross-modal decision-making.

Abstract: Autonomous agentic AI systems powered by vision-language models (VLMs) are
rapidly advancing toward real-world deployment, yet their cross-modal reasoning
capabilities introduce new attack surfaces for adversarial manipulation that
exploit semantic reasoning across modalities. Existing adversarial attacks
typically rely on visible pixel perturbations or require privileged model or
environment access, making them impractical for stealthy, real-world
exploitation. We introduce TRAP, a generative adversarial framework that
manipulates the agent's decision-making using diffusion-based semantic
injections. Our method combines negative prompt-based degradation with positive
semantic optimization, guided by a Siamese semantic network and layout-aware
spatial masking. Without requiring access to model internals, TRAP produces
visually natural images yet induces consistent selection biases in agentic AI
systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)
dataset, building multi-candidate decision scenarios. Across these scenarios,
TRAP achieves a 100% attack success rate on leading models, including
LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such
as SPSA, Bandit, and standard diffusion approaches. These results expose a
critical vulnerability: Autonomous agents can be consistently misled through
human-imperceptible cross-modal manipulations. These findings highlight the
need for defense strategies beyond pixel-level robustness to address semantic
vulnerabilities in cross-modal decision-making.

</details>


### [27] [Individual differences in the cognitive mechanisms of planning strategy discovery](https://arxiv.org/abs/2505.23519)
*Ruiqi He,Falk Lieder*

Main category: cs.AI

TL;DR: 人们通过元认知强化学习（MCRL）获得高效的规划策略，但模型在策略发现速度上仍逊于人类。本研究探讨了结合内在元认知奖励、主观努力评估和终止思考等机制是否能缩小模型与人类表现的差距。结果表明，这些机制有助于策略发现，但仍未完全弥合差异，需进一步探索其他因素。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究表明MCRL模型能有效解释人类经验驱动的策略发现，但其策略发现速度显著慢于人类，且存在显著的个体差异。因此，需要研究如何通过引入可能促进策略发现的认知机制来提升模型性能，使其更接近人类水平。

Method: 本研究在MCRL模型中引入了三种潜在促进策略发现的认知机制：1) 内在生成的元认知伪奖励；2) 主观努力评估；3) 终止思考机制。通过分析人类在规划任务中的数据，比较这些机制对策略发现的影响及个体差异。

Result: 数据分析显示：1) 更多参与者至少使用了一种上述机制；2) 这些机制显著促进了策略发现，尤其在加快学习速度和适应个体差异方面表现出色；3) 尽管如此，加入这些机制后，模型性能仍未能完全达到人类水平，提示可能存在其他未被考虑的因素。

Conclusion: 内在元认知奖励、主观努力评估和终止思考等机制可以部分弥合MCRL模型与人类表现之间的差距，但仍不足以完全解释人类高效策略发现的能力。未来研究应继续探索其他可能的认知或社会因素对策略发现的影响。

Abstract: People employ efficient planning strategies. But how are these strategies
acquired? Previous research suggests that people can discover new planning
strategies through learning from reinforcements, a process known as
metacognitive reinforcement learning (MCRL). While prior work has shown that
MCRL models can learn new planning strategies and explain more participants'
experience-driven discovery better than alternative mechanisms, it also
revealed significant individual differences in metacognitive learning.
Furthermore, when fitted to human data, these models exhibit a slower rate of
strategy discovery than humans. In this study, we investigate whether
incorporating cognitive mechanisms that might facilitate human strategy
discovery can bring models of MCRL closer to human performance. Specifically,
we consider intrinsically generated metacognitive pseudo-rewards, subjective
effort valuation, and termination deliberation. Analysis of planning task data
shows that a larger proportion of participants used at least one of these
mechanisms, with significant individual differences in their usage and varying
impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort
valuation, and learning the value of acting without further planning were found
to facilitate strategy discovery. While these enhancements provided valuable
insights into individual differences and the effect of these mechanisms on
strategy discovery, they did not fully close the gap between model and human
performance, prompting further exploration of additional factors that people
might use to discover new planning strategies.

</details>


### [28] [Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)](https://arxiv.org/abs/2505.23536)
*Janik-Vasily Benzin,Gyunam Park,Stefanie Rinderle-Ma*

Main category: cs.AI

TL;DR: 本论文提出了一种同步模型和事件抽象的形式化基础，证明了通过模型抽象（MA）和从抽象事件日志中发现的流程模型是等价的，并基于行为特征抽象提出了一种新的事件抽象（EA）技术。


<details>
  <summary>Details</summary>
Motivation: 目前在流程智能项目中，虽然可以通过模型抽象和事件抽象来降低复杂性，但缺乏一种技术可以在发现模型后实现底层事件日志的同步抽象，导致失去与实际行为的联系，限制了分析的深度。

Method: 作者提供了同步模型和事件抽象的形式化基础，证明了使用模型抽象（MA）对流程模型进行抽象和从抽象事件日志中发现的流程模型是等价的。并基于非顺序保护的行为特征抽象作为模型抽象技术，提出了一个新的事件抽象（EA）技术。

Result: 证明了同步模型和事件抽象的可行性，确保了抽象后的模型仍然能够反映原始事件日志中的真实世界行为，从而提升了分析的深度和准确性。

Conclusion: 本研究为同步模型和事件抽象提供了理论支持，解决了现有方法中因缺乏同步抽象而导致的分析限制问题，为未来的流程智能项目提供了更强大的分析工具。

Abstract: Model abstraction (MA) and event abstraction (EA) are means to reduce
complexity of (discovered) models and event data. Imagine a process
intelligence project that aims to analyze a model discovered from event data
which is further abstracted, possibly multiple times, to reach optimality
goals, e.g., reducing model size. So far, after discovering the model, there is
no technique that enables the synchronized abstraction of the underlying event
log. This results in loosing the grounding in the real-world behavior contained
in the log and, in turn, restricts analysis insights. Hence, in this work, we
provide the formal basis for synchronized model and event abstraction, i.e., we
prove that abstracting a process model by MA and discovering a process model
from an abstracted event log yields an equivalent process model. We prove the
feasibility of our approach based on behavioral profile abstraction as
non-order preserving MA technique, resulting in a novel EA technique.

</details>


### [29] [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559)
*Kunlun Zhu,Jiaxun Zhang,Ziheng Qi,Nuoxing Shang,Zijia Liu,Peixuan Han,Yue Su,Haofei Yu,Jiaxuan You*

Main category: cs.AI

TL;DR: SafeScientist is an AI scientist framework designed to enhance safety and ethical responsibility in AI-driven scientific exploration, with integrated defensive mechanisms. Experiments show a 35% improvement in safety performance without compromising scientific output quality.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in large language model (LLM) agents have accelerated scientific discovery automation but raised ethical and safety concerns.

Method: Introduce SafeScientist, an innovative AI scientist framework that proactively refuses ethically inappropriate or high-risk tasks and emphasizes safety throughout the research process. It integrates multiple defensive mechanisms such as prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Also propose SciSafetyBench, a benchmark for evaluating AI safety in scientific contexts.

Result: Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35% compared to traditional AI scientist frameworks, without compromising scientific output quality. The safety pipeline is robust against diverse adversarial attack methods.

Conclusion: SafeScientist enhances safety and ethical responsibility in AI-driven scientific exploration without compromising scientific output quality.

Abstract: Recent advancements in large language model (LLM) agents have significantly
accelerated scientific discovery automation, yet concurrently raised critical
ethical and safety concerns. To systematically address these challenges, we
introduce \textbf{SafeScientist}, an innovative AI scientist framework
explicitly designed to enhance safety and ethical responsibility in AI-driven
scientific exploration. SafeScientist proactively refuses ethically
inappropriate or high-risk tasks and rigorously emphasizes safety throughout
the research process. To achieve comprehensive safety oversight, we integrate
multiple defensive mechanisms, including prompt monitoring, agent-collaboration
monitoring, tool-use monitoring, and an ethical reviewer component.
Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel
benchmark specifically designed to evaluate AI safety in scientific contexts,
comprising 240 high-risk scientific tasks across 6 domains, alongside 30
specially designed scientific tools and 120 tool-related risk tasks. Extensive
experiments demonstrate that SafeScientist significantly improves safety
performance by 35\% compared to traditional AI scientist frameworks, without
compromising scientific output quality. Additionally, we rigorously validate
the robustness of our safety pipeline against diverse adversarial attack
methods, further confirming the effectiveness of our integrated approach. The
code and data will be available at https://github.com/ulab-uiuc/SafeScientist.
\textcolor{red}{Warning: this paper contains example data that may be offensive
or harmful.}

</details>


### [30] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575)
*Benjamin Arnav,Pablo Bernabeu-Pérez,Nathan Helm-Burger,Tim Kostolansky,Hannes Whittingham,Mary Phuong*

Main category: cs.AI

TL;DR: In this paper, researchers explore the use of Chain-of-Thought (CoT) monitoring to oversee AI models that may pursue harmful actions. They find CoT monitoring improves detection in subtle sabotage cases but can be deceived in obvious ones. A hybrid protocol combining reasoning and final output scores outperforms both CoT and action-only monitoring.


<details>
  <summary>Details</summary>
Motivation: With increasingly autonomous AI models being deployed, there is a critical need to ensure they do not take harmful actions without notice. The motivation behind this research is to investigate effective ways to monitor these AI models.

Method: The researchers compare CoT monitoring, where a weaker trusted model oversees the intermediate reasoning steps of a more powerful untrusted model, with action-only monitoring, which only reviews final outputs. This is done in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. They also introduce a hybrid protocol that independently scores both reasoning and final outputs and combines them using a weighted average.

Result: CoT monitoring improves detection by up to 27 percentage points in scenarios where action-only monitoring fails. However, CoT traces can deceive the monitor in more obvious sabotage cases. The hybrid protocol consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates over four times higher for subtle deception scenarios.

Conclusion: The study concludes that while CoT monitoring can be beneficial in detecting subtle sabotage, it has limitations in obvious cases due to potential misleading rationalizations. The introduced hybrid protocol provides a more robust solution for monitoring AI models.

Abstract: As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [31] [MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning](https://arxiv.org/abs/2505.23596)
*Linqiang Guo,Wei Liu,Yi Wen Heng,Tse-Hsun,Chen,Yang Wang*

Main category: cs.AI

TL;DR: MAPLE是一个状态感知的多代理框架，通过将应用程序交互建模为有限状态机（FSM），提高了移动GUI代理在任务执行中的成功率、恢复能力和动作准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的移动GUI代理在处理任务时仅依赖当前屏幕信息，缺乏对应用导航流程的结构化模型，这限制了它们理解上下文、检测意外结果和从错误中恢复的能力。

Method: MAPLE框架将每个UI屏幕建模为离散状态，用户操作作为转换，并引入专门负责任务执行四个阶段（规划、执行、验证、错误恢复和知识保留）的代理。这些代理通过实时提取UI屏幕的感知数据动态构建FSM，从而跟踪导航进度和流程，验证动作结果，并通过回滚到先前稳定状态来恢复错误。

Result: 在两个具有挑战性的跨应用基准测试（Mobile-Eval-E和SPA-Bench）上的评估结果显示，与最先进的基线相比，MAPLE将任务成功率提高了12%，恢复成功率提高了13.8%，动作准确性提高了6.5%。

Conclusion: 研究强调了结构化状态建模在指导移动GUI代理任务执行中的重要性，并提出FSM表示可以作为轻量级、模型无关的记忆层集成到未来的GUI代理架构中，以支持结构化规划、执行验证和错误恢复。

Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [32] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: This paper proposes Formula Tuning (Fortune), a reinforcement learning framework that trains large language models to generate spreadsheet formulas for table reasoning tasks, improving their performance especially in multi-step numerical and symbolic reasoning.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas can represent executable symbolic operations but are underutilized.

Method: Formula Tuning uses reinforcement learning to train LMs to generate executable spreadsheet formulas for question answering over tabular data. It reduces reliance on supervised formula annotations by using binary answer correctness as a reward signal.

Result: Formula Tuning significantly improves the performance of LMs on table reasoning tasks, allowing a 7B model to outperform O1 in table understanding, particularly in multi-step numerical and symbolic reasoning tasks.

Conclusion: The study demonstrates the potential of formula-driven reinforcement learning to advance symbolic table reasoning capabilities in large language models.

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform O1 on table understanding. This highlights
the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [33] [ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork](https://arxiv.org/abs/2505.23686)
*Caroline Wang,Arrasy Rahman,Jiaxun Cui,Yoonchang Sung,Peter Stone*

Main category: cs.AI

TL;DR: The paper proposes ROTATE, a regret-driven algorithm that alternates between improving an Ad Hoc Teamwork (AHT) agent and generating challenging teammates to enhance its generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing Ad Hoc Teamwork (AHT) approaches face limitations in creating diverse teammate pools and ensuring the AHT agent can effectively learn from them. Current methods treat training teammates as static, which hinders the agent's ability to generalize to unseen partners.

Method: The authors introduce ROTATE, a unified framework reformulating AHT as an open-ended learning process. It alternates between enhancing the AHT agent and generating adversarial teammates that expose its weaknesses, driven by regret minimization.

Result: ROTATE significantly surpasses baseline methods in generalizing to unseen evaluation teammates across various AHT environments, setting a new benchmark for robust and adaptable teamwork.

Conclusion: ROTATE demonstrates superior performance in AHT scenarios, establishing a novel standard for creating agents capable of collaborating with previously unseen partners.

Abstract: Developing AI agents capable of collaborating with previously unseen partners
is a fundamental generalization challenge in multi-agent learning, known as Ad
Hoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage
pipeline, where first, a fixed population of teammates is generated with the
idea that they should be representative of the teammates that will be seen at
deployment time, and second, an AHT agent is trained to collaborate well with
agents in the population. To date, the research community has focused on
designing separate algorithms for each stage. This separation has led to
algorithms that generate teammate pools with limited coverage of possible
behaviors, and that ignore whether the generated teammates are easy to learn
from for the AHT agent. Furthermore, algorithms for training AHT agents
typically treat the set of training teammates as static, thus attempting to
generalize to previously unseen partner agents without assuming any control
over the distribution of training teammates. In this paper, we present a
unified framework for AHT by reformulating the problem as an open-ended
learning process between an ad hoc agent and an adversarial teammate generator.
We introduce ROTATE, a regret-driven, open-ended training algorithm that
alternates between improving the AHT agent and generating teammates that probe
its deficiencies. Extensive experiments across diverse AHT environments
demonstrate that ROTATE significantly outperforms baselines at generalizing to
an unseen set of evaluation teammates, thus establishing a new standard for
robust and generalizable teamwork.

</details>


### [34] [Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics](https://arxiv.org/abs/2505.23695)
*Ran Zhang,Mohannad Elhamod*

Main category: cs.AI

TL;DR: This paper presents a modular LLM-based agentic system that automates the data-to-dashboard pipeline with enhanced insight generation and domain relevance, evaluated across three datasets with improvements over GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To create an automated system for generating insightful data visualizations that can adapt to various domains without relying on closed ontologies or question templates, simulating the reasoning process of business analysts.

Method: Development of an agentic system using modular LLM agents capable of domain detection, concept extraction, multi-perspective analysis, and iterative self-reflection. The system retrieves domain-relevant knowledge and adapts to diverse datasets.

Result: The system outperforms GPT-4o with a single-prompt baseline in terms of insightfulness, domain relevance, and analytical depth, as measured by tailored evaluation metrics and qualitative human assessment.

Conclusion: This work introduces a novel modular pipeline for automating the data-to-dashboard process, offering new opportunities for human-in-the-loop validation by domain experts in business analytics.

Abstract: The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard

</details>


### [35] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
*Ruida Wang,Yuxin Li,Yi R.,Fung,Tong Zhang*

Main category: cs.AI

TL;DR: The paper introduces NL-FL HybridReasoning, an end-to-end framework integrating formal language (FL) into natural language (NL) math reasoning via alignment and mixed input techniques, achieving higher accuracy on benchmarks than NL baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance the mathematical reasoning capabilities of LLMs by effectively integrating FL knowledge into NL problem-solving, addressing challenges in disparity between NL and FL structures and formats.

Method: Propose NL-FL HybridReasoning framework with components including NL-FL Problem Alignment for bridging input format gap, Mixed Problem Input for concurrent handling of QA and existence problems, and Answer Extraction for mitigating output format gap.

Result: Achieved 89.80% and 84.34% accuracy rates on MATH-500 and AMC benchmarks, surpassing NL baseline by 4.60% and 4.82%, respectively. Some problems were uniquely solved by this framework not achievable by NL baselines even with more trials.

Conclusion: NL-FL HybridReasoning successfully integrates FL into NL math reasoning, providing a more effective approach to enhance LLMs' mathematical reasoning capabilities.

Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [36] [Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons](https://arxiv.org/abs/2505.23746)
*Hugo Henry,Kelly Cohen*

Main category: cs.AI

TL;DR: This study explores the use of Genetic Fuzzy Systems (GFS) to model airfoil self-noise, comparing different fuzzy regression strategies including brute force TSK, GFT architecture, and a novel FCM-based approach, showing that clustering assisted fuzzy inference is effective for complex aeroacoustic regression problems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling self-noise generated by airfoils which is important in aerospace, automotive and drone industries.

Method: Evaluate various fuzzy regression methods using the Airfoil Self Noise dataset, including Takagi Sugeno Kang (TSK) fuzzy system, Geneti Fuzzy Tree (GFT) architecture, and a novel clustered approach based on Fuzzy C-means (FCM).

Result: Clustering assisted fuzzy inference proves to be an effective tool for regression in complex aeroacoustic phenomena.

Conclusion: Genetic Fuzzy Systems with clustering techniques can successfully model airfoil self-noise.

Abstract: This study investigates the application of Genetic Fuzzy Systems (GFS) to
model the self-noise generated by airfoils, a key issue in aeroaccoustics with
significant implications for aerospace, automotive and drone applications.
Using the publicly available Airfoil Self Noise dataset, various Fuzzy
regression strategies are explored and compared. The paper evaluates a brute
force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading
Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on
Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the
viability of clustering assisted fuzzy inference as an effective regression
tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,
Cascading systems, Clustering and AI.

</details>


### [37] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng Dai*

Main category: cs.AI

TL;DR: The paper introduces ZeroGUI, a scalable online learning framework that automates GUI Agent training without human cost. It integrates VLM-based task generation and reward estimation with two-stage online reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing approaches which rely heavily on manual annotations and have limited adaptability to dynamic environments.

Method: ZeroGUI integrates VLM-based automatic task generation for diverse training goals, VLM-based automatic reward estimation for assessing task success, and two-stage online reinforcement learning for continuous interaction and learning from GUI environments.

Result: Experiments on UI-TARS and Aguvis demonstrate significant performance boosts in OSWorld and AndroidLab environments.

Conclusion: ZeroGUI is a promising solution for automating GUI Agent training at zero human cost.

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs](https://arxiv.org/abs/2505.22686)
*Ange-Clement Akazan,Verlon Roel Mbingui,Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.LG

TL;DR: This paper compares different deep learning models, including LSTM, GRU, BiLSTM, BiGRU, KAN and TKAN for weather forecasting in tropical Africa. It also introduces two customized TKAN models with GeLU and MiSH activation functions respectively.


<details>
  <summary>Details</summary>
Motivation: Weather forecasting is crucial in tropical Africa due to the severe impacts of extreme weather events on livelihoods. However, existing methods often fail to accurately predict the complex, non-linear weather patterns in this region.

Method: The study benchmarks several deep recurrent neural networks (LSTM, GRU, BiLSTM, BiGRU) and Kolmogorov-Arnold-based models (KAN and TKAN) for daily forecasting of temperature, precipitation, and pressure in Abidjan and Kigali. Two customized variants of TKAN are introduced, replacing the original SiLU activation function with GeLU and MiSH respectively.

Result: KAN model achieves high accuracy in temperature prediction. TKAN variants minimize absolute errors for precipitation forecasting in low-rainfall regimes. Customized TKAN models show improvements over the standard TKAN across both datasets. Classical RNNs remain competitive for atmospheric pressure forecasting, outperforming KAN-based models in this task.

Conclusion: Spline-based neural architectures have potential for efficient and data-efficient weather forecasting.

Abstract: Weather forecasting is crucial for managing risks and economic planning,
particularly in tropical Africa, where extreme events severely impact
livelihoods. Yet, existing forecasting methods often struggle with the region's
complex, non-linear weather patterns. This study benchmarks deep recurrent
neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and
Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily
forecasting of temperature, precipitation, and pressure in two tropical cities:
Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce
two customized variants of $ \texttt{TKAN}$ that replace its original
$\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH},
respectively. Using station-level meteorological data spanning from 2010 to
2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$
achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,
$\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize
absolute errors for precipitation forecasting in low-rainfall regimes. The
customized $\texttt{TKAN}$ models demonstrate improvements over the standard
$\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly
competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming
$\texttt{KAN}$-based models in this task. These results highlight the potential
of spline-based neural architectures for efficient and data-efficient
forecasting.

</details>


### [39] [SlimLLM: Accurate Structured Pruning for Large Language Models](https://arxiv.org/abs/2505.22689)
*Jialong Guo,Xinghao Chen,Yehui Tang,Yunhe Wang*

Main category: cs.LG

TL;DR: Large language models (LLMs) are costly to deploy due to computational demands. This paper introduces SlimLLM, an efficient structured pruning method that evaluates sub-module importance holistically and uses linear regression for performance recovery. It also proposes a layer-based importance ratio for determining pruning ratios. SlimLLM achieves state-of-the-art performance on the LLaMA benchmark.


<details>
  <summary>Details</summary>
Motivation: To reduce the enormous computational costs associated with deploying and applying large language models (LLMs).

Method: SlimLLM is a structured pruning method that evaluates channel and attention head importance based on the entire channel or head, rather than individual elements. A simple linear regression strategy is used for output matrix to recover performance quickly. Additionally, a layer-based importance ratio is proposed to determine pruning ratios for each layer.

Result: SlimLLM outperforms other methods and achieves state-of-the-art performance on the LLaMA benchmark.

Conclusion: SlimLLM provides an effective solution for compressing large language models while minimizing performance loss.

Abstract: Large language models(LLMs) have garnered significant attention and
demonstrated impressive capabilities in a wide range of applications. However,
due to their enormous computational costs, the deployment and application of
LLMs are often severely limited. To address this issue, structured pruning is
an effective solution to compress the parameters of LLMs. Determining the
importance of each sub-module in LLMs and minimizing performance loss are
critical issues that need to be carefully addressed in structured pruning. In
this paper, we propose an effective and fast structured pruning method named
SlimLLM for large language models. For channel and attention head pruning, we
evaluate the importance based on the entire channel or head, rather than merely
aggregating the importance of individual elements within a sub-module. This
approach enables a more holistic consideration of the interdependence among
elements within the sub-module. In addition, we design a simple linear
regression strategy for the output matrix to quickly recover performance. We
also propose layer-based importance ratio to determine the pruning ratio for
each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other
methods and achieves state-of-the-art performance.

</details>


### [40] [MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning](https://arxiv.org/abs/2505.22694)
*Dacao Zhang,Kun Zhang,Shimao Chu,Le Wu,Xin Li,Si Wei*

Main category: cs.LG

TL;DR: 提出了一种新的多任务参数高效微调方法MoRE，通过联合训练低秩专家和自适应选择器，在不增加推理成本的情况下显著提升了LLM在多任务场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法及其改进版在多任务场景中要么针对单任务优化，要么分别训练多个LoRA模块，限制了效率和效果。因此需要一种更适合多任务微调的方法。

Method: 提出Mixture of Low-Rank Experts (MoRE)，通过为不同任务分配不同秩的LoRA模块（称为低秩专家），并设计自适应秩选择器来选择适合每个任务的专家，从而实现多任务联合训练。

Result: 实验结果表明，与传统LoRA及其变体相比，MoRE显著提高了LLM在多任务场景中的性能，并且没有增加额外的推理成本。

Conclusion: MoRE在多任务场景中提升了LoRA的适应性和效率，验证了其优越性，并开源模型和代码以促进社区发展。

Abstract: With the rapid development of Large Language Models (LLMs),
Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
attention, which aims to achieve efficient fine-tuning of LLMs with fewer
parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)
introduces low-rank matrices to approximate the incremental tuning parameters
and achieves impressive performance over multiple scenarios. After that, plenty
of improvements have been proposed for further improvement. However, these
methods either focus on single-task scenarios or separately train multiple LoRA
modules for multi-task scenarios, limiting the efficiency and effectiveness of
LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in
this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for
multi-task PEFT. Specifically, instead of using an individual LoRA for each
task, we align different ranks of LoRA module with different tasks, which we
named low-rank experts. Moreover, we design a novel adaptive rank selector to
select the appropriate expert for each task. By jointly training low-rank
experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task
scenarios. Finally, we conduct extensive experiments over multiple multi-task
benchmarks along with different LLMs to verify model performance. Experimental
results demonstrate that compared to traditional LoRA and its variants, MoRE
significantly improves the performance of LLMs in multi-task scenarios and
incurs no additional inference cost. We also release the model and code to
facilitate the community.

</details>


### [41] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Main category: cs.LG

TL;DR: This paper presents LLM-ODDR, a framework using Large Language Models for optimizing ride-hailing order dispatching and driver repositioning. It includes three components focusing on order value refinement, fairness in dispatching, and demand-aware repositioning. JointDR-GPT, a specialized model, is also developed. Experiments show significant improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Ride-hailing platforms struggle with optimizing operations due to dynamic urban environments. Traditional methods often neglect aspects like driver income fairness, interpretability, and adaptability.

Method: The LLM-ODDR framework consists of three key components: Multi-objective-guided Order Value Refinement, Fairness-aware Order Dispatching, and Spatiotemporal Demand-Aware Driver Repositioning. A fine-tuned model named JointDR-GPT is used for ODDR tasks.

Result: Experiments on real-world Manhattan taxi data show that the LLM-ODDR framework outperforms traditional methods in effectiveness, adaptability, and interpretability.

Conclusion: This work represents the first application of LLMs as decision-making agents in ride-hailing ODDR tasks, providing foundational insights for integrating language models into intelligent transportation systems.

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


### [42] [When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?](https://arxiv.org/abs/2505.22696)
*Eleni Nisioti,Joachim Winther Pedersen,Erwan Plantec,Milton L. Montero,Sebastian Risi*

Main category: cs.LG

TL;DR: 神经进化（NE）方法在技能转移能力上优于强化学习（RL），特别是在面对任务变化和灾难性遗忘时。本文通过两个基准测试展示了NE在不同复杂度任务间的技能转移潜力，强调了其适应性和未来在现实问题中的扩展挑战。


<details>
  <summary>Details</summary>
Motivation: 生物智能能够跨任务持续高效地迁移技能，这是人工系统长期以来的目标。然而，强化学习在高维控制任务中易受任务变化影响并发生灾难性遗忘，而神经进化表现出鲁棒性、可扩展性和逃离局部最优的能力，但其迁移学习能力尚未被深入研究。

Method: 引入了两个基准测试：1) stepping gates - 神经网络模拟逻辑电路，强调模块化重复与变化；2) ecorobot - 扩展Brax物理引擎，包含墙壁和障碍物，并能切换不同的机器人形态。两者均采用课程学习策略以评估跨任务的技能迁移能力。

Result: 实证分析表明，NE方法在迁移能力上存在差异，且通常优于RL基线模型。这证明了NE在构建更适应性代理方面的潜力。

Conclusion: NE具备作为构建更适应性代理基础的潜力，但仍面临扩展到复杂现实问题的挑战。

Abstract: The ability to continuously and efficiently transfer skills across tasks is a
hallmark of biological intelligence and a long-standing goal in artificial
systems. Reinforcement learning (RL), a dominant paradigm for learning in
high-dimensional control tasks, is known to suffer from brittleness to task
variations and catastrophic forgetting. Neuroevolution (NE) has recently gained
attention for its robustness, scalability, and capacity to escape local optima.
In this paper, we investigate an understudied dimension of NE: its transfer
learning capabilities. To this end, we introduce two benchmarks: a) in stepping
gates, neural networks are tasked with emulating logic circuits, with designs
that emphasize modular repetition and variation b) ecorobot extends the Brax
physics engine with objects such as walls and obstacles and the ability to
easily switch between different robotic morphologies. Crucial in both
benchmarks is the presence of a curriculum that enables evaluating skill
transfer across tasks of increasing complexity. Our empirical analysis shows
that NE methods vary in their transfer abilities and frequently outperform RL
baselines. Our findings support the potential of NE as a foundation for
building more adaptable agents and highlight future challenges for scaling NE
to complex, real-world problems.

</details>


### [43] [Update Your Transformer to the Latest Release: Re-Basin of Task Vectors](https://arxiv.org/abs/2505.22697)
*Filippo Rinaldi,Giacomo Capitani,Lorenzo Bonicelli,Donato Crisostomi,Federico Bolelli,Elisa Ficarra,Emanuele Rodolà,Simone Calderara,Angelo Porrello*

Main category: cs.LG

TL;DR: 为了在无需重新训练的情况下，将微调知识无缝转移到新的预训练模型中，本文提出了一种基于权重排列的方法，特别针对Transformer模型的注意头和参数对进行了两层方法调整。


<details>
  <summary>Details</summary>
Motivation: 当底层预训练模型更新或重新训练时，微调模型变得过时，需要重新训练。因此，研究如何在没有数据的情况下，将微调转移到新模型版本是一个重要问题。

Method: 本文从模型重盆地原则出发，提出了基于权重排列的方法来重新调整原始基础模型的修改（任务向量）。具体来说，该方法结合光谱理论，提出了一个两层方法：首先排列注意力头，然后调整选定头对中的参数。此方法特别针对Transformer模型的残差连接和多头注意力层挑战进行了优化。

Result: 通过在视觉和文本任务上的广泛实验，证明了可以无缝转移微调知识到新的预训练骨干模型，而无需任何训练步骤或数据点。

Conclusion: 本文提出的方法成功实现了无需数据和训练步骤即可将微调知识转移到新的预训练模型中，为解决模型更新导致的微调模型过时问题提供了有效途径。

Abstract: Foundation models serve as the backbone for numerous specialized models
developed through fine-tuning. However, when the underlying pretrained model is
updated or retrained (e.g., on larger and more curated datasets), the
fine-tuned model becomes obsolete, losing its utility and requiring retraining.
This raises the question: is it possible to transfer fine-tuning to a new
release of the model? In this work, we investigate how to transfer fine-tuning
to a new checkpoint without having to re-train, in a data-free manner. To do
so, we draw principles from model re-basin and provide a recipe based on weight
permutations to re-base the modifications made to the original base model,
often called task vector. In particular, our approach tailors model re-basin
for Transformer models, taking into account the challenges of residual
connections and multi-head attention layers. Specifically, we propose a
two-level method rooted in spectral theory, initially permuting the attention
heads and subsequently adjusting parameters within select pairs of heads.
Through extensive experiments on visual and textual tasks, we achieve the
seamless transfer of fine-tuned knowledge to new pre-trained backbones without
relying on a single training step or datapoint. Code is available at
https://github.com/aimagelab/TransFusion.

</details>


### [44] [Private Rate-Constrained Optimization with Applications to Fair Learning](https://arxiv.org/abs/2505.22703)
*Mohammad Yaghini,Tudor Cebere,Michael Menart,Aurélien Bellet,Nicolas Papernot*

Main category: cs.LG

TL;DR: The paper introduces RaCO-DP, a differentially private algorithm for constrained optimization problems in trustworthy ML, which addresses inter-sample dependencies introduced by rate constraints.


<details>
  <summary>Details</summary>
Motivation: Many problems in trustworthy ML can be formulated as minimization of the model error under constraints on the prediction rates. Standard DP techniques are not suitable due to inter-sample dependencies caused by rate constraints.

Method: Developed RaCO-DP, a DP variant of SGDA algorithm, solving Lagrangian formulation of rate constraint problems. The privacy cost is reduced to privately estimating a histogram over the mini-batch at each step.

Result: Proved convergence of the algorithm using a novel analysis of SGDA leveraging the linear structure of the dual parameter. Empirical results show that RaCO-DP Pareto-dominates existing private learning approaches in fairness-utility trade-offs.

Conclusion: RaCO-DP effectively handles constrained optimization problems under differential privacy and shows superior performance in fairness-utility trade-offs compared to existing methods.

Abstract: Many problems in trustworthy ML can be formulated as minimization of the
model error under constraints on the prediction rates of the model for
suitably-chosen marginals, including most group fairness constraints
(demographic parity, equality of odds, etc.). In this work, we study such
constrained minimization problems under differential privacy (DP). Standard DP
optimization techniques like DP-SGD rely on the loss function's decomposability
into per-sample contributions. However, rate constraints introduce inter-sample
dependencies, violating the decomposability requirement. To address this, we
develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)
algorithm which solves the Lagrangian formulation of rate constraint problems.
We demonstrate that the additional privacy cost of incorporating these
constraints reduces to privately estimating a histogram over the mini-batch at
each optimization step. We prove the convergence of our algorithm through a
novel analysis of SGDA that leverages the linear structure of the dual
parameter. Finally, empirical results on learning under group fairness
constraints demonstrate that our method Pareto-dominates existing private
learning approaches in fairness-utility trade-offs.

</details>


### [45] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
*Aniruddha Nrusimha,William Brandon,Mayank Mishra,Yikang Shen,Rameswar Panda,Jonathan Ragan-Kelley,Yoon Kim*

Main category: cs.LG

TL;DR: FlashFormer是一种针对Transformer大语言模型单批次推理加速的概念验证内核，相较于现有的最先进的推理内核，在不同模型大小和量化设置下有显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型的规模和计算特性促使人们开发专门用于训练和推理的内核。然而，现有的内核主要针对大规模批次训练和推理进行优化，而小批次推理（如边缘部署和延迟敏感应用）仍然对许多应用至关重要。

Method: 提出了一种名为FlashFormer的内核，用于加速基于Transformer的大语言模型的单批次推理。

Result: 在不同的模型大小和量化设置下，与现有最先进的推理内核相比，观察到了显著的速度提升。

Conclusion: FlashFormer为小批次推理提供了一个有效的解决方案，特别是在内存带宽和内核启动开销成为重要因素的情况下。

Abstract: The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [46] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/abs/2505.22764)
*Divya Shanmugam,Helen Lu,Swami Sankaranarayanan,John Guttag*

Main category: cs.LG

TL;DR: The paper explores the use of test-time augmentation (TTA) to reduce the size of prediction sets produced by conformal classifiers, showing its effectiveness across different datasets, models and conditions.


<details>
  <summary>Details</summary>
Motivation: Conformal classifiers often produce large sets of predicted classes which are not informative enough. The motivation is to find a way to reduce these set sizes while maintaining the probabilistic guarantee that the true class is included.

Method: The method involves using test-time augmentation (TTA), which introduces inductive biases during inference, combined with any conformal score. This approach requires no model retraining and can be applied flexibly and efficiently.

Result: The approach reduces prediction set sizes by 10%-14% on average across three datasets, three models, and two established conformal scoring methods. It also performs well under different guarantee strengths and several distribution shifts.

Conclusion: Test-time augmentation is a useful addition to the conformal pipeline as it effectively reduces the size of prediction sets without needing model retraining.

Abstract: A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [47] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/abs/2505.22768)
*Mert Onur Cakiroglu,Idil Bilge Altun,Hasan Kurban,Elham Buxton,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: DRAGON is a novel encoder that uses Multivariate de Bruijn Graphs to improve time series forecasting by bridging symbolic representations and neural modeling.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting is challenging due to temporal heterogeneity, high dimensionality, and lack of inherent symbolic structure.

Method: DRAGON discretizes continuous input sequences and maps them onto a fixed graph structure using Multivariate de Bruijn Graphs, enabling dynamic context recovery via graph-based attention.

Result: Not explicitly stated in the abstract.

Conclusion: Not explicitly stated in the abstract.

Abstract: Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [48] [Calibrated Value-Aware Model Learning with Stochastic Environment Models](https://arxiv.org/abs/2505.22772)
*Claas Voelcker,Anastasiia Pedan,Arash Ahmadian,Romina Abachi,Igor Gilitschenski,Amir-massoud Farahmand*

Main category: cs.LG

TL;DR: 在强化学习中，价值感知模型学习（如MuZero损失）的重要性日益突出。本文分析了此类损失函数的优缺点，并提出了改进措施以解决其非校准问题。同时研究了损失校准、潜在模型架构和辅助损失之间的相互作用，表明即使确定性模型能准确预测值，校准随机模型仍有优势。


<details>
  <summary>Details</summary>
Motivation: 尽管MuZero损失在实践中被广泛使用，但对其理论上的强弱点研究有限，因此需要深入分析并改进这些损失函数。

Method: 分析价值感知模型学习损失函数家族（包括MuZero损失），揭示其作为非校准替代损失的问题，并提出修正方法。还探讨了损失校准与模型架构及辅助损失的关系。

Result: 发现常用的MuZero类损失函数并非总是能够恢复正确的模型和价值函数；提出了解决方案；证明了校准随机模型的优势。

Conclusion: 价值感知模型学习中的损失函数需要进一步改进以确保模型和价值函数的准确性，同时校准随机模型在某些情况下更有利。

Abstract: The idea of value-aware model learning, that models should produce accurate
value estimates, has gained prominence in model-based reinforcement learning.
The MuZero loss, which penalizes a model's value function prediction compared
to the ground-truth value function, has been utilized in several prominent
empirical works in the literature. However, theoretical investigation into its
strengths and weaknesses is limited. In this paper, we analyze the family of
value-aware model learning losses, which includes the popular MuZero loss. We
show that these losses, as normally used, are uncalibrated surrogate losses,
which means that they do not always recover the correct model and value
function. Building on this insight, we propose corrections to solve this issue.
Furthermore, we investigate the interplay between the loss calibration, latent
model architectures, and auxiliary losses that are commonly employed when
training MuZero-style agents. We show that while deterministic models can be
sufficient to predict accurate values, learning calibrated stochastic models is
still advantageous.

</details>


### [49] [Machine Learning Models Have a Supply Chain Problem](https://arxiv.org/abs/2505.22778)
*Sarah Meiklejohn,Hayden Blauzvern,Mihai Maruseac,Spencer Schrock,Laurent Simon,Ilia Shumailov*

Main category: cs.LG

TL;DR: The paper explores the risks in the open ML model ecosystem and proposes using Sigstore to bring transparency by allowing model publishers to sign their models and prove properties about their datasets.


<details>
  <summary>Details</summary>
Motivation: The current ecosystem for open ML models contains significant supply-chain risks, including malicious replacements of models or training on restricted or poisoned data.

Method: Investigating the application of Sigstore, a solution originally designed for open-source software supply chains, to provide transparency for open ML models.

Result: Sigstore can be used to enable model publishers to sign their models and prove properties about the datasets they use, bringing transparency to the open ML model ecosystem.

Conclusion: Open ML model ecosystems have significant supply-chain risks, but solutions like Sigstore can help mitigate these risks by providing transparency.

Abstract: Powerful machine learning (ML) models are now readily available online, which
creates exciting possibilities for users who lack the deep technical expertise
or substantial computing resources needed to develop them. On the other hand,
this type of open ecosystem comes with many risks. In this paper, we argue that
the current ecosystem for open ML models contains significant supply-chain
risks, some of which have been exploited already in real attacks. These include
an attacker replacing a model with something malicious (e.g., malware), or a
model being trained using a vulnerable version of a framework or on restricted
or poisoned data. We then explore how Sigstore, a solution designed to bring
transparency to open-source software supply chains, can be used to bring
transparency to open ML models, in terms of enabling model publishers to sign
their models and prove properties about the datasets they use.

</details>


### [50] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)
*Marco Fumero,Luca Moschella,Emanuele Rodolà,Francesco Locatello*

Main category: cs.LG

TL;DR: Neural networks can be interpreted as dynamical systems on a latent manifold, with autoencoders defining a vector field that reveals attractor points useful for analysis.


<details>
  <summary>Details</summary>
Motivation: To provide a new perspective on neural networks by interpreting them as dynamical systems acting on the latent manifold, allowing deeper insights into model behavior and data properties.

Method: Show that autoencoder models define a latent vector field through iterative encoding-decoding, revealing attractor points without additional training. Use this vector field as a tool to analyze model properties.

Result: The vector field representation allows analyzing generalization and memorization, extracting prior knowledge from network parameters, and identifying out-of-distribution samples effectively.

Conclusion: Interpreting neural networks as dynamical systems on the latent manifold offers valuable insights and practical tools for understanding and improving neural models.

Abstract: Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [51] [Efficient Preimage Approximation for Neural Network Certification](https://arxiv.org/abs/2505.22798)
*Anton Björklund,Mykola Zaitsev,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: The paper enhances the PREMAP algorithm for neural network certification, improving efficiency by at least an order of magnitude and scaling to previously infeasible convolutional neural networks.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective neural network certification, especially against 'patch attacks' that can obscure parts of images in safety- and security-critical applications like traffic signs. Current preimage approximation methods struggle with scalability.

Method: The paper introduces novel algorithmic improvements to the PREMAP algorithm including tighter bounds, adaptive Monte Carlo sampling, and improved branching heuristics.

Result: Efficiency improvements of at least an order of magnitude were demonstrated on reinforcement learning control benchmarks. The method also scales to convolutional neural networks that were previously infeasible.

Conclusion: The results show the potential of preimage approximation methodology for reliability and robustness certification in neural networks.

Abstract: The growing reliance on artificial intelligence in safety- and
security-critical applications demands effective neural network certification.
A challenging real-world use case is certification against ``patch attacks'',
where adversarial patches or lighting conditions obscure parts of images, for
example traffic signs. One approach to certification, which also gives
quantitative coverage estimates, utilizes preimages of neural networks, i.e.,
the set of inputs that lead to a specified output. However, these preimage
approximation methods, including the state-of-the-art PREMAP algorithm,
struggle with scalability. This paper presents novel algorithmic improvements
to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved
branching heuristics. We demonstrate efficiency improvements of at least an
order of magnitude on reinforcement learning control benchmarks, and show that
our method scales to convolutional neural networks that were previously
infeasible. Our results demonstrate the potential of preimage approximation
methodology for reliability and robustness certification.

</details>


### [52] [CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment](https://arxiv.org/abs/2505.22803)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: 提出了一种新的神经网络校准方法CLUE，通过学习不确定性-误差对齐来提高模型的校准质量。该方法在训练过程中显式地将预测不确定性与观察到的误差对齐，具有完全可微分、领域无关和兼容标准训练流程的特点。实验表明，CLUE在视觉、回归和语言建模任务中均表现出优越的校准质量和竞争力的预测性能，且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 当前的校准技术存在可扩展性、可微性和跨域泛化能力的限制，无法充分满足实际应用需求。因此需要一种更高效、灵活且能与现有训练流程无缝结合的新方法来改进神经网络的不确定性估计。

Method: CLUE方法通过引入一个新的损失函数，在训练过程中同时优化预测性能和校准质量。该损失函数利用不确定性和损失的汇总统计信息作为代理指标，确保模型产生的不确定性估计与其经验损失相匹配。这种方法无需后处理调整或粗粒度分箱，具备完全可微性，并适用于各种任务和领域。

Result: 在多种任务（包括视觉、回归和语言建模）上的广泛实验表明，CLUE能够实现比现有方法更好的校准质量，同时保持竞争力的预测性能。此外，该方法在处理分布外样本和领域偏移时也表现出色，且不会显著增加计算负担。

Conclusion: CLUE提供了一种有效提升神经网络校准质量的方法，解决了现有技术的局限性。其灵活性、高效性和广泛的适用性使其成为实际应用中可靠不确定性估计的一个有力工具。

Abstract: Reliable uncertainty estimation is critical for deploying neural networks
(NNs) in real-world applications. While existing calibration techniques often
rely on post-hoc adjustments or coarse-grained binning methods, they remain
limited in scalability, differentiability, and generalization across domains.
In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error
Alignment), a novel approach that explicitly aligns predicted uncertainty with
observed error during training, grounded in the principle that well-calibrated
models should produce uncertainty estimates that match their empirical loss.
CLUE adopts a novel loss function that jointly optimizes predictive performance
and calibration, using summary statistics of uncertainty and loss as proxies.
The proposed method is fully differentiable, domain-agnostic, and compatible
with standard training pipelines. Through extensive experiments on vision,
regression, and language modeling tasks, including out-of-distribution and
domain-shift scenarios, we demonstrate that CLUE achieves superior calibration
quality and competitive predictive performance with respect to state-of-the-art
approaches without imposing significant computational overhead.

</details>


### [53] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/abs/2505.22813)
*Josiah Couch,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.LG

TL;DR: The study investigates dataset quality as an intrinsic property independent of model architecture, size, and class balance. By creating thousands of datasets and training classifiers with various architectures, the authors find that classifier performance strongly correlates by subset across architectures (R²=0.79). Dataset quality emerges from the quality of its constituent classes.


<details>
  <summary>Details</summary>
Motivation: Previous research indicated that dataset quality might influence classifier performance but did not clarify whether it was a joint property of the dataset and model architecture or an intrinsic property of the dataset itself.

Method: Thousands of datasets were created, each controlled for size and class balance. These datasets were used to train classifiers with a wide range of architectures including random forests, support-vector machines, and deep networks.

Result: Classifier performance correlated strongly across architectures (R²=0.79), suggesting that dataset quality is an intrinsic property independent of dataset size, class balance, and model architecture.

Conclusion: Dataset quality is identified as an independent factor affecting classifier performance, joining size, class balance, and model architecture as separate targets for optimizing machine-learning-based classification.

Abstract: In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [54] [Preference Learning with Response Time](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: This paper explores the integration of response time data into human preference learning frameworks for more effective reward model elicitation, proposing novel methodologies and demonstrating significant improvements in sample efficiency.


<details>
  <summary>Details</summary>
Motivation: While binary preference data is fundamental in fine-tuning models, the temporal information inherent in user decision-making remains largely unexploited.

Method: Propose novel methodologies to incorporate response time information alongside binary choice data, leveraging the Evidence Accumulation Drift Diffusion (EZ) model and Neyman-orthogonal loss functions.

Result: Theoretical analysis demonstrates that conventional preference learning suffers from error rates that scale exponentially with reward magnitude, whereas the response time-augmented approach reduces this to polynomial scaling. Extensive experiments validate these findings in the context of preference learning over images.

Conclusion: Response time-augmented approach significantly improves sample efficiency in reward model elicitation.

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [55] [PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow](https://arxiv.org/abs/2505.22825)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: The paper introduces PGLearn, a suite of standardized datasets and evaluation tools for ML and OPF problems, aiming to democratize and accelerate research in this field.


<details>
  <summary>Details</summary>
Motivation: Progress in ML for OPF is hindered by the lack of standardized datasets and evaluation metrics.

Method: PGLearn provides datasets representative of real-life operating conditions, supports multiple OPF formulations, and includes a toolkit for training, evaluating, and benchmarking machine learning models.

Result: Standardized datasets are made publicly available, reducing the burden of data generation and enabling fair comparison of methodologies.

Conclusion: By promoting open, standardized datasets and evaluation metrics, PGLearn aims at democratizing and accelerating research and innovation in machine learning applications for optimal power flow problems.

Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have
recently garnered significant attention, reflecting a broader trend of
leveraging ML to approximate and/or accelerate the resolution of complex
optimization problems. These developments are necessitated by the increased
volatility and scale in energy production for modern and future grids. However,
progress in ML for OPF is hindered by the lack of standardized datasets and
evaluation metrics, from generating and solving OPF instances, to training and
benchmarking machine learning models. To address this challenge, this paper
introduces PGLearn, a comprehensive suite of standardized datasets and
evaluation tools for ML and OPF. PGLearn provides datasets that are
representative of real-life operating conditions, by explicitly capturing both
global and local variability in the data generation, and by, for the first
time, including time series data for several large-scale systems. In addition,
it supports multiple OPF formulations, including AC, DC, and second-order cone
formulations. Standardized datasets are made publicly available to democratize
access to this field, reduce the burden of data generation, and enable the fair
comparison of various methodologies. PGLearn also includes a robust toolkit for
training, evaluating, and benchmarking machine learning models for OPF, with
the goal of standardizing performance evaluation across the field. By promoting
open, standardized datasets and evaluation metrics, PGLearn aims at
democratizing and accelerating research and innovation in machine learning
applications for optimal power flow problems. Datasets are available for
download at https://www.huggingface.co/PGLearn.

</details>


### [56] [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634)
*John Halloran*

Main category: cs.LG

TL;DR: MCP协议虽然广泛使用，但易受FBAs攻击。本文提出新的MCP数据集和方法（DPO与RAG-Pref）以增强模型对攻击的防御能力，显著改善了LLMs拒绝FBAs的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MCP协议面临更广泛的威胁，攻击者只需在线发布恶意内容即可欺骗MCP代理执行攻击，因此需要改进模型的防护措施以有效应对这些攻击。

Method: 构建新的MCP数据集包含FBAs和良性样本；研究DPO在LLMs拒绝训练中的效果；引入RAG-Pref策略结合DPO进一步提升LLMs拒绝FBAs的能力。

Result: DPO能改进模型防护，但效果取决于模型原有的对齐方案；RAG-Pref显著提高LLMs拒绝FBAs的能力，尤其与DPO结合时效果更佳。

Conclusion: 通过RAG-Pref和DPO的结合，可以大幅提高LLMs对MCP-based攻击的防护能力，为未来的研究提供了新方向。

Abstract: The model context protocol (MCP) has been widely adapted as an open standard
enabling the seamless integration of generative AI agents. However, recent work
has shown the MCP is susceptible to retrieval-based "falsely benign" attacks
(FBAs), allowing malicious system access and credential theft, but requiring
that users download compromised files directly to their systems. Herein, we
show that the threat model of MCP-based attacks is significantly broader than
previously thought, i.e., attackers need only post malicious content online to
deceive MCP agents into carrying out their attacks on unsuspecting victims'
systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP
dataset of FBAs and (truly) benign samples to explore the effectiveness of
direct preference optimization (DPO) for the refusal training of large language
models (LLMs). While DPO improves model guardrails against such attacks, we
show that the efficacy of refusal learning varies drastically depending on the
model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to
refuse extremely poorly. Thus, to further improve FBA refusals, we introduce
Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel
preference alignment strategy based on RAG. We show that RAG-Pref significantly
improves the ability of LLMs to refuse FBAs, particularly when combined with
DPO alignment, thus drastically improving guardrails against MCP-based attacks.

</details>


### [57] [Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies](https://arxiv.org/abs/2505.22829)
*Chenruo Liu,Kenan Tang,Yao Qin,Qi Lei*

Main category: cs.LG

TL;DR: This paper connects distribution shift and AI safety by analyzing their synergies, establishing two types of connections between them, and providing a unified perspective for their integration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between distribution shift and AI safety through a comprehensive analysis, moving beyond narrow cases or informal analogies.

Method: The method involves establishing two types of connections: methods addressing specific shift types can help achieve safety goals, or certain shifts and safety issues can be formally reduced to each other.

Result: The findings provide a unified perspective that encourages fundamental integration between distribution shift and AI safety research.

Conclusion: This paper concludes that there are significant conceptual and methodological synergies between distribution shift and AI safety, advocating for their integration in future research.

Abstract: This paper bridges distribution shift and AI safety through a comprehensive
analysis of their conceptual and methodological synergies. While prior
discussions often focus on narrow cases or informal analogies, we establish two
types connections between specific causes of distribution shift and
fine-grained AI safety issues: (1) methods addressing a specific shift type can
help achieve corresponding safety goals, or (2) certain shifts and safety
issues can be formally reduced to each other, enabling mutual adaptation of
their methods. Our findings provide a unified perspective that encourages
fundamental integration between distribution shift and AI safety research.

</details>


### [58] [How Do Diffusion Models Improve Adversarial Robustness?](https://arxiv.org/abs/2505.22839)
*Liu Yuezhang,Xue-Xin Wei*

Main category: cs.LG

TL;DR: Diffusion models significantly enhance empirical adversarial robustness, but the exact mechanisms are unclear. This paper investigates how diffusion models improve adversarial robustness and finds that purified images are heavily influenced by internal randomness, with a compression effect arising within each configuration. The improvement drops to about 24% on CIFAR-10 when evaluated under fixed randomness. The remaining robustness gain correlates with the model's ability to compress the input space.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate how and how well diffusion models improve adversarial robustness.

Method: Observe the effects of diffusion models on lp distance to clean samples and the influence of internal randomness. Evaluate robustness under fixed randomness and analyze the correlation between robustness gain and the model's ability to compress the input space.

Result: Diffusion models increase the lp distance to clean samples rather than decrease it. The improvement in adversarial robustness drops to approximately 24% on CIFAR-10 when evaluated under fixed randomness. The remaining robustness gain strongly correlates with the model's ability to compress the input space.

Conclusion: The findings provide new insights into the mechanisms underlying diffusion-based purification and suggest the compression rate as a reliable robustness indicator.

Abstract: Recent findings suggest that diffusion models significantly enhance empirical
adversarial robustness. While some intuitive explanations have been proposed,
the precise mechanisms underlying these improvements remain unclear. In this
work, we systematically investigate how and how well diffusion models improve
adversarial robustness. First, we observe that diffusion models intriguingly
increase, rather than decrease, the $\ell_p$ distance to clean
samples--challenging the intuition that purification denoises inputs closer to
the original data. Second, we find that the purified images are heavily
influenced by the internal randomness of diffusion models, where a compression
effect arises within each randomness configuration. Motivated by this
observation, we evaluate robustness under fixed randomness and find that the
improvement drops to approximately 24% on CIFAR-10--substantially lower than
prior reports approaching 70%. Importantly, we show that this remaining
robustness gain strongly correlates with the model's ability to compress the
input space, revealing the compression rate as a reliable robustness indicator
without requiring gradient-based analysis. Our findings provide novel insights
into the mechanisms underlying diffusion-based purification, and offer guidance
for developing more effective and principled adversarial purification systems.

</details>


### [59] [Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction](https://arxiv.org/abs/2505.22840)
*Dharambir Mahto,Prashant Yadav,Mahesh Banavar,Jim Keany,Alan T Joseph,Srinivas Kilambi*

Main category: cs.LG

TL;DR: This paper introduces SXI++ LNM, a machine learning model for predicting sepsis using deep neural networks. It outperforms existing methods with an AUC of 0.99, precision of 99.9%, and accuracy of 99.99%.


<details>
  <summary>Details</summary>
Motivation: Sepsis is a life-threatening condition with non-specific symptoms and complex pathophysiology, making it challenging to predict. There's a need for more robust prediction systems.

Method: The SXI++ LNM is a machine learning scoring system that leverages multiple algorithms and deep neural networks. The model was trained and tested across multiple scenarios with different dataset distributions.

Result: The SXI++ LNM achieved an AUC of 0.99 (95% CI: 0.98-1.00), a precision of 99.9% (95% CI: 99.8-100.0), and an accuracy of 99.99% (95% CI: 99.98-100.0). It outperformed state-of-the-art methods in three use cases.

Conclusion: The SXI++ LNM demonstrates high reliability and robustness in predicting sepsis, making it a promising tool for clinical applications.

Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people
globally and causing 11 million deaths annually. Despite medical advancements,
predicting sepsis remains a challenge due to non-specific symptoms and complex
pathophysiology. The SXI++ LNM is a machine learning scoring system that
refines sepsis prediction by leveraging multiple algorithms and deep neural
networks. This study aims to improve robustness in clinical applications and
evaluates the predictive performance of the SXI++ LNM for sepsis prediction.
The model, utilizing a deep neural network, was trained and tested using
multiple scenarios with different dataset distributions. The model's
performance was assessed against unseen test data, and accuracy, precision, and
area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the
state of the art in three use cases, achieving an AUC of 0.99 (95% CI:
0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)
and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.

</details>


### [60] [Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study](https://arxiv.org/abs/2505.22841)
*Franck Gabriel,François Ged,Maria Han Veiga,Emmanuel Schertzer*

Main category: cs.LG

TL;DR: Diffusion models can suffer from memorization due to overfitting the learned score. This paper interprets the empirical score as a noisy version of the true score and introduces a kernel-smoothed empirical score to reduce variance. Regularization on the score helps prevent memorization, and reverse diffusion with kernel-smoothed empirical score can be reformulated as a gradient descent drifted toward a Log-Exponential Double-Kernel Density Estimator (LED-KDE). Experiments on synthetic and MNIST datasets are conducted.


<details>
  <summary>Details</summary>
Motivation: To address the issue of memorization in diffusion models by analyzing the empirical score and proposing a method to reduce its variance.

Method: Interpret the empirical score as a noisy version of the true score, introduce a kernel-smoothed empirical score, derive asymptotic bounds on the Kullback-Leibler divergence, and reformulate reverse diffusion with kernel-smoothed empirical score as a gradient descent drifted toward a LED-KDE.

Result: The proposed method reduces variance, helps prevent memorization, and enhances generalization. The results are illustrated through experiments on synthetic and MNIST datasets.

Conclusion: Regularization on the score has the same effect as increasing the size of the training dataset, and even straightforward regularization mitigates memorization and enhances generalization.

Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling,
yet they can, in principle, be prone to memorization. In this case, their
learned score overfits the finite dataset so that the reverse-time SDE samples
are mostly training points. In this paper, we interpret the empirical score as
a noisy version of the true score and show that its covariance matrix is
asymptotically a re-weighted data PCA. In large dimension, the small time limit
makes the noise variance blow up while simultaneously reducing spatial
correlation. To reduce this variance, we introduce a kernel-smoothed empirical
score and analyze its bias-variance trade-off. We derive asymptotic bounds on
the Kullback-Leibler divergence between the true distribution and the one
generated by the modified reverse SDE. Regularization on the score has the same
effect as increasing the size of the training dataset, and thus helps prevent
memorization. A spectral decomposition of the forward diffusion suggests better
variance control under some regularity conditions of the true data
distribution. Reverse diffusion with kernel-smoothed empirical score can be
reformulated as a gradient descent drifted toward a Log-Exponential
Double-Kernel Density Estimator (LED-KDE). This perspective highlights two
regularization mechanisms taking place in denoising diffusions: an initial
Gaussian kernel first diffuses mass isotropically in the ambient space, while a
second kernel applied in score space concentrates and spreads that mass along
the data manifold. Hence, even a straightforward regularization-without any
learning-already mitigates memorization and enhances generalization.
Numerically, we illustrate our results with several experiments on synthetic
and MNIST datasets.

</details>


### [61] [RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation](https://arxiv.org/abs/2505.22846)
*Nikita Khramov,Andrei Kozyrev,Gleb Solovev,Anton Podkopaev*

Main category: cs.LG

TL;DR: Interactive Theorem Proving combined with Generative AI shows promise. This paper evaluates different approaches for Rocq generation, emphasizing the importance of premise selection and proposing a new method using a self-attentive embedder model, which improves generator performance by 28%. A multi-stage agentic system is used for writing Rocq proofs, showing high effectiveness, and an ablation study highlights the use of multi-agent debate in proof synthesis planning.


<details>
  <summary>Details</summary>
Motivation: To explore and improve the combination of Interactive Theorem Proving with Generative Artificial Intelligence by evaluating different approaches to Rocq generation and enhancing proof writing methods.

Method: The paper proposes a novel approach for thorough premise selection using a self-attentive embedder model for retrieval, implements a multi-stage agentic system tailored for formal verification to write Rocq proofs, and conducts an ablation study incorporating multi-agent debate in the planning stage of proof synthesis.

Result: The proposed approach leads to a 28% relative increase in the generator's performance. The multi-stage agentic system demonstrates high effectiveness in writing Rocq proofs, and the ablation study shows the usefulness of multi-agent debate in proof synthesis planning.

Conclusion: Thorough premise selection via a self-attentive embedder model significantly enhances Rocq proof generation, and a multi-stage agentic system effectively writes Rocq proofs. Multi-agent debate during the planning stage contributes positively to proof synthesis.

Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with
Generative Artificial Intelligence. This paper assesses multiple approaches to
Rocq generation and illuminates potential avenues for improvement. We highlight
the importance of thorough premise selection for generating Rocq proofs and
propose a novel approach, leveraging retrieval via a self-attentive embedder
model. The evaluation of the designed approach shows up to 28% relative
increase of the generator's performance. We tackle the problem of writing Rocq
proofs using a multi-stage agentic system, tailored for formal verification,
and demonstrate its high effectiveness. We conduct an ablation study and show
the use of multi-agent debate on the planning stage of proof synthesis.

</details>


### [62] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/abs/2505.22861)
*Carlota Parés-Morlans,Michelle Yi,Claire Chen,Sarah A. Wu,Rika Antonova,Tobias Gerstenberg,Jeannette Bohg*

Main category: cs.LG

TL;DR: Causal-PIK is a method that uses Bayesian optimization and a Physics-Informed Kernel to efficiently search for the best next action in tasks with complex interactions.


<details>
  <summary>Details</summary>
Motivation: Tasks involving complex interactions between objects with unknown dynamics make planning before execution difficult, necessitating agents to iteratively improve their actions after actively exploring causes and effects in the environment.

Method: Causal-PIK leverages Bayesian optimization to reason about causal interactions via a Physics-Informed Kernel.

Result: Experimental results on Virtual Tools and PHYRE physical reasoning benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring fewer actions to reach the goal. It also remains competitive on challenging tasks compared to human problem-solvers.

Conclusion: Causal-PIK is an effective method for tasks with complex interactions, outperforming state-of-the-art methods and remaining competitive with human problem-solvers.

Abstract: Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [63] [Scaling Offline RL via Efficient and Expressive Shortcut Models](https://arxiv.org/abs/2505.22866)
*Nicolas Espinosa-Dice,Yiyi Zhang,Yiding Chen,Bradley Guo,Owen Oertell,Gokul Swamy,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: A new offline RL algorithm called SORL is introduced, which uses shortcut models to scale training and inference, capturing complex data distributions and achieving strong performance in various offline RL tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion and flow models are powerful generative approaches but applying them to offline reinforcement learning remains challenging due to the iterative nature of their noise sampling processes.

Method: SORL leverages shortcut models, a novel class of generative models, to scale both training and inference. It has a one-stage training procedure and introduces sequential and parallel inference scaling by using the learned Q-function as a verifier.

Result: SORL achieves strong performance across a range of offline RL tasks and exhibits positive scaling behavior with increased test-time compute.

Conclusion: The introduction of SORL shows promising results in offline RL tasks, demonstrating the potential of shortcut models in scaling both training and inference.

Abstract: Diffusion and flow models have emerged as powerful generative approaches
capable of modeling diverse and multimodal behavior. However, applying these
models to offline reinforcement learning (RL) remains challenging due to the
iterative nature of their noise sampling processes, making policy optimization
difficult. In this paper, we introduce Scalable Offline Reinforcement Learning
(SORL), a new offline RL algorithm that leverages shortcut models - a novel
class of generative models - to scale both training and inference. SORL's
policy can capture complex data distributions and can be trained simply and
efficiently in a one-stage training procedure. At test time, SORL introduces
both sequential and parallel inference scaling by using the learned Q-function
as a verifier. We demonstrate that SORL achieves strong performance across a
range of offline RL tasks and exhibits positive scaling behavior with increased
test-time compute. We release the code at
nico-espinosadice.github.io/projects/sorl.

</details>


### [64] [Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints](https://arxiv.org/abs/2505.22881)
*Hyungki Im,Wyame Benslimane,Paul Grigas*

Main category: cs.LG

TL;DR: The paper explores an extension of contextual stochastic linear optimization (CSLO) with inequality constraints depending on uncertain parameters predicted by machine learning models, proposing the SPO-RC loss and its convex surrogate SPO-RC+, demonstrating their effectiveness in handling constraint uncertainty through experiments.


<details>
  <summary>Details</summary>
Motivation: To address the limitation in existing CSLO literature which largely ignores constraint uncertainty, especially when constraints depend on uncertain parameters predicted by machine learning models.

Method: 1. Utilize contextual uncertainty sets constructed via methods like conformal prediction to handle constraint uncertainty.
2. Introduce the SPO-RC loss for measuring decision error and its convex surrogate SPO-RC+.
3. Prove Fisher consistency between SPO-RC+ and SPO-RC.
4. Train on truncated datasets where true constraint parameters lie within the uncertainty sets and correct sample selection bias using importance reweighting techniques.

Result: Through experiments on fractional knapsack and alloy production problem instances, it is shown that SPO-RC+ effectively handles constraint uncertainty and combining truncation with importance reweighting can further improve performance.

Conclusion: The proposed SPO-RC and SPO-RC+ methods provide effective solutions for handling uncertainty in constraints in CSLO problems, improving decision-making under uncertainty.

Abstract: We study an extension of contextual stochastic linear optimization (CSLO)
that, in contrast to most of the existing literature, involves inequality
constraints that depend on uncertain parameters predicted by a machine learning
model. To handle the constraint uncertainty, we use contextual uncertainty sets
constructed via methods like conformal prediction. Given a contextual
uncertainty set method, we introduce the "Smart Predict-then-Optimize with
Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the
SPO loss that measures decision error of predicted objective parameters. We
also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with
SPO-RC. To enhance performance, we train on truncated datasets where true
constraint parameters lie within the uncertainty sets, and we correct the
induced sample selection bias using importance reweighting techniques. Through
experiments on fractional knapsack and alloy production problem instances, we
demonstrate that SPO-RC+ effectively handles uncertainty in constraints and
that combining truncation with importance reweighting can further improve
performance.

</details>


### [65] [On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning](https://arxiv.org/abs/2505.22899)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: This paper re-examines the Follow the Regularized Leader (FTRL) framework for Online Convex Optimization (OCO) over compact sets with an emphasis on achieving dynamic regret guarantees. It highlights how FTRL can recover known dynamic regret bounds through optimistic composition of future costs and careful linearization of past costs, leading to a principled way to interpolate between greedy and agile updates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of FTRL in dynamic environments, particularly its tendency to produce 'lazy' iterates, and demonstrate that it can achieve dynamic regret guarantees.

Method: The method involves using optimistic composition of future costs and careful linearization of past costs within the FTRL framework, which can lead to pruning some of them. This approach allows for refined control over regret terms, optimism without cyclic dependence, and minimal recursive regularization akin to AdaFTRL.

Result: The result is a new analysis of FTRL against dynamic comparators that yields a principled way to interpolate between greedy and agile updates, offering several benefits including refined control over regret terms and application of minimal recursive regularization.

Conclusion: It concludes that it is not the 'lazy' projection style of FTRL that hinders dynamic regret, but the decoupling of the algorithm's state from its iterates, and that pruning can synchronize these two when necessary.

Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online
Convex Optimization (OCO) over compact sets, focusing on achieving dynamic
regret guarantees. Prior work has highlighted the framework's limitations in
dynamic environments due to its tendency to produce "lazy" iterates. However,
building on insights showing FTRL's ability to produce "agile" iterates, we
show that it can indeed recover known dynamic regret bounds through optimistic
composition of future costs and careful linearization of past costs, which can
lead to pruning some of them. This new analysis of FTRL against dynamic
comparators yields a principled way to interpolate between greedy and agile
updates and offers several benefits, including refined control over regret
terms, optimism without cyclic dependence, and the application of minimal
recursive regularization akin to AdaFTRL. More broadly, we show that it is not
the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but
the decoupling of the algorithm's state (linearized history) from its iterates,
allowing the state to grow arbitrarily. Instead, pruning synchronizes these two
when necessary.

</details>


### [66] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/abs/2505.22904)
*Youngsoo Choi,Siu Wun Cheung,Youngkyu Kim,Ping-Hsuan Tsai,Alejandro N. Diaz,Ivan Zanardi,Seung Whan Chung,Dylan Matthew Copeland,Coleman Kendrick,William Anderson,Traian Iliescu,Matthias Heinkenschloss*

Main category: cs.LG

TL;DR: The paper proposes a formal definition of foundation models in computational science and introduces DD-FEM, which combines classical FEM with data-driven learning to address key challenges in this field.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of a universally accepted definition for 'foundation model' in computational science, leading to potential confusion and dilution of its scientific meaning.

Method: Propose a formal definition for foundation models based on generality, reusability, and scalability. Introduce DD-FEM, a framework that integrates classical FEM with data-driven learning.

Result: DD-FEM addresses key challenges like scalability, adaptability, and physics consistency in computational science foundation models.

Conclusion: This work provides a rigorous basis for evaluating and developing new approaches for future foundation models in computational science by bridging traditional numerical methods with modern AI paradigms.

Abstract: The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [67] [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/abs/2505.22913)
*Donghyeon Joo,Helya Hosseini,Ramyad Hadidi,Bahar Asgari*

Main category: cs.LG

TL;DR: The paper shows unstructured sparsity can enhance KV cache compression for LLMs, allowing up to 70% sparsity without affecting accuracy or needing fine-tuning. It explores pruning strategies and finds per-token magnitude-based pruning effective. A bitmap-based sparse format and custom attention kernel are used to compress KV caches, achieving up to 45% of dense inference compression and increasing tokens/sec throughput by up to 2.23x.


<details>
  <summary>Details</summary>
Motivation: To improve the decode performance of LLMs by addressing the memory overhead issue associated with large context lengths in KV caches.

Method: Systematically exploring pruning strategies, utilizing unstructured sparsity, employing per-token magnitude-based pruning for Key and Value caches, using a bitmap-based sparse format, and developing a custom attention kernel capable of compressing and computing over compressed caches.

Result: Achieved KV cache compression up to 45% of dense inference, enabled longer context length, and increased tokens/sec throughput by up to 2.23x compared to dense inference without compromising accuracy or requiring fine-tuning.

Conclusion: Unstructured sparsity significantly improves KV cache compression for LLMs; per-token magnitude-based pruning is highly effective; custom attention kernel and sparse format provide substantial benefits.

Abstract: We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.

</details>


### [68] [Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking](https://arxiv.org/abs/2505.22922)
*Athanasios Glentis,Jiaxiang Li,Qiulin Shang,Andi Han,Ioannis Tsaknakis,Quan Wei,Mingyi Hong*

Main category: cs.LG

TL;DR: 大语言模型（LLMs）虽然功能强大，但其训练和微调需要大量的内存和计算资源。本文探讨了参数或内存高效的方法是否可以在保持与完整模型训练性能相当的同时提高预训练效率，并提出了两种技术：权重重构和动量重置，以增强高效预训练方法的性能。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的快速发展，其训练和微调所需的内存和计算资源带来了巨大的挑战。尽管一些方法（如LoRA）在微调方面有效，但在预训练阶段应用这些方法更具挑战性，因为需要学习庞大的数据集。因此，需要探索参数或内存高效的方法来提高预训练效率。

Method: 1. 进行全面调查，总结高效的预训练方法。2. 对多个代表性的内存高效预训练方法进行基准评估，观察优化器和超参数的选择对全秩训练的影响，以及高秩更新对低秩方法性能的改进作用。3. 提出两种实用技术：权重重构和动量重置，以提升高效预训练方法的性能。

Result: 研究发现，适当选择优化器和超参数时，全秩训练表现最佳。同时，在低秩方法中加入高秩更新可以显著改善其性能。提出的权重重构和动量重置技术在1B模型上实现了比GaLore和Fira等流行算法更低的困惑度，同时使用约25%更少的内存。

Conclusion: 参数或内存高效的方法确实可以提高预训练效率，同时达到与完整模型训练相当的性能。通过采用适当的优化策略和技术改进，可以进一步缩小性能差距并减少资源消耗。

Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple
domains, large language models (LLMs) have grown at an unprecedented rate, with
some recent models containing trillions of parameters. This growth is
accompanied by substantial computational challenges, particularly regarding the
memory and compute resources required for training and fine-tuning. Numerous
approaches have been explored to address these issues, such as LoRA. While
these methods are effective for fine-tuning, their application to pre-training
is significantly more challenging due to the need to learn vast datasets.
Motivated by this issue, we aim to address the following questions: Can
parameter- or memory-efficient methods enhance pre-training efficiency while
achieving performance comparable to full-model training? How can the
performance gap be narrowed? To this end, the contributions of this work are
the following. (1) We begin by conducting a comprehensive survey that
summarizes state-of-the-art methods for efficient pre-training. (2) We perform
a benchmark evaluation of several representative memory efficient pre-training
approaches to comprehensively evaluate their performance across model sizes. We
observe that with a proper choice of optimizer and hyperparameters, full-rank
training delivers the best performance, as expected. We also notice that
incorporating high-rank updates in low-rank approaches is the key to improving
their performance. (3) Finally, we propose two practical techniques, namely
weight refactorization and momentum reset, to enhance the performance of
efficient pre-training methods. We observe that applying these techniques to
the low-rank method (on a 1B model) can achieve a lower perplexity than popular
memory efficient algorithms such as GaLore and Fira, while simultaneously using
about 25% less memory.

</details>


### [69] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/abs/2505.22935)
*Jipeng Li,Yanning Shen*

Main category: cs.LG

TL;DR: The paper challenges the assumption that explicit noise-level conditioning is essential for Graph Diffusion Models (GDMs) and investigates if denoisers can infer noise levels from corrupted graph structures. Unconditional GDMs perform as well or better than conditioned ones, with fewer parameters and less computation time.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to challenge the widely held belief that explicit noise-level conditioning is necessary for the effective operation of Graph Diffusion Models (GDMs).

Method: The authors develop a theoretical framework focused on Bernoulli edge-flip corruptions and extend it to more complex scenarios involving coupled structure-attribute noise. They empirically evaluate unconditional GDMs against their conditioned counterparts.

Result: Unconditional GDMs achieve comparable or superior performance to conditioned models while reducing parameters by 4-6% and computation time by 8-10%. The high-dimensional nature of graph data often encodes sufficient information for the denoising process.

Conclusion: The conclusion is that explicit noise-level conditioning may not be essential for GDMs, as the graph data itself can provide enough information for denoising, leading to simpler and more efficient model architectures.

Abstract: Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [70] [Directed Graph Grammars for Sequence-based Learning](https://arxiv.org/abs/2505.22949)
*Michael Sun,Orion Foo,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.LG

TL;DR: 本论文提出了一种基于语法的方法，将有向无环图（DAG）转换为有序的、紧凑的序列表示形式，有助于DAG的解码和多种应用。


<details>
  <summary>Details</summary>
Motivation: 尽管有许多有效的DAG编码方法，但以原则性方式解码DAG仍然具有挑战性，因为DAG的节点可以有许多不同的拓扑顺序。

Method: 作者提出了一种基于语法的方法，将DAG视为在无歧义语法上的推导，每个DAG对应唯一的一系列产生规则，从而构建了一个原则性的、紧凑且等效的序列表示。

Result: 该表示方法可以用于构建图生成的生成模型、学习用于属性预测的潜在空间以及利用序列表示的连续性进行结构化数据的贝叶斯优化等多种应用。

Conclusion: 提出的基于语法的方法能够有效地将DAG转换为紧凑的序列表示形式，为DAG的解码及相关应用提供了新的思路。

Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in
practice, with examples that include electronic circuits, Bayesian networks,
and neural architectures. While many effective encoders exist for DAGs, it
remains challenging to decode them in a principled manner, because the nodes of
a DAG can have many different topological orders. In this work, we propose a
grammar-based approach to constructing a principled, compact and equivalent
sequential representation of a DAG. Specifically, we view a graph as
derivations over an unambiguous grammar, where the DAG corresponds to a unique
sequence of production rules. Equivalently, the procedure to construct such a
description can be viewed as a lossless compression of the data. Such a
representation has many uses, including building a generative model for graph
generation, learning a latent space for property prediction, and leveraging the
sequence representational continuity for Bayesian Optimization over structured
data. Code is available at https://github.com/shiningsunnyday/induction.

</details>


### [71] [MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming](https://arxiv.org/abs/2505.22967)
*Chengqi Zheng,Jianda Chen,Yueming Lyu,Wen Zheng Terence Ng,Haopeng Zhang,Yew-Soon Ong,Ivor Tsang,Haiyan Yin*

Main category: cs.LG

TL;DR: MermaidFlow is a framework that uses safety-constrained graph evolution to improve the success rates and efficiency of autonomous agentic reasoning, representing workflows in a verifiable intermediate Mermaid format.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating workflows often result in plans that are fragile and unexecutable due to unconstrained LLM-driven construction.

Method: MermaidFlow redefines the agentic search space through safety-constrained graph evolution. Workflows are represented as a verifiable intermediate representation using Mermaid. Domain-aware evolutionary operators (crossover, mutation, insertion, deletion) preserve semantic correctness while promoting structural diversity.

Result: MermaidFlow achieves consistent improvements in success rates and faster convergence to executable plans on the agent reasoning benchmark without modifying task settings or evaluation protocols.

Conclusion: Safety-constrained graph evolution provides a scalable, modular foundation for robust and interpretable agentic reasoning systems.

Abstract: Despite the promise of autonomous agentic reasoning, existing workflow
generation methods frequently produce fragile, unexecutable plans due to
unconstrained LLM-driven construction. We introduce MermaidFlow, a framework
that redefines the agentic search space through safety-constrained graph
evolution. At its core, MermaidFlow represent workflows as a verifiable
intermediate representation using Mermaid, a structured and human-interpretable
graph language. We formulate domain-aware evolutionary operators, i.e.,
crossover, mutation, insertion, and deletion, to preserve semantic correctness
while promoting structural diversity, enabling efficient exploration of a
high-quality, statically verifiable workflow space. Without modifying task
settings or evaluation protocols, MermaidFlow achieves consistent improvements
in success rates and faster convergence to executable plans on the agent
reasoning benchmark. The experimental results demonstrate that
safety-constrained graph evolution offers a scalable, modular foundation for
robust and interpretable agentic reasoning systems.

</details>


### [72] [EquiReg: Equivariance Regularized Diffusion for Inverse Problems](https://arxiv.org/abs/2505.22973)
*Bahareh Tolooshams,Aditi Chandrashekar,Rayhan Zirvi,Abbas Mammadov,Jiachen Yao,Chuwei Wang,Anima Anandkumar*

Main category: cs.LG

TL;DR: This paper proposes Equivariance Regularized (EquiReg) diffusion to address the limitations of current diffusion-based inverse problem solvers. By reweighting diffusion trajectories and penalizing deviations from the data manifold, EquiReg improves reconstructions and outperforms state-of-the-art methods in several tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the limitation of current diffusion-based inverse problem solvers which rely on isotropic Gaussian approximations due to the intractability of the likelihood term. These approximations lead to deviations from the data manifold and cause inconsistent, unstable reconstructions.

Method: The method proposed in this paper is Equivariance Regularized (EquiReg) diffusion, a general framework for regularizing posterior sampling in diffusion-based inverse problem solvers. This involves reweighting diffusion trajectories and penalizing those that deviate from the data manifold. A new distribution-dependent equivariance error is defined, functions with low error for on-manifold samples and higher error for off-manifold samples are identified, and these functions are used to regularize the diffusion sampling process.

Result: EquiReg enhances reconstructions by reducing deviation from the data manifold, leading to more consistent and stable reconstructions. It outperforms state-of-the-art diffusion models in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations.

Conclusion: Equivariance Regularized (EquiReg) diffusion provides a novel approach to improve the performance of diffusion-based inverse problem solvers by regularizing posterior sampling. This method shows promise in various applications including image restoration and reconstruction of partial differential equations.

Abstract: Diffusion models represent the state-of-the-art for solving inverse problems
such as image restoration tasks. In the Bayesian framework, diffusion-based
inverse solvers incorporate a likelihood term to guide the prior sampling
process, generating data consistent with the posterior distribution. However,
due to the intractability of the likelihood term, many current methods rely on
isotropic Gaussian approximations, which lead to deviations from the data
manifold and result in inconsistent, unstable reconstructions. We propose
Equivariance Regularized (EquiReg) diffusion, a general framework for
regularizing posterior sampling in diffusion-based inverse problem solvers.
EquiReg enhances reconstructions by reweighting diffusion trajectories and
penalizing those that deviate from the data manifold. We define a new
distribution-dependent equivariance error, empirically identify functions that
exhibit low error for on-manifold samples and higher error for off-manifold
samples, and leverage these functions to regularize the diffusion sampling
process. When applied to a variety of solvers, EquiReg outperforms
state-of-the-art diffusion models in both linear and nonlinear image
restoration tasks, as well as in reconstructing partial differential equations.

</details>


### [73] [A Computational Approach to Improving Fairness in K-means Clustering](https://arxiv.org/abs/2505.22984)
*Guancheng Zhou,Haiping Xu,Hongkang Xu,Chenyu Li,Donghui Yan*

Main category: cs.LG

TL;DR: 为了应对K-means聚类算法中潜在的公平性问题，本文提出了一种两阶段优化方法，即先聚类再调整少量数据点的聚类归属，同时提出了两种高效的算法来识别对公平性影响较大的数据点。实验表明，该方法显著提高了公平性，且对聚类质量的影响极小。


<details>
  <summary>Details</summary>
Motivation: K-means聚类算法可能存在重大弱点，某些集群在敏感变量（如性别或种族）上可能不成比例地包含更多（或更少）来自子群体的点，从而导致公平性问题和偏差。

Method: 提出了一种两阶段优化方法：首先进行聚类，然后调整一小部分选定数据点的聚类归属。此外，还提出了两种计算效率高的算法来识别那些对公平性昂贵的数据点，一种关注簇外最近的数据点，另一种关注高度'混合'的数据点。

Result: 在基准数据集上的实验表明，该方法在公平性方面有了显著的改进，同时对聚类质量的影响最小。

Conclusion: 所提出的算法可以很容易地扩展到一大类聚类算法或公平性度量标准。

Abstract: The popular K-means clustering algorithm potentially suffers from a major
weakness for further analysis or interpretation. Some cluster may have
disproportionately more (or fewer) points from one of the subpopulations in
terms of some sensitive variable, e.g., gender or race. Such a fairness issue
may cause bias and unexpected social consequences. This work attempts to
improve the fairness of K-means clustering with a two-stage optimization
formulation--clustering first and then adjust cluster membership of a small
subset of selected data points. Two computationally efficient algorithms are
proposed in identifying those data points that are expensive for fairness, with
one focusing on nearest data points outside of a cluster and the other on
highly 'mixed' data points. Experiments on benchmark datasets show substantial
improvement on fairness with a minimal impact to clustering quality. The
proposed algorithms can be easily extended to a broad class of clustering
algorithms or fairness metrics.

</details>


### [74] [Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition](https://arxiv.org/abs/2505.22985)
*Masaharu Kagiyama,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: This paper presents PatchEchoClassifier, a new model for human activity recognition from time-series data that is energy-efficient and achieves over 80% accuracy with significantly reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: To create an energy-efficient classifier for time-series data, particularly for human activity recognition using one-dimensional sensor signals.

Method: Introduced PatchEchoClassifier which uses Echo State Network (ESN) as a reservoir-based mechanism. Incorporated a tokenizer for patch-level representations and applied knowledge distillation framework to transfer knowledge from a high-capacity teacher model (MLP-Mixer) to the lightweight student model.

Result: Achieved over 80 percent accuracy in human activity recognition while reducing computational cost significantly. The model requires only about one-sixth of the floating point operations compared to DeepConvLSTM.

Conclusion: PatchEchoClassifier is a promising solution for real-time and energy-efficient human activity recognition in edge computing environments.

Abstract: This paper aims to develop an energy-efficient classifier for time-series
data by introducing PatchEchoClassifier, a novel model that leverages a
reservoir-based mechanism known as the Echo State Network (ESN). The model is
designed for human activity recognition (HAR) using one-dimensional sensor
signals and incorporates a tokenizer to extract patch-level representations. To
train the model efficiently, we propose a knowledge distillation framework that
transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight
reservoir-based student model. Experimental evaluations on multiple HAR
datasets demonstrate that our model achieves over 80 percent accuracy while
significantly reducing computational cost. Notably, PatchEchoClassifier
requires only about one-sixth of the floating point operations (FLOPS) compared
to DeepConvLSTM, a widely used convolutional baseline. These results suggest
that PatchEchoClassifier is a promising solution for real-time and
energy-efficient human activity recognition in edge computing environments.

</details>


### [75] [Model-Preserving Adaptive Rounding](https://arxiv.org/abs/2505.22988)
*Albert Tseng,Zhaofeng Sun,Christopher De Sa*

Main category: cs.LG

TL;DR: The paper presents YAQA, a new adaptive rounding algorithm for post-training quantization (PTQ) of large language models (LLMs), which reduces KL divergence by about 30% and achieves state-of-the-art performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Current PTQ algorithms focus on minimizing immediate activation error for linear layers independently, ignoring the impact on subsequent layers. This can lead to a model that is not as close to the original as intended.

Method: YAQA uses Kronecker-factored approximations of each linear layer's Hessian with respect to the full model KL divergence. It includes Kronecker-factored sketches of the full layerwise Hessian and a quantizer-independent rounding algorithm with theoretical guarantees.

Result: Empirically, YAQA reduces the KL divergence to the original model by approximately 30% across various models and quantizers, achieving state-of-the-art performance on downstream tasks.

Conclusion: YAQA offers an effective approach to PTQ for LLMs, significantly reducing KL divergence and improving performance on downstream tasks.

Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed
model whose output distribution is as close to the original model's as
possible. To do this tractably, almost all LLM PTQ algorithms quantize linear
layers by independently minimizing the immediate activation error. However,
this localized objective ignores the effect of subsequent layers, so reducing
it does not necessarily give a closer model. In this work, we introduce Yet
Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses
Kronecker-factored approximations of each linear layer's Hessian with respect
to the \textit{full model} KL divergence. YAQA consists of two components:
Kronecker-factored sketches of the full layerwise Hessian that can be tractably
computed for hundred-billion parameter LLMs, and a quantizer-independent
rounding algorithm that uses these sketches and comes with theoretical
guarantees. Across a wide range of models and quantizers, YAQA empirically
reduces the KL divergence to the original model by $\approx 30\%$ while
achieving state of the art performance on downstream tasks.

</details>


### [76] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/abs/2505.22991)
*Behzad Kamgar-Parsi,Behrooz Kamgar-Parsi*

Main category: cs.LG

TL;DR: This paper derives rigorous bounds for the critical hyperparameter λ in regularized k-means algorithm assuming ideal clusters and analyzes both additive and multiplicative regularization methods.


<details>
  <summary>Details</summary>
Motivation: Finding the number of meaningful clusters in an unlabeled dataset is important in many applications, but there are no principled guidelines for setting a value for the critical hyperparameter λ in regularized k-means algorithm.

Method: The paper derives rigorous bounds for λ assuming clusters are ideal (d-dimensional spheres with identical radii), which are close proxies for k-means clusters. It also analyzes k-means algorithm with both additive and multiplicative regularizer.

Result: Experiments show that the consensus among k-means solutions with additive and multiplicative regularizations can reduce the ambiguity of multiple solutions in certain cases. Selected experiments also demonstrate the performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.

Conclusion: The derived bounds for λ provide a principled guideline for setting the hyperparameter in regularized k-means algorithm, and analyzing both additive and multiplicative regularization methods can help reduce the ambiguity of multiple solutions.

Abstract: Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [77] [Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation](https://arxiv.org/abs/2505.22994)
*Ari S. Benjamin,Kyle Daruwalla,Christian Pehle,Anthony M. Zador*

Main category: cs.LG

TL;DR: 通过受神经调节启发的方法，学习平滑参数化任务上下文变量的权重，优化权重空间中的光滑流形以实现跨任务知识重用和高效学习。


<details>
  <summary>Details</summary>
Motivation: 在人工神经网络中，通常通过将上下文作为输入注入来完成跨任务的知识重用来高效学习一系列相似任务。而大脑则通过各种神经调节剂（如血清素）调制参数本身。受此启发，提出了一种新的方法，学习平滑参数化的任务上下文变量权重。

Method: 不是优化一个权重向量，而是优化权重空间中的一个光滑流形，其拓扑结构预先定义。推导出一种形式化处理流形优化的方法，即将损失函数的最小化受体积移动约束，类似于梯度下降。推理时，条件选择该流形上的一个点作为特定子任务的有效权重矩阵。

Result: 这种方法在几种拓扑结构上进行了训练，包括权重空间中的直线（用于输入数据噪声水平的条件）和椭圆（用于旋转图像）。尽管这些参数化简单，但它们的表现优于通过输入连接对相同网络进行条件化，并且对分布外样本有更好的泛化能力。

Conclusion: 在权重空间低维流形上调制权重提供了一个有原则且有效的传统条件化替代方案。

Abstract: One frequently wishes to learn a range of similar tasks as efficiently as
possible, re-using knowledge across tasks. In artificial neural networks, this
is typically accomplished by conditioning a network upon task context by
injecting context as input. Brains have a different strategy: the parameters
themselves are modulated as a function of various neuromodulators such as
serotonin. Here, we take inspiration from neuromodulation and propose to learn
weights which are smoothly parameterized functions of task context variables.
Rather than optimize a weight vector, i.e. a single point in weight space, we
optimize a smooth manifold in weight space with a predefined topology. To
accomplish this, we derive a formal treatment of optimization of manifolds as
the minimization of a loss functional subject to a constraint on volumetric
movement, analogous to gradient descent. During inference, conditioning selects
a single point on this manifold which serves as the effective weight matrix for
a particular sub-task. This strategy for conditioning has two main advantages.
First, the topology of the manifold (whether a line, circle, or torus) is a
convenient lever for inductive biases about the relationship between tasks.
Second, learning in one state smoothly affects the entire manifold, encouraging
generalization across states. To verify this, we train manifolds with several
topologies, including straight lines in weight space (for conditioning on e.g.
noise level in input data) and ellipses (for rotated images). Despite their
simplicity, these parameterizations outperform conditioning identical networks
by input concatenation and better generalize to out-of-distribution samples.
These results suggest that modulating weights over low-dimensional manifolds
offers a principled and effective alternative to traditional conditioning.

</details>


### [78] [LLM Agents for Bargaining with Utility-based Feedback](https://arxiv.org/abs/2505.22998)
*Jihwan Oh,Murad Aghazada,Se-Young Yun,Taehyeon Kim*

Main category: cs.LG

TL;DR: Bargaining is challenging for LLMs due to their limitations. This paper introduces BargainArena, a benchmark dataset with complex scenarios, human-aligned evaluation metrics and a structured feedback mechanism to improve LLMs' bargaining capabilities.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' capabilities in realistic bargaining by addressing the challenges posed by strategic depth and complex human factors that existing benchmarks fail to capture.

Method: Introduced BargainArena, a novel benchmark dataset with intricate scenarios; developed human-aligned, economically-grounded evaluation metrics inspired by utility theory; proposed a structured feedback mechanism enabling iterative refinement of bargaining strategies.

Result: Experimental results indicate that LLMs often exhibit negotiation strategies misaligned with human preferences, but the structured feedback mechanism significantly improves their performance, enhancing strategic and opponent-aware reasoning.

Conclusion: The comprehensive framework centered on utility-based feedback enhances LLMs' abilities in realistic bargaining.

Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges
for large language models (LLMs) due to limitations in strategic depth and
adaptation to complex human factors. Existing benchmarks often fail to capture
this real-world complexity. To address this and enhance LLM capabilities in
realistic bargaining, we introduce a comprehensive framework centered on
utility-based feedback. Our contributions are threefold: (1) BargainArena, a
novel benchmark dataset with six intricate scenarios (e.g., deceptive
practices, monopolies) to facilitate diverse strategy modeling; (2)
human-aligned, economically-grounded evaluation metrics inspired by utility
theory, incorporating agent utility and negotiation power, which implicitly
reflect and promote opponent-aware reasoning (OAR); and (3) a structured
feedback mechanism enabling LLMs to iteratively refine their bargaining
strategies. This mechanism can positively collaborate with in-context learning
(ICL) prompts, including those explicitly designed to foster OAR. Experimental
results show that LLMs often exhibit negotiation strategies misaligned with
human preferences, and that our structured feedback mechanism significantly
improves their performance, yielding deeper strategic and opponent-aware
reasoning.

</details>


### [79] [Hybrid Cross-domain Robust Reinforcement Learning](https://arxiv.org/abs/2505.23003)
*Linh Le Pham Van,Minh Hoang Nguyen,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: The paper presents HYDRO, a Hybrid Cross-Domain Robust RL framework that combines offline datasets with an online simulator to improve sample efficiency in offline robust RL.


<details>
  <summary>Details</summary>
Motivation: Existing offline robust RL algorithms require extensive offline data which can be costly and time-consuming to gather. Using simulators is faster and cheaper but suffers from dynamics mismatch.

Method: HYDRO uses an online simulator to complement limited offline datasets in the context of robust RL. It measures and minimizes performance gaps between the simulator and worst-case models via uncertainty filtering and prioritized sampling.

Result: Experiments show HYDRO outperforms existing methods across various tasks, highlighting its potential to enhance sample efficiency in offline robust RL.

Conclusion: HYDRO is a promising approach to improve sample efficiency in offline robust RL by integrating offline data with an online simulator.

Abstract: Robust reinforcement learning (RL) aims to learn policies that remain
effective despite uncertainties in its environment, which frequently arise in
real-world applications due to variations in environment dynamics. The robust
RL methods learn a robust policy by maximizing value under the worst-case
models within a predefined uncertainty set. Offline robust RL algorithms are
particularly promising in scenarios where only a fixed dataset is available and
new data cannot be collected. However, these approaches often require extensive
offline data, and gathering such datasets for specific tasks in specific
environments can be both costly and time-consuming. Using an imperfect
simulator offers a faster, cheaper, and safer way to collect data for training,
but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,
the first Hybrid Cross-Domain Robust RL framework designed to address these
challenges. HYDRO utilizes an online simulator to complement the limited amount
of offline datasets in the non-trivial context of robust RL. By measuring and
minimizing performance gaps between the simulator and the worst-case models in
the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized
sampling to select the most relevant and reliable simulator samples. Our
extensive experiments demonstrate HYDRO's superior performance over existing
methods across various tasks, underscoring its potential to improve sample
efficiency in offline robust RL.

</details>


### [80] [QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining](https://arxiv.org/abs/2505.23004)
*Kyle R. Chickering,Bangzheng Li,Muhao Chen*

Main category: cs.LG

TL;DR: The paper introduces QLIP, an improved replacement for the CLIP vision encoder that enhances coarse and fine-grained visual understanding in Multimodal Large Language Models (MLLMs) like LLaVA without necessitating retraining.


<details>
  <summary>Details</summary>
Motivation: CLIP vision encoder has limitations such as being constrained to fixed input resolutions and failing to produce separated embeddings for dissimilar images. Replacing it typically incurs substantial computational costs due to the need for retraining the entire model pipeline.

Method: QLIP is proposed as a drop-in replacement for CLIP. It uses an image quadtree for content-aware patchification, addressing mesoscopic bias and interpolation bias present in CLIP. This allows seamless integration with existing MLLMs with minimal code changes.

Result: Experiments show that QLIP improves general visual question answering accuracy across different sizes of the LLaVA v1.5 model series without retraining or fine-tuning. It also boosts detailed understanding performance on the challenging $V^{\ast}$ benchmark by up to 13.6%.

Conclusion: QLIP can enhance visual understanding in MLLMs without the need for retraining, providing a more efficient alternative to the CLIP vision encoder.

Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens,
aligning visual and textual signals within a shared latent space to facilitate
crossmodal representation learning. The CLIP model is a widely adopted
foundational vision language model whose vision encoder has played a critical
role in the development of MLLMs such as LLaVA. However, the CLIP vision
encoder suffers from notable limitations including being constrained to only
handling fixed input resolutions and a failure to produce separated embeddings
for dissimilar images. Replacing the vision encoder of an existing model
typically incurs substantial computational costs because such a change often
necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the
CLIP vision encoder: mesoscopic bias and interpolation bias. To address these
issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly
integrated with existing MLLMs with only a few lines of code and can enhance
both coarse-grained and fine-grained visual understanding, without re-training.
QLIP is designed around an image quadtree which replaces the standard uniform
grid patches with a novel content aware patchification. Our experimental
results demonstrate that QLIP improves the general visual question answering
accuracy of the LLaVA v1.5 model series across various model sizes--without
requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts
detailed understanding performance on the challenging $V^{\ast}$ benchmark by
up to 13.6 percent.

</details>


### [81] [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009)
*Ruskin Raj Manku,Yuzhi Tang,Xingjian Shi,Mu Li,Alex Smola*

Main category: cs.LG

TL;DR: 提出EmergentTTS-Eval，一个涵盖六种复杂TTS场景的基准测试框架，通过自动化生成测试用例和模型评估方法，揭示不同TTS系统的细微性能差异，并开源相关代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的TTS基准测试常无法充分衡量模型对复杂语义文本的处理能力。

Method: 基于EmergentTTS构建EmergentTTS-Eval，包含六个挑战性场景；利用LLMs生成1,645个多样化测试用例；采用大型音频语言模型（LALM）作为评估工具。

Result: 该框架能有效揭示TTS系统间的细微性能差异，模型评估法与人类偏好高度相关。

Conclusion: EmergentTTS-Eval为TTS系统的全面评估提供了有力工具，并促进了相关研究的开放与合作。

Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle
nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we
introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six
challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic
complexity, complex pronunciation (e.g. URLs, formulas), and questions.
Crucially, our framework automates both test-case generation and evaluation,
making the benchmark easily extensible. Starting from a small set of
human-written seed prompts, we iteratively extend them using LLMs to target
specific structural, phonetic and prosodic challenges, resulting in 1,645
diverse test cases. Moreover, we employ a model-as-a-judge approach, using a
Large Audio Language Model (LALM) to assess the speech across multiple
dimensions such as expressed emotion, prosodic, intonational, and pronunciation
accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,
such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,
demonstrating its ability to reveal fine-grained performance differences.
Results show that the model-as-a-judge approach offers robust TTS assessment
and a high correlation with human preferences. We open source the evaluation
$\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the
$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

</details>


### [82] [Scalable Complexity Control Facilitates Reasoning Ability of LLMs](https://arxiv.org/abs/2505.23013)
*Liangkai Hang,Junjie Yao,Zhiwei Bai,Tianyi Chen,Yang Chen,Rongjie Diao,Hezhou Li,Pengxiao Lin,Zhiwei Wang,Cheng Xu,Zhongwang Zhang,Zhangchen Zhou,Zhiyu Li,Zehao Lin,Kai Chen,Feiyu Xiong,Yaoyu Zhang,Weinan E,Hongkang Yang,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）的推理能力近年来迅速提升，本研究通过调整初始化速率和权重衰减系数来控制模型复杂度，从而改进不同模型规模和数据规模下的扩展规律。研究表明，在1T token上预训练的2.4B模型中，使用恒定初始化速率（标准差的指数）而非固定初始化标准差，可以使扩展规律在模型规模和数据规模上更快下降。这表明复杂度控制是LLMs持续进步的一个有希望的方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）的推理能力正在迅速发展，但需要一种更基本的方法来可靠地增强其泛化能力。

Method: 通过调整初始化速率和权重衰减系数进行模型复杂度控制，并比较不同复杂度超参数下预训练模型的基准性能，探索恒定初始化速率对扩展规律的影响。

Result: 发现恒定初始化速率使扩展规律在模型规模和数据规模上更快下降，相较于固定初始化标准差有更好的效果。

Conclusion: 复杂度控制是一个有望持续提升LLMs性能的研究方向。

Abstract: The reasoning ability of large language models (LLMs) has been rapidly
advancing in recent years, attracting interest in more fundamental approaches
that can reliably enhance their generalizability. This work demonstrates that
model complexity control, conveniently implementable by adjusting the
initialization rate and weight decay coefficient, improves the scaling law of
LLMs consistently over varying model sizes and data sizes. This gain is further
illustrated by comparing the benchmark performance of 2.4B models pretrained on
1T tokens with different complexity hyperparameters. Instead of fixing the
initialization std, we found that a constant initialization rate (the exponent
of std) enables the scaling law to descend faster in both model and data sizes.
These results indicate that complexity control is a promising direction for the
continual advancement of LLMs.

</details>


### [83] [Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2505.23014)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Xiaodong Li,Taoyu Su,Tingwen Liu,Li Guo*

Main category: cs.LG

TL;DR: The paper proposes a novel approach to enhance spectral graph neural networks using hyperbolic partial differential equations (PDEs) for message passing, improving interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with ensuring topological features due to their reliance on spatial domain learning. The authors aim to address this by formulating message passing in terms of hyperbolic PDEs, which allows explicit mapping of node representations into a solution space that captures the topological structure of graphs.

Method: Message passing is formulated as a system of hyperbolic PDEs, creating a dynamical system that maps node representations into a solution space defined by eigenvectors describing the graph's topology. Node features are decomposed into superpositions of these eigenvectors. Polynomials are introduced to approximate arbitrary filter functions, connecting the proposed method with spectral GNNs.

Result: Extensive experiments show that the use of hyperbolic PDEs in message passing not only offers strong flexibility but also significantly boosts the performance of various spectral GNNs across different graph tasks.

Conclusion: The formulation of message passing using hyperbolic PDEs enhances both the interpretability and effectiveness of spectral GNNs. This method provides a powerful enhancement paradigm for spectral GNNs.

Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the
topological features of graph data. Traditional GNNs learns node features in a
spatial domain unrelated to the topology, which can hardly ensure topological
features. In this paper, we formulates message passing as a system of
hyperbolic partial differential equations (hyperbolic PDEs), constituting a
dynamical system that explicitly maps node representations into a particular
solution space. This solution space is spanned by a set of eigenvectors
describing the topological structure of graphs. Within this system, for any
moment in time, a node features can be decomposed into a superposition of the
basis of eigenvectors. This not only enhances the interpretability of message
passing but also enables the explicit extraction of fundamental characteristics
about the topological structure. Furthermore, by solving this system of
hyperbolic partial differential equations, we establish a connection with
spectral graph neural networks (spectral GNNs), serving as a message passing
enhancement paradigm for spectral GNNs.We further introduce polynomials to
approximate arbitrary filter functions. Extensive experiments demonstrate that
the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also
significantly enhances the performance of various spectral GNNs across diverse
graph tasks.

</details>


### [84] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.23017)
*Xingjian Wu,Xiangfei Qiu,Hongfan Gao,Jilin Hu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: The paper proposes $K^2$VAE, a VAE-based generative model using KoopmanNet and KalmanNet for efficient long-term probabilistic time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing methods for probabilistic time series forecasting are proficient in short-term predictions but struggle with long-term forecasts due to nonlinear dynamics and error accumulation.

Method: The method introduces $K^2$VAE which uses KoopmanNet to convert nonlinear time series into a linear dynamical system and KalmanNet to refine predictions and model uncertainty within this linear system.

Result: Extensive experiments show that $K^2$VAE outperforms current state-of-the-art methods in both short- and long-term probabilistic time series forecasting.

Conclusion: $K^2$VAE provides an efficient and accurate solution for long-term probabilistic time series forecasting.

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [85] [SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference](https://arxiv.org/abs/2505.23022)
*Yinghao Tang,Tingfeng Lan,Xiuqi Huang,Hui Lu,Wei Chen*

Main category: cs.LG

TL;DR: SCORPIO is an SLO-oriented LLM serving system that improves system goodput and SLO adherence by employing TTFT Guard and TPOT Guard.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems focus on maximum throughput but neglect important SLOs like TTFT and TPOT, resulting in suboptimal SLO attainment.

Method: SCORPIO exploits SLO heterogeneity for adaptive scheduling across admission control, queue management, and batch selection. It features a TTFT Guard which employs least-deadline-first reordering and rejects unattainable requests, and a TPOT Guard which utilizes a VBS-based admission control and a novel credit-based batching mechanism.

Result: Evaluations show SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5% compared to state-of-the-art baselines.

Conclusion: SCORPIO successfully enhances the performance of LLM serving systems by focusing on SLOs and improving both system goodput and SLO adherence.

Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum
throughput. They often neglect Service Level Objectives (SLOs) such as Time to
First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal
SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving
system designed to maximize system goodput and SLO attainment for workloads
with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for
adaptive scheduling across admission control, queue management, and batch
selection. SCORPIO features a TTFT Guard, which employs least-deadline-first
reordering and rejects unattainable requests, and a TPOT Guard, which utilizes
a VBS-based admission control and a novel credit-based batching mechanism. Both
guards are supported by a predictive module. Evaluations demonstrate that
SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%
compared to state-of-the-art baselines.

</details>


### [86] [An Empirical Study of Federated Prompt Learning for Vision Language Model](https://arxiv.org/abs/2505.23024)
*Zhihao Wang,Wenke Huang,Tian Chen,Zekun Shi,Guancheng Wan,Yu Qiao,Bin Yang,Jian Wang,Bing Li,Mang Ye*

Main category: cs.LG

TL;DR: The paper explores the differences and robustness of language prompt learning (LPT) and vision prompt learning (VPT) in federated learning scenarios with data heterogeneity, providing insights for optimizing prompt learning in privacy-preserving environments.


<details>
  <summary>Details</summary>
Motivation: To understand how prompt learning with Vision Language Models (VLMs) can be adapted to downstream tasks in federated learning settings, especially under challenges like label skew and domain shift.

Method: Systematically investigate the behavioral differences between LPT and VPT under data heterogeneity, evaluate various federated learning and prompt configurations, and explore strategies for enhancing prompt learning when both label skew and domain shift are present.

Result: Extensive experiments reveal insights into optimizing prompt learning in federated settings, showing the impact of client scale, aggregation strategies, and prompt length on the robustness of Federated Prompt Learning (FPL).

Conclusion: The findings provide practical guidance for improving prompt learning in federated learning environments, facilitating broader deployment of VLMs while preserving privacy.

Abstract: The Vision Language Model (VLM) excels in aligning vision and language
representations, and prompt learning has emerged as a key technique for
adapting such models to downstream tasks. However, the application of prompt
learning with VLM in federated learning (\fl{}) scenarios remains
underexplored. This paper systematically investigates the behavioral
differences between language prompt learning (LPT) and vision prompt learning
(VPT) under data heterogeneity challenges, including label skew and domain
shift. We conduct extensive experiments to evaluate the impact of various \fl{}
and prompt configurations, such as client scale, aggregation strategies, and
prompt length, to assess the robustness of Federated Prompt Learning (FPL).
Furthermore, we explore strategies for enhancing prompt learning in complex
scenarios where label skew and domain shift coexist, including leveraging both
prompt types when computational resources allow. Our findings offer practical
insights into optimizing prompt learning in federated settings, contributing to
the broader deployment of VLMs in privacy-preserving environments.

</details>


### [87] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/abs/2505.23027)
*Minh Nguyen Nhat To,Paul F RWilson,Viet Nguyen,Mohamed Harmanani,Michael Cooper,Fahimeh Fooladgar,Purang Abolmaesumi,Parvin Mousavi,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: The paper proposes Diverse Prototypical Ensembles (DPEs) to handle subpopulation shift in machine learning models by using an ensemble of diverse classifiers, which often outperforms prior state-of-the-art methods in worst-group accuracy across nine real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Subpopulation shift can significantly degrade the performance of machine learning models. Current solutions rely on assumptions about subpopulations and annotations that are not always available.

Method: Propose using an ensemble of diverse classifiers by replacing the standard linear classification layer with a mixture of prototypical classifiers, each focusing on different features and samples.

Result: Empirical evaluation on nine real-world datasets shows that DPEs often outperform prior state-of-the-art methods in worst-group accuracy.

Conclusion: DPEs provide a promising approach to handling subpopulation shift without relying on specific assumptions or annotations.

Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [88] [Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks](https://arxiv.org/abs/2505.23032)
*Dongwoo Lee,Dong Bok Lee,Steven Adriaensen,Juho Lee,Sung Ju Hwang,Frank Hutter,Seon Joo Kim,Hae Beom Lee*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian framework using Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation, which outperforms existing methods especially in data-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting neural scaling behavior rely on point estimation without quantifying uncertainty, making them unsuitable for real-world decision-making.

Method: The authors design a prior distribution to sample synthetic functions resembling real-world neural scaling laws and use this within a Bayesian framework based on PFNs to meta-learn the extrapolation.

Result: The method shows superior performance compared to existing point estimation and Bayesian approaches, particularly excelling in data-limited situations like Bayesian active learning.

Conclusion: This Bayesian approach provides reliable, uncertainty-aware extrapolation of neural scaling laws, offering practical advantages for real-world applications.

Abstract: Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [89] [From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data](https://arxiv.org/abs/2505.23042)
*Siwen Wang,Shitou Zhang,Wan-Lin Chen,Dung Truong,Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: The paper explores the potential of Large EEG Models (LEMs) in real-world stress classification by fine-tuning LaBraM, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To assess the applicability of Large EEG Models (LEMs) in real-world environments beyond controlled clinical settings.

Method: Fine-tuned LaBraM, a state-of-the-art foundation EEG model, on a real-world stress classification dataset collected from graduate students in a classroom setting. A binary classifier was trained to distinguish between normal and elevated stress states using resting-state EEG data.

Result: The best-performing fine-tuned model achieved a balanced accuracy of 90.47% with a 5-second window, outperforming traditional classifiers in both accuracy and inference efficiency. The model also showed robustness under random data shuffling and reduced channel counts.

Conclusion: LEMs can effectively process real-world EEG data, highlighting their potential to revolutionize brain-computer interface applications through a data-centric design approach.

Abstract: Recent advancements in Large Language Models have inspired the development of
foundation models across various domains. In this study, we evaluate the
efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art
foundation EEG model, on a real-world stress classification dataset collected
in a graduate classroom. Unlike previous studies that primarily evaluate LEMs
using data from controlled clinical settings, our work assesses their
applicability to real-world environments. We train a binary classifier that
distinguishes between normal and elevated stress states using resting-state EEG
data recorded from 18 graduate students during a class session. The
best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a
5-second window, significantly outperforming traditional stress classifiers in
both accuracy and inference efficiency. We further evaluate the robustness of
the fine-tuned LEM under random data shuffling and reduced channel counts.
These results demonstrate the capability of LEMs to effectively process
real-world EEG data and highlight their potential to revolutionize
brain-computer interface applications by shifting the focus from model-centric
to data-centric design.

</details>


### [90] [ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation](https://arxiv.org/abs/2505.23048)
*Tianci Bu,Le Zhou,Wenchuan Yang,Jianhong Mou,Kang Yang,Suoyi Tan,Feng Yao,Jingyuan Wang,Xin Lu*

Main category: cs.LG

TL;DR: 提出ProDiff框架，仅使用两个端点作为最小信息进行轨迹补全，结合原型学习和去噪扩散概率模型，提升轨迹补全精度。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据在多种应用中至关重要，但常因设备限制和不同采集场景而存在不完整性。现有的补全方法依赖稀疏轨迹或旅行信息，假设这些稀轨迹保留了基本的行为模式，这给数据获取带来了很大压力，并忽略了大规模人类轨迹嵌入的潜力。

Method: 提出ProDiff，一个轨迹补全框架，仅使用两个端点作为最小信息。该框架整合了原型学习以嵌入人类运动模式，并采用去噪扩散概率模型进行鲁棒时空重建。通过定制的损失函数进行联合训练以确保有效的补全。

Result: ProDiff在FourSquare上准确率提高了6.28%，在WuXi上提高了2.52%。进一步分析显示生成轨迹与真实轨迹之间的相关性为0.927，证明了该方法的有效性。

Conclusion: ProDiff框架在轨迹补全任务中表现出色，超越现有最先进方法，证明了其在利用少量信息进行有效轨迹补全的能力。

Abstract: Trajectory data is crucial for various applications but often suffers from
incompleteness due to device limitations and diverse collection scenarios.
Existing imputation methods rely on sparse trajectory or travel information,
such as velocity, to infer missing points. However, these approaches assume
that sparse trajectories retain essential behavioral patterns, which place
significant demands on data acquisition and overlook the potential of
large-scale human trajectory embeddings. To address this, we propose ProDiff, a
trajectory imputation framework that uses only two endpoints as minimal
information. It integrates prototype learning to embed human movement patterns
and a denoising diffusion probabilistic model for robust spatiotemporal
reconstruction. Joint training with a tailored loss function ensures effective
imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by
6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927
correlation between generated and real trajectories, demonstrating the
effectiveness of our approach.

</details>


### [91] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
*Tianteng Gu,Bei Liu,Bo Xiao,Ke Zeng,Jiacheng Liu,Yanmin Qian*

Main category: cs.LG

TL;DR: The paper introduces DenoiseRotator, a method that redistributes parameter importance in large language models (LLMs) to enhance pruning robustness. It applies learnable orthogonal transformations to weight matrices and complements existing pruning techniques. Evaluated on LLaMA3, Qwen2.5, and Mistral models, it consistently improves perplexity and zero-shot accuracy.


<details>
  <summary>Details</summary>
Motivation: Pruning large language models often leads to significant performance degradation, especially under semi-structured sparsity constraints. Current methods focus on estimating the importance of individual weights rather than making the model inherently more amenable to pruning.

Method: DenoiseRotator applies learnable orthogonal transformations to the model's weight matrices. By minimizing the information entropy of normalized importance scores, it concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness.

Result: DenoiseRotator consistently improves perplexity and zero-shot accuracy on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity. For example, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, it reduces the perplexity gap to the dense model by 58%.

Conclusion: DenoiseRotator is a model-agnostic approach that can be seamlessly integrated with existing pruning techniques to improve the performance of pruned large language models.

Abstract: Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [92] [CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents](https://arxiv.org/abs/2505.23055)
*Zhen Xiang,Aliyah R. Hsu,Austin V. Zane,Aaron E. Kornblith,Margaret J. Lin-Martore,Jasmanpreet C. Kaur,Vasuda M. Dokiparthi,Bo Li,Bin Yu*

Main category: cs.LG

TL;DR: An LLM-based system named CDR-Agent is developed to assist clinical decision-making in emergency departments by identifying and applying appropriate Clinical Decision Rules (CDRs). It improves accuracy in CDR selection, reduces computational overhead, and outperforms traditional LLM prompting approaches.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making, especially in emergency departments, is complex and fast-paced. The use of Clinical Decision Rules (CDRs) can improve decision-making consistency and accuracy, but their application is often limited by the clinician's cognitive load.

Method: The researchers developed CDR-Agent, a novel LLM-based system that autonomously identifies and applies the most appropriate CDRs based on unstructured clinical notes. They curated two datasets, synthetic and CDR-Bench, to validate CDR-Agent's performance.

Result: CDR-Agent achieves a 56.3% (synthetic) and 8.7% (CDR-Bench) accuracy gain relative to the standalone LLM baseline in CDR selection. It significantly reduces computational overhead and makes cautious yet effective imaging decisions.

Conclusion: CDR-Agent not only selects relevant CDRs efficiently, but also minimizes unnecessary interventions while successfully identifying most positively diagnosed cases, thus outperforming traditional LLM prompting approaches.

Abstract: Clinical decision-making is inherently complex and fast-paced, particularly
in emergency departments (EDs) where critical, rapid and high-stakes decisions
are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools
that combine signs, symptoms, and clinical variables into decision trees to
make consistent and accurate diagnoses. CDR usage is often hindered by the
clinician's cognitive load, limiting their ability to quickly recall and apply
the appropriate rules. We introduce CDR-Agent, a novel LLM-based system
designed to enhance ED decision-making by autonomously identifying and applying
the most appropriate CDRs based on unstructured clinical notes. To validate
CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although
CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\%
(synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM
baseline in CDR selection. Moreover, CDR-Agent significantly reduces
computational overhead. Using these datasets, we demonstrated that CDR-Agent
not only selects relevant CDRs efficiently, but makes cautious yet effective
imaging decisions by minimizing unnecessary interventions while successfully
identifying most positively diagnosed cases, outperforming traditional LLM
prompting approaches. Code for our work can be found at:
https://github.com/zhenxianglance/medagent-cdr-agent

</details>


### [93] [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061)
*Tarun Suresh,Debangshu Banerjee,Shubham Ugare,Sasa Misailovic,Gagandeep Singh*

Main category: cs.LG

TL;DR: Diffusion LLMs show promise for runtime efficiency but struggle with structured outputs. The paper proposes DINGO, a dynamic programming-based decoding strategy that ensures distribution preservation and satisfies formal constraints, achieving significant improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To make diffusion LLMs reliable for tasks requiring structured outputs by enforcing user-specified formal constraints.

Method: Propose DINGO, an efficient and provably distribution-preserving constrained decoding strategy based on dynamic programming.

Result: DINGO achieves up to a 68 percentage point improvement over unconstrained inference on symbolic math and JSON generation benchmarks.

Conclusion: DINGO addresses the limitation of diffusion LLMs in handling structured outputs by ensuring constraint satisfaction and maintaining output distribution integrity.

Abstract: Diffusion LLMs have emerged as a promising alternative to conventional
autoregressive LLMs, offering significant potential for improved runtime
efficiency. However, existing diffusion models lack the ability to provably
enforce user-specified formal constraints, such as regular expressions, which
makes them unreliable for tasks that require structured outputs, such as
fixed-schema JSON generation. Unlike autoregressive models that generate tokens
sequentially, diffusion LLMs predict a block of tokens in parallel. This
parallelism makes traditional constrained decoding algorithms, which are
designed for sequential token prediction, ineffective at preserving the true
output distribution. To address this limitation, we propose DINGO, a dynamic
programming-based constrained decoding strategy that is both efficient and
provably distribution-preserving. DINGO enables sampling of output strings with
the highest probability under the model's predicted distribution, while
strictly satisfying any user-specified regular expression. On standard symbolic
math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point
improvement over unconstrained inference

</details>


### [94] [Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data](https://arxiv.org/abs/2505.23062)
*Lingkai Kong,Haichuan Wang,Tonghan Wang,Guojun Xiong,Milind Tambe*

Main category: cs.LG

TL;DR: CompFlow是一种基于流动匹配和最优传输理论的方法，通过建模目标动态作为源域流动输出分布上的条件流，提供改进的目标动态学习泛化能力和对动态差距的合理估计。此外，结合乐观主动数据收集策略，CompFlow在多个RL基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的处理源和目标环境动态差异的方法通常通过惩罚或过滤掉高动态差距区域的源转换来解决此问题，但其动态差距估计可能因源和目标动态具有分离支持而未明确定义。因此需要一种新方法以克服这些限制。

Method: 提出了一种名为CompFlow的方法，该方法基于流动匹配和最优传输的理论联系。具体而言，将目标动态建模为基于源域流动输出分布的条件流，而不是直接从高斯先验中学习。这种方法提供了两个关键优势：1）改进目标动态学习的泛化能力；2）通过对源和目标转换之间的Wasserstein距离进行合理估计动态差距。进一步引入了一种乐观主动数据收集策略，优先探索高动态差距区域，并从理论上证明它减少了与最优策略的性能差异。

Result: CompFlow在几个具有动态偏移的强化学习基准上超越了强大的基线方法。

Conclusion: CompFlow通过改进目标动态学习的泛化能力和合理估计动态差距，结合乐观主动数据收集策略，在实践中表现优异并理论上减少与最优策略的性能差异。

Abstract: Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.

</details>


### [95] [Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification](https://arxiv.org/abs/2505.23063)
*Denis Mamba Kabala,Adel Hafiane,Laurent Bobelin,Raphael Canals*

Main category: cs.LG

TL;DR: The paper proposes a Decentralized Federated Learning (DFL) framework for crop disease detection using validation loss to guide model sharing and correct local training.


<details>
  <summary>Details</summary>
Motivation: Crop disease detection is crucial in agriculture, but data privacy concerns limit the deployment of deep learning models. Federated Learning addresses privacy issues, yet centralized FL has vulnerabilities.

Method: A novel DFL framework is introduced which uses validation loss to guide model sharing between peers and corrects local training via an adaptive loss function controlled by a weighting parameter.

Result: Experiments with PlantVillage datasets and three deep learning architectures show that the DFL approach improves accuracy, convergence speed, generalization, and robustness across heterogeneous data environments.

Conclusion: The DFL framework is well-suited for privacy-preserving agricultural applications, enhancing both performance and data security.

Abstract: Crop disease detection and classification is a critical challenge in
agriculture, with major implications for productivity, food security, and
environmental sustainability. While deep learning models such as CNN and ViT
have shown excellent performance in classifying plant diseases from images,
their large-scale deployment is often limited by data privacy concerns.
Federated Learning (FL) addresses this issue, but centralized FL remains
vulnerable to single-point failures and scalability limits. In this paper, we
introduce a novel Decentralized Federated Learning (DFL) framework that uses
validation loss (Loss_val) both to guide model sharing between peers and to
correct local training via an adaptive loss function controlled by weighting
parameter. We conduct extensive experiments using PlantVillage datasets with
three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the
impact of weighting parameter, the number of shared models, the number of
clients, and the use of Loss_val versus Loss_train of other clients. Results
demonstrate that our DFL approach not only improves accuracy and convergence
speed, but also ensures better generalization and robustness across
heterogeneous data environments making it particularly well-suited for
privacy-preserving agricultural applications.

</details>


### [96] [Multi-Modal Learning with Bayesian-Oriented Gradient Calibration](https://arxiv.org/abs/2505.23071)
*Peizheng Guo,Jingyao Wang,Huijie Guo,Jiangmeng Li,Chuxiong Sun,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: The paper proposes BOGC-MML, a Bayesian-Oriented Gradient Calibration method for Multi-Modal Learning (MML), which models gradient uncertainty to improve model optimization and predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing MML methods aggregate gradients with fixed weights and treat all dimensions equally, ignoring the intrinsic gradient uncertainty of each modality. This can degrade performance in sensitive dimensions and hinder learning in less sensitive ones.

Method: BOGC-MML models each modality's gradient as a random variable, derives its probability distribution, converts the precision of each gradient distribution into scalar evidence, quantifies per-dimension uncertainties, and fuses them via a reduced Dempster-Shafer rule.

Result: Extensive experiments on multiple benchmark datasets demonstrate the effectiveness and advantages of BOGC-MML.

Conclusion: BOGC-MML addresses the issue of gradient uncertainty in MML by providing a calibrated update direction that balances sensitivity and conservatism across dimensions.

Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to
improve predictive accuracy. However, existing methods mainly aggregate
gradients with fixed weights and treat all dimensions equally, overlooking the
intrinsic gradient uncertainty of each modality. This may lead to (i) excessive
updates in sensitive dimensions, degrading performance, and (ii) insufficient
updates in less sensitive dimensions, hindering learning. To address this
issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for
MML to explicitly model the gradient uncertainty and guide the model
optimization towards the optimal direction. Specifically, we first model each
modality's gradient as a random variable and derive its probability
distribution, capturing the full uncertainty in the gradient space. Then, we
propose an effective method that converts the precision (inverse variance) of
each gradient distribution into a scalar evidence. This evidence quantifies the
confidence of each modality in every gradient dimension. Using these evidences,
we explicitly quantify per-dimension uncertainties and fuse them via a reduced
Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a
calibrated update direction that balances sensitivity and conservatism across
dimensions. Extensive experiments on multiple benchmark datasets demonstrate
the effectiveness and advantages of the proposed method.

</details>


### [97] [Gradient Boosting Decision Tree with LSTM for Investment Prediction](https://arxiv.org/abs/2505.23084)
*Chang Yu,Fang Liu,Jie Zhu,Shaobo Guo,Yifan Gao,Zhongheng Yang,Meiwei Liu,Qianwen Xing*

Main category: cs.LG

TL;DR: This paper proposes a hybrid framework that combines LSTM networks with LightGBM and CatBoost for stock price prediction, improving accuracy by 10-15% compared to individual models.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve the accuracy of stock price prediction by leveraging the strengths of both sequential (LSTM) and tree-based (LightGBM, CatBoost) machine learning models.

Method: The method involves processing time-series financial data using seven different models (ANNs, CNNs, BiLSTM, vanilla LSTM, XGBoost, LightGBM, standard NNs) and then developing an ensemble model that integrates the strengths of sequential and tree-based approaches.

Result: Experimental results indicate that the proposed hybrid framework enhances accuracy by 10 to 15 percent in comparison to individual models and effectively reduces error during market changes.

Conclusion: This study concludes that ensemble methods have significant potential in financial forecasting and offers a flexible design for incorporating new machine learning techniques.

Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term
Memory) networks with LightGBM and CatBoost for stock price prediction. The
framework processes time-series financial data and evaluates performance using
seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks
(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and
standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and
RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the
strengths of sequential and tree-based approaches. Experimental results show
that the proposed framework improves accuracy by 10 to 15 percent compared to
individual models and reduces error during market changes. This study
highlights the potential of ensemble methods for financial forecasting and
provides a flexible design for integrating new machine learning techniques.

</details>


### [98] [Equivariant Spherical Transformer for Efficient Molecular Modeling](https://arxiv.org/abs/2505.23086)
*Junyi An,Xinyu Lu,Chao Qu,Yunfei Shi,Peijia Lin,Qianwei Tang,Licheng Xu,Fenglei Cao,Yuan Qi*

Main category: cs.LG

TL;DR: SE(3)-equivariant Graph Neural Networks (GNNs) have advanced molecular system modeling, but their expressiveness is restricted. This paper introduces Equivariant Spherical Transformer (EST), which uses a Transformer structure within the spatial domain of group representations after Fourier transform to improve expressiveness.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of SE(3)-equivariant GNNs in terms of insufficient non-linearity and incomplete group representations that restrict expressiveness.

Method: The Equivariant Spherical Transformer (EST) leverages a Transformer structure within the spatial domain of group representations after Fourier transform. It encompasses the function space of tensor products while achieving superior expressiveness with equivariant inductive bias guaranteed through uniform sampling strategy for the Fourier transform.

Result: Experiments demonstrate state-of-the-art performance by EST on various molecular benchmarks, including OC20 and QM9.

Conclusion: EST theoretically and empirically surpasses existing methods in expressiveness and achieves state-of-the-art performance on molecular benchmarks.

Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced
molecular system modeling by employing group representations. However, their
message passing processes, which rely on tensor product-based convolutions, are
limited by insufficient non-linearity and incomplete group representations,
thereby restricting expressiveness. To overcome these limitations, we introduce
the Equivariant Spherical Transformer (EST), a novel framework that leverages a
Transformer structure within the spatial domain of group representations after
Fourier transform. We theoretically and empirically demonstrate that EST can
encompass the function space of tensor products while achieving superior
expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed
through a uniform sampling strategy for the Fourier transform. Our experiments
demonstrate state-of-the-art performance by EST on various molecular
benchmarks, including OC20 and QM9.

</details>


### [99] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
*Chongjie Si,Zhiyi Shi,Yadao Wang,Xiaokang Yang,Susanto Rahardja,Wei Shen*

Main category: cs.LG

TL;DR: A new framework named MAP is proposed to reformulate weight matrices as high-dimensional vectors and decouple their adaptation into direction and magnitude, which can be integrated into existing PEFT methods and significantly improve performance.


<details>
  <summary>Details</summary>
Motivation: The fine-tuning of large language models is computationally expensive and existing PEFT methods lack a principled geometric foundation when decomposing weight adaptation into direction and magnitude components.

Method: MAP normalizes the pre-trained weights, learns a directional update, and introduces two scalar coefficients to independently scale the magnitude of the base and update vectors.

Result: Extensive experiments show that MAP significantly improves performance when coupling with existing methods.

Conclusion: MAP offers a simple yet powerful enhancement to existing PEFT methods and could serve as a default setting for designing future PEFT methods.

Abstract: The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [100] [Learning to Search for Vehicle Routing with Multiple Time Windows](https://arxiv.org/abs/2505.23098)
*Kuan Xu,Zhiguang Cao,Chenlong Zheng,Linong Liu*

Main category: cs.LG

TL;DR: An RL-AVNS method was developed for solving VRPMTW, showing significant improvements over traditional methods and strong generalization ability.


<details>
  <summary>Details</summary>
Motivation: To effectively solve the Vehicle Routing Problem with Multiple Time Windows (VRPMTW) by integrating reinforcement learning to dynamically select neighborhood operators based on real-time solution states and learned experience.

Method: The proposed RL-AVNS method uses a reinforcement learning framework to guide the selection of neighborhood operators. It introduces a fitness metric quantifying customers' temporal flexibility for the shaking phase and employs a transformer-based neural policy network for intelligent operator selection during local search.

Result: RL-AVNS significantly outperforms traditional VNS, adaptive VNS, and state-of-the-art learning-based heuristics in terms of solution quality and computational efficiency across various instance scales and time window complexities. The algorithm also demonstrates strong generalization ability to unseen problem instances.

Conclusion: The RL-AVNS method is a powerful approach for solving VRPMTW, offering substantial improvements in both solution quality and computational efficiency, as well as practical utility for complex logistics scenarios.

Abstract: In this study, we propose a reinforcement learning-based adaptive variable
neighborhood search (RL-AVNS) method designed for effectively solving the
Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional
adaptive approaches that rely solely on historical operator performance, our
method integrates a reinforcement learning framework to dynamically select
neighborhood operators based on real-time solution states and learned
experience. We introduce a fitness metric that quantifies customers' temporal
flexibility to improve the shaking phase, and employ a transformer-based neural
policy network to intelligently guide operator selection during the local
search. Extensive computational experiments are conducted on realistic
scenarios derived from the replenishment of unmanned vending machines,
characterized by multiple clustered replenishment windows. Results demonstrate
that RL-AVNS significantly outperforms traditional variable neighborhood search
(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,
achieving substantial improvements in solution quality and computational
efficiency across various instance scales and time window complexities.
Particularly notable is the algorithm's capability to generalize effectively to
problem instances not encountered during training, underscoring its practical
utility for complex logistics scenarios.

</details>


### [101] [Weight Spectra Induced Efficient Model Adaptation](https://arxiv.org/abs/2505.23099)
*Chongjie Si,Xuankun Yang,Muqing Liu,Yadao Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 大规模基础模型在众多下游任务中表现出惊人的通用性。然而，完全微调这些模型会产生巨大的计算成本，因此推动了参数高效微调（PEFT）方法的发展，如LoRA。尽管PEFT方法取得了经验上的成功，但其修改模型参数的底层机制尚未得到充分研究。本文通过奇异值分解（SVD），系统地研究了全量微调过程中权重矩阵的结构变化，发现微调主要放大了顶级奇异值，而其余部分基本保持不变，这表明任务特定知识被注入到低维子空间中。此外，主导奇异向量被重新定向到任务特定方向，而非主导子空间保持稳定。基于这些见解，我们提出了一种新方法，通过学习对顶级奇异方向进行重新缩放，从而精确调节最具影响力的组件，同时不破坏整体结构。我们的方法在多个任务上对强大的基线模型实现了持续改进，突显了结构化微调的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然PEFT方法在实践中表现良好，但对其如何修改模型参数的机制仍缺乏深入了解。为了填补这一空白，并探索更有效的微调策略，作者决定深入研究全量微调过程中权重矩阵的变化模式。

Method: 作者使用奇异值分解（SVD）技术，分析了在全量微调过程中权重矩阵的结构变化。他们观察到微调主要影响顶级奇异值和向量，而其他部分则相对稳定。基于此发现，作者提出了一种新的微调方法，该方法通过学习调整顶级奇异方向来实现对模型最具影响力部分的精确控制，同时保持整体结构不变。

Result: 实验结果表明，所提出的方法在多个任务上显著优于现有的强大基线模型，证明了利用结构信息进行微调的有效性和优越性。

Conclusion: 通过对权重矩阵变化的系统研究，作者揭示了全量微调的本质特性，并据此设计出一种新型微调方法。这种方法不仅提升了微调的效果，还为未来参数高效微调的研究提供了新的视角和方向。

Abstract: Large-scale foundation models have demonstrated remarkable versatility across
a wide range of downstream tasks. However, fully fine-tuning these models
incurs prohibitive computational costs, motivating the development of
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces
low-rank updates to pre-trained weights. Despite their empirical success, the
underlying mechanisms by which PEFT modifies model parameters remain
underexplored. In this work, we present a systematic investigation into the
structural changes of weight matrices during fully fine-tuning. Through
singular value decomposition (SVD), we reveal that fine-tuning predominantly
amplifies the top singular values while leaving the remainder largely intact,
suggesting that task-specific knowledge is injected into a low-dimensional
subspace. Furthermore, we find that the dominant singular vectors are
reoriented in task-specific directions, whereas the non-dominant subspace
remains stable. Building on these insights, we propose a novel method that
leverages learnable rescaling of top singular directions, enabling precise
modulation of the most influential components without disrupting the global
structure. Our approach achieves consistent improvements over strong baselines
across multiple tasks, highlighting the efficacy of structurally informed
fine-tuning.

</details>


### [102] [LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics](https://arxiv.org/abs/2505.23105)
*Abhishek Vijaya Kumar,Eric Ding,Arjun Devraj,Darius Bunandar,Rachee Singh*

Main category: cs.LG

TL;DR: In modern ML datacenters, job migration upon accelerator failure is inefficient. LUMION, a new optical fabric, integrates spare accelerators dynamically into ongoing workloads, avoiding migrations and maintaining performance. Experiments show it restarts jobs within ~1 second of failure and improves fine-tuning throughput nearly 2X.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of reserving full racks of idle accelerators for fault tolerance in ML datacenters.

Method: Introducing LUMION, a reconfigurable optical fabric that connects accelerators within a datacenter rack and dynamically integrates spare accelerators into ongoing workloads upon failures.

Result: LUMION swaps a failed GPU with a healthy one and restarts the ML job within ~1 second of failure, achieving nearly 2X improvement in fine-tuning throughput compared to traditional electrical racks.

Conclusion: LUMION provides a more efficient solution for handling accelerator failures in ML datacenters by dynamically integrating spare accelerators, reducing the need for reserved idle racks.

Abstract: When accelerators fail in modern ML datacenters, operators migrate the
affected ML training or inference jobs to entirely new racks. This approach,
while preserving network performance, is highly inefficient, requiring
datacenters to reserve full racks of idle accelerators for fault tolerance. In
this paper, we address this resource inefficiency by introducing LUMION, a
novel reconfigurable optical fabric for connecting accelerators within a
datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically
integrates spare accelerators into ongoing workloads as failures occur, thereby
maintaining consistent performance without costly migrations. We show the
benefits of LUMION by building an end-to-end hardware prototype. Our
experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a
healthy one and restarts the ML job within ~ 1 second of the failure. LUMION
achieves higher inter-GPU bandwidth compared to traditional electrical racks
after replacing failed accelerators with spare ones, leading to nearly 2X
improvement in fine-tuning throughput.

</details>


### [103] [Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery](https://arxiv.org/abs/2505.23106)
*Ning Liu,Yue Yu*

Main category: cs.LG

TL;DR: Attention mechanisms are powerful in AI but underused in physical systems. This paper introduces Neural Interpretable PDEs (NIPS), an improved neural operator architecture based on Nonlocal Attention Operators (NAO). NIPS uses linear attention and a learnable kernel network to enhance both accuracy and efficiency, outperforming NAO and other baselines in physics learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the untapped potential of attention mechanisms for modeling complex physical systems, addressing the challenge of discovering operators that map between functional spaces using limited data, often framed as an ill-posed inverse PDE problem.

Method: NIPS employs a linear attention mechanism for scalable learning and integrates a learnable kernel network functioning as a channel-independent convolution in Fourier space. This approach avoids explicitly computing and storing large pairwise interactions by amortizing spatial interaction handling into the Fourier transform.

Result: Empirical evaluations show that NIPS consistently outperforms NAO and other baselines across various benchmarks, indicating significant improvements in scalable, interpretable, and efficient physics learning.

Conclusion: NIPS represents a substantial advancement in the application of attention mechanisms to physical system modeling, offering superior performance in terms of predictive accuracy and computational efficiency compared to existing methods.

Abstract: Attention mechanisms have emerged as transformative tools in core AI domains
such as natural language processing and computer vision. Yet, their largely
untapped potential for modeling intricate physical systems presents a
compelling frontier. Learning such systems often entails discovering operators
that map between functional spaces using limited instances of function pairs --
a task commonly framed as a severely ill-posed inverse PDE problem. In this
work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator
architecture that builds upon and enhances Nonlocal Attention Operators (NAO)
in both predictive accuracy and computational efficiency. NIPS employs a linear
attention mechanism to enable scalable learning and integrates a learnable
kernel network that acts as a channel-independent convolution in Fourier space.
As a consequence, NIPS eliminates the need to explicitly compute and store
large pairwise interactions, effectively amortizing the cost of handling
spatial interactions into the Fourier transform. Empirical evaluations
demonstrate that NIPS consistently surpasses NAO and other baselines across
diverse benchmarks, heralding a substantial leap in scalable, interpretable,
and efficient physics learning. Our code and data accompanying this paper are
available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

</details>


### [104] [CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2505.23116)
*Pengfei Zhou,Yunlong Liu,Junli Liang,Qi Song,Xiangyang Li*

Main category: cs.LG

TL;DR: 本研究提出了一种名为CrossLinear的新模型，通过引入交叉相关嵌入模块解决了时间序列预测中变量依赖性建模的难题。该模型轻量高效，能捕捉变量间的时间不变直接依赖关系，并通过块处理和全局线性头提升短期与长期预测精度。实验表明，CrossLinear在12个真实数据集上表现优异，且其模块具有广泛的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型难以有效区分内生和外生变量，导致效率低下和过拟合问题。为解决这一挑战，需要一种能够高效捕捉变量间依赖性的新方法。

Method: CrossLinear模型包含一个可插拔的交叉相关嵌入模块，用于捕捉变量间的直接依赖关系，同时忽略间接或时间变化依赖，从而降低过拟合风险。此外，模型采用块状处理和全局线性头来捕捉时间依赖性。

Result: 在12个真实世界数据集上的实验结果表明，CrossLinear在短期和长期预测任务中均表现出色，消融实验证明了交叉相关嵌入模块的有效性。

Conclusion: CrossLinear提供了一种高效且通用的时间序列预测解决方案，其核心模块可以作为插件应用于不同领域的预测任务中。

Abstract: Time series forecasting with exogenous variables is a critical emerging
paradigm that presents unique challenges in modeling dependencies between
variables. Traditional models often struggle to differentiate between
endogenous and exogenous variables, leading to inefficiencies and overfitting.
In this paper, we introduce CrossLinear, a novel Linear-based forecasting model
that addresses these challenges by incorporating a plug-and-play
cross-correlation embedding module. This lightweight module captures the
dependencies between variables with minimal computational cost and seamlessly
integrates into existing neural networks. Specifically, it captures
time-invariant and direct variable dependencies while disregarding time-varying
or indirect dependencies, thereby mitigating the risk of overfitting in
dependency modeling and contributing to consistent performance improvements.
Furthermore, CrossLinear employs patch-wise processing and a global linear head
to effectively capture both short-term and long-term temporal dependencies,
further improving its forecasting precision. Extensive experiments on 12
real-world datasets demonstrate that CrossLinear achieves superior performance
in both short-term and long-term forecasting tasks. The ablation study
underscores the effectiveness of the cross-correlation embedding module.
Additionally, the generalizability of this module makes it a valuable plug-in
for various forecasting tasks across different domains. Codes are available at
https://github.com/mumiao2000/CrossLinear.

</details>


### [105] [Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking](https://arxiv.org/abs/2505.23117)
*Yuatyong Chaichana,Thanapat Trachu,Peerat Limkonchotiwat,Konpat Preechakul,Tirasan Khandhawit,Ekapol Chuangsuwanich*

Main category: cs.LG

TL;DR: In the era of large-scale training, model merging is important for creating multitasking models efficiently. Existing methods have limitations due to their assumptions about weight matrices. The paper introduces Decom-Renorm-Merge (DRM), which uses Singular Value Decomposition to align weight matrices into a joint space for effective merging. DRM outperforms other techniques across various models and settings.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing model merging methods that assume entries at identical positions in weight matrices serve the same function, which is problematic for finetuned neural networks with distinct feature compositions.

Method: The proposed method, Decom-Renorm-Merge (DRM), leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, enabling entry-wise merging. Renormalization is identified as a crucial component for creating a robust joint space.

Result: DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings on various models including ViT, DeBERTa, T5, and Llama3.1-8B.

Conclusion: DRM is a simple yet effective approach for model merging that overcomes the limitations of existing methods by using SVD to align weight matrices and highlights the importance of renormalization for successful merging.

Abstract: In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.

</details>


### [106] [DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs](https://arxiv.org/abs/2505.23131)
*Xinyu Yao,Daniel Bourgeois,Abhinav Jain,Yuxin Tang,Jiawen Yao,Zhimin Ding,Arlei Silva,Chris Jermaine*

Main category: cs.LG

TL;DR: This paper proposes Doppler, a three-stage framework for training dual-policy networks (SEL for selecting operations and PLC for placing operations on devices) to optimize the assignment of operations in a dataflow graph for minimizing execution time in complex machine learning workloads. It overcomes limitations of prior methods by not relying on bulk-synchronous systems, considering underlying system scheduling mechanisms, and incorporating expert-designed heuristics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of prior learning-based methods which rely on bulk-synchronous systems like TensorFlow, lack awareness of underlying system scheduling mechanisms, and exclusively depend on reinforcement learning without leveraging effective heuristics designed by experts.

Method: The method involves proposing Doppler, a three-stage framework for training dual-policy networks. These networks consist of a SEL policy for selecting operations and a PLC policy for placing chosen operations on devices.

Result: Experiments show that Doppler outperforms all baseline methods across tasks by reducing system execution time and demonstrates sampling efficiency by reducing per-episode training time.

Conclusion: Doppler provides an effective solution for optimizing operation assignments in dataflow graphs for complex machine learning workloads, overcoming previous limitations and achieving better performance.

Abstract: We study the problem of assigning operations in a dataflow graph to devices
to minimize execution time in a work-conserving system, with emphasis on
complex machine learning workloads. Prior learning-based methods often struggle
due to three key limitations: (1) reliance on bulk-synchronous systems like
TensorFlow, which under-utilize devices due to barrier synchronization; (2)
lack of awareness of the scheduling mechanism of underlying systems when
designing learning-based methods; and (3) exclusive dependence on reinforcement
learning, ignoring the structure of effective heuristics designed by experts.
In this paper, we propose \textsc{Doppler}, a three-stage framework for
training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for
selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen
operations on devices. Our experiments show that \textsc{Doppler} outperforms
all baseline methods across tasks by reducing system execution time and
additionally demonstrates sampling efficiency by reducing per-episode training
time.

</details>


### [107] [VERINA: Benchmarking Verifiable Code Generation](https://arxiv.org/abs/2505.23135)
*Zhe Ye,Zhengxu Yan,Jingxuan He,Timothe Kasriel,Kaiyu Yang,Dawn Song*

Main category: cs.LG

TL;DR: This paper presents Verina, a benchmark for evaluating verifiable code generation by LLMs. It highlights the challenges in generating correct code, specifications, and proofs, particularly focusing on the limitations of current LLMs in proof generation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of support for end-to-end verifiable code generation in current benchmarks and to improve the evaluation of LLMs' capabilities in generating correct code, specifications, and proofs.

Method: Introduced Verina, a high-quality benchmark consisting of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Evaluated state-of-the-art LLMs on this benchmark.

Result: State-of-the-art LLMs face significant challenges in verifiable code generation. The best model, OpenAI o4-mini, achieved only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs.

Conclusion: Verina provides a rigorous and comprehensive benchmark that can catalyze progress in verifiable code generation by highlighting the need for improvement in LLM-based theorem provers.

Abstract: Large language models (LLMs) are increasingly integrated in software
development, but ensuring correctness in LLM-generated code remains challenging
and often requires costly manual review. Verifiable code generation -- jointly
generating code, specifications, and proofs of code-specification alignment --
offers a promising path to address this limitation and further unleash LLMs'
benefits in coding. Yet, there exists a significant gap in evaluation: current
benchmarks often lack support for end-to-end verifiable code generation. In
this paper, we introduce Verina (Verifiable Code Generation Arena), a
high-quality benchmark enabling a comprehensive and modular evaluation of code,
specification, and proof generation as well as their compositions. Verina
consists of 189 manually curated coding tasks in Lean, with detailed problem
descriptions, reference implementations, formal specifications, and extensive
test suites. Our extensive evaluation of state-of-the-art LLMs reveals
significant challenges in verifiable code generation, especially in proof
generation, underscoring the need for improving LLM-based theorem provers in
verification domains. The best model, OpenAI o4-mini, generates only 61.4%
correct code, 51.0% sound and complete specifications, and 3.6% successful
proofs, with one trial per task. We hope Verina will catalyze progress in
verifiable code generation by providing a rigorous and comprehensive benchmark.
We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina
and our evaluation code on https://github.com/sunblaze-ucb/verina.

</details>


### [108] [Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners](https://arxiv.org/abs/2505.23150)
*Michal Nauman,Marek Cygan,Carmelo Sferrazza,Aviral Kumar,Pieter Abbeel*

Main category: cs.LG

TL;DR: Recent advances in language modeling and vision have limited impact in value-based reinforcement learning (RL). This work shows that high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings can address the problem of task interference in online RL, leading to state-of-the-art single and multi-task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current value-based reinforcement learning approaches which often use small models trained in a single-task context. The aim is to develop an approach that can robustly and scalably handle multi-task training by addressing issues like sparse rewards and gradient conflicts.

Method: The method involves using high-capacity value models trained via cross-entropy loss and conditioned on learnable task embeddings. This approach is designed to handle task interference in online RL, enabling effective multi-task training.

Result: The approach was tested on 7 multi-task benchmarks with over 280 unique tasks. It resulted in state-of-the-art single and multi-task performance as well as sample-efficient transfer to new tasks.

Conclusion: This study concludes that by using high-capacity value models with cross-entropy training and task embeddings, it is possible to achieve robust multi-task training in reinforcement learning, outperforming existing methods.

Abstract: Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.

</details>


### [109] [Best Arm Identification with Possibly Biased Offline Data](https://arxiv.org/abs/2505.23165)
*Le Yang,Vincent Y. F. Tan,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究了在固定置信度设定下可能带有偏见的离线数据的最佳臂识别（BAI）问题。提出了LUCB-H算法，通过引入辅助偏差校正，在LUCB框架内平衡离线和在线数据。理论分析表明，LUCB-H在离线数据误导时与标准LUCB样本复杂度相当，而在离线数据有帮助时显著优于标准LUCB。还推导了实例相关的下界，并通过数值实验验证了LUCB-H的有效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 最佳臂识别问题在实际场景中常见，如临床试验，但可能存在带有偏见的离线数据。现有的自适应算法在没有偏差界限先验知识的情况下无法有效处理这一问题。

Method: 提出了一种新的算法LUCB-H，它通过引入辅助偏差校正来调整离线和在线数据间的权重，从而在LUCB框架内实现自适应置信区间。

Result: 理论分析和数值实验表明，LUCB-H算法在离线数据误导时表现与标准LUCB相当，而当离线数据有用时，LUCB-H显著优于标准LUCB。同时，其上界与特定情况下的实例相关下界匹配。

Conclusion: LUCB-H算法为处理潜在偏差的离线数据提供了一种鲁棒且自适应的方法，能够在不同情况下有效结合离线和在线数据。

Abstract: We study the best arm identification (BAI) problem with potentially biased
offline data in the fixed confidence setting, which commonly arises in
real-world scenarios such as clinical trials. We prove an impossibility result
for adaptive algorithms without prior knowledge of the bias bound between
online and offline distributions. To address this, we propose the LUCB-H
algorithm, which introduces adaptive confidence bounds by incorporating an
auxiliary bias correction to balance offline and online data within the LUCB
framework. Theoretical analysis shows that LUCB-H matches the sample complexity
of standard LUCB when offline data is misleading and significantly outperforms
it when offline data is helpful. We also derive an instance-dependent lower
bound that matches the upper bound of LUCB-H in certain scenarios. Numerical
experiments further demonstrate the robustness and adaptability of LUCB-H in
effectively incorporating offline data.

</details>


### [110] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/abs/2505.23173)
*Shohei Enomoto*

Main category: cs.LG

TL;DR: Deep learning models face challenges in maintaining performance on different data distributions. Multi-source Domain Generalization (MDG) addresses this but is costly. Pseudo Multi-source Domain Generalization (PMDG), a new framework, uses style transfer and data augmentation to generate pseudo-domains from a single source domain for use with MDG algorithms. Experiments show a positive correlation between MDG and PMDG performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of MDG which include significant costs and difficulties associated with creating multi-domain datasets.

Method: PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, creating a synthetic multi-domain dataset that can be used with existing MDG algorithms.

Result: Experiments with PseudoDomainBed reveal a positive correlation between MDG and PMDG performance and the potential of pseudo-domains to match or exceed actual multi-domain performance with sufficient data.

Conclusion: PMDG provides valuable insights for future research in domain generalization and is a promising approach for applying MDG algorithms in more practical SDG settings.

Abstract: Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [111] [The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.23176)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Shijie Xu,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: This paper aims to improve the performance of low-rank decomposition methods in federated learning by introducing three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD). These techniques address key issues related to decomposition in FL, including what to decompose, how to decompose, and how to aggregate. Theoretical analysis ensures the convergence of MUD, while experimental results demonstrate faster convergence and superior accuracy compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: Previous research has used low-rank decomposition techniques to reduce communication overhead in federated learning, but there is room for improvement in the performance of these methods.

Method: The authors introduce three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD) to address specific issues related to decomposition in federated learning. They also provide a rigorous theoretical analysis to ensure the convergence of MUD.

Result: Extensive experimental results indicate that the proposed approach achieves faster convergence and superior accuracy when compared to relevant baseline methods.

Conclusion: The three introduced techniques are complementary and can be applied simultaneously to achieve optimal performance in federated learning.

Abstract: To improve the training efficiency of federated learning (FL), previous
research has employed low-rank decomposition techniques to reduce communication
overhead. In this paper, we seek to enhance the performance of these low-rank
decomposition methods. Specifically, we focus on three key issues related to
decomposition in FL: what to decompose, how to decompose, and how to aggregate.
Subsequently, we introduce three novel techniques: Model Update Decomposition
(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware
Decomposition (AAD), each targeting a specific issue. These techniques are
complementary and can be applied simultaneously to achieve optimal performance.
Additionally, we provide a rigorous theoretical analysis to ensure the
convergence of the proposed MUD. Extensive experimental results show that our
approach achieves faster convergence and superior accuracy compared to relevant
baseline methods. The code is available at
https://github.com/Leopold1423/fedmud-icml25.

</details>


### [112] [FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification](https://arxiv.org/abs/2505.23181)
*Tian Tian,Chunyan Miao,Hangwei Qian*

Main category: cs.LG

TL;DR: 提出了一种新的时间序列对比学习增强方法FreRA，该方法在分类、异常检测和迁移学习任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的时间域增强方法主要借鉴自视觉领域，未针对时间序列数据进行优化，可能导致语义信息失真。

Method: 从频域角度出发，提出了一种轻量级且有效的增强方法Frequency Refined Augmentation (FreRA)，通过自动分离关键和非关键频率成分，并分别应用语义感知的身份修改和语义无关的自适应修改来保护语义相关的信息并引入多样性。

Result: 在多个基准数据集上的实验表明，FreRA在时间序列分类、异常检测和迁移学习任务上优于十个领先基线方法。

Conclusion: FreRA是一种有效的时间序列对比学习增强策略，能够生成语义保持的视图，并在不同数据集上展现出卓越的对比表示学习和迁移学习泛化能力。

Abstract: Contrastive learning has emerged as a competent approach for unsupervised
representation learning. However, the design of an optimal augmentation
strategy, although crucial for contrastive learning, is less explored for time
series classification tasks. Existing predefined time-domain augmentation
methods are primarily adopted from vision and are not specific to time series
data. Consequently, this cross-modality incompatibility may distort the
semantically relevant information of time series by introducing mismatched
patterns into the data. To address this limitation, we present a novel
perspective from the frequency domain and identify three advantages for
downstream classification: global, independent, and compact. To fully utilize
the three properties, we propose the lightweight yet effective Frequency
Refined Augmentation (FreRA) tailored for time series contrastive learning on
classification tasks, which can be seamlessly integrated with contrastive
learning frameworks in a plug-and-play manner. Specifically, FreRA
automatically separates critical and unimportant frequency components.
Accordingly, we propose semantic-aware Identity Modification and
semantic-agnostic Self-adaptive Modification to protect semantically relevant
information in the critical frequency components and infuse variance into the
unimportant ones respectively. Theoretically, we prove that FreRA generates
semantic-preserving views. Empirically, we conduct extensive experiments on two
benchmark datasets, including UCR and UEA archives, as well as five large-scale
datasets on diverse applications. FreRA consistently outperforms ten leading
baselines on time series classification, anomaly detection, and transfer
learning tasks, demonstrating superior capabilities in contrastive
representation learning and generalization in transfer learning scenarios
across diverse datasets.

</details>


### [113] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/abs/2505.23182)
*Srijith Nair,Michael Lin,Amirreza Talebi,Peizhong Ju,Elizabeth Bentley,Jia Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Collaborative training methods like Federated Learning (FL) and Split
Learning (SL) enable distributed machine learning without sharing raw data.
However, FL assumes clients can train entire models, which is infeasible for
large-scale models. In contrast, while SL alleviates the client memory
constraint in FL by offloading most training to the server, it increases
network latency due to its sequential nature. Other methods address the
conundrum by using local loss functions for parallel client-side training to
improve efficiency, but they lack server feedback and potentially suffer poor
accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation
Gradient Estimation), a new federated split learning algorithm that estimates
server-side gradient feedback via auxiliary models. These auxiliary models
periodically adapt to emulate server behavior on local datasets. We show that
FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is
the number of communication rounds. This result matches FedAvg, while
significantly reducing communication costs and client memory requirements. Our
empirical results also verify that it outperforms existing state-of-the-art FSL
methods, offering both communication efficiency and accuracy.

</details>


### [114] [Two Is Better Than One: Rotations Scale LoRAs](https://arxiv.org/abs/2505.23184)
*Hongcan Guo,Guoshun Nan,Yuan Yang,Diyang Zhang,Haotian Li,Zhican Chen,Qinchuan Zhou,Yuhan Ran,Xinye Cao,Sicong Leng,Xiaofeng Tao,Xudong Jiang*

Main category: cs.LG

TL;DR: RadarGate is a new gating method for scalable LLMs that uses rotational operations of LoRA representations to improve expressiveness and feature interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional gating mechanisms in MoE models hinder scalability of LLMs, leading to poor generalization and underfitting issues due to restricted expressiveness of weighted-sum mechanisms.

Method: Proposes RadarGate, which fuses LoRA representations using a learnable component and applies a rotation matrix with learnable parameters defining angular relationships between LoRA representations.

Result: Extensive experiments across 21 tasks on 6 public benchmarks demonstrate the effectiveness of RadarGate for scaling LoRAs.

Conclusion: RadarGate improves expressiveness and addresses generalization and underfitting issues as the number of LoRAs grows; rotations encourage alignment of semantically similar representations.

Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates
large language models (LLMs) to efficiently adapt to diverse tasks. However,
traditional gating mechanisms that route inputs to the best experts may
fundamentally hinder LLMs' scalability, leading to poor generalization and
underfitting issues. We identify that the root cause lies in the restricted
expressiveness of existing weighted-sum mechanisms, both within and outside the
convex cone of LoRA representations. This motivates us to propose RadarGate, a
novel geometrically inspired gating method that introduces rotational
operations of LoRAs representations to boost the expressiveness and facilitate
richer feature interactions among multiple LoRAs for scalable LLMs.
Specifically, we first fuse each LoRA representation to other LoRAs using a
learnable component and then feed the output to a rotation matrix. This matrix
involves learnable parameters that define the relative angular relationship
between LoRA representations. Such a simple yet effective mechanism provides an
extra degree of freedom, facilitating the learning of cross-LoRA synergies and
properly tracking the challenging poor generalization and underfitting issues
as the number of LoRA grows. Extensive experiments on 6 public benchmarks
across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We
also provide valuable insights, revealing that the rotations to each pair of
representations are contrastive, encouraging closer alignment of semantically
similar representations during geometrical transformation while pushing
distance ones further apart. We will release our code to the community.

</details>


### [115] [Improving the Effective Receptive Field of Message-Passing Neural Networks](https://arxiv.org/abs/2505.23185)
*Shahaf E. Finder,Ron Shapira Weber,Moshe Eliasof,Oren Freifeld,Eran Treister*

Main category: cs.LG

TL;DR: Message-Passing Neural Networks (MPNNs) face a limited Effective Receptive Field (ERF) problem, which restrains their ability to capture long-range dependencies. Inspired by CNN's ERF augmentation techniques, this paper proposes an Interleaved Multiscale Message-Passing Neural Networks (IM-MPNN) architecture that incorporates hierarchical graph coarsening to enhance long-range interactions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limited ERF problem in MPNNs, which mirrors the challenges faced by CNNs and hinders the effective capturing of long-range dependencies or interactions in graph-structured data.

Method: The proposed method, IM-MPNN, involves hierarchical coarsening of the graph, allowing message-passing across multiscale representations. This facilitates long-range interactions without requiring excessive depth or parameterization.

Result: Extensive evaluations on benchmarks like LRGB show substantial improvements over baseline MPNNs in capturing long-range dependencies while maintaining computational efficiency.

Conclusion: The IM-MPNN architecture effectively addresses the limited ERF problem in MPNNs, enhancing their capability to capture long-range dependencies in graph-structured data.

Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for
processing and analyzing graph-structured data. However, their effectiveness is
often hindered by phenomena such as over-squashing, where long-range
dependencies or interactions are inadequately captured and expressed in the
MPNN output. This limitation mirrors the challenges of the Effective Receptive
Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical
receptive field is underutilized in practice. In this work, we show and
theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired
by recent advances in ERF augmentation for CNNs, we propose an Interleaved
Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address
these problems in MPNNs. Our method incorporates a hierarchical coarsening of
the graph, enabling message-passing across multiscale representations and
facilitating long-range interactions without excessive depth or
parameterization. Through extensive evaluations on benchmarks such as the
Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over
baseline MPNNs in capturing long-range dependencies while maintaining
computational efficiency.

</details>


### [116] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/abs/2505.23190)
*Yekun Zhu,Min Tang,Zheng Ma*

Main category: cs.LG

TL;DR: A new neural network method, DeepRTE, is developed to solve the Radiative Transfer Equation with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The Radiative Transfer Equation is crucial in many fields but challenging to solve accurately and efficiently.

Method: Proposes DeepRTE, a framework using pre-trained attention-based neural networks to solve the RTE.

Result: Demonstrates effectiveness through comprehensive numerical experiments.

Conclusion: DeepRTE offers a promising approach for solving the RTE.

Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our proposed DeepRTE framework leverages pre-trained
attention-based neural networks to solve the RTE with high accuracy and
computational efficiency. The efficacy of the proposed approach is
substantiated through comprehensive numerical experiments.

</details>


### [117] [Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics](https://arxiv.org/abs/2505.23194)
*Shiwei Li,Xiandi Luo,Xing Tang,Haozhao Wang,Hao Chen,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: 研究了LoRA中非零初始化对微调动力学的影响，发现非零初始化能提高对次优学习率的鲁棒性且不影响微调性能。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA方法中，矩阵A或B通常初始化为零以确保从预训练模型开始微调，但缺乏理论支持。本文旨在探讨非零初始化对LoRA微调动力学的影响。

Method: 从无限宽度视角分析LoRA层中同时将矩阵A和B初始化为非零值的情况，评估其对微调动力学、学习率鲁棒性和微调性能的影响。

Result: 非零初始化提高了LoRA对次优（特别是较小）学习率的鲁棒性，并且引入的随机噪声一般不会影响微调性能，表明微调不一定需要严格从预训练模型开始。

Conclusion: 通过理论分析和广泛实验验证，证明了LoRA中非零初始化的可行性和优势，为改进LoRA方法提供了新思路。

Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method. In standard LoRA layers, one of the matrices, $A$ or $B$, is
initialized to zero, ensuring that fine-tuning starts from the pretrained
model. However, there is no theoretical support for this practice. In this
paper, we investigate the impact of non-zero initialization on LoRA's
fine-tuning dynamics from an infinite-width perspective. Our analysis reveals
that, compared to zero initialization, simultaneously initializing $A$ and $B$
to non-zero values improves LoRA's robustness to suboptimal learning rates,
particularly smaller ones. Further analysis indicates that although the
non-zero initialization of $AB$ introduces random noise into the pretrained
weight, it generally does not affect fine-tuning performance. In other words,
fine-tuning does not need to strictly start from the pretrained model. The
validity of our findings is confirmed through extensive experiments across
various models and datasets. The code is available at
https://github.com/Leopold1423/non_zero_lora-icml25.

</details>


### [118] [Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning](https://arxiv.org/abs/2505.23195)
*Lifan Zhao,Yanyan Shen,Zhaoyang Liu,Xue Wang,Jiaji Deng*

Main category: cs.LG

TL;DR: 尽管时间序列基础模型（TSFM）具有显著的零样本预测性能，但在微调后仍无法始终优于专门的小模型。本文通过研究发现TSFM中存在计算的固有稀疏性和冗余性，并提出了一种结构化剪枝方法来优化微调过程，从而在多个基准测试中显著提升了TSFM的预测性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效地将时间序列基础模型（TSFM）适应于目标预测任务，解决其微调后无法一致超越小规模专用模型的问题。

Method: 通过对多种TSFM进行实证研究，发现预训练模型中存在计算的固有稀疏性和冗余性。基于此，提出一种结构化剪枝方法，通过聚焦于更相关和紧凑的参数空间来规范微调过程，保留了有价值的先验知识。

Result: 在七个TSFM和六个基准数据集上的广泛实验表明，微调剪枝后的TSFM显著提高了预测性能，且通常达到最先进的水平，超越了强大的专用基线模型。

Conclusion: 提出的“剪枝-再微调”范式能够有效提升TSFM的预测性能，使其在多种任务上超越专用模型，为实现有效的TSFM适应提供了新途径。

Abstract: Scaling laws motivate the development of Time Series Foundation Models
(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot
forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot
consistently outperform smaller, specialized models trained on full-shot
downstream data. A key question is how to realize effective adaptation of TSFMs
for a target forecasting task. Through empirical studies on various TSFMs, the
pre-trained models often exhibit inherent sparsity and redundancy in
computation, suggesting that TSFMs have learned to activate task-relevant
network substructures to accommodate diverse forecasting tasks. To preserve
this valuable prior knowledge, we propose a structured pruning method to
regularize the subsequent fine-tuning process by focusing it on a more relevant
and compact parameter space. Extensive experiments on seven TSFMs and six
benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly
improves forecasting performance compared to fine-tuning original models. This
"prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art
performance and surpass strong specialized baselines.

</details>


### [119] [Daunce: Data Attribution through Uncertainty Estimation](https://arxiv.org/abs/2505.23223)
*Xingyuan Pan,Chenlu Ye,Joseph Melkonian,Jiaqi W. Ma,Tong Zhang*

Main category: cs.LG

TL;DR: The paper presents Daunce, a new method for training data attribution that estimates uncertainty to improve scalability and accuracy compared to existing methods. It works effectively on large models like LLMs and even proprietary ones such as OpenAI's GPT.


<details>
  <summary>Details</summary>
Motivation: Existing TDA methods either struggle with scalability due to reliance on gradients or suffer from reduced accuracy when using random projection-based techniques. To address these limitations, the authors draw inspiration from the relationship between uncertainty and influence functions.

Method: Daunce fine-tunes perturbed models and calculates the covariance of per-example losses across them as an attribution score, leveraging uncertainty estimation for scalable and accurate data attribution.

Result: Daunce demonstrates superior scalability and accuracy in comparison to previous TDA methods. It successfully performs data attribution on various tasks including LLM fine-tuning and is compatible with black-box models. Notably, it achieves the first instance of data attribution on proprietary LLMs like OpenAI's GPT models.

Conclusion: Daunce provides a novel approach to data attribution that balances scalability and accuracy, making it suitable for large-scale applications such as debugging and valuating data for LLMs.

Abstract: Training data attribution (TDA) methods aim to identify which training
examples influence a model's predictions on specific test data most. By
quantifying these influences, TDA supports critical applications such as data
debugging, curation, and valuation. Gradient-based TDA methods rely on
gradients and second-order information, limiting their applicability at scale.
While recent random projection-based methods improve scalability, they often
suffer from degraded attribution accuracy. Motivated by connections between
uncertainty and influence functions, we introduce Daunce - a simple yet
effective data attribution approach through uncertainty estimation. Our method
operates by fine-tuning a collection of perturbed models and computing the
covariance of per-example losses across these models as the attribution score.
Daunce is scalable to large language models (LLMs) and achieves more accurate
attribution compared to existing TDA methods. We validate Daunce on tasks
ranging from vision tasks to LLM fine-tuning, and further demonstrate its
compatibility with black-box model access. Applied to OpenAI's GPT models, our
method achieves, to our knowledge, the first instance of data attribution on
proprietary LLMs.

</details>


### [120] [Generalizability vs. Counterfactual Explainability Trade-Off](https://arxiv.org/abs/2505.23225)
*Fabiano Veglianti,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: The paper explores the connection between model generalization and counterfactual explainability in supervised learning, introducing ε-Valid Counterfactual Probability (ε-VCP) as a metric. It shows that ε-VCP increases with model overfitting, linking poor generalization to easier counterfactual generation. Empirical results support this theory.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between model generalization and counterfactual explainability, aiming to find a quantitative measure for overfitting using counterfactuals.

Method: Introduced ε-Valid Counterfactual Probability (ε-VCP), analyzed its theoretical relationship with the geometry of the model's decision boundary, and demonstrated how it increases with model overfitting.

Result: Established a connection between poor generalization and the ease of counterfactual generation, showing an inherent trade-off between generalization and counterfactual explainability. Empirical results validated the theory.

Conclusion: ε-VCP serves as a practical proxy for quantitatively characterizing overfitting, providing insights into the trade-off between generalization and counterfactual explainability.

Abstract: In this work, we investigate the relationship between model generalization
and counterfactual explainability in supervised learning. We introduce the
notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) --
the probability of finding perturbations of a data point within its
$\varepsilon$-neighborhood that result in a label change. We provide a
theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the
model's decision boundary, showing that $\varepsilon$-VCP tends to increase
with model overfitting. Our findings establish a rigorous connection between
poor generalization and the ease of counterfactual generation, revealing an
inherent trade-off between generalization and counterfactual explainability.
Empirical results validate our theory, suggesting $\varepsilon$-VCP as a
practical proxy for quantitatively characterizing overfitting.

</details>


### [121] [Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method](https://arxiv.org/abs/2505.23228)
*Wanfu Gao,Jun Gao,Qingqi Han,Hanlin Pan,Kunpeng Liu*

Main category: cs.LG

TL;DR: The paper proposes a random walk graph to capture complex nonlinear associations in multi-label datasets and aligns variable spaces using low-dimensional representation coefficients.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in feature dimension introduces implicit associations between features and labels, making their relationships increasingly complex. Existing methods using low-dimensional linear decomposition fail to capture these complex nonlinear associations effectively.

Method: 1) Design a random walk graph integrating feature-feature, label-label, and feature-label relationships to capture nonlinear and implicit indirect associations while optimizing latent representations after low-rank decomposition. 2) Align variable spaces by leveraging low-dimensional representation coefficients while preserving the manifold structure between original high-dimensional data and low-dimensional space.

Result: Extensive experiments and ablation studies on seven benchmark datasets and three representative datasets using various evaluation metrics demonstrate the superiority of the proposed method.

Conclusion: The proposed method effectively addresses the challenges of capturing complex nonlinear associations and aligning feature and label spaces in multi-label datasets.

Abstract: The rapid growth in feature dimension may introduce implicit associations
between features and labels in multi-label datasets, making the relationships
between features and labels increasingly complex. Moreover, existing methods
often adopt low-dimensional linear decomposition to explore the associations
between features and labels. However, linear decomposition struggles to capture
complex nonlinear associations and may lead to misalignment between the feature
space and the label space. To address these two critical challenges, we propose
innovative solutions. First, we design a random walk graph that integrates
feature-feature, label-label, and feature-label relationships to accurately
capture nonlinear and implicit indirect associations, while optimizing the
latent representations of associations between features and labels after
low-rank decomposition. Second, we align the variable spaces by leveraging
low-dimensional representation coefficients, while preserving the manifold
structure between the original high-dimensional multi-label data and the
low-dimensional representation space. Extensive experiments and ablation
studies conducted on seven benchmark datasets and three representative datasets
using various evaluation metrics demonstrate the superiority of the proposed
method\footnote{Code: https://github.com/Heilong623/-GRW-}.

</details>


### [122] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/abs/2505.23244)
*Emo Todorov*

Main category: cs.LG

TL;DR: 在特定MDP家族中，展示了确定性和随机性策略梯度之间的关系，并提出了一种构建等效确定性策略MDP的一般方法。结果表明，通过逼近状态值函数而非状态-控制值函数，可以统一策略梯度方法。


<details>
  <summary>Details</summary>
Motivation: 研究连续控制中确定性和随机性策略梯度之间的关系，以期找到统一的策略梯度方法。

Method: 分析了具有高斯控制噪声和二次控制成本的MDP家族中的确定性和随机性策略梯度、自然梯度和状态值函数的关系；提出了构造与给定随机策略MDP等效的确定性策略MDP的一般过程。

Result: 在所研究的MDP家族中，确定性和随机性策略梯度、自然梯度和状态值函数相同，但状态-控制值函数不同；并且提供了一种构建等效确定性策略MDP的方法。

Conclusion: 可以通过逼近状态值函数而非状态-控制值函数来统一策略梯度方法。

Abstract: Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [123] [Measuring Participant Contributions in Decentralized Federated Learning](https://arxiv.org/abs/2505.23246)
*Honoka Anada,Tatsuya Kaneko,Shinya Takamaeda-Yamazaki*

Main category: cs.LG

TL;DR: 本研究提出了用于去中心化联邦学习（DFL）的参与者贡献度量的新方法，包括DFL-Shapley和其可计算近似DFL-MR，通过实验验证了其有效性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有的贡献度量方法仅适用于集中式联邦学习（CFL），而无法直接应用于去中心化联邦学习（DFL）。因此，需要开发适合DFL的贡献度量方法以激励参与者并确保透明性。

Method: 1. 提出DFL-Shapley：将Shapley值扩展到DFL场景。
2. 提出DFL-MR：作为DFL-Shapley的可计算近似，通过累积每轮的Shapley值来估计整体贡献。
3. 在多种联邦学习场景下评估DFL-Shapley和DFL-MR，并与现有的CFL度量进行比较。

Result: 实验结果表明：
1. DFL-Shapley是一个有效的基准度量。
2. DFL-MR在各种设置下接近DFL-Shapley，具有较高的准确性。

Conclusion: 所提出的DFL-Shapley和DFL-MR为DFL提供了一种有效的参与者贡献度量方法，解决了现有方法在去中心化场景中的局限性。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their data. Measuring participant contributions in FL is
crucial for incentivizing clients and ensuring transparency. While various
methods have been proposed for contribution measurement, they are designed
exclusively for centralized federated learning (CFL), where a central server
collects and aggregates client models, along with evaluating their
contributions. Meanwhile, decentralized federated learning (DFL), in which
clients exchange models directly without a central server, has gained
significant attention for mitigating communication bottlenecks and eliminating
a single point of failure. However, applying existing contribution measurement
methods to DFL is challenging due to the presence of multiple global models and
the absence of a central server. In this study, we present novel methodologies
for measuring participant contributions in DFL. We first propose DFL-Shapley,
an extension of the Shapley value tailored for DFL, adapting this widely used
CFL metric to decentralized settings. Given the impracticality of computing the
ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable
approximation that estimates overall contributions by accumulating round-wise
Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios
and compare them with existing CFL metrics. The experimental results confirm
DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity
to DFL-Shapley across various settings, highlighting their effectiveness as
contribution metrics in DFL.

</details>


### [124] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Main category: cs.LG

TL;DR: The paper proposes a reward adjustment model to accelerate RLHF training for LLMs by increasing reward variance, while preserving preferences and expectation. It integrates with GRPO to form GRPOVI, improving training efficiency as demonstrated in experiments.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning from human feedback (RLHF) is crucial for aligning large language models (LLMs) with human values. While GRPO has been successful in RLHF, efficient training remains challenging. Recent findings indicate that higher reward variance can speed up RLHF training, motivating the development of a reward adjustment model.

Method: A reward adjustment model is designed to increase reward variance while maintaining relative preferences and reward expectation. This involves solving a nonconvex optimization problem, addressed via an O(n log n) algorithm identifying extreme points of the feasible set. The model is integrated into GRPO, creating GRPOVI for more efficient RLHF training.

Result: Experiments show that GRPOVI significantly improves RLHF training efficiency compared to the original GRPO algorithm.

Conclusion: The proposed reward adjustment model, when integrated into GRPO as GRPOVI, accelerates RLHF training for LLMs, providing both practical improvements and theoretical insights.

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [125] [Efficiently Access Diffusion Fisher: Within the Outer Product Span Space](https://arxiv.org/abs/2505.23264)
*Fangyikang Wang,Hubery Yin,Shaobin Zhuang,Huminhao Zhu,Yinan Li,Lei Qian,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Main category: cs.LG

TL;DR: Recent advancements in Diffusion models have explored incorporating the second-order diffusion Fisher information (DF) into various tasks. Current practices typically approximate DF using auto-differentiation, which lacks accuracy guarantees and is time-consuming. This paper proposes two efficient approximation algorithms for accessing the trace and matrix-vector multiplication of DF, based on the outer-product structure. These algorithms bypass auto-differentiation operations with time-efficient vector-product calculations and establish error bounds.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of current methods used to approximate the diffusion Fisher information (DF), which include lack of accuracy guarantees and high computational costs. By developing more efficient algorithms, the authors aim to improve both the accuracy and speed of DF-related computations.

Method: The authors demonstrate that the diffusion Fisher resides within a space spanned by the outer products of score and initial data. Based on this insight, they develop two efficient approximation algorithms: one for accessing the trace of DF and another for matrix-vector multiplication. These algorithms utilize time-efficient vector-product calculations instead of auto-differentiation. Additionally, the authors establish approximation error bounds for these algorithms.

Result: Experiments conducted in likelihood evaluation and adjoint optimization show that the proposed algorithms offer superior accuracy and reduced computational cost compared to existing methods. Furthermore, the novel outer-product formulation of DF enables the design of the first numerical verification experiment for the optimal transport property of the general PF-ODE deduced map.

Conclusion: This paper introduces two efficient approximation algorithms for the diffusion Fisher information (DF) that significantly improve upon current methods in terms of both accuracy and computational efficiency. The algorithms' effectiveness is demonstrated through experiments, and the work contributes to a deeper understanding of the optimal transport property.

Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the
second-order diffusion Fisher information (DF), defined as the negative Hessian
of log density, into various downstream tasks and theoretical analysis.
However, current practices typically approximate the diffusion Fisher by
applying auto-differentiation to the learned score network. This black-box
method, though straightforward, lacks any accuracy guarantee and is
time-consuming. In this paper, we show that the diffusion Fisher actually
resides within a space spanned by the outer products of score and initial data.
Based on the outer-product structure, we develop two efficient approximation
algorithms to access the trace and matrix-vector multiplication of DF,
respectively. These algorithms bypass the auto-differentiation operations with
time-efficient vector-product calculations. Furthermore, we establish the
approximation error bounds for the proposed algorithms. Experiments in
likelihood evaluation and adjoint optimization demonstrate the superior
accuracy and reduced computational cost of our proposed algorithms.
Additionally, based on the novel outer-product formulation of DF, we design the
first numerical verification experiment for the optimal transport property of
the general PF-ODE deduced map.

</details>


### [126] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
*Haokun Chen,Yueqi Zhang,Yuan Bi,Yao Zhang,Tong Liu,Jinhe Bi,Jian Lan,Jindong Gu,Claudia Grosser,Denis Krompass,Nassir Navab,Volker Tresp*

Main category: cs.LG

TL;DR: In this paper, researchers recognize the challenge of evaluating machine unlearning algorithms for Large Language Models (LLMs) and propose a comprehensive auditing framework including three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. They also suggest a new technique using intermediate activation perturbations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is the increasing concerns about data privacy and ownership in LLMs trained on massive datasets that may contain sensitive or copyrighted content. Existing regulatory frameworks like GDPR provide rights to individuals to request removal of their information, necessitating effective machine unlearning algorithms.

Method: The authors introduce a comprehensive auditing framework which includes three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. Additionally, they propose an alternative novel technique that leverages intermediate activation perturbations for auditing.

Result: Through various auditing algorithms, the effectiveness and robustness of different unlearning strategies are evaluated. The newly proposed technique using intermediate activation perturbations addresses some limitations of existing auditing methods.

Conclusion: This work contributes to the field by providing a thorough evaluation framework for machine unlearning algorithms and suggesting a promising alternative auditing method beyond prompt-based approaches.

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [127] [Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data](https://arxiv.org/abs/2505.23285)
*Muhammad Shafi,Syed Mohsin Bokhari*

Main category: cs.LG

TL;DR: This paper uses Sentinel-2 satellite data and supervised machine learning algorithms to analyze and compare land cover and land use changes in the Sultanate of Oman from 2016 to 2021.


<details>
  <summary>Details</summary>
Motivation: Land cover and land use changes play critical roles in resource management, urbanization, soil and environmental protection, and promoting sustainable development.

Method: Multispectral spatiotemporal data were obtained from the open-source Sentinel-2 satellite dataset and supervised machine learning algorithms were used for training and classifying different types of land covers.

Result: The model allowed for an effective comparative evaluation of LULC changes across various governorates of Oman from 2016 to 2021.

Conclusion: Using multispectral data and machine learning can successfully monitor and predict LULC changes.

Abstract: Land cover and land use (LULC) changes are key applications of satellite
imagery, and they have critical roles in resource management, urbanization,
protection of soils and the environment, and enhancing sustainable development.
The literature has heavily utilized multispectral spatiotemporal satellite data
alongside advanced machine learning algorithms to monitor and predict LULC
changes. This study analyzes and compares LULC changes across various
governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using
annual time steps. For the chosen region, multispectral spatiotemporal data
were acquired from the open-source Sentinel-2 satellite dataset. Supervised
machine learning algorithms were used to train and classify different land
covers, such as water bodies, crops, urban, etc. The constructed model was
subsequently applied within the study region, allowing for an effective
comparative evaluation of LULC changes within the given timeframe.

</details>


### [128] [Score-based Generative Modeling for Conditional Independence Testing](https://arxiv.org/abs/2505.23309)
*Yixin Ren,Chenghou Jin,Yewei Xia,Li Ke,Longtao Huang,Hui Xue,Hao Zhang,Jihong Guan,Shuigeng Zhou*

Main category: cs.LG

TL;DR: This paper proposes a new method for conditional independence (CI) testing using score-based generative modeling, which addresses the shortcomings of existing GAN-based methods by ensuring precise Type I error control and strong testing power. The approach involves sliced conditional score matching, Langevin dynamics sampling, and a goodness-of-fit stage, with theoretical guarantees and superior performance demonstrated through extensive experiments.


<details>
  <summary>Details</summary>
Motivation: Determining CI relationships between random variables is crucial in machine learning and statistics but remains challenging, especially in high dimensions. Current GAN-based CI testing methods suffer from issues like inaccurate conditional distribution modeling and training instability, leading to poor performance.

Method: The proposed method uses score-based generative modeling for CI testing. It includes sliced conditional score matching for accurate estimation of conditional scores, Langevin dynamics conditional sampling for generating null hypothesis samples, and a goodness-of-fit stage to verify generated samples and improve interpretability.

Result: Theoretical analysis establishes an error bound for conditional distributions modeled by score-based generative models and proves the validity of the CI tests. Experimental results on synthetic and real-world datasets demonstrate that the method outperforms state-of-the-art approaches significantly.

Conclusion: The novel score-based generative modeling approach provides a promising solution for CI testing, revitalizing the use of generative models in this domain.

Abstract: Determining conditional independence (CI) relationships between random
variables is a fundamental yet challenging task in machine learning and
statistics, especially in high-dimensional settings. Existing generative
model-based CI testing methods, such as those utilizing generative adversarial
networks (GANs), often struggle with undesirable modeling of conditional
distributions and training instability, resulting in subpar performance. To
address these issues, we propose a novel CI testing method via score-based
generative modeling, which achieves precise Type I error control and strong
testing power. Concretely, we first employ a sliced conditional score matching
scheme to accurately estimate conditional score and use Langevin dynamics
conditional sampling to generate null hypothesis samples, ensuring precise Type
I error control. Then, we incorporate a goodness-of-fit stage into the method
to verify generated samples and enhance interpretability in practice. We
theoretically establish the error bound of conditional distributions modeled by
score-based generative models and prove the validity of our CI tests. Extensive
experiments on both synthetic and real-world datasets show that our method
significantly outperforms existing state-of-the-art methods, providing a
promising way to revitalize generative model-based CI testing.

</details>


### [129] [Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing](https://arxiv.org/abs/2505.23320)
*Connor Cooper,Geoffrey I. Webb,Daniel F. Schmidt*

Main category: cs.LG

TL;DR: This paper presents a new method for parameter estimation in Bayesian network classifiers (BNCs) using log-linear regression, which is simpler and faster than current techniques while maintaining competitive performance with random forests.


<details>
  <summary>Details</summary>
Motivation: Bayesian network classifiers have attractive properties but have underperformed compared to other methods like random forests. Recent improvements using hierarchical Dirichlet processes are complex and inflexible.

Method: The authors introduce a log-linear regression approach to approximate the behavior of hierarchical Dirichlet processes for parameter estimation in BNCs.

Result: Experiments demonstrate that this new method outperforms HDP smoothing, is significantly faster, and remains competitive with random forests on categorical data.

Conclusion: The novel log-linear regression method offers a flexible, interpretable, and efficient alternative for parameter estimation in BNCs.

Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable
for a modern classifier: They are easily interpretable, highly scalable, and
offer adaptable complexity. However, traditional methods for learning BNCs have
historically underperformed when compared to leading classification methods
such as random forests. Recent parameter smoothing techniques using
hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve
performance competitive with random forests on categorical data, but these
techniques are relatively inflexible, and require a complicated, specialized
sampling process. In this paper, we introduce a novel method for parameter
estimation that uses a log-linear regression to approximate the behaviour of
HDPs. As a linear model, our method is remarkably flexible and simple to
interpret, and can leverage the vast literature on learning linear models. Our
experiments show that our method can outperform HDP smoothing while being
orders of magnitude faster, remaining competitive with random forests on
categorical data.

</details>


### [130] [X2Graph for Cancer Subtyping Prediction on Biological Tabular Data](https://arxiv.org/abs/2505.23334)
*Tu Bui,Mohamed Suliman,Aparajita Haldar,Mohammed Amer,Serban Georgescu*

Main category: cs.LG

TL;DR: 在医疗领域中，深度学习对文本、音频和图像数据集产生了变革性的影响，但在表格数据中的主导地位仍不明确。本文提出了X2Graph，一种新的深度学习方法，在小型生物表格数据集上表现出色。X2Graph利用列之间的外部知识（如基因相互作用），将每个样本转换为图结构，从而实现标准消息传递算法的图建模。X2Graph在三个癌症亚型数据集上的表现优于现有的基于树和深度学习的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在文本、音频和图像数据集上产生了变革性影响，但其在表格数据（特别是在数据稀缺的医疗领域）中的优势尚不明确，需要探索新的方法来提升小规模生物表格数据集上的性能。

Method: 提出了一种名为X2Graph的新方法，该方法通过利用表列之间（例如基因相互作用）的外部知识，将每个样本转化为图结构，并使用标准的消息传递算法进行图建模。

Result: X2Graph在三个癌症亚型数据集上的表现优于现有的基于树和深度学习的方法。

Conclusion: X2Graph是一种有效的深度学习方法，适用于小型生物表格数据集，并在医疗领域（如癌症亚型分类）展现出优越性能。

Abstract: Despite the transformative impact of deep learning on text, audio, and image
datasets, its dominance in tabular data, especially in the medical domain where
data are often scarce, remains less clear. In this paper, we propose X2Graph, a
novel deep learning method that achieves strong performance on small biological
tabular datasets. X2Graph leverages external knowledge about the relationships
between table columns, such as gene interactions, to convert each sample into a
graph structure. This transformation enables the application of standard
message passing algorithms for graph modeling. Our X2Graph method demonstrates
superior performance compared to existing tree-based and deep learning methods
across three cancer subtyping datasets.

</details>


### [131] [Matryoshka Model Learning for Improved Elastic Student Models](https://arxiv.org/abs/2505.23337)
*Chetan Verma,Aditya Srinivas Timmaraju,Cho Jui-Hsieh,Suyash Damle,Ngot Bui,Yang Zhang,Wen Chen,Xin Liu,Prateek Jain,Inderjit S Dhillon*

Main category: cs.LG

TL;DR: MatTA is a framework that uses a Teacher-TA-Student method to train multiple accurate Student models from a TA model, providing servable options to trade off accuracy for lower serving cost, with demonstrated improvements in live A/B tests and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Industry-grade ML models require significant resources for development to meet serving constraints. The motivation is to create a framework that can efficiently produce multiple servable models with varying accuracy-serving cost trade-offs.

Method: Propose MatTA, a framework using a Teacher-TA-Student recipe where TA models (larger versions of Student models) help bridge the gap between Teacher and Student models, enabling the extraction of multiple accurate Student models from one training run.

Result: Demonstrated through live A/B tests within a production ML system with 20% improvement on a key metric. Also showcased on GPT-2 Medium with over 24% relative improvement on SAT Math and over 10% on LAMBADA benchmark.

Conclusion: MatTA provides an effective methodology to train multiple servable models with different accuracy-serving cost trade-offs, showing practical efficacy in both proprietary and public models.

Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving
serving constraints, which requires significant resources for model
development. In this paper, we propose MatTA, a framework for training multiple
accurate Student models using a novel Teacher-TA-Student recipe. TA models are
larger versions of the Student models with higher capacity, and thus allow
Student models to better relate to the Teacher model and also bring in more
domain-specific expertise. Furthermore, multiple accurate Student models can be
extracted from the TA model. Therefore, despite only one training run, our
methodology provides multiple servable options to trade off accuracy for lower
serving cost. We demonstrate the proposed method, MatTA, on proprietary
datasets and models. Its practical efficacy is underscored by live A/B tests
within a production ML system, demonstrating 20% improvement on a key metric.
We also demonstrate our method on GPT-2 Medium, a public model, and achieve
relative improvements of over 24% on SAT Math and over 10% on the LAMBADA
benchmark.

</details>


### [132] [Graph Positional Autoencoders as Self-supervised Learners](https://arxiv.org/abs/2505.23345)
*Yang Liu,Deyu Bo,Wenxuan Cao,Yuan Fang,Yawen Li,Chuan Shi*

Main category: cs.LG

TL;DR: Graph self-supervised learning method, Graph Positional Autoencoders (GraphPAE), is proposed to reconstruct both node features and positions with a dual-path architecture. It achieves state-of-the-art performance in various experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional graph autoencoders primarily capture low-frequency signals and fail to learn expressive structural information when using node or edge masking paradigms.

Method: The proposed Graph Positional Autoencoders (GraphPAE) employs a dual-path architecture. The feature path uses positional encoding to enhance message-passing processing for predicting corrupted information, while the position path leverages node representations to refine positions and approximate eigenvectors, allowing the encoder to learn diverse frequency information.

Result: Extensive experiments verify the effectiveness of GraphPAE in heterophilic node classification, graph property prediction, and transfer learning. It consistently outperforms baselines by a large margin and achieves state-of-the-art performance.

Conclusion: GraphPAE addresses the limitations of traditional graph autoencoders by employing a dual-path architecture to reconstruct both node features and positions, leading to superior performance in various tasks.

Abstract: Graph self-supervised learning seeks to learn effective graph representations
without relying on labeled data. Among various approaches, graph autoencoders
(GAEs) have gained significant attention for their efficiency and scalability.
Typically, GAEs take incomplete graphs as input and predict missing elements,
such as masked nodes or edges. While effective, our experimental investigation
reveals that traditional node or edge masking paradigms primarily capture
low-frequency signals in the graph and fail to learn the expressive structural
information. To address these issues, we propose Graph Positional Autoencoders
(GraphPAE), which employs a dual-path architecture to reconstruct both node
features and positions. Specifically, the feature path uses positional encoding
to enhance the message-passing processing, improving GAE's ability to predict
the corrupted information. The position path, on the other hand, leverages node
representations to refine positions and approximate eigenvectors, thereby
enabling the encoder to learn diverse frequency information. We conduct
extensive experiments to verify the effectiveness of GraphPAE, including
heterophilic node classification, graph property prediction, and transfer
learning. The results demonstrate that GraphPAE achieves state-of-the-art
performance and consistently outperforms baselines by a large margin.

</details>


### [133] [Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2505.23347)
*Yuting Li,Shaoyuan Huang,Tengwen Zhang,Cheng Zhang,Xiaofei Wang,Victor C. M. Leung*

Main category: cs.LG

TL;DR: 为了应对直播服务的快速增长，本文提出了一种基于主动异常检测的调度框架Sentinel，用于优化Crowdsourced Cloud-edge服务平台（CCPs）的收入。该框架通过两阶段的调度范式，在预调度阶段进行异常检测和策略池构建，在后调度阶段根据请求触发合适的调度策略。实验表明，Sentinel能显著降低70%的异常频率，提高74%的收入，并将调度速度提高一倍。


<details>
  <summary>Details</summary>
Motivation: 随着直播服务的快速发展，Crowdsourced Cloud-edge服务平台（CCPs）在满足日益增长的需求方面起着越来越重要的作用。然而，由于CCPs的不稳定性以及其规模庞大，现有的流调度优化策略难以实现理想的实际效果，尤其是在时间敏感的调度中，异常检测变得尤为困难。

Method: 提出了一种名为Sentinel的主动异常检测调度框架，采用两阶段调度范式：1) 预调度阶段 - 进行异常检测并构建策略池；2) 后调度阶段 - 根据到达的请求，从预生成的策略中选择合适的调度策略以实施调度过程。

Result: 通过在真实数据集上的广泛实验，Sentinel能够将异常频率降低70%，收入提高74%，并且调度速度翻倍。

Conclusion: Sentinel框架通过有效的异常检测和调度优化，显著提升了CCPs平台的性能和收益，证明了其在实际应用中的有效性和优越性。

Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge
service Platforms (CCPs) are playing an increasingly important role in meeting
the increasing demand. Although stream scheduling plays a critical role in
optimizing CCPs' revenue, most optimization strategies struggle to achieve
practical results due to various anomalies in unstable CCPs. Additionally, the
substantial scale of CCPs magnifies the difficulties of anomaly detection in
time-sensitive scheduling. To tackle these challenges, this paper proposes
Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel
models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in
the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a
strategy pool; in the post-scheduling stage, upon request arrival, it triggers
an appropriate scheduling based on a pre-generated strategy to implement the
scheduling process. Extensive experiments on realistic datasets show that
Sentinel significantly reduces anomaly frequency by 70%, improves revenue by
74%, and doubles the scheduling speed.

</details>


### [134] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Main category: cs.LG

TL;DR: This paper addresses the problem of reward unfairness in Reinforcement Learning from Human Feedback (RLHF) that can misalign large language models (LLMs). It proposes a bias-agnostic method to ensure reward fairness by modeling preference learning as a resource allocation problem. Two methods, Fairness Regularization and Fairness Coefficient, are introduced and applied in verification and reinforcement learning scenarios, showing more fair alignment of LLMs with human preferences.


<details>
  <summary>Details</summary>
Motivation: Rewards in RLHF may contain various biases which can lead to misalignment of LLMs with human preferences.

Method: The paper models preference learning as a resource allocation problem, treating rewards as resources to be allocated fairly. Two methods are proposed: Fairness Regularization and Fairness Coefficient, to achieve fairness in rewards without specifically addressing each type of bias.

Result: Experiments in both verification and reinforcement learning scenarios show that the proposed methods effectively align LLMs with human preferences in a more fair manner.

Conclusion: The proposed bias-agnostic methods for ensuring reward fairness can successfully mitigate the adverse effects of biased rewards on LLMs, leading to better alignment with human preferences.

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [135] [Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control](https://arxiv.org/abs/2505.23355)
*Maxiu Xiao,Jianglin Lan,Jingxing Yu,Eldert van Henten,Congcong Sun*

Main category: cs.LG

TL;DR: 在温室气候控制中应用交互式强化学习（RL），尽管输入不完美，仍能提升RL代理的表现。政策塑造和控制共享算法分别使利润提高8.4%和6.8%，而奖励塑造算法由于对不完美输入敏感，导致利润下降9.4%。


<details>
  <summary>Details</summary>
Motivation: 气候控制对温室生产至关重要，因为它直接影响作物生长和资源利用。虽然强化学习（RL）在这个领域受到越来越多的关注，但仍然面临训练效率有限和对初始学习条件高度依赖的挑战。交互式RL通过结合人类（种植者）输入与RL代理的学习，提供了克服这些挑战的潜在解决方案。然而，交互式RL尚未应用于温室气候控制，并可能面临输入不完美的挑战。

Method: 1. 开发了三种代表性的交互式RL算法（奖励塑造、政策塑造和控制共享），专为温室气候控制定制。
2. 分析输入特征通常相互矛盾，权衡使种植者的输入难以完善。
3. 提出基于神经网络的方法以增强交互式RL代理在有限输入可用性下的鲁棒性。
4. 在模拟温室环境中对这三种交互式RL算法进行综合评估。

Result: 演示表明，将不完美的种植者输入纳入交互式RL有可能提高RL代理的表现。影响行动选择的RL算法（如政策塑造和控制共享）在处理不完美输入时表现更好，分别提高了8.4%和6.8%的利润。相反，操纵奖励函数的奖励塑造算法对不完美输入敏感，导致利润减少了9.4%。

Conclusion: 本研究探索了在温室气候控制中应用交互式RL的可能性和性能。结果强调了在结合不完美输入时选择适当机制的重要性。

Abstract: Climate control is crucial for greenhouse production as it directly affects
crop growth and resource use. Reinforcement learning (RL) has received
increasing attention in this field, but still faces challenges, including
limited training efficiency and high reliance on initial learning conditions.
Interactive RL, which combines human (grower) input with the RL agent's
learning, offers a potential solution to overcome these challenges. However,
interactive RL has not yet been applied to greenhouse climate control and may
face challenges related to imperfect inputs. Therefore, this paper aims to
explore the possibility and performance of applying interactive RL with
imperfect inputs into greenhouse climate control, by: (1) developing three
representative interactive RL algorithms tailored for greenhouse climate
control (reward shaping, policy shaping and control sharing); (2) analyzing how
input characteristics are often contradicting, and how the trade-offs between
them make grower's inputs difficult to perfect; (3) proposing a neural
network-based approach to enhance the robustness of interactive RL agents under
limited input availability; (4) conducting a comprehensive evaluation of the
three interactive RL algorithms with imperfect inputs in a simulated greenhouse
environment. The demonstration shows that interactive RL incorporating
imperfect grower inputs has the potential to improve the performance of the RL
agent. RL algorithms that influence action selection, such as policy shaping
and control sharing, perform better when dealing with imperfect inputs,
achieving 8.4% and 6.8% improvement in profit, respectively. In contrast,
reward shaping, an algorithm that manipulates the reward function, is sensitive
to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the
importance of selecting an appropriate mechanism when incorporating imperfect
inputs.

</details>


### [136] [Dynamic Spectral Backpropagation for Efficient Neural Network Training](https://arxiv.org/abs/2505.23369)
*Mannmohan Muthuraman*

Main category: cs.LG

TL;DR: Dynamic Spectral Backpropagation (DSBP) enhances neural network training by projecting gradients onto principal eigenvectors, reducing complexity and promoting flat minima. Five extensions are proposed to address challenges in robustness, fewshot learning, and hardware efficiency. Supported by a third order stochastic differential equation (SDE) and a PAC Bayes limit, DSBP outperforms SAM, LoRA, and MAML on CIFAR 10, Fashion MNIST, MedMNIST, and Tiny ImageNet.


<details>
  <summary>Details</summary>
Motivation: To improve neural network training under resource constraints by reducing complexity and promoting flat minima through gradient projection onto principal eigenvectors.

Method: Propose Dynamic Spectral Backpropagation (DSBP) with five extensions: dynamic spectral inference, spectral architecture optimization, spectral meta learning, spectral transfer regularization, and Lie algebra inspired dynamics.

Result: Outperforms Sharpness Aware Minimization (SAM), Low Rank Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion MNIST, MedMNIST, and Tiny ImageNet.

Conclusion: Future work focuses on scalability, bias mitigation, and ethical considerations.

Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training
under resource constraints by projecting gradients onto principal eigenvectors,
reducing complexity and promoting flat minima. Five extensions are proposed,
dynamic spectral inference, spectral architecture optimization, spectral meta
learning, spectral transfer regularization, and Lie algebra inspired dynamics,
to address challenges in robustness, fewshot learning, and hardware efficiency.
Supported by a third order stochastic differential equation (SDE) and a PAC
Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank
Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion
MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive
experiments and visualizations. Future work focuses on scalability, bias
mitigation, and ethical considerations.

</details>


### [137] [Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models](https://arxiv.org/abs/2505.23378)
*Roseline Polle,Agnes Norbury,Alexandra Livia Georgescu,Nicholas Cummins,Stefano Goria*

Main category: cs.LG

TL;DR: The paper explores three meta-learning approaches for speaker-dependent modeling in speech-based health monitoring, finding that transformer-based methods perform best.


<details>
  <summary>Details</summary>
Motivation: To improve performance in speech-based health monitoring applications by using speaker-dependent modeling without the computational expense of mixed-effect models.

Method: Reformulated speaker adaptation as a meta-learning problem and explored ensemble-based distance models, prototypical networks, and transformer-based sequence models using pre-trained speech embeddings.

Result: All meta-learning approaches outperformed cross-sectional and conventional mixed-effects models; transformer-based method showed the strongest performance.

Conclusion: Transformer-based meta-learning is effective for speaker-dependent modeling in predicting time since sleep from speech.

Abstract: Speaker-dependent modelling can substantially improve performance in
speech-based health monitoring applications. While mixed-effect models are
commonly used for such speaker adaptation, they require computationally
expensive retraining for each new observation, making them impractical in a
production environment. We reformulate this task as a meta-learning problem and
explore three approaches of increasing complexity: ensemble-based distance
models, prototypical networks, and transformer-based sequence models. Using
pre-trained speech embeddings, we evaluate these methods on a large
longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting
time since sleep from speech as a function of fatigue, a symptom commonly
associated with ill-health. Our results demonstrate that all meta-learning
approaches tested outperformed both cross-sectional and conventional
mixed-effects models, with a transformer-based method achieving the strongest
performance.

</details>


### [138] [Automated Modeling Method for Pathloss Model Discovery](https://arxiv.org/abs/2505.23383)
*Ahmad Anaqreh,Shih-Kai Chou,Mihael Mohorčič,Carolina Fortuna*

Main category: cs.LG

TL;DR: This paper proposes a novel AI-based approach that accelerates the discovery of path loss models while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional modeling methods for wireless propagation have relied on statistic-based techniques, but with the expansion of wireless communication systems, there is a growing demand for more accurate and interoperable methods. AI-based techniques are increasingly adopted, but often lack interpretability.

Method: The proposed method automates model formulation, evaluation, and refinement using two AI techniques: Deep Symbolic Regression (offering full interpretability) and Kolmogorov-Arnold Networks (providing two levels of interpretability).

Result: Kolmogorov-Arnold Networks achieve R^2 values close to 1 with minimal prediction error. Deep Symbolic Regression generates compact models with moderate accuracy. Automated methods outperform traditional ones by reducing prediction errors by up to 75%.

Conclusion: The novel AI-based approach offers accurate and explainable solutions for discovering next-generation path loss models, with potential to increase efficiency.

Abstract: Modeling propagation is the cornerstone for designing and optimizing
next-generation wireless systems, with a particular emphasis on 5G and beyond
era. Traditional modeling methods have long relied on statistic-based
techniques to characterize propagation behavior across different environments.
With the expansion of wireless communication systems, there is a growing demand
for methods that guarantee the accuracy and interoperability of modeling.
Artificial intelligence (AI)-based techniques, in particular, are increasingly
being adopted to overcome this challenge, although the interpretability is not
assured with most of these methods. Inspired by recent advancements in AI, this
paper proposes a novel approach that accelerates the discovery of path loss
models while maintaining interpretability. The proposed method automates the
model formulation, evaluation, and refinement, facilitating model discovery. We
evaluate two techniques: one based on Deep Symbolic Regression, offering full
interpretability, and the second based on Kolmogorov-Arnold Networks, providing
two levels of interpretability. Both approaches are evaluated on two synthetic
and two real-world datasets. Our results show that Kolmogorov-Arnold Networks
achieve R^2 values close to 1 with minimal prediction error, while Deep
Symbolic Regression generates compact models with moderate accuracy. Moreover,
on the selected examples, we demonstrate that automated methods outperform
traditional methods, achieving up to 75% reduction in prediction errors,
offering accurate and explainable solutions with potential to increase the
efficiency of discovering next-generation path loss models.

</details>


### [139] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/abs/2505.23412)
*Srishti Gupta,Daniele Angioni,Maura Pintor,Ambra Demontis,Lea Schönherr,Battista Biggio,Fabio Roli*

Main category: cs.LG

TL;DR: 在类增量学习（CIL）中，模型需要学习新类别而不会遗忘旧类别，并处理来自未知类别的输入。近期工作通过多头模型和OOD检测解决这些问题，但依赖于联合训练和记忆缓冲区。本文研究了后验OOD检测方法以消除对记忆缓冲区的需求，并发现这种方法在推理时可有效替代基于缓冲区的OOD检测，性能相当甚至更优。实验结果支持这一发现，为高效且保护隐私的CIL系统设计提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法虽然有效，但主要依赖于包含过去数据的记忆缓冲区，这引发了隐私、可扩展性和训练时间增加等问题。因此，需要探索一种不依赖记忆缓冲区的方法来解决这些问题。

Method: 本文深入分析了后验OOD检测方法，并研究了它们在推理时的应用是否可以作为基于缓冲区的OOD检测的有效替代方案。通过适当的OOD检测方法应用，实现了无需记忆缓冲区的类增量学习。

Result: 实验结果表明，这种无缓冲区的方法在类增量学习和拒绝未知样本方面均能达到与基于缓冲区的方法相当或更优的性能。

Conclusion: 后验OOD检测方法可以在推理时有效替代基于缓冲区的OOD检测，从而为设计高效且保护隐私的开放世界环境下的CIL系统提供了新的思路。

Abstract: Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [140] [Bidirectional predictive coding](https://arxiv.org/abs/2505.23415)
*Gaspard Oliviers,Mufeng Tang,Rafal Bogacz*

Main category: cs.LG

TL;DR: The paper proposes bidirectional Predictive Coding (bPC) which integrates generative and discriminative inference for improved performance in visual tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of unidirectional Predictive Coding models that only perform well in either generative or discriminative tasks, but not both.

Method: Developed bidirectional Predictive Coding (bPC) model incorporating both generative and discriminative inference with a biologically plausible circuit implementation.

Result: bPC matches or outperforms unidirectional models in specialised tasks and shows superior performance in multimodal learning and inference with missing information.

Conclusion: bPC more closely resembles biological visual inference compared to unidirectional models.

Abstract: Predictive coding (PC) is an influential computational model of visual
learning and inference in the brain. Classical PC was proposed as a top-down
generative model, where the brain actively predicts upcoming visual inputs, and
inference minimises the prediction errors. Recent studies have also shown that
PC can be formulated as a discriminative model, where sensory inputs predict
neural activities in a feedforward manner. However, experimental evidence
suggests that the brain employs both generative and discriminative inference,
while unidirectional PC models show degraded performance in tasks requiring
bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC
model that incorporates both generative and discriminative inference while
maintaining a biologically plausible circuit implementation. We show that bPC
matches or outperforms unidirectional models in their specialised generative or
discriminative tasks, by developing an energy landscape that simultaneously
suits both tasks. We also demonstrate bPC's superior performance in two
biologically relevant tasks including multimodal learning and inference with
missing information, suggesting that bPC resembles biological visual inference
more closely.

</details>


### [141] [OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses](https://arxiv.org/abs/2505.23421)
*Zheming Zhang,Yan Jiang,Qingshan Li,Ai Han*

Main category: cs.LG

TL;DR: In China's competitive fresh e-commerce market, the OTPTO method significantly enhances the full order fulfillment rate and narrows the gap to the optimal rate compared to traditional PTO methods.


<details>
  <summary>Details</summary>
Motivation: Traditional predict-then-optimize (PTO) methods for inventory management in front-end warehouses fail to align prediction with inventory goals and prioritize consumer satisfaction.

Method: The paper proposes a multi-task Optimize-then-Predict-then-Optimize (OTPTO) approach that uses a 0-1 mixed integer programming model OM1 to determine historically optimal inventory levels, product selection model PM1, stocking model PM2 for prediction, and a post-processing algorithm OM2 to refine results.

Result: Experimental results from JD.com's 7Fresh platform show that the OTPTO method enhances the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and narrows the gap to the optimal full order fulfillment rate by 5.27%.

Conclusion: The OTPTO method is effective in managing inventory at front-end warehouses of fresh e-commerce platforms and provides valuable insights for future research.

Abstract: In China's competitive fresh e-commerce market, optimizing operational
strategies, especially inventory management in front-end warehouses, is key to
enhance customer satisfaction and to gain a competitive edge. Front-end
warehouses are placed in residential areas to ensure the timely delivery of
fresh goods and are usually in small size. This brings the challenge of
deciding which goods to stock and in what quantities, taking into account
capacity constraints. To address this issue, traditional predict-then-optimize
(PTO) methods that predict sales and then decide on inventory often don't align
prediction with inventory goals, as well as fail to prioritize consumer
satisfaction. This paper proposes a multi-task
Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes
product selection and inventory management, aiming to increase consumer
satisfaction by maximizing the full order fulfillment rate. Our method employs
a 0-1 mixed integer programming model OM1 to determine historically optimal
inventory levels, and then uses a product selection model PM1 and the stocking
model PM2 for prediction. The combined results are further refined through a
post-processing algorithm OM2. Experimental results from JD.com's 7Fresh
platform demonstrate the robustness and significant advantages of our OTPTO
method. Compared to the PTO approach, our OTPTO method substantially enhances
the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and
narrows the gap to the optimal full order fulfillment rate by 5.27%. These
findings substantiate the efficacy of the OTPTO method in managing inventory at
front-end warehouses of fresh e-commerce platforms and provide valuable
insights for future research in this domain.

</details>


### [142] [Enhanced DACER Algorithm with High Diffusion Efficiency](https://arxiv.org/abs/2505.23426)
*Yinuo Wang,Mining Tan,Wenjun Zou,Haotian Lin,Xujie Song,Wenxuan Wang,Tong Liu,Likun Wang,Guojian Zhan,Tianze Zhu,Shiqi Liu,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: To improve training and inference efficiency of DACER while maintaining or enhancing performance, the paper proposes a Q-gradient field objective as an auxiliary optimization target and introduces a temporal weighting mechanism. Experimental results show that DACER2 achieves state-of-the-art performance in most MuJoCo control tasks with only five diffusion steps.


<details>
  <summary>Details</summary>
Motivation: The lack of inference efficiency in DACER becomes a significant bottleneck for applying diffusion policies in real-time online RL settings. Reducing diffusion steps leads to noticeable performance degradation.

Method: Propose a Q-gradient field objective as an auxiliary optimization target to guide the denoising process at each diffusion step and introduce a temporal weighting mechanism.

Result: DACER2 algorithm achieves state-of-the-art performance in most MuJoCo control tasks with only five diffusion steps, exhibiting stronger multimodality compared to DACER.

Conclusion: The proposed method improves training and inference efficiency while maintaining or even enhancing performance.

Abstract: Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.

</details>


### [143] [On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers](https://arxiv.org/abs/2505.23427)
*Monika Gahalawat,Maneesh Bilalpur,Raul Fernandez Rojas,Jeffrey F. Cohn,Roland Goecke,Ramanathan Subramanian*

Main category: cs.LG

TL;DR: The paper explores the use of head motion patterns, termed kinemes, as biomarkers for estimating depression severity across multiple datasets from different cultures. It demonstrates their effectiveness and generalisability compared to other methods.


<details>
  <summary>Details</summary>
Motivation: To address the underutilisation of head motion in automated depression assessment and improve model generalisability beyond single-dataset validation.

Method: Utilised kinemes (elementary head motion units) for depression severity estimation. Evaluated through k-fold cross-validation over individual/multiple datasets and model reuse on other datasets. Performance assessed with classification and regression tasks using classical machine learning methods.

Result: Head motion patterns are efficient biomarkers for depression severity estimation, showing competitive performance across datasets. Kineme-based features outperform raw head motion descriptors in binary classification and other visual cues in regression tasks.

Conclusion: Kinemes derived from head motion are effective and generalisable biomarkers for automated depression severity estimation.

Abstract: Depression is a debilitating mood disorder negatively impacting millions
worldwide. While researchers have explored multiple verbal and non-verbal
behavioural cues for automated depression assessment, head motion has received
little attention thus far. Further, the common practice of validating machine
learning models via a single dataset can limit model generalisability. This
work examines the effectiveness and generalisability of models utilising
elementary head motion units, termed kinemes, for depression severity
estimation. Specifically, we consider three depression datasets from different
western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt
datasets) with varied contextual and recording settings to investigate the
generalisability of the derived kineme patterns via two methods: (i) k-fold
cross-validation over individual/multiple datasets, and (ii) model reuse on
other datasets. Evaluating classification and regression performance with
classical machine learning methods, our results show that: (1) head motion
patterns are efficient biomarkers for estimating depression severity, achieving
highly competitive performance for both classification and regression tasks on
a variety of datasets, including achieving the second best Mean Absolute Error
(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more
generalisable than (a) raw head motion descriptors for binary severity
classification, and (b) other visual behavioural cues for severity estimation
(regression).

</details>


### [144] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: This paper investigates the role of diversity in reinforcement learning (RL) for enhancing large language models (LLMs) reasoning and proposes a new diversity-aware policy optimization method.


<details>
  <summary>Details</summary>
Motivation: Despite the importance of diversity in RL, its influence on LLM reasoning has not been thoroughly examined.

Method: The authors propose a novel diversity-aware policy optimization method that promotes solution diversity during RL training by designing a token-level diversity and reformulating it into a practical objective, then selectively applying it to positive samples within the R1-zero training framework.

Result: The method leads to a 3.5 percent average improvement across four mathematical reasoning benchmarks and generates more diverse and robust solutions.

Conclusion: Promoting diversity during RL training can enhance LLM reasoning capabilities.

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [145] [Bounded-Abstention Pairwise Learning to Rank](https://arxiv.org/abs/2505.23437)
*Antonio Ferrara,Andrea Pugnana,Francesco Bonchi,Salvatore Ruggieri*

Main category: cs.LG

TL;DR: Ranking systems are crucial in high-stakes domains, where safety mechanisms like abstention can defer uncertain decisions to human experts. This paper introduces a novel method for abstention in pairwise learning-to-rank tasks, including a theoretical characterization of the optimal strategy, a model-agnostic algorithm, and empirical evaluations.


<details>
  <summary>Details</summary>
Motivation: Abstention as a safety mechanism in ranking systems is underexplored beyond classification tasks, yet it is essential for deferring uncertain or low-confidence decisions to human experts in high-stakes domains.

Method: The approach involves thresholding the ranker's conditional risk, abstaining from making a decision when the estimated risk exceeds a predefined threshold. It includes a theoretical characterization of the optimal abstention strategy and a model-agnostic, plug-in algorithm for constructing abstaining ranking models.

Result: Empirical evaluations across multiple datasets demonstrate the effectiveness of the proposed abstention method in pairwise learning-to-rank tasks.

Conclusion: This work provides a novel method for abstention in pairwise learning-to-rank tasks, contributing both theoretically and practically to the integration of safety mechanisms in ranking systems.

Abstract: Ranking systems influence decision-making in high-stakes domains like health,
education, and employment, where they can have substantial economic and social
impacts. This makes the integration of safety mechanisms essential. One such
mechanism is $\textit{abstention}$, which enables algorithmic decision-making
system to defer uncertain or low-confidence decisions to human experts. While
abstention have been predominantly explored in the context of classification
tasks, its application to other machine learning paradigms remains
underexplored. In this paper, we introduce a novel method for abstention in
pairwise learning-to-rank tasks. Our approach is based on thresholding the
ranker's conditional risk: the system abstains from making a decision when the
estimated risk exceeds a predefined threshold. Our contributions are threefold:
a theoretical characterization of the optimal abstention strategy, a
model-agnostic, plug-in algorithm for constructing abstaining ranking models,
and a comprehensive empirical evaluations across multiple datasets,
demonstrating the effectiveness of our approach.

</details>


### [146] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Haoran Duan,Zhengwei Tao,Xuan Zhang,Jiandong Li*

Main category: cs.LG

TL;DR: The paper rethinks the application of regularization methods in knowledge graph completion (KGC) and introduces a novel sparse-regularization method called SPR.


<details>
  <summary>Details</summary>
Motivation: Most previous efforts on KGC have neglected to take advantage of regularization from a deeper perspective and therefore have not been used to their full potential.

Method: The authors conducted extensive empirical studies on various KGC models and introduced a novel sparse-regularization method named SPR, which embeds the concept of rank-based selective sparsity into the KGC regularizer.

Result: Various comparative experiments on multiple datasets and multiple models show that the SPR regularization method is better than other regularization methods and can enable the KGC model to further break through the performance margin.

Conclusion: Carefully designed regularization not only alleviates overfitting and reduces variance but also enables these models to break through the upper bounds of their original performance.

Abstract: Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [147] [Strategic Classification with Non-Linear Classifiers](https://arxiv.org/abs/2505.23443)
*Benyamin Trachtenberg,Nir Rosenfeld*

Main category: cs.LG

TL;DR: The paper explores how strategic behavior manifests under non-linear classifiers and its implications for learning.


<details>
  <summary>Details</summary>
Motivation: Strategic classification has been mostly dedicated to linear classifiers, but there is a need to understand how strategic behavior affects non-linear classifiers.

Method: A bottom-up approach is taken to show how non-linearity affects decision boundary points, classifier expressivity, and model classes complexity. The study also examines the performance of universal approximators in a strategic environment.

Result: Key finding is that universal approximators are no longer universal once the environment is strategic, which can create performance gaps even on an unrestricted model class.

Conclusion: This work expands the understanding of strategic classification by exploring non-linear classifiers and highlights the limitations of universal approximators in strategic environments.

Abstract: In strategic classification, the standard supervised learning setting is
extended to support the notion of strategic user behavior in the form of costly
feature manipulations made in response to a classifier. While standard learning
supports a broad range of model classes, the study of strategic classification
has, so far, been dedicated mostly to linear classifiers. This work aims to
expand the horizon by exploring how strategic behavior manifests under
non-linear classifiers and what this implies for learning. We take a bottom-up
approach showing how non-linearity affects decision boundary points, classifier
expressivity, and model classes complexity. A key finding is that universal
approximators (e.g., neural nets) are no longer universal once the environment
is strategic. We demonstrate empirically how this can create performance gaps
even on an unrestricted model class.

</details>


### [148] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/abs/2505.23448)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Main category: cs.LG

TL;DR: 提出了一种结合网络反演和分类器训练的新框架，通过扩展分类器为(n+1)类模型来同时解决OOD检测和不确定性估计问题。该方法无需外部OOD数据集或事后校准技术，具有可扩展性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，构建安全的机器学习系统需要有效的OOD检测和不确定性估计方法，以应对不可避免的意外输入。现有的方法可能需要外部OOD数据集或复杂的校准技术，而本研究旨在提供一种统一且高效的解决方案。

Method: 通过引入一个'垃圾'类别，将标准n类分类任务扩展为(n+1)类模型，并用随机高斯噪声初始化该类别以表示异常值输入。在每次训练周期后，使用网络反演重建所有输出类别的输入图像，初始表现为噪声和不连贯的样本被归入垃圾类别进行重新训练。此过程迭代进行，直到反演样本更接近分布内数据，从而优化分类器决策边界并清理类流形。

Result: 该方法能够在推理阶段有效检测并拒绝OOD样本，并通过预测置信度评分估计分布内和OOD输入的不确定性。实验结果表明，该方法在多个基准数据集上表现优异，具有良好的可扩展性和可解释性。

Conclusion: 所提出的框架提供了一种无需外部OOD数据集或事后校准技术的统一解决方案，适用于同时处理OOD检测和不确定性估计问题，为构建更安全的机器学习系统提供了新思路。

Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [149] [Diffusion Guidance Is a Controllable Policy Improvement Operator](https://arxiv.org/abs/2505.23458)
*Kevin Frans,Seohong Park,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: This paper introduces CFGRL, a framework that links policy improvement with diffusion model guidance for reinforcement learning (RL). It simplifies training through supervised learning methods and eliminates the need for explicit value function learning. Increased performance is observed with higher guidance weighting in offline RL tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address the challenges in scaling reinforcement learning systems while leveraging the scalability and simplicity of generative modeling techniques. By combining these strengths, the authors aim to create a more efficient and effective approach to policy improvement in RL.

Method: The method involves deriving a direct relation between policy improvement and the guidance of diffusion models. This leads to the development of the CFGRL framework, which is trained using supervised learning techniques and can enhance policies without explicitly learning a value function.

Result: On offline RL tasks, the study shows a consistent trend where increased guidance weighting results in better performance. The framework successfully generalizes simple supervised methods like goal-conditioned behavioral cloning, prioritizing optimality and improving performance across various scenarios.

Conclusion: CFGRL offers a novel way to improve policies in reinforcement learning by integrating diffusion model guidance. Its ability to operate without learning an explicit value function allows for the enhancement of existing supervised methods, leading to performance improvements 'for free'.

Abstract: At the core of reinforcement learning is the idea of learning beyond the
performance in the data. However, scaling such systems has proven notoriously
tricky. In contrast, techniques from generative modeling have proven remarkably
scalable and are simple to train. In this work, we combine these strengths, by
deriving a direct relation between policy improvement and guidance of diffusion
models. The resulting framework, CFGRL, is trained with the simplicity of
supervised learning, yet can further improve on the policies in the data. On
offline RL tasks, we observe a reliable trend -- increased guidance weighting
leads to increased performance. Of particular importance, CFGRL can operate
without explicitly learning a value function, allowing us to generalize simple
supervised methods (e.g., goal-conditioned behavioral cloning) to further
prioritize optimality, gaining performance for "free" across the board.

</details>


### [150] [On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment](https://arxiv.org/abs/2505.23459)
*Safwan Labbi,Paul Mangold,Daniil Tiapkin,Eric Moulines*

Main category: cs.LG

TL;DR: The paper addresses the challenge of ensuring convergence in policy gradient methods for federated reinforcement learning (FRL) under environmental heterogeneity. It establishes that heterogeneity may require optimal policies to be non-deterministic or time-varying, even in tabular environments. The authors prove global convergence results for the federated policy gradient (FedPG) algorithm with local updates under a specific condition, demonstrating linear speed-up concerning the number of agents. They introduce b-RS-FedPG, a novel method that achieves explicit convergence rates towards near-optimal stationary policies and empirically outperforms federated Q-learning in heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Ensuring convergence of policy gradient methods in FRL under environment heterogeneity is challenging. Heterogeneity can necessitate non-deterministic or time-varying optimal policies, even in tabular environments.

Method: The authors establish theoretical results on the necessity of non-deterministic policies in heterogeneous environments, prove global convergence of FedPG algorithms under a Lojasiewicz condition, and introduce b-RS-FedPG, which uses a softmax-inspired parameterization and appropriate regularization.

Result: Theoretical analysis shows FedPG attains linear speed-up with respect to the number of agents. b-RS-FedPG demonstrates explicit convergence rates towards near-optimal stationary policies. Empirical results indicate both FedPG and b-RS-FedPG outperform federated Q-learning in heterogeneous settings.

Conclusion: The study provides theoretical insights into the role of heterogeneity in FRL and introduces b-RS-FedPG as an effective policy gradient method that ensures convergence and outperforms existing techniques.

Abstract: Ensuring convergence of policy gradient methods in federated reinforcement
learning (FRL) under environment heterogeneity remains a major challenge. In
this work, we first establish that heterogeneity, perhaps counter-intuitively,
can necessitate optimal policies to be non-deterministic or even time-varying,
even in tabular environments. Subsequently, we prove global convergence results
for federated policy gradient (FedPG) algorithms employing local updates, under
a {\L}ojasiewicz condition that holds only for each individual agent, in both
entropy-regularized and non-regularized scenarios. Crucially, our theoretical
analysis shows that FedPG attains linear speed-up with respect to the number of
agents, a property central to efficient federated learning. Leveraging insights
from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient
method that employs a carefully constructed softmax-inspired parameterization
coupled with an appropriate regularization scheme. We further demonstrate
explicit convergence rates for b-RS-FedPG toward near-optimal stationary
policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG
consistently outperform federated Q-learning on heterogeneous settings.

</details>


### [151] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/abs/2505.23470)
*Chenjie Li,Amir Gilad,Boris Glavic,Zhengjie Miao,Sudeepa Roy*

Main category: cs.LG

TL;DR: This paper presents a method for improving the quality of labeling functions (LFs) in programmatic weak supervision by repairing them using a small set of labeled examples, leading to more accurate generated labels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the quality of LFs which directly affects the accuracy of generated labels in programmatic weak supervision. This can be achieved by fixing LFs based on a small set of labeled examples.

Method: Develop novel techniques for repairing LFs by minimally changing their results on the labeled examples, ensuring sufficient evidence for correct labels and high accuracy of repaired LFs. Model LFs as conditional rules to selectively change their output for some inputs.

Result: Demonstrated experimentally that the system improves the quality of LFs even with surprisingly small sets of labeled datapoints.

Conclusion: Repairing labeling functions using minimal changes based on a small set of labeled examples can significantly improve the quality of LFs and thus the accuracy of generated labels.

Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [152] [SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training](https://arxiv.org/abs/2505.23489)
*Ildus Sadrtdinov,Ivan Klimov,Ekaterina Lobacheva,Dmitry Vetrov*

Main category: cs.LG

TL;DR: The paper explores the thermodynamic interpretation of SGD in neural network training, showing that it minimizes free energy and how different learning rates affect model stabilization.


<details>
  <summary>Details</summary>
Motivation: To understand why high learning rates prevent convergence to loss minima and how different learning rates lead to stabilization at different loss levels.

Method: Empirical validation of the free energy framework on both underparameterized (UP) and overparameterized (OP) models, analyzing the role of temperature determined by learning rate.

Result: UP models follow free energy minimization with temperature increasing monotonically with learning rate, while for OP models, temperature drops to zero at low learning rates causing direct loss minimization.

Conclusion: The mismatch between UP and OP models is attributed to differences in the signal-to-noise ratio of stochastic gradients near optima.

Abstract: We present a thermodynamic interpretation of the stationary behavior of
stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural
network training. We show that SGD implicitly minimizes a free energy function
$F=U-TS$, balancing training loss $U$ and the entropy of the weights
distribution $S$, with temperature $T$ determined by the LR. This perspective
offers a new lens on why high LRs prevent training from converging to the loss
minima and how different LRs lead to stabilization at different loss levels. We
empirically validate the free energy framework on both underparameterized (UP)
and overparameterized (OP) models. UP models consistently follow free energy
minimization, with temperature increasing monotonically with LR, while for OP
models, the temperature effectively drops to zero at low LRs, causing SGD to
minimize the loss directly and converge to an optimum. We attribute this
mismatch to differences in the signal-to-noise ratio of stochastic gradients
near optima, supported by both a toy example and neural network experiments.

</details>


### [153] [Epistemic Errors of Imperfect Multitask Learners When Distributions Shift](https://arxiv.org/abs/2505.23496)
*Sabina J. Sloman,Michele Caprio,Samuel Kaski*

Main category: cs.LG

TL;DR: 在数据存在噪声的情况下，统计学习者的目标是解决关于测试时遇到的数据的epistemic uncertainty（认识不确定性），即识别测试（目标）数据的分布。本文引入了认识错误的定义和通用的认识错误界限，并针对贝叶斯迁移学习和分布偏移提供了专门的认识错误界限及相应的泛化界限，同时提出了负迁移的新定义并在实验中验证了其见解。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的学习场景引入了仅凭训练数据无法解决的认识不确定性，如多任务学习、目标数据与源数据系统性差异以及不完美的学习等。因此需要一种方法来分解和量化这些不确定性来源的认识错误。

Method: 引入认识错误的定义和通用的认识错误界限，将错误分别归因于学习过程和环境的多个方面。此外，还提供了针对贝叶斯迁移学习和分布偏移的认识错误界限以及相应的泛化界限。最后提出了负迁移的新定义。

Result: 成功推导出认识错误界限并将其应用于贝叶斯迁移学习和分布偏移场景，同时通过合成实验验证了负迁移定义的有效性。

Conclusion: 本文提出的认识错误界限可以更好地理解各种认识不确定性的来源及其对学习效果的影响，而新定义的负迁移则有助于更准确地评估迁移学习的效果。

Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic
uncertainty about the data it will encounter at test-time, i.e., to identify
the distribution of test (target) data. Many real-world learning settings
introduce sources of epistemic uncertainty that can not be resolved on the
basis of training (source) data alone: The source data may arise from multiple
tasks (multitask learning), the target data may differ systematically from the
source data tasks (distribution shift), and/or the learner may not arrive at an
accurate characterization of the source data (imperfect learning). We introduce
a principled definition of epistemic error, and provide a generic,
decompositional epistemic error bound. Our error bound is the first to (i)
consider epistemic error specifically, (ii) accommodate all the sources of
epistemic uncertainty above, and (iii) separately attribute the error to each
of multiple aspects of the learning procedure and environment. As corollaries
of the generic result, we provide (i) epistemic error bounds specialized to the
settings of Bayesian transfer learning and distribution shift within
$\epsilon$-neighborhoods, and (ii) a set of corresponding generalization
bounds. Finally, we provide a novel definition of negative transfer, and
validate its insights in a synthetic experimental setting.

</details>


### [154] [Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty](https://arxiv.org/abs/2505.23506)
*Sebastián Jiménez,Mira Jürgens,Willem Waegeman*

Main category: cs.LG

TL;DR: In this paper, authors argue that existing supervised learning methods which disentangle aleatoric and epistemic uncertainty fail to capture critical components of epistemic uncertainty due to model bias. They use a fine-grained taxonomy and simulation-based evaluation protocol to illustrate that current methods rarely capture the full spectrum of epistemic uncertainty.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight the limitations in current methods that separate aleatoric and epistemic uncertainties in machine learning models, especially regarding the neglected aspect of model bias.

Method: Authors use a more fine-grained taxonomy of epistemic uncertainty sources and decompose the classical bias-variance into different parts reflecting these uncertainties. They also employ a simulation-based evaluation protocol encompassing both procedural- and data-driven uncertainty components.

Result: Findings show that high model bias can lead to misleadingly low estimates of epistemic uncertainty and common second-order uncertainty quantification methods underrepresent it by blurring bias-induced errors into aleatoric estimates.

Conclusion: Meaningful aleatoric estimates are only feasible if all relevant sources of epistemic uncertainty are properly represented.

Abstract: In recent years various supervised learning methods that disentangle
aleatoric and epistemic uncertainty based on second-order distributions have
been proposed. We argue that these methods fail to capture critical components
of epistemic uncertainty, particularly due to the often-neglected component of
model bias. To show this, we make use of a more fine-grained taxonomy of
epistemic uncertainty sources in machine learning models, and analyse how the
classical bias-variance decomposition of the expected prediction error can be
decomposed into different parts reflecting these uncertainties. By using a
simulation-based evaluation protocol which encompasses epistemic uncertainty
due to both procedural- and data-driven uncertainty components, we illustrate
that current methods rarely capture the full spectrum of epistemic uncertainty.
Through theoretical insights and synthetic experiments, we show that high model
bias can lead to misleadingly low estimates of epistemic uncertainty, and
common second-order uncertainty quantification methods systematically blur
bias-induced errors into aleatoric estimates, thereby underrepresenting
epistemic uncertainty. Our findings underscore that meaningful aleatoric
estimates are feasible only if all relevant sources of epistemic uncertainty
are properly represented.

</details>


### [155] [AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity](https://arxiv.org/abs/2505.23520)
*Yu Zhang,Dong Guo,Fang Wu,Guoliang Zhu,Dian Ding,Yiming Zhang*

Main category: cs.LG

TL;DR: 在大规模语言模型中，由于自注意力机制的二次复杂度，预填充阶段面临重大计算挑战。现有方法因局部信息限制和块状稀疏性导致效率和准确性不足。本文提出AnchorAttention机制，通过模式锚点计算、差异感知条纹稀疏性识别和细粒度稀疏计算，在保持高召回率的同时实现1.44倍加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长上下文时，由于自注意力机制的计算复杂度较高，面临显著的计算挑战。现有的动态模式匹配和块状稀疏低级实现方法无法有效捕获全局上下文，并且存在持续的内部稀疏性问题，影响了准确性和效率。

Method: 本文提出了AnchorAttention，一种差异感知的动态稀疏注意力机制。它包含三个关键组件：(1)基于模式的锚点计算，利用输入中的共性快速计算接近最大值的分数作为锚点；(2)差异感知条纹稀疏性识别，与锚点进行比较以获得重要区域的离散坐标；(3)细粒度稀疏计算，用同时离散KV位置加载代替传统的连续KV块加载，最大化稀疏性并保留硬件计算潜力。

Result: AnchorAttention通过更精细的稀疏策略，在相同召回水平下实现了更高的稀疏性，大幅减少了计算时间。在文本长度为128k的情况下，相比最先进的方法，速度提高了1.44倍，同时保持更高的召回率。

Conclusion: AnchorAttention机制通过差异感知和动态稀疏注意力，在长上下文处理中显著提升了计算效率和准确性，为大规模语言模型提供了一种有效的解决方案。

Abstract: Large Language Models (LLMs) with extended context lengths face significant
computational challenges during the pre-filling phase, primarily due to the
quadratic complexity of self-attention. Existing methods typically employ
dynamic pattern matching and block-sparse low-level implementations. However,
their reliance on local information for pattern identification fails to capture
global contexts, and the coarse granularity of blocks leads to persistent
internal sparsity, resulting in suboptimal accuracy and efficiency. To address
these limitations, we propose \textbf{AnchorAttention}, a difference-aware,
dynamic sparse attention mechanism that efficiently identifies critical
attention regions at a finer stripe granularity while adapting to global
contextual information, achieving superior speed and accuracy. AnchorAttention
comprises three key components: (1) \textbf{Pattern-based Anchor Computation},
leveraging the commonalities present across all inputs to rapidly compute a set
of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe
Sparsity Identification}, performing difference-aware comparisons with the
anchor to quickly obtain discrete coordinates of significant regions in a
stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation},
replacing the traditional contiguous KV block loading approach with
simultaneous discrete KV position loading to maximize sparsity rates while
preserving full hardware computational potential. With its finer-grained
sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at
the same recall level, significantly reducing computation time. Compared to
previous state-of-the-art methods, at a text length of 128k, it achieves a
speedup of 1.44$\times$ while maintaining higher recall rates.

</details>


### [156] [Accelerating AllReduce with a Persistent Straggler](https://arxiv.org/abs/2505.23523)
*Arjun Devraj,Eric Ding,Abhishek Vijaya Kumar,Robert Kleinberg,Rachee Singh*

Main category: cs.LG

TL;DR: StragglAR is an AllReduce algorithm that accelerates distributed training and inference by implementing a ReduceScatter among remaining GPUs during straggler-induced delay, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: Bulk-synchronous AllReduce algorithms can be delayed by persistent stragglers, which affects the efficiency of distributed machine learning workloads.

Method: StragglAR implements a ReduceScatter among the remaining GPUs during the delay caused by stragglers and executes a novel collective algorithm to complete the AllReduce once the straggler reaches the synchronization barrier.

Result: StragglAR achieves a 2x theoretical speedup over popular AllReduce algorithms for large GPU clusters with persistent stragglers and a 22% speedup on an 8-GPU server.

Conclusion: StragglAR effectively addresses the challenge of persistent stragglers in distributed machine learning, improving the performance of AllReduce operations.

Abstract: Distributed machine learning workloads use data and tensor parallelism for
training and inference, both of which rely on the AllReduce collective to
synchronize gradients or activations. However, bulk-synchronous AllReduce
algorithms can be delayed by a persistent straggler that is slower to reach the
synchronization barrier required to begin the collective. To address this
challenge, we propose StragglAR: an AllReduce algorithm that accelerates
distributed training and inference in the presence of persistent stragglers.
StragglAR implements a ReduceScatter among the remaining GPUs during the
straggler-induced delay, and then executes a novel collective algorithm to
complete the AllReduce once the straggler reaches the synchronization barrier.
StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient
AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent
stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%
speedup over state-of-the-art AllReduce algorithms.

</details>


### [157] [Normalizing Flows are Capable Models for RL](https://arxiv.org/abs/2505.23527)
*Raj Ghugare,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: Normalizing flows can be effectively integrated into RL algorithms, providing simpler algorithms and achieving higher performance in various RL settings.


<details>
  <summary>Details</summary>
Motivation: Despite the computational challenges of modern RL algorithms incorporating powerful probabilistic models, there is a need to explore alternatives that offer simplicity and efficiency without compromising expressivity.

Method: The paper proposes a single normalizing flow (NF) architecture that can be seamlessly integrated into RL algorithms, functioning as a policy, Q-function, and occupancy measure. This approach avoids the complexities associated with solving differential equations or using autoregressive architectures.

Result: The proposed method results in much simpler RL algorithms and achieves higher performance in imitation learning, offline RL, goal-conditioned RL, and unsupervised RL.

Conclusion: Normalizing flows have significant potential in reinforcement learning, offering an efficient alternative to other probabilistic models without sacrificing expressivity.

Abstract: Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [158] [Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images](https://arxiv.org/abs/2505.23528)
*Maria Eleftheria Vlontzou,Maria Athanasiou,Christos Davatzikos,Konstantina S. Nikita*

Main category: cs.LG

TL;DR: The study conducts a comprehensive fairness analysis of ML models diagnosing MCI and AD from MRI data, examining biases related to age, race, and gender. It evaluates fairness metrics and mitigation strategies, revealing significant biases in age and race but not gender.


<details>
  <summary>Details</summary>
Motivation: To investigate the presence of biases in machine learning models used for diagnosing Mild Cognitive Impairment (MCI) and Alzheimer's disease (AD) based on MRI-derived neuroimaging features, particularly focusing on biases associated with age, race, and gender.

Method: 1. Conducted a comprehensive fairness analysis using multi-cohort datasets.
2. Investigated biases linked to age, race, and gender as well as proxy features encoding these attributes.
3. Assessed the reliability of various fairness definitions and metrics.
4. Performed comparative analysis of bias mitigation strategies including pre-processing, in-processing, and post-processing techniques.
5. Introduced a novel composite measure considering F1-score and equalized odds ratio to quantify trade-offs between fairness and performance.

Result: Significant biases were found related to age and race, while no notable gender bias was observed. Mitigation strategies showed varying improvements across different sensitive attributes and subproblems. For race and gender, Reject Option Classification significantly improved equalized odds, whereas for age, adversarial debiasing yielded the highest improvement in the same metric.

Conclusion: Biases exist in ML models diagnosing MCI and AD, particularly concerning age and race. Various mitigation strategies provide improvements in fairness across different demographic characteristics.

Abstract: The present study performs a comprehensive fairness analysis of machine
learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and
Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases
associated with age, race, and gender in a multi-cohort dataset, as well as the
influence of proxy features encoding these sensitive attributes, are
investigated. The reliability of various fairness definitions and metrics in
the identification of such biases is also assessed. Based on the most
appropriate fairness measures, a comparative analysis of widely used
pre-processing, in-processing, and post-processing bias mitigation strategies
is performed. Moreover, a novel composite measure is introduced to quantify the
trade-off between fairness and performance by considering the F1-score and the
equalized odds ratio, making it appropriate for medical diagnostic
applications. The obtained results reveal the existence of biases related to
age and race, while no significant gender bias is observed. The deployed
mitigation strategies yield varying improvements in terms of fairness across
the different sensitive attributes and studied subproblems. For race and
gender, Reject Option Classification improves equalized odds by 46% and 57%,
respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI
versus AD subproblem, whereas for age, in the same subproblem, adversarial
debiasing yields the highest equalized odds improvement of 40% with a harmonic
mean score of 0.69. Insights are provided into how variations in AD
neuropathology and risk factors, associated with demographic characteristics,
influence model fairness.

</details>


### [159] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2505.23529)
*Shifeng Xie,Aref Einizade,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: The paper introduces SubGEC, a method for graph representation learning using self-supervised learning that employs subgraph Gaussian embedding and optimal transport distances to improve contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective self-supervised learning approach for graph representation learning that preserves subgraph characteristics while ensuring controlled distribution of generated subgraphs.

Method: Proposes SubGEC with a subgraph Gaussian embedding module that maps subgraphs to a structured Gaussian space. Utilizes Wasserstein and Gromov-Wasserstein distances to measure subgraph similarity.

Result: SubGEC outperforms or matches state-of-the-art methods in extensive experiments across multiple benchmarks.

Conclusion: Highlights the importance of the distribution of generated contrastive pairs in designing SSL methods for GRL.

Abstract: Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [160] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
*Giorgos Iacovides,Wuyang Zhou,Chao Li,Qibin Zhao,Danilo Mandic*

Main category: cs.LG

TL;DR: The paper introduces tnLLM, a novel tensor network structure search (TN-SS) framework leveraging large language models and domain information to predict suitable TN structures with fewer function evaluations than current SOTA algorithms.


<details>
  <summary>Details</summary>
Motivation: Current TN-SS methods are computationally expensive and lack transparency in utilizing domain information inherent in real-world tensor data.

Method: Proposes tnLLM framework which uses domain-aware prompting pipeline to instruct LLMs for inferring TN structures based on relationships between tensor modes.

Result: tnLLM achieves comparable TN-SS objective function values with much fewer function evaluations compared to SOTA algorithms. Also, LLM-enabled domain information accelerates convergence of sampling-based SOTA methods while preserving performance guarantees.

Conclusion: tnLLM offers an efficient and transparent approach for TN-SS by integrating domain knowledge and LLM reasoning capabilities.

Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [161] [Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis](https://arxiv.org/abs/2505.23552)
*Alex Adams*

Main category: cs.LG

TL;DR: 本文研究了两种解决线性回归问题的基本方法：闭式 Moore-Penrose 广义逆和迭代梯度下降法的比较性能。通过理论分析、计算复杂度分析以及在合成数据集和真实数据集上的实证评估，结果明确了每种方法在计算时间、数值稳定性和预测准确性方面表现出色的条件，为机器学习研究人员和从业者提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 线性回归是预测建模的基础，选择合适的求解器对效率和准确性有显著影响。因此，有必要对比分析不同求解方法的优劣，以帮助研究者和实践者做出更明智的选择。

Method: 作者回顾并讨论了 Moore-Penrose 广义逆和梯度下降法的理论基础，分析了两者的计算复杂度，并通过在受控特性的合成数据集和已建立的真实世界数据集上进行实证评估来比较其性能。

Result: 结果明确了每种方法在不同条件下（如计算时间、数值稳定性、预测准确性）的优势。Moore-Penrose 广义逆可能更适合小规模或需要精确解的问题，而梯度下降法可能在大规模数据集上表现更好。

Conclusion: 研究为机器学习领域中的研究者和从业者提供了实际建议，帮助他们在解决线性回归任务时选择适合的方法：直接精确解或迭代近似解。

Abstract: This paper investigates the comparative performance of two fundamental
approaches to solving linear regression problems: the closed-form Moore-Penrose
pseudoinverse and the iterative gradient descent method. Linear regression is a
cornerstone of predictive modeling, and the choice of solver can significantly
impact efficiency and accuracy. I review and discuss the theoretical
underpinnings of both methods, analyze their computational complexity, and
evaluate their empirical behavior on synthetic datasets with controlled
characteristics, as well as on established real-world datasets. My results
delineate the conditions under which each method excels in terms of
computational time, numerical stability, and predictive accuracy. This work
aims to provide practical guidance for researchers and practitioners in machine
learning when selecting between direct, exact solutions and iterative,
approximate solutions for linear regression tasks.

</details>


### [162] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2505.23555)
*Yanzhao Hou,Jiaxiang Geng,Boyu Li,Xiaofeng Tao,Juncheng Wang,Xiaodong Xu,Bing Luo*

Main category: cs.LG

TL;DR: The paper proposes an adaptive federated LoRA strategy with independent client sampling to reduce the wall-clock convergence time in federated fine-tuning of LLMs, considering computation and communication heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Existing federated LoRA approaches inadequately address system and data heterogeneity, leading to suboptimal training efficiency.

Method: Derive a new convergence bound for federated LoRA without bounded gradient assumption; introduce an adaptive bandwidth allocation scheme; solve a non-convex optimization problem to determine LoRA sketching ratios and sampling probabilities.

Result: Extensive experiments show significant reduction in wall-clock training time compared to state-of-the-art methods across various models and datasets.

Conclusion: Adaptive federated LoRA with independent client sampling effectively minimizes wall-clock convergence time in federated fine-tuning.

Abstract: Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [163] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
*Yiran Guo,Lijie Xu,Jie Liu,Dan Ye,Shuang Qiu*

Main category: cs.LG

TL;DR: The paper proposes Segment Policy Optimization (SPO), a new reinforcement learning framework that improves the reasoning abilities of large language models by providing more accurate advantage estimation at an intermediate granularity, leading to significant improvements in accuracy for short and long chain-of-thought tasks.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches for enhancing reasoning capabilities of large language models either suffer from inaccurate token-level advantage estimation or imprecise trajectory-level credit assignment. This motivates the need for a method that can achieve a better balance between these two extremes.

Method: SPO is a novel RL framework that uses segment-level advantage estimation at an intermediate granularity. It includes three components: flexible segment partition, accurate segment advantage estimation using Monte Carlo without a critic model, and policy optimization with a probability-mask strategy. SPO-chain is used for short chain-of-thought tasks with cutpoint-based partition and chain-based advantage estimation, while SPO-tree is designed for long chain-of-thought tasks with tree-based advantage estimation.

Result: SPO-chain achieves 6-12 percentage point improvements in accuracy over PPO and GRPO on GSM8K for short chain-of-thought tasks. SPO-tree achieves 7-11 percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation for long chain-of-thought tasks.

Conclusion: Segment Policy Optimization offers a more precise and efficient approach to reinforce learning for large language models, achieving significant improvements in reasoning tasks compared to existing methods.

Abstract: Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [164] [DRO: A Python Library for Distributionally Robust Optimization in Machine Learning](https://arxiv.org/abs/2505.23565)
*Jiashuo Liu,Tianyu Wang,Henry Lam,Hongseok Namkoong,Jose Blanchet*

Main category: cs.LG

TL;DR: The paper introduces dro, an open-source Python library for distributionally robust optimization (DRO) which implements multiple DRO formulations and backbone models, compatible with scikit-learn and PyTorch, significantly reducing runtime on large-scale datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, efficient, and versatile tool for distributionally robust optimization in regression and classification problems.

Method: Development of the dro library which includes 14 DRO formulations, 9 backbone models, enabling 79 distinct DRO methods. The library is compatible with scikit-learn and PyTorch and utilizes vectorization and optimization approximation techniques to improve runtime efficiency.

Result: dro reduces runtime by 10x to over 1000x compared to baseline implementations on large-scale datasets.

Conclusion: dro is an effective and efficient open-source library for DRO, providing extensive documentation at https://python-dro.org.

Abstract: We introduce dro, an open-source Python library for distributionally robust
optimization (DRO) for regression and classification problems. The library
implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO
methods. Furthermore, dro is compatible with both scikit-learn and PyTorch.
Through vectorization and optimization approximation techniques, dro reduces
runtime by 10x to over 1000x compared to baseline implementations on
large-scale datasets. Comprehensive documentation is available at
https://python-dro.org.

</details>


### [165] [Maximum Likelihood Learning of Latent Dynamics Without Reconstruction](https://arxiv.org/abs/2505.23569)
*Samo Hromadka,Kai Biegun,Lior Fox,James Heald,Maneesh Sahani*

Main category: cs.LG

TL;DR: A novel unsupervised learning method for time series data, RP-GSSM, is introduced which combines the intuition of contrastive methods with the flexible tools of probabilistic generative models.


<details>
  <summary>Details</summary>
Motivation: To address the need for a more effective unsupervised learning method for time series data with latent dynamical structure.

Method: The RP-GSSM model, which learns Markovian Gaussian latents explaining statistical dependence between observations at different time steps. It uses maximum likelihood learning and has an arbitrarily nonlinear neural network link between observations and latents.

Result: RP-GSSM outperforms alternatives on problems that include learning nonlinear stochastic dynamics from video, with or without background distractors.

Conclusion: The RP-GSSM is a useful foundation model for a variety of downstream applications.

Abstract: We introduce a novel unsupervised learning method for time series data with
latent dynamical structure: the recognition-parametrized Gaussian state space
model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian
Gaussian latents explaining statistical dependence between observations at
different time steps, combining the intuition of contrastive methods with the
flexible tools of probabilistic generative models. Unlike contrastive
approaches, the RP-GSSM is a valid probabilistic model learned via maximum
likelihood. Unlike generative approaches, the RP-GSSM has no need for an
explicit network mapping from latents to observations, allowing it to focus
model capacity on inference of latents. The model is both tractable and
expressive: it admits exact inference thanks to its jointly Gaussian latent
prior, while maintaining expressivity with an arbitrarily nonlinear neural
network link between observations and latents. These qualities allow the
RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary
losses, or optimizer scheduling. We show how this approach outperforms
alternatives on problems that include learning nonlinear stochastic dynamics
from video, with or without background distractors. Our results position the
RP-GSSM as a useful foundation model for a variety of downstream applications.

</details>


### [166] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Main category: cs.LG

TL;DR: BioReason将DNA基础模型与大语言模型结合，首次实现对基因组信息的直接处理和推理，通过监督微调和强化学习提升多步推理能力，在疾病通路预测等任务上显著优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前的DNA基础模型虽能提供强大的序列表示，但在多步推理及透明、直观的生物解释方面存在不足。

Method: 开发了BioReason架构，将DNA基础模型与大语言模型（LLM）深度集成，使LLM能够直接处理和推理基因组信息，并通过监督微调和定向强化学习培养复杂的多步推理能力。

Result: 在生物推理基准测试中表现出色，如KEGG疾病通路预测准确率从88%提升至97%，变体效应预测性能平均提高15%。

Conclusion: BioReason提供了一种可解释的、逐步的生物痕迹推理方式，为AI在生物学中的应用提供了变革性的方法，加速了从基因组数据生成可检验假设的过程。

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [167] [Improving Time Series Forecasting via Instance-aware Post-hoc Revision](https://arxiv.org/abs/2505.23583)
*Zhiding Liu,Mingyue Cheng,Guanhao Zhao,Jiqian Yang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: The paper introduces PIR, a model-agnostic framework that improves time series forecasting by identifying biased instances and revising forecasts using contextual information, leading to better instance-level accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting is crucial in many applications, but despite recent improvements in overall accuracy, instance-level variations remain challenging due to factors like distribution shifts, missing data, and long-tail patterns.

Method: PIR enhances forecasting performance through Post-forecasting Identification and Revision. It identifies biased forecasting instances by estimating their accuracy and then revises the forecasts using contextual information from both local and global perspectives.

Result: Experiments on real-world datasets show that PIR effectively reduces instance-level errors and significantly boosts forecasting reliability when used with mainstream forecasting models.

Conclusion: PIR is a valuable tool for improving the robustness and reliability of time series forecasting models against instance-level variations.

Abstract: Time series forecasting plays a vital role in various real-world applications
and has attracted significant attention in recent decades. While recent methods
have achieved remarkable accuracy by incorporating advanced inductive biases
and training strategies, we observe that instance-level variations remain a
significant challenge. These variations--stemming from distribution shifts,
missing data, and long-tail patterns--often lead to suboptimal forecasts for
specific instances, even when overall performance appears strong. To address
this issue, we propose a model-agnostic framework, PIR, designed to enhance
forecasting performance through Post-forecasting Identification and Revision.
Specifically, PIR first identifies biased forecasting instances by estimating
their accuracy. Based on this, the framework revises the forecasts using
contextual information, including covariates and historical time series, from
both local and global perspectives in a post-processing fashion. Extensive
experiments on real-world datasets with mainstream forecasting models
demonstrate that PIR effectively mitigates instance-level errors and
significantly improves forecasting reliability.

</details>


### [168] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
*Yaru Hao,Li Dong,Xun Wu,Shaohan Huang,Zewen Chi,Furu Wei*

Main category: cs.LG

TL;DR: This paper proposes a new reinforcement learning algorithm called On-Policy RL with Optimal reward baseline (OPO). It aims to solve the problems of training instability and computational inefficiency in current algorithms. OPO emphasizes exact on-policy training for stability and introduces an optimal reward baseline to minimize gradient variance. Evaluated on mathematical reasoning benchmarks, OPO shows superior performance and stability without extra models or terms, achieving lower policy shifts and higher output entropy.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning algorithms often encounter issues of training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models.

Method: Propose a novel algorithm named On-Policy RL with Optimal reward baseline (OPO). This method focuses on exact on-policy training to stabilize the process and enhance exploration, while introducing the optimal reward baseline that theoretically minimizes gradient variance.

Result: OPO demonstrates superior performance and training stability on mathematical reasoning benchmarks, without additional models or regularization terms. It also achieves lower policy shifts and higher output entropy.

Conclusion: OPO represents a promising direction for stable and effective reinforcement learning in aligning large language models with human preferences and enhancing their reasoning capabilities.

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [169] [Accelerated Training of Federated Learning via Second-Order Methods](https://arxiv.org/abs/2505.23588)
*Mrinmay Sen,Sidhant R Nair,C Krishna Mohan*

Main category: cs.LG

TL;DR: This paper explores second-order optimization methods in Federated Learning (FL) to address slow convergence and excessive communication rounds.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of slow convergence and high communication costs in FL, especially when data across clients are highly heterogeneous.

Method: Examine various FL methods that leverage second-order optimization to accelerate training. Provide a comprehensive categorization and comparison of state-of-the-art second-order FL methods based on multiple performance metrics.

Result: Findings show potential in incorporating Hessian curvature through second-order optimization into FL, but highlight challenges like efficient utilization of Hessian and its inverse.

Conclusion: Lays the groundwork for future research on scalable and efficient federated optimization methods.

Abstract: This paper explores second-order optimization methods in Federated Learning
(FL), addressing the critical challenges of slow convergence and the excessive
communication rounds required to achieve optimal performance from the global
model. While existing surveys in FL primarily focus on challenges related to
statistical and device label heterogeneity, as well as privacy and security
concerns in first-order FL methods, less attention has been given to the issue
of slow model training. This slow training often leads to the need for
excessive communication rounds or increased communication costs, particularly
when data across clients are highly heterogeneous. In this paper, we examine
various FL methods that leverage second-order optimization to accelerate the
training process. We provide a comprehensive categorization of state-of-the-art
second-order FL methods and compare their performance based on convergence
speed, computational cost, memory usage, transmission overhead, and
generalization of the global model. Our findings show the potential of
incorporating Hessian curvature through second-order optimization into FL and
highlight key challenges, such as the efficient utilization of Hessian and its
inverse in FL. This work lays the groundwork for future research aimed at
developing scalable and efficient federated optimization methods for improving
the training of the global model in FL.

</details>


### [170] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/abs/2505.23593)
*Nikita Agrawal,Simon Mertel,Ruben Mayer*

Main category: cs.LG

TL;DR: This position paper critically analyzes the use of black-box models in federated post-training, arguing it contradicts core FL principles like data privacy and autonomy.


<details>
  <summary>Details</summary>
Motivation: To evaluate the implications and challenges of using black-box foundation language models in federated learning for post-training, considering the potential contradiction with FL's core principles.

Method: A critical analysis approach is used to examine various aspects of openness associated with black-box models in federated post-training scenarios.

Result: The analysis reveals that employing black-box models in FL may conflict with essential FL principles including data privacy and user autonomy.

Conclusion: The paper concludes by emphasizing the need for more transparent and open approaches in federated post-training to align with the fundamental tenets of federated learning.

Abstract: Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


### [171] [LLM Performance for Code Generation on Noisy Tasks](https://arxiv.org/abs/2505.23598)
*Radzim Sendyka,Christian Cabrera,Andrei Paleyes,Diana Robinson,Neil Lawrence*

Main category: cs.LG

TL;DR: 这篇论文研究了大型语言模型（LLMs）识别和解决被混淆到无法辨认的任务的能力。通过比较多个模型和混淆方法在竞争性编程和基准任务中的表现，发现所有评估的LLMs都能解决使文本对人类读者不可理解且缺乏关键指令或上下文的任务。这种行为被描述为急切模式匹配，表明强记忆或过拟合训练数据，而非合理推理问题。实验揭示了污染和未见数据集之间的性能衰减模式差异，并建议使用标准数据集设计实验时应谨慎。此外，提出测量在混淆下的性能衰减作为检测数据集污染的一种策略，以突出自动化软件系统的潜在安全风险和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在面对被混淆的任务时的表现，了解其解决问题的能力是否依赖于记忆或过拟合训练数据，而非真正的推理能力。

Method: 通过在竞争性编程和基准任务中应用不同的混淆方法（如噪声和删减），评估多个大型语言模型的表现，并引入急切模式匹配的概念来描述模型的行为。

Result: 所有评估的LLMs都能解决被混淆到人类无法理解的任务，表现出强记忆或过拟合训练数据的特点。并且在污染和未见数据集之间发现了不同的性能衰减模式。

Conclusion: 应在使用标准数据集设计实验时保持谨慎，测量在混淆下的性能衰减可能是一种检测数据集污染的策略，同时有助于揭示自动化软件系统的潜在安全风险和可解释性问题。

Abstract: This paper investigates the ability of large language models (LLMs) to
recognise and solve tasks which have been obfuscated beyond recognition.
Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we
compare performance across multiple models and obfuscation methods, such as
noise and redaction. We demonstrate that all evaluated LLMs can solve tasks
obfuscated to a level where the text would be unintelligible to human readers,
and does not contain key pieces of instruction or context. We introduce the
concept of eager pattern matching to describe this behaviour, which is not
observed in tasks published after the models' knowledge cutoff date, indicating
strong memorisation or overfitting to training data, rather than legitimate
reasoning about the presented problem. We report empirical evidence of distinct
performance decay patterns between contaminated and unseen datasets. We discuss
the implications for benchmarking and evaluations of model behaviour, arguing
for caution when designing experiments using standard datasets. We also propose
measuring the decay of performance under obfuscation as a possible strategy for
detecting dataset contamination and highlighting potential safety risks and
interpretability issues for automated software systems.

</details>


### [172] [On Transferring Transferability: Towards a Theory for Size Generalization](https://arxiv.org/abs/2505.23599)
*Eitan Levin,Yuxin Ma,Mateo Díaz,Soledad Villar*

Main category: cs.LG

TL;DR: 本论文提出了一种跨维度可转移性的一般框架，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多现代学习任务需要处理不同大小的输入，因此需要与维度无关的架构。图神经网络领域已经探索了在低维数据上训练的模型是否可以将其性能转移到高维输入上。

Method: 作者引入了一个通用的跨维度转移性框架，该框架将转移性对应到由小问题实例等价于大问题实例形成的空间中的连续性。根据数据和学习任务进行识别。对现有架构进行实例化，并实现必要的更改以确保其可转移性。最后，提供了设计新的可转移模型的设计原则。

Result: 数值实验支持作者的研究成果。

Conclusion: 作者提出了一个通用框架来研究跨维度的可转移性，并且提供了设计新可转移模型的原则。

Abstract: Many modern learning tasks require models that can take inputs of varying
sizes. Consequently, dimension-independent architectures have been proposed for
domains where the inputs are graphs, sets, and point clouds. Recent work on
graph neural networks has explored whether a model trained on low-dimensional
data can transfer its performance to higher-dimensional inputs. We extend this
body of work by introducing a general framework for transferability across
dimensions. We show that transferability corresponds precisely to continuity in
a limit space formed by identifying small problem instances with equivalent
large ones. This identification is driven by the data and the learning task. We
instantiate our framework on existing architectures, and implement the
necessary changes to ensure their transferability. Finally, we provide design
principles for designing new transferable models. Numerical experiments support
our findings.

</details>


### [173] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)
*Qingyu Shi,Jinbin Bai,Zhuoran Zhao,Wenhao Chai,Kaidong Yu,Jianzong Wu,Shuangyong Song,Yunhai Tong,Xiangtai Li,Xuelong Li,Shuicheng Yan*

Main category: cs.LG

TL;DR: Muddit是一种统一的离散扩散变换器，能够在文本和图像模态之间进行快速并行生成。它结合了预训练的文本到图像主干的强视觉先验和轻量级文本解码器，实现了高质量的多模态生成，并在质量和效率上优于大型自回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归统一模型因顺序解码而推理速度慢，而非自回归统一模型由于预训练主干的限制导致泛化能力弱。

Method: Muddit通过整合预训练的文本到图像主干的强视觉先验与轻量级文本解码器，采用统一的架构进行灵活高质量的多模态生成。

Result: 实证结果显示，Muddit在质量和效率上都达到了与显著更大的自回归模型竞争或超越的性能。

Conclusion: 该工作强调了当配备强视觉先验时，纯粹的离散扩散可以作为一个可扩展且有效的统一生成骨干。

Abstract: Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [174] [Data Model Design for Explainable Machine Learning-based Electricity Applications](https://arxiv.org/abs/2505.23607)
*Carolina Fortuna,Gregor Cerar,Blaz Bertalanic,Andrej Campa,Mihael Mohorcic*

Main category: cs.LG

TL;DR: The paper proposes a taxonomy for energy application data, validates it in household electricity forecasting, and explains feature contributions to model accuracy.


<details>
  <summary>Details</summary>
Motivation: Digital transformation of energy infrastructure enables data driven applications with machine learning models. However, most models rely on univariate data and there is a lack of structured study on the role of meta-data and multivariate data.

Method: A taxonomy is proposed to identify and structure data types for energy applications. This taxonomy guides data model development for machine learning models. The paper focuses on household electricity forecasting to validate the taxonomy's effectiveness in feature selection for different models.

Result: The study explores the impact of domain, contextual, and behavioral features on the forecasting accuracy of four interpretable machine learning techniques across three datasets. Feature importance techniques are used to explain individual feature contributions.

Conclusion: The proposed taxonomy provides a useful framework for selecting features in machine learning models for energy applications, enhancing forecasting accuracy.

Abstract: The transition from traditional power grids to smart grids, significant
increase in the use of renewable energy sources, and soaring electricity prices
has triggered a digital transformation of the energy infrastructure that
enables new, data driven, applications often supported by machine learning
models. However, the majority of the developed machine learning models rely on
univariate data. To date, a structured study considering the role meta-data and
additional measurements resulting in multivariate data is missing. In this
paper we propose a taxonomy that identifies and structures various types of
data related to energy applications. The taxonomy can be used to guide
application specific data model development for training machine learning
models. Focusing on a household electricity forecasting application, we
validate the effectiveness of the proposed taxonomy in guiding the selection of
the features for various types of models. As such, we study of the effect of
domain, contextual and behavioral features on the forecasting accuracy of four
interpretable machine learning techniques and three openly available datasets.
Finally, using a feature importance techniques, we explain individual feature
contributions to the forecasting accuracy.

</details>


### [175] [The Generalized Skew Spectrum of Graphs](https://arxiv.org/abs/2505.23609)
*Armando Bellante,Martin Plávala,Alessandro Luongo*

Main category: cs.LG

TL;DR: This paper proposes a family of permutation-invariant graph embeddings, which generalizes the Skew Spectrum of graphs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to introduce a new class of graph invariants that are isomorphism-invariant and capable of embedding richer graph structures.

Method: The method is grounded in group theory and harmonic analysis, introducing a family of functions that enables a trade-off between computational complexity and expressivity.

Result: The result is an improvement on the expressivity of the Skew Spectrum at the same computational cost.

Conclusion: The conclusion is that the proposed generalization has been formally proved to be invariant, with improved expressiveness demonstrated through experiments and efficient computation discussed.

Abstract: This paper proposes a family of permutation-invariant graph embeddings,
generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded
in group theory and harmonic analysis, our method introduces a new class of
graph invariants that are isomorphism-invariant and capable of embedding richer
graph structures - including attributed graphs, multilayer graphs, and
hypergraphs - which the Skew Spectrum could not handle. Our generalization
further defines a family of functions that enables a trade-off between
computational complexity and expressivity. By applying
generalization-preserving heuristics to this family, we improve the Skew
Spectrum's expressivity at the same computational cost. We formally prove the
invariance of our generalization, demonstrate its improved expressiveness
through experiments, and discuss its efficient computation.

</details>


### [176] [Inference-time Scaling of Diffusion Models through Classical Search](https://arxiv.org/abs/2505.23614)
*Xiangcheng Zhang,Haowei Lin,Haotian Ye,James Zou,Jianzhu Ma,Yitao Liang,Yilun Du*

Main category: cs.LG

TL;DR: This paper proposes a framework that combines local and global search methods to improve the efficiency and performance of diffusion models during inference time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of inference-time control in diffusion models by adapting generated outputs to meet diverse test-time objectives using principles from classical search.

Method: The method involves a general framework that orchestrates local and global search. It uses annealed Langevin MCMC for theoretically grounded local search and breadth-first and depth-first tree search for compute-efficient global exploration.

Result: The approach was evaluated on various challenging domains such as planning, offline reinforcement learning, and image generation, resulting in significant gains in both performance and efficiency across all tasks.

Conclusion: Classical search provides a principled and practical foundation for inference-time scaling in diffusion models.

Abstract: Classical search algorithms have long underpinned modern artificial
intelligence. In this work, we tackle the challenge of inference-time control
in diffusion models -- adapting generated outputs to meet diverse test-time
objectives -- using principles from classical search. We propose a general
framework that orchestrates local and global search to efficiently navigate the
generative space. It employs a theoretically grounded local search via annealed
Langevin MCMC and performs compute-efficient global exploration using
breadth-first and depth-first tree search. We evaluate our approach on a range
of challenging domains, including planning, offline reinforcement learning, and
image generation. Across all tasks, we observe significant gains in both
performance and efficiency. These results show that classical search provides a
principled and practical foundation for inference-time scaling in diffusion
models. Project page at diffusion-inference-scaling.github.io.

</details>


### [177] [Learning Interpretable Differentiable Logic Networks for Tabular Regression](https://arxiv.org/abs/2505.23615)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: Neural networks perform well but have opaque decision processes and high computational cost. Differentiable Logic Networks (DLNs) were proposed to address these issues in tabular classification by relaxing discrete logic into differentiable form for gradient-based learning. The paper extends DLNs to supervised tabular regression, redesigning the output layer for continuous targets and unifying the training procedure. Evaluated on 15 benchmarks, regression DLNs match or exceed baseline accuracy with interpretability and fast inference.


<details>
  <summary>Details</summary>
Motivation: To address the opaque decision processes and high computational costs of neural networks in resource-constrained environments, especially for tabular data classification.

Method: Extending the DLN framework to supervised tabular regression by redesigning the final output layer to support continuous targets and unifying the original two-phase training procedure into a single differentiable stage.

Result: Regression DLNs match or exceed baseline accuracy while preserving interpretability and fast inference on 15 public regression benchmarks.

Conclusion: DLNs are a viable, cost-effective alternative for regression tasks, particularly where model transparency and computational efficiency are important.

Abstract: Neural networks (NNs) achieve outstanding performance in many domains;
however, their decision processes are often opaque and their inference can be
computationally expensive in resource-constrained environments. We recently
proposed Differentiable Logic Networks (DLNs) to address these issues for
tabular classification based on relaxing discrete logic into a differentiable
form, thereby enabling gradient-based learning of networks built from binary
logic operations. DLNs offer interpretable reasoning and substantially lower
inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically,
we redesign the final output layer to support continuous targets and unify the
original two-phase training procedure into a single differentiable stage. We
evaluate the resulting model on 15 public regression benchmarks, comparing it
with modern neural networks and classical regression baselines. Regression DLNs
match or exceed baseline accuracy while preserving interpretability and fast
inference. Our results show that DLNs are a viable, cost-effective alternative
for regression tasks, especially where model transparency and computational
efficiency are important.

</details>


### [178] [Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection](https://arxiv.org/abs/2505.23627)
*Griffin Dietz Smith,Dianna Yee,Jennifer King Chen,Leah Findlater*

Main category: cs.LG

TL;DR: The paper proposes a novel end-to-end architecture that improves verbatim transcription and enables direct miscue detection by incorporating target reading text via prompting.


<details>
  <summary>Details</summary>
Motivation: Current post-hoc methods for identifying reading mistakes perform poorly when automatic speech recognition inaccurately transcribes verbatim speech.

Method: A novel end-to-end architecture is proposed which incorporates the target reading text via prompting and is trained for both improved verbatim transcription and direct miscue detection.

Result: Two case studies demonstrated that the proposed strategies improve verbatim transcription and miscue detection compared to current state-of-the-art methods in both children's read-aloud and adult atypical speech scenarios.

Conclusion: Incorporating reading text through prompting benefits verbatim transcription more than fine-tuning, and it is feasible to augment speech recognition tasks for end-to-end miscue detection.

Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly
approached post-hoc by comparing automatic speech recognition (ASR)
transcriptions to the target reading text. However, post-hoc methods perform
poorly when ASR inaccurately transcribes verbatim speech. To improve on current
methods for reading error annotation, we propose a novel end-to-end
architecture that incorporates the target reading text via prompting and is
trained for both improved verbatim transcription and direct miscue detection.
Our contributions include: first, demonstrating that incorporating reading text
through prompting benefits verbatim transcription performance over fine-tuning,
and second, showing that it is feasible to augment speech recognition tasks for
end-to-end miscue detection. We conducted two case studies -- children's
read-aloud and adult atypical speech -- and found that our proposed strategies
improve verbatim transcription and miscue detection compared to current
state-of-the-art.

</details>


### [179] [Global optimization of graph acquisition functions for neural architecture search](https://arxiv.org/abs/2505.23640)
*Yilin Xie,Shiqiang Zhang,Jixiang Qing,Ruth Misener,Calvin Tsay*

Main category: cs.LG

TL;DR: Graph Bayesian optimization is enhanced by explicit optimization formulations for graph input space, showing efficacy in neural architecture search.


<details>
  <summary>Details</summary>
Motivation: To address the lack of study on acquisition optimization in graph structures for neural architecture search.

Method: Present explicit optimization formulations for graph input space including reachability and shortest paths, formulate graph kernels and the acquisition function, and provide theoretical proof for the encoding as an equivalent representation of the graph space.

Result: Numerical results over several NAS benchmarks show that the method efficiently finds the optimal architecture for most cases.

Conclusion: The proposed method provides a powerful and data-efficient tool for neural architecture search with explicit graph input space optimization.

Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and
data-efficient tool for neural architecture search (NAS). Most existing graph
BO works focus on developing graph surrogates models, i.e., metrics of networks
and/or different kernels to quantify the similarity between networks. However,
the acquisition optimization, as a discrete optimization task over graph
structures, is not well studied due to the complexity of formulating the graph
search space and acquisition functions. This paper presents explicit
optimization formulations for graph input space including properties such as
reachability and shortest paths, which are used later to formulate graph
kernels and the acquisition function. We theoretically prove that the proposed
encoding is an equivalent representation of the graph space and provide
restrictions for the NAS domain with either node or edge labels. Numerical
results over several NAS benchmarks show that our method efficiently finds the
optimal architecture for most cases, highlighting its efficacy.

</details>


### [180] [Continuous Chain of Thought Enables Parallel Exploration and Reasoning](https://arxiv.org/abs/2505.23648)
*Halil Alperen Gozeten,M. Emrullah Ildiz,Xuechen Zhang,Hrayr Harutyunyan,Ankit Singh Rawat,Samet Oymak*

Main category: cs.LG

TL;DR: 本文研究了连续值token链式思维（CoT2）在逻辑推理任务中的优势，提出了优化和探索方法，并验证了其对推理效率的提升。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型通过自回归采样离散token生成链式思维轨迹，但这种方法在表达能力和搜索能力上存在局限性，连续值token提供了一种更丰富、更具表现力的替代方案。

Method: 理论分析表明CoT2允许模型并行跟踪多条轨迹，提升了推理效率；提出了一种新的监督策略，将softmax输出与目标轨迹的经验token分布相匹配；引入了两种采样策略：一种是在每个解码步骤中采样和组合K个离散token以控制并行度，另一种是基于概率单纯形的连续探索。

Result: 实验结果证实，使用CoT2进行策略优化确实可以提高模型性能，超越其初始的离散或连续监督效果。

Conclusion: 连续值token链式思维（CoT2）在逻辑推理任务中具有显著优势，能够提升模型的推理效率和性能，为语言模型的发展提供了新方向。

Abstract: Current language models generate chain-of-thought traces by autoregressively
sampling tokens from a finite vocabulary. While this discrete sampling has
achieved remarkable success, conducting chain-of-thought with
continuously-valued tokens (CoT2) offers a richer and more expressive
alternative. Our work examines the benefits of CoT2 through logical reasoning
tasks that inherently require search capabilities and provide optimization and
exploration methods for CoT2. Theoretically, we show that CoT2 allows the model
to track multiple traces in parallel and quantify its benefits for inference
efficiency. Notably, one layer transformer equipped with CoT2 can provably
solve the combinatorial "subset sum problem" given sufficient embedding
dimension. These insights lead to a novel and effective supervision strategy
where we match the softmax outputs to the empirical token distributions of a
set of target traces. Complementing this, we introduce sampling strategies that
unlock policy optimization and self-improvement for CoT2. Our first strategy
samples and composes $K$ discrete tokens at each decoding step to control the
level of parallelism, and reduces to standard CoT when $K=1$. Our second
strategy relies on continuous exploration over the probability simplex.
Experiments confirm that policy optimization with CoT2 indeed improves the
performance of the model beyond its initial discrete or continuous supervision.

</details>


### [181] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/abs/2505.23651)
*Juncheol Shin,Minsang Seok,Seonggon Kim,Eunhyeok Park*

Main category: cs.LG

TL;DR: 本研究探讨了量化对多目标领域适应中模型合并的影响，并提出了一种新的后训练量化方法HDRQ，以最小化与源预训练模型的偏差并平滑损失面，从而实现顺畅的模型合并。这是对此挑战的首次研究，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前模型合并技术在处理量化模型时面临新挑战，因为量化限制了领域范围并引入离散化效应，使得模型合并变得复杂。

Method: 通过误差屏障的视角分析量化对模型合并的影响，并提出一种新的后训练量化方法HDRQ（Hessian and distant regularizing quantization），该方法旨在考虑多目标领域适应中的模型合并问题。

Result: 所提出的HDRQ方法能够确保量化过程与源预训练模型的偏差最小，并平滑损失面以促进模型合并。

Conclusion: 这是首个针对此挑战的研究，广泛的实验结果证明了所提方法的有效性。

Abstract: Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [182] [How does Transformer Learn Implicit Reasoning?](https://arxiv.org/abs/2505.23653)
*Jiaran Ye,Zijun Yao,Zhidian Huang,Liangming Pan,Jinxin Liu,Yushi Bai,Amy Xin,Liu Weichuan,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 近期研究表明，大型语言模型（LLMs）可以进行隐式多步推理——在不明确表达中间步骤的情况下得出正确答案，但其底层机制尚不明确。本文通过在受控的符号环境中从零训练Transformer模型，研究了这种隐式推理的出现过程。发现了一个三阶段的发展轨迹：早期的记忆、随后的同分布泛化以及最终的跨分布泛化。研究结果表明，使用原子三元组训练不是必须的，但能加速学习；第二步泛化依赖于对特定组合结构的查询级暴露。为了解释这些行为，引入了两种诊断工具：跨查询语义修补和基于余弦的表示镜头。这些发现为隐式多步推理的可解释性提供了新见解，有助于阐明复杂推理过程如何在内部展开，并为提高此类模型的透明度提供了途径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）能够进行隐式多步推理，但其背后的机制尚未被充分理解。因此，需要深入研究这种推理能力是如何形成的，特别是在可控环境下的发展轨迹和影响因素。

Method: 在受控的符号环境中从头训练Transformer模型，分析其推理能力的发展轨迹。通过两个诊断工具：跨查询语义修补和基于余弦的表示镜头，揭示成功推理与隐藏空间中余弦基聚类之间的关系。

Result: 发现了三阶段发展轨迹：早期记忆、同分布泛化和跨分布泛化。证明了原子三元组训练可加速学习，而第二步泛化依赖于特定组合结构的查询级暴露。此外，隐藏空间中的聚类现象可以解释整个训练过程中观察到的行为动态。

Conclusion: 这些发现为隐式多步推理的可解释性提供了新的视角，帮助理解复杂推理过程的内部展开方式，并为提高LLMs的透明度提供了可能的改进路径。

Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop
reasoning implicitly -- producing correct answers without explicitly
verbalizing intermediate steps -- but the underlying mechanisms remain poorly
understood. In this paper, we study how such implicit reasoning emerges by
training transformers from scratch in a controlled symbolic environment. Our
analysis reveals a three-stage developmental trajectory: early memorization,
followed by in-distribution generalization, and eventually cross-distribution
generalization. We find that training with atomic triples is not necessary but
accelerates learning, and that second-hop generalization relies on query-level
exposure to specific compositional structures. To interpret these behaviors, we
introduce two diagnostic tools: cross-query semantic patching, which identifies
semantically reusable intermediate representations, and a cosine-based
representational lens, which reveals that successful reasoning correlates with
the cosine-base clustering in hidden space. This clustering phenomenon in turn
provides a coherent explanation for the behavioral dynamics observed across
training, linking representational structure to reasoning capability. These
findings provide new insights into the interpretability of implicit multi-hop
reasoning in LLMs, helping to clarify how complex reasoning processes unfold
internally and offering pathways to enhance the transparency of such models.

</details>


### [183] [AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction](https://arxiv.org/abs/2505.23663)
*Niklas Freymuth,Tobias Würth,Nicolas Schreiber,Balazs Gyenes,Andreas Boltres,Johannes Mitsch,Aleksandar Taranovic,Tai Hoang,Philipp Dahlinger,Philipp Becker,Luise Kärger,Gerhard Neumann*

Main category: cs.LG

TL;DR: AMBER is a supervised learning approach to adaptive meshing that predicts sizing fields and uses data augmentation to generate meshes for complex physical simulations, outperforming recent baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve computational efficiency in simulating complex physical systems using FEM by automating the creation of adaptive meshes without task-specific heuristics or manual design.

Method: AMBER starts from a coarse mesh and iteratively predicts the sizing field using a hierarchical graph neural network. It then generates a new intermediate mesh with an out-of-the-box mesh generator, employing data augmentation by projecting expert labels onto AMBER-generated data during training.

Result: AMBER generalizes to unseen geometries and consistently outperforms multiple recent baselines, including those using Graph and Convolutional Neural Networks, and Reinforcement Learning-based approaches.

Conclusion: AMBER presents a novel supervised learning approach to adaptive meshing, demonstrating superior performance on 2D and 3D datasets compared to existing methods.

Abstract: The cost and accuracy of simulating complex physical systems using the Finite
Element Method (FEM) scales with the resolution of the underlying mesh.
Adaptive meshes improve computational efficiency by refining resolution in
critical regions, but typically require task-specific heuristics or cumbersome
manual design by a human expert. We propose Adaptive Meshing By Expert
Reconstruction (AMBER), a supervised learning approach to mesh adaptation.
Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,
a function mapping from the geometry to the local element size of the target
mesh, and uses this prediction to produce a new intermediate mesh using an
out-of-the-box mesh generator. This process is enabled through a hierarchical
graph neural network, and relies on data augmentation by automatically
projecting expert labels onto AMBER-generated data during training. We evaluate
AMBER on 2D and 3D datasets, including classical physics problems, mechanical
components, and real-world industrial designs with human expert meshes. AMBER
generalizes to unseen geometries and consistently outperforms multiple recent
baselines, including ones using Graph and Convolutional Neural Networks, and
Reinforcement Learning-based approaches.

</details>


### [184] [Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds](https://arxiv.org/abs/2505.23673)
*Aya Kayal,Sattar Vakili,Laura Toni,Da-shan Shiu,Alberto Bernacchia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bayesian optimization (BO) with preference-based feedback has recently
garnered significant attention due to its emerging applications. We refer to
this problem as Bayesian Optimization from Human Feedback (BOHF), which differs
from conventional BO by learning the best actions from a reduced feedback
model, where only the preference between two actions is revealed to the learner
at each time step. The objective is to identify the best action using a limited
number of preference queries, typically obtained through costly human feedback.
Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model,
provides regret bounds for the performance of several algorithms. In this work,
within the same framework we develop tighter performance guarantees.
Specifically, we derive regret bounds of
$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the
maximum information gain$\unicode{x2014}$a kernel-specific complexity
term$\unicode{x2014}$and $T$ is the number of queries. Our results
significantly improve upon existing bounds. Notably, for common kernels, we
show that the order-optimal sample complexities of conventional
BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are
recovered. In other words, the same number of preferential samples as
scalar-valued samples is sufficient to find a nearly optimal solution.

</details>


### [185] [Understanding Mode Connectivity via Parameter Space Symmetry](https://arxiv.org/abs/2505.23681)
*Bo Zhao,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TL;DR: 通过参数空间对称性探索神经网络极小值的连通性，并推导线性网络极小值的连通分量数量，同时研究模式连通性和线性模式连通性的成立或失败条件，最后给出由对称性诱导的连接曲线的显式表达式。


<details>
  <summary>Details</summary>
Motivation: 尽管模式连通性在模型合并和微调等应用中发挥了作用，但其理论解释尚不清楚，因此需要一种新的方法来探索极小值的连通性。

Method: 提出了一种基于参数空间对称性探索极小值连通性的新方法，将对称群的拓扑与极小值的拓扑联系起来；推导线性网络极小值的连通分量数量；研究模式连通性和线性模式连通性的成立或失败条件；给出由对称性诱导的连接曲线的显式表达式并推导线性模式连通性近似成立的条件。

Result: 成功推导出线性网络极小值的连通分量数量，发现跳过连接会减少该数量；明确了模式连通性和线性模式连通性的成立条件；给出了连接曲线的显式表达式及线性模式连通性近似成立的条件。

Conclusion: 连续对称性在理解神经网络损失景观中起着重要作用，为模式连通性的理论解释提供了新的视角。

Abstract: Neural network minima are often connected by curves along which train and
test loss remain nearly constant, a phenomenon known as mode connectivity.
While this property has enabled applications such as model merging and
fine-tuning, its theoretical explanation remains unclear. We propose a new
approach to exploring the connectedness of minima using parameter space
symmetry. By linking the topology of symmetry groups to that of the minima, we
derive the number of connected components of the minima of linear networks and
show that skip connections reduce this number. We then examine when mode
connectivity and linear mode connectivity hold or fail, using parameter
symmetries which account for a significant part of the minimum. Finally, we
provide explicit expressions for connecting curves in the minima induced by
symmetry. Using the curvature of these curves, we derive conditions under which
linear mode connectivity approximately holds. Our findings highlight the role
of continuous symmetries in understanding the neural network loss landscape.

</details>


### [186] [Learning Compositional Functions with Transformers from Easy-to-Hard Data](https://arxiv.org/abs/2505.23683)
*Zixuan Wang,Eshaan Nichani,Alberto Bietti,Alex Damian,Daniel Hsu,Jason D. Lee,Denny Wu*

Main category: cs.LG

TL;DR: 本研究探讨了Transformer在k重组合任务中的可学习性，揭示了统计计算鸿沟，并提出了两种课程学习策略以实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 尽管已知Transformer能够高效执行涉及并行计算的多步推理任务，但其可学习性以及通过基于梯度的优化实现高效学习的数据分布条件仍是一个开放问题。

Method: 研究分析了k重组合任务的可学习性，证明了统计查询（SQ）下线性查询次数需要指数级样本数的下界，同时展示了通过两种课程学习策略（逐步增加难度或同时呈现所有数据），可以利用深度为O(log k)的Transformer进行多项式时间的学习。

Result: 证明了在统计查询框架下存在统计计算鸿沟；展示通过课程学习策略，可以在时间和样本复杂度均为k的多项式范围内学习此类函数类。

Conclusion: 研究表明，数据分布中同时包含简单和复杂示例对于Transformer学习复杂的组合任务是必要且充分的。

Abstract: Transformer-based language models have demonstrated impressive capabilities
across a range of complex reasoning tasks. Prior theoretical work exploring the
expressive power of transformers has shown that they can efficiently perform
multi-step reasoning tasks involving parallelizable computations. However, the
learnability of such constructions, particularly the conditions on the data
distribution that enable efficient learning via gradient-based optimization,
remains an open question. Towards answering this question, in this work we
study the learnability of the $k$-fold composition task, which requires
computing an interleaved composition of $k$ input permutations and $k$ hidden
permutations, and can be expressed by a transformer with $O(\log k)$ layers. On
the negative front, we prove a Statistical Query (SQ) lower bound showing that
any SQ learner that makes only polynomially-many queries to an SQ oracle for
the $k$-fold composition task distribution must have sample size exponential in
$k$, thus establishing a statistical-computational gap. On the other hand, we
show that this function class can be efficiently learned, with runtime and
sample complexity polynomial in $k$, by gradient descent on an $O(\log
k)$-depth transformer via two different curriculum learning strategies: one in
which data consists of $k'$-fold composition functions with $k' \le k$
presented in increasing difficulty, and another in which all such data is
presented simultaneously. Our work sheds light on the necessity and sufficiency
of having both easy and hard examples in the data distribution for transformers
to learn complex compositional tasks.

</details>


### [187] [Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms](https://arxiv.org/abs/2505.23696)
*Hiroshi Kera,Nico Pelleriti,Yuki Ishihara,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本研究提出了一种新的深度学习方法——Oracle Border Basis Algorithm，用于加速多项式方程组的Border basis计算。通过设计基于Transformer的oracle来识别并消除昂贵的简化步骤，同时保持结果正确性，实现了高达3.5倍的速度提升。此外，还开发了采样方法、定理及特定于单项式计算的嵌入方案，提高了算法效率和数据表达能力。


<details>
  <summary>Details</summary>
Motivation: 求解具有有限解的多项式方程组是许多科学领域的重要挑战。传统方法（如Gröbner基和Border基）虽然基础但计算成本高，而现有深度学习方法虽提高效率但牺牲了输出正确性。因此需要一种既能加速计算又保证结果正确性的新方法。

Method: 1. 设计基于Transformer的oracle，用于识别和消除计算昂贵的简化步骤。
2. 开发采样方法和首个Border基采样定理，生成训练数据。
3. 构建针对单项式代数计算的标记化和嵌入方案，减少输入表示中的token数量。
4. 在关键计算阶段选择性调用oracle以加速计算。

Result: 与基础算法相比，该方法实现了高达3.5倍的速度提升，同时保持了结果的正确性。此外，提出的嵌入方案显著减少了编码n元多项式的token数量（降低O(n)因子），并且学习方法对数据高效且稳定。

Conclusion: Oracle Border Basis Algorithm是一种实用的增强方法，结合了深度学习和传统计算机代数算法的优势，为多项式方程组求解提供了更高效、更稳定的解决方案。

Abstract: Solving systems of polynomial equations, particularly those with finitely
many solutions, is a crucial challenge across many scientific fields.
Traditional methods like Gr\"obner and Border bases are fundamental but suffer
from high computational costs, which have motivated recent Deep Learning
approaches to improve efficiency, albeit at the expense of output correctness.
In this work, we introduce the Oracle Border Basis Algorithm, the first Deep
Learning approach that accelerates Border basis computation while maintaining
output guarantees. To this end, we design and train a Transformer-based oracle
that identifies and eliminates computationally expensive reduction steps, which
we find to dominate the algorithm's runtime. By selectively invoking this
oracle during critical phases of computation, we achieve substantial speedup
factors of up to 3.5x compared to the base algorithm, without compromising the
correctness of results. To generate the training data, we develop a sampling
method and provide the first sampling theorem for border bases. We construct a
tokenization and embedding scheme tailored to monomial-centered algebraic
computations, resulting in a compact and expressive input representation, which
reduces the number of tokens to encode an $n$-variate polynomial by a factor of
$O(n)$. Our learning approach is data efficient, stable, and a practical
enhancement to traditional computer algebra algorithms and symbolic
computation.

</details>


### [188] [DiCoFlex: Model-agnostic diverse counterfactuals with flexible control](https://arxiv.org/abs/2505.23700)
*Oleksii Furman,Ulvi Movsum-zada,Patryk Marszalek,Maciej Zięba,Marek Śmieja*

Main category: cs.LG

TL;DR: The paper introduces DiCoFlex, a model-agnostic framework for generating multiple diverse counterfactual explanations with real-time user-defined constraints, showing superior performance in validity, diversity, proximity, and constraint adherence.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating counterfactual explanations require constant model access, are computationally expensive, and lack flexibility to adapt to new constraints without retraining.

Method: Proposes DiCoFlex, a conditional generative framework using conditional normalizing flows trained on labeled data to produce multiple diverse counterfactuals in one forward pass, allowing real-time customization of constraints.

Result: DiCoFlex outperforms existing methods in validity, diversity, proximity, and constraint adherence as shown through extensive experiments on benchmark datasets.

Conclusion: DiCoFlex is a practical and scalable solution for generating counterfactuals in sensitive decision-making domains.

Abstract: Counterfactual explanations play a pivotal role in explainable artificial
intelligence (XAI) by offering intuitive, human-understandable alternatives
that elucidate machine learning model decisions. Despite their significance,
existing methods for generating counterfactuals often require constant access
to the predictive model, involve computationally intensive optimization for
each instance and lack the flexibility to adapt to new user-defined constraints
without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,
conditional generative framework that produces multiple diverse counterfactuals
in a single forward pass. Leveraging conditional normalizing flows trained
solely on labeled data, DiCoFlex addresses key limitations by enabling
real-time user-driven customization of constraints such as sparsity and
actionability at inference time. Extensive experiments on standard benchmark
datasets show that DiCoFlex outperforms existing methods in terms of validity,
diversity, proximity, and constraint adherence, making it a practical and
scalable solution for counterfactual generation in sensitive decision-making
domains.

</details>


### [189] [(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs](https://arxiv.org/abs/2505.23702)
*Nathan Lichtlé,Alexi Canesse,Zhe Fu,Hossein Nick Zinat Matin,Maria Laura Delle Monache,Alexandre M. Bayen*

Main category: cs.LG

TL;DR: (U)NFV是一种基于神经网络的架构，它推广了用于求解双曲守恒律的经典有限体积(FV)方法。通过学习扩展时空模板上的更新规则，同时保留守恒结构，(U)NFV在求解双曲偏微分方程(PDEs)方面比传统FV方法更准确和灵活。


<details>
  <summary>Details</summary>
Motivation: 双曲偏微分方程（PDEs）尤其是含有激波和不连续性的守恒律具有挑战性，传统的有限体积法虽然具备良好的数学性质（如熵解收敛、流量守恒或总变差递减），但在复杂设置下往往缺乏精度和灵活性。

Method: 提出了一种名为(U)NFV的模块化神经网络架构，该架构推广了经典的有限体积方法。它通过在扩展的空间和时间模板上学习更新规则，同时保留守恒结构来解决这些问题。支持通过解数据进行监督训练(NFV)和通过弱形式残差损失进行无监督训练(UNFV)。

Result: 应用于一阶守恒律时，(U)NFV比Godunov方法误差低10倍，优于ENO/WENO，并且在复杂度远低于间断Galerkin求解器的情况下表现相当。对于交通建模问题，无论是从PDE还是实验高速公路数据来看，(U)NFV都比传统FV方法具有更高的保真度和可扩展性。

Conclusion: (U)NFV提供了一种新的、更精确和灵活的方法来求解双曲守恒律，同时保持了守恒结构。

Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes
classical finite volume (FV) methods for solving hyperbolic conservation laws.
Hyperbolic partial differential equations (PDEs) are challenging to solve,
particularly conservation laws whose physically relevant solutions contain
shocks and discontinuities. FV methods are widely used for their mathematical
properties: convergence to entropy solutions, flow conservation, or total
variation diminishing, but often lack accuracy and flexibility in complex
settings. Neural Finite Volume addresses these limitations by learning update
rules over extended spatial and temporal stencils while preserving conservation
structure. It supports both supervised training on solution data (NFV) and
unsupervised training via weak-form residual loss (UNFV). Applied to
first-order conservation laws, (U)NFV achieves up to 10x lower error than
Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin
solvers with far less complexity. On traffic modeling problems, both from PDEs
and from experimental highway data, (U)NFV captures nonlinear wave dynamics
with significantly higher fidelity and scalability than traditional FV
approaches.

</details>


### [190] [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/abs/2505.23705)
*Danny Driess,Jost Tobias Springenberg,Brian Ichter,Lili Yu,Adrian Li-Bell,Karl Pertsch,Allen Z. Ren,Homer Walke,Quan Vuong,Lucy Xiaoyang Shi,Sergey Levine*

Main category: cs.LG

TL;DR: VLA模型结合了端到端学习与大规模视觉语言模型的知识迁移，但实时控制需求与VLM设计存在矛盾。本文研究了在VLA中加入连续扩散或流匹配动作专家对训练速度和知识迁移的影响，并提出了一种保护VLM主干的技术。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型为物理系统提供了强大的控制策略训练方法，但现有的VLM由于参数量庞大且输出离散，难以满足实时控制需求。因此需要探索如何通过添加模块（如动作专家或连续输出头）来实现高效连续控制，同时不损害预训练VLM中的语义知识。

Method: 研究了包含连续扩散或流匹配动作专家的VLA模型，分析其对性能和知识迁移的影响，并提出一种技术以隔离VLM主干，减轻负面影响。

Result: 发现简单地加入这些专家模块会显著损害训练速度和知识迁移效果，而所提出的隔离技术可以有效缓解这一问题。

Conclusion: 在VLA模型中，加入连续控制模块需谨慎处理，以免破坏预训练VLM的知识；本文提出的隔离技术可改善训练动态并保留语义知识。

Abstract: Vision-language-action (VLA) models provide a powerful approach to training
control policies for physical systems, such as robots, by combining end-to-end
learning with transfer of semantic knowledge from web-scale vision-language
model (VLM) training. However, the constraints of real-time control are often
at odds with the design of VLMs: the most powerful VLMs have tens or hundreds
of billions of parameters, presenting an obstacle to real-time inference, and
operate on discrete tokens rather than the continuous-valued outputs that are
required for controlling robots. To address this challenge, recent VLA models
have used specialized modules for efficient continuous control, such as action
experts or continuous output heads, which typically require adding new
untrained parameters to the pretrained VLM backbone. While these modules
improve real-time and control capabilities, it remains an open question whether
they preserve or degrade the semantic knowledge contained in the pretrained
VLM, and what effect they have on the VLA training dynamics. In this paper, we
study this question in the context of VLAs that include a continuous diffusion
or flow matching action expert, showing that naively including such experts
significantly harms both training speed and knowledge transfer. We provide an
extensive analysis of various design choices, their impact on performance and
knowledge transfer, and propose a technique for insulating the VLM backbone
during VLA training that mitigates this issue. Videos are available at
https://pi.website/research/knowledge_insulation.

</details>


### [191] [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](https://arxiv.org/abs/2505.23719)
*Andreas Auer,Patrick Podest,Daniel Klotz,Sebastian Böck,Günter Klambauer,Sepp Hochreiter*

Main category: cs.LG

TL;DR: The paper presents TiRex, an enhanced LSTM model with strong in-context learning abilities for zero-shot time series forecasting. It introduces a training-time masking strategy called CPM and achieves state-of-the-art performance on HuggingFace benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of transformers and LSTMs in zero-shot time series forecasting, where transformers fall short and LSTMs lack strong in-context learning abilities.

Method: Development of TiRex using xLSTM, an enhanced LSTM with competitive in-context learning skills. Introduction of a training-time masking strategy called CPM to facilitate state-tracking ability.

Result: TiRex sets a new state of the art in zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and Chronos-ZS, outperforming significantly larger models across both short- and long-term forecasts.

Conclusion: TiRex bridges the gap between transformers and LSTMs for zero-shot time series forecasting, demonstrating superior performance and potential for practical applications.

Abstract: In-context learning, the ability of large language models to perform tasks
using only examples provided in the prompt, has recently been adapted for time
series forecasting. This paradigm enables zero-shot prediction, where past
values serve as context for forecasting future values, making powerful
forecasting tools accessible to non-experts and increasing the performance when
training data are scarce. Most existing zero-shot forecasting approaches rely
on transformer architectures, which, despite their success in language, often
fall short of expectations in time series forecasting, where recurrent models
like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited
for time series modeling due to their state-tracking capabilities, they lack
strong in-context learning abilities. We introduce TiRex that closes this gap
by leveraging xLSTM, an enhanced LSTM with competitive in-context learning
skills. Unlike transformers, state-space models, or parallelizable RNNs such as
RWKV, TiRex retains state-tracking, a critical property for long-horizon
forecasting. To further facilitate its state-tracking ability, we propose a
training-time masking strategy called CPM. TiRex sets a new state of the art in
zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and
Chronos-ZS, outperforming significantly larger models including TabPFN-TS
(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)
across both short- and long-term forecasts.

</details>


### [192] [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents](https://arxiv.org/abs/2505.23720)
*Arun Verma,Indrajit Saha,Makoto Yokoo,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: In this paper, a contextual bandit problem with multiple agents is considered. Unlike previous work assuming truthful reports from agents, this study proposes COBRA, an algorithm that discourages strategic behavior without monetary incentives while ensuring incentive compatibility and sub-linear regret. Experimental results validate the performance of COBRA.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of existing contextual bandit algorithms which assume agents truthfully report their arms. In real-life applications, such as online platforms with multiple sellers, some agents may misrepresent information (e.g., product quality) to gain advantages. This necessitates an algorithm that can handle strategic agents effectively without relying on monetary incentives.

Method: The proposed method is called COBRA (Contextual Bandits with Strategic Agents). It is designed for contextual bandit problems involving strategic agents. COBRA disincentivizes strategic behavior by ensuring incentive compatibility and providing a sub-linear regret guarantee. Importantly, it achieves these goals without using any monetary incentives.

Result: The experimental results demonstrate the different performance aspects of the proposed COBRA algorithm, validating its effectiveness in handling strategic agents within the contextual bandit framework.

Conclusion: The conclusion of this paper is that COBRA successfully addresses the issue of strategic behavior in contextual bandit problems with multiple agents. By ensuring incentive compatibility and maintaining a sub-linear regret without monetary incentives, COBRA offers a robust solution to real-life challenges in multi-agent systems.

Abstract: This paper considers a contextual bandit problem involving multiple agents,
where a learner sequentially observes the contexts and the agent's reported
arms, and then selects the arm that maximizes the system's overall reward.
Existing work in contextual bandits assumes that agents truthfully report their
arms, which is unrealistic in many real-life applications. For instance,
consider an online platform with multiple sellers; some sellers may
misrepresent product quality to gain an advantage, such as having the platform
preferentially recommend their products to online users. To address this
challenge, we propose an algorithm, COBRA, for contextual bandit problems
involving strategic agents that disincentivize their strategic behavior without
using any monetary incentives, while having incentive compatibility and a
sub-linear regret guarantee. Our experimental results also validate the
different performance aspects of our proposed algorithm.

</details>


### [193] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/abs/2505.23721)
*Sean Current,Ziqi Chen,Daniel Adu-Ampratwum,Xia Ning,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: DiffER是一种用于化学 retrosynthesis 预测的无模板方法，基于分类扩散模型，能同时预测整个输出SMILES序列，达到顶级准确率。


<details>
  <summary>Details</summary>
Motivation: 当前自动化学逆合成方法受限于自回归性质，需要一种新的无模板方法来提升预测性能。

Method: 提出DiffER方法，采用分类扩散模型进行逆合成预测，引入长度预测组件，并构建扩散模型集成。

Result: 在top-1准确率上达到最先进水平，在top-3、top-5和top-10准确率上具有竞争力，证明了该方法作为新一类无模板模型的强大基线。

Conclusion: DiffER方法为无模板模型提供了一个强大基线，准确预测SMILES序列长度对于提升分类扩散模型性能至关重要。

Abstract: Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [194] [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/abs/2505.23724)
*Minrui Luo,Fuhang Kuang,Yu Wang,Zirui Liu,Tianxing He*

Main category: cs.LG

TL;DR: The paper proposes Subspace-Constrained LoRA (SC-LoRA), an improved LoRA initialization framework that balances efficient fine-tuning and knowledge preservation by constraining adapter outputs in a low-rank subspace. Experiments show SC-LoRA outperforms existing methods in fine-tuning performance and reducing knowledge forgetting.


<details>
  <summary>Details</summary>
Motivation: Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA are crucial for customizing Large Language Models (LLMs) but suffer from slow convergence and knowledge forgetting. Existing solutions address either efficiency or knowledge preservation, not both simultaneously.

Method: SC-LoRA introduces a novel LoRA initialization framework that constrains the output of trainable adapters in a low-rank subspace to balance fine-tuning efficiency and knowledge preservation. This is achieved by preserving context information of fine-tuning data while minimizing retention of preserved knowledge context.

Result: Experiments on various downstream tasks demonstrate that SC-LoRA achieves superior fine-tuning performance and significantly reduces knowledge forgetting compared to contemporary LoRA initialization methods.

Conclusion: SC-LoRA successfully navigates the trade-off between efficient fine-tuning and knowledge preservation, offering a more effective solution than existing methods.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), are indispensable for efficiently customizing Large Language
Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and
knowledge forgetting problems. Recent studies have leveraged the power of
designed LoRA initialization, to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM. However, none of these works can
address the two cases at the same time. To this end, we introduce
Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework
engineered to navigate the trade-off between efficient fine-tuning and
knowledge preservation. We achieve this by constraining the output of trainable
LoRA adapters in a low-rank subspace, where the context information of
fine-tuning data is most preserved while the context information of preserved
knowledge is least retained, in a balanced way. Such constraint enables the
trainable weights to primarily focus on the main features of fine-tuning data
while avoiding damaging the preserved knowledge features. We provide
theoretical analysis on our method, and conduct extensive experiments including
safety preservation and world knowledge preservation, on various downstream
tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning
performance while markedly diminishing knowledge forgetting, surpassing
contemporary LoRA initialization methods.

</details>


### [195] [MuLoCo: Muon is a practical inner optimizer for DiLoCo](https://arxiv.org/abs/2505.23725)
*Benjamin Thérien,Xiaolong Huang,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: DiLoCo框架在数据中心环境下能有效训练大语言模型，但其通信步骤仍需全量减少模型参数。本研究探讨了Top-k稀疏化和量化等压缩方法与两种本地优化器（AdamW和Muon）结合以减少DiLoCo的通信开销的效果。实验表明，使用Muon作为内部优化器并结合误差反馈累加器，可将通信数据压缩至2位而几乎无性能损失。MuLoCo（Muon内优器DiLoCo）相较于DiLoCo，在通信量减少8倍的情况下性能显著更优且内存复杂度相同。


<details>
  <summary>Details</summary>
Motivation: 尽管DiLoCo框架在训练大语言模型时表现出色，但其通信步骤仍然需要全量减少模型参数，这对通信资源提出了较高的要求。因此，研究如何通过压缩技术进一步降低DiLoCo的通信开销具有重要意义。

Method: 研究采用了标准压缩方法，包括Top-k稀疏化和量化，并将其与两种本地优化器（AdamW和Muon）结合，用于预训练仅解码器的变压器语言模型。通过实验分析不同配置下的性能表现，特别是Muon优化器与误差反馈累加器结合的效果。

Result: 实验结果表明，使用Muon作为内部优化器并结合误差反馈累加器，可以将通信数据压缩至2位，同时几乎不造成性能下降。此外，MuLoCo在通信量减少8倍的情况下显著优于DiLoCo，且内存复杂度保持不变。

Conclusion: Muon优化器与误差反馈累加器结合能够显著提高DiLoCo框架的性能，同时大幅降低通信开销。MuLoCo在减少通信量和提升性能方面表现出色，为未来的大规模语言模型训练提供了新的优化方向。

Abstract: DiLoCo is a powerful framework for training large language models (LLMs)
under networking constraints with advantages for increasing parallelism and
accelerator utilization in data center settings. Despite significantly reducing
communication frequency, however, DiLoCo's communication steps still involve
all-reducing a complete copy of the model's parameters. While existing works
have explored ways to reduce communication in DiLoCo, the role of error
feedback accumulators and the effect of the inner-optimizer on compressibility
remain under-explored. In this work, we investigate the effectiveness of
standard compression methods including Top-k sparsification and quantization
for reducing the communication overhead of DiLoCo when paired with two local
optimizers (AdamW and Muon). Our experiments pre-training decoder-only
transformer language models (LMs) reveal that leveraging Muon as the inner
optimizer for DiLoCo along with an error-feedback accumulator allows to
aggressively compress the communicated delta to 2-bits with next to no
performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)
significantly outperforms DiLoCo while communicating 8X less and having
identical memory complexity.

</details>


### [196] [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast](https://arxiv.org/abs/2505.23732)
*Shreeram Suresh Chandra,Lucas Goncalves,Junchen Lu,Carlos Busso,Berrak Sisman*

Main category: cs.LG

TL;DR: EmotionRankCLAP is a supervised contrastive learning approach that captures fine-grained emotion variations and improves cross-modal alignment using dimensional attributes of emotional speech and natural language prompts.


<details>
  <summary>Details</summary>
Motivation: Current emotion-based CLAP methods fail to capture the ordinal nature of emotions, leading to insufficient inter-emotion understanding and a wide modality gap between audio and text embeddings.

Method: EmotionRankCLAP uses dimensional attributes of emotional speech and natural language prompts with a Rank-N-Contrast objective to learn ordered relationships by contrasting samples based on their rankings in the valence-arousal space.

Result: EmotionRankCLAP outperforms existing emotion-CLAP methods in modeling emotion ordinality across modalities as measured by a cross-modal retrieval task.

Conclusion: EmotionRankCLAP successfully addresses the limitations of current emotion-based CLAP methods by capturing fine-grained emotion variations and improving cross-modal alignment.

Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods
typically learn by na\"ively aligning audio samples with corresponding text
prompts. Consequently, this approach fails to capture the ordinal nature of
emotions, hindering inter-emotion understanding and often resulting in a wide
modality gap between the audio and text embeddings due to insufficient
alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a
supervised contrastive learning approach that uses dimensional attributes of
emotional speech and natural language prompts to jointly capture fine-grained
emotion variations and improve cross-modal alignment. Our approach utilizes a
Rank-N-Contrast objective to learn ordered relationships by contrasting samples
based on their rankings in the valence-arousal space. EmotionRankCLAP
outperforms existing emotion-CLAP methods in modeling emotion ordinality across
modalities, measured via a cross-modal retrieval task.

</details>


### [197] [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)
*Paul Gölz,Nika Haghtalab,Kunhe Yang*

Main category: cs.LG

TL;DR: 在预训练后，大型语言模型通过成对比较与人类偏好对齐。现有的对齐方法（如基于PPO的RLHF和DPO）假设单一的偏好模型，但实际应用中用户偏好多样。本文引入了对齐方法的失真概念，即最优平均效用与学习策略平均效用之间的最差比率。Nash Learning from Human Feedback在各种条件下达到最小最大最优失真，而RLHF和DPO则表现出显著更高的失真。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型对齐方法假设单一的偏好模型，但在实际应用中用户偏好是多样的。这导致现有方法可能无法满足用户的平均需求，因此需要一种新的理论框架来评估和改进对齐方法的有效性。

Method: 本文结合社会选择理论和Bradley-Terry模型，提出了对齐方法的失真概念。通过分析不同方法的失真率，可以区分其在不同条件下的表现，并提出Nash Learning from Human Feedback作为一种更优的对齐方法。

Result: 研究表明，Nash Learning from Human Feedback在各种分布和约束条件下达到了最小最大最优失真，而RLHF和DPO则表现出显著更高的失真率，特别是在完整设置下。

Conclusion: 本文通过引入失真概念，为评估和改进大型语言模型的对齐方法提供了一个新的理论框架，强调了考虑用户多样性的重要性。

Abstract: After pre-training, large language models are aligned with human preferences
based on pairwise comparisons. State-of-the-art alignment methods (such as
PPO-based RLHF and DPO) are built on the assumption of aligning with a single
preference model, despite being deployed in settings where users have diverse
preferences. As a result, it is not even clear that these alignment methods
produce models that satisfy users on average -- a minimal requirement for
pluralistic alignment. Drawing on social choice theory and modeling users'
comparisons through individual Bradley-Terry (BT) models, we introduce an
alignment method's distortion: the worst-case ratio between the optimal
achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment
methods: Nash Learning from Human Feedback achieves the minimax optimal
distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature
$\beta$), robustly across utility distributions, distributions of comparison
pairs, and permissible KL divergences from the reference policy. RLHF and DPO,
by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a
KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full
setting, depending on how comparison pairs are sampled.

</details>


### [198] [REOrdering Patches Improves Vision Models](https://arxiv.org/abs/2505.23751)
*Declan Kutscher,David M. Chan,Yutong Bai,Trevor Darrell,Ritwik Gupta*

Main category: cs.LG

TL;DR: Sequence models like transformers need one-dimensional input sequences. Usually, images are flattened using a fixed row-major order. But modern transformers use approximations that make them sensitive to patch ordering. This paper proposes REOrder, a framework for finding the best patch orderings.


<details>
  <summary>Details</summary>
Motivation: The motivation is that patch order significantly affects model performance in settings where modern long-sequence transformers rely on architectural approximations that break permutation-equivariance.

Method: REOrder is a two-stage framework. First, an information-theoretic prior is derived by evaluating the compressibility of various patch sequences. Then, a policy over permutations is learned by optimizing a Plackett-Luce policy using REINFORCE.

Result: REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.

Conclusion: This shows that finding task-optimal patch orderings can greatly improve model performance.

Abstract: Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


### [199] [Model Immunization from a Condition Number Perspective](https://arxiv.org/abs/2505.23760)
*Amber Yijia Zheng,Cedar Site Bai,Brian Bullins,Raymond A. Yeh*

Main category: cs.LG

TL;DR: The paper explores model immunization through pre-training models resistant to harmful-task fine-tuning while maintaining utility. It proposes a framework using the Hessian matrix's condition number for linear models and an algorithm with regularization terms, showing effectiveness in both linear and non-linear models.


<details>
  <summary>Details</summary>
Motivation: There is a need for a deeper understanding of model immunization, specifically when it is possible and what defines an immunized model, especially after prior work demonstrated empirical evidence for text-to-image model immunization.

Method: The authors propose a framework based on the condition number of a Hessian matrix to analyze model immunization for linear models. They also design an algorithm incorporating regularization terms to control the condition numbers post pre-training.

Result: Empirical results indicate that the proposed algorithm effectively immunizes both linear models and non-linear deep-nets.

Conclusion: The framework and algorithm provide valuable insights into model immunization, demonstrating their effectiveness across different types of models.

Abstract: Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.

</details>


### [200] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
*Yunjae Won,Hyunji Lee,Hyeonbin Hwang,Minjoon Seo*

Main category: cs.LG

TL;DR: Direct Preference Optimization (DPO) is analyzed through Differential Information Distribution (DID). Log-ratio reward is optimal when preferences encode differential info. Low-entropy DID reinforces policy; high-entropy DID smooths it.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical justification for the log-ratio reward parameterization in Direct Preference Optimization (DPO), which is a standard technique for aligning language models with human preferences in a supervised manner.

Method: Utilize Differential Information Distribution (DID) to show that when preference labels encode the differential information needed to transform a reference policy into a target policy, the log-ratio reward in DPO is the uniquely optimal form. Analyze the entropy of DID to understand its effects on policy distribution.

Result: Derive a closed-form expression for the optimal sampling distribution over rejected responses. Find a link between differential information and log-margin ordered policies. Show that low-entropy DID reinforces policy distribution while high-entropy DID induces smoothing.

Conclusion: Present a unifying perspective on DPO objective, structure of preference data, and resulting policy behaviors through differential information. Suggest that learning high-entropy differential information is crucial for general instruction-following, while low-entropy benefits knowledge-intensive question answering.

Abstract: Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [201] [TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE](https://arxiv.org/abs/2505.22735)
*Tong Sun,Bowen Jiang,Hailong Lin,Borui Li,Yixiao Teng,Yi Gao,Wei Dong*

Main category: cs.CR

TL;DR: TensorShield is an efficient on-device inference method that protects critical tensors of a model in TEEs, providing security against MS and MIA while significantly reducing latency compared to shielding the entire model.


<details>
  <summary>Details</summary>
Motivation: On-device inference is crucial for safeguarding user data privacy, but existing methods struggle to balance full model security with low inference latency when using TEEs due to memory constraints.

Method: TensorShield uses a novel XAI technique to identify and shield critical tensors in TEEs, along with two designs for critical feature identification and latency-aware placement to ensure secure and fast inference.

Result: Evaluations demonstrate TensorShield offers similar security to protecting the whole model within TEEs, yet it's up to 25.35$\times$ (avg. 5.85$\times$) faster than current top solutions without sacrificing accuracy.

Conclusion: TensorShield successfully addresses the challenge of achieving both strong security and high efficiency in on-device inference by selectively protecting tensors.

Abstract: To safeguard user data privacy, on-device inference has emerged as a
prominent paradigm on mobile and Internet of Things (IoT) devices. This
paradigm involves deploying a model provided by a third party on local devices
to perform inference tasks. However, it exposes the private model to two
primary security threats: model stealing (MS) and membership inference attacks
(MIA). To mitigate these risks, existing wisdom deploys models within Trusted
Execution Environments (TEEs), which is a secure isolated execution space.
Nonetheless, the constrained secure memory capacity in TEEs makes it
challenging to achieve full model security with low inference latency. This
paper fills the gap with TensorShield, the first efficient on-device inference
work that shields partial tensors of the model while still fully defending
against MS and MIA. The key enabling techniques in TensorShield include: (i) a
novel eXplainable AI (XAI) technique exploits the model's attention transition
to assess critical tensors and shields them in TEE to achieve secure inference,
and (ii) two meticulous designs with critical feature identification and
latency-aware placement to accelerate inference while maintaining security.
Extensive evaluations show that TensorShield delivers almost the same security
protection as shielding the entire model inside TEE, while being up to
25.35$\times$ (avg. 5.85$\times$) faster than the state-of-the-art work,
without accuracy loss.

</details>


### [202] [Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?](https://arxiv.org/abs/2505.22843)
*Alexander Herzog,Aliai Eusebi,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: The abstract introduces AURORA, a framework for evaluating malware classifiers based on confidence quality and operational resilience. It highlights the need for reliable confidence estimates and holistic assessment metrics beyond point-in-time performance.


<details>
  <summary>Details</summary>
Motivation: Modern drift-adaptive malware classifiers show promising performance figures, but their genuine operational reliability remains untested. There is a gap in understanding how well these classifiers maintain reliable confidence estimates under distribution shifts.

Method: AURORA is proposed as a framework to evaluate malware classifiers by assessing their confidence quality and operational resilience. The framework verifies the confidence profile of models to ensure reliable estimates and complements this with metrics that provide a more comprehensive evaluation of operational stability over time.

Result: AURORA identifies fragility in state-of-the-art frameworks across datasets with varying drift severity. This suggests that current methods may not be operationally stable or reliable when subjected to real-world conditions and distribution shifts.

Conclusion: There is a need to revisit and rethink the design of malware classifiers to improve their operational reliability and stability. AURORA provides a path forward by offering a more thorough evaluation approach.

Abstract: The performance figures of modern drift-adaptive malware classifiers appear
promising, but does this translate to genuine operational reliability? The
standard evaluation paradigm primarily focuses on baseline performance metrics,
neglecting confidence-error alignment and operational stability. While
TESSERACT established the importance of temporal evaluation, we take a
complementary direction by investigating whether malware classifiers maintain
reliable confidence estimates under distribution shifts and exploring the
tensions between scientific advancement and practical impacts when they do not.
We propose AURORA, a framework to evaluate malware classifiers based on their
confidence quality and operational resilience. AURORA subjects the confidence
profile of a given model to verification to assess the reliability of its
estimates. Unreliable confidence estimates erode operational trust, waste
valuable annotation budget on non-informative samples for active learning, and
leave error-prone instances undetected in selective classification. AURORA is
further complemented by a set of metrics designed to go beyond point-in-time
performance, striving towards a more holistic assessment of operational
stability throughout temporal evaluation periods. The fragility we observe in
state-of-the-art frameworks across datasets of varying drift severity suggests
the need for a return to the whiteboard.

</details>


### [203] [Security Benefits and Side Effects of Labeling AI-Generated Images](https://arxiv.org/abs/2505.22845)
*Sandra Höltervennhoff,Jonas Ricker,Maike M. Raphael,Charlotte Schwedes,Rebecca Weil,Asja Fischer,Thorsten Holz,Lea Schönherr,Sascha Fahl*

Main category: cs.CR

TL;DR: 生成式人工智能快速发展，影响人类与信息和数字媒体的互动。尽管法规要求披露AI生成内容，但这些标签是否能减少AI生成虚假信息的风险尚不清楚。本文研究了AI生成图像标签的影响，包括错误标签的可能性。通过焦点小组和在线调查，我们发现虽然用户认为标签有助于识别AI生成图像，但也可能导致过度依赖标签，甚至降低对真实声明的信任。此外，标签还可能使包含不准确声明的人类制作图像显得更可信。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的发展带来了越来越逼真的虚假信息，因此了解AI生成内容标签的有效性变得至关重要。

Method: 1. 通过五个焦点小组定性探索用户对AI标签的意见和期望。2. 进行预先注册的在线调查，收集超过1300名美国和欧盟参与者的数据，以量化AI标签对用户识别虚假信息能力的影响。

Result: 1. 焦点小组显示用户认为标签有助于识别AI生成图像，但也担心其实现难度。2. 在线调查显示标签可能让用户过度依赖，导致对真实声明的信任下降。3. 标签还可能使人造图像中的不实声明显得更可信。

Conclusion: AI生成内容的标签在帮助用户识别虚假信息方面具有潜力，但也存在用户过度依赖和信任下降的风险。需要进一步研究如何优化标签设计以最大化其正面效果并最小化负面副作用。

Abstract: Generative artificial intelligence is developing rapidly, impacting humans'
interaction with information and digital media. It is increasingly used to
create deceptively realistic misinformation, so lawmakers have imposed
regulations requiring the disclosure of AI-generated content. However, only
little is known about whether these labels reduce the risks of AI-generated
misinformation.
  Our work addresses this research gap. Focusing on AI-generated images, we
study the implications of labels, including the possibility of mislabeling.
Assuming that simplicity, transparency, and trust are likely to impact the
successful adoption of such labels, we first qualitatively explore users'
opinions and expectations of AI labeling using five focus groups. Second, we
conduct a pre-registered online survey with over 1300 U.S. and EU participants
to quantitatively assess the effect of AI labels on users' ability to recognize
misinformation containing either human-made or AI-generated images. Our focus
groups illustrate that, while participants have concerns about the practical
implementation of labeling, they consider it helpful in identifying
AI-generated images and avoiding deception. However, considering security
benefits, our survey revealed an ambiguous picture, suggesting that users might
over-rely on labels. While inaccurate claims supported by labeled AI-generated
images were rated less credible than those with unlabeled AI-images, the belief
in accurate claims also decreased when accompanied by a labeled AI-generated
image. Moreover, we find the undesired side effect that human-made images
conveying inaccurate claims were perceived as more credible in the presence of
labels.

</details>


### [204] [Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment](https://arxiv.org/abs/2505.22852)
*Krti Tallam,Emma Miller*

Main category: cs.CR

TL;DR: CaMeL introduces a capability-based sandbox for LLM agents, but has limitations such as assuming trusted user prompts and performance tradeoffs. This response proposes improvements including prompt screening, output auditing, a tiered-risk access model, and a verified intermediate language.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of CaMeL's current design which assumes trusted user prompts, omits side-channel concerns, and has performance tradeoffs.

Method: Proposing engineering improvements such as prompt screening for initial inputs, output auditing to detect instruction leakage, a tiered-risk access model, and a verified intermediate language for formal guarantees.

Result: These upgrades expand CaMeL's threat coverage and operational usability, aligning it with enterprise security best practices and supporting scalable deployment.

Conclusion: The proposed enhancements will make CaMeL more robust against prompt injection attacks while maintaining usability and performance.

Abstract: CaMeL (Capabilities for Machine Learning) introduces a capability-based
sandbox to mitigate prompt injection attacks in large language model (LLM)
agents. While effective, CaMeL assumes a trusted user prompt, omits
side-channel concerns, and incurs performance tradeoffs due to its dual-LLM
design. This response identifies these issues and proposes engineering
improvements to expand CaMeL's threat coverage and operational usability. We
introduce: (1) prompt screening for initial inputs, (2) output auditing to
detect instruction leakage, (3) a tiered-risk access model to balance usability
and control, and (4) a verified intermediate language for formal guarantees.
Together, these upgrades align CaMeL with best practices in enterprise security
and support scalable deployment.

</details>


### [205] [Permissioned LLMs: Enforcing Access Control in Large Language Models](https://arxiv.org/abs/2505.22860)
*Bargav Jayaraman,Virendra J. Marathe,Hamid Mozaffari,William F. Shen,Krishnaram Kenthapadi*

Main category: cs.CR

TL;DR: 在企业环境中，组织数据被隔离、分散并由复杂的访问控制框架仔细保护。如果一个在分散数据上微调的大型语言模型（LLM）为具有不同访问权限的个人提供下游任务请求服务，这些访问控制结构可能会完全崩溃。本文提出了一种新的LLM类别——授权LLM（PermLLM），它将组织数据访问控制结构叠加到生成的查询响应上。


<details>
  <summary>Details</summary>
Motivation: 当前在企业环境中，数据访问控制是通过复杂的框架实现的，但如果使用在分散数据上微调的LLM来响应来自不同权限用户的请求，这种控制可能失效。因此需要一种新方法来确保LLM在生成响应时正确实施访问控制。

Method: 提出了PermLLM机制，该机制基于参数高效微调技术，并引入了相关响应的概念以证明PermLLM机制是否正确实施。此外，还提出了一种称为访问优势的新度量标准以及两个实例化指标：领域可区分性指数（DDI）和效用差距指数（UGI）。

Result: 通过在四个公共数据集（GPQA、RCV1、SimpleQA和WMDP）上的广泛实验，验证了PermLLM机制的有效性，同时也评估了DDI和UGI指标本身在量化LLM访问控制方面的有效性。

Conclusion: PermLLM机制能够有效实施组织数据访问控制，同时提出的访问优势及其实例化指标可以用于评估此类机制的效能。

Abstract: In enterprise settings, organizational data is segregated, siloed and
carefully protected by elaborate access control frameworks. These access
control structures can completely break down if an LLM fine-tuned on the siloed
data serves requests, for downstream tasks, from individuals with disparate
access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs
that superimpose the organizational data access control structures on query
responses they generate. We formalize abstractions underpinning the means to
determine whether access control enforcement happens correctly over LLM query
responses. Our formalism introduces the notion of a relevant response that can
be used to prove whether a PermLLM mechanism has been implemented correctly. We
also introduce a novel metric, called access advantage, to empirically evaluate
the efficacy of a PermLLM mechanism. We introduce three novel PermLLM
mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired
access control. We furthermore present two instantiations of access
advantage--(i) Domain Distinguishability Index (DDI) based on Membership
Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility
evaluation. We demonstrate the efficacy of our PermLLM mechanisms through
extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP),
in addition to evaluating the validity of DDI and UGI metrics themselves for
quantifying access control in LLMs.

</details>


### [206] [BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection](https://arxiv.org/abs/2505.22878)
*Shams Tarek,Dipayan Saha,Sujan Kumar Saha,Farimah Farahmandi*

Main category: cs.CR

TL;DR: The paper proposes BugWhisperer, a framework using a fine-tuned LLM for automating SoC security verification, improving efficiency and flexibility. It also introduces an open-source LLM and a hardware vulnerability database.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of manual, labor-intensive, and inflexible methodologies in SoC security verification which limit scalability and effectiveness of security protocols.

Method: Propose BugWhisperer framework utilizing a specialized, fine-tuned LLM to automate and improve adaptability and reusability of verification process by enhancing hardware security knowledge and leveraging text inference and knowledge transfer capabilities.

Result: The tailored LLM effectively enhances the efficiency and flexibility of the security verification process.

Conclusion: Introduction of BugWhisperer, an open-source fine-tuned LLM, and a comprehensive hardware vulnerability database significantly aids in improving SoC security verification.

Abstract: The current landscape of system-on-chips (SoCs) security verification faces
challenges due to manual, labor-intensive, and inflexible methodologies. These
issues limit the scalability and effectiveness of security protocols, making
bug detection at the Register-Transfer Level (RTL) difficult. This paper
proposes a new framework named BugWhisperer that utilizes a specialized,
fine-tuned Large Language Model (LLM) to address these challenges. By enhancing
the LLM's hardware security knowledge and leveraging its capabilities for text
inference and knowledge transfer, this approach automates and improves the
adaptability and reusability of the verification process. We introduce an
open-source, fine-tuned LLM specifically designed for detecting security
vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM
effectively enhances the efficiency and flexibility of the security
verification process. Additionally, we introduce a comprehensive hardware
vulnerability database that supports this work and will further assist the
research community in enhancing the security verification process.

</details>


### [207] [Chainless Apps: A Modular Framework for Building Apps with Web2 Capability and Web3 Trust](https://arxiv.org/abs/2505.22989)
*Brian Seong,Paul Gebheim*

Main category: cs.CR

TL;DR: Chainless Apps introduce a new application architecture that separates execution, trust, bridging, and settlement into distinct layers. This allows for app-specific sequencing, verifiable off-chain computation, and Web2-like user experience with Web3 verifiability.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between user experience and trust in modern blockchain applications.

Method: Separating execution, trust, bridging, and settlement into distinct compostable layers; leveraging zkVMs and decentralized validation services for verifiable off-chain computation.

Result: Enables fast Web2-like UX with Web3-grade verifiability, app-specific sequencing, chain-agnostic asset and message routing via Agglayer, and finality on Ethereum.

Conclusion: Chainless Apps provide modular, scalable applications maintaining interoperability with the broader blockchain ecosystem while allowing domain-specific trade-offs.

Abstract: Modern blockchain applications are often constrained by a trade-off between
user experience and trust. Chainless Apps present a new paradigm of application
architecture that separates execution, trust, bridging, and settlement into
distinct compostable layers. This enables app-specific sequencing, verifiable
off-chain computation, chain-agnostic asset and message routing via Agglayer,
and finality on Ethereum - resulting in fast Web2-like UX with Web3-grade
verifiability. Although consensus mechanisms have historically underpinned
verifiable computation, the advent of zkVMs and decentralized validation
services opens up new trust models for developers. Chainless Apps leverage this
evolution to offer modular, scalable applications that maintain
interoperability with the broader blockchain ecosystem while allowing
domain-specific trade-offs.

</details>


### [208] [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
*Jinchuan Zhang,Lu Yin,Yan Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: The paper introduces AgentAlign, a framework enhancing LLM agents' safety by synthesizing safety alignment data from abstract behavior chains. It improves model safety significantly while maintaining or boosting helpfulness.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to improve the safety of LLM-based agents without reducing their utility, as current agents execute malicious tasks even without being attacked.

Method: AgentAlign leverages abstract behavior chains for safety alignment data synthesis, generating authentic and executable instructions in simulated environments with diverse tool instances. It also synthesizes benign instructions through non-malicious interpretations to calibrate the boundary between helpfulness and harmlessness.

Result: Evaluation on AgentHarm shows fine-tuning open-source models with AgentAlign improves safety by 35.8% to 79.5% while minimally impacting or enhancing helpfulness, outperforming prompting methods.

Conclusion: AgentAlign provides an effective way to enhance LLM agents' safety without compromising their utility, demonstrated through significant improvements in safety metrics.

Abstract: The acquisition of agentic capabilities has transformed LLMs from "knowledge
providers" to "action executors", a trend that while expanding LLMs' capability
boundaries, significantly increases their susceptibility to malicious use.
Previous work has shown that current LLM-based agents execute numerous
malicious tasks even without being attacked, indicating a deficiency in agentic
use safety alignment during the post-training phase. To address this gap, we
propose AgentAlign, a novel framework that leverages abstract behavior chains
as a medium for safety alignment data synthesis. By instantiating these
behavior chains in simulated environments with diverse tool instances, our
framework enables the generation of highly authentic and executable
instructions while capturing complex multi-step dynamics. The framework further
ensures model utility by proportionally synthesizing benign instructions
through non-malicious interpretations of behavior chains, precisely calibrating
the boundary between helpfulness and harmlessness. Evaluation results on
AgentHarm demonstrate that fine-tuning three families of open-source models
using our method substantially improves their safety (35.8% to 79.5%
improvement) while minimally impacting or even positively enhancing their
helpfulness, outperforming various prompting methods. The dataset and code have
both been open-sourced.

</details>


### [209] [Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion](https://arxiv.org/abs/2505.23266)
*Chunlong Xie,Jialing He,Shangwei Guo,Jiacheng Wang,Shudong Zhang,Tianwei Zhang,Tao Xiang*

Main category: cs.CR

TL;DR: The paper introduces Adversarial Object Fusion (AdvOF), a novel attack framework targeting VLN agents in service-oriented environments by generating adversarial 3D objects. AdvOF effectively degrades agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks, advancing the understanding of service security in VLM-powered navigation systems.


<details>
  <summary>Details</summary>
Motivation: To investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents, particularly in service-oriented environments where reliability and quality-of-service are crucial.

Method: AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space to define and render adversarial objects. Then, it collaboratively optimizes the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Importance weights are assigned to varying views for stable and multi-view optimization through iterative fusions from local updates and justifications.

Result: Extensive evaluations show that AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks.

Conclusion: This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments.

Abstract: We present Adversarial Object Fusion (AdvOF), a novel attack framework
targeting vision-and-language navigation (VLN) agents in service-oriented
environments by generating adversarial 3D objects. While foundational models
like Large Language Models (LLMs) and Vision Language Models (VLMs) have
enhanced service-oriented navigation systems through improved perception and
decision-making, their integration introduces vulnerabilities in
mission-critical service workflows. Existing adversarial attacks fail to
address service computing contexts, where reliability and quality-of-service
(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of
adversarial environments on the VLM-based perception module of VLN agents. In
particular, AdvOF first precisely aggregates and aligns the victim object
positions in both 2D and 3D space, defining and rendering adversarial objects.
Then, we collaboratively optimize the adversarial object with regularization
between the adversarial and victim object across physical properties and VLM
perceptions. Through assigning importance weights to varying views, the
optimization is processed stably and multi-viewedly by iterative fusions from
local updates and justifications. Our extensive evaluations demonstrate AdvOF
can effectively degrade agent performance under adversarial conditions while
maintaining minimal interference with normal navigation tasks. This work
advances the understanding of service security in VLM-powered navigation
systems, providing computational foundations for robust service composition in
physical-world deployments.

</details>


### [210] [Joint Data Hiding and Partial Encryption of Compressive Sensed Streams](https://arxiv.org/abs/2505.23357)
*Cristina-Elena Popa,Cristian Damian,Daniela Coltuc*

Main category: cs.CR

TL;DR: This paper proposes a method to secure Compressive Sensing (CS) streams by protecting part of the measurements with a secret key and inserting the code into the rest using a reversible data hiding scheme. The image distortion caused by this method can be controlled by the number of embedded levels.


<details>
  <summary>Details</summary>
Motivation: To provide security for Compressive Sensing streams while avoiding the need to buffer measurements for subsequent encryption and thumbnail generation.

Method: The method involves generating a secret key via a cryptographically secure pseudo-random number generator (CSPRNG), XORing it with selected measurements, and inserting the result into the remaining measurements using a modified reversible data hiding scheme based on prediction error expansion that matches the statistics of CS measurements.

Result: In tests, embedding on 10 levels resulted in approximately 18 dB distortion for 256x256 pixel images reconstructed with the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).

Conclusion: The proposed method allows for on-the-fly insertion of protected data into Compressive Sensing streams, making it suitable for sequential acquisition by a Single Pixel Camera without needing to buffer measurements.

Abstract: The paper proposes a method to secure the Compressive Sensing (CS) streams.
It consists in protecting part of the measurements by a secret key and
inserting the code into the rest. The secret key is generated via a
cryptographically secure pseudo-random number generator (CSPRNG) and XORed with
the measurements to be inserted. For insertion, we use a reversible data hiding
(RDH) scheme, which is a prediction error expansion algorithm, modified to
match the statistics of CS measurements. The reconstruction from the embedded
stream conducts to visibly distorted images. The image distortion is controlled
by the number of embedded levels. In our tests, the embedding on 10 levels
results in $\approx 18 dB $ distortion for images of 256x256 pixels
reconstructed with the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).
A particularity of the presented method is on-the-fly insertion that makes it
appropriate for the sequential acquisition of measurements by a Single Pixel
Camera. On-the-fly insertion avoids the buffering of CS measurements for a
subsequent standard encryption and generation of a thumbnail image.

</details>


### [211] [Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models](https://arxiv.org/abs/2505.23561)
*Zenghui Yuan,Yangming Xu,Jiawen Shi,Pan Zhou,Lichao Sun*

Main category: cs.CR

TL;DR: The paper introduces Merge Hijacking, the first backdoor attack targeting model merging in Large Language Models (LLMs), and demonstrates its effectiveness and robustness.


<details>
  <summary>Details</summary>
Motivation: Model merging in LLMs is susceptible to backdoor attacks due to potential vulnerabilities in open-source models.

Method: The attacker constructs a malicious upload model that, when merged with other models, creates a merged model inheriting the backdoor while maintaining task utility. The process defines two main objectives - effectiveness and utility - achieved through four steps.

Result: Extensive experiments show the attack's effectiveness across different models, merging algorithms, and tasks, including real-world models. The attack also demonstrates robustness against various defenses.

Conclusion: Merge Hijacking presents a significant backdoor threat to model merging in LLMs, highlighting the need for enhanced security measures.

Abstract: Model merging for Large Language Models (LLMs) directly fuses the parameters
of different models finetuned on various tasks, creating a unified model for
multi-domain tasks. However, due to potential vulnerabilities in models
available on open-source platforms, model merging is susceptible to backdoor
attacks. In this paper, we propose Merge Hijacking, the first backdoor attack
targeting model merging in LLMs. The attacker constructs a malicious upload
model and releases it. Once a victim user merges it with any other models, the
resulting merged model inherits the backdoor while maintaining utility across
tasks. Merge Hijacking defines two main objectives-effectiveness and
utility-and achieves them through four steps. Extensive experiments demonstrate
the effectiveness of our attack across different models, merging algorithms,
and tasks. Additionally, we show that the attack remains effective even when
merging real-world models. Moreover, our attack demonstrates robustness against
two inference-time defenses (Paraphrasing and CLEANGEN) and one training-time
defense (Fine-pruning).

</details>


### [212] [Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643)
*Manuel Costa,Boris Köpf,Aashish Kolluri,Andrew Paverd,Mark Russinovich,Ahmed Salem,Shruti Tople,Lukas Wutschitz,Santiago Zanella-Béguelin*

Main category: cs.CR

TL;DR: 这篇论文探讨了使用信息流控制（IFC）为AI代理提供安全保证的方法，并提出了Fides，一个能够跟踪保密性和完整性标签、确定性地执行安全策略并引入选择性隐藏信息新原始方法的规划器。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理变得越来越自主和强大，确保其对诸如提示注入等漏洞的安全性变得至关重要。因此，研究如何利用信息流控制来增强AI代理的安全性成为了一个重要课题。

Method: 本研究提出了一种形式化模型，用于分析代理规划器的安全性和表达能力，并通过该模型描述了可以通过动态污点追踪强制执行的属性类别，构建了一个任务分类法以评估规划器设计的安全性和实用性权衡。基于这些探索，研究开发了Fides，一个能够跟踪保密性和完整性标签、确定性地执行安全策略并引入选择性隐藏信息新原始方法的规划器。

Result: Fides在AgentDojo中的评估表明，这种方法扩展了可以安全完成的任务范围。

Conclusion: 信息流控制（IFC）可以有效地为AI代理提供安全保证，而Fides作为一种新型规划器，在增强AI代理安全性的同时，也扩大了可安全执行任务的范围。

Abstract: As AI agents become increasingly autonomous and capable, ensuring their
security against vulnerabilities such as prompt injection becomes critical.
This paper explores the use of information-flow control (IFC) to provide
security guarantees for AI agents. We present a formal model to reason about
the security and expressiveness of agent planners. Using this model, we
characterize the class of properties enforceable by dynamic taint-tracking and
construct a taxonomy of tasks to evaluate security and utility trade-offs of
planner designs. Informed by this exploration, we present Fides, a planner that
tracks confidentiality and integrity labels, deterministically enforces
security policies, and introduces novel primitives for selectively hiding
information. Its evaluation in AgentDojo demonstrates that this approach
broadens the range of tasks that can be securely accomplished. A tutorial to
walk readers through the the concepts introduced in the paper can be found at
https://github.com/microsoft/fides

</details>


### [213] [Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference](https://arxiv.org/abs/2505.23655)
*Peter David Fagan*

Main category: cs.CR

TL;DR: This paper presents a new framework using chaotic dynamical systems for secure neural network inference.


<details>
  <summary>Details</summary>
Motivation: To provide a secure, privacy-preserving and efficient method for neural network inference.

Method: Using keyed chaotic dynamical transformations on tensors which are seeded cryptographically to create user-specific non-invertible transformations.

Result: Achieves scalable and lightweight security features including authenticated inference, tensor-level watermarking and data attribution.

Conclusion: Proposes a novel direction in tensor-level security for AI systems that is more efficient than conventional cryptographic techniques.

Abstract: This work introduces a novel framework for secure and privacy-preserving
neural network inference based on keyed chaotic dynamical transformations. The
proposed method applies a deterministic, cryptographically seeded chaotic
system to tensors, producing non-invertible, user-specific transformations that
enable authenticated inference, tensor-level watermarking, and data
attribution. This framework offers a scalable and lightweight alternative to
conventional cryptographic techniques, and establishes a new direction for
tensor-level security in AI systems.

</details>


### [214] [Bayesian Perspective on Memorization and Reconstruction](https://arxiv.org/abs/2505.23658)
*Haim Kaplan,Yishay Mansour,Kobbi Nissim,Uri Stemmer*

Main category: cs.CR

TL;DR: The paper introduces a new Bayesian perspective on data reconstruction, proposes a security definition to prevent reconstruction attacks in certain settings, and reinterprets fingerprinting code attacks as membership inference attacks rather than reconstruction attacks.


<details>
  <summary>Details</summary>
Motivation: To provide a new understanding of data reconstruction from a Bayesian perspective and address the limitations of existing security definitions that do not adequately prevent reconstruction attacks. Also, to clarify the nature of fingerprinting code attacks.

Method: Introduce a Bayesian framework for data reconstruction, propose a novel security definition based on this framework, and analyze fingerprinting code attacks within this context to differentiate them as membership inference attacks.

Result: Demonstrates that under the proposed security definition, reconstruction attacks can be provably prevented in certain settings. Shows that fingerprinting code attacks are fundamentally membership inference attacks and that impossibility results derived from these attacks no longer apply if the focus is solely on preventing reconstruction.

Conclusion: The new Bayesian perspective and associated security definition offer a way to prevent data reconstruction attacks in specific scenarios and clarify the distinction between reconstruction and membership inference attacks.

Abstract: We introduce a new Bayesian perspective on the concept of data
reconstruction, and leverage this viewpoint to propose a new security
definition that, in certain settings, provably prevents reconstruction attacks.
We use our paradigm to shed new light on one of the most notorious attacks in
the privacy and memorization literature - fingerprinting code attacks (FPC). We
argue that these attacks are really a form of membership inference attacks,
rather than reconstruction attacks. Furthermore, we show that if the goal is
solely to prevent reconstruction (but not membership inference), then in some
cases the impossibility results derived from FPC no longer apply.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [215] [Towards Explainable Sequential Learning](https://arxiv.org/abs/2505.23624)
*Giacomo Bergami,Emma Packer,Kirsty Scott,Silvia Del Din*

Main category: cs.DB

TL;DR: This paper presents EMeriTAte+DF, a hybrid explainable temporal data processing pipeline that combines numerical-driven and event-based approaches to provide human-explainable results for multivariate time series classifications.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between numerical-driven temporal data classification and event-based classification using verified artificial intelligence principles, enabling more human-understandable explanations of the data processing outcomes.

Method: Developed a hybrid explainable temporal data processing pipeline named EMeriTAte+DF, which uses a preliminary a posteriori explainable phase to describe numerical input data in terms of concurrent constituents with numerical payloads. This involved extending event-based literature to create specification mining algorithms supporting concurrent constituents.

Result: The proposed solutions outperform current state-of-the-art methods for multivariate time series classifications, demonstrating the effectiveness of the methodology.

Conclusion: EMeriTAte+DF successfully bridges numerical-driven and event-based temporal data classification, providing human-explainable results and showcasing significant improvements over existing methods.

Abstract: This paper offers a hybrid explainable temporal data processing pipeline,
DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence
(EMeriTAte+DF), bridging numerical-driven temporal data classification with an
event-based one through verified artificial intelligence principles, enabling
human-explainable results. This was possible through a preliminary a posteriori
explainable phase describing the numerical input data in terms of concurrent
constituents with numerical payloads. This further required extending the
event-based literature to design specification mining algorithms supporting
concurrent constituents. Our previous and current solutions outperform
state-of-the-art solutions for multivariate time series classifications, thus
showcasing the effectiveness of the proposed methodology.

</details>


### [216] [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416)
*Jang-Hyun Kim,Jinuk Kim,Sangwoo Kwon,Jae W. Lee,Sangdoo Yun,Hyun Oh Song*

Main category: cs.DB

TL;DR: Transformer-based large language models (LLMs) face memory overhead and attention latency issues as context length grows. This paper introduces KVzip, a query-agnostic KV cache eviction method that reduces KV cache size by 3-4 times and FlashAttention decoding latency by approximately 2 times with negligible performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the substantial memory overhead and increased attention latency in LLMs as context length grows during inference, which is caused by expanding KV cache sizes.

Method: KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, then evicts pairs with lower importance, enabling effective reuse of compressed KV caches across diverse queries.

Result: KVzip reduces KV cache size by 3-4 times and FlashAttention decoding latency by approximately 2 times. It shows negligible performance loss in various tasks including question-answering, retrieval, reasoning, and code comprehension. It outperforms existing query-aware KV eviction methods, especially under multi-query scenarios.

Conclusion: KVzip is an effective query-agnostic KV cache eviction method that significantly reduces memory usage and latency in LLMs while maintaining high performance.

Abstract: Transformer-based large language models (LLMs) cache context as key-value
(KV) pairs during inference. As context length grows, KV cache sizes expand,
leading to substantial memory overhead and increased attention latency. This
paper introduces KVzip, a query-agnostic KV cache eviction method enabling
effective reuse of compressed KV caches across diverse queries. KVzip
quantifies the importance of a KV pair using the underlying LLM to reconstruct
original contexts from cached KV pairs, subsequently evicting pairs with lower
importance. Extensive empirical evaluations demonstrate that KVzip reduces KV
cache size by 3-4$\times$ and FlashAttention decoding latency by approximately
2$\times$, with negligible performance loss in question-answering, retrieval,
reasoning, and code comprehension tasks. Evaluations include various models
such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching
up to 170K tokens. KVzip significantly outperforms existing query-aware KV
eviction methods, which suffer from performance degradation even at a 90% cache
budget ratio under multi-query scenarios.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [217] [A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors](https://arxiv.org/abs/2505.23344)
*Jakub Martinka,Lina Zhang,Yi-Fan Hou,Mikołaj Martyka,Jiří Pittner,Mario Barbatti,Pavlo O. Dral*

Main category: physics.comp-ph

TL;DR: The paper designs NAC-specific descriptors and a new ML phase-correction procedure to machine learn nonadiabatic couplings (NACs) with high accuracy ($R^2$ exceeding 0.99). Demonstrates the efficiency on FSSH simulations of fulvene.


<details>
  <summary>Details</summary>
Motivation: NACs are important for modeling photochemical and photophysical processes, such as in FSSH. There is a need to accelerate simulations through machine learning of NACs, but it is challenging due to their vectorial, double-valued character and singularities near conical intersections.

Method: Design NAC-specific descriptors based on domain expertise and develop a new ML phase-correction procedure to accurately machine learn NACs. Use these methods to perform fully ML-driven FSSH simulations.

Result: Achieved unprecedented accuracy in learning NACs with $R^2$ exceeding 0.99. The ML-FSSH dynamics accurately described $S_1$ decay and reduced error bars by enabling large ensembles of trajectories.

Conclusion: The newly designed NAC-specific descriptors and ML phase-correction procedure significantly improve the accuracy and efficiency of simulating photochemical processes via ML-driven FSSH.

Abstract: Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical
and photophysical processes with methods such as the widely used
fewest-switches surface hopping (FSSH). There is therefore a strong incentive
to machine learn NACs for accelerating simulations. However, this is
challenging due to NACs' vectorial, double-valued character and the singularity
near a conical intersection seam. For the first time, we design NAC-specific
descriptors based on our domain expertise and show that they allow learning
NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to
success is also our new ML phase-correction procedure. We demonstrate the
efficiency and robustness of our approach on a prototypical example of fully
ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic
structure level. This ML-FSSH dynamics leads to an accurate description of
$S_1$ decay while reducing error bars by allowing the execution of a large
ensemble of trajectories. Our implementations are available in open-source
MLatom.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [218] [A comprehensive analysis of PINNs: Variants, Applications, and Challenges](https://arxiv.org/abs/2505.22761)
*Afila Ajithkumar Sophiya,Akarsh K Nair,Sepehr Maleki,Senthil K. Krishnababu*

Main category: cs.CE

TL;DR: This paper conducts a comprehensive survey on PINNs, including their architecture, application domains, challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to provide a more standardized and comprehensive overview of PINNs approaches, addressing the limitations in existing surveys which either focus on different application scenarios or remain at a superficial level.

Method: Through this survey, the authors explore various aspects related to PINNs' architecture, variants, areas of application, real-world use cases, and challenges. They also analyze the performance of PINNs on different equations and application domains.

Result: The results include a detailed overview of PINNs architecture and variants, a performance analysis across different domains, and a discussion of current issues and future research directions.

Conclusion: This survey successfully bridges the gap in the existing literature by presenting a detailed analysis of all factors related to PINNs combined with recent advancements and state-of-the-art research.

Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful
computational tool for solving differential equations. However, the
applicability of these models is still in its initial stages and requires more
standardization to gain wider popularity. Through this survey, we present a
comprehensive overview of PINNs approaches exploring various aspects related to
their architecture, variants, areas of application, real-world use cases,
challenges, and so on. Even though existing surveys can be identified, they
fail to provide a comprehensive view as they primarily focus on either
different application scenarios or limit their study to a superficial level.
This survey attempts to bridge the gap in the existing literature by presenting
a detailed analysis of all these factors combined with recent advancements and
state-of-the-art research in PINNs. Additionally, we discuss prevalent
challenges in PINNs implementation and present some of the future research
directions as well. The overall contributions of the survey can be summarised
into three sections: A detailed overview of PINNs architecture and variants, a
performance analysis of PINNs on different equations and application domains
highlighting their features. Finally, we present a detailed discussion of
current issues and future research directions.

</details>


### [219] [Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data](https://arxiv.org/abs/2505.22670)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Main category: cs.CE

TL;DR: BIM has transformed the construction industry, but lacks effective methods for capturing complex relationships between components. This study proposes a unified network-based representation method that integrates multi-dimensional design features of BIM models.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks effective methods for capturing the complex spatial and topological relationships between components in BIM models, which are essential for understanding design patterns and enhancing decision-making.

Method: This study proposes a unified network-based representation method that integrates the 'semantic-spatial-topological' multi-dimensional design features of BIM models by extending the IFC standard to include local spatial relationships and topological connections between components.

Result: This representation method enables a more detailed understanding of component interactions, dependencies, and implicit design patterns, effectively capturing the semantic, topological, and spatial relationships in BIM.

Conclusion: The proposed network-based representation holds significant potential for the representation and learning of design patterns.

Abstract: Building Information Modeling (BIM) has revolutionized the construction
industry by providing a comprehensive digital representation of building
structures throughout their lifecycle. However, existing research lacks
effective methods for capturing the complex spatial and topological
relationships between components in BIM models, which are essential for
understanding design patterns and enhancing decision-making. This study
proposes a unified network-based representation method that integrates the
"semantic-spatial-topological" multi-dimensional design features of BIM models.
By extending the IFC (Industry Foundation Classes) standard, we introduce local
spatial relationships and topological connections between components to enrich
the network structure. This representation method enables a more detailed
understanding of component interactions, dependencies, and implicit design
patterns, effectively capturing the semantic, topological, and spatial
relationships in BIM, and holds significant potential for the representation
and learning of design patterns.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [220] [Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats](https://arxiv.org/abs/2505.23706)
*Utku Demir,Yalin E. Sagduyu,Tugba Erpek,Hossein Jafari,Sastry Kompella,Mengran Xue*

Main category: cs.NI

TL;DR: 在车联网中，分布式联邦学习（DFL）通过车辆间协作训练和模型更新传播，显著提高了安全消息分类的准确性。研究发现，DFL特别有助于提高个体准确率较低的车辆的表现，并揭示了本地训练数据大小和网络连通性对整体准确率的影响。然而，在多域攻击下，DFL表现出一定的脆弱性，需要更强大的防御策略。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习方法用于检测恶意或异常行为时，由于依赖集中式数据收集或纯本地训练，在大规模、高移动性和异构数据分布的车联网中面临局限性。

Method: 采用分布式联邦学习（DFL），使车辆通过与一跳邻居交换模型更新并在多跳范围内传播模型来协作训练深度学习模型。使用Vehicular Reference Misbehavior（VeReMi）扩展数据集进行实验验证。

Result: 相比仅使用本地数据的学习，DFL显著提高了所有车辆的分类准确率，特别是个体准确率较低的车辆获得了大幅改善。本地训练数据大小和时间变化的网络连通性与模型的整体准确率高度相关。在无线干扰和训练数据投毒攻击等多域攻击下，DFL显示出其脆弱性。

Conclusion: DFL在提高车联网中安全消息分类准确性方面具有显著优势，但面对多域攻击时存在漏洞，需要开发更鲁棒的安全策略以保障DFL在车联网中的应用。

Abstract: In connected and autonomous vehicles, machine learning for safety message
classification has become critical for detecting malicious or anomalous
behavior. However, conventional approaches that rely on centralized data
collection or purely local training face limitations due to the large scale,
high mobility, and heterogeneous data distributions inherent in inter-vehicle
networks. To overcome these challenges, this paper explores Distributed
Federated Learning (DFL), whereby vehicles collaboratively train deep learning
models by exchanging model updates among one-hop neighbors and propagating
models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)
Extension Dataset, we show that DFL can significantly improve classification
accuracy across all vehicles compared to learning strictly with local data.
Notably, vehicles with low individual accuracy see substantial accuracy gains
through DFL, illustrating the benefit of knowledge sharing across the network.
We further show that local training data size and time-varying network
connectivity correlate strongly with the model's overall accuracy. We
investigate DFL's resilience and vulnerabilities under attacks in multiple
domains, namely wireless jamming and training data poisoning attacks. Our
results reveal important insights into the vulnerabilities of DFL when
confronted with multi-domain attacks, underlining the need for more robust
strategies to secure DFL in vehicular networks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [221] [Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games](https://arxiv.org/abs/2505.22781)
*Antonio Ocello,Daniil Tiapkin,Lorenzo Mancini,Mathieu Laurière,Eric Moulines*

Main category: stat.ML

TL;DR: The paper introduces MF-TRPO, an algorithm that extends TRPO to compute Nash equilibria for Mean-Field Games, providing convergence guarantees and sample complexity analysis.


<details>
  <summary>Details</summary>
Motivation: To develop a robust algorithm for computing Nash equilibria in Mean-Field Games by leveraging the strengths of TRPO from reinforcement learning.

Method: MF-TRPO is designed based on TRPO methodology and adapted for Mean-Field Games, ensuring stability and performance in policy optimization.

Result: Theoretical guarantees on convergence are established for both exact and sample-based versions of MF-TRPO, with high-probability and finite sample complexity.

Conclusion: MF-TRPO bridges RL techniques with mean-field decision-making, offering a solid theoretical foundation for solving complex multi-agent problems.

Abstract: We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel
algorithm designed to compute approximate Nash equilibria for ergodic
Mean-Field Games (MFG) in finite state-action spaces. Building on the
well-established performance of TRPO in the reinforcement learning (RL)
setting, we extend its methodology to the MFG framework, leveraging its
stability and robustness in policy optimization. Under standard assumptions in
the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing
theoretical guarantees on its convergence. Our results cover both the exact
formulation of the algorithm and its sample-based counterpart, where we derive
high-probability guarantees and finite sample complexity. This work advances
MFG optimization by bridging RL techniques with mean-field decision-making,
offering a theoretically grounded approach to solving complex multi-agent
problems.

</details>


### [222] [Highly Efficient and Effective LLMs with Multi-Boolean Architectures](https://arxiv.org/abs/2505.22811)
*Ba-Hien Tran,Van Minh Nguyen*

Main category: stat.ML

TL;DR: This paper presents a new framework that transforms LLMs into multi-kernel Boolean parameters and finetunes them directly in the Boolean domain, reducing complexity and outperforming recent quantization methods.


<details>
  <summary>Details</summary>
Motivation: Current weight binarization approaches for LLMs either lead to significant performance loss or introduce substantial complexity.

Method: The method introduces a novel framework that transforms LLMs into multi-kernel Boolean parameters and finetunes them directly in the Boolean domain, eliminating the need for latent weights.

Result: Through extensive experiments, the method outperforms recent ultra low-bit quantization and binarization methods for LLMs.

Conclusion: This novel approach effectively reduces the complexity of LLMs during both finetuning and inference while improving performance.

Abstract: Weight binarization has emerged as a promising strategy to drastically reduce
the complexity of large language models (LLMs). It is mainly classified into
two approaches: post-training binarization and finetuning with training-aware
binarization methods. The first approach, while having low complexity, leads to
significant loss of information from the original LLMs, resulting in poor
performance. The second approach, on the other hand, relies heavily on
full-precision latent weights for gradient approximation of binary weights,
which not only remains suboptimal but also introduces substantial complexity.
In this paper, we introduce a novel framework that effectively transforms LLMs
into multi-kernel Boolean parameters, for the first time, finetunes them
directly in the Boolean domain, eliminating the need for expensive latent
weights. This significantly reduces complexity during both finetuning and
inference. Through extensive and insightful experiments across a wide range of
LLMs, we demonstrate that our method outperforms recent ultra low-bit
quantization and binarization methods.

</details>


### [223] [Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features](https://arxiv.org/abs/2505.22997)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: The paper presents Deep Copula Classifier (DCC), a generative model that uses neural network-parameterized copulas to capture complex feature dependencies, converging to the Bayes-optimal classifier with explicit rates. Extensions include high-dimensional scalability, semi-supervised learning, and online adaptation.


<details>
  <summary>Details</summary>
Motivation: Traditional classifiers often assume feature independence or overly simplistic relationships, leading to poor performance in scenarios where real-world dependencies are important.

Method: DCC separates the learning of each feature's marginal distribution from the modeling of their joint dependence structure using neural network-parameterized copulas. Lightweight neural networks are used for each class to capture feature interactions flexibly and adaptively.

Result: DCC is particularly effective when classification is driven by complex dependencies. Theoretical guarantees establish convergence to the Bayes-optimal classifier with rates of O(n^{-r/(2r + d)}) for r-smooth copula densities.

Conclusion: DCC provides a mathematically grounded and interpretable framework for dependency-aware classification, unifying statistical rigor with the representational power of neural networks.

Abstract: Traditional classifiers often assume feature independence or rely on overly
simplistic relationships, leading to poor performance in settings where
real-world dependencies matter. We introduce the Deep Copula Classifier (DCC),
a generative model that separates the learning of each feature's marginal
distribution from the modeling of their joint dependence structure via neural
network-parameterized copulas. For each class, lightweight neural networks are
used to flexibly and adaptively capture feature interactions, making DCC
particularly effective when classification is driven by complex dependencies.
We establish that DCC converges to the Bayes-optimal classifier under standard
conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for
r-smooth copula densities. Beyond theoretical guarantees, we outline several
practical extensions, including high-dimensional scalability through vine and
factor copula architectures, semi-supervised learning via entropy
regularization, and online adaptation using streaming gradient methods. By
unifying statistical rigor with the representational power of neural networks,
DCC offers a mathematically grounded and interpretable framework for
dependency-aware classification.

</details>


### [224] [JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows](https://arxiv.org/abs/2505.23196)
*Eshant English,Christoph Lippert*

Main category: stat.ML

TL;DR: JAPAN is a new conformal prediction framework using density-based conformity scores, providing tighter prediction areas and good calibration for complex distributions.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods often use residual-based conformity scores which can be too conservative and fail to capture the true shape of complex, multimodal distributions.

Method: JAPAN leverages flow-based models to estimate predictive density and constructs prediction areas by thresholding on the estimated density scores, allowing for compact, adaptive regions with finite-sample coverage guarantees.

Result: JAPAN demonstrates good calibration and tighter prediction areas compared to existing baselines across multivariate regression and forecasting tasks.

Conclusion: JAPAN offers an efficient conformal prediction framework that addresses limitations of residual-based methods, especially for complex distributions.

Abstract: Conformal prediction provides a model-agnostic framework for uncertainty
quantification with finite-sample validity guarantees, making it an attractive
tool for constructing reliable prediction sets. However, existing approaches
commonly rely on residual-based conformity scores, which impose geometric
constraints and struggle when the underlying distribution is multimodal. In
particular, they tend to produce overly conservative prediction areas centred
around the mean, often failing to capture the true shape of complex predictive
distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction
Areas with Normalising-Flows), a conformal prediction framework that uses
density-based conformity scores. By leveraging flow-based models, JAPAN
estimates the (predictive) density and constructs prediction areas by
thresholding on the estimated density scores, enabling compact, potentially
disjoint, and context-adaptive regions that retain finite-sample coverage
guarantees. We theoretically motivate the efficiency of JAPAN and empirically
validate it across multivariate regression and forecasting tasks, demonstrating
good calibration and tighter prediction areas compared to existing baselines.
We also provide several \emph{extensions} adding flexibility to our proposed
framework.

</details>


### [225] [Stable Thompson Sampling: Valid Inference via Variance Inflation](https://arxiv.org/abs/2505.23260)
*Budhaditya Halder,Shubhayan Pan,Koulik Khamaru*

Main category: stat.ML

TL;DR: An abstract sampling algorithm named Stable Thompson Sampling is proposed to enable valid statistical inference for adaptive decision-making algorithms with modest cost.


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling (TS) has been known as an effective method in statistical inference. However, its adaptive sampling scheme poses challenges for constructing confidence intervals for model parameters.

Method: A variant of TS called Stable Thompson Sampling was proposed where the posterior variance is inflated by a logarithmic factor.

Result: This modification leads to asymptotically normal estimates of the arm means despite the non-i.i.d. nature of the data and increases regret by only a logarithmic factor compared to standard TS.

Conclusion: There exists a trade-off that by paying a small price in regret, one can enable valid statistical inference for adaptive decision-making algorithms.

Abstract: We consider the problem of statistical inference when the data is collected
via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known
to be both asymptotically optimal and empirically effective, its adaptive
sampling scheme poses challenges for constructing confidence intervals for
model parameters. We propose and analyze a variant of TS, called Stable
Thompson Sampling, in which the posterior variance is inflated by a logarithmic
factor. We show that this modification leads to asymptotically normal estimates
of the arm means, despite the non-i.i.d. nature of the data. Importantly, this
statistical benefit comes at a modest cost: the variance inflation increases
regret by only a logarithmic factor compared to standard TS. Our results reveal
a principled trade-off: by paying a small price in regret, one can enable valid
statistical inference for adaptive decision-making algorithms.

</details>


### [226] [The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation](https://arxiv.org/abs/2505.23445)
*Adrien Majka,El-Mahdi El-Mhamdi*

Main category: stat.ML

TL;DR: 在研究目标和代理指标耦合对古德哈特定律影响的工作中，发现轻尾目标和重尾差异情况下，过度优化与目标和指标间差异的重尾性成反比。


<details>
  <summary>Details</summary>
Motivation: 之前的工作为研究古德哈特定律做了简化独立性假设和学习范式假设，本文旨在消除这些假设来研究代理指标和目标间的耦合对古德哈特定律的影响。

Method: 通过消除之前工作中的简化独立性假设和学习范式假设，研究目标和代理指标间的耦合对古德哈特定律的影响。

Result: 结果表明，在轻尾目标和轻尾差异情况下，依赖性不会改变古德哈特定律的本质；但在轻尾目标和重尾差异情况下，存在一个例子表明过度优化的发生率与目标和指标间差异的重尾性成反比。

Conclusion: 在轻尾目标和重尾差异情况下，目标和代理指标间的耦合会导致过度优化，且其发生率与差异的重尾性成反比。

Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a
measure becomes a target, it ceases to be a good measure''. As machine learning
models and the optimisation capacity to train them grow, growing empirical
evidence reinforced the belief in the validity of this law without however
being formalised. Recently, a few attempts were made to formalise Goodhart's
law, either by categorising variants of it, or by looking at how optimising a
proxy metric affects the optimisation of an intended goal. In this work, we
alleviate the simplifying independence assumption, made in previous works, and
the assumption on the learning paradigm made in most of them, to study the
effect of the coupling between the proxy metric and the intended goal on
Goodhart's law. Our results show that in the case of light tailed goal and
light tailed discrepancy, dependence does not change the nature of Goodhart's
effect. However, in the light tailed goal and heavy tailed discrepancy case, we
exhibit an example where over-optimisation occurs at a rate inversely
proportional to the heavy tailedness of the discrepancy between the goal and
the metric. %

</details>


### [227] [Learning Parametric Distributions from Samples and Preferences](https://arxiv.org/abs/2505.23557)
*Marc Jourdan,Gizem Yüce,Nicolas Flammarion*

Main category: stat.ML

TL;DR: Recent advances in language modeling emphasize the role of preference feedback. This paper explores its impact on parameter estimation for continuous parametric distributions, proposing an estimator with improved error scaling and establishing a matching lower bound.


<details>
  <summary>Details</summary>
Motivation: Preference feedback has been shown to enhance model performance in language modeling. The study aims to understand under what conditions preference feedback can improve parameter estimation in continuous parametric distributions.

Method: The learner observes pairs of samples from an unknown distribution along with their relative preferences. Preference-based M-estimators are used, showing better asymptotic variance than sample-only M-estimators. Deterministic preferences further improve this by leveraging hard constraints, leading to an estimation error scaling of $\mathcal{O}(1/n)$.

Result: The proposed estimator achieves an estimation error scaling of $\mathcal{O}(1/n)$, significantly improving upon the $\Theta(1/\sqrt{n})$ rate with samples alone. A matching lower bound is established, demonstrating the optimality of the approach under certain assumptions.

Conclusion: Preference feedback can lead to significant improvements in parameter estimation for certain continuous parametric distributions, although the assumptions required may be restrictive.

Abstract: Recent advances in language modeling have underscored the role of preference
feedback in enhancing model performance. This paper investigates the conditions
under which preference feedback improves parameter estimation in classes of
continuous parametric distributions. In our framework, the learner observes
pairs of samples from an unknown distribution along with their relative
preferences depending on the same unknown parameter. We show that
preference-based M-estimators achieve a better asymptotic variance than
sample-only M-estimators, further improved by deterministic preferences.
Leveraging the hard constraints revealed by deterministic preferences, we
propose an estimator achieving an estimation error scaling of
$\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$
rate attainable with samples alone. Next, we establish a lower bound that
matches this accelerated rate; up to dimension and problem-dependent constants.
While the assumptions underpinning our analysis are restrictive, they are
satisfied by notable cases such as Gaussian or Laplace distributions for
preferences based on the log-probability reward.

</details>


### [228] [Multilook Coherent Imaging: Theoretical Guarantees and Algorithms](https://arxiv.org/abs/2505.23594)
*Xi Chen,Soham Jana,Christopher A. Metzler,Arian Maleki,Shirin Jalali*

Main category: stat.ML

TL;DR: 本文研究了多视相干成像的最大似然估计方法的理论和算法方面，提出了均方误差的理论上限，并通过改进投影梯度下降法提高了实际性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多视相干成像技术在数字全息、超声成像和合成孔径雷达等领域广泛应用，但其理论基础尚未得到充分探索，尤其是如何处理降低图像质量的乘性噪声（即斑点噪声）。

Method: 作者首先建立了最大似然估计器在深度图像先验假设下的均方误差的理论上限，分析了误差与参数数量、视图数量、信号维度和每视图测量次数的关系。在算法方面，采用了投影梯度下降法计算最大似然解，并引入了Newton-Schulz算法以降低计算复杂度，同时开发了一种装袋策略来减少投影误差。

Result: 结合这些技术的投影梯度下降法达到了最先进的性能水平。

Conclusion: 本文为多视相干成像提供了严格的理论和算法框架，不仅明确了最大似然估计器的误差界限，还通过改进的投影梯度下降法提升了实际应用中的性能。

Abstract: Multilook coherent imaging is a widely used technique in applications such as
digital holography, ultrasound imaging, and synthetic aperture radar. A central
challenge in these systems is the presence of multiplicative noise, commonly
known as speckle, which degrades image quality. Despite the widespread use of
coherent imaging systems, their theoretical foundations remain relatively
underexplored. In this paper, we study both the theoretical and algorithmic
aspects of likelihood-based approaches for multilook coherent imaging,
providing a rigorous framework for analysis and method development. Our
theoretical contributions include establishing the first theoretical upper
bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under
the deep image prior hypothesis. Our results capture the dependence of MSE on
the number of parameters in the deep image prior, the number of looks, the
signal dimension, and the number of measurements per look. On the algorithmic
side, we employ projected gradient descent (PGD) as an efficient method for
computing the maximum likelihood solution. Furthermore, we introduce two key
ideas to enhance the practical performance of PGD. First, we incorporate the
Newton-Schulz algorithm to compute matrix inverses within the PGD iterations,
significantly reducing computational complexity. Second, we develop a bagging
strategy to mitigate projection errors introduced during PGD updates. We
demonstrate that combining these techniques with PGD yields state-of-the-art
performance. Our code is available at
https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.

</details>


### [229] [Instance-Optimality for Private KL Distribution Estimation](https://arxiv.org/abs/2505.23620)
*Jiayuan Ye,Vitaly Feldman,Kunal Talwar*

Main category: stat.ML

TL;DR: The paper explores the problem of estimating an unknown discrete distribution with a focus on minimizing KL divergence. It constructs minimax optimal private estimators and then examines instance-optimality, proposing algorithms that achieve this optimality with or without differential privacy constraints.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of minimax optimality in evaluating an algorithm's performance on individual (non-worst-case) instances, the study shifts to an instance-optimality viewpoint for more precise estimation error evaluation.

Method: The research first constructs minimax optimal private estimators, then proposes algorithms achieving instance-optimality up to constant factors, utilizing (private) variants of the Good-Turing estimator.

Result: Algorithms are developed that achieve instance-optimality both with and without differential privacy constraints. Upper and lower bounds for estimation errors are established.

Conclusion: The study successfully provides a framework for instance-optimal distribution estimation under KL divergence, offering improved methods over traditional minimax approaches.

Abstract: We study the fundamental problem of estimating an unknown discrete
distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the
distribution. We are interested in minimizing the KL divergence between the
true distribution and the algorithm's estimate. We first construct minimax
optimal private estimators. Minimax optimality however fails to shed light on
an algorithm's performance on individual (non-worst-case) instances $p$ and
simple minimax-optimal DP estimators can have poor empirical performance on
real distributions. We then study this problem from an instance-optimality
viewpoint, where the algorithm's error on $p$ is compared to the minimum
achievable estimation error over a small local neighborhood of $p$. Under
natural notions of local neighborhood, we propose algorithms that achieve
instance-optimality up to constant factors, with and without a differential
privacy constraint. Our upper bounds rely on (private) variants of the
Good-Turing estimator. Our lower bounds use additive local neighborhoods that
more precisely captures the hardness of distribution estimation in KL
divergence, compared to ones considered in prior works.

</details>


### [230] [On the Convergence Analysis of Muon](https://arxiv.org/abs/2505.23737)
*Wei Shen,Ruichuan Huang,Minhui Huang,Cong Shen,Jiawei Zhang*

Main category: stat.ML

TL;DR: 研究了Muon优化器的收敛性，发现其在低秩和近似块对角结构的Hessian矩阵情况下优于梯度下降法，并通过实验验证了这一理论。


<details>
  <summary>Details</summary>
Motivation: 尽管Muon优化器在神经网络训练中表现出色，但对其收敛行为和优越性能背后的理论理解仍然有限。

Method: 对Muon优化器进行了全面的收敛率分析，并与梯度下降法进行比较，同时刻画了Muon优于梯度下降法的条件。

Result: 理论结果表明，Muon可以从低秩和近似块对角结构的Hessian矩阵中受益，这种现象在实际神经网络训练中广泛存在。实验结果支持并证实了这些理论发现。

Conclusion: Muon优化器在具有低秩和近似块对角结构的Hessian矩阵的情况下表现优于梯度下降法。

Abstract: The majority of parameters in neural networks are naturally represented as
matrices. However, most commonly used optimizers treat these matrix parameters
as flattened vectors during optimization, potentially overlooking their
inherent structural properties. Recently, an optimizer called Muon has been
proposed, specifically designed to optimize matrix-structured parameters.
Extensive empirical evidence shows that Muon can significantly outperform
traditional optimizers when training neural networks. Nonetheless, the
theoretical understanding of Muon's convergence behavior and the reasons behind
its superior performance remain limited. In this work, we present a
comprehensive convergence rate analysis of Muon and its comparison with
Gradient Descent (GD). We further characterize the conditions under which Muon
can outperform GD. Our theoretical results reveal that Muon can benefit from
the low-rank and approximate blockwise diagonal structure of Hessian matrices
-- phenomena widely observed in practical neural network training. Our
experimental results support and corroborate the theoretical findings.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [231] [Learning to Charge More: A Theoretical Study of Collusion by Q-Learning Agents](https://arxiv.org/abs/2505.22909)
*Cristian Chica,Yinglong Guo,Gilad Lerman*

Main category: econ.GN

TL;DR: In infinite repeated games, Q-learning agents may learn to charge supracompetitive prices due to certain conditions on the Q-function and game properties.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical explanation for why Q-learning agents might learn to charge supracompetitive prices in infinite repeated games.

Method: Analyze the conditions under which firms update their pricing policies based solely on observed profits and how this leads to supracompetitive pricing. Introduce new class of one-memory subgame perfect equilibria (SPEs) and examine conditions for learned behavior supported by naive collusion, grim trigger policies, or increasing strategies.

Result: Firms can learn to consistently charge supracompetitive prices when specific inequalities on the Q-function are satisfied at the end of experimentation, given the existence of both a one-stage Nash equilibrium price and a collusive-enabling price.

Conclusion: Naive collusion only constitutes an SPE if the collusive-enabling price is also a one-stage Nash equilibrium; otherwise, grim trigger policies can support such learned behaviors.

Abstract: There is growing experimental evidence that $Q$-learning agents may learn to
charge supracompetitive prices. We provide the first theoretical explanation
for this behavior in infinite repeated games. Firms update their pricing
policies based solely on observed profits, without computing equilibrium
strategies. We show that when the game admits both a one-stage Nash equilibrium
price and a collusive-enabling price, and when the $Q$-function satisfies
certain inequalities at the end of experimentation, firms learn to consistently
charge supracompetitive prices. We introduce a new class of one-memory subgame
perfect equilibria (SPEs) and provide conditions under which learned behavior
is supported by naive collusion, grim trigger policies, or increasing
strategies. Naive collusion does not constitute an SPE unless the
collusive-enabling price is a one-stage Nash equilibrium, whereas grim trigger
policies can.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [232] [An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book](https://arxiv.org/abs/2505.22678)
*Jiahao Yang,Ran Fang,Ming Zhang,Jun Zhou*

Main category: q-fin.TR

TL;DR: In high-frequency trading, modeling stock price movements using limit order books is challenging due to data complexity. This paper proposes a Siamese architecture approach to process ask and bid sides separately with shared parameters, enhancing deep learning models' performance. Integrated with Multi-Head Attention mechanisms, the method improves strong baselines in over 75% of cases, especially effective for shorter forecasting horizons.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the challenge of effectively modeling stock price movements using high-dimensional and volatile limit order book data. Recent deep learning models often fail to capture patterns without well-designed features, prompting the need for a new approach.

Method: The proposed method uses a Siamese architecture to process the ask and bid sides of LOB data separately with shared parameters. It also integrates Multi-Head Attention mechanisms with LSTM modules to model stock price movements. Experiments were conducted on raw data and Order Flow Imbalance features.

Result: The results indicate that the proposed method improves the performance of strong baselines in over 75% of cases. Multi-Head Attention was found to enhance model performance, particularly for shorter forecasting horizons.

Conclusion: The Siamese architecture combined with Multi-Head Attention mechanisms provides an effective way to enhance deep learning models for modeling stock price movements in high-frequency trading.

Abstract: In high-frequency trading (HFT), leveraging limit order books (LOB) to model
stock price movements is crucial for achieving profitable outcomes. However,
this task is challenging due to the high-dimensional and volatile nature of the
original data. Even recent deep learning models often struggle to capture price
movement patterns effectively, particularly without well-designed features. We
observed that raw LOB data exhibits inherent symmetry between the ask and bid
sides, and the bid-ask differences demonstrate greater stability and lower
complexity compared to the original data. Building on this insight, we propose
a novel approach in which leverages the Siamese architecture to enhance the
performance of existing deep learning models. The core idea involves processing
the ask and bid sides separately using the same module with shared parameters.
We applied our Siamese-based methods to several widely used strong baselines
and validated their effectiveness using data from 14 military industry stocks
in the Chinese A-share market. Furthermore, we integrated multi-head attention
(MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate
its role in modeling stock price movements. Our experiments used raw data and
widely used Order Flow Imbalance (OFI) features as input with some strong
baseline models. The results show that our method improves the performance of
strong baselines in over 75$% of cases, excluding the Multi-Layer Perception
(MLP) baseline, which performed poorly and is not considered practical.
Furthermore, we found that Multi-Head Attention can enhance model performance,
particularly over shorter forecasting horizons.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [233] [Distribution free M-estimation](https://arxiv.org/abs/2505.22807)
*John C. Duchi*

Main category: math.ST

TL;DR: The paper explores the solvability of univariate convex M-estimation or stochastic optimization problems without assumptions on data distribution, identifying conditions distinct from classical learnability characterizations.


<details>
  <summary>Details</summary>
Motivation: To delineate statistical problems solvable without assumptions on the underlying data distribution.

Method: Characterizing when a univariate convex M-estimation or stochastic optimization problem is solvable in an assumption-free setting.

Result: Identified conditions for solvability that show Lipschitz continuity of the loss being minimized is not necessary for distribution free minimization.

Conclusion: Provides a precise dividing line between solvable and unsolvable problems in this context.

Abstract: The basic question of delineating those statistical problems that are
solvable without making any assumptions on the underlying data distribution has
long animated statistics and learning theory. This paper characterizes when a
(univariate) convex M-estimation or stochastic optimization problem is solvable
in such an assumption-free setting, providing a precise dividing line between
solvable and unsolvable problems. The conditions we identify show, perhaps
surprisingly, that Lipschitz continuity of the loss being minimized is not
necessary for distribution free minimization, and they are also distinct from
classical characterizations of learnability in machine learning.

</details>


### [234] [Joint estimation of smooth graph signals from partial linear measurements](https://arxiv.org/abs/2505.23240)
*Hemant Tyagi*

Main category: math.ST

TL;DR: 在给定图G和潜在信号的情况下，通过部分线性测量来估计信号，并对均方误差进行了非渐近界分析。当T趋向于无穷时，在特定的图G选择下，即使采样非常严格，该估计器也能达到弱一致性。结果还扩展到了多层排名问题。


<details>
  <summary>Details</summary>
Motivation: 研究者希望在给定部分线性测量值的情况下，估计与图G顶点相关联的潜在信号$x_t$，并探讨其均方误差的非渐近界。特别地，他们关注在极端稀疏采样的情况下，估计器能否保持一致性。

Method: 假设信号相对于图G是平滑的（即信号在图上的二次变化很小），使用平滑惩罚最小二乘估计器联合恢复$x_t$。然后，推导出该估计器的非渐近均方误差界。对于多层排名问题，将结果扩展到每个层$t$通过测量图$G_t$获得带噪声的成对差值测量的情况。

Result: 获得了平滑惩罚最小二乘估计器的非渐近均方误差界。对于某些选择的图G，即使仅在一个坐标上测量了极小比例的顶点，估计器在T趋向于无穷时也表现出弱一致性。此外，在个体$G_t$非常稀疏和不连通的情况下，多层排名问题中的估计器也达到了弱一致性。

Conclusion: 该研究表明，在极端稀疏采样的条件下，平滑惩罚最小二乘估计器可以有效地恢复图G上的潜在信号。这一结果也被推广到多层排名问题中，进一步验证了估计器的有效性。

Abstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each
vertex $t$ has a latent signal $x_t \in \mathbb{R}^n$ associated to it. Given
partial linear measurements of the signals, for a potentially small subset of
the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are
smooth w.r.t $G$, in the sense that the quadratic variation of the signals over
the graph is small, we obtain non-asymptotic bounds on the mean squared error
for jointly recovering $x_t$'s, for the smoothness penalized least squares
estimator. In particular, this implies for certain choices of $G$ that this
estimator is weakly consistent (as $T \rightarrow \infty$) under potentially
very stringent sampling, where only one coordinate is measured per vertex for a
vanishingly small fraction of the vertices. The results are extended to a
``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths
of a collection of $n$ items, and noisy pairwise difference measurements are
obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency
is established for certain choices of $G$ even when the individual $G_t$'s are
very sparse and disconnected.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [235] [Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges](https://arxiv.org/abs/2505.22688)
*Palur Venkata Raghuvamsi,Siyuan Brandon Loh,Prasanta Bhattacharya,Joses Ho,Raphael Lee Tze Chuen,Alvin X. Han,Sebastian Maurer-Stroh*

Main category: q-bio.QM

TL;DR: The paper explores using diverse data types, including biological, public health, and human behavioral features, to predict COVID-19 case surges. It finds varying effectiveness across countries and suggests tailoring models to specific contexts.


<details>
  <summary>Details</summary>
Motivation: To improve the prediction of case surges during pandemics by exploring alternative datasets beyond traditional epidemiological data.

Method: Investigated the usefulness of different feature sets (biological, public health, human behavioral) in predicting country-level case surges of COVID-19.

Result: Biological, public health, and human behavioral features showed relative effectiveness in predicting case surges, with considerable heterogeneity in performance across countries and feature modalities.

Conclusion: Integrating alternative data sources into disease surveillance frameworks can enhance the prediction of pandemic dynamics.

Abstract: The COVID-19 pandemic response relied heavily on statistical and machine
learning models to predict key outcomes such as case prevalence and fatality
rates. These predictions were instrumental in enabling timely public health
interventions that helped break transmission cycles. While most existing models
are grounded in traditional epidemiological data, the potential of alternative
datasets, such as those derived from genomic information and human behavior,
remains underexplored. In the current study, we investigated the usefulness of
diverse modalities of feature sets in predicting case surges. Our results
highlight the relative effectiveness of biological (e.g., mutations), public
health (e.g., case counts, policy interventions) and human behavioral features
(e.g., mobility and social media conversations) in predicting country-level
case surges. Importantly, we uncover considerable heterogeneity in predictive
performance across countries and feature modalities, suggesting that surge
prediction models may need to be tailored to specific national contexts and
pandemic phases. Overall, our work highlights the value of integrating
alternative data sources into existing disease surveillance frameworks to
enhance the prediction of pandemic dynamics.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [236] [Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning](https://arxiv.org/abs/2505.22684)
*Yufeng Wang,Yiguang Bai,Tianqing Zhu,Ismail Ben Ayed,Jing Yuan*

Main category: cs.SI

TL;DR: 社区划分在网络分析中至关重要，模块化优化是最常用的技术。然而，传统的基于模块化的方法常常忽视了现实应用中关键的公平性方面。为了解决这一问题，我们引入了保护组网络并提出了一种新的公平性-模块化度量标准。该度量标准通过明确地融入公平性来扩展传统的模块化，并且我们证明了最小化它能够为保护组产生自然公平的划分，同时保持理论上的合理性。我们开发了一个通用的公平划分优化框架，并设计了高效的公平快速纽曼（FairFN）算法，增强了快速纽曼（FN）方法以优化模块化和公平性。实验表明，与最先进的方法相比，FairFN在不平衡数据集上尤其能够显著提高公平性并获得高质量的划分。


<details>
  <summary>Details</summary>
Motivation: 传统模块化方法忽视了现实应用中关键的公平性方面，需要一种新的方法来解决这个问题。

Method: 引入保护组网络，提出公平性-模块化度量标准，开发通用优化框架并设计FairFN算法。

Result: 实验表明，与现有方法相比，FairFN显著提高了公平性并获得了高质量的划分，特别是在不平衡数据集上。

Conclusion: 提出的公平性-模块化度量标准和FairFN算法有效地解决了社区划分中的公平性问题，具有重要的应用价值。

Abstract: Community partitioning is crucial in network analysis, with modularity
optimization being the prevailing technique. However, traditional
modularity-based methods often overlook fairness, a critical aspect in
real-world applications. To address this, we introduce protected group networks
and propose a novel fairness-modularity metric. This metric extends traditional
modularity by explicitly incorporating fairness, and we prove that minimizing
it yields naturally fair partitions for protected groups while maintaining
theoretical soundness. We develop a general optimization framework for fairness
partitioning and design the efficient Fair Fast Newman (FairFN) algorithm,
enhancing the Fast Newman (FN) method to optimize both modularity and fairness.
Experiments show FairFN achieves significantly improved fairness and
high-quality partitions compared to state-of-the-art methods, especially on
unbalanced datasets.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [237] [In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge](https://arxiv.org/abs/2505.22767)
*Eleni Vasilaki*

Main category: cs.HC

TL;DR: The paper re-frames Large Language Models (LLMs) as dynamic instantiations of Collective human Knowledge (CK), exploring emergent dialogue patterns, fine-tuning implications, and co-augmentation between human and machine cognition.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical and experiential re-framing of LLMs as dynamic instantiations of Collective human Knowledge (CK), shifting from the traditional analysis through architectural, behavioural, or training-data lenses.

Method: Examine emergent dialogue patterns, the implications of fine-tuning, and the notion of co-augmentation by drawing on concepts from neuroscience and AI, and interacting with ChatGPT-4.

Result: This perspective offers a new lens for understanding interaction, representation, and agency in contemporary AI systems.

Conclusion: LLMs can be viewed as evoking intelligence through dialogue rather than storing knowledge statically, leading to mutual enhancement between human and machine cognition.

Abstract: Large Language Models (LLMs) are typically analysed through architectural,
behavioural, or training-data lenses. This article offers a theoretical and
experiential re-framing: LLMs as dynamic instantiations of Collective human
Knowledge (CK), where intelligence is evoked through dialogue rather than
stored statically. Drawing on concepts from neuroscience and AI, and grounded
in sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,
the implications of fine-tuning, and the notion of co-augmentation: mutual
enhancement between human and machine cognition. This perspective offers a new
lens for understanding interaction, representation, and agency in contemporary
AI systems.

</details>


### [238] [Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages](https://arxiv.org/abs/2505.22831)
*Peiling Jiang,Haijun Xia*

Main category: cs.HC

TL;DR: Orca is a prototype web browser that uses AI to augment users' interactions with webpages, treating them as malleable materials for collaborative manipulation and composition, enhancing information foraging, user control, and sensemaking flexibility.


<details>
  <summary>Details</summary>
Motivation: Conventional browsers struggle to handle large volumes of information across pages, and fully automated AI systems can reduce user agency and contextual understanding. There's a need for AI to augment user interaction with web content instead of replacing it.

Method: Through literature review on information tasks and web browsing challenges, followed by an iterative design process, the authors developed Orca, a prototype web browser leveraging AI to support user-driven exploration, operation, organization, and synthesis of web content at scale.

Result: Evaluation showed increased appetite for information foraging, enhanced user control, and more flexibility in making sense of a broader information landscape on the web.

Conclusion: AI-augmented interactions with webpages can significantly improve user experience, offering greater control and flexibility in managing large-scale information across webpages.

Abstract: Web-based activities are fundamentally distributed across webpages. However,
conventional browsers with stacks of tabs fail to support operating and
synthesizing large volumes of information across pages. While recent AI systems
enable fully automated web browsing and information synthesis, they often
diminish user agency and hinder contextual understanding. Therefore, we explore
how AI could instead augment users' interactions with content across webpages
and mitigate cognitive and manual efforts. Through literature on information
tasks and web browsing challenges, and an iterative design process, we present
a rich set of novel interactions with our prototype web browser, Orca.
Leveraging AI, Orca supports user-driven exploration, operation, organization,
and synthesis of web content at scale. To enable browsing at scale, webpages
are treated as malleable materials that humans and AI can collaboratively
manipulate and compose into a malleable, dynamic, and browser-level workspace.
Our evaluation revealed an increased "appetite" for information foraging,
enhanced user control, and more flexibility in sensemaking across a broader
information landscape on the web.

</details>


### [239] [HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding](https://arxiv.org/abs/2505.22906)
*Emmanuel Anaya González,Raven Rothkopf,Sorin Lerner,Nadia Polikarpova*

Main category: cs.HC

TL;DR: 在AI编程工具可能增加程序员能力和生产力的同时，它们也可能使用户过度依赖系统提供的解决方案。为解决这一问题，本文提出了Human-in-the-loop Decoding（人类参与解码）技术，并通过HiLDe代码补全助手实现。该技术让用户能够观察和直接影响LLM决策，从而减少漏洞生成并更好地实现用户的个人需求。实验表明，与传统代码补全助手相比，HiLDe能显著减少安全相关任务中的漏洞并更好地满足用户目标。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编程工具有潜力显著提升程序员的能力和生产力，但它们往往将用户排除在关键决策过程之外，导致用户过度依赖系统提供的解决方案，这在软件安全性等关键领域可能会产生严重后果。

Method: 提出了一种名为Human-in-the-loop Decoding（人类参与解码）的新型交互技术，并将其应用于HiLDe代码补全助手。该技术允许用户观察和直接影响大型语言模型（LLM）在代码生成过程中的决策，并提供本地替代方案供用户探索。

Result: 通过一项包含18名参与者的组内研究，在与安全性相关的任务中发现，使用HiLDe的参与者生成的漏洞显著减少，并且代码生成更能符合其目标，相较于传统的代码补全助手表现更优。

Conclusion: Human-in-the-loop Decoding技术可以有效提高代码生成的安全性和用户满意度，建议在未来的AI编程工具中采用类似方法以增强用户对代码生成过程的控制力和参与度。

Abstract: While AI programming tools hold the promise of increasing programmers'
capabilities and productivity to a remarkable degree, they often exclude users
from essential decision-making processes, causing many to effectively "turn off
their brains" and over-rely on solutions provided by these systems. These
behaviors can have severe consequences in critical domains, like software
security. We propose Human-in-the-loop Decoding, a novel interaction technique
that allows users to observe and directly influence LLM decisions during code
generation, in order to align the model's output with their personal
requirements. We implement this technique in HiLDe, a code completion assistant
that highlights critical decisions made by the LLM and provides local
alternatives for the user to explore. In a within-subjects study (N=18) on
security-related tasks, we found that HiLDe led participants to generate
significantly fewer vulnerabilities and better align code generation with their
goals compared to a traditional code completion assistant.

</details>


### [240] [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
*Boning Zhao*

Main category: cs.HC

TL;DR: This paper presents Human Empathy as Encoder (HEAE), a novel AI framework that integrates student narratives and teacher-derived empathy vectors to assess depression severity in special education settings, achieving 82.74% classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Standardized questionnaires and automated methods struggle to capture the nuanced understanding of student depression provided by teachers' empathetic connections with students in sensitive educational environments.

Method: The HEAE framework combines student narrative text with a 9-dimensional 'Empathy Vector' derived from teachers, based on the PHQ-9 framework, to create a structured input for AI while preserving human judgment.

Result: The framework achieved 82.74% accuracy in classifying depression severity across seven levels through rigorous experimentation optimizing multimodal fusion, text representation, and classification architecture.

Conclusion: This work shows promise for more responsible and ethical affective computing by embedding human empathy into AI systems for mental health assessment.

Abstract: Assessing student depression in sensitive environments like special education
is challenging. Standardized questionnaires may not fully reflect students'
true situations. Furthermore, automated methods often falter with rich student
narratives, lacking the crucial, individualized insights stemming from
teachers' empathetic connections with students. Existing methods often fail to
address this ambiguity or effectively integrate educator understanding. To
address these limitations by fostering a synergistic human-AI collaboration,
this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered
AI framework for transparent and socially responsible depression severity
assessment. Our approach uniquely integrates student narrative text with a
teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by
the PHQ-9 framework,to explicitly translate tacit empathetic insight into a
structured AI input enhancing rather than replacing human judgment. Rigorous
experiments optimized the multimodal fusion, text representation, and
classification architecture, achieving 82.74% accuracy for 7-level severity
classification. This work demonstrates a path toward more responsible and
ethical affective computing by structurally embedding human empathy

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [241] [Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera](https://arxiv.org/abs/2505.22880)
*Xiaoyang Zhan,Shixin Zhou,Qianqian Yang,Yixuan Zhao,Hao Liu,Srinivas Chowdary Ramineni,Kenji Shimada*

Main category: cs.RO

TL;DR: This paper presents a system for autonomous semantic exploration and dense semantic target mapping using a ground robot equipped with a LiDAR-panoramic camera suite.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal.

Method: The paper proposes a complete system combining mapping and planning, redefining the task as completing both geometric coverage and semantic viewpoint observation. It manages semantic and geometric viewpoints separately and proposes a Priority-driven Decoupled Local Sampler to generate local viewpoint sets. A hierarchical planner is developed to ensure efficient global coverage and a Safe Aggressive Exploration State Machine is proposed to allow aggressive exploration while ensuring safety.

Result: Simulation results show that the planner achieves faster exploration and shorter travel distances while guaranteeing multi-view inspections. Real-world experiments confirm the system's effectiveness in accurate dense semantic object mapping of unstructured environments.

Conclusion: The system successfully combines mapping and planning to achieve autonomous semantic exploration and dense semantic target mapping in complex unknown environments.

Abstract: This paper presents a system for autonomous semantic exploration and dense
semantic target mapping of a complex unknown environment using a ground robot
equipped with a LiDAR-panoramic camera suite. Existing approaches often
struggle to balance collecting high-quality observations from multiple view
angles and avoiding unnecessary repetitive traversal. To fill this gap, we
propose a complete system combining mapping and planning. We first redefine the
task as completing both geometric coverage and semantic viewpoint observation.
We then manage semantic and geometric viewpoints separately and propose a novel
Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This
enables explicit multi-view semantic inspection and voxel coverage without
unnecessary repetition. Building on this, we develop a hierarchical planner to
ensure efficient global coverage. In addition, we propose a Safe Aggressive
Exploration State Machine, which allows aggressive exploration behavior while
ensuring the robot's safety. Our system includes a plug-and-play semantic
target mapping module that integrates seamlessly with state-of-the-art SLAM
algorithms for pointcloud-level dense semantic target mapping. We validate our
approach through extensive experiments in both realistic simulations and
complex real-world environments. Simulation results show that our planner
achieves faster exploration and shorter travel distances while guaranteeing a
specified number of multi-view inspections. Real-world experiments further
confirm the system's effectiveness in achieving accurate dense semantic object
mapping of unstructured environments.

</details>


### [242] [VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation](https://arxiv.org/abs/2505.23267)
*Jianlin Ye,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.RO

TL;DR: This paper proposes VLM-RRT, a new path planning method for UAVs that combines Vision Language Models with Rapidly-exploring Random Trees to improve sampling efficiency and path quality.


<details>
  <summary>Details</summary>
Motivation: Traditional path planning methods like RRT have limitations including high search space complexity, suboptimal path quality, and slow convergence which are problematic in critical applications such as disaster response.

Method: The proposed method, VLM-RRT, integrates Vision Language Models (VLMs) with RRT. VLMs provide initial directional guidance based on environmental snapshots, biasing sampling towards regions more likely to contain feasible paths.

Result: Extensive experiments with state-of-the-art VLMs show the effectiveness of VLM-RRT in improving sampling efficiency and path quality.

Conclusion: VLM-RRT is a promising approach to enhance path-planning efficiency for UAVs by leveraging the strengths of both VLMs and RRT.

Abstract: Path planning is a fundamental capability of autonomous Unmanned Aerial
Vehicles (UAVs), enabling them to efficiently navigate toward a target region
or explore complex environments while avoiding obstacles. Traditional
pathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven
effective but often encounter significant challenges. These include high search
space complexity, suboptimal path quality, and slow convergence, issues that
are particularly problematic in high-stakes applications like disaster
response, where rapid and efficient planning is critical. To address these
limitations and enhance path-planning efficiency, we propose Vision Language
Model RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition
capabilities of Vision Language Models (VLMs) with the path-planning strengths
of RRT. By leveraging VLMs to provide initial directional guidance based on
environmental snapshots, our method biases sampling toward regions more likely
to contain feasible paths, significantly improving sampling efficiency and path
quality. Extensive quantitative and qualitative experiments with various
state-of-the-art VLMs demonstrate the effectiveness of this proposed approach.

</details>


### [243] [A Robot-Assisted Approach to Small Talk Training for Adults with ASD](https://arxiv.org/abs/2505.23508)
*Rebecca Ramnauth,Dražen Brščić,Brian Scassellati*

Main category: cs.RO

TL;DR: A autonomous robot system for adults with ASD to practice small talk is developed and evaluated, showing positive results in conversation initiation and eye contact improvement.


<details>
  <summary>Details</summary>
Motivation: Small talk is a crucial social skill that can be particularly challenging for adults with Autism Spectrum Disorder (ASD). Improvement in this area is essential for their social integration, relationship building, and professional opportunities.

Method: An in-home autonomous robot system was developed to allow users, specifically adults with ASD, to practice small talk. A week-long study was conducted to evaluate the system.

Result: Participants enjoyed the training, made progress in initiating conversations and improving eye contact, and viewed the system as valuable for enhancing conversational skills.

Conclusion: The autonomous robot system is effective in helping adults with ASD improve their small talk abilities.

Abstract: From dating to job interviews, making new friends or simply chatting with the
cashier at checkout, engaging in small talk is a vital, everyday social skill.
For adults with Autism Spectrum Disorder (ASD), small talk can be particularly
challenging, yet it is essential for social integration, building
relationships, and accessing professional opportunities. In this study, we
present our development and evaluation of an in-home autonomous robot system
that allows users to practice small talk. Results from the week-long study show
that adults with ASD enjoyed the training, made notable progress in initiating
conversations and improving eye contact, and viewed the system as a valuable
tool for enhancing their conversational skills.

</details>


### [244] [Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms](https://arxiv.org/abs/2505.23576)
*Jane Cleland-Huang,Pedro Antonio Alarcon Granadeno,Arturo Miguel Russell Bernal,Demetrius Hernandez,Michael Murphy,Maureen Petterson,Walter Scheirer*

Main category: cs.RO

TL;DR: Incorporating large language models (LLMs) into small Uncrewed Aerial Systems (sUAS) swarms for search-and-rescue missions to enhance object recognition and decision-making, while ensuring safe operations through cognitive guardrails.


<details>
  <summary>Details</summary>
Motivation: Traditional computer vision systems struggle with recognizing unfamiliar objects in open-world environments and inferring their relevance for mission planning in autonomous sUAS swarms used in search-and-rescue scenarios.

Method: Integrate large language models (LLMs) with sUAS swarms to reason about detected objects and their implications, while implementing cognitive guardrails to prevent hallucinations and ensure safe and sensible decision-making.

Result: The design, simulation, and real-world integration of cognitive guardrails for sUAS swarms in search-and-rescue missions was successfully presented, enhancing the reliability and safety of autonomous systems.

Conclusion: Using LLMs alongside cognitive guardrails can improve the performance of sUAS swarms in complex environments, providing better insights and safer operations during search-and-rescue missions.

Abstract: Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous
swarms in search-and-rescue and other disaster-response scenarios. In these
settings, they use computer vision (CV) to detect objects of interest and
autonomously adapt their missions. However, traditional CV systems often
struggle to recognize unfamiliar objects in open-world environments or to infer
their relevance for mission planning. To address this, we incorporate large
language models (LLMs) to reason about detected objects and their implications.
While LLMs can offer valuable insights, they are also prone to hallucinations
and may produce incorrect, misleading, or unsafe recommendations. To ensure
safe and sensible decision-making under uncertainty, high-level decisions must
be governed by cognitive guardrails. This article presents the design,
simulation, and real-world integration of these guardrails for sUAS swarms in
search-and-rescue missions.

</details>


### [245] [Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation](https://arxiv.org/abs/2505.22805)
*Siddharth Ancha,Sunshine Jiang,Travis Manderson,Laura Brandt,Yilun Du,Philip R. Osteen,Nicholas Roy*

Main category: cs.RO

TL;DR: The paper proposes an analysis-by-synthesis method using generative diffusion models for pixel-wise anomaly detection in off-road robot navigation without assuming OOD data characteristics.


<details>
  <summary>Details</summary>
Motivation: To enable robots to navigate safely and reliably in off-road and unstructured environments, it is crucial to detect anomalies that differ from the training data.

Method: An analysis-by-synthesis approach is used where a generative diffusion model synthesizes an edited image removing anomalies while keeping the rest unchanged. Anomaly detection is formulated as analyzing which segments were modified by the model. A novel inference approach for guided diffusion is proposed, which analyzes ideal guidance gradients and derives a principled approximation.

Result: The editing technique can be integrated into existing workflows at test-time without retraining or fine-tuning. Combining vision-language foundation models allows semantically meaningful edits to be detected, leading to accurate anomaly detection for off-road navigation.

Conclusion: This method offers a way to perform pixel-wise anomaly detection without assumptions about OOD data, enhancing safe and reliable off-road robot navigation.

Abstract: In order to navigate safely and reliably in off-road and unstructured
environments, robots must detect anomalies that are out-of-distribution (OOD)
with respect to the training data. We present an analysis-by-synthesis approach
for pixel-wise anomaly detection without making any assumptions about the
nature of OOD data. Given an input image, we use a generative diffusion model
to synthesize an edited image that removes anomalies while keeping the
remaining image unchanged. Then, we formulate anomaly detection as analyzing
which image segments were modified by the diffusion model. We propose a novel
inference approach for guided diffusion by analyzing the ideal guidance
gradient and deriving a principled approximation that bootstraps the diffusion
model to predict guidance gradients. Our editing technique is purely test-time
that can be integrated into existing workflows without the need for retraining
or fine-tuning. Finally, we use a combination of vision-language foundation
models to compare pixels in a learned feature space and detect semantically
meaningful edits, enabling accurate anomaly detection for off-road navigation.
Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/

</details>


### [246] [Learning coordinated badminton skills for legged manipulators](https://arxiv.org/abs/2505.22974)
*Yuntao Ma,Andrei Cramariuc,Farbod Farshidian,Marco Hutter*

Main category: cs.RO

TL;DR: This paper presents a unified reinforcement learning-based control policy for legged mobile manipulators to play badminton, validated through extensive experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of coordinating limb motion and aligning control with perception in dynamic environments for robotics.

Method: The method involves a unified reinforcement learning-based control policy for whole-body visuomotor skills, incorporating a perception noise model, shuttlecock prediction model, constrained reinforcement learning, and system identification techniques.

Result: The robot successfully predicts shuttlecock trajectories, navigates effectively, and executes precise strikes against human players in various environments.

Conclusion: The study demonstrates the feasibility of using legged mobile manipulators in complex and dynamic sports scenarios.

Abstract: Coordinating the motion between lower and upper limbs and aligning limb
control with perception are substantial challenges in robotics, particularly in
dynamic environments. To this end, we introduce an approach for enabling legged
mobile manipulators to play badminton, a task that requires precise
coordination of perception, locomotion, and arm swinging. We propose a unified
reinforcement learning-based control policy for whole-body visuomotor skills
involving all degrees of freedom to achieve effective shuttlecock tracking and
striking. This policy is informed by a perception noise model that utilizes
real-world camera data, allowing for consistent perception error levels between
simulation and deployment and encouraging learned active perception behaviors.
Our method includes a shuttlecock prediction model, constrained reinforcement
learning for robust motion control, and integrated system identification
techniques to enhance deployment readiness. Extensive experimental results in a
variety of environments validate the robot's capability to predict shuttlecock
trajectories, navigate the service area effectively, and execute precise
strikes against human players, demonstrating the feasibility of using legged
mobile manipulators in complex and dynamic sports scenarios.

</details>


### [247] [Mobi-$π$: Mobilizing Your Robot Learning Policy](https://arxiv.org/abs/2505.23692)
*Jingyun Yang,Isabella Huang,Brandon Vu,Max Bajracharya,Rika Antonova,Jeannette Bohg*

Main category: cs.RO

TL;DR: Learned visuomotor policies struggle with generalization to novel robot positions. This paper introduces policy mobilization, a method that finds suitable mobile robot base poses for manipulation tasks without retraining the policy. The Mobi-$\pi$ framework is introduced for studying this problem, and a novel approach outperforms baselines in aligning the robot's base pose with an in-distribution pose.


<details>
  <summary>Details</summary>
Motivation: Current learned visuomotor policies have poor generalization to new robot positions due to limited training data from fixed viewpoints, restricting their use on mobile platforms for precise tasks.

Method: The policy mobilization problem is formulated to find a mobile robot base pose that matches the distribution of a manipulation policy trained on limited viewpoints. The Mobi-$\pi$ framework includes metrics, simulated tasks, visualization tools, and baseline methods. A novel approach optimizes the robot's base pose using 3D Gaussian Splatting, a score function, and sampling-based optimization.

Result: The proposed approach surpasses baselines in both simulation and real-world environments, proving its effectiveness for policy mobilization.

Conclusion: Policy mobilization decouples navigation from manipulation, enhancing the use of learned visuomotor policies on mobile platforms without requiring additional retraining.

Abstract: Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [248] [ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging](https://arxiv.org/abs/2505.22683)
*Xuhang Chen,Michael Kwok-Po Ng,Kim-Fung Tsang,Chi-Man Pun,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: ConnectomeDiffuser是一种新的基于扩散的框架，用于从DTI自动构建端到端脑网络，结合了模板网络、扩散模型和图卷积网络分类器，以提高诊断准确性和对神经退行性疾病的治疗监测。


<details>
  <summary>Details</summary>
Motivation: 现有的用于从扩散张量成像（DTI）构建结构脑网络的方法存在固有限制：操作员主观性、劳动密集型工作流程以及捕获复杂拓扑特征和疾病特定生物标志物的能力有限。

Method: ConnectomeDiffuser结合了三个关键组件：1) 模板网络，使用黎曼几何原理从3D DTI扫描中提取拓扑特征；2) 扩散模型，生成具有增强拓扑保真度的综合脑网络；3) 图卷积网络分类器，结合疾病特定标记以提高诊断准确性。

Result: 实验验证表明，在代表两种不同神经退行性疾病条件的数据集上，该方法显著优于其他脑网络方法。

Conclusion: 这项工作通过提供一个强大且可推广的测量框架，有助于推动神经退行性疾病背景下仪器设备的发展，从而促进更准确的诊断、更深入的机制理解和更好的治疗监测。

Abstract: Brain network analysis plays a crucial role in diagnosing and monitoring
neurodegenerative disorders such as Alzheimer's disease (AD). Existing
approaches for constructing structural brain networks from diffusion tensor
imaging (DTI) often rely on specialized toolkits that suffer from inherent
limitations: operator subjectivity, labor-intensive workflows, and restricted
capacity to capture complex topological features and disease-specific
biomarkers. To overcome these challenges and advance computational neuroimaging
instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based
framework for automated end-to-end brain network construction from DTI. The
proposed model combines three key components: (1) a Template Network that
extracts topological features from 3D DTI scans using Riemannian geometric
principles, (2) a diffusion model that generates comprehensive brain networks
with enhanced topological fidelity, and (3) a Graph Convolutional Network
classifier that incorporates disease-specific markers to improve diagnostic
accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a
broader range of structural connectivity and pathology-related information,
enabling more sensitive analysis of individual variations in brain networks.
Experimental validation on datasets representing two distinct neurodegenerative
conditions demonstrates significant performance improvements over other brain
network methods. This work contributes to the advancement of instrumentation in
the context of neurological disorders, providing clinicians and researchers
with a robust, generalizable measurement framework that facilitates more
accurate diagnosis, deeper mechanistic understanding, and improved therapeutic
monitoring of neurodegenerative diseases such as AD.

</details>


### [249] [Self-orthogonalizing attractor neural networks emerging from the free energy principle](https://arxiv.org/abs/2505.22749)
*Tamas Spisak,Karl Friston*

Main category: q-bio.NC

TL;DR: This paper uses the free energy principle to formalize attractor networks in random dynamical systems, leading to a multi-level Bayesian active inference process that enhances generalization and mutual information. It provides insights for AI and neuroscience.


<details>
  <summary>Details</summary>
Motivation: To understand how self-organizing dynamics emerge in complex systems like the brain, which is crucial for neuronal computations and artificial intelligence.

Method: The authors apply the free energy principle to a universal partitioning of random dynamical systems to formalize attractor networks. This approach eliminates the need for explicitly imposed learning and inference rules.

Result: The proposed networks favor orthogonalized attractor representations, enhancing generalization and mutual information. Sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics.

Conclusion: The findings present a unifying theory of self-organizing attractor networks, offering novel insights for both AI and neuroscience.

Abstract: Attractor dynamics are a hallmark of many complex systems, including the
brain. Understanding how such self-organizing dynamics emerge from first
principles is crucial for advancing our understanding of neuronal computations
and the design of artificial intelligence systems. Here we formalize how
attractor networks emerge from the free energy principle applied to a universal
partitioning of random dynamical systems. Our approach obviates the need for
explicitly imposed learning and inference rules and identifies emergent, but
efficient and biologically plausible inference and learning dynamics for such
self-organizing systems. These result in a collective, multi-level Bayesian
active inference process. Attractors on the free energy landscape encode prior
beliefs; inference integrates sensory data into posterior beliefs; and learning
fine-tunes couplings to minimize long-term surprise. Analytically and via
simulations, we establish that the proposed networks favor approximately
orthogonalized attractor representations, a consequence of simultaneously
optimizing predictive accuracy and model complexity. These attractors
efficiently span the input subspace, enhancing generalization and the mutual
information between hidden causes and observable effects. Furthermore, while
random data presentation leads to symmetric and sparse couplings, sequential
data fosters asymmetric couplings and non-equilibrium steady-state dynamics,
offering a natural extension to conventional Boltzmann Machines. Our findings
offer a unifying theory of self-organizing attractor networks, providing novel
insights for AI and neuroscience.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [250] [Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side](https://arxiv.org/abs/2505.23733)
*Truong,Luu,Binny M. Samuel*

Main category: cs.CY

TL;DR: The paper uses psychological frameworks and interrupted time series analysis to provide empirical evidence on the connection between generative AI and cybercrime, aiming to inform strategies for policymakers, developers, and the public.


<details>
  <summary>Details</summary>
Motivation: To empirically examine the association of generative AI with malicious internet-related activities and cybercrime using psychological frameworks of technological amplification and affordance theory.

Method: Employing a quasi-experimental design with interrupted time series analysis on two datasets (general and cryptocurrency-focused) to assess generative AI's role in cybercrime.

Result: Provides empirical evidence linking generative AI to cybercrime, contributing to discussions on AI governance.

Conclusion: Stresses the need for balanced strategies that guide policymakers, inform developers and cybersecurity professionals, and educate the public to maximize AI benefits while mitigating risks.

Abstract: In recent years, the rapid advancement and democratization of generative AI
models have sparked significant debate over safety, ethical risks, and dual-use
concerns, particularly in the context of cybersecurity. While anecdotally
known, this paper provides empirical evidence regarding generative AI's
association with malicious internet-related activities and cybercrime by
examining the phenomenon through psychological frameworks of technological
amplification and affordance theory. Using a quasi-experimental design with
interrupted time series analysis, we analyze two datasets, one general and one
cryptocurrency-focused, to empirically assess generative AI's role in
cybercrime. The findings contribute to ongoing discussions about AI governance
by balancing control and fostering innovation, underscoring the need for
strategies to guide policymakers, inform AI developers and cybersecurity
professionals, and educate the public to maximize AI's benefits while
mitigating its risks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [251] [Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks](https://arxiv.org/abs/2505.23192)
*Run Hao,Peng Ying*

Main category: cs.CV

TL;DR: The paper presents an automated adversarial prompt generation framework that can evade AIGC detectors, which ranked first in a competition and can be used for creating adversarial datasets.


<details>
  <summary>Details</summary>
Motivation: With the development of text-to-image models, there is an increasing concern about identity misuse and the robustness of AIGC detectors. Thus, there is a need to develop methods to test and improve these detectors.

Method: The method proposed uses a grammar tree structure along with a variant of the Monte Carlo tree search algorithm to explore the semantic prompt space, generating diverse and controllable prompts that can bypass AIGC detectors.

Result: Experiments across multiple T2I models demonstrate the effectiveness of the approach, even ranking first in a real-world adversarial AIGC detection competition.

Conclusion: This framework not only serves as a tool for evading AIGC detectors but also provides high-quality adversarial datasets that are useful for training and evaluating more robust AIGC detection and defense systems.

Abstract: The rise of text-to-image (T2I) models has enabled the synthesis of
photorealistic human portraits, raising serious concerns about identity misuse
and the robustness of AIGC detectors. In this work, we propose an automated
adversarial prompt generation framework that leverages a grammar tree structure
and a variant of the Monte Carlo tree search algorithm to systematically
explore the semantic prompt space. Our method generates diverse, controllable
prompts that consistently evade both open-source and commercial AIGC detectors.
Extensive experiments across multiple T2I models validate its effectiveness,
and the approach ranked first in a real-world adversarial AIGC detection
competition. Beyond attack scenarios, our method can also be used to construct
high-quality adversarial datasets, providing valuable resources for training
and evaluating more robust AIGC detection and defense systems.

</details>


### [252] [MIAS-SAM: Medical Image Anomaly Segmentation without thresholding](https://arxiv.org/abs/2505.22762)
*Marco Colussi,Dragan Ahmetovic,Sergio Mascetti*

Main category: cs.CV

TL;DR: This paper presents MIAS-SAM, a novel approach for the segmentation of anomalous regions in medical images which uses a patch-based memory bank and SAM encoder to accurately segment anomalies without requiring a threshold value. Experimental results on three datasets show accurate anomaly segmentation capabilities measured using DICE score.


<details>
  <summary>Details</summary>
Motivation: To develop a new method for the segmentation of anomalous regions in medical images that does not require defining a threshold value from the anomaly map.

Method: MIAS-SAM uses a patch-based memory bank to store relevant image features extracted from normal data using the SAM encoder. At inference time, embedding patches extracted from the SAM encoder are compared with those in the memory bank to obtain the anomaly map. The center of gravity of the anomaly map is then computed to prompt the SAM decoder, obtaining an accurate segmentation from the previously extracted features.

Result: Experimental results conducted on three publicly available datasets (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score.

Conclusion: MIAS-SAM is a novel approach for the segmentation of anomalous regions in medical images that does not require a threshold value to obtain the segmentation from the anomaly map. It shows accurate anomaly segmentation capabilities across different imaging modalities.

Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of
anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to
store relevant image features, which are extracted from normal data using the
SAM encoder. At inference time, the embedding patches extracted from the SAM
encoder are compared with those in the memory bank to obtain the anomaly map.
Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt
the SAM decoder, obtaining an accurate segmentation from the previously
extracted features. Differently from prior works, MIAS-SAM does not require to
define a threshold value to obtain the segmentation from the anomaly map.
Experimental results conducted on three publicly available datasets, each with
a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show
accurate anomaly segmentation capabilities measured using DICE score. The code
is available at: https://github.com/warpcut/MIAS-SAM

</details>


### [253] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/abs/2505.22815)
*Zhangyi Hu,Jiemin Wu,Hua Xu,Mingqian Liao,Ninghui Feng,Bo Gao,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: The paper proposes VIMTS, a framework adapting Visual Mask AutoEncoder (MAE) for Irregular Multivariate Time Series (IMTS) forecasting. It processes IMTS data into feature patches, uses learned cross-channel dependencies, leverages visual MAE for patch reconstruction and integrates self-supervised learning. Experiments show superior performance and few-shot capability.


<details>
  <summary>Details</summary>
Motivation: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to unaligned multi-channel signals and extensive missing data. Existing methods struggle with such data, especially when there are significant missing values. The success of pre-trained foundation models in other domains motivates the exploration of similar techniques for IMTS forecasting.

Method: VIMTS first processes IMTS along the timeline into feature patches at equal intervals, then complements these patches using learned cross-channel dependencies. It leverages the visual Mask AutoEncoder's (MAE) ability to handle sparse multichannel data for patch reconstruction and applies a coarse-to-fine technique to generate precise predictions. Self-supervised learning is integrated to improve IMTS modeling by adapting visual MAE to IMTS data.

Result: Extensive experiments demonstrate that VIMTS shows superior performance compared to existing methods in IMTS forecasting tasks. It also exhibits strong few-shot learning capabilities, advancing the application of visual foundation models in more general time series tasks.

Conclusion: VIMTS effectively addresses the challenges posed by IMTS forecasting through its innovative adaptation of visual Mask AutoEncoder technology. The framework demonstrates superior performance and strong few-shot learning abilities, contributing significantly to the broader application of visual foundation models in time series analysis.

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [254] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/abs/2505.22944)
*Angtian Wang,Haibin Huang,Jacob Zhiyuan Fang,Yiding Yang,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出了一种统一框架，通过轨迹输入实现视频生成中的运动控制，包括摄像机移动、物体级平移和精细局部运动。该方法通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间，提供比现有方法更好的可控性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法通常分别处理不同类型的运动（如摄像机移动、物体运动等），缺乏一个统一且无缝集成的解决方案。

Method: 该方法设计了一个统一框架，使用基于轨迹的输入来控制视频生成中的运动。通过轻量级运动注入器，将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间，从而实现对局部变形、整体物体运动和虚拟摄像机动态的控制。

Result: 实验表明，该方法在多种视频运动控制任务中表现出色，包括动态视点变化和精确的局部运动操控，提供了显著优于先前方法和商业解决方案的可控性和视觉质量。

Conclusion: 该框架为视频生成中的运动控制提供了一种新的统一且高效的解决方案，具有广泛的兼容性，并可应用于多种视频生成任务。

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [255] [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/abs/2505.22976)
*Kewei Lian,Shaofei Cai,Yilun Du,Yitao Liang*

Main category: cs.CV

TL;DR: 为了推动具有空间一致性的世界模型的发展，本文构建了一个数据集和相应的基准测试。该数据集通过在Minecraft的开放世界环境中采样150个不同位置，收集了约250小时（2000万帧）基于循环的导航视频和动作。数据集设计了序列长度的课程，允许模型在越来越复杂的导航轨迹上学习空间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前没有专门用于促进记忆模块发展的数据集，这些模块需要显式地强制执行空间一致性约束。现有的基准测试主要强调视觉连贯性或生成质量，而忽略了长期的空间一致性。

Method: 在Minecraft的开放世界环境中采样150个不同位置，收集约250小时（2000万帧）基于循环的导航视频和动作，构建数据集和基准测试。数据集采用序列长度的课程设计，使模型能够在复杂度递增的导航轨迹上学到空间一致性。

Result: 四个代表性世界模型基线在新构建的基准测试上进行了评估，结果表明该数据集能有效推动具有空间一致性的世界模型的发展。

Conclusion: 本文构建的数据集和基准测试填补了现有研究中的空白，为未来研究提供了支持，并且数据收集管道易于扩展到新的Minecraft环境和模块。

Abstract: The ability to simulate the world in a spatially consistent manner is a
crucial requirements for effective world models. Such a model enables
high-quality visual generation, and also ensures the reliability of world
models for downstream tasks such as simulation and planning. Designing a memory
module is a crucial component for addressing spatial consistency: such a model
must not only retain long-horizon observational information, but also enables
the construction of explicit or implicit internal spatial representations.
However, there are no dataset designed to promote the development of memory
modules by explicitly enforcing spatial consistency constraints. Furthermore,
most existing benchmarks primarily emphasize visual coherence or generation
quality, neglecting the requirement of long-range spatial consistency. To
bridge this gap, we construct a dataset and corresponding benchmark by sampling
150 distinct locations within the open-world environment of Minecraft,
collecting about 250 hours (20 million frames) of loop-based navigation videos
with actions. Our dataset follows a curriculum design of sequence lengths,
allowing models to learn spatial consistency on increasingly complex navigation
trajectories. Furthermore, our data collection pipeline is easily extensible to
new Minecraft environments and modules. Four representative world model
baselines are evaluated on our benchmark. Dataset, benchmark, and code are
open-sourced to support future research.

</details>


### [256] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
*Jonathan Li,Zoltan Csaki,Nidhi Hiremath,Etash Guha,Fenglu Hong,Edward Ma,Urmish Thakker*

Main category: cs.CV

TL;DR: The paper introduces Hungarian document VQA datasets (HuDocVQA and HuDocVQA-manual) to address the challenge of lower resource languages in DocVQA, along with a dataset for Hungarian OCR (HuCCPDF). Fine-tuning on these datasets improved Llama 3.2 11B Instruct accuracy by +7.2%.


<details>
  <summary>Details</summary>
Motivation: Modern VLMs have nearly saturated accuracy in English document visual question-answering but still struggle with lower resource languages due to lack of suitable training and evaluation data.

Method: Created two Hungarian document VQA datasets - HuDocVQA (larger, synthetically generated) and HuDocVQA-manual (smaller, manually curated). Applied quality filtering and deduplication to HuDocVQA. Also introduced HuCCPDF, a dataset of Hungarian Common Crawl PDFs for training Hungarian OCR models.

Result: Fine-tuning on a mixture of these datasets improved accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%.

Conclusion: The datasets and code will be publicly released to promote further research in multilingual DocVQA.

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [257] [Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation](https://arxiv.org/abs/2505.23043)
*Jihai Zhang,Tianle Li,Linjie Li,Zhengyuan Yang,Yu Cheng*

Main category: cs.CV

TL;DR: Recent advancements in unified vision-language models (VLMs) have attracted significant attention. This paper systematically investigates the generalization across understanding and generation tasks in unified VLMs.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored hypothesis that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation in prior works on unified VLMs.

Method: Design a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations, evaluate multiple unified VLM architectures.

Result: 1. Unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, and this mutual benefits can scale up with increased data; 2. Better alignment between multimodal input and output spaces will lead to better generalization; 3. The knowledge acquired during generation tasks can transfer to understanding tasks, and this cross-task generalization occurs within the base language model, beyond modality adapters.

Conclusion: The findings underscore the critical necessity of unifying understanding and generation in VLMs, offering valuable insights for the design and optimization of unified VLMs.

Abstract: Recent advancements in unified vision-language models (VLMs), which integrate
both visual understanding and generation capabilities, have attracted
significant attention. The underlying hypothesis is that a unified architecture
with mixed training on both understanding and generation tasks can enable
mutual enhancement between understanding and generation. However, this
hypothesis remains underexplored in prior works on unified VLMs. To address
this gap, this paper systematically investigates the generalization across
understanding and generation tasks in unified VLMs. Specifically, we design a
dataset closely aligned with real-world scenarios to facilitate extensive
experiments and quantitative evaluations. We evaluate multiple unified VLM
architectures to validate our findings. Our key findings are as follows. First,
unified VLMs trained with mixed data exhibit mutual benefits in understanding
and generation tasks across various architectures, and this mutual benefits can
scale up with increased data. Second, better alignment between multimodal input
and output spaces will lead to better generalization. Third, the knowledge
acquired during generation tasks can transfer to understanding tasks, and this
cross-task generalization occurs within the base language model, beyond
modality adapters. Our findings underscore the critical necessity of unifying
understanding and generation in VLMs, offering valuable insights for the design
and optimization of unified VLMs.

</details>


### [258] [Multi-Sourced Compositional Generalization in Visual Question Answering](https://arxiv.org/abs/2505.23045)
*Chuanhao Li,Wenbo Ye,Zhen Li,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: The paper explores multi-sourced compositional generalization (MSCG) in visual question answering, proposing a retrieval-augmented training framework to enhance MSCG ability and constructing a GQA-MSCG dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Compositional generalization is crucial in vision-and-language tasks, but the specific challenge of multi-sourced compositional generalization has not been addressed yet.

Method: A retrieval-augmented training framework is proposed where semantically equivalent primitives are retrieved for each primitive in training samples and their features are aggregated to refine model representations across different modalities.

Result: The experimental results on the newly constructed GQA-MSCG dataset show the effectiveness of the proposed framework.

Conclusion: The retrieval-augmented training framework enhances multi-sourced compositional generalization in VQA models, as demonstrated by experiments on the GQA-MSCG dataset.

Abstract: Compositional generalization is the ability of generalizing novel
compositions from seen primitives, and has received much attention in
vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L
tasks, the primitives composing compositions source from different modalities,
resulting in multi-sourced novel compositions. However, the generalization
ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced
compositional generalization (MSCG) remains unexplored. In this paper, we
explore MSCG in the context of visual question answering (VQA), and propose a
retrieval-augmented training framework to enhance the MSCG ability of VQA
models by learning unified representations for primitives from different
modalities. Specifically, semantically equivalent primitives are retrieved for
each primitive in the training samples, and the retrieved features are
aggregated with the original primitive to refine the model. This process helps
the model learn consistent representations for the same semantic primitives
across different modalities. To evaluate the MSCG ability of VQA models, we
construct a new GQA-MSCG dataset based on the GQA dataset, in which samples
include three types of novel compositions composed of primitives from different
modalities. Experimental results demonstrate the effectiveness of the proposed
framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.

</details>


### [259] [GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](https://arxiv.org/abs/2505.23085)
*Gwanghyun Kim,Xueting Li,Ye Yuan,Koki Nagano,Tianye Li,Jan Kautz,Se Young Chun,Umar Iqbal*

Main category: cs.CV

TL;DR: GeoMan是一种新架构，用于从单目人类视频中生成准确且时间一致的深度和法线估计。通过结合图像模型和视频扩散模型，并引入根相对深度表示，GeoMan在3D人体几何估计方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要针对单幅图像进行优化，在处理视频时容易出现时间不一致性和无法捕捉精细动态细节的问题。为了解决这些问题，需要一种新的架构来生成准确且时间一致的深度和法线估计。

Method: GeoMan采用图像模型估计视频第一帧的深度和法线，然后使用这些结果条件化一个视频扩散模型，将视频几何估计任务转化为图像到视频生成问题。此外，还引入了根相对深度表示以更准确地估计人体尺寸。

Result: GeoMan在定性和定量评估中均达到了最先进的性能，证明了其在克服3D人体几何估计长期挑战方面的有效性。

Conclusion: GeoMan通过简化视频模型的角色并引入新的深度表示，显著提高了时间一致性和泛化能力，同时减少了对4D训练数据的需求。

Abstract: Estimating accurate and temporally consistent 3D human geometry from videos
is a challenging problem in computer vision. Existing methods, primarily
optimized for single images, often suffer from temporal inconsistencies and
fail to capture fine-grained dynamic details. To address these limitations, we
present GeoMan, a novel architecture designed to produce accurate and
temporally consistent depth and normal estimations from monocular human videos.
GeoMan addresses two key challenges: the scarcity of high-quality 4D training
data and the need for metric depth estimation to accurately model human size.
To overcome the first challenge, GeoMan employs an image-based model to
estimate depth and normals for the first frame of a video, which then
conditions a video diffusion model, reframing video geometry estimation task as
an image-to-video generation problem. This design offloads the heavy lifting of
geometric estimation to the image model and simplifies the video model's role
to focus on intricate details while using priors learned from large-scale video
datasets. Consequently, GeoMan improves temporal consistency and
generalizability while requiring minimal 4D training data. To address the
challenge of accurate human size estimation, we introduce a root-relative depth
representation that retains critical human-scale details and is easier to be
estimated from monocular inputs, overcoming the limitations of traditional
affine-invariant and metric depth representations. GeoMan achieves
state-of-the-art performance in both qualitative and quantitative evaluations,
demonstrating its effectiveness in overcoming longstanding challenges in 3D
human geometry estimation from videos.

</details>


### [260] [EAD: An EEG Adapter for Automated Classification](https://arxiv.org/abs/2505.23107)
*Pushapdeep Singh,Jyoti Nigam,Medicherla Vamsi Krishna,Arnav Bhavsar,Aditya Nigam*

Main category: cs.CV

TL;DR: 提出了一种名为EEG Adapter (EAD)的灵活框架，该框架与任何信号采集设备兼容，并在两个公开数据集上达到了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）数据获取通常特定于任务，且分类步骤依赖于EEG通道数量，不同采集设备的数据难以使用同一管道处理，因此需要一种能够学习EEG嵌入的框架来应对多样的EEG样本和通道数。

Method: 提出了EEG Adapter (EAD)，一个灵活且与任何信号采集设备兼容的框架，通过利用最近的EEG基础模型并进行重大调整，以从EEG数据中学习稳健的表示用于分类任务。

Result: 在两个公开数据集EEG-ImageNet和BrainLat上分别达到99.33%和92.31%的准确率，并展示了在EEG-ImageNet任务上的零样本分类能力，证明了框架的有效性和泛化能力。

Conclusion: EEG Adapter (EAD)框架在多样化的EEG数据集上表现出色，适用于不同的感知任务，如刺激和静息状态EEG信号分类。

Abstract: While electroencephalography (EEG) has been a popular modality for neural
decoding, it often involves task specific acquisition of the EEG data. This
poses challenges for the development of a unified pipeline to learn embeddings
for various EEG signal classification, which is often involved in various
decoding tasks. Traditionally, EEG classification involves the step of signal
preprocessing and the use of deep learning techniques, which are highly
dependent on the number of EEG channels in each sample. However, the same
pipeline cannot be applied even if the EEG data is collected for the same
experiment but with different acquisition devices. This necessitates the
development of a framework for learning EEG embeddings, which could be highly
beneficial for tasks involving multiple EEG samples for the same task but with
varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a
flexible framework compatible with any signal acquisition device. More
specifically, we leverage a recent EEG foundational model with significant
adaptations to learn robust representations from the EEG data for the
classification task. We evaluate EAD on two publicly available datasets
achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and
BrainLat respectively. This illustrates the effectiveness of the proposed
framework across diverse EEG datasets containing two different perception
tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG
classification on EEG-ImageNet task to demonstrate the generalization
capability of the proposed approach.

</details>


### [261] [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](https://arxiv.org/abs/2505.23134)
*Tongtong Su,Chengyu Wang,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: This paper presents Zero-to-Hero, a novel reference-based video editing approach that disentangles the editing process into two distinct problems for overcoming limitations of existing text-guided methods. It provides zero-shot initialization ensuring accuracy and temporal consistency but faces imaging degradation issues. The Hero-Stage learns a conditional generative model for video restoration. This method outperforms the best baseline with a PSNR improvement of 2.6 dB.


<details>
  <summary>Details</summary>
Motivation: Appearance editing according to user needs is crucial in video editing, but existing text-guided methods often lead to ambiguities regarding user intentions and restrict fine-grained control over editing specific aspects of objects.

Method: The paper introduces Zero-to-Hero which focuses on reference-based video editing. It edits an anchor frame to satisfy user requirements as a reference image and then consistently propagates its appearance across other frames. Correspondence within the original frames guides the attention mechanism for robustness.

Result: It offers a solid zero-shot initialization ensuring both accuracy and temporal consistency. However, there are issues with compounded imaging degradation including over-saturated colors and unknown blurring. The Hero-Stage improves this by learning a conditional generative model for video restoration.

Conclusion: Zero-to-Hero outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. A set of videos with multiple appearances was constructed using Blender for evaluation.

Abstract: Appearance editing according to user needs is a pivotal task in video
editing. Existing text-guided methods often lead to ambiguities regarding user
intentions and restrict fine-grained control over editing specific aspects of
objects. To overcome these limitations, this paper introduces a novel approach
named {Zero-to-Hero}, which focuses on reference-based video editing that
disentangles the editing process into two distinct problems. It achieves this
by first editing an anchor frame to satisfy user requirements as a reference
image and then consistently propagating its appearance across other frames. We
leverage correspondence within the original frames to guide the attention
mechanism, which is more robust than previously proposed optical flow or
temporal modules in memory-friendly video generative models, especially when
dealing with objects exhibiting large motions. It offers a solid ZERO-shot
initialization that ensures both accuracy and temporal consistency. However,
intervention in the attention mechanism results in compounded imaging
degradation with over-saturated colors and unknown blurring issues. Starting
from Zero-Stage, our Hero-Stage Holistically learns a conditional generative
model for vidEo RestOration. To accurately evaluate the consistency of the
appearance, we construct a set of videos with multiple appearances using
Blender, enabling a fine-grained and deterministic evaluation. Our method
outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The
project page is at https://github.com/Tonniia/Zero2Hero.

</details>


### [262] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/abs/2505.23145)
*Jeongsol Kim,Yeobin Hong,Jong Chul Ye*

Main category: cs.CV

TL;DR: FlowAlign是一种新的无反转流式框架，通过引入流匹配损失来实现一致的图像编辑和原则性的轨迹控制，支持正向和反向编辑，实验表明其在源保持和编辑可控性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无反转、基于流的图像编辑方法（如FlowEdit）虽然避免了精确的潜在反转，但常常导致不稳定的编辑轨迹和较差的源一致性。

Method: 提出了一种名为FlowAlign的无反转流式框架，该框架引入了流匹配损失作为正则化机制，以促进编辑过程中更平滑和稳定的轨迹。流匹配损失能够在编辑轨迹中显式平衡语义对齐和源图像的结构一致性。此外，FlowAlign通过简单地反转ODE轨迹自然支持反向编辑。

Result: 广泛的实验证明，FlowAlign在源保持和编辑可控性方面优于现有方法。

Conclusion: FlowAlign解决了现有无反转方法在编辑轨迹不稳定和源一致性差的问题，通过流匹配损失实现了更平滑的轨迹，并且支持可逆编辑，提升了图像编辑的一致性和可控性。

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit
leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,
enabling text-driven manipulation by solving an ordinary differential equation
(ODE). While the lack of exact latent inversion is a core advantage of these
methods, it often results in unstable editing trajectories and poor source
consistency. To address this limitation, we propose FlowAlign, a novel
inversion-free flow-based framework for consistent image editing with
principled trajectory control. FlowAlign introduces a flow-matching loss as a
regularization mechanism to promote smoother and more stable trajectories
during the editing process. Notably, the flow-matching loss is shown to
explicitly balance semantic alignment with the edit prompt and structural
consistency with the source image along the trajectory. Furthermore, FlowAlign
naturally supports reverse editing by simply reversing the ODE trajectory,
highlighting the reversible and consistent nature of the transformation.
Extensive experiments demonstrate that FlowAlign outperforms existing methods
in both source preservation and editing controllability.

</details>


### [263] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/abs/2505.23161)
*Antonio D'Orazio,Maria Rosaria Briglia,Donato Crisostomi,Dario Loi,Emanuele Rodolà,Iacopo Masi*

Main category: cs.CV

TL;DR: 本研究展示了一种仅使用CLIP模型（无需解码器、训练或微调）实现图像合成的方法。通过优化一种频率感知的隐式神经表示，并引入对抗鲁棒初始化、正交Procustes投影和混合损失等技术，稳定了从共享嵌入空间到图像的逆映射。该框架在不改变CLIP权重的情况下，实现了文本到图像生成、风格迁移和图像重建等功能，揭示了判别模型可能具有的未开发生成潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP作为一个判别模型被广泛用于生成任务的骨干网络，但目前这些生成任务通常需要额外的解码器。研究者希望探索是否可以直接利用CLIP本身进行图像生成，而无需额外的解码器或训练过程。

Method: 研究提出了一种优化方法，即频率感知的隐式神经表示，鼓励粗略到精细的生成过程。为稳定逆映射，引入了对抗鲁棒初始化、轻量级正交Procustes投影以及混合损失函数，后者将输出锚定到自然图像统计上。整个过程无需修改CLIP的权重。

Result: 实验表明，该方法可以成功地在不改变CLIP权重的情况下实现多种生成任务，包括文本到图像生成、风格迁移和图像重建。这证明了CLIP本身具有生成潜力。

Conclusion: 判别模型如CLIP可能隐藏着未被发掘的生成能力，未来的研究可以进一步探索如何充分利用这些模型的生成潜力，而不依赖额外的生成组件。

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [264] [SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection](https://arxiv.org/abs/2505.23214)
*Wenhao Xu,Shuchen Zheng,Changwei Wang,Zherui Zhang,Chuan Ren,Rongtao Xu,Shibiao Xu*

Main category: cs.CV

TL;DR: SAMamba is a new framework for infrared small target detection that integrates hierarchical feature learning with selective sequence modeling, featuring innovations like FS-Adapter, CSI module, and DPCF module to address domain gap, detail preservation, and global context modeling.


<details>
  <summary>Details</summary>
Motivation: Infrared small target detection faces challenges such as targets occupying less than 0.15% of the image and low distinguishability from complex backgrounds. Existing deep learning methods suffer from information loss during downsampling and inefficient global context modeling.

Method: SAMamba combines SAM2's hierarchical feature learning with Mamba's selective sequence modeling. It includes a Feature Selection Adapter (FS-Adapter) for domain adaptation via dual-stage selection, a Cross-Channel State-Space Interaction (CSI) module for global context modeling, and a Detail-Preserving Contextual Fusion (DPCF) module for combining multi-scale features.

Result: Experiments on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets demonstrate that SAMamba significantly outperforms state-of-the-art methods, particularly in challenging scenarios with heterogeneous backgrounds and varying target scales.

Conclusion: SAMamba successfully addresses key challenges in infrared small target detection by bridging the domain gap, preserving fine-grained details, and efficiently modeling long-range dependencies.

Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance
in military, maritime, and early warning applications. ISTD is challenged by
targets occupying less than 0.15% of the image and low distinguishability from
complex backgrounds. Existing deep learning methods often suffer from
information loss during downsampling and inefficient global context modeling.
This paper presents SAMamba, a novel framework integrating SAM2's hierarchical
feature learning with Mamba's selective sequence modeling. Key innovations
include: (1) A Feature Selection Adapter (FS-Adapter) for efficient
natural-to-infrared domain adaptation via dual-stage selection (token-level
with a learnable task embedding and channel-wise adaptive transformations); (2)
A Cross-Channel State-Space Interaction (CSI) module for efficient global
context modeling with linear complexity using selective state space modeling;
and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively
combines multi-scale features with a gating mechanism to balance
high-resolution and low-resolution feature contributions. SAMamba addresses
core ISTD challenges by bridging the domain gap, maintaining fine-grained
details, and efficiently modeling long-range dependencies. Experiments on
NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly
outperforms state-of-the-art methods, especially in challenging scenarios with
heterogeneous backgrounds and varying target scales. Code:
https://github.com/zhengshuchen/SAMamba.

</details>


### [265] [Unsupervised Transcript-assisted Video Summarization and Highlight Detection](https://arxiv.org/abs/2505.23268)
*Spyros Barbakos,Charalampos Antoniadis,Gerasimos Potamianos,Gianluca Setti*

Main category: cs.CV

TL;DR: 本研究提出了一种多模态管道，利用视频帧和转录本来生成更精简的视频版本并检测高光时刻，通过强化学习框架进行训练，该框架奖励生成多样且具有代表性的摘要，同时确保包含具有有意义转录内容的视频片段。实验表明，结合转录本进行视频摘要和高光检测比仅依赖视频视觉内容效果更好。


<details>
  <summary>Details</summary>
Motivation: 观看完整视频可能很乏味，因此需要一种方法来识别关键视频片段，即视频摘要和高光检测。目前还没有工作在强化学习框架内整合视频帧和转录本这两种模态。

Method: 提出了一种多模态管道，利用视频帧和转录本生成更精简的视频版本并检测高光时刻，通过强化学习框架进行训练，奖励生成多样且具有代表性的摘要，同时确保包含具有有意义转录内容的视频片段。这种方法是无监督的，可以从大规模未标注数据集中学习。

Result: 实验表明，使用转录本进行视频摘要和高光检测比仅依赖视频视觉内容的效果更好。

Conclusion: 结合视频帧和转录本的多模态方法可以生成更有效的视频摘要和高光检测结果，无监督训练方法克服了现有标注数据集规模有限的挑战。

Abstract: Video consumption is a key part of daily life, but watching entire videos can
be tedious. To address this, researchers have explored video summarization and
highlight detection to identify key video segments. While some works combine
video frames and transcripts, and others tackle video summarization and
highlight detection using Reinforcement Learning (RL), no existing work, to the
best of our knowledge, integrates both modalities within an RL framework. In
this paper, we propose a multimodal pipeline that leverages video frames and
their corresponding transcripts to generate a more condensed version of the
video and detect highlights using a modality fusion mechanism. The pipeline is
trained within an RL framework, which rewards the model for generating diverse
and representative summaries while ensuring the inclusion of video segments
with meaningful transcript content. The unsupervised nature of the training
allows for learning from large-scale unannotated datasets, overcoming the
challenge posed by the limited size of existing annotated datasets. Our
experiments show that using the transcript in video summarization and highlight
detection achieves superior results compared to relying solely on the visual
content of the video.

</details>


### [266] [Federated Unsupervised Semantic Segmentation](https://arxiv.org/abs/2505.23292)
*Evangelos Charalampakis,Vasileios Mygdalis,Ioannis Pitas*

Main category: cs.CV

TL;DR: FUSS is a new framework for Federated Unsupervised image Semantic Segmentation, promoting global consistency in feature and prototype space.


<details>
  <summary>Details</summary>
Motivation: To explore the application of Federated Learning (FL) in Unsupervised Semantic image Segmentation (USS), addressing the difficulty of feature representation and cluster centroid alignment across distributed clients under heterogeneous data distributions.

Method: Propose FUSS which introduces novel federation strategies that jointly optimize local segmentation heads and shared semantic centroids to promote global consistency in feature and prototype space.

Result: Experiments on benchmark and real-world datasets show FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions.

Conclusion: FUSS is the first framework to enable fully decentralized, label-free semantic segmentation training and shows superior performance.

Abstract: This work explores the application of Federated Learning (FL) in Unsupervised
Semantic image Segmentation (USS). Recent USS methods extract pixel-level
features using frozen visual foundation models and refine them through
self-supervised objectives that encourage semantic grouping. These features are
then grouped to semantic clusters to produce segmentation masks. Extending
these ideas to federated settings requires feature representation and cluster
centroid alignment across distributed clients -- an inherently difficult task
under heterogeneous data distributions in the absence of supervision. To
address this, we propose FUSS Federated Unsupervised image Semantic
Segmentation) which is, to our knowledge, the first framework to enable fully
decentralized, label-free semantic segmentation training. FUSS introduces novel
federation strategies that promote global consistency in feature and prototype
space, jointly optimizing local segmentation heads and shared semantic
centroids. Experiments on both benchmark and real-world datasets, including
binary and multi-class segmentation tasks, show that FUSS consistently
outperforms local-only client trainings as well as extensions of classical FL
algorithms under varying client data distributions. To support reproducibility,
full code will be released upon manuscript acceptance.

</details>


### [267] [Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition](https://arxiv.org/abs/2505.23313)
*Weizhe Kong,Xiao Wang,Ruichong Gao,Chenglong Li,Yu Zhang,Xing Yang,Yaowei Wang,Jin Tang*

Main category: cs.CV

TL;DR: This paper proposes the first adversarial attack and defense framework for pedestrian attribute recognition, demonstrating its effectiveness through extensive experiments.


<details>
  <summary>Details</summary>
Motivation: Despite the progress in pedestrian attribute recognition (PAR) with deep neural networks, the potential vulnerability and anti-interference ability have not been fully explored.

Method: The paper uses a CLIP-based PAR framework where input images are divided into patches and embedded into feature embeddings. Attributes are expanded into sentences and embedded using a pre-trained CLIP text encoder. A multi-modal Transformer fuses vision and text tokens, while a feed-forward network is used for attribute recognition. Adversarial semantic and label-perturbation generate adversarial noise (ASL-PAR). A semantic offset defense strategy is also designed.

Result: Extensive experiments on both digital (PETA, PA100K, MSP60K, RAPv2) and physical domains validate the effectiveness of the proposed adversarial attack and defense strategies.

Conclusion: The authors present an adversarial attack and defense framework for PAR, demonstrating its effectiveness across various datasets.

Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in
human-centered research and has made great progress in recent years with the
development of deep neural networks. However, the potential vulnerability and
anti-interference ability have still not been fully explored. To bridge this
gap, this paper proposes the first adversarial attack and defense framework for
pedestrian attribute recognition. Specifically, we exploit both global- and
patch-level attacks on the pedestrian images, based on the pre-trained
CLIP-based PAR framework. It first divides the input pedestrian image into
non-overlapping patches and embeds them into feature embeddings using a
projection layer. Meanwhile, the attribute set is expanded into sentences using
prompts and embedded into attribute features using a pre-trained CLIP text
encoder. A multi-modal Transformer is adopted to fuse the obtained vision and
text tokens, and a feed-forward network is utilized for attribute recognition.
Based on the aforementioned PAR framework, we adopt the adversarial semantic
and label-perturbation to generate the adversarial noise, termed ASL-PAR. We
also design a semantic offset defense strategy to suppress the influence of
adversarial attacks. Extensive experiments conducted on both digital domains
(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the
effectiveness of our proposed adversarial attack and defense strategies for the
pedestrian attribute recognition. The source code of this paper will be
released on https://github.com/Event-AHU/OpenPAR.

</details>


### [268] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/abs/2505.23331)
*Matteo Gallici,Haitz Sáez de Ocáriz Borde*

Main category: cs.CV

TL;DR: This paper explores using Group Relative Policy Optimization (GRPO) to fine-tune visual autoregressive (VAR) models via reinforcement learning, achieving better image quality and style control while allowing VAR models to generalize beyond their pre-training data.


<details>
  <summary>Details</summary>
Motivation: To align outputs of generative models more closely with human preferences by leveraging reinforcement learning techniques specifically for next-scale visual autoregressive models.

Method: Fine-tuning VAR models using GRPO within a reinforcement learning framework, incorporating aesthetic predictors and CLIP embeddings as reward signals.

Result: Significant enhancement in image quality and precise control over generation style, with the ability for VAR models to generalize beyond ImageNet distribution.

Conclusion: RL-based fine-tuning is efficient and effective for VAR models, particularly benefiting from their fast inference speeds.

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [269] [PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening](https://arxiv.org/abs/2505.23367)
*Jeonghyeok Do,Sungpyo Kim,Geunhyuk Youk,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: PAN-Crafter is a new framework for PAN-sharpening that addresses cross-modality misalignment through modality-consistent alignment, outperforming state-of-the-art methods in metrics, speed, and memory usage.


<details>
  <summary>Details</summary>
Motivation: Conventional deep learning methods for PAN-sharpening assume perfect pixel-wise alignment and rely on per-pixel reconstruction losses, leading to spectral distortion, double edges, and blurring when dealing with cross-modality misalignment.

Method: The method involves PAN-Crafter, a modality-consistent alignment framework with two core components: Modality-Adaptive Reconstruction (MARs) which enables joint reconstruction of HRMS and PAN images using PAN's high-frequency details as auxiliary self-supervision, and Cross-Modality Alignment-Aware Attention (CM3A) which bidirectionally aligns MS texture to PAN structure and vice versa for adaptive feature refinement.

Result: PAN-Crafter outperforms the most recent state-of-the-art method in all metrics on multiple benchmark datasets, with faster inference time and smaller memory size. It also demonstrates strong generalization performance on unseen satellite datasets.

Conclusion: PAN-Crafter effectively mitigates cross-modality misalignment in PAN-sharpening, providing superior performance, efficiency, and robustness across different conditions.

Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multi-spectral (MS) images to generate high-resolution
multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused
by sensor placement, acquisition timing, and resolution disparity -- induces a
fundamental challenge. Conventional deep learning methods assume perfect
pixel-wise alignment and rely on per-pixel reconstruction losses, leading to
spectral distortion, double edges, and blurring when misalignment is present.
To address this, we propose PAN-Crafter, a modality-consistent alignment
framework that explicitly mitigates the misalignment gap between PAN and MS
modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a
single network to jointly reconstruct HRMS and PAN images, leveraging PAN's
high-frequency details as auxiliary self-supervision. Additionally, we
introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism
that bidirectionally aligns MS texture to PAN structure and vice versa,
enabling adaptive feature refinement across modalities. Extensive experiments
on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the
most recent state-of-the-art method in all metrics, even with 50.11$\times$
faster inference time and 0.63$\times$ the memory size. Furthermore, it
demonstrates strong generalization performance on unseen satellite datasets,
showing its robustness across different conditions.

</details>


### [270] [VModA: An Effective Framework for Adaptive NSFW Image Moderation](https://arxiv.org/abs/2505.23386)
*Han Bao,Qinying Wang,Zhi Chen,Qingming Li,Xuhong Zhang,Changjiang Li,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 提出了一种名为VModA的框架，用于更有效地检测NSFW内容，解决了当前方法在复杂语义图像和多样化审核规则下的不足。实验表明，该框架显著提高了检测准确率，并对公共数据集中的标签问题进行了修正。


<details>
  <summary>Details</summary>
Motivation: 当前的NSFW内容检测方法难以应对复杂的图像语义和多样化的审核规则，导致检测偏差和准确性下降。

Method: 提出了一个通用且有效的框架VModA，可以适应不同的审核规则并处理具有复杂语义的NSFW内容。

Result: 实验结果显示VModA在各类NSFW内容上的准确率提高了54.3%，并且在不同类别、场景和基础视觉语言模型上表现出强大的适应性。

Conclusion: VModA显著提升了NSFW内容的检测效果，并对公共数据集中的标签问题进行了改进，增强了其实际应用价值。

Abstract: Not Safe/Suitable for Work (NSFW) content is rampant on social networks and
poses serious harm to citizens, especially minors. Current detection methods
mainly rely on deep learning-based image recognition and classification.
However, NSFW images are now presented in increasingly sophisticated ways,
often using image details and complex semantics to obscure their true nature or
attract more views. Although still understandable to humans, these images often
evade existing detection methods, posing a significant threat. Further
complicating the issue, varying regulations across platforms and regions create
additional challenges for effective moderation, leading to detection bias and
reduced accuracy. To address this, we propose VModA, a general and effective
framework that adapts to diverse moderation rules and handles complex,
semantically rich NSFW content across categories. Experimental results show
that VModA significantly outperforms existing methods, achieving up to a 54.3%
accuracy improvement across NSFW types, including those with complex semantics.
Further experiments demonstrate that our method exhibits strong adaptability
across categories, scenarios, and base VLMs. We also identified inconsistent
and controversial label samples in public NSFW benchmark datasets, re-annotated
them, and submitted corrections to the original maintainers. Two datasets have
confirmed the updates so far. Additionally, we evaluate VModA in real-world
scenarios to demonstrate its practical effectiveness.

</details>


### [271] [Video Editing for Audio-Visual Dubbing](https://arxiv.org/abs/2505.23406)
*Binyamin Manela,Sharon Gannot,Ethan Fetyaya*

Main category: cs.CV

TL;DR: This paper introduces EdiDub, a new framework that reformulates visual dubbing as a content-aware editing task, preserving original video context and improving lip synchronization and identity preservation. It outperforms existing methods in benchmarks and human evaluations.


<details>
  <summary>Details</summary>
Motivation: Visual dubbing is crucial for making content accessible across different languages, but current methods either generate talking faces or use inpainting techniques that discard important visual information.

Method: EdiDub reformulates visual dubbing as a content-aware editing task, using a specialized conditioning scheme to ensure faithful modifications while preserving the original video context.

Result: EdiDub significantly improves identity preservation and synchronization on multiple benchmarks, including an occluded-lip dataset, and achieves higher scores in human evaluations compared to leading methods.

Conclusion: The content-aware editing approach of EdiDub outperforms traditional generation or inpainting methods, particularly in maintaining complex visual elements while ensuring accurate lip synchronization.

Abstract: Visual dubbing, the synchronization of facial movements with new speech, is
crucial for making content accessible across different languages, enabling
broader global reach. However, current methods face significant limitations.
Existing approaches often generate talking faces, hindering seamless
integration into original scenes, or employ inpainting techniques that discard
vital visual information like partial occlusions and lighting variations. This
work introduces EdiDub, a novel framework that reformulates visual dubbing as a
content-aware editing task. EdiDub preserves the original video context by
utilizing a specialized conditioning scheme to ensure faithful and accurate
modifications rather than mere copying. On multiple benchmarks, including a
challenging occluded-lip dataset, EdiDub significantly improves identity
preservation and synchronization. Human evaluations further confirm its
superiority, achieving higher synchronization and visual naturalness scores
compared to the leading methods. These results demonstrate that our
content-aware editing approach outperforms traditional generation or
inpainting, particularly in maintaining complex visual elements while ensuring
accurate lip synchronization.

</details>


### [272] [CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis](https://arxiv.org/abs/2505.23444)
*Runmin Jiang,Genpei Zhang,Yuntian Yang,Siqi Wu,Yuheng Zhang,Wanyue Feng,Yizhou Zhao,Xi Xiao,Xiao Wang,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: CryoCCD is a synthesis framework that integrates biophysical modeling with generative techniques to produce realistic cryo-EM micrographs and enhances performance in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality annotated data hinders the development of robust models for cryo-EM imaging analysis. Existing synthetic data generation methods fail to capture both structural diversity and complex noise patterns.

Method: CryoCCD produces multi-scale cryo-EM micrographs by integrating biophysical modeling with generative techniques, incorporating compositional heterogeneity, cellular context, and physics-informed imaging. A conditional diffusion model generates realistic noise, enhanced by cycle consistency and mask-aware contrastive learning.

Result: Extensive experiments demonstrate that CryoCCD generates structurally accurate micrographs and improves performance in downstream tasks such as particle picking and reconstruction, outperforming state-of-the-art baselines.

Conclusion: CryoCCD addresses limitations in synthetic data generation for cryo-EM imaging, providing a valuable tool for enhancing downstream analysis tasks.

Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of
macromolecules, but developing robust models for downstream analysis is
hindered by the scarcity of high-quality annotated data. While synthetic data
generation has emerged as a potential solution, existing methods often fail to
capture both the structural diversity of biological specimens and the complex,
spatially varying noise inherent in cryo-EM imaging. To overcome these
limitations, we propose CryoCCD, a synthesis framework that integrates
biophysical modeling with generative techniques. Specifically, CryoCCD produces
multi-scale cryo-EM micrographs that reflect realistic biophysical variability
through compositional heterogeneity, cellular context, and physics-informed
imaging. To generate realistic noise, we employ a conditional diffusion model,
enhanced by cycle consistency to preserve structural fidelity and mask-aware
contrastive learning to capture spatially adaptive noise patterns. Extensive
experiments show that CryoCCD generates structurally accurate micrographs and
enhances performance in downstream tasks, outperforming state-of-the-art
baselines in both particle picking and reconstruction.

</details>


### [273] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
*Zifu Wang,Junyi Zhu,Bo Tang,Zhiyu Li,Feiyu Xiong,Jiaqian Yu,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: The paper explores rule-based visual reinforcement learning (RL) using jigsaw puzzles with multimodal large language models (MLLMs). It finds that MLLMs can improve from near random guessing to near-perfect accuracy, generalize to other visual tasks, learn with or without explicit reasoning, show pre-existing complex reasoning patterns, and that RL generalizes better than Supervised Fine-Tuning (SFT).


<details>
  <summary>Details</summary>
Motivation: To understand the application and potential of rule-based reinforcement learning in multimodal large language models, particularly for perception-heavy tasks.

Method: Using jigsaw puzzles as an experimental framework, the study applies rule-based visual RL to MLLMs, observing their performance, generalization capabilities, and reasoning patterns.

Result: MLLMs improved from near random guessing to near-perfect accuracy on jigsaw puzzles, generalized to other visual tasks, exhibited both explicit and implicit learning, showed pre-existing complex reasoning patterns, and demonstrated that RL generalizes better than SFT.

Conclusion: Rule-based visual RL shows promise for MLLMs in perception-heavy tasks, with findings indicating effective generalization and pre-existing reasoning patterns. However, results may vary across different visual tasks.

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [274] [DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification](https://arxiv.org/abs/2505.23595)
*Youssef Mohamed,Noran Mohamed,Khaled Abouhashad,Feilong Tang,Sara Atito,Shoaib Jameel,Imran Razzak,Ahmed B. Zaky*

Main category: cs.CV

TL;DR: The paper presents DeepChest, a dynamic task-weighting framework for multi-label chest X-ray classification that improves upon existing MTL methods by adaptively adjusting task importance based on loss trends, leading to faster training, reduced memory usage, and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Multi-Task Learning (MTL) has inherent advantages in complex domains like medical imaging but struggles with effectively balancing task contributions. Current methods for dynamic task weighting either have high computational overhead or rely heavily on gradients, making them less efficient.

Method: DeepChest is introduced as a computationally efficient and effective dynamic task-weighting framework. It uses a performance-driven weighting mechanism that analyzes task-specific loss trends to adjust task importance without needing gradient access. This approach significantly reduces memory usage and speeds up training.

Result: DeepChest outperforms state-of-the-art MTL methods by 7% in overall accuracy on a large-scale CXR dataset. It also reduces individual task losses, indicating better generalization and mitigation of negative transfer.

Conclusion: DeepChest offers significant efficiency and performance gains, making it more practical for real-world medical diagnostic applications. The model-agnostic approach can be applied to various state-of-the-art methods to improve their performance.

Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains
such as medical imaging by enabling shared representation learning, effectively
balancing task contributions remains a significant challenge. This paper
addresses this critical issue by introducing DeepChest, a novel,
computationally efficient and effective dynamic task-weighting framework
specifically designed for multi-label chest X-ray (CXR) classification. Unlike
existing heuristic or gradient-based methods that often incur substantial
overhead, DeepChest leverages a performance-driven weighting mechanism based on
effective analysis of task-specific loss trends. Given a network architecture
(e.g., ResNet18), our model-agnostic approach adaptively adjusts task
importance without requiring gradient access, thereby significantly reducing
memory usage and achieving a threefold increase in training speed. It can be
easily applied to improve various state-of-the-art methods. Extensive
experiments on a large-scale CXR dataset demonstrate that DeepChest not only
outperforms state-of-the-art MTL methods by 7% in overall accuracy but also
yields substantial reductions in individual task losses, indicating improved
generalization and effective mitigation of negative transfer. The efficiency
and performance gains of DeepChest pave the way for more practical and robust
deployment of deep learning in critical medical diagnostic applications. The
code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL

</details>


### [275] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/abs/2505.23617)
*Chenhao Zheng,Jieyu Zhang,Mohammadreza Salehi,Ziqi Gao,Vishnu Iyengar,Norimasa Kobori,Quan Kong,Ranjay Krishna*

Main category: cs.CV

TL;DR: 为了应对长视频处理中的挑战，本文提出了一种基于轨迹的视频标记化方法，并设计了相应的视频编码器TrajViT。该方法在减少冗余的同时保持时间连贯性，在多个视频理解基准上显著优于现有的ViT3D模型。此外，TrajViT作为现代VideoLLM的视频编码器也表现出色，训练速度更快且推理时FLOPs更少。这是首个在各种视频分析任务中始终优于ViT3D的有效编码器。


<details>
  <summary>Details</summary>
Motivation: 当前的视频标记化方法使用空间-时间补丁来分割视频，导致生成过多的标记和计算效率低下。特别是当摄像机移动时，最佳的标记减少策略性能下降且难以有效减少标记数量。因此需要一种新的、高效的视频标记化方法。

Method: 本文引入了基于全景子对象轨迹而非固定补丁的“grounded video tokenization”方法。通过提取对象轨迹并将其转换为具有语义意义的标记，这种方法能够显著降低冗余同时保持时间连贯性。编码器TrajViT利用对比学习进行训练，以实现高效且准确的视频理解。

Result: 1. 在视频文本检索任务中，TrajViT相较于ViT3D提高了6%的前五召回率，同时减少了10倍的标记数量。
2. 作为现代VideoLLM的视频编码器，TrajViT在6个VideoQA基准测试中平均性能提升了5.2%，并且训练速度提高4倍，推理时FLOPs减少18倍。
3. TrajViT是首个在多种视频分析任务中持续优于ViT3D的有效编码器。

Conclusion: TrajViT是一种基于轨迹的视频编码器，它通过减少冗余和保持时间连贯性，显著提高了视频理解任务的性能。实验结果表明，TrajViT不仅在多个基准测试中优于现有方法，而且在训练和推理效率方面也有显著提升，成为一种稳健且可扩展的解决方案。

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [276] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/abs/2505.23637)
*Dashti A. Ali,Richard K. G. Do,William R. Jarnagin,Aras T. Asaad,Amber L. Simpson*

Main category: cs.CV

TL;DR: In medical image analysis, Persistent homology (PH) is used to extract robust topological and geometrical features. This study compares two approaches for constructing feature vectors from PH barcodes across multiple medical imaging datasets and finds that feature concatenation preserves detailed information and yields better classification performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional feature extraction methods in machine learning models where small changes in input result in large changes in feature representation, this paper explores the use of Persistent Homology (PH) for more stable feature extraction in medical image analysis.

Method: The method involves using PH to store persistent topological and geometrical features as persistence barcodes. Two approaches are then compared for constructing the final topological feature vector: aggregating persistence barcodes followed by featurization or concatenating topological feature vectors derived from each barcode.

Result: The analysis across diverse medical imaging datasets shows that feature concatenation preserves detailed topological information from individual barcodes and results in better classification performance compared to aggregation.

Conclusion: Feature concatenation is a preferred approach when conducting similar experiments with PH barcodes for medical image analysis as it leads to improved classification performance.

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [277] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
*Tingyu Song,Tongyan Hu,Guo Gan,Yilun Zhao*

Main category: cs.CV

TL;DR: MLLMs are widely studied for video QA but lack assessment on AI-generated content (AIGC) videos. This study introduces VF-Eval, a benchmark with four tasks to evaluate MLLMs' abilities on AIGC videos. Evaluating 13 frontier MLLMs reveals challenges in consistent performance. An experiment, RePrompt, shows aligning MLLMs with human feedback can improve video generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of assessments focusing on synthetic or AI-generated content (AIGC) videos and the underexplored capabilities of MLLMs in interpreting such videos.

Method: Propose a new benchmark named VF-Eval which includes four tasks: coherence validation, error awareness, error type detection, and reasoning evaluation. Evaluate 13 frontier MLLMs using this benchmark and conduct an experiment called RePrompt to explore practical applications in improving video generation.

Result: Even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks in the VF-Eval benchmark. The RePrompt experiment demonstrates that aligning MLLMs more closely with human feedback can benefit video generation.

Conclusion: The proposed VF-Eval benchmark highlights the challenging nature of evaluating MLLMs on AIGC videos and suggests that aligning MLLMs with human feedback could enhance video generation.

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [278] [CLDTracker: A Comprehensive Language Description for Visual Tracking](https://arxiv.org/abs/2505.23704)
*Mohamad Alansari,Sajid Javed,Iyyakutti Iyappan Ganapathi,Sara Alansari,Muzammal Naseer*

Main category: cs.CV

TL;DR: The paper proposes CLDTracker, a novel framework for robust visual tracking that leverages comprehensive language descriptions and VLMs to address limitations in traditional trackers.


<details>
  <summary>Details</summary>
Motivation: VOT remains challenging due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers relying on visual cues struggle in complex scenarios, while direct application of VLMs is hindered by limitations such as lack of rich textual representation, inefficient fusion mechanisms, and lack of temporal modeling.

Method: CLDTracker introduces a dual-branch architecture with textual and visual branches. The textual branch constructs a rich bag of textual descriptions using VLMs like CLIP and GPT-4V, enriched with semantic and contextual cues. This addresses the lack of rich textual representation.

Result: Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves state-of-the-art performance.

Conclusion: The effectiveness of leveraging robust and temporally-adaptive vision-language representations for tracking is validated.

Abstract: VOT remains a fundamental yet challenging task in computer vision due to
dynamic appearance changes, occlusions, and background clutter. Traditional
trackers, relying primarily on visual cues, often struggle in such complex
scenarios. Recent advancements in VLMs have shown promise in semantic
understanding for tasks like open-vocabulary detection and image captioning,
suggesting their potential for VOT. However, the direct application of VLMs to
VOT is hindered by critical limitations: the absence of a rich and
comprehensive textual representation that semantically captures the target
object's nuances, limiting the effective use of language information;
inefficient fusion mechanisms that fail to optimally integrate visual and
textual features, preventing a holistic understanding of the target; and a lack
of temporal modeling of the target's evolving appearance in the language
domain, leading to a disconnect between the initial description and the
object's subsequent visual changes. To bridge these gaps and unlock the full
potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive
Language Description framework for robust visual Tracking. Our tracker
introduces a dual-branch architecture consisting of a textual and a visual
branch. In the textual branch, we construct a rich bag of textual descriptions
derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with
semantic and contextual cues to address the lack of rich textual
representation. Experiments on six standard VOT benchmarks demonstrate that
CLDTracker achieves SOTA performance, validating the effectiveness of
leveraging robust and temporally-adaptive vision-language representations for
tracking. Code and models are publicly available at:
https://github.com/HamadYA/CLDTracker

</details>


### [279] [Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning](https://arxiv.org/abs/2505.23709)
*Dionysis Christopoulos,Sotiris Spanos,Eirini Baltzi,Valsamis Ntouskos,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: The paper introduces SLIMP, a method that uses nested contrastive learning to capture relationships between skin lesion images and metadata, improving melanoma detection and classification.


<details>
  <summary>Details</summary>
Motivation: Melanoma detection and skin lesion classification based solely on images are challenging due to variations in imaging conditions and lack of clinical context. Clinicians use a holistic approach considering medical history and appearance of other lesions.

Method: SLIMP combines the appearance and metadata of individual skin lesions with patient-level metadata related to medical records using a nested contrastive learning approach.

Result: The proposed pre-training strategy improves performance on downstream skin lesion classification tasks compared to other pre-training strategies.

Conclusion: By fully exploiting all available data modalities throughout the learning process, SLIMP enhances the quality of learned representations for skin lesion classification.

Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning
rich representations of skin lesions through a novel nested contrastive
learning approach that captures complex relationships between images and
metadata. Melanoma detection and skin lesion classification based solely on
images, pose significant challenges due to large variations in imaging
conditions (lighting, color, resolution, distance, etc.) and lack of clinical
and phenotypical context. Clinicians typically follow a holistic approach for
assessing the risk level of the patient and for deciding which lesions may be
malignant and need to be excised, by considering the patient's medical history
as well as the appearance of other lesions of the patient. Inspired by this,
SLIMP combines the appearance and the metadata of individual skin lesions with
patient-level metadata relating to their medical record and other clinically
relevant information. By fully exploiting all available data modalities
throughout the learning process, the proposed pre-training strategy improves
performance compared to other pre-training strategies on downstream skin
lesions classification tasks highlighting the learned representations quality.

</details>


### [280] [CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models](https://arxiv.org/abs/2505.22869)
*Junbo Yin,Chao Zha,Wenjia He,Chencheng Xu,Xin Gao*

Main category: cs.CV

TL;DR: CFP-Gen is a novel diffusion language model for generating protein sequences that can satisfy multiple constraints across different modalities, including functional, sequence, and structural constraints.


<details>
  <summary>Details</summary>
Motivation: Existing PLMs are limited in generating protein sequences based on single-condition constraint from a specific modality and cannot simultaneously satisfy multiple constraints across different modalities.

Method: The model integrates multimodal conditions with functional, sequence, and structural constraints through an Annotation-Guided Feature Modulation (AGFM) module and a Residue-Controlled Functional Encoding (RCFE) module. Additionally, off-the-shelf 3D structure encoders can be integrated to impose geometric constraints.

Result: CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins and achieves a high success rate in designing multifunctional proteins.

Conclusion: CFP-Gen provides a powerful tool for de novo protein design by integrating multimodal conditions and enabling the generation of novel proteins with desired functionalities.

Abstract: Existing PLMs generate protein sequences based on a single-condition
constraint from a specific modality, struggling to simultaneously satisfy
multiple constraints across different modalities. In this work, we introduce
CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein
GENeration. CFP-Gen facilitates the de novo protein design by integrating
multimodal conditions with functional, sequence, and structural constraints.
Specifically, an Annotation-Guided Feature Modulation (AGFM) module is
introduced to dynamically adjust the protein feature distribution based on
composable functional annotations, e.g., GO terms, IPR domains and EC numbers.
Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures
residue-wise interaction to ensure more precise control. Additionally,
off-the-shelf 3D structure encoders can be seamlessly integrated to impose
geometric constraints. We demonstrate that CFP-Gen enables high-throughput
generation of novel proteins with functionality comparable to natural proteins,
while achieving a high success rate in designing multifunctional proteins. Code
and data available at https://github.com/yinjunbo/cfpgen.

</details>


### [281] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: The paper proposes a multi-modal CAD reconstruction model using advances in vision-language models (VLM) and large language model (LLM) training paradigms, including supervised fine-tuning (SFT) and reinforcement learning (RL) fine-tuning. This model processes point clouds, images, and text simultaneously and outperforms existing single-modal approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods for CAD reconstruction typically focus on a single input modality such as point clouds, images, or text which limits their generalizability and robustness.

Method: The authors adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data followed by reinforcement learning (RL) fine-tuning using online feedback obtained programmatically.

Result: In the DeepCAD benchmark, the SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. After RL fine-tuning, the model sets new state-of-the-art on three challenging datasets including a real-world one.

Conclusion: The proposed multi-modal CAD reconstruction model demonstrates superior performance compared to existing single-modal approaches and sets new benchmarks in the field.

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


### [282] [MAGREF: Masked Guidance for Any-Reference Video Generation](https://arxiv.org/abs/2505.23742)
*Yufan Deng,Xun Guo,Yuanyang Yin,Jacob Zhiyuan Fang,Yiding Yang,Yizhi Wang,Shenghai Yuan,Angtian Wang,Bo Liu,Haibin Huang,Chongyang Ma*

Main category: cs.CV

TL;DR: MAGREF is a new framework for multi-reference video generation that ensures consistency and high quality through masked guidance and novel mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current video generation struggles with maintaining consistency and quality when using multiple reference subjects.

Method: MAGREF uses a region-aware dynamic masking mechanism and a pixel-wise channel concatenation mechanism to handle diverse subject inference and preserve appearance features.

Result: MAGREF outperforms existing baselines in generating high-quality, coherent multi-subject videos and introduces a comprehensive benchmark for evaluation.

Conclusion: MAGREF provides a scalable, controllable, and high-fidelity solution for multi-subject video synthesis.

Abstract: Video generation has made substantial strides with the emergence of deep
generative models, especially diffusion-based approaches. However, video
generation based on multiple reference subjects still faces significant
challenges in maintaining multi-subject consistency and ensuring high
generation quality. In this paper, we propose MAGREF, a unified framework for
any-reference video generation that introduces masked guidance to enable
coherent multi-subject video synthesis conditioned on diverse reference images
and a textual prompt. Specifically, we propose (1) a region-aware dynamic
masking mechanism that enables a single model to flexibly handle various
subject inference, including humans, objects, and backgrounds, without
architectural changes, and (2) a pixel-wise channel concatenation mechanism
that operates on the channel dimension to better preserve appearance features.
Our model delivers state-of-the-art video generation quality, generalizing from
single-subject training to complex multi-subject scenarios with coherent
synthesis and precise control over individual subjects, outperforming existing
open-source and commercial baselines. To facilitate evaluation, we also
introduce a comprehensive multi-subject video benchmark. Extensive experiments
demonstrate the effectiveness of our approach, paving the way for scalable,
controllable, and high-fidelity multi-subject video synthesis. Code and model
can be found at: https://github.com/MAGREF-Video/MAGREF

</details>


### [283] [Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need](https://arxiv.org/abs/2505.23744)
*Qiang Wang,Xiang Song,Yuhang He,Jizhou Han,Chenhao Ding,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: SOYO is a lightweight framework that improves domain selection in Parameter-Isolation Domain Incremental Learning (PIDIL). It includes a Gaussian Mixture Compressor, Domain Feature Resampler, and Multi-level Domain Feature Fusion Network to store, balance, and enhance domain features. SOYO supports multiple PEFT methods and shows superiority in various tasks and benchmarks.


<details>
  <summary>Details</summary>
Motivation: DNNs struggle with real-world dynamic settings where data distributions change over time. Existing PIDIL methods have difficulty with accurate parameter selection as the number of domains and classes increases.

Method: SOYO introduces a Gaussian Mixture Compressor (GMC) and Domain Feature Resampler (DFR) for efficient storage and balancing of prior domain data. A Multi-level Domain Feature Fusion Network (MDFN) is used to improve domain feature extraction. The framework also supports multiple Parameter-Efficient Fine-Tuning (PEFT) methods.

Result: SOYO demonstrates consistent superiority over existing baselines across six benchmarks in tasks such as image classification, object detection, and speech enhancement.

Conclusion: SOYO offers an effective solution to improve domain selection in PIDIL, showcasing robustness and adaptability in complex environments.

Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic
settings where data distributions change over time. Domain Incremental Learning
(DIL) offers a solution by enabling continual model adaptation, with
Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce
knowledge conflicts. However, existing PIDIL methods struggle with parameter
selection accuracy, especially as the number of domains and corresponding
classes grows. To address this, we propose SOYO, a lightweight framework that
improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture
Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior
domain data efficiently, while a Multi-level Domain Feature Fusion Network
(MDFN) enhances domain feature extraction. Our framework supports multiple
Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks
such as image classification, object detection, and speech enhancement.
Experimental results on six benchmarks demonstrate SOYO's consistent
superiority over existing baselines, showcasing its robustness and adaptability
in complex, evolving environments. The codes will be released in
https://github.com/qwangcv/SOYO.

</details>


### [284] [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/abs/2505.23745)
*Hao Dong,Moru Liu,Jian Liang,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs) have strong capabilities in aligning visual and textual modalities, but they are still prone to misclassification. This paper introduces TrustVLM, a training-free framework that estimates when VLM's predictions can be trusted by proposing a novel confidence-scoring function.


<details>
  <summary>Details</summary>
Motivation: VLMs are powerful in multimodal understanding and generation, but their susceptibility to misclassification poses significant risks in safety-critical domains.

Method: The authors propose TrustVLM, a training-free framework that includes a novel confidence-scoring function leveraging the image embedding space to improve misclassification detection.

Result: TrustVLM demonstrates state-of-the-art performance across 17 diverse datasets with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines.

Conclusion: By enhancing model reliability without retraining, TrustVLM facilitates safer deployment of VLMs in real-world applications.

Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in
aligning visual and textual modalities, enabling a wide range of applications
in multimodal understanding and generation. While they excel in zero-shot and
transfer learning scenarios, VLMs remain susceptible to misclassification,
often yielding confident yet incorrect predictions. This limitation poses a
significant risk in safety-critical domains, where erroneous predictions can
lead to severe consequences. In this work, we introduce TrustVLM, a
training-free framework designed to address the critical challenge of
estimating when VLM's predictions can be trusted. Motivated by the observed
modality gap in VLMs and the insight that certain concepts are more distinctly
represented in the image embedding space, we propose a novel confidence-scoring
function that leverages this space to improve misclassification detection. We
rigorously evaluate our approach across 17 diverse datasets, employing 4
architectures and 2 VLMs, and demonstrate state-of-the-art performance, with
improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95
compared to existing baselines. By improving the reliability of the model
without requiring retraining, TrustVLM paves the way for safer deployment of
VLMs in real-world applications. The code will be available at
https://github.com/EPFL-IMOS/TrustVLM.

</details>


### [285] [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/abs/2505.23747)
*Diankun Wu,Fangfu Liu,Yi-Hsin Hung,Yueqi Duan*

Main category: cs.CV

TL;DR: 近期多模态大语言模型（MLLMs）在2D视觉任务上取得了显著进展，但其空间智能的提升仍面临挑战。现有的3D MLLMs依赖额外的3D或2.5D数据来融入空间意识，限制了其在仅包含2D输入（如图像或视频）场景中的应用。本文提出了一种名为Spatial-MLLM的新框架，可以从纯2D观察中进行基于视觉的空间推理。该框架通过释放前馈视觉几何基础模型的强大结构先验信息，提出了双编码器架构：预训练的2D视觉编码器提取语义特征，以及从视觉几何模型主干初始化的空间编码器提取3D结构特征。然后通过连接器将两种特征整合为统一的视觉标记，以增强空间理解能力。此外，我们还提出了一种空间感知帧采样策略，在推理时选择视频序列中具有空间信息的帧，确保即使在标记长度有限的情况下，模型也能专注于对空间推理至关重要的帧。除了架构改进外，我们还构建了Spatial-MLLM-120k数据集，并使用监督微调和GRPO对其进行训练。广泛的实验证明，我们的Spatial-MLLM在各种基于视觉的空间理解和推理任务中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在2D视觉任务上表现出色，但在提升空间智能方面仍然存在挑战。现有方法需要额外的3D或2.5D数据来增强空间意识，这限制了它们在仅有2D输入（如图像或视频）场景中的应用。因此，研究者希望开发一种能够从纯2D数据中提取空间信息的方法，以克服这一限制并扩展模型的应用范围。

Method: 本研究提出了一种名为Spatial-MLLM的新框架，主要包含以下几个部分：1) 双编码器架构：预训练的2D视觉编码器用于提取语义特征；空间编码器（从视觉几何模型主干初始化）用于提取3D结构特征。2) 连接器：将语义特征和3D结构特征整合为统一的视觉标记，从而增强空间理解能力。3) 空间感知帧采样策略：在推理时选择具有空间信息的关键帧，确保模型在标记长度有限的情况下仍能聚焦于对空间推理至关重要的帧。4) 数据集与训练：构建了Spatial-MLLM-120k数据集，并使用监督微调和GRPO进行训练。

Result: 经过广泛实验验证，Spatial-MLLM在多种实际数据集上的表现优于现有方法，在一系列基于视觉的空间理解和推理任务中达到了最先进的性能水平。这表明该框架能够在不依赖额外3D或2.5D数据的情况下，有效提升模型的空间智能。

Conclusion: 本文提出的Spatial-MLLM框架成功实现了从纯2D数据中进行基于视觉的空间推理，解决了现有方法对额外3D或2.5D数据的依赖问题。通过引入双编码器架构、连接器和空间感知帧采样策略，模型在空间理解和推理任务中表现出色。此外，构建的Spatial-MLLM-120k数据集为未来研究提供了重要资源。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced performance on 2D visual tasks. However, improving their
spatial intelligence remains a challenge. Existing 3D MLLMs always rely on
additional 3D or 2.5D data to incorporate spatial awareness, restricting their
utility in scenarios with only 2D inputs, such as images or videos. In this
paper, we present Spatial-MLLM, a novel framework for visual-based spatial
reasoning from purely 2D observations. Unlike conventional video MLLMs which
rely on CLIP-based visual encoders optimized for semantic understanding, our
key insight is to unleash the strong structure prior from the feed-forward
visual geometry foundation model. Specifically, we propose a dual-encoder
architecture: a pretrained 2D visual encoder to extract semantic features, and
a spatial encoder-initialized from the backbone of the visual geometry model-to
extract 3D structure features. A connector then integrates both features into
unified visual tokens for enhanced spatial understanding. Furthermore, we
propose a space-aware frame sampling strategy at inference time, which selects
the spatially informative frames of a video sequence, ensuring that even under
limited token length, the model focuses on frames critical for spatial
reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k
dataset and train the model on it using supervised fine-tuning and GRPO.
Extensive experiments on various real-world datasets demonstrate that our
spatial-MLLM achieves state-of-the-art performance in a wide range of
visual-based spatial understanding and reasoning tasks. Project page:
https://diankun-wu.github.io/Spatial-MLLM/.

</details>


### [286] [LADA: Scalable Label-Specific CLIP Adapter for Continual Learning](https://arxiv.org/abs/2505.23271)
*Mao-Lin Luo,Zi-Hao Zhou,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: LADA (Label-specific ADApter) is introduced to address the problem of parameter selection error in CLIP-based continual learning methods. It appends lightweight, label-specific memory units to the frozen CLIP image encoder and employs feature distillation for seen classes.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods adapt the pre-trained image encoder by adding multiple sets of learnable parameters, with each task using a partial set of parameters. This requires selecting the expected parameters for input images during inference, which is prone to error that degrades performance.

Method: LADA appends lightweight, label-specific memory units to the frozen CLIP image encoder, enabling discriminative feature generation by aggregating task-agnostic knowledge. To prevent catastrophic forgetting, LADA employs feature distillation for seen classes.

Result: Extensive results show that LADA achieves state-of-the-art performance in continual learning settings.

Conclusion: LADA addresses the problem of parameter selection error in CLIP-based continual learning methods and achieves state-of-the-art performance.

Abstract: Continual learning with vision-language models like CLIP offers a pathway
toward scalable machine learning systems by leveraging its transferable
representations. Existing CLIP-based methods adapt the pre-trained image
encoder by adding multiple sets of learnable parameters, with each task using a
partial set of parameters. This requires selecting the expected parameters for
input images during inference, which is prone to error that degrades
performance. To address this problem, we introduce LADA (Label-specific
ADApter). Instead of partitioning parameters across tasks, LADA appends
lightweight, label-specific memory units to the frozen CLIP image encoder,
enabling discriminative feature generation by aggregating task-agnostic
knowledge. To prevent catastrophic forgetting, LADA employs feature
distillation for seen classes, preventing their features from being interfered
with by new classes. Positioned after the image encoder, LADA prevents gradient
flow to the frozen CLIP parameters, ensuring efficient training. Extensive
results show that LADA achieves state-of-the-art performance in continual
learning settings. The implementation code is available at
https://github.com/MaolinLuo/LADA.

</details>


### [287] [TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning](https://arxiv.org/abs/2505.23475)
*Ron Shapira Weber,Shahar Ben Ishay,Andrey Lavrinenko,Shahaf E. Finder,Oren Freifeld*

Main category: cs.CV

TL;DR: TimePoint是一种自监督方法，通过从合成数据中学习关键点和描述符，加速基于DTW的时间序列对齐，并提高准确性。它利用有效的1D微分同胚生成现实的训练数据，结合全卷积和小波卷积架构提取信息关键点和描述符。实验表明，TimePoint比标准DTW更快、更准确。


<details>
  <summary>Details</summary>
Motivation: 时间序列的快速和可扩展对齐在许多领域都是一个基本挑战，而传统的解决方案动态时间规整（DTW）在可扩展性和对噪声的敏感性方面存在困难。

Method: 提出了一种名为TimePoint的自监督方法，该方法通过从合成数据中学习关键点和描述符，大幅加速基于DTW的时间序列对齐，同时通常提高了对齐精度。TimePoint受到2D关键点检测的启发，但经过精心调整以应对1D信号的独特挑战。它利用高效的1D微分同胚生成现实的训练数据，结合全卷积和小波卷积架构提取信息关键点和描述符。

Result: 将DTW应用于这些稀疏表示可以带来显著的速度提升，并且通常比将标准DTW应用于完整信号时具有更高的对齐精度。当仅在合成数据上进行训练时，TimePoint展示了对真实世界时间序列的强大泛化能力，并且通过在真实数据上的微调可以进一步改进。

Conclusion: 广泛的实验证明，TimePoint始终比标准DTW实现更快和更准确的对齐，使其成为时间序列分析的可扩展解决方案。

Abstract: Fast and scalable alignment of time series is a fundamental challenge in many
domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor
scalability and sensitivity to noise. We introduce TimePoint, a self-supervised
method that dramatically accelerates DTW-based alignment while typically
improving alignment accuracy by learning keypoints and descriptors from
synthetic data. Inspired by 2D keypoint detection but carefully adapted to the
unique challenges of 1D signals, TimePoint leverages efficient 1D
diffeomorphisms, which effectively model nonlinear time warping, to generate
realistic training data. This approach, along with fully convolutional and
wavelet convolutional architectures, enables the extraction of informative
keypoints and descriptors. Applying DTW to these sparse representations yield
major speedups and typically higher alignment accuracy than standard DTW
applied to the full signals. TimePoint demonstrates strong generalization to
real-world time series when trained solely on synthetic data, and further
improves with fine-tuning on real data. Extensive experiments demonstrate that
TimePoint consistently achieves faster and more accurate alignments than
standard DTW, making it a scalable solution for time-series analysis. Our code
is available at https://github.com/BGU-CS-VIL/TimePoint

</details>


### [288] [OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data](https://arxiv.org/abs/2505.23522)
*Fengxiang Wang,Mingshuo Chen,Xuming He,YiFan Zhang,Feng Liu,Zijie Guo,Zhenghao Hu,Jiong Wang,Jingyi Xu,Zhangrui Li,Fenghua Ling,Ben Fei,Weijia Li,Long Lan,Wenjing Yang,Wenlong Zhang,Lei Bai*

Main category: cs.CV

TL;DR: 现存的地球科学多模态学习基准在系统覆盖地球系统组件和跨圈层交互方面存在关键局限性。为解决这些差距，我们引入了OmniEarth-Bench，这是首个涵盖所有六个地球科学圈层（大气圈、岩石圈、海洋圈、冰冻圈、生物圈和人类活动圈）并跨越圈层的全面多模态基准，具有100个专家策划的评估维度。实验表明，即使是最先进的模型在我们的基准上也难以达到35％的准确率，特别是在某些跨圈层任务中，如GPT-4o等领先模型的表现甚至降至0.0％。OmniEarth-Bench为具有地球系统意识的人工智能设定了新标准，推动了科学发现以及环境监测和灾害预测的实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有的地球科学多模态学习基准在系统覆盖地球系统组件和跨圈层交互方面存在关键局限性，通常局限于孤立的子系统（例如仅限于人类活动圈或大气圈），并且评估维度有限（少于16个任务）。

Method: 通过整合卫星传感器和实地测量的观测数据，OmniEarth-Bench集成了29,779个注释，涵盖了四个层级：感知、一般推理、科学知识推理和连贯推理（CoT推理）。这涉及到每个圈层2到5名专家的努力，以建立权威的评估维度和策划相关的观测数据集，并由40名众包注释者协助专家进行注释。最后，通过混合专家-众包工作流程验证OmniEarth-Bench以减少标签歧义。

Result: 在9个最先进的多模态语言模型上的实验表明，即使是最先进的模型在我们的基准上也难以达到35％的准确率。特别是在某些跨圈层任务中，如GPT-4o等领先模型的表现甚至降至0.0％。

Conclusion: OmniEarth-Bench为具有地球系统意识的人工智能设定了新标准，推动了科学发现以及环境监测和灾害预测的实际应用。

Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical
limitations in systematic coverage of geosystem components and cross-sphere
interactions, often constrained to isolated subsystems (only in
Human-activities sphere or atmosphere) with limited evaluation dimensions (less
than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first
comprehensive multimodal benchmark spanning all six Earth science spheres
(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and
Human-activities sphere) and cross-spheres with one hundred expert-curated
evaluation dimensions. Leveraging observational data from satellite sensors and
in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four
tiers: perception, general reasoning, scientific knowledge reasoning and
chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per
sphere to establish authoritative evaluation dimensions and curate relevant
observational datasets, 40 crowd-sourcing annotators to assist experts for
annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd
workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs
reveal that even the most advanced models struggle with our benchmarks, where
none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the
performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets
a new standard for geosystem-aware AI, advancing both scientific discovery and
practical applications in environmental monitoring and disaster prediction. The
dataset, source code, and trained models were released.

</details>


### [289] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/abs/2505.23558)
*Xu Chu,Xinrong Chen,Guanyu Wang,Zhijie Tan,Kui Huang,Wenyu Lv,Tong Mo,Weiping Li*

Main category: cs.CV

TL;DR: Qwen-LookAgain (Qwen-LA) is a new Vision-Language Reasoning Model that mitigates hallucinations by incorporating a vision-text reflection process. It uses Balanced Reflective Policy Optimization to decide when to generate reflections and introduces Visual Token COPY and ROUTE to enhance visual attention.


<details>
  <summary>Details</summary>
Motivation: Extended reasoning in Vision-Language Models can lead to dilution of visual tokens and hallucinations. Text-only reflection processes are insufficient to suppress these hallucinations.

Method: The model uses a vision-text reflection process to re-attention visual information during reasoning. A reinforcement learning method, Balanced Reflective Policy Optimization, is used to guide the model to decide when to generate reflections and balance their number and length. Visual Token COPY and ROUTE are introduced to force the model to re-attention visual information at the visual level.

Result: Experiments on multiple visual QA datasets and hallucination metrics show that Qwen-LA achieves leading accuracy performance while reducing hallucinations.

Conclusion: Qwen-LookAgain addresses the issue of hallucinations in Vision-Language Models by effectively incorporating vision-text reflection and enhancing visual attention.

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at:
https://github.com/Liar406/Look_Again.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [290] [Trajectory Generator Matching for Time Series](https://arxiv.org/abs/2505.23215)
*T. Jahn,J. Chemseddine,P. Hagemann,C. Wald,G. Steidl*

Main category: math.NA

TL;DR: The paper introduces new methods for generating time series data using SDEs and jump processes inspired by trajectory flow matching, handling discontinuities via scaled Gaussian jump kernel densities.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling time-continuous stochastic processes from irregular observations remains a significant challenge.

Method: Leverage ideas from generative modeling of image data to push the boundary of time series generation. Find new generators of SDEs and jump processes inspired by trajectory flow matching that have the marginal distributions of the time series of interest.

Result: Able to handle discontinuities of the underlying processes by parameterizing the jump kernel densities by scaled Gaussians that allow for closed form formulas of the corresponding Kullback-Leibler divergence in the loss.

Conclusion: Unlike most other approaches, this method is able to handle irregularly sampled time series.

Abstract: Accurately modeling time-continuous stochastic processes from irregular
observations remains a significant challenge. In this paper, we leverage ideas
from generative modeling of image data to push the boundary of time series
generation. For this, we find new generators of SDEs and jump processes,
inspired by trajectory flow matching, that have the marginal distributions of
the time series of interest. Specifically, we can handle discontinuities of the
underlying processes by parameterizing the jump kernel densities by scaled
Gaussians that allow for closed form formulas of the corresponding
Kullback-Leibler divergence in the loss. Unlike most other approaches, we are
able to handle irregularly sampled time series.

</details>


### [291] [Optimization-Free Diffusion Model -- A Perturbation Theory Approach](https://arxiv.org/abs/2505.23652)
*Yuehaw Khoo,Mathias Oster,Yifan Peng*

Main category: math.NA

TL;DR: This paper presents an optimization-free and forward SDE-free method for estimating the score function in diffusion models by expanding it in a sparse set of eigenbasis of the backward Kolmogorov operator, reformulating score estimation as solving a linear system.


<details>
  <summary>Details</summary>
Motivation: Diffusion models typically rely on optimizing neural networks to estimate the score function via forward SDE simulations. This process can be computationally expensive and complex.

Method: The authors propose expanding the score function in a sparse set of eigenbasis of the backward Kolmogorov operator associated with the diffusion process, which allows them to reformulate score estimation as the solution to a linear system, thus avoiding iterative optimization and time-dependent sample generation.

Result: The approximation error is analyzed using perturbation theory, and the effectiveness of the method is demonstrated on high-dimensional Boltzmann distributions and real-world datasets.

Conclusion: The proposed method provides an alternative approach to score estimation in diffusion models that is both optimization-free and forward SDE-free, showing promise for more efficient generative modeling.

Abstract: Diffusion models have emerged as a powerful framework in generative modeling,
typically relying on optimizing neural networks to estimate the score function
via forward SDE simulations. In this work, we propose an alternative method
that is both optimization-free and forward SDE-free. By expanding the score
function in a sparse set of eigenbasis of the backward Kolmogorov operator
associated with the diffusion process, we reformulate score estimation as the
solution to a linear system, avoiding iterative optimization and time-dependent
sample generation. We analyze the approximation error using perturbation theory
and demonstrate the effectiveness of our method on high-dimensional Boltzmann
distributions and real-world datasets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [292] [Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders](https://arxiv.org/abs/2505.23053)
*Wei-Hsiang Huang,Chen-Wei Ke,Wei-Ning Chiu,Yu-Xuan Su,Chun-Chun Yang,Chieh-Yuan Cheng,Yun-Nung Chen,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: The paper introduces a taxonomy for LLM-based recommender systems, classifying them into Pure LLM Recommenders and Augmented LLM Recommenders. It also presents a unified evaluation platform for benchmarking these models and discusses future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic understanding of how large language models (LLMs) are being used in recommender systems, and to offer practical guidance for advancing next-generation LLM-powered recommenders.

Method: Proposing a taxonomy that categorizes LLM-based recommendation approaches into two types: Pure LLM Recommenders and Augmented LLM Recommenders. Introducing a unified evaluation platform for benchmarking representative models under consistent settings.

Result: Establishment of a novel taxonomy and a unified evaluation platform, providing insights into key design choices affecting the effectiveness of LLM-based recommenders.

Conclusion: This study concludes by addressing open challenges in LLM-based recommenders and suggesting promising future research directions, offering both an overview and guidance for advancing the field.

Abstract: Large language models (LLMs) have introduced new paradigms for recommender
systems by enabling richer semantic understanding and incorporating implicit
world knowledge. In this study, we propose a systematic taxonomy that
classifies existing approaches into two categories: (1) Pure LLM Recommenders,
which rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate
additional non-LLM techniques to enhance performance. This taxonomy provides a
novel lens through which to examine the evolving landscape of LLM-based
recommendation. To support fair comparison, we introduce a unified evaluation
platform that benchmarks representative models under consistent experimental
settings, highlighting key design choices that impact effectiveness. We
conclude by discussing open challenges and outlining promising directions for
future research. This work offers both a comprehensive overview and practical
guidance for advancing next-generation LLM-powered recommender.

</details>


### [293] [From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval](https://arxiv.org/abs/2505.23059)
*Dohyeon Lee,Yeonseok Jeong,Seung-won Hwang*

Main category: cs.IR

TL;DR: The paper presents State Machine Reasoning (SMR), a new framework for information retrieval that addresses issues of redundancy and misguided reasoning in Chain-of-Thought prompting. It improves retrieval performance by 3.4% and reduces token usage by 74.4%. SMR offers fine-grained control and is adaptable across different models without task-specific tuning.


<details>
  <summary>Details</summary>
Motivation: To overcome the problems of overthinking, redundant trajectories, and misguided reasoning in Chain-of-Thought prompting within large language models used for information retrieval.

Method: State Machine Reasoning (SMR) is introduced, a transition-based reasoning framework with discrete actions (Refine, Rerank, Stop) enabling early stopping and precise control to enhance information retrieval processes.

Result: Experiments on BEIR and BRIGHT benchmarks demonstrate a 3.4% improvement in retrieval performance (nDCG@10) and a 74.4% reduction in token usage.

Conclusion: SMR provides a practical alternative to conventional CoT reasoning, improving efficiency and effectiveness in information retrieval without needing task-specific tuning.

Abstract: Chain-of-Thought (CoT) prompting enables complex reasoning in large language
models (LLMs), including applications in information retrieval (IR). However,
it often leads to overthinking, where models produce excessively long and
semantically redundant traces with little or no benefit. We identify two key
challenges in IR: redundant trajectories that revisit similar states and
misguided reasoning that diverges from user intent. To address these, we
propose State Machine Reasoning (SMR), a transition-based reasoning framework
composed of discrete actions (Refine, Rerank, Stop) that support early stopping
and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show
that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token
usage by 74.4%. It generalizes across LLMs and retrievers without requiring
task-specific tuning, offering a practical alternative to conventional CoT
reasoning. The code and details are available at https://github.com/ldilab/SMR.

</details>


### [294] [Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking](https://arxiv.org/abs/2505.23250)
*Pascal J. Sager,Ashwini Kamaraj,Benjamin F. Grewe,Thilo Stadelmann*

Main category: cs.IR

TL;DR: The paper presents a hybrid retrieval pipeline that combines lexical precision, semantic generalization, and deep contextual re-ranking to retrieve relevant scientific literature for social media posts. It uses BM25 and FAISS with INF-Retriever-v1 for initial retrieval and LLM-based cross-encoder for re-ranking. The method achieved MRR@5 scores of 76.46% on the development set and 66.43% on the test set, ranking 1st in development and 3rd in test (out of 31 teams).


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between informal language used in social media posts and formal language in scientific literature, thus improving the retrieval of relevant scientific documents.

Method: A hybrid retrieval pipeline combining BM25-based keyword matching, FAISS vector store using a fine-tuned INF-Retriever-v1 model for dense semantic retrieval, and large language model (LLM)-based cross-encoder for re-ranking.

Result: Achieved MRR@5 scores of 76.46% on the development set and 66.43% on the hidden test set, ranking 1st on the development leaderboard and 3rd on the test leaderboard (out of 31 teams).

Conclusion: The proposed hybrid retrieval pipeline demonstrates strong performance without external training data, proving the effectiveness of carefully designed and fine-tuned open-source models.

Abstract: We present the methodology and results of the Deep Retrieval team for subtask
4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving
relevant scientific literature for given social media posts. To address this
task, we propose a hybrid retrieval pipeline that combines lexical precision,
semantic generalization, and deep contextual re-ranking, enabling robust
retrieval that bridges the informal-to-formal language gap. Specifically, we
combine BM25-based keyword matching with a FAISS vector store using a
fine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns
the top 30 candidates, and semantic search yields 100 candidates, which are
then merged and re-ranked via a large language model (LLM)-based cross-encoder.
  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the
development set and 66.43% on the hidden test set, securing the 1st position on
the development leaderboard and ranking 3rd on the test leaderboard (out of 31
teams), with a relative performance gap of only 2 percentage points compared to
the top-ranked system. We achieve this strong performance by running
open-source models locally and without external training data, highlighting the
effectiveness of a carefully designed and fine-tuned retrieval pipeline.

</details>


### [295] [Engineering Serendipity through Recommendations of Items with Atypical Aspects](https://arxiv.org/abs/2505.23580)
*Ramit Aditya,Razvan Bunescu,Smita Nannaware,Erfan Al-Hossami*

Main category: cs.IR

TL;DR: 本论文提出了一种通过推荐具有非典型特征的项目来创造意外惊喜的新任务，并开发了一个基于LLM的系统管道，用于提取、评估和聚合这些特征的用户特定效用。还引入了相关数据集和动态选择学习示例的自定义程序，实验表明该系统生成的意外惊喜排名与人工标注的基准高度相关。


<details>
  <summary>Details</summary>
Motivation: 受顾客在餐厅或酒店遇到符合兴趣的意外元素可能产生难忘体验的启发，例如等待区的折纸站或酒店大堂的18世纪大键琴展览。

Method: 描述了一个基于LLM的系统管道，从项目评论中提取非典型特征，估计并聚合其用户特定效用，以意外惊喜潜力衡量标准重新排序推荐给用户的项目列表。引入了带有非典型特征的手动标注Yelp评论数据集和人工生成的用户档案数据集，以及用户-特征效用值的众包注释。还介绍了一种动态选择上下文学习示例的自定义程序。

Result: 实验评估显示，系统生成的基于意外惊喜的排名与根据非典型特征及其用户依赖效用的人工注释计算出的意外惊喜分数的基准高度相关。

Conclusion: 希望新的推荐任务和关联系统能够推动进一步研究超越准确性的推荐方法，以提高用户满意度。

Abstract: A restaurant dinner or a hotel stay may lead to memorable experiences when
guests encounter unexpected aspects that also match their interests. For
example, an origami-making station in the waiting area of a restaurant may be
both surprising and enjoyable for a customer who is passionate about paper
crafts. Similarly, an exhibit of 18th century harpsichords would be atypical
for a hotel lobby and likely pique the interest of a guest who has a passion
for Baroque music. Motivated by this insight, in this paper we introduce the
new task of engineering serendipity through recommendations of items with
atypical aspects. We describe an LLM-based system pipeline that extracts
atypical aspects from item reviews, then estimates and aggregates their
user-specific utility in a measure of serendipity potential that is used to
rerank a list of items recommended to the user. To facilitate system
development and evaluation, we introduce a dataset of Yelp reviews that are
manually annotated with atypical aspects and a dataset of artificially
generated user profiles, together with crowdsourced annotations of user-aspect
utility values. Furthermore, we introduce a custom procedure for dynamic
selection of in-context learning examples, which is shown to improve LLM-based
judgments of atypicality and utility. Experimental evaluations show that
serendipity-based rankings generated by the system are highly correlated with
ground truth rankings for which serendipity scores are computed from manual
annotations of atypical aspects and their user-dependent utility. Overall, we
hope that the new recommendation task and the associated system presented in
this paper catalyze further research into recommendation approaches that go
beyond accuracy in their pursuit of enhanced user satisfaction.
  The datasets and the code are made publicly available at
https://github.com/ramituncc49er/ATARS .

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [296] [BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models](https://arxiv.org/abs/2505.22865)
*Susan Liang,Dejan Markovic,Israel D. Gebru,Steven Krenn,Todd Keebler,Jacob Sandakly,Frank Yu,Samuel Hassel,Chenliang Xu,Alexander Richard*

Main category: cs.SD

TL;DR: 该论文提出了一种名为BinauralFlow的基于流匹配的可流式双耳语音合成框架，解决了双耳渲染中的质量问题和实时推理需求。通过条件流匹配模型、因果U-Net架构及连续推理流水线等设计，实现了高质量、高连续性且接近真实录音的双耳音频合成，评估显示其优于现有方法，混淆率达到42%。


<details>
  <summary>Details</summary>
Motivation: 现有的双耳渲染方法在渲染质量和实时推理方面存在不足，难以生成与真实录音无异的高质量双耳音频，同时无法满足实际应用中对流式推理的需求。

Method: 1. 将双耳渲染视为生成问题而非回归问题，设计了条件流匹配模型以生成高质量音频。
2. 提出因果U-Net架构，仅基于过去信息估计当前音频帧，使生成模型适用于流式推理。
3. 引入包含流式STFT/ISTFT操作、缓冲区、中点求解器和早期跳过调度的连续推理流水线，提升渲染连续性和速度。

Result: 定量和定性评估表明，该方法在性能上优于现有最佳方法。感知研究表明，所提出的模型生成的音频几乎与真实录音无法区分，混淆率高达42%。

Conclusion: BinauralFlow框架成功解决了双耳渲染中的质量与实时性问题，其生成的音频在质量上接近真实录音，并能满足实际应用中的流式推理需求，展现出显著优势。

Abstract: Binaural rendering aims to synthesize binaural audio that mimics natural
hearing based on a mono audio and the locations of the speaker and listener.
Although many methods have been proposed to solve this problem, they struggle
with rendering quality and streamable inference. Synthesizing high-quality
binaural audio that is indistinguishable from real-world recordings requires
precise modeling of binaural cues, room reverb, and ambient sounds.
Additionally, real-world applications demand streaming inference. To address
these challenges, we propose a flow matching based streaming binaural speech
synthesis framework called BinauralFlow. We consider binaural rendering to be a
generation problem rather than a regression problem and design a conditional
flow matching model to render high-quality audio. Moreover, we design a causal
U-Net architecture that estimates the current audio frame solely based on past
information to tailor generative models for streaming inference. Finally, we
introduce a continuous inference pipeline incorporating streaming STFT/ISTFT
operations, a buffer bank, a midpoint solver, and an early skip schedule to
improve rendering continuity and speed. Quantitative and qualitative
evaluations demonstrate the superiority of our method over SOTA approaches. A
perceptual study further reveals that our model is nearly indistinguishable
from real-world recordings, with a $42\%$ confusion rate.

</details>


### [297] [Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone](https://arxiv.org/abs/2505.23132)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: This paper explores using smartphone microphones for lung sound diagnosis, overcoming challenges with PD-SCL and improving the AST model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional in-person lung sound assessments highlighted by the COVID-19 pandemic.

Method: The method involves developing Patient Domain Supervised Contrastive Learning (PD-SCL) integrated with the Audio Spectrogram Transformer (AST) model to analyze lung sounds recorded by smartphone microphones.

Result: The integration of PD-SCL with AST improved performance by 2.4% compared to the original AST model.

Conclusion: Smartphones can effectively diagnose lung sounds, addressing patient data inconsistencies and showing potential for use beyond clinical settings.

Abstract: Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic
has revealed the limitations of traditional, in-person lung sound assessments.
To overcome these issues, advancements in digital stethoscopes and artificial
intelligence (AI) have led to the development of new diagnostic methods. In
this context, our study aims to use smartphone microphones to record and
analyze lung sounds. We faced two major challenges: the difference in audio
style between electronic stethoscopes and smartphone microphones, and the
variability among patients. To address these challenges, we developed a method
called Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating
this method with the Audio Spectrogram Transformer (AST) model, we
significantly improved its performance by 2.4\% compared to the original AST
model. This progress demonstrates that smartphones can effectively diagnose
lung sounds, addressing inconsistencies in patient data and showing potential
for broad use beyond traditional clinical settings. Our research contributes to
making lung disease detection more accessible in the post-COVID-19 world.

</details>


### [298] [Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM](https://arxiv.org/abs/2505.23207)
*Zhaokai Sun,Li Zhang,Qing Wang,Pan Zhou,Lei Xie*

Main category: cs.SD

TL;DR: The paper proposes a speaker-aware progressive OSD model that uses a progressive training strategy and explores SSL models with a speaker attention module to improve acoustic representation, achieving SOTA performance with an F1 score of 82.76% on the AMI test set.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting overlapping speech in multi-party conversations by enhancing the correlation between subtasks like VAD and overlap detection.

Method: Proposes a speaker-aware progressive OSD model using progressive training strategy, incorporates state-of-the-art SSL models (WavLM and wav2vec 2.0) and a speaker attention module for enriching features with frame-level speaker information.

Result: Achieves state-of-the-art performance with an F1 score of 82.76% on the AMI test set, demonstrating robustness and effectiveness.

Conclusion: The proposed speaker-aware progressive OSD model effectively detects overlapping speech, outperforming existing methods.

Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple
speakers overlap in a conversation, a critical challenge in multi-party speech
processing. This work proposes a speaker-aware progressive OSD model that
leverages a progressive training strategy to enhance the correlation between
subtasks such as voice activity detection (VAD) and overlap detection. To
improve acoustic representation, we explore the effectiveness of
state-of-the-art self-supervised learning (SSL) models, including WavLM and
wav2vec 2.0, while incorporating a speaker attention module to enrich features
with frame-level speaker information. Experimental results show that the
proposed method achieves state-of-the-art performance, with an F1 score of
82.76\% on the AMI test set, demonstrating its robustness and effectiveness in
OSD.

</details>


### [299] [MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction](https://arxiv.org/abs/2505.23305)
*Yunkee Chae,Kyogu Lee*

Main category: cs.SD

TL;DR: MGE-LDM是一种统一的潜在扩散框架，用于同时进行音乐生成、源插补和查询驱动的源分离。该模型在一个紧凑的潜在扩散模型中学习完整混合、子混合和单个茎的联合分布，支持灵活、类别无关的任意乐器源操作，并且可以跨异构多轨数据集进行联合训练。


<details>
  <summary>Details</summary>
Motivation: 现有的方法受限于固定的乐器类别，而本文旨在开发一种能够处理完整混合、子混合和单个茎的联合分布的统一框架，以实现更灵活的音乐生成、源插补和源分离任务。

Method: MGE-LDM通过在一个紧凑的潜在扩散模型中学习完整混合、子混合和单个茎的联合分布，将分离和插补都作为潜在空间中的条件修复任务来实现。该模型可以在不依赖预定义乐器类别的前提下，跨异构多轨数据集（如Slakh2100、MUSDB18、MoisesDB）进行联合训练。

Result: MGE-LDM实现了完整的混合生成、部分生成（即源插补）和文本条件下的任意源提取，支持灵活、类别无关的任意乐器源操作。

Conclusion: MGE-LDM提供了一种新的方法，可以在不依赖预定义乐器类别的前提下，同时进行音乐生成、源插补和查询驱动的源分离，具有较高的灵活性和广泛的适用性。

Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous
music generation, source imputation, and query-driven source separation. Unlike
prior approaches constrained to fixed instrument classes, MGE-LDM learns a
joint distribution over full mixtures, submixtures, and individual stems within
a single compact latent diffusion model. At inference, MGE-LDM enables (1)
complete mixture generation, (2) partial generation (i.e., source imputation),
and (3) text-conditioned extraction of arbitrary sources. By formulating both
separation and imputation as conditional inpainting tasks in the latent space,
our approach supports flexible, class-agnostic manipulation of arbitrary
instrument sources. Notably, MGE-LDM can be trained jointly across
heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without
relying on predefined instrument categories. Audio samples are available at our
project page: https://yoongi43.github.io/MGELDM_Samples/.

</details>


### [300] [Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds](https://arxiv.org/abs/2505.23509)
*Andrew Chang,Yike Li,Iran R. Roman,David Poeppel*

Main category: cs.SD

TL;DR: This paper proposes a novel approach using spectrotemporal modulation (STM) features for audio classification, which performs comparably to pretrained audio DNNs without any pretraining.


<details>
  <summary>Details</summary>
Motivation: Current audio DNN representations are computationally costly and uninterpretable, so there is a need for more efficient and understandable feature representations.

Method: The method uses spectrotemporal modulation (STM) features, a signal processing technique that mimics the neurophysiological representation in the human auditory cortex, for audio classification.

Result: The STM-based model achieves classification performance comparable to pretrained audio DNNs across diverse naturalistic speech, music, and environmental sounds without any pretraining.

Conclusion: STM is an efficient and interpretable feature representation for audio classification, advancing machine listening and offering new possibilities in speech and auditory sciences, as well as audio BCI and cognitive computing.

Abstract: Audio DNNs have demonstrated impressive performance on various machine
listening tasks; however, most of their representations are computationally
costly and uninterpretable, leaving room for optimization. Here, we propose a
novel approach centered on spectrotemporal modulation (STM) features, a signal
processing method that mimics the neurophysiological representation in the
human auditory cortex. The classification performance of our STM-based model,
without any pretraining, is comparable to that of pretrained audio DNNs across
diverse naturalistic speech, music, and environmental sounds, which are
essential categories for both human cognition and machine perception. These
results show that STM is an efficient and interpretable feature representation
for audio classification, advancing the development of machine listening and
unlocking exciting new possibilities for basic understanding of speech and
auditory sciences, as well as developing audio BCI and cognitive computing.

</details>


### [301] [Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](https://arxiv.org/abs/2505.23619)
*Neta Glazer,David Chernin,Idan Achituve,Sharon Gannot,Ethan Fetaya*

Main category: cs.SD

TL;DR: The paper introduces ADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD), which combines a deep embedding model with GP flexibility for strong performance and adaptability, personalized detection, robustness to new TTS models, and one-shot adaptability. A benchmark dataset is also constructed.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the increasing demand for adaptable and efficient deepfake detection methods due to advancements in Text-to-Speech models, particularly voice cloning. Detection models need to adapt efficiently to unseen generation models with minimal data.

Method: The method involves introducing ADD-GP, a few-shot adaptive framework that uses a Gaussian Process (GP) classifier for Audio Deepfake Detection. It combines a deep embedding model with the flexibility of Gaussian processes for enhanced performance and adaptability.

Result: This approach not only achieves strong performance and adaptability but also supports personalized detection with greater robustness to new TTS models and one-shot adaptability.

Conclusion: The conclusion highlights the effectiveness of ADD-GP in Audio Deepfake Detection, its adaptability to new TTS models, and its potential for personalized detection. A benchmark dataset was constructed to support evaluation.

Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice
cloning, have intensified the demand for adaptable and efficient deepfake
detection methods. As TTS systems continue to evolve, detection models must be
able to efficiently adapt to previously unseen generation models with minimal
data. This paper introduces ADD-GP, a few-shot adaptive framework based on a
Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show
how the combination of a powerful deep embedding model with the Gaussian
processes flexibility can achieve strong performance and adaptability.
Additionally, we show this approach can also be used for personalized
detection, with greater robustness to new TTS models and one-shot adaptability.
To support our evaluation, a benchmark dataset is constructed for this task
using new state-of-the-art voice cloning models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [302] [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
*Feng Yao,Zilong Wang,Liyuan Liu,Junxia Cui,Li Zhong,Xiaohan Fu,Haohui Mai,Vish Krishnan,Jianfeng Gao,Jingbo Shang*

Main category: cs.CL

TL;DR: The paper introduces REAL, a reinforcement learning framework that encourages LLMs to generate high-quality code by using program analysis and unit test feedback. It operates without prompts or references, improving scalability and reducing manual intervention.


<details>
  <summary>Details</summary>
Motivation: Code generation with large language models (LLMs) has been increasingly adopted but fails to ensure quality in security and maintainability. Current methods for improvement are labor-intensive or not robust enough.

Method: REAL integrates automated signals from program analysis for detecting defects and unit tests for ensuring correctness. It uses reinforcement learning to incentivize LLMs to produce production-quality code without needing specific prompts or reference codes.

Result: Experiments show that REAL surpasses state-of-the-art methods in functionality and code quality assessments across multiple datasets and model scales.

Conclusion: REAL bridges the gap between rapid prototyping and production-ready code, allowing LLMs to efficiently deliver both speed and quality.

Abstract: Code generation with large language models (LLMs), often termed vibe coding,
is increasingly adopted in production but fails to ensure code quality,
particularly in security (e.g., SQL injection vulnerabilities) and
maintainability (e.g., missing type annotations). Existing methods, such as
supervised fine-tuning and rule-based post-processing, rely on labor-intensive
annotations or brittle heuristics, limiting their scalability and
effectiveness. We propose REAL, a reinforcement learning framework that
incentivizes LLMs to generate production-quality code using program
analysis-guided feedback. Specifically, REAL integrates two automated signals:
(1) program analysis detecting security or maintainability defects and (2) unit
tests ensuring functional correctness. Unlike prior work, our framework is
prompt-agnostic and reference-free, enabling scalable supervision without
manual intervention. Experiments across multiple datasets and model scales
demonstrate that REAL outperforms state-of-the-art methods in simultaneous
assessments of functionality and code quality. Our work bridges the gap between
rapid prototyping and production-ready code, enabling LLMs to deliver both
speed and quality.

</details>


### [303] [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
*Ansar Aynetdinov,Alan Akbik*

Main category: cs.CL

TL;DR: Multi-token prediction (MTP) is a new pre-training objective for language models which predicts the next k tokens at each step. While it improves performance and efficiency, smaller language models struggle with MTP. This paper proposes two curriculum learning strategies: forward and reverse curricula. The forward curriculum helps small models leverage MTP, improving downstream NTP performance and generative quality while retaining self-speculative decoding benefits. The reverse curriculum achieves better NTP performance but lacks decoding benefits.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge that smaller language models face when using the Multi-token prediction (MTP) pre-training objective. By proposing curriculum learning strategies, the aim is to help these smaller models benefit from MTP in terms of improved performance and efficiency.

Method: The authors explore two variants of curriculum learning strategies for MTP training: forward curriculum and reverse curriculum. The forward curriculum gradually increases the complexity of the pre-training objective from NTP to MTP, whereas the reverse curriculum does the opposite.

Result: The forward curriculum enables smaller language models to better utilize the MTP objective during pre-training, leading to improvements in downstream NTP performance and generative output quality while retaining the advantages of self-speculative decoding. On the other hand, the reverse curriculum results in stronger NTP performance and output quality but does not provide any self-speculative decoding benefits.

Conclusion: Curriculum learning strategies, particularly the forward curriculum, can significantly aid smaller language models in leveraging the MTP pre-training objective. This leads to enhancements in both performance and generative quality while preserving the benefits of self-speculative decoding.

Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.

</details>


### [304] [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
*Sara Papi,Marco Gaido,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Main category: cs.CL

TL;DR: The paper introduces FAMA, the first family of open science speech foundation models for English and Italian, which are trained on 150k+ hours of open-source speech data. Along with this, a new dataset with 16k hours of cleaned and pseudo-labeled speech is presented. FAMA shows competitive performance compared to existing models while being significantly faster.


<details>
  <summary>Details</summary>
Motivation: To address the reproducibility and fair evaluation challenges posed by the closed nature of current speech foundation models (SFMs), such as inaccessible training data and code. The authors aim to promote openness in speech technology research by developing fully transparent models trained on open-source code and data.

Method: The authors introduce FAMA, the first family of open science SFMs for English and Italian, trained on over 150,000 hours of open-source speech data. They also present a new dataset containing 16,000 hours of cleaned and pseudo-labeled speech for both languages.

Result: FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster.

Conclusion: All artifacts, including code, datasets, and models, are released under open-source-compliant licenses, promoting openness in speech technology research.

Abstract: The development of speech foundation models (SFMs) like Whisper and
SeamlessM4T has significantly advanced the field of speech processing. However,
their closed nature--with inaccessible training data and code--poses major
reproducibility and fair evaluation challenges. While other domains have made
substantial progress toward open science by developing fully transparent models
trained on open-source (OS) code and data, similar efforts in speech remain
limited. To fill this gap, we introduce FAMA, the first family of open science
SFMs for English and Italian, trained on 150k+ hours of OS speech data.
Moreover, we present a new dataset containing 16k hours of cleaned and
pseudo-labeled speech for both languages. Results show that FAMA achieves
competitive performance compared to existing SFMs while being up to 8 times
faster. All artifacts, including code, datasets, and models, are released under
OS-compliant licenses, promoting openness in speech technology research.

</details>


### [305] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: This study shows how feedback-oriented annotations can improve automated essay scoring accuracy using the PERSUADE corpus and two LLMs.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of automated essay scoring by incorporating feedback-oriented annotations into the scoring pipeline.

Method: Integrating two types of annotations - those identifying spelling and grammatical errors, and those highlighting argumentative components - into the AES process using the PERSUADE corpus. Employing a generative language model for spell-correction and an encoder-based token classifier to identify argumentative elements.

Result: Demonstrated improvements in performance when using encoder-based large language models fine-tuned as classifiers with the added annotations.

Conclusion: Incorporating feedback-oriented annotations enhances the accuracy of automated essay scoring.

Abstract: This study illustrates how incorporating feedback-oriented annotations into
the scoring pipeline can enhance the accuracy of automated essay scoring (AES).
This approach is demonstrated with the Persuasive Essays for Rating, Selecting,
and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We
integrate two types of feedback-driven annotations: those that identify
spelling and grammatical errors, and those that highlight argumentative
components. To illustrate how this method could be applied in real-world
scenarios, we employ two LLMs to generate annotations -- a generative language
model used for spell-correction and an encoder-based token classifier trained
to identify and mark argumentative elements. By incorporating annotations into
the scoring process, we demonstrate improvements in performance using
encoder-based large language models fine-tuned as classifiers.

</details>


### [306] [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
*Andrew Zhu,Evan Osgood,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 研究人员探索了一种名为'overhearing agents'的新范式，这些代理通过聆听人类对话来执行背景任务或提供建议。他们以Dungeons & Dragons游戏为背景，研究了大型多模式音频-语言模型如何帮助地下城主，并发现了一些模型利用隐含音频线索执行任务的能力。


<details>
  <summary>Details</summary>
Motivation: 当前关于对话型LLM代理的研究大多集中在直接协助人类用户完成任务，而本研究旨在探索一种新的交互方式——'overhearing agents'，即不主动参与对话，而是通过聆听人类之间的对话来提供辅助。

Method: 研究使用大型多模式音频-语言模型作为'overhearing agents'，在Dungeons & Dragons游戏中协助地下城主。通过人类评估来检测这些代理的帮助性，并分析模型是否具备利用隐含音频线索执行任务的能力。

Result: 研究发现，一些大型音频-语言模型确实具有利用隐含音频线索执行'overhearing agent'任务的新兴能力。

Conclusion: 本研究表明，'overhearing agents'是一种可行且有潜力的LLM应用新范式，同时研究人员发布了Python库和项目代码以支持进一步研究。

Abstract: Much work has been done on conversational LLM agents which directly assist
human users with tasks. We present an alternative paradigm for interacting with
LLM agents, which we call "overhearing agents". These overhearing agents do not
actively participate in conversation -- instead, they "listen in" on
human-to-human conversations and perform background tasks or provide
suggestions to assist the user. In this work, we explore the overhearing agents
paradigm through the lens of Dungeons & Dragons gameplay. We present an
in-depth study using large multimodal audio-language models as overhearing
agents to assist a Dungeon Master. We perform a human evaluation to examine the
helpfulness of such agents and find that some large audio-language models have
the emergent ability to perform overhearing agent tasks using implicit audio
cues. Finally, we release Python libraries and our project code to support
further research into the overhearing agents paradigm at
https://github.com/zhudotexe/overhearing_agents.

</details>


### [307] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
*Alexander Gill,Abhilasha Ravichander,Ana Marasović*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）可用于生成数据，但创建评估基准仍具挑战。通过两个案例研究，我们探讨了LLMs是否能生成有效的推理文本基准，并与精心设计的人工众包基准进行了比较。结果表明，LLMs生成的基准在成本上更具优势，但在难度和挑战性方面不及人工生成的版本。这提示我们需要重新评估使用LLMs生成评估数据的即时性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型被越来越多地用于数据生成，但创建合适的评估基准仍然是一个难题。需要针对特定现象、避免利用捷径并且具有挑战性的基准。因此，研究者希望通过案例研究来探索LLMs生成的基准是否能够满足这些需求。

Method: 研究者选择了两个高质量的阅读理解数据集CondaQA和DROP作为案例，分别评估关于否定和数量的推理能力。通过提示LLMs生成这些数据集的变体，并根据标注指南评估其有效性和难度，最后将其与人工众包生成的数据集进行比较。

Result: LLMs生成的数据集变体在有效性上符合标注指南，且成本远低于原始的人工众包工作。然而，它们对LLMs本身的挑战性较小，相较于人工生成的版本显得较为简单。

Conclusion: 虽然LLMs可以生成有效的评估数据，但其生成的基准可能缺乏足够的挑战性，提示我们在实际应用中需要谨慎对待这种日益普遍的基准创建方法。

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [308] [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
*Haobo Zhang,Jiayu Zhou*

Main category: cs.CL

TL;DR: 提出OSRM方法，解决LoRA模型合并中的性能下降问题，确保多任务模型的高效整合与单任务准确性。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型以适应特定任务虽然效果好，但部署和存储成本高，现有模型合并方法在处理使用低秩适应（LoRA）微调的模型时往往会导致显著的性能下降。

Method: 提出Orthogonal Subspaces for Robust model Merging (OSRM) 方法，在微调前约束LoRA子空间，保证一个任务的更新不会对其他任务的输出产生不良影响，该方法可与大多数现有合并算法无缝集成，减少任务间的干扰。

Result: 在八个数据集上进行的广泛实验表明，该方法不仅提高了模型合并的性能，还保留了单任务的准确性，并且对合并的超参数具有更强的鲁棒性。

Conclusion: 研究强调了模型合并中数据-参数交互的重要性，并为合并LoRA模型提供了一个即插即用的解决方案。

Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong
performance but is expensive for deployment and storage. Recent works explore
model merging to combine multiple task-specific models into a single multi-task
model without additional training. However, existing merging methods often fail
for models fine-tuned with low-rank adaptation (LoRA), due to significant
performance degradation. In this paper, we show that this issue arises from a
previously overlooked interplay between model parameters and data
distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)
to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates
relevant to one task do not adversely shift outputs for others. Our approach
can seamlessly integrate with most existing merging algorithms, reducing the
unintended interference among tasks. Extensive experiments on eight datasets,
tested with three widely used LMs and two large LMs, demonstrate that our
method not only boosts merging performance but also preserves single-task
accuracy. Furthermore, our approach exhibits greater robustness to the
hyperparameters of merging. These results highlight the importance of
data-parameter interaction in model merging and offer a plug-and-play solution
for merging LoRA models.

</details>


### [309] [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
*Yuchen Zhuang,Di Jin,Jiaao Chen,Wenqi Shi,Hanrui Wang,Chao Zhang*

Main category: cs.CL

TL;DR: WorkForceAgent-R1 is an LLM-based web agent trained with a rule-based R1-style reinforcement learning framework to improve single-step reasoning and planning for business-oriented web navigation tasks. It uses a structured reward function to learn robust intermediate reasoning without explicit annotations or expert demonstrations. Experiments show it outperforms SFT baselines by 10.26-16.59% on the WorkArena benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing web agents using supervised fine-tuning often lack generalization and robustness due to insufficient reasoning capabilities when dealing with dynamic web interactions.

Method: The method involves training an LLM-based web agent, WorkForceAgent-R1, using a rule-based R1-style reinforcement learning framework. A structured reward function is employed to evaluate adherence to output formats and correctness of actions, allowing the agent to learn robust intermediate reasoning implicitly.

Result: WorkForceAgent-R1 outperforms SFT baselines by 10.26-16.59% on the WorkArena benchmark and achieves competitive performance compared to proprietary LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

Conclusion: WorkForceAgent-R1, trained with a rule-based R1-style reinforcement learning framework, successfully enhances single-step reasoning and planning for business-oriented web navigation tasks, demonstrating superior performance over SFT baselines.

Abstract: Large language models (LLMs)-empowered web agents enables automating complex,
real-time web navigation tasks in enterprise environments. However, existing
web agents relying on supervised fine-tuning (SFT) often struggle with
generalization and robustness due to insufficient reasoning capabilities when
handling the inherently dynamic nature of web interactions. In this study, we
introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based
R1-style reinforcement learning framework designed explicitly to enhance
single-step reasoning and planning for business-oriented web navigation tasks.
We employ a structured reward function that evaluates both adherence to output
formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly
learn robust intermediate reasoning without explicit annotations or extensive
expert demonstrations. Extensive experiments on the WorkArena benchmark
demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by
10.26-16.59%, achieving competitive performance relative to proprietary
LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

</details>


### [310] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
*Jaewoo Ahn,Heeseung Yun,Dayoon Ko,Gunhee Kim*

Main category: cs.CL

TL;DR: Pre-trained multimodal representations have compositional vulnerabilities. This paper introduces MAC, a benchmark using LLMs to generate deceptive text samples targeting these vulnerabilities across different modalities.


<details>
  <summary>Details</summary>
Motivation: To address the significant compositional vulnerabilities in pre-trained multimodal representations that lead to counterintuitive judgments.

Method: Introduced MAC benchmark which uses LLMs to create deceptive text samples and evaluates them through sample-wise attack success rate and group-wise entropy-based diversity. Also proposed a self-training approach with rejection-sampling fine-tuning and diversity-promoting filtering.

Result: The self-training approach improves zero-shot methods by enhancing both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, it reveals compositional vulnerabilities across various multimodal representations including images, videos, and audios effectively.

Conclusion: MAC benchmark successfully exploits vulnerabilities in multimodal representations and the proposed self-training method significantly enhances performance.

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [311] [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
*Alisha Srivastava,Emir Korukluoglu,Minh Nhat Le,Duyen Tran,Chau Minh Pham,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 这篇论文探讨了大型语言模型在多语言和跨语言环境下的记忆能力，创建了一个名为OWL的数据集，并通过三个任务评估了不同模型的记忆表现。


<details>
  <summary>Details</summary>
Motivation: 尽管已知大型语言模型能够记忆并回忆英文文本，但其在非英语语言中的记忆能力以及跨语言的记忆能力尚不清楚。

Method: 引入OWL数据集，包含多种语言的书籍摘录，通过直接探测、名字填空和前缀探测三个任务，评估模型在不同语言和大小条件下的记忆能力。

Result: 发现大型语言模型能够在多种语言中持续回忆内容，即使对于没有直接翻译的预训练数据也是如此。例如，GPT-4o在新翻译的摘录中能够以69%的准确率识别作者和书名。

Conclusion: 研究揭示了大型语言模型的跨语言记忆程度，并提供了关于不同模型之间差异的见解。

Abstract: Large language models (LLMs) are known to memorize and recall English text
from their pretraining data. However, the extent to which this ability
generalizes to non-English languages or transfers across languages remains
unclear. This paper investigates multilingual and cross-lingual memorization in
LLMs, probing if memorized content in one language (e.g., English) can be
recalled when presented in translation. To do so, we introduce OWL, a dataset
of 31.5K aligned excerpts from 20 books in ten languages, including English
originals, official translations (Vietnamese, Spanish, Turkish), and new
translations in six low-resource languages (Sesotho, Yoruba, Maithili,
Malagasy, Setswana, Tahitian). We evaluate memorization across model families
and sizes through three tasks: (1) direct probing, which asks the model to
identify a book's title and author; (2) name cloze, which requires predicting
masked character names; and (3) prefix probing, which involves generating
continuations. We find that LLMs consistently recall content across languages,
even for texts without direct translation in pretraining data. GPT-4o, for
example, identifies authors and titles 69% of the time and masked entities 6%
of the time in newly translated excerpts. Perturbations (e.g., masking
characters, shuffling words) modestly reduce direct probing accuracy (7% drop
for shuffled official translations). Our results highlight the extent of
cross-lingual memorization and provide insights on the differences between the
models.

</details>


### [312] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
*Yuhui Zhang,Yuchang Su,Yiming Liu,Serena Yeung-Levy*

Main category: cs.CL

TL;DR: 研究构建了一个名为NegVQA的视觉问答基准，包含7379个涉及否定情境的问题，用以评估20个最先进的视觉语言模型对否定的理解能力。结果表明，这些模型在处理否定时性能显著下降，并且发现了模型规模与性能之间的U型关系。


<details>
  <summary>Details</summary>
Motivation: 否定是语言中的基本现象，能够完全改变句子的意义。随着视觉语言模型被应用在高风险场景中，评估它们对否定的理解变得至关重要。

Method: 通过利用大型语言模型生成现有视觉问答数据集问题的否定版本，构建了NegVQA基准。该基准包括7379个二选一的问题，涵盖了多样化的否定情景和图像-问题分布。使用这个基准，研究者评估了来自七个模型家族的20个最先进的视觉语言模型。

Result: 发现这些视觉语言模型在处理否定相关问题时表现不佳，相较于回答原始问题，性能有显著下降。此外，还揭示了模型规模与NegVQA性能之间存在U型趋势：模型规模增加最初会降低性能，之后才会带来改善。

Conclusion: NegVQA基准揭示了当前视觉语言模型在否定理解方面的关键缺陷，并为未来视觉语言模型的发展提供了有价值的见解。

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [313] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
*Sheng Zhang,Qin Liu,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: This paper explores scaling laws in Electronic Health Records (EHR) foundation models, finding similarities to Large Language Models (LLMs) and providing insights for resource-efficient training.


<details>
  <summary>Details</summary>
Motivation: Scaling laws have greatly influenced the development of large language models (LLMs), but their application to electronic health records (EHRs) remains largely unexplored. EHRs are a rich data source that structurally differ from natural language.

Method: The authors train transformer architectures on patient timeline data from the MIMIC-IV database with varying model sizes and compute budgets to identify scaling patterns in EHR foundation models.

Result: Consistent scaling patterns were found, such as parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility, indicating that EHR models exhibit scaling behavior similar to LLMs.

Conclusion: The findings provide predictive insights into resource-efficient training strategies for EHR models, paving the way for powerful EHR foundation models that can transform clinical prediction tasks and advance personalized healthcare.

Abstract: The emergence of scaling laws has profoundly shaped the development of large
language models (LLMs), enabling predictable performance gains through
systematic increases in model size, dataset volume, and compute. Yet, these
principles remain largely unexplored in the context of electronic health
records (EHRs) -- a rich, sequential, and globally abundant data source that
differs structurally from natural language. In this work, we present the first
empirical investigation of scaling laws for EHR foundation models. By training
transformer architectures on patient timeline data from the MIMIC-IV database
across varying model sizes and compute budgets, we identify consistent scaling
patterns, including parabolic IsoFLOPs curves and power-law relationships
between compute, model parameters, data size, and clinical utility. These
findings demonstrate that EHR models exhibit scaling behavior analogous to
LLMs, offering predictive insights into resource-efficient training strategies.
Our results lay the groundwork for developing powerful EHR foundation models
capable of transforming clinical prediction tasks and advancing personalized
healthcare.

</details>


### [314] [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: VeGraph is a novel framework for claim verification which integrates LLMs' reasoning abilities and operates in three phases (Graph Representation, Entity Disambiguation, Verification) to address challenges in complex real-world claims.


<details>
  <summary>Details</summary>
Motivation: Claim verification requires not only high accuracy but also explainability. The rise of large language models (LLMs) makes this an emerging research issue due to the complexity of real-world claims with intricate semantic structures or obfuscated entities.

Method: VeGraph operates in three phases: Graph Representation where input claims are decomposed into structured triplets forming a graph-based representation; Entity Disambiguation where VeGraph interacts iteratively with the knowledge base to resolve ambiguous entities within the graph; Verification where remaining triplets are verified to complete the fact-checking process.

Result: Experiments using Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive performance compared to baselines on two benchmarks HoVer and FEVEROUS.

Conclusion: VeGraph effectively addresses claim verification challenges by leveraging the reasoning and comprehension abilities of LLM agents.

Abstract: Claim verification is a long-standing and challenging task that demands not
only high accuracy but also explainability of the verification process. This
task becomes an emerging research issue in the era of large language models
(LLMs) since real-world claims are often complex, featuring intricate semantic
structures or obfuscated entities. Traditional approaches typically address
this by decomposing claims into sub-claims and querying a knowledge base to
resolve hidden or ambiguous entities. However, the absence of effective
disambiguation strategies for these entities can compromise the entire
verification process. To address these challenges, we propose
Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and
comprehension abilities of LLM agents. VeGraph operates in three phases: (1)
Graph Representation - an input claim is decomposed into structured triplets,
forming a graph-based representation that integrates both structured and
unstructured information; (2) Entity Disambiguation -VeGraph iteratively
interacts with the knowledge base to resolve ambiguous entities within the
graph for deeper sub-claim verification; and (3) Verification - remaining
triplets are verified to complete the fact-checking process. Experiments using
Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive
performance compared to baselines on two benchmarks HoVer and FEVEROUS,
effectively addressing claim verification challenges. Our source code and data
are available for further exploitation.

</details>


### [315] [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
*Chiwan Park,Wonjun Jang,Daeryong Kim,Aelim Ahn,Kichang Yang,Woosung Hwang,Jihyeon Roh,Hyerin Park,Hyosun Wang,Min Seok Kim,Jihoon Kang*

Main category: cs.CL

TL;DR: The paper discusses the challenges of applying Large Language Models (LLMs) in industrial settings, particularly focusing on maintaining conversational abilities while adhering to service-specific constraints. It presents a case study of an e-commerce conversational agent and introduces a framework for creating scalable, controllable, and reliable AI-driven agents.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of applying state-of-the-art LLM research in industrial settings where there is a need to maintain flexible conversational abilities while strictly complying with service-specific constraints.

Method: Propose strategies to overcome limitations of LLMs in real-world applications through a practical case study of a conversational agent designed for the e-commerce domain, detailing the implementation workflow and optimizations.

Result: Provides insights into bridging the gap between academic research and real-world application by developing a framework for scalable, controllable, and reliable AI-driven agents.

Conclusion: The proposed approach and framework offer valuable insights into successfully deploying LLMs in industrial applications, particularly in the e-commerce sector.

Abstract: The advancement of Large Language Models (LLMs) has led to significant
improvements in various service domains, including search, recommendation, and
chatbot applications. However, applying state-of-the-art (SOTA) research to
industrial settings presents challenges, as it requires maintaining flexible
conversational abilities while also strictly complying with service-specific
constraints. This can be seen as two conflicting requirements due to the
probabilistic nature of LLMs. In this paper, we propose our approach to
addressing this challenge and detail the strategies we employed to overcome
their inherent limitations in real-world applications. We conduct a practical
case study of a conversational agent designed for the e-commerce domain,
detailing our implementation workflow and optimizations. Our findings provide
insights into bridging the gap between academic research and real-world
application, introducing a framework for developing scalable, controllable, and
reliable AI-driven agents.

</details>


### [316] [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
*Haewon Park,Gyubin Choi,Minjun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: Knowledge editing (KE) methods modify knowledge in large language models. Current evaluations lack context consideration, so CHED benchmark is developed to evaluate context robustness of KE methods. CoRE method strengthens context robustness by minimizing context-sensitive variance.


<details>
  <summary>Details</summary>
Motivation: Current KE evaluations typically assess editing success without any preceding contexts, but in real-world applications, preceding contexts often trigger the retrieval of the original knowledge and undermine the intended edit.

Method: Develop CHED -- a benchmark designed to evaluate the context robustness of KE methods; introduce CoRE, a KE method designed to strengthen context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge.

Result: Evaluations on CHED show that KE methods often fail when preceding contexts are present. CoRE not only improves the editing success rate in situations where a preceding context is present but also preserves the overall capabilities of the model.

Conclusion: The paper concludes with an in-depth analysis of the differing impacts of preceding contexts when introduced as user utterances versus assistant responses, and dissects attention-score patterns to assess how specific tokens influence editing success.

Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in
large language models. Current KE evaluations typically assess editing success
by considering only the edited knowledge without any preceding contexts. In
real-world applications, however, preceding contexts often trigger the
retrieval of the original knowledge and undermine the intended edit. To address
this issue, we develop CHED -- a benchmark designed to evaluate the context
robustness of KE methods. Evaluations on CHED show that they often fail when
preceding contexts are present. To mitigate this shortcoming, we introduce
CoRE, a KE method designed to strengthen context robustness by minimizing
context-sensitive variance in hidden states of the model for edited knowledge.
This method not only improves the editing success rate in situations where a
preceding context is present but also preserves the overall capabilities of the
model. We provide an in-depth analysis of the differing impacts of preceding
contexts when introduced as user utterances versus assistant responses, and we
dissect attention-score patterns to assess how specific tokens influence
editing success.

</details>


### [317] [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: MBR-OT是一种基于Wasserstein距离的MBR解码变体，用于文档级文本生成任务，效果优于标准MBR。


<details>
  <summary>Details</summary>
Motivation: 文档级文本生成任务比句子级更难，需要理解更长的上下文以生成高质量文本，但现有的MBR解码在文档级任务上的表现受限于句子级效用函数的设计。

Method: 提出MBR-OT，利用Wasserstein距离计算文档效用，结合句子级效用函数进行文档级文本生成。

Result: 实验表明，MBR-OT在文档级机器翻译、文本简化和密集图像说明任务中的表现优于标准MBR。

Conclusion: MBR-OT通过改进效用计算方法提升了文档级文本生成任务的效果。

Abstract: Document-level text generation tasks are known to be more difficult than
sentence-level text generation tasks as they require the understanding of
longer context to generate high-quality texts. In this paper, we investigate
the adaption of Minimum Bayes Risk (MBR) decoding for document-level text
generation tasks. MBR decoding makes use of a utility function to estimate the
output with the highest expected utility from a set of candidate outputs.
Although MBR decoding is shown to be effective in a wide range of
sentence-level text generation tasks, its performance on document-level text
generation tasks is limited as many of the utility functions are designed for
evaluating the utility of sentences. To this end, we propose MBR-OT, a variant
of MBR decoding using Wasserstein distance to compute the utility of a document
using a sentence-level utility function. The experimental result shows that the
performance of MBR-OT outperforms that of the standard MBR in document-level
machine translation, text simplification, and dense image captioning tasks. Our
code is available at https://github.com/jinnaiyuu/mbr-optimal-transport

</details>


### [318] [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
*Linjie Mu,Zhongzhen Huang,Yakun Zhu,Xiangyu Zhao,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为MedE²的两阶段后训练管道，用于激发和增强医学领域的多模态推理能力。通过大量实验验证了其在提高医学多模态模型推理性能方面的有效性、可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推理模型在数学和科学领域取得了显著成功，但在医学领域的应用仍待探索。需要一种方法来激发和增强医学领域中的多模态推理能力。

Method: 提出了MedE²，一个两阶段后训练管道：第一阶段使用2000个精心设计的文本数据样本微调模型以激发推理行为；第二阶段使用1500个多模态医学案例进一步增强模型推理能力，并与提出的多模态医学推理偏好对齐。

Result: MedE²显著提高了医学多模态模型的推理性能，在多个医学多模态基准上表现优于基线模型。额外验证表明该方法在更大模型和推理时间扩展下具有鲁棒性和实用性。

Conclusion: MedE²是一种有效且可靠的工具，可以提升医学多模态模型的推理性能，适用于实际应用场景。

Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning
across diverse sources of evidence. The recent emergence of multimodal
reasoning models has significantly transformed the landscape of solving complex
tasks. Although such models have achieved notable success in mathematics and
science, their application to medical domains remains underexplored. In this
work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that
elicits and then enhances multimodal reasoning for medical domains. In Stage-I,
we fine-tune models using 2,000 text-only data samples containing precisely
orchestrated reasoning demonstrations to elicit reasoning behaviors. In
Stage-II, we further enhance the model's reasoning capabilities using 1,500
rigorously curated multimodal medical cases, aligning model reasoning outputs
with our proposed multimodal medical reasoning preference. Extensive
experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in
improving the reasoning performance of medical multimodal models. Notably,
models trained with \textit{MedE$^2$} consistently outperform baselines across
multiple medical multimodal benchmarks. Additional validation on larger models
and under inference-time scaling further confirms the robustness and practical
utility of our approach.

</details>


### [319] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
*Yiming Lei,Zhizheng Yang,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: Multi-modal large language models, though powerful, struggle with multi-turn interactions. This paper introduces ContextQFormer to enhance contextual information and TMDialog, a new dataset for multi-turn multi-modal dialogues. Experiments show ContextQFormer improves over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing open-source multi-modal models have weak multi-turn interaction capabilities, especially for long contexts.

Method: Introduced ContextQFormer, a context modeling module using a memory block to enhance contextual information, and TMDialog, a new multi-turn multi-modal dialogue dataset.

Result: ContextQFormer shows an improvement of 2%-4% in available rate over baselines on TMDialog.

Conclusion: ContextQFormer and TMDialog contribute to better multi-turn multi-modal interactions.

Abstract: Multi-modal large language models have demonstrated remarkable zero-shot
abilities and powerful image-understanding capabilities. However, the existing
open-source multi-modal models suffer from the weak capability of multi-turn
interaction, especially for long contexts. To address the issue, we first
introduce a context modeling module, termed ContextQFormer, which utilizes a
memory block to enhance the presentation of contextual information.
Furthermore, to facilitate further research, we carefully build a new
multi-turn multi-modal dialogue dataset (TMDialog) for pre-training,
instruction-tuning, and evaluation, which will be open-sourced lately. Compared
with other multi-modal dialogue datasets, TMDialog contains longer
conversations, which supports the research of multi-turn multi-modal dialogue.
In addition, ContextQFormer is compared with three baselines on TMDialog and
experimental results illustrate that ContextQFormer achieves an improvement of
2%-4% in available rate over baselines.

</details>


### [320] [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
*Gabriele Sarti,Vilém Zouhar,Malvina Nissim,Arianna Bisazza*

Main category: cs.CL

TL;DR: This paper explores efficient alternatives for word-level quality estimation (WQE) using advances in language model interpretability and uncertainty quantification, evaluates 14 metrics across 12 translation directions, and highlights issues with label uncertainty and single-annotator evaluations.


<details>
  <summary>Details</summary>
Motivation: Modern WQE techniques are costly due to reliance on large language models or extensive human-labeled data training.

Method: Investigates the use of unsupervised metrics derived from language model interpretability and uncertainty quantification to identify translation errors without additional training.

Result: Unsupervised metrics show potential, supervised methods struggle with label uncertainty, and single-annotator evaluations are unreliable.

Conclusion: Efficient WQE alternatives based on unsupervised approaches could reduce costs while addressing challenges posed by label uncertainty and evaluation practices.

Abstract: Word-level quality estimation (WQE) aims to automatically identify
fine-grained error spans in machine-translated outputs and has found many uses,
including assisting translators during post-editing. Modern WQE techniques are
often expensive, involving prompting of large language models or ad-hoc
training on large amounts of human-labeled data. In this work, we investigate
efficient alternatives exploiting recent advances in language model
interpretability and uncertainty quantification to identify translation errors
from the inner workings of translation models. In our evaluation spanning 14
metrics across 12 translation directions, we quantify the impact of human label
variation on metric performance by using multiple sets of human labels. Our
results highlight the untapped potential of unsupervised metrics, the
shortcomings of supervised methods when faced with label uncertainty, and the
brittleness of single-annotator evaluation practices.

</details>


### [321] [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
*Yilong Li,Chen Qian,Yu Xia,Ruijie Shi,Yufan Dang,Zihao Xie,Ziming You,Weize Chen,Cheng Yang,Weichuan Liu,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: MAEL is a new framework that improves multi-agent systems by allowing agents to learn across tasks, enhancing efficiency and solution quality.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems perform redundant computations and lack generalization across similar tasks.

Method: Model task-solving as a graph-structured collaboration network. Use experiential learning to store and retrieve task-relevant experiences for better reasoning.

Result: Experiments show MAEL enables faster convergence and higher-quality solutions on various datasets.

Conclusion: MAEL effectively allows agents to learn from past experiences, improving task-solving performance.

Abstract: Large Language Model-based multi-agent systems (MAS) have shown remarkable
progress in solving complex tasks through collaborative reasoning and
inter-agent critique. However, existing approaches typically treat each task in
isolation, resulting in redundant computations and limited generalization
across structurally similar tasks. To address this, we introduce multi-agent
cross-task experiential learning (MAEL), a novel framework that endows
LLM-driven agents with explicit cross-task learning and experience
accumulation. We model the task-solving workflow on a graph-structured
multi-agent collaboration network, where agents propagate information and
coordinate via explicit connectivity. During the experiential learning phase,
we quantify the quality for each step in the task-solving workflow and store
the resulting rewards along with the corresponding inputs and outputs into each
agent's individual experience pool. During inference, agents retrieve
high-reward, task-relevant experiences as few-shot examples to enhance the
effectiveness of each reasoning step, thereby enabling more accurate and
efficient multi-agent collaboration. Experimental results on diverse datasets
demonstrate that MAEL empowers agents to learn from prior task experiences
effectively-achieving faster convergence and producing higher-quality solutions
on current tasks.

</details>


### [322] [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bibo Cai,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: An autonomous experience transfer framework is designed to allow LLMs to mimic human cognitive intelligence for transferring experience from source tasks to target tasks, reducing costs and improving performance.


<details>
  <summary>Details</summary>
Motivation: To solve the impracticality of relying on substantial human labor or time to gather textual task-solving experiences for each task due to the growing variety of task types in user queries to LLMs.

Method: Design an autonomous experience transfer framework that enables LLMs to autonomously transfer experience from existing source tasks to newly encountered target tasks.

Result: Experimental results on 13 datasets demonstrate the effective improvement of LLMs' performance.

Conclusion: This framework not only allows the acquisition of experience without extensive costs but also offers a novel path for the generalization of LLMs.

Abstract: Recent studies provide large language models (LLMs) with textual task-solving
experiences via prompts to improve their performance. However, previous methods
rely on substantial human labor or time to gather such experiences for each
task, which is impractical given the growing variety of task types in user
queries to LLMs. To address this issue, we design an autonomous experience
transfer framework to explore whether LLMs can mimic human cognitive
intelligence to autonomously transfer experience from existing source tasks to
newly encountered target tasks. This not only allows the acquisition of
experience without extensive costs of previous methods, but also offers a novel
path for the generalization of LLMs. Experimental results on 13 datasets
demonstrate that our framework effectively improves the performance of LLMs.
Furthermore, we provide a detailed analysis of each module in the framework.

</details>


### [323] [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
*Hao Lu,Yanchi Gu,Haoyuan Huang,Yulin Zhou,Ningxin Zhu,Chen Li*

Main category: cs.CL

TL;DR: MCTSr-Zero是一种专为开放式、以人为中心的对话设计的蒙特卡洛树搜索框架，通过域对齐、重生和元提示适应机制来生成高质量对话数据，实验表明其在心理辅导对话中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结果导向的MCTS方法在处理如心理辅导等开放性对话时存在局限性，因为这些对话的成功依赖于主观因素（如共情、伦理遵循和与人类偏好的一致性），而非明确的正确性标准。

Method: MCTSr-Zero引入了“域对齐”以将搜索目标从预定义的终态转向符合目标领域原则的对话轨迹；同时采用“重生”和“元提示适应”机制来扩展探索空间，考虑不同的初始对话策略。

Result: 通过生成多轮对话数据微调LLM（PsyLLM）并在新基准PsyEval上进行评估，结果显示PsyLLM在多个相关指标上达到了最先进的性能。

Conclusion: MCTSr-Zero有效地生成了高质量、符合原则的对话数据，解决了LLM在复杂心理标准下的一致性问题，适用于以人为中心的领域对话。

Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models
(LLMs) has demonstrated significant success in structured, problem-oriented
tasks. However, applying these methods to open-ended dialogues, such as those
in psychological counseling, presents unique challenges. Unlike tasks with
objective correctness, success in therapeutic conversations depends on
subjective factors like empathetic engagement, ethical adherence, and alignment
with human preferences, for which strict "correctness" criteria are
ill-defined. Existing result-oriented MCTS approaches can therefore produce
misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS
framework designed for open-ended, human-centric dialogues. Its core innovation
is "domain alignment", which shifts the MCTS search objective from predefined
end-states towards conversational trajectories that conform to target domain
principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates
"Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden
exploration by allowing the MCTS to consider fundamentally different initial
dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by
generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.
We also introduce PsyEval, a benchmark for assessing multi-turn psychological
counseling dialogues. Experiments demonstrate that PsyLLM achieves
state-of-the-art performance on PsyEval and other relevant metrics, validating
MCTSr-Zero's effectiveness in generating high-quality, principle-aligned
conversational data for human-centric domains and addressing the LLM challenge
of consistently adhering to complex psychological standards.

</details>


### [324] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
*Maged S. Al-Shaibani,Moataz Ahmed*

Main category: cs.CL

TL;DR: This paper conducts a comprehensive investigation into Arabic machine-generated text using various generation strategies and model architectures, revealing detectable signatures in LLM outputs and developing BERT-based detection models with high performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by Large Language Models (LLMs) generating human-like Arabic text which can compromise information integrity across critical domains including education, social media, and academia.

Method: Examining multiple generation strategies (title-only generation, content-aware generation, and text refinement) across diverse model architectures (ALLaM, Jais, Llama, GPT-4) in academic and social media domains. Performing stylometric analysis to identify distinctive linguistic patterns differentiating human-written from machine-generated Arabic text.

Result: LLMs produce detectable signatures in their Arabic outputs with domain-specific characteristics that vary significantly between different contexts. BERT-based detection models achieved exceptional performance in formal contexts (up to 99.9% F1-score). Cross-domain analysis confirms generalization challenges.

Conclusion: This work represents the most comprehensive investigation of Arabic machine-generated text to date, combining multiple prompt generation methods, diverse model architectures, and in-depth stylometric analysis across varied textual domains.

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [325] [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
*Yong Zhang,Yanwen Huang,Ning Cheng,Yang Guo,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Main category: cs.CL

TL;DR: The paper introduces Sentinel, a lightweight sentence-level compression framework for Retrieval-augmented generation (RAG). It repurposes decoder attention from an off-the-shelf proxy LLM and uses a lightweight classifier to identify sentence relevance. This approach allows for significant context compression while maintaining QA performance.


<details>
  <summary>Details</summary>
Motivation: Retrieved passages used in RAG can be lengthy, noisy, or exceed input limits. Current compression methods require supervised training of dedicated models, which is costly and reduces portability. There is a need for a more efficient and portable method for context compression.

Method: Sentinel reframes context filtering as an attention-based understanding task. It probes decoder attention from a 0.5B proxy LLM using a lightweight classifier to determine sentence relevance. This avoids the need for training a separate compression model.

Result: On the LongBench benchmark, Sentinel achieves up to 5x compression while matching the QA performance of larger 7B-scale compression systems.

Conclusion: Probing native attention signals from smaller proxy models can enable fast, effective, and question-aware context compression. This approach offers a cost-effective alternative to training dedicated compression models.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external context, but retrieved passages are often lengthy, noisy, or
exceed input limits. Existing compression methods typically require supervised
training of dedicated compression models, increasing cost and reducing
portability. We propose Sentinel, a lightweight sentence-level compression
framework that reframes context filtering as an attention-based understanding
task. Rather than training a compression model, Sentinel probes decoder
attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier
to identify sentence relevance. Empirically, we find that query-context
relevance estimation is consistent across model scales, with 0.5B proxies
closely matching the behaviors of larger models. On the LongBench benchmark,
Sentinel achieves up to 5$\times$ compression while matching the QA performance
of 7B-scale compression systems. Our results suggest that probing native
attention signals enables fast, effective, and question-aware context
compression. Code available at: https://github.com/yzhangchuck/Sentinel.

</details>


### [326] [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
*James Xu Zhao,Jimmy Z. J. Liu,Bryan Hooi,See-Kiong Ng*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在长文本生成中广泛应用，但其响应中的事实错误影响了可靠性。本文通过引入自动双层长文本事实评估框架，发现响应越长事实精度越低，并揭示了主要原因在于模型逐渐耗尽更可靠的知识。


<details>
  <summary>Details</summary>
Motivation: 研究长篇响应生成中事实性偏差的问题，以提高LLMs的可靠性。

Method: 提出自动双层长文本事实评估框架，进行受控实验并验证三种假设：错误传播、长上下文和知识耗尽。

Result: 发现长响应的事实精度较低，且主要原因是知识耗尽而非其他两种假设。

Conclusion: 长度偏差确实存在，需进一步研究和改进LLMs以减少长文本生成中的事实错误。

Abstract: Large language models (LLMs) are widely used for long-form text generation.
However, factual errors in the responses would undermine their reliability.
Despite growing attention to LLM factuality, the effect of response length on
factuality remains underexplored. In this work, we systematically investigate
this relationship by first introducing an automatic and bi-level long-form
factuality evaluation framework, which achieves high agreement with human
annotations while being cost-effective. Using this framework, we conduct
controlled experiments and find that longer responses exhibit lower factual
precision, confirming the presence of length bias. To explain this phenomenon,
we empirically examine three hypotheses: error propagation, long context, and
facts exhaustion. Our results reveal that facts exhaustion, where the model
gradually exhausts more reliable knowledge, is the primary cause of factual
degradation, rather than the other two hypotheses.

</details>


### [327] [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
*Abhirup Chakravarty,Mark Brenchley,Trevor Breakspear,Ian Lewin,Yan Huang*

Main category: cs.CL

TL;DR: An AES confidence model is developed using a classification approach with novel loss functions, achieving high reliability in scoring and enabling selective score release.


<details>
  <summary>Details</summary>
Motivation: To address the ethical challenge of ensuring high reliability standards in Automated Essay Scoring (AES) before releasing scores.

Method: Frame confidence estimation as a classification task predicting correct CEFR level placement. Reformulate it as an n-ary classification problem using score binning and introduce KWOCCE loss functions that consider the ordinal structure of CEFR labels.

Result: The best-performing model achieved an F1 score of 0.97, allowing 47% of scores to be released with 100% CEFR agreement and 99% with at least 95% CEFR agreement, improving upon the standalone AES model's ~92% CEFR agreement.

Conclusion: The developed confidence model significantly enhances the reliability of AES scores through selective score release based on high-confidence predictions.

Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that
scores are only released when they meet high reliability standards. Confidence
modelling addresses this by assigning a reliability estimate measure, in the
form of a confidence score, to each automated score. In this study, we frame
confidence estimation as a classification task: predicting whether an
AES-generated score correctly places a candidate in the appropriate CEFR level.
While this is a binary decision, we leverage the inherent granularity of the
scoring domain in two ways. First, we reformulate the task as an n-ary
classification problem using score binning. Second, we introduce a set of novel
Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that
incorporate the ordinal structure of CEFR labels. Our best-performing model
achieves an F1 score of 0.97, and enables the system to release 47% of scores
with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to
approximately 92% (approx.) CEFR agreement from the standalone AES model where
we release all AM predicted scores.

</details>


### [328] [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
*Nawar Turk,Eeham Khan,Leila Kosseim*

Main category: cs.CL

TL;DR: The paper presents three model architectures for the SemEval-2025 Task 6 (PromiseEval) focusing on verifying promises in corporate ESG reports. The first model uses ESG-BERT with task-specific classifier heads, the second enhances this architecture with linguistic features, and the third implements a combined subtask model with attention-based sequence pooling and multi-objective learning. Experiments show progressive improvement across models, with the combined subtask approach outperforming the baseline.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying promises in corporate ESG reports by exploring different model architectures for the four subtasks of promise identification, supporting evidence assessment, clarity evaluation, and verification timing.

Method: Three model architectures were explored: 1) ESG-BERT with task-specific classifier heads, 2) Enhanced architecture with linguistic features tailored for each subtask, 3) Combined subtask model with attention-based sequence pooling, transformer representations augmented with document metadata, and multi-objective learning.

Result: Experiments on the English portion of the ML-Promise dataset demonstrate progressive improvement across the models. The combined subtask approach achieved a leaderboard score of 0.5268, surpassing the provided baseline of 0.5227.

Conclusion: The work highlights the effectiveness of linguistic feature extraction, attention pooling, and multi-objective learning in promise verification tasks, despite challenges such as class imbalance and limited training data.

Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),
which focuses on verifying promises in corporate ESG (Environmental, Social,
and Governance) reports. We explore three model architectures to address the
four subtasks of promise identification, supporting evidence assessment,
clarity evaluation, and verification timing. Our first model utilizes ESG-BERT
with task-specific classifier heads, while our second model enhances this
architecture with linguistic features tailored for each subtask. Our third
approach implements a combined subtask model with attention-based sequence
pooling, transformer representations augmented with document metadata, and
multi-objective learning. Experiments on the English portion of the ML-Promise
dataset demonstrate progressive improvement across our models, with our
combined subtask approach achieving a leaderboard score of 0.5268,
outperforming the provided baseline of 0.5227. Our work highlights the
effectiveness of linguistic feature extraction, attention pooling, and
multi-objective learning in promise verification tasks, despite challenges
posed by class imbalance and limited training data.

</details>


### [329] [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
*Guangtao Zeng,Maohao Shen,Delin Chen,Zhenting Qi,Subhro Das,Dan Gutfreund,David Cox,Gregory Wornell,Wei Lu,Zhang-Wei Hong,Chuang Gan*

Main category: cs.CL

TL;DR: EvoScale是一种高效的测试时扩展方法，通过进化过程优化生成结果，减少样本需求，并利用强化学习实现模型的自进化，从而在少量样本下达到甚至超越参数量超过100B模型的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在标准化编码基准上表现出色，但在实际软件工程任务（如SWE-Bench中的GitHub问题解决）中表现不佳，特别是当模型参数少于100B时。现有的改进方法要么依赖高质量数据的监督微调，要么需要大量采样和昂贵的评分策略，限制了其应用。

Method: 提出了一种名为EvoScale的方法，将生成过程视为进化过程，通过选择和变异迭代地优化输出，使输出分布向高分区域移动。同时，采用强化学习训练模型以实现自进化，减少了反复采样和选择的开销，并让模型在推理时能够自我改进生成结果的分数。

Result: 在SWE-Bench-Verified上的评估表明，EvoScale使得32B参数的模型（Satori-SWE-32B）仅使用少量样本即可匹配或超过参数量超过100B的模型的性能。

Conclusion: EvoScale提供了一种样本高效且成本较低的测试时扩展方法，显著提高了小规模模型在实际软件工程任务中的性能，并计划开源代码、数据和模型。

Abstract: Language models (LMs) perform well on standardized coding benchmarks but
struggle with real-world software engineering tasks such as resolving GitHub
issues in SWE-Bench, especially when model parameters are less than 100B. While
smaller models are preferable in practice due to their lower computational
cost, improving their performance remains challenging. Existing approaches
primarily rely on supervised fine-tuning (SFT) with high-quality data, which is
expensive to curate at scale. An alternative is test-time scaling: generating
multiple outputs, scoring them using a verifier, and selecting the best one.
Although effective, this strategy often requires excessive sampling and costly
scoring, limiting its practical application. We propose Evolutionary Test-Time
Scaling (EvoScale), a sample-efficient method that treats generation as an
evolutionary process. By iteratively refining outputs via selection and
mutation, EvoScale shifts the output distribution toward higher-scoring
regions, reducing the number of samples needed to find correct solutions. To
reduce the overhead from repeatedly sampling and selection, we train the model
to self-evolve using reinforcement learning (RL). Rather than relying on
external verifiers at inference time, the model learns to self-improve the
scores of its own generations across iterations. Evaluated on
SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or
exceed the performance of models with over 100B parameters while using a few
samples. Code, data, and models will be fully open-sourced.

</details>


### [330] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
*Jiaxin Bai,Wei Fan,Qi Hu,Qing Zong,Chunyang Li,Hong Ting Tsang,Hongyu Luo,Yauwai Yim,Haoyu Huang,Xiao Zhou,Feng Qin,Tianshi Zheng,Xi Peng,Xin Yao,Huiwen Yang,Leijie Wu,Yi Ji,Gong Zhang,Renhai Chen,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoSchemaKG is a framework that uses large language models to construct knowledge graphs without predefined schemas, extracting triples and inducing schemas from text. It processes 50 million documents to create ATLAS, a family of knowledge graphs with over 900 million nodes and 5.9 billion edges, outperforming baselines in multi-hop QA tasks and enhancing LLM factuality.


<details>
  <summary>Details</summary>
Motivation: To eliminate the need for predefined schemas in knowledge graph construction and leverage large language models to extract knowledge triples and induce comprehensive schemas directly from text.

Method: The system processes large amounts of documents using large language models to simultaneously extract knowledge triples and induce schemas, modeling both entities and events while employing conceptualization to organize instances into semantic categories.

Result: The approach constructs ATLAS, a family of knowledge graphs with 900+ million nodes and 5.9 billion edges, outperforming state-of-the-art baselines on multi-hop QA tasks and enhancing LLM factuality. The schema induction achieves 95% semantic alignment with human-crafted schemas without manual intervention.

Conclusion: Billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.

Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph
construction that eliminates the need for predefined schemas. Our system
leverages large language models to simultaneously extract knowledge triples and
induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic
categories. Processing over 50 million documents, we construct ATLAS (Automated
Triple Linking And Schema induction), a family of knowledge graphs with 900+
million nodes and 5.9 billion edges. This approach outperforms state-of-the-art
baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our
schema induction achieves 95\% semantic alignment with human-crafted schemas
with zero manual intervention, demonstrating that billion-scale knowledge
graphs with dynamically induced schemas can effectively complement parametric
knowledge in large language models.

</details>


### [331] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: Active Layer-Contrastive Decoding (ActLCD) is a new decoding strategy that uses reinforcement learning to improve the factuality of large language models.


<details>
  <summary>Details</summary>
Motivation: Despite recent improvements in decoding methods for enhancing factuality in large language models, these models still struggle with hallucinations, especially over longer contexts.

Method: ActLCD casts decoding as a sequential decision-making problem and employs a reinforcement learning policy guided by a reward-aware classifier to decide when to apply contrasting layers during generation.

Result: Experiments show that ActLCD surpasses state-of-the-art methods across five benchmarks, effectively mitigating hallucinations in various generation scenarios.

Conclusion: ActLCD is an effective decoding strategy that improves the factuality of large language models.

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [332] [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
*Arthur S. Bianchessi,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CL

TL;DR: Transformer-based language models use positional encoding (PE) for token order and context length extrapolation, but current PE methods lack theoretical clarity. This paper introduces Bayesian Attention Mechanism (BAM), which treats PE as a prior in a probabilistic model, unifying existing methods and proposing a new Generalized Gaussian positional prior that enhances long-context generalization. BAM achieves superior performance in long context retrieval accuracy while maintaining perplexity and adding minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical ambiguity and limited evaluation metrics of current positional encoding methods in transformer-based language models.

Method: The paper proposes the Bayesian Attention Mechanism (BAM), which formulates positional encoding as a prior within a probabilistic model. This framework unifies existing methods and introduces a new Generalized Gaussian positional prior.

Result: BAM enables accurate information retrieval at 500 times the training context length, outperforming previous state-of-the-art methods in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.

Conclusion: BAM provides a theoretically grounded approach to positional encoding, enhancing long-context generalization in transformer-based language models.

Abstract: Transformer-based language models rely on positional encoding (PE) to handle
token order and support context length extrapolation. However, existing PE
methods lack theoretical clarity and rely on limited evaluation metrics to
substantiate their extrapolation claims. We propose the Bayesian Attention
Mechanism (BAM), a theoretical framework that formulates positional encoding as
a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE
and ALiBi) and motivates a new Generalized Gaussian positional prior that
substantially improves long-context generalization. Empirically, BAM enables
accurate information retrieval at $500\times$ the training context length,
outperforming previous state-of-the-art context length generalization in long
context retrieval accuracy while maintaining comparable perplexity and
introducing minimal additional parameters.

</details>


### [333] [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
*Roksana Goworek,Harpal Karlcut,Muhammad Shezad,Nijaguna Darshana,Abhishek Mane,Syam Bondada,Raghav Sikka,Ulvi Mammadov,Rauf Allahverdiyev,Sriram Purighella,Paridhi Gupta,Muhinyia Ndegwa,Haim Dubossarsky*

Main category: cs.CL

TL;DR: This paper addresses the need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer by releasing new sense-annotated datasets of sentences containing polysemous words, spanning nine low-resource languages. They also present a semi-automatic annotation method and demonstrate the utility of the datasets through experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is the critical need for high-quality evaluation datasets in low-resource languages to improve cross-lingual transfer and expand language technologies to understudied and typologically diverse languages.

Method: They release new sense-annotated datasets of sentences containing polysemous words, spanning nine low-resource languages across diverse language families and scripts. A demonstrably beneficial semi-automatic annotation method is presented to facilitate dataset creation.

Result: Results from Word-in-Context (WiC) formatted experiments highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings and transfer studies.

Conclusion: The released datasets and code aim to support further research into fair, robust, and truly multilingual NLP.

Abstract: This paper addresses the critical need for high-quality evaluation datasets
in low-resource languages to advance cross-lingual transfer. While
cross-lingual transfer offers a key strategy for leveraging multilingual
pretraining to expand language technologies to understudied and typologically
diverse languages, its effectiveness is dependent on quality and suitable
benchmarks. We release new sense-annotated datasets of sentences containing
polysemous words, spanning nine low-resource languages across diverse language
families and scripts. To facilitate dataset creation, the paper presents a
demonstrably beneficial semi-automatic annotation method. The utility of the
datasets is demonstrated through Word-in-Context (WiC) formatted experiments
that evaluate transfer on these low-resource languages. Results highlight the
importance of targeted dataset creation and evaluation for effective polysemy
disambiguation in low-resource settings and transfer studies. The released
datasets and code aim to support further research into fair, robust, and truly
multilingual NLP.

</details>


### [334] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: An agentic ML training framework with exploration-enriched fine-tuning, step-wise RL, and a reward module is proposed to train ML-Agent based on Qwen-2.5 LLM for autonomous ML.


<details>
  <summary>Details</summary>
Motivation: Existing approaches in autonomous ML engineering rely heavily on manual prompt engineering, which lacks adaptability and optimization based on diverse experimental experiences.

Method: The method involves a novel agentic ML training framework that includes exploration-enriched fine-tuning for diverse actions, step-wise RL for efficient training, and a reward module for unifying feedback signals.

Result: ML-Agent, driven by a 7B-sized Qwen-2.5 LLM, outperforms the 671B-sized DeepSeek-R1 agent despite being trained on only 9 ML tasks. It shows continuous performance improvements and strong cross-task generalization.

Conclusion: The learning-based agentic ML paradigm, realized through the proposed framework, successfully enables an LLM agent to autonomously perform ML tasks with superior performance and adaptability.

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [335] [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
*Mohamad Chehade,Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Dinesh Manocha,Hao Zhu,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: SITAlign is an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria.


<details>
  <summary>Details</summary>
Motivation: Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. Existing approaches typically frame this as a multi-objective optimization problem but often overlook human decision-making strategies, such as satisficing.

Method: The proposed method, SITAlign, operationalizes the notion of satisficing alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. Theoretical insights are provided through sub-optimality bounds derivation.

Result: SITAlign outperforms the state-of-the-art multi-objective decoding strategy by 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness on the PKU-SafeRLHF dataset.

Conclusion: SITAlign bridges the gap between existing multi-objective optimization approaches and human decision-making strategies, providing both theoretical and empirical support for its effectiveness.

Abstract: Aligning large language models with humans is challenging due to the
inherently multifaceted nature of preference feedback. While existing
approaches typically frame this as a multi-objective optimization problem, they
often overlook how humans actually make decisions. Research on bounded
rationality suggests that human decision making follows satisficing
strategies-optimizing primary objectives while ensuring others meet acceptable
thresholds. To bridge this gap and operationalize the notion of satisficing
alignment, we propose SITAlign: an inference time framework that addresses the
multifaceted nature of alignment by maximizing a primary objective while
satisfying threshold-based constraints on secondary criteria. We provide
theoretical insights by deriving sub-optimality bounds of our satisficing based
inference alignment approach. We empirically validate SITAlign's performance
through extensive experimentation on multiple benchmarks. For instance, on the
PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while
ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art
multi objective decoding strategy by a margin of 22.3% in terms of GPT-4
win-tie rate for helpfulness reward while adhering to the threshold on
harmlessness.

</details>


### [336] [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
*Ali Behrouz,Zeman Li,Praneeth Kacham,Majid Daliri,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.CL

TL;DR: ATLAS is a new long-term memory module that overcomes the online nature of long-term memory models and enhances memory capacity, leading to improved performance in language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic memory and time complexity which limits their applicability in longer sequences. Modern recurrent neural networks also struggle with tasks requiring long context understanding and extrapolation to longer sequences.

Method: ATLAS optimizes memory based on current and past tokens, overcoming the online nature of long-term memory models. DeepTransformers, a new family of Transformer-like architectures, are strict generalizations of the original Transformer architecture.

Result: Experimental results show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. It further improves the long context performance of Titans, achieving +80% accuracy in 10M context length of BABILong benchmark.

Conclusion: ATLAS and DeepTransformers provide significant improvements in tasks requiring long context understanding and extrapolation to longer sequences.

Abstract: Transformers have been established as the most popular backbones in sequence
modeling, mainly due to their effectiveness in in-context retrieval tasks and
the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated
researchers to explore effective alternative architectures such as modern
recurrent neural networks (a.k.a long-term recurrent memory module). Despite
their recent success in diverse downstream tasks, they struggle in tasks that
requires long context understanding and extrapolation to longer sequences. We
observe that these shortcomings come from three disjoint aspects in their
design: (1) limited memory capacity that is bounded by the architecture of
memory and feature mapping of the input; (2) online nature of update, i.e.,
optimizing the memory only with respect to the last input; and (3) less
expressive management of their fixed-size memory. To enhance all these three
aspects, we present ATLAS, a long-term memory module with high capacity that
learns to memorize the context by optimizing the memory based on the current
and past tokens, overcoming the online nature of long-term memory models.
Building on this insight, we present a new family of Transformer-like
architectures, called DeepTransformers, that are strict generalizations of the
original Transformer architecture. Our experimental results on language
modeling, common-sense reasoning, recall-intensive, and long-context
understanding tasks show that ATLAS surpasses the performance of Transformers
and recent linear recurrent models. ATLAS further improves the long context
performance of Titans, achieving +80\% accuracy in 10M context length of
BABILong benchmark.

</details>


### [337] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: 大型语言模型在说服任务中展现出潜力，但现有的LLM说服者仍存在初步性问题。为解决当前LLM缺乏心理理论（ToM）推理的问题，研究提出了ToMAP方法，通过加入两个ToM模块提升说服者的对手意识和分析能力。实验表明，尽管参数量仅为3B，ToMAP在多个说服对象模型和多样语料库上超越了如GPT-4o等更大基线模型，相对增益达39.4%。


<details>
  <summary>Details</summary>
Motivation: 人类擅长主动、动态地模拟对手的想法和观点，而当前的LLM在这方面表现不佳，导致多样性不足和对手意识有限。因此，需要一种新方法来增强LLM的心理理论推理能力，从而生成更多样化、更有效的说服论点。

Method: 提出了一种名为ToMAP的方法，包含两个心理理论模块：首先提示说服者考虑目标核心主张可能的反对意见，然后使用文本编码器和训练好的MLP分类器预测对手对这些反对意见的立场。此外，设计了一种强化学习方案，使说服者学会分析对手相关信息并生成更有效的论点。

Result: ToMAP在多个说服对象模型和多样语料库上表现出色，尽管参数量仅为3B，却超越了如GPT-4o等更大基线模型，相对增益达39.4%。此外，ToMAP展现出复杂的推理链和减少重复的特点，生成更多样化和有效的论点，适合长时间对话并采用更逻辑化和对手感知的策略。

Conclusion: ToMAP方法通过增强心理理论推理能力显著提升了说服者的性能，证明了其在发展更具说服力的语言代理方面的有效性和潜力。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


### [338] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
*Ziyin Zhang,Jiahao Xu,Zhiwei He,Tian Liang,Qiuzhi Liu,Yansi Li,Linfeng Song,Zhengwen Liang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: DeepTheorem是一个利用自然语言增强LLM数学推理的全面非正式定理证明框架，包含大规模基准数据集和新型强化学习策略RL-Zero，显著提升LLM定理证明性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动定理证明方法依赖于与LLM优势不匹配的形式化证明系统，需要一种结合自然语言知识的方法来增强LLM数学推理能力。

Method: 提出DeepTheorem框架，包括121K高质量IMO级别的非正式定理和证明的数据集、针对非正式定理证明设计的强化学习策略RL-Zero以及综合评估指标。

Result: 实验表明，DeepTheorem相比现有数据集和监督微调协议显著提高了LLM定理证明性能，达到最先进的准确率和推理质量。

Conclusion: DeepTheorem有潜力从根本上推动自动化非正式定理证明和数学探索的发展。

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [339] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
*Heekyung Lee,Jiaxin Ge,Tsung-Han Wu,Minwoo Kang,Trevor Darrell,David M. Chan*

Main category: cs.CL

TL;DR: 当前视觉-语言模型（VLMs）在解码简单的视觉线索方面表现出一些惊人的能力，但在需要抽象推理、横向思维和理解视觉隐喻的任务上仍显著受困。


<details>
  <summary>Details</summary>
Motivation: 研究视觉-语言模型（VLMs）对复杂多模态任务的处理能力，特别是其在解读和解决文字图像谜题（rebus puzzles）方面的表现。

Method: 构建一个手工生成和注释的英语文字图像谜题基准数据集，涵盖从简单图形替换到空间依赖提示的多种类型，并分析不同VLMs在该数据集上的表现。

Result: 发现VLMs虽然能够部分解读简单的视觉线索，但在涉及抽象推理、横向思考及理解视觉隐喻的任务上表现不佳。

Conclusion: 文字图像谜题为评估VLMs的多模态抽象和符号推理能力提供了一个独特且具有挑战性的场景，现有的VLMs在这一领域还有很大的提升空间。

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


### [340] [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
*Wentao Zhang,Woojeong Kim,Yuntian Deng*

Main category: cs.CL

TL;DR: Conversational agents using LLMs generate massive conversational data. Existing approaches miss insights from aggregating large-scale conversation logs. This paper introduces Aggregative Question Answering and constructs a benchmark WildChat-AQA to enable research in extracting collective insights.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing methods that treat user-chatbot interactions as independent, missing critical insights from aggregating and reasoning across large-scale conversation logs.

Method: Introduced Aggregative Question Answering task requiring models to reason explicitly over thousands of user-chatbot interactions to answer aggregative queries. Constructed a benchmark WildChat-AQA with 6,027 questions derived from 182,330 real-world conversations.

Result: Experiments showed existing methods struggle to reason effectively or incur prohibitive computational costs when handling aggregative queries.

Conclusion: There is a need for new approaches capable of extracting collective insights from large-scale conversational data.

Abstract: Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.

</details>


### [341] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
*Liangliang Zhang,Zhuorui Jiang,Hongliang Chi,Haoyang Chen,Mohammed Elkoumy,Fali Wang,Qiong Wu,Zhengyi Zhou,Shirui Pan,Suhang Wang,Yao Ma*

Main category: cs.CL

TL;DR: 尽管现有的KGQA数据集存在许多质量问题，本文提出了KGQAGen框架，通过结合结构化知识、LLM生成和符号验证，创建了高质量的KGQA基准数据集KGQAGen-10k，并揭示了现有模型的不足。


<details>
  <summary>Details</summary>
Motivation: 当前流行的KGQA数据集（如WebQSP和CWQ）存在标注不准确、问题设计不佳以及知识过时或不一致等问题，平均事实正确率仅为57%，严重影响了KGQA系统的评估效果。

Method: 提出了一种基于LLM的KGQAGen框架，该框架通过结构化知识支持、LLM引导生成和符号验证，系统性地解决了现有数据集的问题，并构建了一个基于Wikidata的包含一万个实例的高质量基准数据集KGQAGen-10k。

Result: 实验表明，即使是最先进的KG-RAG模型在KGQAGen-10k上也表现不佳，突显了该基准能够有效揭示现有模型的局限性。

Conclusion: KGQAGen为更严格的基准构建提供了可扩展的框架，有助于推动KGQA领域的进步和模型性能的提升。

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


### [342] [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
*Zijun Yao,Yantao Liu,Yanxu Chen,Jianhui Chen,Junfeng Fang,Lei Hou,Juanzi Li,Tat-Seng Chua*

Main category: cs.CL

TL;DR: Recent large reasoning models (LRMs) show strong performance in complex tasks but their tendency to hallucinate in fact-seeking tasks is unclear. This paper evaluates LRMs' hallucination, analyzes post-training impacts, identifies critical cognitive behaviors affecting factuality, and investigates the relationship between model uncertainty and factual accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand if reasoning models are more prone to hallucination and how different post-training methods affect this issue.

Method: 1) Holistic evaluation of hallucination in LRMs. 2) Behavior analysis identifying Flaw Repetition and Think-Answer Mismatch. 3) Investigation into the relationship between model uncertainty and factual accuracy.

Result: LRMs with full post-training including cold start SFT and verifiable reward RL reduce hallucination, while distillation alone or RL without cold start fine-tuning increases nuanced hallucinations. Two cognitive behaviors significantly impact factuality. Increased hallucination correlates with misalignment between model uncertainty and factual accuracy.

Conclusion: This work provides an initial understanding of hallucination in LRMs through evaluation, behavior analysis, and investigation into model uncertainty.

Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in
solving complex tasks with long chain-of-thought (CoT) reasoning capability. As
these LRMs are mostly developed by post-training on formal reasoning tasks,
whether they generalize the reasoning capability to help reduce hallucination
in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1
reports increased performance on SimpleQA, a fact-seeking benchmark, while
OpenAI-o3 observes even severer hallucination. This discrepancy naturally
raises the following research question: Are reasoning models more prone to
hallucination? This paper addresses the question from three perspectives. (1)
We first conduct a holistic evaluation for the hallucination in LRMs. Our
analysis reveals that LRMs undergo a full post-training pipeline with cold
start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate
their hallucination. In contrast, both distillation alone and RL training
without cold start fine-tuning introduce more nuanced hallucinations. (2) To
explore why different post-training pipelines alters the impact on
hallucination in LRMs, we conduct behavior analysis. We characterize two
critical cognitive behaviors that directly affect the factuality of a LRM: Flaw
Repetition, where the surface-level reasoning attempts repeatedly follow the
same underlying flawed logic, and Think-Answer Mismatch, where the final answer
fails to faithfully match the previous CoT process. (3) Further, we investigate
the mechanism behind the hallucination of LRMs from the perspective of model
uncertainty. We find that increased hallucination of LRMs is usually associated
with the misalignment between model uncertainty and factual accuracy. Our work
provides an initial understanding of the hallucination in LRMs.

</details>


### [343] [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
*Luke McDermott,Robert W. Heath Jr.,Rahul Parhi*

Main category: cs.CL

TL;DR: Transformer-based large language models face challenges with quadratic complexity during inference on long sequences. While linear attention methods are more efficient, they do not accurately approximate softmax attention. This paper introduces LoLA (Low-rank Linear Attention with sparse caching), which addresses these issues by separately storing key-value pairs that might interfere with past associative memories. LoLA uses three forms of memory to close the gap between linear attention models and transformers. It significantly improves accuracy in base subquadratic models at 4K context lengths and performs well on zero-shot commonsense reasoning tasks. Additionally, LoLA is a lightweight approach that can be reproduced on a single consumer GPU.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to address the limitations of transformer-based large language models, specifically their quadratic complexity during inference on long sequences. The authors also aim to improve upon linear attention methods, which, although more efficient, fail to provide an accurate approximation of softmax attention.

Method: LoLA employs Low-rank Linear Attention with sparse caching. It stores additional key-value pairs that could otherwise interfere with past associative memories. Furthermore, it distributes past key-value pairs into three types of memory: recent pairs in a local sliding window, difficult-to-memorize pairs in a sparse global cache, and generic pairs in the recurrent hidden state of linear attention.

Result: LoLA enables pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to 97.4% at 4K context lengths, with a 4.6x smaller cache than Llama-3.1 8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning tasks among 1B and 8B parameter subquadratic models.

Conclusion: LoLA proves to be an effective and lightweight solution for improving the efficiency and accuracy of linear attention models when dealing with long context lengths. Its results can be reproduced on a single consumer GPU, making it an accessible approach.

Abstract: Transformer-based large language models suffer from quadratic complexity at
inference on long sequences. Linear attention methods are efficient
alternatives, however, they fail to provide an accurate approximation of
softmax attention. By additionally incorporating sliding window attention into
each linear attention head, this gap can be closed for short context-length
tasks. Unfortunately, these approaches cannot recall important information from
long contexts due to "memory collisions". In this paper , we propose LoLA:
Low-rank Linear Attention with sparse caching. LoLA separately stores
additional key-value pairs that would otherwise interfere with past associative
memories. Moreover, LoLA further closes the gap between linear attention models
and transformers by distributing past key-value pairs into three forms of
memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize
pairs in a sparse, global cache; and (iii) generic pairs in the recurrent
hidden state of linear attention. As an inference-only strategy, LoLA enables
pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks
from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to
97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1
8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning
tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an
extremely lightweight approach: Nearly all of our results can be reproduced on
a single consumer GPU.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [344] [CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems](https://arxiv.org/abs/2505.22868)
*Md Hasibul Amin,Mohammadreza Mohammadi,Jason D. Bakos,Ramtin Zand*

Main category: cs.ET

TL;DR: In this paper, the authors present CrossNAS, an automated framework that optimizes machine learning workload deployment on analog PIM systems by exploring a multidimensional search space across circuits, architecture, and systems. It combines a single-path one-shot weight-sharing strategy with evolutionary search, setting a new benchmark in PIM NAS for accuracy and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to develop an automated approach to optimize the deployment of machine learning workloads on analog PIM systems by efficiently searching through various design abstraction layers.

Method: The proposed method, CrossNAS, explores a vast, multidimensional search space spanning circuits, architecture, and systems. It leverages the single-path one-shot weight-sharing strategy combined with evolutionary search for PIM system mapping and optimization.

Result: CrossNAS sets a new benchmark for PIM neural architecture search (NAS), outperforming previous methods in both accuracy and energy efficiency while maintaining comparable or shorter search times.

Conclusion: The conclusion is that CrossNAS provides an effective automated solution for optimizing machine learning workload deployment on analog PIM systems, achieving superior performance in terms of accuracy and energy efficiency.

Abstract: In this paper, we propose the CrossNAS framework, an automated approach for
exploring a vast, multidimensional search space that spans various design
abstraction layers-circuits, architecture, and systems-to optimize the
deployment of machine learning workloads on analog processing-in-memory (PIM)
systems. CrossNAS leverages the single-path one-shot weight-sharing strategy
combined with the evolutionary search for the first time in the context of PIM
system mapping and optimization. CrossNAS sets a new benchmark for PIM neural
architecture search (NAS), outperforming previous methods in both accuracy and
energy efficiency while maintaining comparable or shorter search times.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [345] [A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems](https://arxiv.org/abs/2505.22814)
*Jonghan Lim,Ilya Kovalenko*

Main category: cs.MA

TL;DR: In this paper, the authors address the challenges faced by traditional control approaches in manufacturing environments due to increasing complexity and unpredictability. They propose a large language model-enabled control architecture for multi-agent manufacturing systems that enhances real-time adaptation and context-aware decision-making, leading to improved system resilience and flexibility.


<details>
  <summary>Details</summary>
Motivation: Manufacturing environments are becoming more complex and unpredictable, which requires real-time decision-making and adaptation to disruptions. Traditional control approaches show limitations in responsiveness within dynamic industrial settings.

Method: The authors introduce a large language model-enabled control architecture for multi-agent manufacturing systems. This architecture aims to dynamically explore resource capabilities in response to real-time disruptions.

Result: A simulation-based case study demonstrates that the proposed architecture improves system resilience and flexibility. The findings indicate improved throughput and efficient resource utilization compared to existing approaches.

Conclusion: The large language model-enabled control architecture for multi-agent manufacturing systems successfully addresses the challenges related to real-time adaptation, context-aware decision-making, and dynamic exploration of resource capabilities.

Abstract: Manufacturing environments are becoming more complex and unpredictable due to
factors such as demand variations and shorter product lifespans. This
complexity requires real-time decision-making and adaptation to disruptions.
Traditional control approaches highlight the need for advanced control
strategies capable of overcoming unforeseen challenges, as they demonstrate
limitations in responsiveness within dynamic industrial settings. Multi-agent
systems address these challenges through decentralization of decision-making,
enabling systems to respond dynamically to operational changes. However,
current multi-agent systems encounter challenges related to real-time
adaptation, context-aware decision-making, and the dynamic exploration of
resource capabilities. Large language models provide the possibility to
overcome these limitations through context-aware decision-making capabilities.
This paper introduces a large language model-enabled control architecture for
multi-agent manufacturing systems to dynamically explore resource capabilities
in response to real-time disruptions. A simulation-based case study
demonstrates that the proposed architecture improves system resilience and
flexibility. The case study findings show improved throughput and efficient
resource utilization compared to existing approaches.

</details>


### [346] [Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.23352)
*Xu Shen,Yixin Liu,Yiwei Dai,Yili Wang,Rui Miao,Yue Tan,Shirui Pan,Xin Wang*

Main category: cs.MA

TL;DR: 在基于大型语言模型的多智能体系统中，通信拓扑结构对智能体间的协作模式有着根本性的影响，并且对于集体决策的效率和效果至关重要。本文提出了一种因果框架，分析了在不同稀疏程度的拓扑结构下，智能体输出（无论是正确还是错误）如何传播。通过实证研究发现，适度稀疏的拓扑结构能够有效抑制错误传播，同时保留有益信息的扩散，从而达到最佳的任务性能。基于这一见解，提出了一种新的拓扑设计方法——EIB-learner，该方法通过融合稠密图和稀疏图的连接模式，在错误抑制和有益信息传播之间取得平衡。广泛的实验表明，EIB-learner在有效性、通信成本和鲁棒性方面具有优越的表现。


<details>
  <summary>Details</summary>
Motivation: 当前关于通信拓扑自动化设计的研究倾向于构建稀疏结构以提高效率，但往往忽视了稀疏和稠密拓扑在何时以及为何有助于或阻碍协作的问题。

Method: 1. 提出一种因果框架，用于分析不同稀疏程度的拓扑结构下，智能体输出的传播方式。
2. 通过实证研究揭示适度稀疏的拓扑结构在抑制错误传播和保留有益信息扩散方面的优势。
3. 基于上述分析，提出EIB-learner方法，通过融合稠密图和稀疏图的连接模式来优化拓扑设计。

Result: 实验证明，EIB-learner在任务性能、通信成本和鲁棒性等方面表现优异，能够在错误抑制和有益信息传播之间实现良好的平衡。

Conclusion: 适度稀疏的拓扑结构是实现高效多智能体协作的关键，而EIB-learner提供了一种有效的解决方案，能够在错误传播和信息扩散之间找到平衡点。

Abstract: The communication topology in large language model-based multi-agent systems
fundamentally governs inter-agent collaboration patterns, critically shaping
both the efficiency and effectiveness of collective decision-making. While
recent studies for communication topology automated design tend to construct
sparse structures for efficiency, they often overlook why and when sparse and
dense topologies help or hinder collaboration. In this paper, we present a
causal framework to analyze how agent outputs, whether correct or erroneous,
propagate under topologies with varying sparsity. Our empirical studies reveal
that moderately sparse topologies, which effectively suppress error propagation
while preserving beneficial information diffusion, typically achieve optimal
task performance. Guided by this insight, we propose a novel topology design
approach, EIB-leanrner, that balances error suppression and beneficial
information propagation by fusing connectivity patterns from both dense and
sparse graphs. Extensive experiments show the superior effectiveness,
communication cost, and robustness of EIB-leanrner.

</details>


### [347] [Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging](https://arxiv.org/abs/2505.23584)
*Sumbal Malik,Majid Khonji,Khaled Elbassioni,Jorge Dias*

Main category: cs.MA

TL;DR: This paper introduces a novel VRP-DR problem with drones and robots for collaborative delivery, formulates it as MILP, and develops a scalable heuristic algorithm FINDER. The study shows significant time and cost savings.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of e-commerce and the demand for timely, cost-effective last-mile delivery have increased interest in collaborative logistics.

Method: The VRP-DR problem is formulated as a mixed-integer linear program (MILP) to minimize operational costs and makespan. A scalable heuristic algorithm, FINDER, is developed to provide near-optimal solutions for large-scale instances.

Result: Numerical experiments show significant time savings of the combined delivery mode over the truck-only mode and substantial cost reductions from enabling multi-visits. Insights are provided on the effects of en-route charging, docking flexibility, drone count, speed, and payload capacity on system performance.

Conclusion: The VRP-DR model and FINDER algorithm offer efficient solutions for collaborative synchronized multi-platform vehicle routing problems with drones and robots.

Abstract: The rapid growth of e-commerce and the increasing demand for timely,
cost-effective last-mile delivery have increased interest in collaborative
logistics. This research introduces a novel collaborative synchronized
multi-platform vehicle routing problem with drones and robots (VRP-DR), where a
fleet of $\mathcal{M}$ trucks, $\mathcal{N}$ drones and $\mathcal{K}$ robots,
cooperatively delivers parcels. Trucks serve as mobile platforms, enabling the
launching, retrieving, and en-route charging of drones and robots, thereby
addressing critical limitations such as restricted payload capacities, limited
range, and battery constraints. The VRP-DR incorporates five realistic
features: (1) multi-visit service per trip, (2) multi-trip operations, (3)
flexible docking, allowing returns to the same or different trucks (4) cyclic
and acyclic operations, enabling return to the same or different nodes; and (5)
en-route charging, enabling drones and robots to recharge while being
transported on the truck, maximizing operational efficiency by utilizing idle
transit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)
to minimize both operational costs and makespan. To overcome the computational
challenges of solving large-scale instances, a scalable heuristic algorithm,
FINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to
provide efficient, near-optimal solutions. Numerical experiments across various
instance sizes evaluate the performance of the MILP and heuristic approaches in
terms of solution quality and computation time. The results demonstrate
significant time savings of the combined delivery mode over the truck-only mode
and substantial cost reductions from enabling multi-visits. The study also
provides insights into the effects of en-route charging, docking flexibility,
drone count, speed, and payload capacity on system performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [348] [Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters](https://arxiv.org/abs/2505.23554)
*Hayden Moore,Sirui Qi,Ninad Hogade,Dejan Milojicic,Cullen Bash,Sudeep Pasricha*

Main category: cs.DC

TL;DR: The paper proposes SLIT, a framework to co-optimize LLM's quality of service, carbon emissions, water usage, and energy costs using an ML-based metaheuristic approach.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of LLMs, there is growing concern about their environmental impact during inference phase which exceeds that of the training phase. This includes higher operational costs, greater carbon footprint, and significant water usage.

Method: The proposed framework SLIT uses a machine learning based metaheuristic to enhance sustainability in hosting LLMs across geo-distributed cloud datacenters by optimizing time-to-first token, carbon emissions, water usage, and energy costs.

Result: SLIT aims to significantly improve the sustainability metrics of LLM operations without compromising on the quality of service.

Conclusion: As LLMs continue to grow in usage, frameworks like SLIT will be essential in addressing the sustainability challenges associated with their operation.

Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and
Gemini have been widely adopted in different areas. As the use of LLMs
continues to grow, many efforts have focused on reducing the massive training
overheads of these models. But it is the environmental impact of handling user
requests to LLMs that is increasingly becoming a concern. Recent studies
estimate that the costs of operating LLMs in their inference phase can exceed
training costs by 25x per year. As LLMs are queried incessantly, the cumulative
carbon footprint for the operational phase has been shown to far exceed the
footprint during the training phase. Further, estimates indicate that 500 ml of
fresh water is expended for every 20-50 requests to LLMs during inference. To
address these important sustainability issues with LLMs, we propose a novel
framework called SLIT to co-optimize LLM quality of service (time-to-first
token), carbon emissions, water usage, and energy costs. The framework utilizes
a machine learning (ML) based metaheuristic to enhance the sustainability of
LLM hosting across geo-distributed cloud datacenters. Such a framework will
become increasingly vital as LLMs proliferate.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [349] [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
*Vladimir Bataev,Andrei Andrusenko,Lilit Grigoryan,Aleksandr Laptev,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: This paper presents NGPU-LM, a new approach for statistical n-gram language models that enhances computational efficiency and parallel operations for ASR context-biasing tasks, reducing the accuracy gap between greedy and beam search without significant slowdown.


<details>
  <summary>Details</summary>
Motivation: Existing implementations of statistical n-gram language models for ASR context-biasing lack computational efficiency due to poor parallelization, making them less appealing for industrial use.

Method: The proposed method, NGPU-LM, rethinks data structures for statistical n-gram language models to enable fast and parallel operations optimized for GPU inference. It introduces customizable greedy decoding applicable to all major ASR model types with minimal computational overhead.

Result: NGPU-LM can eliminate more than 50% of the accuracy gap between greedy and beam search in out-of-domain scenarios while avoiding the significant slowdown typically caused by beam search.

Conclusion: The implementation of NGPU-LM is open-sourced, offering an efficient solution for context-biasing tasks in ASR.

Abstract: Statistical n-gram language models are widely used for context-biasing tasks
in Automatic Speech Recognition (ASR). However, existing implementations lack
computational efficiency due to poor parallelization, making context-biasing
less appealing for industrial use. This work rethinks data structures for
statistical n-gram language models to enable fast and parallel operations for
GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable
greedy decoding for all major ASR model types - including transducers,
attention encoder-decoder models, and CTC - with less than 7% computational
overhead. The proposed approach can eliminate more than 50% of the accuracy gap
between greedy and beam search for out-of-domain scenarios while avoiding
significant slowdown caused by beam search. The implementation of the proposed
NGPU-LM is open-sourced.

</details>


### [350] [Spoken question answering for visual queries](https://arxiv.org/abs/2505.23308)
*Nimrod Shabtay,Zvi Kons,Avihu Dekel,Hagai Aronowitz,Ron Hoory,Assaf Arbelle*

Main category: eess.AS

TL;DR: This paper explores the creation of a spoken visual question answering (SVQA) system that integrates text, speech, and image modalities to answer spoken questions on images. Given the lack of a suitable dataset, the authors synthesize VQA datasets using zero-shot Text-to-Speech (TTS) models. Results show that models trained with synthesized speech perform nearly as well as those trained on textual QAs, with TTS model choice having minimal impact on accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a system capable of handling multi-modal inputs, specifically integrating speech and visual data for more natural user interaction beyond traditional text-based QA systems.

Method: Fusion of text, speech, and image modalities to create an SVQA system. Synthesis of VQA datasets using two zero-shot TTS models to overcome the absence of a three-modality dataset.

Result: Models trained solely with synthesized speech achieve performance close to models trained on textual QAs. The selection of TTS model has little effect on the model's accuracy.

Conclusion: The study successfully demonstrates the feasibility of creating an SVQA system using synthesized speech data, paving the way for further advancements in multi-modal QA systems.

Abstract: Question answering (QA) systems are designed to answer natural language
questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA
system to accept visual and spoken input respectively.
  This work aims to create a system that enables user interaction through both
speech and images. That is achieved through the fusion of text, speech, and
image modalities to tackle the task of spoken VQA (SVQA). The resulting
multi-modal model has textual, visual, and spoken inputs and can answer spoken
questions on images.
  Training and evaluating SVQA models requires a dataset for all three
modalities, but no such dataset currently exists. We address this problem by
synthesizing VQA datasets using two zero-shot TTS models. Our initial findings
indicate that a model trained only with synthesized speech nearly reaches the
performance of the upper-bounding model trained on textual QAs. In addition, we
show that the choice of the TTS model has a minor impact on accuracy.

</details>


### [351] [DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration](https://arxiv.org/abs/2505.23515)
*Sanberk Serbest,Tijana Stojkovic,Milos Cernak,Andrew Harper*

Main category: eess.AS

TL;DR: The paper proposes a real-time speech enhancement system using GAN-based stochastic regeneration, highlighting the significance of noisy conditioning through an ablation study.


<details>
  <summary>Details</summary>
Motivation: Predictive models tend to over-suppress speech content when enhancing audio, leading to distortion. Combining predictive and generative models can mitigate this issue.

Method: The authors employ a GAN-based stochastic regeneration framework to create a real-time speech enhancement system with 3.58M parameters and low latency, focusing on a lightweight architecture suitable for streaming applications.

Result: Experiments indicate improvements in the NISQA-MOS metric compared to the first stage, and an ablation study demonstrates the importance of noisy conditioning in the system's performance.

Conclusion: The proposed real-time speech enhancement system effectively reduces distortion and highlights the critical role of noisy conditioning. Further enhancements were made after participating in the 2025 Urgent Challenge.

Abstract: In this work, we propose a full-band real-time speech enhancement system with
GAN-based stochastic regeneration. Predictive models focus on estimating the
mean of the target distribution, whereas generative models aim to learn the
full distribution. This behavior of predictive models may lead to
over-suppression, i.e. the removal of speech content. In the literature, it was
shown that combining a predictive model with a generative one within the
stochastic regeneration framework can reduce the distortion in the output. We
use this framework to obtain a real-time speech enhancement system. With 3.58M
parameters and a low latency, our system is designed for real-time streaming
with a lightweight architecture. Experiments show that our system improves over
the first stage in terms of NISQA-MOS metric. Finally, through an ablation
study, we show the importance of noisy conditioning in our system. We
participated in 2025 Urgent Challenge with our model and later made further
improvements.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [352] [Quantum Hilbert Transform](https://arxiv.org/abs/2505.23581)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: The paper introduces a quantum Hilbert transform (QHT) and applies it to a quantum steganography protocol.


<details>
  <summary>Details</summary>
Motivation: There is no existing quantum analogue for the Hilbert transform, which is fundamental in signal processing across multiple disciplines.

Method: The authors formulate the quantum Hilbert transform by bridging classical phase-shift techniques with quantum operations.

Result: The quantum Hilbert transform is introduced and applied successfully to a quantum steganography protocol, opening new pathways in various quantum technologies.

Conclusion: The introduction of QHT provides new opportunities in quantum signal processing, communications, sensing, and secure information hiding.

Abstract: The Hilbert transform has been one of the foundational transforms in signal
processing, finding it's way into multiple disciplines from cryptography to
biomedical sciences. However, there does not exist any quantum analogue for the
Hilbert transform. In this work, we introduce a formulation for the quantum
Hilbert transform (QHT)and apply it to a quantum steganography protocol. By
bridging classical phase-shift techniques with quantum operations, QHT opens
new pathways in quantum signal processing, communications, sensing, and secure
information hiding.

</details>


### [353] [Towards A Global Quantum Internet: A Review of Challenges Facing Aerial Quantum Networks](https://arxiv.org/abs/2505.23603)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: Quantum networks using aerial links like drones, balloons or satellites can enhance scalability of secure communication networks.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of aerial links in increasing the scalability of quantum networks.

Method: Reviewing how aerial links work, identifying difficulties in building them and proposing possible solutions.

Result: Combining ground stations, aerial relays and orbiting satellites into a seamless system could bring us closer to a practical quantum internet.

Conclusion: Using aerial links in quantum networks is promising for creating a scalable and secure quantum internet.

Abstract: Quantum networks use principles of quantum physics to create secure
communication networks. Moving these networks off the ground using drones,
balloons, or satellites could help increase the scalability of these networks.
This article reviews how such aerial links work, what makes them difficult to
build, and the possible solutions that can be used to overcome these problems.
By combining ground stations, aerial relays, and orbiting satellites into one
seamless system, we move closer to a practical quantum internet.

</details>


### [354] [Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing](https://arxiv.org/abs/2505.23066)
*Shuyin Xia,Xiaojiang Tian,Suzhen Yuan,Jeremiah D. Deng*

Main category: quant-ph

TL;DR: 提出了一种名为GB-QkNN的新算法，通过使用颗粒球和HNSW方法的量化来减少kNN算法的时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 高时间复杂度是k-最近邻(kNN)算法面临的主要挑战之一，现有的经典和量子kNN算法在处理大数据时仍存在速度瓶颈。

Method: 首先采用颗粒球减少需要处理的数据量，然后通过采用分层可导航小世界(HNSW)方法加速搜索过程，并通过量化优化耗时步骤（如距离计算），从而进一步降低构建和搜索过程的时间复杂度。

Result: 结合使用颗粒球和HNSW方法的量化，该方法能够显著降低kNN类算法的时间复杂度。

Conclusion: 全面的复杂性分析表明，所提出的方法有效地减少了kNN类算法的时间复杂度。

Abstract: High time complexity is one of the biggest challenges faced by $k$-Nearest
Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have
made some improvements, they still have a speed bottleneck when facing large
amounts of data. To address this issue, we propose an innovative algorithm
called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves
higher efficiency by first employing granular-balls, which reduces the data
size needed to processed. The search process is then accelerated by adopting a
Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the
time-consuming steps, such as distance calculation, of the HNSW via
quantization, further reducing the time complexity of the construct and search
process. By combining the use of granular-balls and quantization of the HNSW
method, our approach manages to take advantage of these treatments and
significantly reduces the time complexity of the $k$NN-like algorithms, as
revealed by a comprehensive complexity analysis.

</details>


### [355] [Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood](https://arxiv.org/abs/2505.22743)
*Sitan Chen,Weiyuan Gong,Jonas Haferkamp,Yihui Quek*

Main category: quant-ph

TL;DR: 这篇文章将经典低度多项式方法扩展到量子领域，用以研究量子数据学习中的计算困难问题，并在多个具体场景下证明了信息-计算鸿沟的存在。


<details>
  <summary>Details</summary>
Motivation: 尽管经典统计推断问题的平均情况硬度证据工具丰富，但量子领域的相应工具极为有限，因此需要一种框架来预测量子推理问题的难度。

Method: 作者建立了状态设计与低度硬度之间的通用联系，将其应用于吉布斯态学习、随机浅层量子电路态学习以及量子误差缓解等领域。同时，定义了一个新的量子广义种植二团问题，并研究了其在不同测量方式下的计算复杂性。

Result: 首次获得了关于随机稀疏非局域哈密顿量吉布斯态学习的信息-计算鸿沟；证明了自适应基测量模型下随机浅层量子电路态学习的困难性；得到了局部测量和更纠缠单拷贝测量下种植二团问题的计算复杂性转变；展示了噪声学习稳定器和产品态不可知学习的平均情况硬度。

Conclusion: 本文通过将低度方法推广到量子领域，为理解量子数据学习中的计算困难提供了新工具，并揭示了若干重要问题中的信息-计算鸿沟。

Abstract: In a variety of physically relevant settings for learning from quantum data,
designing protocols that can computationally efficiently extract information
remains largely an art, and there are important cases where we believe this to
be impossible, that is, where there is an information-computation gap. While
there is a large array of tools in the classical literature for giving evidence
for average-case hardness of statistical inference problems, the corresponding
tools in the quantum literature are far more limited. One such framework in the
classical literature, the low-degree method, makes predictions about hardness
of inference problems based on the failure of estimators given by low-degree
polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree
hardness. We use this to obtain the first information-computation gaps for
learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it
to prove hardness for learning random shallow quantum circuit states in a
challenging model where states can be measured in adaptively chosen bases. To
our knowledge, the ability to model adaptivity within the low-degree framework
was open even in classical settings. In addition, we also obtain a low-degree
hardness result for quantum error mitigation against strategies with
single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and
identify the threshold at which this problem becomes computationally hard for
protocols that perform local measurements. Interestingly, the complexity
landscape for this problem shifts when going from local measurements to more
entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning
Stabilizers with Noise and for agnostically learning product states.

</details>


### [356] [Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference](https://arxiv.org/abs/2505.23389)
*Ivana Nikoloska,Hamdi Joudeh,Ruud van Sloun,Osvaldo Simeone*

Main category: quant-ph

TL;DR: The paper proposes an online control framework for variational quantum sensing (VQS) that dynamically updates parameters and provides error bars, ensuring reliable performance on NISQ devices.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current VQS methods which lack rigorous performance guarantees when applied to noisy intermediate-scale quantum (NISQ) devices.

Method: An online control framework is proposed for VQS that dynamically updates variational parameters while providing deterministic error bars using online conformal inference techniques.

Result: Experiments on a quantum magnetometry task show that the proposed dynamic VQS approach maintains reliability over time and yields precise estimates.

Conclusion: Combining variational quantum algorithms with online conformal inference offers practical benefits for achieving reliable quantum sensing on NISQ devices.

Abstract: Quantum sensing exploits non-classical effects to overcome limitations of
classical sensors, with applications ranging from gravitational-wave detection
to nanoscale imaging. However, practical quantum sensors built on noisy
intermediate-scale quantum (NISQ) devices face significant noise and sampling
constraints, and current variational quantum sensing (VQS) methods lack
rigorous performance guarantees. This paper proposes an online control
framework for VQS that dynamically updates the variational parameters while
providing deterministic error bars on the estimates. By leveraging online
conformal inference techniques, the approach produces sequential estimation
sets with a guaranteed long-term risk level. Experiments on a quantum
magnetometry task confirm that the proposed dynamic VQS approach maintains the
required reliability over time, while still yielding precise estimates. The
results demonstrate the practical benefits of combining variational quantum
algorithms with online conformal inference to achieve reliable quantum sensing
on NISQ devices.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [357] [StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis](https://arxiv.org/abs/2505.22746)
*Jose Guadalupe Hernandez,Attri Ghosh,Philip J. Freda,Yufei Meng,Nicholas Matsumoto,Jason H. Moore*

Main category: cs.NE

TL;DR: The paper introduces StarBASE-GP, a tool using genetic programming to discover genetic variants linked to phenotypic changes. It optimizes machine learning pipelines for genomic data, incorporates biological knowledge, and performs well in identifying BMI-related variants in rats.


<details>
  <summary>Details</summary>
Motivation: To create an automated framework for discovering meaningful genetic variants associated with phenotypic variation in large-scale genomic datasets.

Method: StarBASE-GP uses genetic programming-based multi-objective optimization strategy to evolve machine learning pipelines that maximize explanatory power and minimize complexity. It integrates biological domain knowledge at multiple stages including inheritance encoding strategies, linkage disequilibrium pruning, and dynamic variant recommendation system.

Result: StarBASE-GP consistently evolves Pareto fronts with superior performance, yielding higher accuracy in identifying both ground truth and novel quantitative trait loci related to body mass index in Rattus norvegicus.

Conclusion: StarBASE-GP demonstrates robust potential for advancing variant discovery in complex traits by incorporating evolutionary search and relevant biological theory into a flexible automated machine learning framework.

Abstract: We present the Star-Based Automated Single-locus and Epistasis analysis tool
- Genetic Programming (StarBASE-GP), an automated framework for discovering
meaningful genetic variants associated with phenotypic variation in large-scale
genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective
optimization strategy to evolve machine learning pipelines that simultaneously
maximize explanatory power (r2) and minimize pipeline complexity. Biological
domain knowledge is integrated at multiple stages, including the use of nine
inheritance encoding strategies to model deviations from additivity, a custom
linkage disequilibrium pruning node that minimizes redundancy among features,
and a dynamic variant recommendation system that prioritizes informative
candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of
Rattus norvegicus (brown rat) to identify variants associated with body mass
index, benchmarking its performance against a random baseline and a
biologically naive version of the tool. StarBASE-GP consistently evolves Pareto
fronts with superior performance, yielding higher accuracy in identifying both
ground truth and novel quantitative trait loci, highlighting relevant targets
for future validation. By incorporating evolutionary search and relevant
biological theory into a flexible automated machine learning framework,
StarBASE-GP demonstrates robust potential for advancing variant discovery in
complex traits.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [358] [Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints](https://arxiv.org/abs/2505.22889)
*Hamidreza Montazeri Hedesh,Moh Kamalul Wafi,Milad Siami*

Main category: eess.SY

TL;DR: This paper explores the local stability of nonlinear Lur'e systems with static nonlinear feedback implemented by feedforward neural networks (FFNNs). It introduces two methods for estimating the Region of Attraction (ROA) and shows improvements over existing approaches in terms of ROA size and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study the local stability of nonlinear systems in the Lur'e form with static nonlinear feedback realized by FFNNs, aiming to develop methods for estimating the Region of Attraction (ROA) that are less conservative and more scalable than current techniques.

Method: The authors use a localized variant of the Aizerman conjecture providing sufficient conditions for exponential stability. They then develop two distinct methods: a Lyapunov-based approach constructing invariant sublevel sets of a quadratic function satisfying an LMI, and a novel technique computing tight local sector bounds for FFNNs via layer-wise propagation of linear relaxations.

Result: Numerical results indicate significant improvements in both the size of the estimated ROA and scalability compared to existing integral quadratic constraint-based approaches.

Conclusion: The proposed methods offer substantial enhancements in estimating the ROA for nonlinear systems with FFNN feedback, demonstrating better performance and scalability.

Abstract: We study the local stability of nonlinear systems in the Lur'e form with
static nonlinear feedback realized by feedforward neural networks (FFNNs). By
leveraging positivity system constraints, we employ a localized variant of the
Aizerman conjecture, which provides sufficient conditions for exponential
stability of trajectories confined to a compact set. Using this foundation, we
develop two distinct methods for estimating the Region of Attraction (ROA): (i)
a less conservative Lyapunov-based approach that constructs invariant sublevel
sets of a quadratic function satisfying a linear matrix inequality (LMI), and
(ii) a novel technique for computing tight local sector bounds for FFNNs via
layer-wise propagation of linear relaxations. These bounds are integrated into
the localized Aizerman framework to certify local exponential stability.
Numerical results demonstrate substantial improvements over existing integral
quadratic constraint-based approaches in both ROA size and scalability.

</details>


### [359] [User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model](https://arxiv.org/abs/2505.11047)
*Arghya Mallick,Georgios Pantazis,Mohammad Khosravi,Peyman Mohajerin Esfahani,Sergio Grammatico*

Main category: eess.SY

TL;DR: The paper proposes a data-driven, user-centric V2G methodology using multi-objective optimization to balance battery degradation and V2G revenue based on EV user preference. The method uses ICNNs for predicting battery degradation and integrates it into an optimization framework to generate optimal smart charging profiles.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accurate and generalizable battery degradation models and provide a user-centric approach to balance financial benefits from V2G participation with battery health.

Method: Using input convex neural networks (ICNNs) to develop a data-driven degradation model trained on extensive experimental datasets, then integrating this model into a multi-objective optimization framework to generate optimal smart charging profiles for each EV.

Result: Numerical simulations show high accuracy in predicting battery degradation for unseen data and present a trade-off curve illustrating financial benefits versus battery health losses based on user preferences.

Conclusion: The proposed methodology effectively balances battery degradation and V2G revenue according to EV user preference, providing smart charging strategies under realistic scenarios.

Abstract: We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology
based on multi-objective optimization to balance battery degradation and V2G
revenue according to EV user preference. Given the lack of accurate and
generalizable battery degradation models, we leverage input convex neural
networks (ICNNs) to develop a data-driven degradation model trained on
extensive experimental datasets. This approach enables our model to capture
nonconvex dependencies on battery temperature and time while maintaining
convexity with respect to the charging rate. Such a partial convexity property
ensures that the second stage of our methodology remains computationally
efficient. In the second stage, we integrate our data-driven degradation model
into a multi-objective optimization framework to generate an optimal smart
charging profile for each EV. This profile effectively balances the trade-off
between financial benefits from V2G participation and battery degradation,
controlled by a hyperparameter reflecting the user prioritization of battery
health. Numerical simulations show the high accuracy of the ICNN model in
predicting battery degradation for unseen data. Finally, we present a trade-off
curve illustrating financial benefits from V2G versus losses from battery
health degradation based on user preferences and showcase smart charging
strategies under realistic scenarios.

</details>


### [360] [From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems](https://arxiv.org/abs/2505.23710)
*Zeinab Nezami,Syed Danial Ali Shah,Maryam Hafeez,Karim Djemame,Syed Ali Raza Zaidi*

Main category: eess.SY

TL;DR: The paper discusses the vision of 6G as a self-evolving telecom ecosystem, emphasizing AI-driven intelligence and autonomous communication systems to improve real-time decision-making, efficiency, and reduce latency in networked control systems.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of 6G networks as a self-evolving telecom ecosystem with AI-driven intelligence for dynamic adaptation beyond static connectivity.

Method: Investigating key enablers such as reconfigurable infrastructure, adaptive middleware, intelligent network functions, and multi-agent collaboration within the context of industrial IoT frameworks.

Result: Improved real-time decision-making, optimized efficiency, and reduced latency in networked control systems were highlighted as potential benefits. The research also addresses ethical challenges, research directions, and standardization efforts.

Conclusion: This research contributes to the next generation of intelligent automation solutions by bridging the gap between theoretical advancements and real-world industrial applications through state-of-the-art 6G network management techniques.

Abstract: This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven
intelligence enables dynamic adaptation beyond static connectivity. We explore
the key enablers of autonomous communication systems, spanning reconfigurable
infrastructure, adaptive middleware, and intelligent network functions,
alongside multi-agent collaboration for distributed decision-making. We explore
how these methodologies align with emerging industrial IoT frameworks, ensuring
seamless integration within digital manufacturing processes. Our findings
emphasize the potential for improved real-time decision-making, optimizing
efficiency, and reducing latency in networked control systems. The discussion
addresses ethical challenges, research directions, and standardization efforts,
concluding with a technology stack roadmap to guide future developments. By
leveraging state-of-the-art 6G network management techniques, this research
contributes to the next generation of intelligent automation solutions,
bridging the gap between theoretical advancements and real-world industrial
applications.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [361] [PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models](https://arxiv.org/abs/2505.22674)
*Pawan Neupane,Jian Liu,Jianlin Cheng*

Main category: q-bio.BM

TL;DR: PSBench是一个包含超过一百万个蛋白质结构模型的基准套件，涵盖了广泛的蛋白质特性。它提供了多种质量评分和评估指标，并展示了其在训练和评估EMA方法（如GATE）中的实用性，证明了其对推动蛋白质复合物建模中EMA研究的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管AI方法能够预测许多蛋白质复合物的准确结构模型，但可靠地估计这些预测模型的质量（即EMA）仍然是一个主要挑战。缺乏大规模、多样化和良好标注的数据集是开发有效机器学习EMA方法的关键障碍。

Method: PSBench包含四个大型标注数据集，来自CASP15和CASP16。它包括超过一百万个结构模型，涵盖不同的蛋白质序列长度、复合物化学计量、功能类别和建模难度。每个模型都有多个补充质量评分，并提供多种评估指标和基线EMA方法。

Result: 使用PSBench训练和评估的GATE方法，在CASP16中表现优异，排名在顶级EMA方法之列。这表明PSBench在推进EMA研究方面的价值。

Conclusion: PSBench为蛋白质复合物建模中的EMA研究提供了一个有价值的资源，促进了该领域的发展。

Abstract: Predicting protein complex structures is essential for protein function
analysis, protein design, and drug discovery. While AI methods like AlphaFold
can predict accurate structural models for many protein complexes, reliably
estimating the quality of these predicted models (estimation of model accuracy,
or EMA) for model ranking and selection remains a major challenge. A key
barrier to developing effective machine learning-based EMA methods is the lack
of large, diverse, and well-annotated datasets for training and evaluation. To
address this gap, we introduce PSBench, a benchmark suite comprising four
large-scale, labeled datasets generated during the 15th and 16th community-wide
Critical Assessment of Protein Structure Prediction (CASP15 and CASP16).
PSBench includes over one million structural models covering a wide range of
protein sequence lengths, complex stoichiometries, functional classes, and
modeling difficulties. Each model is annotated with multiple complementary
quality scores at the global, local, and interface levels. PSBench also
provides multiple evaluation metrics and baseline EMA methods to facilitate
rigorous comparisons. To demonstrate PSBench's utility, we trained and
evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE
was blindly tested in CASP16 (2024), where it ranked among the top-performing
EMA methods. These results highlight PSBench as a valuable resource for
advancing EMA research in protein complex modeling. PSBench is publicly
available at: https://github.com/BioinfoMachineLearning/PSBench.

</details>


### [362] [Representing local protein environments with atomistic foundation models](https://arxiv.org/abs/2505.23354)
*Meital Bojan,Sanketh Vedula,Advaith Maddipatla,Nadav Bojan Sellam,Federico Napoli,Paul Schanda,Alex M. Bronstein*

Main category: q-bio.BM

TL;DR: The paper proposes a new representation for local protein environments using atomistic foundation models (AFMs), demonstrating its effectiveness in capturing structural and chemical features, enabling data-driven priors, and achieving state-of-the-art accuracy in NMR spectroscopy.


<details>
  <summary>Details</summary>
Motivation: Current representations of local protein environments are insufficient due to the extensive structural and chemical variability.

Method: A novel representation derived from intermediate features of atomistic foundation models (AFMs) is proposed to represent local protein environments.

Result: This representation captures both local structure and chemical features effectively, shows meaningful structure in the representation space, and achieves state-of-the-art accuracy in physics-informed chemical shift prediction in NMR spectroscopy.

Conclusion: Atomistic foundation models provide an effective alternative beyond traditional molecular simulations for protein modeling, opening new avenues for constructing functional representations for protein environments.

Abstract: The local structure of a protein strongly impacts its function and
interactions with other molecules. Therefore, a concise, informative
representation of a local protein environment is essential for modeling and
designing proteins and biomolecular interactions. However, these environments'
extensive structural and chemical variability makes them challenging to model,
and such representations remain under-explored. In this work, we propose a
novel representation for a local protein environment derived from the
intermediate features of atomistic foundation models (AFMs). We demonstrate
that this embedding effectively captures both local structure (e.g., secondary
motifs), and chemical features (e.g., amino-acid identity and protonation
state). We further show that the AFM-derived representation space exhibits
meaningful structure, enabling the construction of data-driven priors over the
distribution of biomolecular environments. Finally, in the context of
biomolecular NMR spectroscopy, we demonstrate that the proposed representations
enable a first-of-its-kind physics-informed chemical shift predictor that
achieves state-of-the-art accuracy. Our results demonstrate the surprising
effectiveness of atomistic foundation models and their emergent representations
for protein modeling beyond traditional molecular simulations. We believe this
will open new lines of work in constructing effective functional
representations for protein environments.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [363] [Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion](https://arxiv.org/abs/2505.22673)
*Wasif Khan,Kyle B. See,Simon Kato,Ziqian Huang,Amy Lazarte,Kyle Douglas,Xiangyang Lou,Teng J. Peng,Dhanashree Rajderkar,John Rees,Pina Sanelli,Amita Singh,Ibrahim Tuna,Christina A. Wilson,Ruogu Fang*

Main category: q-bio.TO

TL;DR: A novel deep learning framework named MAGIC is proposed to generate contrast-free CTP imaging maps from non-contrast CT images, providing a cost-effective and safe alternative for stroke assessment with high diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: CTP imaging is crucial for stroke treatment but involves expensive contrast agents that can cause adverse effects. There's a need for a safer and more economical method for assessing brain perfusion.

Method: The MAGIC framework uses deep learning to convert non-contrast CT images into multiple CTP maps by integrating generative AI and physiological data. The network was trained on patient data and incorporates physiological characteristics in loss terms to improve image quality.

Result: MAGIC showed robust performance even with abnormal brain perfusion and was validated through a double-blinded study with experienced medical professionals, demonstrating comparable diagnostic accuracy to traditional CTP with contrast.

Conclusion: MAGIC offers a promising solution for contrast-free, cost-efficient, and rapid perfusion imaging which could significantly impact healthcare positively.

Abstract: Perfusion imaging is extensively utilized to assess hemodynamic status and
tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging
plays a key role in the early assessment and planning of stroke treatment.
While CTP provides essential perfusion parameters to identify abnormal blood
flow in the brain, the use of contrast agents in CTP can lead to allergic
reactions and adverse side effects, along with costing USD 4.9 billion
worldwide in 2022. To address these challenges, we propose a novel deep
learning framework called Multitask Automated Generation of Intermodal CT
perfusion maps (MAGIC). This framework combines generative artificial
intelligence and physiological information to map non-contrast computed
tomography (CT) imaging to multiple contrast-free CTP imaging maps. We
demonstrate enhanced image fidelity by incorporating physiological
characteristics into the loss terms. Our network was trained and validated
using CT image data from patients referred for stroke at UF Health and
demonstrated robustness to abnormalities in brain perfusion activity. A
double-blinded study was conducted involving seven experienced
neuroradiologists and vascular neurologists. This study validated MAGIC's
visual quality and diagnostic accuracy showing favorable performance compared
to clinical perfusion imaging with intravenous contrast injection. Overall,
MAGIC holds great promise in revolutionizing healthcare by offering
contrast-free, cost-effective, and rapid perfusion imaging.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [364] [A Tool for Generating Exceptional Behavior Tests With Large Language Models](https://arxiv.org/abs/2505.22818)
*Linghan Zhong,Samuel Yuan,Jiyang Zhang,Yu Liu,Pengyu Nie,Junyi Jessy Li,Milos Gligoric*

Main category: cs.SE

TL;DR: exLong is a framework that uses a fine-tuned LLM to automatically generate exceptional behavior tests (EBTs) for software development, addressing the gap of under-tested exceptional scenarios.


<details>
  <summary>Details</summary>
Motivation: Developers often prioritize testing 'happy paths' rather than exceptional scenarios, leading to a need for better tools to generate EBTs.

Method: exLong leverages a large language model fine-tuned from CodeLlama and incorporates reasoning about exception-throwing traces, conditional expressions guarding throw statements, and non-exceptional behavior tests executing similar traces.

Result: exLong can effectively assist developers in creating comprehensive EBTs for their projects, as shown in a demonstration video.

Conclusion: exLong presents a solution to the under-testing of exceptional scenarios by automating the generation of EBTs.

Abstract: Exceptional behavior tests (EBTs) are crucial in software development for
verifying that code correctly handles unwanted events and throws appropriate
exceptions. However, prior research has shown that developers often prioritize
testing "happy paths", e.g., paths without unwanted events over exceptional
scenarios. We present exLong, a framework that automatically generates EBTs to
address this gap. exLong leverages a large language model (LLM) fine-tuned from
CodeLlama and incorporates reasoning about exception-throwing traces,
conditional expressions that guard throw statements, and non-exceptional
behavior tests that execute similar traces. Our demonstration video illustrates
how exLong can effectively assist developers in creating comprehensive EBTs for
their project (available at https://youtu.be/Jro8kMgplZk).

</details>


### [365] [OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software](https://arxiv.org/abs/2505.23239)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.SE

TL;DR: OSS-UAgent is an automated usability evaluation framework for open source software using intelligent agents powered by large language models, providing comprehensive measures of software usability.


<details>
  <summary>Details</summary>
Motivation: Usability evaluation is crucial for open source software but traditional methods are costly and not scalable.

Method: The framework employs intelligent agents driven by large language models to simulate developers at various experience levels performing programming tasks. It dynamically constructs platform-specific knowledge bases for accurate code generation and automatically evaluates the generated code across multiple dimensions.

Result: OSS-UAgent provides a comprehensive measure of software usability including compliance, correctness, and readability. The demonstration on graph analytics platforms highlights its effectiveness in automating usability evaluations.

Conclusion: OSS-UAgent addresses the limitations of traditional usability evaluation methods by offering an automated, configurable, and interactive agent-based framework specifically designed for open source software.

Abstract: Usability evaluation is critical to the impact and adoption of open source
software (OSS), yet traditional methods relying on human evaluators suffer from
high costs and limited scalability. To address these limitations, we introduce
OSS-UAgent, an automated, configurable, and interactive agent-based usability
evaluation framework specifically designed for open source software. Our
framework employs intelligent agents powered by large language models (LLMs) to
simulate developers performing programming tasks across various experience
levels (from Junior to Expert). By dynamically constructing platform-specific
knowledge bases, OSS-UAgent ensures accurate and context-aware code generation.
The generated code is automatically evaluated across multiple dimensions,
including compliance, correctness, and readability, providing a comprehensive
measure of the software's usability. Additionally, our demonstration showcases
OSS-UAgent's practical application in evaluating graph analytics platforms,
highlighting its effectiveness in automating usability evaluation.

</details>


### [366] [Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization](https://arxiv.org/abs/2505.23387)
*Mingzhe Du,Luu Tuan Tuan,Yue Liu,Yuhao Qing,Dong Huang,Xinyi He,Qian Liu,Zejun Ma,See-kiong Ng*

Main category: cs.SE

TL;DR: Large Language Models (LLMs) can create correct solutions but lack code efficiency. This paper presents an iterative optimization framework where LLMs refine code based on sandbox feedback. Three training strategies are explored, with GRPO showing the most significant improvement in code performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the code efficiency of Large Language Models (LLMs) which is a critical factor for real-world deployment.

Method: A test-time iterative optimization framework is introduced, using a closed-loop system where LLMs refine code based on feedback from an execution sandbox. Three training strategies - Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO) are explored.

Result: Experiments show that SFT and DPO quickly reach a plateau in efficiency gains, whereas GRPO continuously optimizes code performance, significantly improving pass@1 rates and the likelihood of outperforming human submissions in efficiency.

Conclusion: This work effectively improves code efficiency at test-time and highlights the potential of reinforcement learning in enabling LLMs to self-improve their code efficiency.

Abstract: Large Language Models (LLMs) generate functionally correct solutions but
often fall short in code efficiency, a critical bottleneck for real-world
deployment. In this paper, we introduce a novel test-time iterative
optimization framework to address this, employing a closed-loop system where
LLMs iteratively refine code based on empirical performance feedback from an
execution sandbox. We explore three training strategies: Supervised Fine-Tuning
(SFT), Direct Preference Optimization (DPO), and Group Relative Policy
Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark
show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,
using reinforcement learning (RL) with execution feedback, continuously
optimizes code performance, significantly boosting both pass@1 (from 47% to
62%) and the likelihood of outperforming human submissions in efficiency (from
31% to 45%). Our work demonstrates effective test-time code efficiency
improvement and critically reveals the power of RL in teaching LLMs to truly
self-improve code efficiency.

</details>


### [367] [Toward Effective AI Governance: A Review of Principles](https://arxiv.org/abs/2505.23417)
*Danilo Ribeiro,Thayssa Rocha,Gustavo Pinto,Bruno Cartaxo,Marcelo Amaral,Nicole Davila,Ana Camargo*

Main category: cs.SE

TL;DR: AI治理是确保AI系统负责任、道德和安全开发与部署的实践。尽管AI治理是负责任AI的核心支柱，但当前文献仍然缺乏对这些治理框架和实践的综合分析。


<details>
  <summary>Details</summary>
Motivation: 确定在AI治理的次级文献中强调哪些框架、原则、机制和利益相关者角色。

Method: 进行了快速三级审查，选择了来自IEEE和ACM（2020-2024）的九个同行评审的次级研究，使用结构化的纳入标准和主题语义综合方法。

Result: 引用最多的框架包括欧盟AI法案和NIST风险管理框架；透明度和问责制是最常见的原则。很少有评论详细说明可操作的治理机制或利益相关者策略。

Conclusion: 本综述整合了AI治理的关键方向，并强调了实证验证和包容性方面的空白。研究结果为学术探究和组织的实际应用提供了依据。

Abstract: Artificial Intelligence (AI) governance is the practice of establishing
frameworks, policies, and procedures to ensure the responsible, ethical, and
safe development and deployment of AI systems. Although AI governance is a core
pillar of Responsible AI, current literature still lacks synthesis across such
governance frameworks and practices. Objective: To identify which frameworks,
principles, mechanisms, and stakeholder roles are emphasized in secondary
literature on AI governance. Method: We conducted a rapid tertiary review of
nine peer-reviewed secondary studies from IEEE and ACM (20202024), using
structured inclusion criteria and thematic semantic synthesis. Results: The
most cited frameworks include the EU AI Act and NIST RMF; transparency and
accountability are the most common principles. Few reviews detail actionable
governance mechanisms or stakeholder strategies. Conclusion: The review
consolidates key directions in AI governance and highlights gaps in empirical
validation and inclusivity. Findings inform both academic inquiry and practical
adoption in organizations.

</details>


### [368] [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
*Linghao Zhang,Shilin He,Chaoyun Zhang,Yu Kang,Bowen Li,Chengxing Xie,Junhao Wang,Maoquan Wang,Yufan Huang,Shengyu Fu,Elsie Nallipogu,Qingwei Lin,Yingnong Dang,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: The paper introduces SWE-bench-Live, a live-updatable benchmark for evaluating LLMs in real-world bug-fixing tasks, addressing limitations of existing benchmarks by providing automated curation, reproducibility through Docker images, and scalability.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks like SWE-bench suffer from lack of updates, narrow repository coverage, heavy manual effort, and risks of overfitting/data contamination. This limits their effectiveness in evaluating LLM capabilities in dynamic software development settings.

Method: SWE-bench-Live includes an automated curation pipeline that enables instance creation, environment setup, and continuous updates without manual intervention. It also provides dedicated Docker images for reproducible execution and consists of 1,319 tasks from 93 repositories with real GitHub issues.

Result: Evaluation on SWE-bench-Live reveals a significant performance gap between state-of-the-art LLMs/agents and static benchmarks, even under controlled conditions. Detailed analyses highlight discrepancies based on repository origin, issue recency, and task difficulty.

Conclusion: SWE-bench-Live offers a fresh, diverse, and executable benchmark that addresses the limitations of existing benchmarks, enabling rigorous evaluation of LLMs and agents in real-world software development scenarios.

Abstract: The issue-resolving task, where a model generates patches to fix real-world
bugs, has emerged as a critical benchmark for evaluating the capabilities of
large language models (LLMs). While SWE-bench and its variants have become
standard in this domain, they suffer from key limitations: they have not been
updated since their initial releases, cover a narrow set of repositories, and
depend heavily on manual effort for instance construction and environment
setup. These factors hinder scalability and introduce risks of overfitting and
data contamination. In this work, we present \textbf{SWE-bench-Live}, a
\textit{live-updatable} benchmark designed to overcome these challenges. Our
initial release consists of 1,319 tasks derived from real GitHub issues created
since 2024, spanning 93 repositories. Each task is accompanied by a dedicated
Docker image to ensure reproducible execution. Central to our benchmark is
\method, an automated curation pipeline that streamlines the entire process
from instance creation to environment setup, removing manual bottlenecks and
enabling scalability and continuous updates. We evaluate a range of
state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a
substantial performance gap compared to static benchmarks like SWE-bench, even
under controlled evaluation conditions. To better understand this discrepancy,
we perform detailed analyses across repository origin, issue recency, and task
difficulty. By providing a fresh, diverse, and executable benchmark grounded in
live repository activity, SWE-bench-Live facilitates rigorous,
contamination-resistant evaluation of LLMs and agents in dynamic, real-world
software development settings.

</details>


### [369] [From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents](https://arxiv.org/abs/2505.23422)
*Tobias Lindenbauer,Georg Groh,Hinrich Schütze*

Main category: cs.SE

TL;DR: CTIM-Rover is an AI agent for Software Engineering that uses Cross-Task-Instance Memory (CTIM) but does not outperform AutoCodeRover due to noise in CTIM items.


<details>
  <summary>Details</summary>
Motivation: Existing SE agents discard long-term memory after a single task instance, which is inefficient. Repository-level understanding is crucial for bug fixing, thus the authors hypothesize SE can benefit from CTIM.

Method: Built CTIM-Rover on top of AutoCodeRover and used ExpeL approach to create a mixture-of-experts inspired method for general-purpose and repository-level CTIM.

Result: CTIM-Rover did not outperform AutoCodeRover in any configuration. Noise from distracting CTIM items or exemplar trajectories likely caused performance degradation.

Conclusion: ExpeL and DoT-Bank do not scale to real-world SE problems due to performance degradation from noise in CTIM items.

Abstract: We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on
top of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning
frameworks with an episodic memory, more specifically, a general and
repository-level Cross-Task-Instance Memory (CTIM). While existing open-source
SE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,
2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning
frameworks inefficiently discard their long-term memory after a single task
instance. As repository-level understanding is pivotal for identifying all
locations requiring a patch for fixing a bug, we hypothesize that SE is
particularly well positioned to benefit from CTIM. For this, we build on the
Experiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a
Mixture-Of-Experts (MoEs) inspired approach to create both a general-purpose
and repository-level CTIM. We find that CTIM-Rover does not outperform
AutoCodeRover in any configuration and thus conclude that neither ExpeL nor
DoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis
indicates noise introduced by distracting CTIM items or exemplar trajectories
as the likely source of the performance degradation.

</details>


### [370] [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
*Manish Shetty,Naman Jain,Jinjian Liu,Vijay Kethanaboyina,Koushik Sen,Ion Stoica*

Main category: cs.SE

TL;DR: A new benchmark named GSO is introduced to evaluate language models' capabilities in developing high-performance software. The automated pipeline identifies 102 challenging optimization tasks across 10 codebases. Leading SWE-Agents achieve less than 5% success rate, with failure modes including difficulties with low-level languages and inaccurate bottleneck localization.


<details>
  <summary>Details</summary>
Motivation: To assess the capabilities of language models in creating high-performance software, which requires specialized expertise.

Method: Developed an automated pipeline that generates performance tests and analyzes repository commit histories to identify optimization tasks. Agents are tasked to improve runtime efficiency based on a given codebase and performance test.

Result: Quantitative evaluation shows leading SWE-Agents have a less than 5% success rate, with limited improvements from inference-time scaling. Qualitative analysis reveals key failure modes.

Conclusion: GSO benchmark highlights significant struggles of current language models in software optimization, providing opportunities for future research.

Abstract: Developing high-performance software is a complex task that requires
specialized expertise. We introduce GSO, a benchmark for evaluating language
models' capabilities in developing high-performance software. We develop an
automated pipeline that generates and executes performance tests to analyze
repository commit histories to identify 102 challenging optimization tasks
across 10 codebases, spanning diverse domains and programming languages. An
agent is provided with a codebase and performance test as a precise
specification, and tasked to improve the runtime efficiency, which is measured
against the expert developer optimization. Our quantitative evaluation reveals
that leading SWE-Agents struggle significantly, achieving less than 5% success
rate, with limited improvements even with inference-time scaling. Our
qualitative analysis identifies key failure modes, including difficulties with
low-level languages, practicing lazy optimization strategies, and challenges in
accurately localizing bottlenecks. We release the code and artifacts of our
benchmark along with agent trajectories to enable future research.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [371] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/abs/2505.22685)
*Marcus J. Vroemen,Yuqian Chen,Yui Lo,Tengfei Xu,Weidong Cai,Fan Zhang,Josien P. W. Pluim,Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: Diffusion MRI (dMRI) tractography is vital for mapping brain structural connections. However, traditional connectome generation is time-consuming and requires gray matter parcellation. The paper introduces DeepMultiConnectome, a deep-learning model that predicts structural connectomes directly from tractography without needing gray matter parcellation while supporting multiple parcellation schemes. It uses a point-cloud-based neural network with multi-task learning to classify streamlines according to their connected regions across two parcellation schemes. After training on the Human Connectome Project Young Adult dataset, DeepMultiConnectome can predict multiple structural connectomes within 40 seconds, showing high correlation with traditionally generated connectomes and preserving network properties. Test-retest analysis demonstrates its reproducibility, and it performs similarly in predicting age and cognitive function.


<details>
  <summary>Details</summary>
Motivation: Traditional connectome generation methods are time-consuming and require gray matter parcellation, which poses challenges for large-scale studies. Therefore, there is a need for a more scalable and fast method for generating subject-specific connectomes across multiple parcellation schemes.

Method: The method involves developing a deep-learning model called DeepMultiConnectome. This model uses a point-cloud-based neural network with multi-task learning to classify streamlines according to their connected regions across two parcellation schemes, sharing a learned representation. It bypasses the need for gray matter parcellation while supporting multiple parcellation schemes.

Result: DeepMultiConnectome can predict multiple structural connectomes from a whole-brain tractogram containing 3 million streamlines in approximately 40 seconds. The predicted connectomes show high correlation with traditionally generated connectomes (r = 0.992 for an 84-region scheme; r = 0.986 for a 164-region scheme) and largely preserve network properties. Test-retest analysis demonstrates reproducibility comparable to traditionally generated connectomes, and the predicted connectomes perform similarly in predicting age and cognitive function.

Conclusion: DeepMultiConnectome provides a scalable, fast model for generating subject-specific connectomes across multiple parcellation schemes, making it suitable for large-scale studies.

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>


### [372] [Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis](https://arxiv.org/abs/2505.23353)
*Alexandra G. Roberts,Ha M. Luu,Mert Şişman,Alexey V. Dimov,Ceren Tozlu,Ilhami Kovanlikaya,Susan A. Gauthier,Thanh D. Nguyen,Yi Wang*

Main category: eess.IV

TL;DR: Quantitative susceptibility maps can help detect and segment paramagnetic rim lesions (PRLs) in multiple sclerosis, which are rare but important biomarkers. This study generates synthetic PRL data to address class imbalance, improving classifier performance with a novel denoising approach. Both methods enhance detection of PRLs in a clinically interpretable way.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the detection and segmentation of rare paramagnetic rim lesions (PRLs) in multiple sclerosis using quantitative susceptibility maps, as these lesions serve as emerging biomarkers for the disease.

Method: The method involves producing synthetic quantitative susceptibility maps of PRLs and incorporating them into classifiers to address class imbalance. A multi-channel extension generates accompanying contrasts and probabilistic segmentation maps. Additionally, a novel denoising approach using the projection capability of a trained generative network is exploited to handle ambiguous rim cases and increase the minority class.

Result: Both synthetic lesion synthesis and the proposed rim lesion label denoising method approximate the unseen rim lesion distribution well, leading to improved detection in a clinically interpretable manner.

Conclusion: Synthetic data generation and the novel denoising approach effectively address the class imbalance issue in detecting PRLs, enhancing classifier performance and providing clinically meaningful results.

Abstract: Quantitative susceptibility maps from magnetic resonance images can provide
both prognostic and diagnostic information in multiple sclerosis, a
neurodegenerative disease characterized by the formation of lesions in white
matter brain tissue. In particular, susceptibility maps provide adequate
contrast to distinguish between "rim" lesions, surrounded by deposited
paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions
(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been
devoted to both detection and segmentation of such lesions to monitor
longitudinal change. As paramagnetic rim lesions are rare, addressing this
problem requires confronting the class imbalance between rim and non-rim
lesions. We produce synthetic quantitative susceptibility maps of paramagnetic
rim lesions and show that inclusion of such synthetic data improves classifier
performance and provide a multi-channel extension to generate accompanying
contrasts and probabilistic segmentation maps. We exploit the projection
capability of our trained generative network to demonstrate a novel denoising
approach that allows us to train on ambiguous rim cases and substantially
increase the minority class. We show that both synthetic lesion synthesis and
our proposed rim lesion label denoising method best approximate the unseen rim
lesion distribution and improve detection in a clinically interpretable manner.
We release our code and generated data at https://github.com/agr78/PRLx-GAN
upon publication.

</details>


### [373] [Can Large Language Models Challenge CNNS in Medical Image Analysis?](https://arxiv.org/abs/2505.23503)
*Shibbir Ahmed,Shahnewaz Karim Sakib,Anindya Bijoy Das*

Main category: eess.IV

TL;DR: This study explores a multimodal AI framework for medical image classification using CNNs and LLMs, revealing that filtered LLMs can enhance performance while maintaining efficiency and sustainability.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of multimodal AI systems in improving medical diagnostics by comparing CNNs with various LLMs.

Method: A multimodal AI framework was developed to classify medical diagnostic images. The system used publicly available datasets and compared CNNs with different LLMs based on accuracy, F1-score, execution time, energy consumption, and CO2 emission.

Result: CNN-based models outperformed multimodal techniques combining images and contextual data, but filtering LLMs led to significant performance improvements.

Conclusion: Multimodal AI systems have transformative potential to increase the reliability, efficiency, and scalability of medical diagnostics.

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [374] [PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2505.23587)
*Christian Schmidt,Heinrich Martin Overhoff*

Main category: eess.IV

TL;DR: The paper explores the use of PCA in medical image segmentation to improve model performance across unseen ultrasound datasets. By reducing dimensionality, PCA enhances essential features and reduces noise, leading to better recall and Dice scores when using reconstructed datasets. This method significantly mitigates performance decline in external validation.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation models face challenges with external validity, especially in ultrasound imaging where datasets are small and diverse. Current solutions like domain adaptation or GAN-based style transfer have limitations in such scenarios.

Method: The authors apply PCA preprocessing to reduce noise and highlight essential features by retaining 90% of dataset variance. They create PCA-reconstructed datasets from six breast tumor ultrasound datasets and train U-Net-based segmentation models on both original and PCA datasets. Models are then tested across out-of-domain datasets for comparison.

Result: PCA reconstruction improved recall and Dice scores compared to baseline results, particularly for challenging cases. Statistically significant gains were observed in recall (0.57 ± 0.07 vs. 0.70 ± 0.05, p = 0.0004) and Dice scores (0.50 ± 0.06 vs. 0.58 ± 0.06, p = 0.03). The method reduced recall decline during external validation by 33%.

Conclusion: PCA reconstruction can serve as an effective safeguard against declines in segmentation performance due to external validation, offering potential improvements in real-world medical applications.

Abstract: In medical image segmentation, limited external validity remains a critical
obstacle when models are deployed across unseen datasets, an issue particularly
pronounced in the ultrasound image domain. Existing solutions-such as domain
adaptation and GAN-based style transfer-while promising, often fall short in
the medical domain where datasets are typically small and diverse. This paper
presents a novel application of principal component analysis (PCA) to address
this limitation. PCA preprocessing reduces noise and emphasizes essential
features by retaining approximately 90\% of the dataset variance. We evaluate
our approach across six diverse breast tumor ultrasound datasets comprising
3,983 B-mode images and corresponding expert tumor segmentation masks. For each
dataset, a corresponding dimensionality reduced PCA-dataset is created and
U-Net-based segmentation models are trained on each of the twelve datasets.
Each model trained on an original dataset was inferenced on the remaining five
out-of-domain original datasets (baseline results), while each model trained on
a PCA dataset was inferenced on five out-of-domain PCA datasets. Our
experimental results indicate that using PCA reconstructed datasets, instead of
original images, improves the model's recall and Dice scores, particularly for
model-dataset pairs where baseline performance was lowest, achieving
statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05,
$p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p =
0.03$). Our method reduced the decline in recall values due to external
validation by $33\%$. These findings underscore the potential of PCA
reconstruction as a safeguard to mitigate declines in segmentation performance,
especially in challenging cases, with implications for enhancing external
validity in real-world medical applications.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [375] [Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model](https://arxiv.org/abs/2505.23682)
*Rachel Cummings,Alessandro Epasto,Jieming Mao,Tamalika Mukherjee,Tingting Ou,Peilin Zhong*

Main category: cs.DS

TL;DR: 本论文设计了首个在turnstile流模型中具有次线性空间复杂度的差分隐私算法，用于计算不同元素的数量，并提供了理论下界。


<details>
  <summary>Details</summary>
Motivation: 在实时数据分析中，数据流长度和数据集大小可能非常大，因此需要研究在turnstile设置下使用比T和|U|次线性的空间复杂度的私有算法。

Method: 提出了一种新的算法，在任意数据流上实现~O_η(T^(1/3))的空间复杂度和加性误差，并提供(1+η)相对近似值。当项目出现次数W已知时，算法还提供~O_η(√W)的加性误差和空间复杂度。

Result: 该算法显著改善了现有技术对空间的需求，接近已知的Ω(T^(1/4))加性误差下界。并且，其结果解决了由Jain等人提出的关于设计低内存机制的问题。

Conclusion: 论文提出了首个次线性空间复杂度的差分隐私算法来解决计算不同元素数量的问题，并证明了任何类似技术的算法必须在任意数据流上使用~Ω(T^(1/3))的空间。

Abstract: The turnstile continual release model of differential privacy captures
scenarios where a privacy-preserving real-time analysis is sought for a dataset
evolving through additions and deletions. In typical applications of real-time
data analysis, both the length of the stream $T$ and the size of the universe
$|U|$ from which data come can be extremely large. This motivates the study of
private algorithms in the turnstile setting using space sublinear in both $T$
and $|U|$. In this paper, we give the first sublinear space differentially
private algorithms for the fundamental problem of counting distinct elements in
the turnstile streaming model. Our algorithm achieves, on arbitrary streams,
$\tilde{O}_{\eta}(T^{1/3})$ space and additive error, and a $(1+\eta)$-relative
approximation for all $\eta \in (0,1)$. Our result significantly improves upon
the space requirements of the state-of-the-art algorithms for this problem,
which is linear, approaching the known $\Omega(T^{1/4})$ additive error lower
bound for arbitrary streams. Moreover, when a bound $W$ on the number of times
an item appears in the stream is known, our algorithm provides
$\tilde{O}_{\eta}(\sqrt{W})$ additive error, using $\tilde{O}_{\eta}(\sqrt{W})$
space. This additive error asymptotically matches that of prior work which
required instead linear space. Our results address an open question posed by
[Jain, Kalemaj, Raskhodnikova, Sivakumar, Smith, Neurips23] about designing
low-memory mechanisms for this problem. We complement these results with a
space lower bound for this problem, which shows that any algorithm that uses
similar techniques must use space $\tilde{\Omega}(T^{1/3})$ on arbitrary
streams.

</details>


### [376] [Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves](https://arxiv.org/abs/2505.23431)
*Amer Krivošija,Alexander Munteanu,André Nusser,Chris Schwiegelshohn*

Main category: cs.DS

TL;DR: The paper introduces $k$-Dynamic Time Warping ($k$-DTW), a new dissimilarity measure for polygonal curves with stronger metric properties than DTW and more robustness to outliers than Fréchet distance. It provides exact and approximation algorithms, dimension-free learning bounds, and demonstrates its benefits in clustering and classification.


<details>
  <summary>Details</summary>
Motivation: Current dissimilarity measures like DTW and Fréchet distance have limitations in terms of metric properties and robustness to outliers.

Method: Introduced $k$-DTW as a novel measure, provided exact and $(1+\varepsilon)$-approximation algorithms using parametric search for the $k$-th largest matched distance, proved dimension-free learning bounds, and analyzed Rademacher and Gaussian complexities.

Result: $k$-DTW has stronger metric properties and is more robust to outliers. It allows smaller sample sizes for learning curve medians and admits strictly smaller bounds than DTW by a factor $\tilde\Omega(\sqrt{m})$ when $k\ll m$. Experimental results show its benefits in clustering and nearest neighbor classification.

Conclusion: $k$-DTW offers improved properties over existing measures and shows practical benefits in curve analysis tasks.

Abstract: This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel
dissimilarity measure for polygonal curves. $k$-DTW has stronger metric
properties than Dynamic Time Warping (DTW) and is more robust to outliers than
the Fr\'{e}chet distance, which are the two gold standards of dissimilarity
measures for polygonal curves. We show interesting properties of $k$-DTW and
give an exact algorithm as well as a $(1+\varepsilon)$-approximation algorithm
for $k$-DTW by a parametric search for the $k$-th largest matched distance. We
prove the first dimension-free learning bounds for curves and further learning
theoretic results. $k$-DTW not only admits smaller sample size than DTW for the
problem of learning the median of curves, where some factors depending on the
curves' complexity $m$ are replaced by $k$, but we also show a surprising
separation on the associated Rademacher and Gaussian complexities: $k$-DTW
admits strictly smaller bounds than DTW, by a factor $\tilde\Omega(\sqrt{m})$
when $k\ll m$. We complement our theoretical findings with an experimental
illustration of the benefits of using $k$-DTW for clustering and nearest
neighbor classification.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [377] [Non-convex entropic mean-field optimization via Best Response flow](https://arxiv.org/abs/2505.22760)
*Razvan-Andrei Lascu,Mateusz B. Majka*

Main category: math.OC

TL;DR: 研究了在概率测度空间上最小化非凸泛函的问题，以及求解熵正则化的非凸-非凹极小极大问题。通过选择合适的正则化器，可以使Best Response算子成为收缩映射，从而确保优化问题的唯一全局最小值。此方法可应用于强化学习中的策略优化。


<details>
  <summary>Details</summary>
Motivation: 将Best Response流（或虚构博弈流）应用于解决带相对熵正则化的非凸优化问题和非凸-非凹极小极大问题，探索如何通过选择正则化器来放松对凸性的假设。

Method: 利用Best Response流分析其收敛性，并根据非凸泛函、正则化参数和参考测度尾部行为之间的关系选择合适的正则化器，使得Best Response算子在$L^1$-Wasserstein距离下成为收缩映射。

Result: 当Best Response算子成为收缩映射时，存在其唯一的不动点，该点为优化问题的唯一全局最小值。此方法扩展了先前关于凸优化问题的结果，允许放松凸性假设。

Conclusion: 通过适当选择正则化器，可以解决非凸优化问题并确保唯一全局最小值的存在。此方法在强化学习中的策略优化方面具有应用潜力。

Abstract: We study the problem of minimizing non-convex functionals on the space of
probability measures, regularized by the relative entropy (KL divergence) with
respect to a fixed reference measure, as well as the corresponding problem of
solving entropy-regularized non-convex-non-concave min-max problems. We utilize
the Best Response flow (also known in the literature as the fictitious play
flow) and study how its convergence is influenced by the relation between the
degree of non-convexity of the functional under consideration, the
regularization parameter and the tail behaviour of the reference measure. In
particular, we demonstrate how to choose the regularizer, given the non-convex
functional, so that the Best Response operator becomes a contraction with
respect to the $L^1$-Wasserstein distance, which then ensures the existence of
its unique fixed point, which is then shown to be the unique global minimizer
for our optimization problem. This extends recent results where the Best
Response flow was applied to solve convex optimization problems regularized by
the relative entropy with respect to arbitrary reference measures, and with
arbitrary values of the regularization parameter. Our results explain precisely
how the assumption of convexity can be relaxed, at the expense of making a
specific choice of the regularizer. Additionally, we demonstrate how these
results can be applied in reinforcement learning in the context of policy
optimization for Markov Decision Processes and Markov games with softmax
parametrized policies in the mean-field regime.

</details>


### [378] [Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization](https://arxiv.org/abs/2505.23056)
*Zijian Liu,Zhengyuan Zhou*

Main category: math.OC

TL;DR: The paper investigates the convergence of shuffling gradient methods for minimizing finite-sum functions with regularization, providing improved last-iterate analysis for nonsmooth cases and demonstrating that Random Reshuffle and Single Shuffle strategies are faster than Proximal GD.


<details>
  <summary>Details</summary>
Motivation: To enhance the theoretical understanding of shuffling gradient methods and provide improved bounds for nonsmooth (strongly) convex functions.

Method: Providing first improved last-iterate analysis for nonsmooth case using Random Reshuffle (RR) and Single Shuffle (SS) strategies to demonstrate their superiority over Proximal GD.

Result: Proves that RR and SS strategies are provably faster than Proximal GD for nonsmooth (strongly) convex optimization and gives nearly optimal convergence result for suffix average under RR sampling scheme in general convex case.

Conclusion: This work advances the understanding of shuffling gradient methods by offering improved convergence rates and confirming the benefit of randomness in RR and SS strategies.

Abstract: We study the convergence of the shuffling gradient method, a popular
algorithm employed to minimize the finite-sum function with regularization, in
which functions are passed to apply (Proximal) Gradient Descent (GD) one by one
whose order is determined by a permutation on the indices of functions. In
contrast to its easy implementation and effective performance in practice, the
theoretical understanding remains limited. A recent advance by (Liu & Zhou,
2024b) establishes the first last-iterate convergence results under various
settings, especially proving the optimal rates for smooth (strongly) convex
optimization. However, their bounds for nonsmooth (strongly) convex functions
are only as fast as Proximal GD. In this work, we provide the first improved
last-iterate analysis for the nonsmooth case demonstrating that the widely used
Random Reshuffle ($\textsf{RR}$) and Single Shuffle ($\textsf{SS}$) strategies
are both provably faster than Proximal GD, reflecting the benefit of
randomness. As an important implication, we give the first (nearly) optimal
convergence result for the suffix average under the $\textsf{RR}$ sampling
scheme in the general convex case, matching the lower bound shown by (Koren et
al., 2022).

</details>


### [379] [Gradient Methods with Online Scaling Part I. Theoretical Foundations](https://arxiv.org/abs/2505.23081)
*Wenzhi Gao,Ya-Chi Chu,Yinyu Ye,Madeleine Udell*

Main category: math.OC

TL;DR: This paper introduces Online Scaled Gradient Methods (OSGM), which uses online learning to adapt stepsizes and accelerate first-order optimization methods, achieving convergence rates comparable to the optimal stepsize and explaining the success of hypergradient-descent heuristic.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the effectiveness of first-order optimization methods by adapting stepsizes using online learning techniques, thereby achieving faster convergence rates.

Method: OSGM quantifies the effectiveness of a stepsize via a feedback function derived from a convergence measure and adjusts the stepsize through an online learning algorithm. This framework ensures that its convergence rates are asymptotically no worse than those achieved with the optimal stepsize.

Result: OSGM provides convergence guarantees for smooth convex problems, including trajectory-dependent global convergence, improved complexity results on smooth strongly convex problems, and local superlinear convergence. It also constitutes a new family of first-order methods with non-asymptotic superlinear convergence.

Conclusion: OSGM not only accelerates first-order methods but also theoretically explains the empirical success of the hypergradient-descent heuristic in machine learning optimization.

Abstract: This paper establishes the theoretical foundations of the online scaled
gradient methods (OSGM), a framework that utilizes online learning to adapt
stepsizes and provably accelerate first-order methods. OSGM quantifies the
effectiveness of a stepsize by a feedback function motivated from a convergence
measure and uses the feedback to adjust the stepsize through an online learning
algorithm. Consequently, instantiations of OSGM achieve convergence rates that
are asymptotically no worse than the optimal stepsize. OSGM yields desirable
convergence guarantees on smooth convex problems, including 1)
trajectory-dependent global convergence on smooth convex objectives; 2) an
improved complexity result on smooth strongly convex problems, and 3) local
superlinear convergence. Notably, OSGM constitutes a new family of first-order
methods with non-asymptotic superlinear convergence, joining the celebrated
quasi-Newton methods. Finally, OSGM explains the empirical success of the
popular hypergradient-descent heuristic in optimization for machine learning.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [380] [Generative Social Choice: The Next Generation](https://arxiv.org/abs/2505.22939)
*Niclas Boehmer,Sara Fish,Ariel D. Procaccia*

Main category: cs.GT

TL;DR: The paper discusses a method to generate a concise set of statements that represent diverse user opinions by integrating social choice theory and large language models, specifically GPT-4o, showing its effectiveness in two datasets.


<details>
  <summary>Details</summary>
Motivation: To create a concise summary of statements that proportionally represents all user opinions, addressing the challenge through generative social choice.

Method: Extends a framework combining social choice and large language models with theoretical guarantees for approximately optimal queries and a budget limit on total statement length, using GPT-4o for query implementation.

Result: Demonstrated effectiveness in generating representative slates from unstructured user opinions in city improvement measures and drug reviews datasets.

Conclusion: The approach successfully generates representative summaries of user opinions under constraints, advancing the field of generative social choice.

Abstract: A key task in certain democratic processes is to produce a concise slate of
statements that proportionally represents the full spectrum of user opinions.
This task is similar to committee elections, but unlike traditional settings,
the candidate set comprises all possible statements of varying lengths, and so
it can only be accessed through specific queries. Combining social choice and
large language models, prior work has approached this challenge through a
framework of generative social choice. We extend the framework in two
fundamental ways, providing theoretical guarantees even in the face of
approximately optimal queries and a budget limit on the overall length of the
slate. Using GPT-4o to implement queries, we showcase our approach on datasets
related to city improvement measures and drug reviews, demonstrating its
effectiveness in generating representative slates from unstructured user
opinions.

</details>


### [381] [Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals](https://arxiv.org/abs/2505.23124)
*Junyan Liu,Arnab Maiti,Artin Tajdini,Kevin Jamieson,Lillian J. Ratliff*

Main category: cs.GT

TL;DR: 研究了一个在有限时间范围内的重复主代理问题，分析了两种可实现次线性后悔的关键设置，并提出了相应的算法及后悔界。


<details>
  <summary>Details</summary>
Motivation: 研究者关注一个主代理问题，在该问题中，主体需要与以对抗顺序到达的不同类型的代理交互，并通过选择激励来最小化后悔。由于代理行为的不确定性，此问题变得难以处理。因此，研究者希望探索在不同设定下如何实现次线性后悔。

Method: 研究者首先定义了问题模型和目标，即最小化后悔。然后，他们分析了两种关键设置：1) 主体知道每种代理类型在给定激励下会贪婪选择的臂；2) 代理的响应随激励平滑变化并由Lipschitz常数控制。针对每种设置，研究者提出了对应的算法，并提供了后悔界的理论分析。最后，研究者扩展了算法结果，允许主体在同一轮内同时激励多个臂。

Result: 在第一种设置下，研究者提出了一种算法，其后悔界为$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$，并与之匹配的下界相差一个$\log K$因子。在第二种设置下，研究者证明存在一种算法，其后悔界为$\tilde{O}((LN)^{1/3}T^{2/3})$，并且建立了匹配的下界（相差对数因子）。此外，研究者成功将这些算法结果扩展到允许同时激励多个臂的情况。

Conclusion: 本研究表明，在特定条件下，可以通过设计适当的算法来实现次线性后悔。这为主代理问题的研究提供了新的视角和方法，尤其是在面对不确定性和对抗顺序时。

Abstract: We initiate the study of a repeated principal-agent problem over a finite
horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of
agents arriving in an adversarial order. At each round, the principal
strategically chooses one of the $N$ arms to incentivize for an arriving agent
of unknown type. The agent then chooses an arm based on its own utility and the
provided incentive, and the principal receives a corresponding reward. The
objective is to minimize regret against the best incentive in hindsight.
Without prior knowledge of agent behavior, we show that the problem becomes
intractable, leading to linear regret. We analyze two key settings where
sublinear regret is achievable. In the first setting, the principal knows the
arm each agent type would select greedily for any given incentive. Under this
setting, we propose an algorithm that achieves a regret bound of
$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to
a $\log K$ factor. In the second setting, an agent's response varies smoothly
with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under
this setting, we show that there is an algorithm with a regret bound of
$\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to
logarithmic factors. Finally, we extend our algorithmic results for both
settings by allowing the principal to incentivize multiple arms simultaneously
in each round.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [382] [LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals](https://arxiv.org/abs/2505.23454)
*Yanbin Wang,Xingyu Chen,Yumiao Wang,Xiang Wang,Chuanfei Zang,Guolong Cui,Jiahuan Liu*

Main category: eess.SP

TL;DR: The paper proposes LCB-CV-UNet with a Logarithmic Connect Block and Dual Hybrid Dataset Construction method to handle HDR radar signals, improving detection probability with minimal computational complexity.


<details>
  <summary>Details</summary>
Motivation: To solve the performance degradation issue caused by High Dynamic Range (HDR) radar signals.

Method: Proposes LCB-CV-UNet which includes a hardware-efficient Logarithmic Connect Block (LCB) for phase coherence preservation and a Dual Hybrid Dataset Construction method for generating semi-synthetic datasets.

Result: Simulation results demonstrate a 1% improvement in total detection probability with less than 0.9% added computational complexity compared to the baseline. A 5% improvement is seen in the 11-13 dB signal-to-noise ratio range typical for urban targets.

Conclusion: The proposed model's practicality is validated through real experiments.

Abstract: We propose the LCB-CV-UNet to tackle performance degradation caused by High
Dynamic Range (HDR) radar signals. Initially, a hardware-efficient,
plug-and-play module named Logarithmic Connect Block (LCB) is proposed as a
phase coherence preserving solution to address the inherent challenges in
handling HDR features. Then, we propose the Dual Hybrid Dataset Construction
method to generate a semi-synthetic dataset, approximating typical HDR signal
scenarios with adjustable target distributions. Simulation results show about
1% total detection probability improvement with under 0.9% computational
complexity added compared with the baseline. Furthermore, it excels 5% over the
baseline at the range in 11-13 dB signal-to-noise ratio typical for urban
targets. Finally, the real experiment validates the practicality of our model.

</details>


### [383] [Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters](https://arxiv.org/abs/2505.22783)
*Charles E. Thornton,Jamie Sloop,Samuel Brown,Aaron Orndorff,William C. Headley,Stephen Young*

Main category: eess.SP

TL;DR: An end-to-end altitude estimation performance of a convolutional autoencoder-based interference mitigation approach for FMCW radar altimeters is investigated. A TCN autoencoder provides superior interference suppression compared to LMS adaptive filter and operates directly on the received FMCW signal.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of a TCN autoencoder in exploiting temporal correlations in the received signal for interference suppression in FMCW radar altimeters, as opposed to traditional methods like LMS adaptive filters.

Method: Investigating the performance of a convolutional autoencoder-based interference mitigation approach using a TCN autoencoder that operates directly on the received FMCW signal.

Result: The TCN autoencoder shows superior interference suppression capabilities compared to the LMS adaptive filter.

Conclusion: A TCN autoencoder is effective for interference mitigation in FMCW radar altimeters and offers superior performance over traditional methods. However, challenges remain in applying deep learning to wideband FMCW interference mitigation.

Abstract: We investigate the end-to-end altitude estimation performance of a
convolutional autoencoder-based interference mitigation approach for
frequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we
show that a Temporal Convolutional Network (TCN) autoencoder effectively
exploits temporal correlations in the received signal, providing superior
interference suppression compared to a Least Mean Squares (LMS) adaptive
filter. Unlike existing approaches, the present method operates directly on the
received FMCW signal. Additionally, we identify key challenges in applying deep
learning to wideband FMCW interference mitigation and outline directions for
future research to enhance real-time feasibility and generalization to
arbitrary interference conditions.

</details>


### [384] [Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes](https://arxiv.org/abs/2505.23160)
*Lorenzo Marinucci,Claudio Battiloro,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: 这篇论文提出了一种新颖的自适应框架，用于处理单纯复形上的动态流信号，将经典的最小均方（LMS）方法扩展到高阶拓扑域。通过离散霍奇理论，作者提出了一个拓扑LMS算法，该算法能够高效地处理随时间变化的边子集上观察到的流信号。论文对算法进行了详细的随机分析，推导了其稳定性条件、稳态均方误差和收敛速度，并探讨了边采样对性能的影响。此外，还提出了设计最优边采样概率的策略，以在确保期望估计精度的同时最小化速率。假设部分了解复杂结构（例如底层图），引入了一种自适应拓扑推断方法，与提出的LMS框架集成。此外，还提出了一种分布式版本的算法，并分析了其稳定性和均方误差特性。实验证明，在集中式和分布式设置中，该方法通过利用高阶拓扑特征优于基于图的LMS方法。


<details>
  <summary>Details</summary>
Motivation: 当前对于动态流信号的处理方法主要集中在图结构上，而未充分利用高阶拓扑信息。为了更有效地处理单纯复形上的动态流信号，需要一种新的框架来扩展经典LMS方法至高阶拓扑域。

Method: 1. 提出基于离散霍奇理论的拓扑LMS算法，用于处理动态流信号。
2. 对算法进行详细随机分析，包括稳定性条件、稳态均方误差和收敛速度。
3. 探讨边采样对性能的影响并提出最优边采样概率的设计策略。
4. 引入自适应拓扑推断方法，结合部分已知复杂结构知识。
5. 提出分布式版本的算法并分析其稳定性和均方误差特性。

Result: 实验证明，所提方法在集中式和分布式设置下均优于基于图的LMS方法，特别是在利用高阶拓扑特征方面表现突出。

Conclusion: 本文提出的自适应框架成功将经典LMS方法扩展到高阶拓扑域，通过有效处理动态流信号并在性能上超越传统方法，展示了利用高阶拓扑信息的优势。

Abstract: This paper introduces a novel adaptive framework for processing dynamic flow
signals over simplicial complexes, extending classical least-mean-squares (LMS)
methods to high-order topological domains. Building on discrete Hodge theory,
we present a topological LMS algorithm that efficiently processes streaming
signals observed over time-varying edge subsets. We provide a detailed
stochastic analysis of the algorithm, deriving its stability conditions,
steady-state mean-square-error, and convergence speed, while exploring the
impact of edge sampling on performance. We also propose strategies to design
optimal edge sampling probabilities, minimizing rate while ensuring desired
estimation accuracy. Assuming partial knowledge of the complex structure (e.g.,
the underlying graph), we introduce an adaptive topology inference method that
integrates with the proposed LMS framework. Additionally, we propose a
distributed version of the algorithm and analyze its stability and
mean-square-error properties. Empirical results on synthetic and real-world
traffic data demonstrate that our approach, in both centralized and distributed
settings, outperforms graph-based LMS methods by leveraging higher-order
topological features.

</details>
